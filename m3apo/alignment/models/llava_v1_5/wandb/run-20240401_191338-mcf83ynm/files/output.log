  0%|          | 0/10740 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/code/mm/MAPO/m3apo/alignment/trainer/llava_dpo_trainer.py:179: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/10740 [00:23<71:05:49, 23.83s/it]
{'loss': 0.6931, 'learning_rate': 6.1919504643962845e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'policy_logps/rejected': -513.5743408203125, 'policy_logps/chosen': -498.71343994140625, 'referece_logps/rejected': -513.5743408203125, 'referece_logps/chosen': -498.71343994140625, 'logits/rejected': -1.003807544708252, 'logits/chosen': -1.154007077217102, 'epoch': 0.0}


  0%|          | 3/10740 [01:03<61:39:37, 20.67s/it]
{'loss': 0.6922, 'learning_rate': 1.8575851393188852e-08, 'rewards/chosen': -0.04481248930096626, 'rewards/rejected': -0.02196197584271431, 'rewards/accuracies': 0.5, 'rewards/margins': -0.022850515320897102, 'policy_logps/rejected': -393.2767028808594, 'policy_logps/chosen': -456.56622314453125, 'referece_logps/rejected': -393.05712890625, 'referece_logps/chosen': -456.11810302734375, 'logits/rejected': -0.6224277019500732, 'logits/chosen': -0.6298500299453735, 'epoch': 0.0}

  0%|          | 4/10740 [01:20<57:26:23, 19.26s/it]

  0%|          | 5/10740 [01:41<58:58:08, 19.78s/it]


  0%|          | 7/10740 [02:14<53:45:38, 18.03s/it]

  0%|          | 8/10740 [02:36<57:13:21, 19.20s/it]
[2024-04-01 19:16:19,607] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6905, 'learning_rate': 4.9535603715170276e-08, 'rewards/chosen': -0.011748503893613815, 'rewards/rejected': -0.011624335311353207, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0001241695135831833, 'policy_logps/rejected': -461.9600524902344, 'policy_logps/chosen': -474.193603515625, 'referece_logps/rejected': -461.84381103515625, 'referece_logps/chosen': -474.07611083984375, 'logits/rejected': -1.0032861232757568, 'logits/chosen': -0.8940410614013672, 'epoch': 0.0}

  0%|          | 9/10740 [02:57<58:52:46, 19.75s/it]


  0%|          | 11/10740 [03:34<57:08:06, 19.17s/it]
{'loss': 0.7031, 'learning_rate': 6.811145510835913e-08, 'rewards/chosen': -0.018851377069950104, 'rewards/rejected': 0.017733382061123848, 'rewards/accuracies': 0.375, 'rewards/margins': -0.0365847572684288, 'policy_logps/rejected': -414.94854736328125, 'policy_logps/chosen': -494.8109436035156, 'referece_logps/rejected': -415.1258850097656, 'referece_logps/chosen': -494.6224365234375, 'logits/rejected': 0.09200985729694366, 'logits/chosen': -0.06180351972579956, 'epoch': 0.01}


  0%|          | 13/10740 [04:06<51:47:14, 17.38s/it]
{'loss': 0.6914, 'learning_rate': 8.04953560371517e-08, 'rewards/chosen': 0.033030129969120026, 'rewards/rejected': 0.029457664117217064, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0035724644549191, 'policy_logps/rejected': -294.8175354003906, 'policy_logps/chosen': -309.031005859375, 'referece_logps/rejected': -295.1121520996094, 'referece_logps/chosen': -309.3612976074219, 'logits/rejected': -0.9454291462898254, 'logits/chosen': -0.9443410634994507, 'epoch': 0.01}

  0%|          | 14/10740 [04:19<47:50:30, 16.06s/it]

  0%|          | 15/10740 [04:35<47:32:18, 15.96s/it]

  0%|          | 16/10740 [04:47<44:39:03, 14.99s/it]


  0%|          | 18/10740 [05:30<53:54:56, 18.10s/it]
{'loss': 0.6961, 'learning_rate': 1.1145510835913312e-07, 'rewards/chosen': 0.014376450330018997, 'rewards/rejected': -0.001906681340187788, 'rewards/accuracies': 0.75, 'rewards/margins': 0.016283130273222923, 'policy_logps/rejected': -249.22201538085938, 'policy_logps/chosen': -278.7533874511719, 'referece_logps/rejected': -249.2029571533203, 'referece_logps/chosen': -278.89715576171875, 'logits/rejected': -0.9637190103530884, 'logits/chosen': -1.0019800662994385, 'epoch': 0.01}


  0%|          | 20/10740 [06:12<58:18:15, 19.58s/it]
{'loss': 0.6933, 'learning_rate': 1.238390092879257e-07, 'rewards/chosen': -0.004392243921756744, 'rewards/rejected': 0.042581748217344284, 'rewards/accuracies': 0.25, 'rewards/margins': -0.04697398841381073, 'policy_logps/rejected': -295.1412353515625, 'policy_logps/chosen': -332.2171325683594, 'referece_logps/rejected': -295.56707763671875, 'referece_logps/chosen': -332.1732177734375, 'logits/rejected': -0.6110608577728271, 'logits/chosen': -0.5183873176574707, 'epoch': 0.01}
[2024-04-01 19:20:16,707] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 21/10740 [06:33<59:23:04, 19.94s/it]

  0%|          | 22/10740 [06:48<54:32:53, 18.32s/it]


  0%|          | 24/10740 [07:16<49:11:46, 16.53s/it]

  0%|          | 25/10740 [07:37<52:36:58, 17.68s/it]
{'loss': 0.6992, 'learning_rate': 1.5479876160990712e-07, 'rewards/chosen': -0.018660547211766243, 'rewards/rejected': 0.026804348453879356, 'rewards/accuracies': 0.0, 'rewards/margins': -0.045464903116226196, 'policy_logps/rejected': -324.7320251464844, 'policy_logps/chosen': -364.38897705078125, 'referece_logps/rejected': -325.0000915527344, 'referece_logps/chosen': -364.2023620605469, 'logits/rejected': -0.41278013586997986, 'logits/chosen': -0.4559080898761749, 'epoch': 0.01}

  0%|          | 26/10740 [07:51<49:38:03, 16.68s/it]


  0%|          | 28/10740 [08:24<51:03:00, 17.16s/it]
{'loss': 0.6938, 'learning_rate': 1.7337461300309596e-07, 'rewards/chosen': 0.012211416848003864, 'rewards/rejected': -0.005424880888313055, 'rewards/accuracies': 0.625, 'rewards/margins': 0.01763629913330078, 'policy_logps/rejected': -285.16650390625, 'policy_logps/chosen': -388.29681396484375, 'referece_logps/rejected': -285.11224365234375, 'referece_logps/chosen': -388.4189453125, 'logits/rejected': -1.1753617525100708, 'logits/chosen': -1.1847175359725952, 'epoch': 0.02}


  0%|          | 30/10740 [09:03<54:03:38, 18.17s/it]

  0%|          | 31/10740 [09:14<48:30:55, 16.31s/it]

  0%|          | 32/10740 [09:29<46:34:13, 15.66s/it]
{'loss': 0.688, 'learning_rate': 1.981424148606811e-07, 'rewards/chosen': -0.014566516503691673, 'rewards/rejected': 0.000202178955078125, 'rewards/accuracies': 0.5, 'rewards/margins': -0.014768697321414948, 'policy_logps/rejected': -287.0580749511719, 'policy_logps/chosen': -324.82330322265625, 'referece_logps/rejected': -287.0600891113281, 'referece_logps/chosen': -324.6776123046875, 'logits/rejected': -0.2253774106502533, 'logits/chosen': -0.27816933393478394, 'epoch': 0.02}

  0%|          | 33/10740 [09:47<48:52:32, 16.43s/it]


  0%|          | 35/10740 [10:20<49:10:55, 16.54s/it]
{'loss': 0.6993, 'learning_rate': 2.1671826625386997e-07, 'rewards/chosen': -0.006025504786521196, 'rewards/rejected': 0.02856168895959854, 'rewards/accuracies': 0.125, 'rewards/margins': -0.03458719328045845, 'policy_logps/rejected': -409.2535095214844, 'policy_logps/chosen': -385.2397155761719, 'referece_logps/rejected': -409.53912353515625, 'referece_logps/chosen': -385.179443359375, 'logits/rejected': -0.62862229347229, 'logits/chosen': -0.7435054779052734, 'epoch': 0.02}

  0%|          | 36/10740 [10:41<53:28:14, 17.98s/it]

  0%|          | 37/10740 [10:54<48:34:25, 16.34s/it]

  0%|          | 38/10740 [11:05<43:49:43, 14.74s/it]

  0%|          | 39/10740 [11:23<46:54:51, 15.78s/it]

  0%|          | 40/10740 [11:43<50:46:37, 17.08s/it]

  0%|          | 41/10740 [12:02<52:06:59, 17.54s/it]

  0%|          | 42/10740 [12:15<48:24:12, 16.29s/it]
[2024-04-01 19:26:10,831] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 43/10740 [12:27<44:26:48, 14.96s/it]

  0%|          | 44/10740 [12:38<40:35:03, 13.66s/it]

  0%|          | 45/10740 [12:53<42:02:40, 14.15s/it]

  0%|          | 46/10740 [13:17<50:53:06, 17.13s/it]


  0%|          | 48/10740 [13:55<53:56:25, 18.16s/it]
{'loss': 0.6929, 'learning_rate': 2.9721362229102163e-07, 'rewards/chosen': 0.01713571511209011, 'rewards/rejected': 0.012714480981230736, 'rewards/accuracies': 0.375, 'rewards/margins': 0.004421234130859375, 'policy_logps/rejected': -301.36468505859375, 'policy_logps/chosen': -388.41070556640625, 'referece_logps/rejected': -301.4918212890625, 'referece_logps/chosen': -388.5820617675781, 'logits/rejected': -1.0769743919372559, 'logits/chosen': -0.9705965518951416, 'epoch': 0.03}

  0%|          | 49/10740 [14:08<48:53:44, 16.46s/it]

  0%|          | 50/10740 [14:18<43:41:34, 14.71s/it]


  0%|          | 52/10740 [14:43<39:40:20, 13.36s/it]

  0%|          | 53/10740 [15:03<45:46:18, 15.42s/it]
[2024-04-01 19:28:46,844] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 54/10740 [15:21<48:01:31, 16.18s/it]
[2024-04-01 19:29:04,797] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6971, 'learning_rate': 3.3436532507739936e-07, 'rewards/chosen': -0.0067268372513353825, 'rewards/rejected': -6.160885095596313e-05, 'rewards/accuracies': 0.375, 'rewards/margins': -0.006665228866040707, 'policy_logps/rejected': -513.0840454101562, 'policy_logps/chosen': -439.9394836425781, 'referece_logps/rejected': -513.0834350585938, 'referece_logps/chosen': -439.8721923828125, 'logits/rejected': -0.40878957509994507, 'logits/chosen': -0.1655600219964981, 'epoch': 0.03}


  1%|          | 56/10740 [15:57<49:11:15, 16.57s/it]

  1%|          | 57/10740 [16:17<52:19:44, 17.63s/it]
[2024-04-01 19:30:00,516] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.685, 'learning_rate': 3.529411764705882e-07, 'rewards/chosen': 0.018810361623764038, 'rewards/rejected': -0.01363677904009819, 'rewards/accuracies': 0.5, 'rewards/margins': 0.03244714438915253, 'policy_logps/rejected': -524.7714233398438, 'policy_logps/chosen': -491.6153869628906, 'referece_logps/rejected': -524.6350708007812, 'referece_logps/chosen': -491.803466796875, 'logits/rejected': 0.9278380870819092, 'logits/chosen': 0.9556046724319458, 'epoch': 0.03}

  1%|          | 58/10740 [16:32<49:59:57, 16.85s/it]


  1%|          | 60/10740 [17:11<54:40:46, 18.43s/it]
{'loss': 0.6974, 'learning_rate': 3.715170278637771e-07, 'rewards/chosen': 0.012884712778031826, 'rewards/rejected': 0.018096257001161575, 'rewards/accuracies': 0.5, 'rewards/margins': -0.005211543291807175, 'policy_logps/rejected': -389.45001220703125, 'policy_logps/chosen': -260.20269775390625, 'referece_logps/rejected': -389.6309814453125, 'referece_logps/chosen': -260.3315734863281, 'logits/rejected': -0.22212359309196472, 'logits/chosen': -0.19492512941360474, 'epoch': 0.03}

  1%|          | 61/10740 [17:32<56:40:32, 19.11s/it]

  1%|          | 62/10740 [17:52<57:49:33, 19.50s/it]


  1%|          | 64/10740 [18:35<60:25:33, 20.38s/it]

  1%|          | 65/10740 [18:53<58:14:47, 19.64s/it]

  1%|          | 66/10740 [19:17<62:18:52, 21.02s/it]
[2024-04-01 19:33:00,540] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 67/10740 [19:39<63:14:26, 21.33s/it]
[2024-04-01 19:33:22,605] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6935, 'learning_rate': 4.148606811145511e-07, 'rewards/chosen': -0.02896737866103649, 'rewards/rejected': 0.017588473856449127, 'rewards/accuracies': 0.375, 'rewards/margins': -0.04655585438013077, 'policy_logps/rejected': -435.03558349609375, 'policy_logps/chosen': -495.3497314453125, 'referece_logps/rejected': -435.21142578125, 'referece_logps/chosen': -495.06005859375, 'logits/rejected': -0.02712448686361313, 'logits/chosen': 0.04931863397359848, 'epoch': 0.04}


  1%|          | 69/10740 [20:19<60:31:21, 20.42s/it]
{'loss': 0.6865, 'learning_rate': 4.2724458204334363e-07, 'rewards/chosen': 0.0036700256168842316, 'rewards/rejected': -0.006997489836066961, 'rewards/accuracies': 0.625, 'rewards/margins': 0.010667516849935055, 'policy_logps/rejected': -277.3849182128906, 'policy_logps/chosen': -295.9603271484375, 'referece_logps/rejected': -277.3149719238281, 'referece_logps/chosen': -295.9970397949219, 'logits/rejected': -0.4323120415210724, 'logits/chosen': -0.4206438660621643, 'epoch': 0.04}
[2024-04-01 19:34:24,001] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 70/10740 [20:40<61:21:51, 20.70s/it]
[2024-04-01 19:34:44,249] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  1%|          | 72/10740 [21:15<55:22:13, 18.69s/it]

  1%|          | 73/10740 [21:35<57:05:02, 19.27s/it]
[2024-04-01 19:35:19,162] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6969, 'learning_rate': 4.5201238390092875e-07, 'rewards/chosen': -0.01542358472943306, 'rewards/rejected': 0.010657120496034622, 'rewards/accuracies': 0.375, 'rewards/margins': -0.026080705225467682, 'policy_logps/rejected': -360.2402648925781, 'policy_logps/chosen': -373.9270935058594, 'referece_logps/rejected': -360.3468017578125, 'referece_logps/chosen': -373.7728576660156, 'logits/rejected': -0.1927100419998169, 'logits/chosen': -0.18899540603160858, 'epoch': 0.04}

  1%|          | 74/10740 [21:50<52:52:23, 17.85s/it]


  1%|          | 76/10740 [22:17<46:59:41, 15.86s/it]
{'loss': 0.6893, 'learning_rate': 4.705882352941176e-07, 'rewards/chosen': 0.0023616794496774673, 'rewards/rejected': -0.00556449918076396, 'rewards/accuracies': 0.625, 'rewards/margins': 0.007926177233457565, 'policy_logps/rejected': -428.8702392578125, 'policy_logps/chosen': -418.29595947265625, 'referece_logps/rejected': -428.8145751953125, 'referece_logps/chosen': -418.31951904296875, 'logits/rejected': -0.319198876619339, 'logits/chosen': -0.2939278483390808, 'epoch': 0.04}

  1%|          | 77/10740 [22:40<53:15:08, 17.98s/it]


  1%|          | 79/10740 [23:23<58:14:35, 19.67s/it]

  1%|          | 80/10740 [23:47<62:11:19, 21.00s/it]
{'loss': 0.694, 'learning_rate': 4.953560371517028e-07, 'rewards/chosen': -0.016981124877929688, 'rewards/rejected': -0.01959962770342827, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0026185037568211555, 'policy_logps/rejected': -377.22027587890625, 'policy_logps/chosen': -294.1824951171875, 'referece_logps/rejected': -377.0242919921875, 'referece_logps/chosen': -294.0126953125, 'logits/rejected': -0.7574684619903564, 'logits/chosen': -0.7639052867889404, 'epoch': 0.04}

  1%|          | 81/10740 [24:11<64:29:52, 21.78s/it]
[2024-04-01 19:38:16,135] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 82/10740 [24:32<64:21:21, 21.74s/it]

  1%|          | 83/10740 [24:52<62:38:16, 21.16s/it]


  1%|          | 85/10740 [25:22<53:42:30, 18.15s/it]
{'loss': 0.6999, 'learning_rate': 5.263157894736842e-07, 'rewards/chosen': -0.005069827660918236, 'rewards/rejected': 0.014831542037427425, 'rewards/accuracies': 0.25, 'rewards/margins': -0.019901372492313385, 'policy_logps/rejected': -282.6747741699219, 'policy_logps/chosen': -305.33441162109375, 'referece_logps/rejected': -282.8230895996094, 'referece_logps/chosen': -305.2837219238281, 'logits/rejected': -0.6097516417503357, 'logits/chosen': -0.5506839156150818, 'epoch': 0.05}

  1%|          | 86/10740 [25:38<52:02:10, 17.58s/it]

  1%|          | 87/10740 [25:53<49:38:22, 16.77s/it]

  1%|          | 88/10740 [26:14<53:30:22, 18.08s/it]

  1%|          | 89/10740 [26:29<50:52:25, 17.20s/it]

  1%|          | 90/10740 [26:50<54:26:07, 18.40s/it]


  1%|          | 92/10740 [27:28<53:53:32, 18.22s/it]
{'loss': 0.6858, 'learning_rate': 5.696594427244582e-07, 'rewards/chosen': -0.005350494757294655, 'rewards/rejected': -0.0023696902208030224, 'rewards/accuracies': 0.5, 'rewards/margins': -0.002980804070830345, 'policy_logps/rejected': -371.5052185058594, 'policy_logps/chosen': -469.49481201171875, 'referece_logps/rejected': -371.48150634765625, 'referece_logps/chosen': -469.4412841796875, 'logits/rejected': -0.7159429788589478, 'logits/chosen': -0.9707550406455994, 'epoch': 0.05}
[2024-04-01 19:41:33,073] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 93/10740 [27:49<56:42:50, 19.18s/it]

  1%|          | 94/10740 [28:05<53:39:43, 18.15s/it]

  1%|          | 95/10740 [28:27<56:43:25, 19.18s/it]

  1%|          | 96/10740 [28:50<59:58:31, 20.28s/it]

  1%|          | 97/10740 [29:06<56:44:27, 19.19s/it]


  1%|          | 99/10740 [29:34<47:52:45, 16.20s/it]
{'loss': 0.6909, 'learning_rate': 6.130030959752321e-07, 'rewards/chosen': 0.004072570241987705, 'rewards/rejected': 0.022812463343143463, 'rewards/accuracies': 0.375, 'rewards/margins': -0.018739890307188034, 'policy_logps/rejected': -489.0523681640625, 'policy_logps/chosen': -445.310546875, 'referece_logps/rejected': -489.2804870605469, 'referece_logps/chosen': -445.35125732421875, 'logits/rejected': -0.377626895904541, 'logits/chosen': -0.3072953224182129, 'epoch': 0.06}

  1%|          | 100/10740 [29:51<49:00:25, 16.58s/it]

  1%|          | 101/10740 [30:12<52:13:53, 17.67s/it]

  1%|          | 102/10740 [30:26<48:55:42, 16.56s/it]


  1%|          | 104/10740 [31:04<52:21:07, 17.72s/it]
{'loss': 0.6881, 'learning_rate': 6.439628482972136e-07, 'rewards/chosen': -0.033114053308963776, 'rewards/rejected': -0.00670089665800333, 'rewards/accuracies': 0.25, 'rewards/margins': -0.026413150131702423, 'policy_logps/rejected': -243.76681518554688, 'policy_logps/chosen': -292.58349609375, 'referece_logps/rejected': -243.69981384277344, 'referece_logps/chosen': -292.2523498535156, 'logits/rejected': -0.9090031981468201, 'logits/chosen': -1.0293643474578857, 'epoch': 0.06}

  1%|          | 105/10740 [31:21<51:53:36, 17.57s/it]

  1%|          | 106/10740 [31:38<51:04:18, 17.29s/it]

  1%|          | 107/10740 [31:57<53:04:19, 17.97s/it]

  1%|          | 108/10740 [32:14<51:35:18, 17.47s/it]

  1%|          | 109/10740 [32:33<53:37:12, 18.16s/it]

  1%|          | 110/10740 [32:55<56:15:24, 19.05s/it]


  1%|          | 112/10740 [33:32<55:04:52, 18.66s/it]
{'loss': 0.6931, 'learning_rate': 6.934984520123838e-07, 'rewards/chosen': 0.020664216950535774, 'rewards/rejected': -0.01981182210147381, 'rewards/accuracies': 0.625, 'rewards/margins': 0.04047603905200958, 'policy_logps/rejected': -437.13818359375, 'policy_logps/chosen': -513.3504638671875, 'referece_logps/rejected': -436.9400634765625, 'referece_logps/chosen': -513.55712890625, 'logits/rejected': 0.4402281641960144, 'logits/chosen': 0.5128605961799622, 'epoch': 0.06}

  1%|          | 113/10740 [33:48<52:09:40, 17.67s/it]

  1%|          | 114/10740 [34:05<52:23:26, 17.75s/it]

  1%|          | 115/10740 [34:21<50:36:51, 17.15s/it]

  1%|          | 116/10740 [34:36<48:29:56, 16.43s/it]

  1%|          | 117/10740 [34:57<52:47:39, 17.89s/it]

  1%|          | 118/10740 [35:10<47:54:11, 16.24s/it]

  1%|          | 119/10740 [35:28<49:35:12, 16.81s/it]

  1%|          | 120/10740 [35:41<46:17:53, 15.69s/it]

  1%|          | 121/10740 [36:00<49:04:43, 16.64s/it]

  1%|          | 122/10740 [36:18<50:07:45, 17.00s/it]

  1%|          | 123/10740 [36:42<56:29:58, 19.16s/it]

  1%|          | 124/10740 [36:57<53:15:21, 18.06s/it]

  1%|          | 125/10740 [37:09<47:36:20, 16.15s/it]

  1%|          | 126/10740 [37:25<47:32:07, 16.12s/it]

  1%|          | 127/10740 [37:45<51:19:37, 17.41s/it]

  1%|          | 128/10740 [38:08<56:04:45, 19.02s/it]

  1%|          | 129/10740 [38:29<57:51:00, 19.63s/it]

  1%|          | 130/10740 [38:49<57:48:46, 19.62s/it]

  1%|          | 131/10740 [39:10<59:35:27, 20.22s/it]


  1%|          | 133/10740 [39:51<59:27:07, 20.18s/it]

  1%|          | 134/10740 [40:09<57:43:50, 19.60s/it]

  1%|▏         | 135/10740 [40:29<57:55:49, 19.67s/it]
{'loss': 0.6832, 'learning_rate': 8.359133126934984e-07, 'rewards/chosen': 0.009958459064364433, 'rewards/rejected': -0.0023252475075423717, 'rewards/accuracies': 0.75, 'rewards/margins': 0.012283707037568092, 'policy_logps/rejected': -495.8485107421875, 'policy_logps/chosen': -521.6861572265625, 'referece_logps/rejected': -495.82525634765625, 'referece_logps/chosen': -521.7857055664062, 'logits/rejected': -0.027866125106811523, 'logits/chosen': 0.07412539422512054, 'epoch': 0.08}


  1%|▏         | 137/10740 [41:03<54:08:08, 18.38s/it]

  1%|▏         | 138/10740 [41:18<51:36:17, 17.52s/it]

  1%|▏         | 139/10740 [41:36<52:02:23, 17.67s/it]
{'loss': 0.6742, 'learning_rate': 8.606811145510835e-07, 'rewards/chosen': -0.016785716637969017, 'rewards/rejected': -0.05097847431898117, 'rewards/accuracies': 0.625, 'rewards/margins': 0.034192752093076706, 'policy_logps/rejected': -359.69244384765625, 'policy_logps/chosen': -389.0140075683594, 'referece_logps/rejected': -359.1826477050781, 'referece_logps/chosen': -388.84613037109375, 'logits/rejected': 0.1523006409406662, 'logits/chosen': 0.20906585454940796, 'epoch': 0.08}


  1%|▏         | 141/10740 [42:08<48:39:18, 16.53s/it]

  1%|▏         | 142/10740 [42:20<44:47:04, 15.21s/it]

  1%|▏         | 143/10740 [42:36<45:05:55, 15.32s/it]

  1%|▏         | 144/10740 [42:50<44:34:01, 15.14s/it]

  1%|▏         | 145/10740 [43:11<49:50:12, 16.93s/it]

  1%|▏         | 146/10740 [43:33<53:48:19, 18.28s/it]

  1%|▏         | 147/10740 [43:46<49:29:24, 16.82s/it]

  1%|▏         | 148/10740 [44:06<52:26:49, 17.83s/it]

  1%|▏         | 149/10740 [44:28<55:45:05, 18.95s/it]

  1%|▏         | 150/10740 [44:50<58:39:22, 19.94s/it]

  1%|▏         | 151/10740 [45:09<57:56:46, 19.70s/it]

  1%|▏         | 152/10740 [45:30<59:06:16, 20.10s/it]
{'loss': 0.6683, 'learning_rate': 9.411764705882352e-07, 'rewards/chosen': 0.06968002766370773, 'rewards/rejected': -0.009540366940200329, 'rewards/accuracies': 0.625, 'rewards/margins': 0.07922039926052094, 'policy_logps/rejected': -350.1507568359375, 'policy_logps/chosen': -319.5318908691406, 'referece_logps/rejected': -350.0553894042969, 'referece_logps/chosen': -320.22869873046875, 'logits/rejected': -0.48280447721481323, 'logits/chosen': -0.4516140818595886, 'epoch': 0.08}

  1%|▏         | 153/10740 [45:49<57:30:10, 19.55s/it]


  1%|▏         | 155/10740 [46:15<47:48:07, 16.26s/it]

  1%|▏         | 156/10740 [46:27<44:05:40, 15.00s/it]

  1%|▏         | 157/10740 [46:49<50:26:59, 17.16s/it]

  1%|▏         | 158/10740 [47:08<51:37:03, 17.56s/it]
{'loss': 0.6679, 'learning_rate': 9.783281733746129e-07, 'rewards/chosen': 0.04960136115550995, 'rewards/rejected': -0.017807770520448685, 'rewards/accuracies': 0.75, 'rewards/margins': 0.06740912795066833, 'policy_logps/rejected': -299.64984130859375, 'policy_logps/chosen': -287.0080871582031, 'referece_logps/rejected': -299.4718017578125, 'referece_logps/chosen': -287.50408935546875, 'logits/rejected': -1.3379180431365967, 'logits/chosen': -1.1832971572875977, 'epoch': 0.09}


  1%|▏         | 160/10740 [47:50<56:40:42, 19.29s/it]

  1%|▏         | 161/10740 [48:08<55:44:39, 18.97s/it]

  2%|▏         | 162/10740 [48:27<56:08:52, 19.11s/it]
{'loss': 0.6773, 'learning_rate': 1.003095975232198e-06, 'rewards/chosen': -0.02161865122616291, 'rewards/rejected': -0.04815292730927467, 'rewards/accuracies': 0.5, 'rewards/margins': 0.026534270495176315, 'policy_logps/rejected': -394.250732421875, 'policy_logps/chosen': -335.1806640625, 'referece_logps/rejected': -393.76922607421875, 'referece_logps/chosen': -334.9645080566406, 'logits/rejected': -0.4831145703792572, 'logits/chosen': -0.2856878936290741, 'epoch': 0.09}

  2%|▏         | 163/10740 [48:41<51:12:42, 17.43s/it]

  2%|▏         | 164/10740 [49:01<53:24:05, 18.18s/it]


  2%|▏         | 166/10740 [49:33<51:10:38, 17.42s/it]

  2%|▏         | 167/10740 [49:51<50:56:55, 17.35s/it]

  2%|▏         | 168/10740 [50:02<45:38:36, 15.54s/it]

  2%|▏         | 169/10740 [50:18<46:32:37, 15.85s/it]

  2%|▏         | 170/10740 [50:31<43:18:18, 14.75s/it]

  2%|▏         | 171/10740 [50:42<40:24:03, 13.76s/it]

  2%|▏         | 172/10740 [50:56<40:30:10, 13.80s/it]

  2%|▏         | 173/10740 [51:09<39:51:26, 13.58s/it]

  2%|▏         | 174/10740 [51:30<46:18:19, 15.78s/it]

  2%|▏         | 175/10740 [51:46<46:52:25, 15.97s/it]

  2%|▏         | 176/10740 [52:04<48:40:09, 16.59s/it]

  2%|▏         | 177/10740 [52:21<48:49:08, 16.64s/it]

  2%|▏         | 178/10740 [52:39<49:29:07, 16.87s/it]

  2%|▏         | 179/10740 [52:54<48:38:01, 16.58s/it]

  2%|▏         | 180/10740 [53:16<52:47:01, 17.99s/it]

  2%|▏         | 181/10740 [53:36<54:49:37, 18.69s/it]

  2%|▏         | 182/10740 [53:56<55:54:16, 19.06s/it]

  2%|▏         | 183/10740 [54:17<57:35:33, 19.64s/it]

  2%|▏         | 184/10740 [54:36<57:26:34, 19.59s/it]

  2%|▏         | 185/10740 [55:00<61:12:13, 20.87s/it]

  2%|▏         | 186/10740 [55:23<62:36:13, 21.35s/it]

  2%|▏         | 187/10740 [55:38<57:11:00, 19.51s/it]

  2%|▏         | 188/10740 [56:00<59:14:16, 20.21s/it]

  2%|▏         | 189/10740 [56:12<52:28:22, 17.90s/it]
[2024-04-01 20:09:56,153] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 190/10740 [56:24<47:00:53, 16.04s/it]

  2%|▏         | 191/10740 [56:35<42:19:50, 14.45s/it]

  2%|▏         | 192/10740 [56:46<39:20:39, 13.43s/it]

  2%|▏         | 193/10740 [56:58<37:55:34, 12.95s/it]

  2%|▏         | 194/10740 [57:19<44:51:47, 15.31s/it]

  2%|▏         | 195/10740 [57:37<47:35:07, 16.25s/it]

  2%|▏         | 196/10740 [57:56<50:21:26, 17.19s/it]

  2%|▏         | 197/10740 [58:10<47:05:07, 16.08s/it]

  2%|▏         | 198/10740 [58:21<42:25:03, 14.49s/it]

  2%|▏         | 199/10740 [58:37<43:53:58, 14.99s/it]
{'loss': 0.6561, 'learning_rate': 1.2321981424148607e-06, 'rewards/chosen': -0.05413703992962837, 'rewards/rejected': -0.023128701373934746, 'rewards/accuracies': 0.5, 'rewards/margins': -0.031008338555693626, 'policy_logps/rejected': -439.10418701171875, 'policy_logps/chosen': -488.07379150390625, 'referece_logps/rejected': -438.8728942871094, 'referece_logps/chosen': -487.5324401855469, 'logits/rejected': 0.07671299576759338, 'logits/chosen': 0.08897518366575241, 'epoch': 0.11}
[2024-04-01 20:12:35,793] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  2%|▏         | 201/10740 [59:03<40:26:14, 13.81s/it]
{'loss': 0.6614, 'learning_rate': 1.2445820433436532e-06, 'rewards/chosen': -0.07351359724998474, 'rewards/rejected': -0.01988220028579235, 'rewards/accuracies': 0.5, 'rewards/margins': -0.05363140255212784, 'policy_logps/rejected': -370.8779296875, 'policy_logps/chosen': -359.738525390625, 'referece_logps/rejected': -370.6790771484375, 'referece_logps/chosen': -359.0033874511719, 'logits/rejected': -0.09561577439308167, 'logits/chosen': -0.2870480716228485, 'epoch': 0.11}


  2%|▏         | 203/10740 [59:35<43:58:37, 15.02s/it]

  2%|▏         | 204/10740 [59:48<41:53:45, 14.32s/it]

  2%|▏         | 205/10740 [1:00:06<44:59:34, 15.37s/it]
[2024-04-01 20:13:49,294] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 206/10740 [1:00:27<50:04:27, 17.11s/it]

  2%|▏         | 207/10740 [1:00:39<46:12:28, 15.79s/it]
{'loss': 0.6649, 'learning_rate': 1.2817337461300307e-06, 'rewards/chosen': 0.049857139587402344, 'rewards/rejected': -0.004308508709073067, 'rewards/accuracies': 0.75, 'rewards/margins': 0.05416564643383026, 'policy_logps/rejected': -232.9816436767578, 'policy_logps/chosen': -298.220458984375, 'referece_logps/rejected': -232.93853759765625, 'referece_logps/chosen': -298.71905517578125, 'logits/rejected': -0.1435176134109497, 'logits/chosen': -0.19097059965133667, 'epoch': 0.12}


  2%|▏         | 209/10740 [1:01:15<49:47:51, 17.02s/it]

  2%|▏         | 210/10740 [1:01:30<47:44:38, 16.32s/it]

  2%|▏         | 211/10740 [1:01:48<49:24:28, 16.89s/it]

  2%|▏         | 212/10740 [1:02:05<49:17:14, 16.85s/it]

  2%|▏         | 213/10740 [1:02:19<47:35:17, 16.27s/it]

  2%|▏         | 214/10740 [1:02:41<52:04:39, 17.81s/it]

  2%|▏         | 215/10740 [1:03:01<54:28:48, 18.63s/it]

  2%|▏         | 216/10740 [1:03:12<47:26:38, 16.23s/it]

  2%|▏         | 217/10740 [1:03:26<45:02:14, 15.41s/it]
{'loss': 0.6636, 'learning_rate': 1.3436532507739937e-06, 'rewards/chosen': -0.03263254091143608, 'rewards/rejected': -0.08907890319824219, 'rewards/accuracies': 0.75, 'rewards/margins': 0.056446366012096405, 'policy_logps/rejected': -283.2266845703125, 'policy_logps/chosen': -247.77398681640625, 'referece_logps/rejected': -282.3359069824219, 'referece_logps/chosen': -247.44764709472656, 'logits/rejected': -0.9683284163475037, 'logits/chosen': -0.8051449656486511, 'epoch': 0.12}


  2%|▏         | 219/10740 [1:04:05<51:52:30, 17.75s/it]

  2%|▏         | 220/10740 [1:04:22<51:32:09, 17.64s/it]

  2%|▏         | 221/10740 [1:04:43<54:37:48, 18.70s/it]

  2%|▏         | 222/10740 [1:05:04<56:24:33, 19.31s/it]

  2%|▏         | 223/10740 [1:05:23<56:23:49, 19.30s/it]

  2%|▏         | 224/10740 [1:05:41<55:09:01, 18.88s/it]
[2024-04-01 20:19:24,812] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 225/10740 [1:05:57<52:18:08, 17.91s/it]

  2%|▏         | 226/10740 [1:06:08<46:13:36, 15.83s/it]

  2%|▏         | 227/10740 [1:06:23<45:33:51, 15.60s/it]

  2%|▏         | 228/10740 [1:06:34<41:48:49, 14.32s/it]

  2%|▏         | 229/10740 [1:06:56<48:49:31, 16.72s/it]
{'loss': 0.6715, 'learning_rate': 1.4179566563467492e-06, 'rewards/chosen': 0.016855239868164062, 'rewards/rejected': -0.12412485480308533, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1409800946712494, 'policy_logps/rejected': -326.7143249511719, 'policy_logps/chosen': -390.08056640625, 'referece_logps/rejected': -325.47308349609375, 'referece_logps/chosen': -390.2491149902344, 'logits/rejected': 0.3651190996170044, 'logits/chosen': 0.2545613646507263, 'epoch': 0.13}


  2%|▏         | 231/10740 [1:07:32<49:27:19, 16.94s/it]

  2%|▏         | 232/10740 [1:07:49<49:38:03, 17.00s/it]

  2%|▏         | 233/10740 [1:08:07<50:32:51, 17.32s/it]

  2%|▏         | 234/10740 [1:08:21<47:22:06, 16.23s/it]

  2%|▏         | 235/10740 [1:08:42<51:03:19, 17.50s/it]

  2%|▏         | 236/10740 [1:08:56<48:38:47, 16.67s/it]
{'loss': 0.659, 'learning_rate': 1.4613003095975231e-06, 'rewards/chosen': -0.16814708709716797, 'rewards/rejected': -0.3036459982395172, 'rewards/accuracies': 0.75, 'rewards/margins': 0.13549891114234924, 'policy_logps/rejected': -269.6441345214844, 'policy_logps/chosen': -306.21856689453125, 'referece_logps/rejected': -266.607666015625, 'referece_logps/chosen': -304.53704833984375, 'logits/rejected': -0.562150239944458, 'logits/chosen': -0.5885726809501648, 'epoch': 0.13}


  2%|▏         | 238/10740 [1:09:36<53:19:50, 18.28s/it]
{'loss': 0.6262, 'learning_rate': 1.4736842105263156e-06, 'rewards/chosen': -0.07208251953125, 'rewards/rejected': -0.09650403261184692, 'rewards/accuracies': 0.375, 'rewards/margins': 0.024421505630016327, 'policy_logps/rejected': -290.8232727050781, 'policy_logps/chosen': -327.6020202636719, 'referece_logps/rejected': -289.8582458496094, 'referece_logps/chosen': -326.8811950683594, 'logits/rejected': -1.5006577968597412, 'logits/chosen': -1.4447838068008423, 'epoch': 0.13}


  2%|▏         | 240/10740 [1:10:15<56:00:38, 19.20s/it]

  2%|▏         | 241/10740 [1:10:32<53:22:21, 18.30s/it]
{'loss': 0.626, 'learning_rate': 1.4922600619195044e-06, 'rewards/chosen': 0.16991682350635529, 'rewards/rejected': -0.03331923484802246, 'rewards/accuracies': 0.625, 'rewards/margins': 0.20323607325553894, 'policy_logps/rejected': -190.35061645507812, 'policy_logps/chosen': -199.826171875, 'referece_logps/rejected': -190.01742553710938, 'referece_logps/chosen': -201.52532958984375, 'logits/rejected': -1.7118358612060547, 'logits/chosen': -1.811428189277649, 'epoch': 0.13}
[2024-04-01 20:24:38,706] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 242/10740 [1:10:55<57:44:11, 19.80s/it]
[2024-04-01 20:24:58,457] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  2%|▏         | 244/10740 [1:11:34<57:23:28, 19.68s/it]

  2%|▏         | 245/10740 [1:11:50<53:45:11, 18.44s/it]

  2%|▏         | 246/10740 [1:12:10<55:32:00, 19.05s/it]
{'loss': 0.6428, 'learning_rate': 1.523219814241486e-06, 'rewards/chosen': -0.21604804694652557, 'rewards/rejected': -0.4007144868373871, 'rewards/accuracies': 0.375, 'rewards/margins': 0.1846664547920227, 'policy_logps/rejected': -531.6732788085938, 'policy_logps/chosen': -448.7044677734375, 'referece_logps/rejected': -527.6661987304688, 'referece_logps/chosen': -446.5439758300781, 'logits/rejected': -0.8198632597923279, 'logits/chosen': -0.8912757635116577, 'epoch': 0.14}


  2%|▏         | 248/10740 [1:12:48<55:26:49, 19.02s/it]
{'loss': 0.6898, 'learning_rate': 1.5356037151702786e-06, 'rewards/chosen': -0.09345169365406036, 'rewards/rejected': -0.1729714423418045, 'rewards/accuracies': 0.625, 'rewards/margins': 0.07951974868774414, 'policy_logps/rejected': -339.693603515625, 'policy_logps/chosen': -414.03668212890625, 'referece_logps/rejected': -337.9638977050781, 'referece_logps/chosen': -413.1021423339844, 'logits/rejected': -0.5947608351707458, 'logits/chosen': -0.6936767101287842, 'epoch': 0.14}


  2%|▏         | 250/10740 [1:13:31<58:51:46, 20.20s/it]
{'loss': 0.5869, 'learning_rate': 1.5479876160990713e-06, 'rewards/chosen': 0.024586006999015808, 'rewards/rejected': -0.27679529786109924, 'rewards/accuracies': 0.625, 'rewards/margins': 0.30138131976127625, 'policy_logps/rejected': -339.7298889160156, 'policy_logps/chosen': -286.99725341796875, 'referece_logps/rejected': -336.9619140625, 'referece_logps/chosen': -287.24310302734375, 'logits/rejected': -0.7032397985458374, 'logits/chosen': -0.6319207549095154, 'epoch': 0.14}

  2%|▏         | 251/10740 [1:13:47<55:31:09, 19.06s/it]
[2024-04-01 20:27:50,798] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  2%|▏         | 253/10740 [1:14:24<53:58:42, 18.53s/it]

  2%|▏         | 254/10740 [1:14:43<54:33:27, 18.73s/it]

  2%|▏         | 255/10740 [1:15:02<55:05:49, 18.92s/it]

  2%|▏         | 256/10740 [1:15:18<52:28:55, 18.02s/it]
{'loss': 0.6434, 'learning_rate': 1.5851393188854488e-06, 'rewards/chosen': -0.20508593320846558, 'rewards/rejected': -0.20501843094825745, 'rewards/accuracies': 0.375, 'rewards/margins': -6.752088665962219e-05, 'policy_logps/rejected': -381.2936706542969, 'policy_logps/chosen': -321.61663818359375, 'referece_logps/rejected': -379.24346923828125, 'referece_logps/chosen': -319.5657653808594, 'logits/rejected': 0.09079673886299133, 'logits/chosen': 0.0535711944103241, 'epoch': 0.14}

  2%|▏         | 257/10740 [1:15:37<53:22:37, 18.33s/it]


  2%|▏         | 259/10740 [1:16:16<55:06:21, 18.93s/it]

  2%|▏         | 260/10740 [1:16:34<54:18:30, 18.66s/it]

  2%|▏         | 261/10740 [1:16:56<56:48:01, 19.51s/it]
[2024-04-01 20:30:39,559] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 262/10740 [1:17:13<54:20:32, 18.67s/it]

  2%|▏         | 263/10740 [1:17:34<56:43:38, 19.49s/it]

  2%|▏         | 264/10740 [1:17:48<52:16:21, 17.96s/it]

  2%|▏         | 265/10740 [1:18:10<55:38:34, 19.12s/it]

  2%|▏         | 266/10740 [1:18:24<50:45:20, 17.45s/it]

  2%|▏         | 267/10740 [1:18:40<49:46:00, 17.11s/it]
{'loss': 0.6562, 'learning_rate': 1.653250773993808e-06, 'rewards/chosen': -0.2870555818080902, 'rewards/rejected': -0.2796746790409088, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00738091766834259, 'policy_logps/rejected': -483.0505065917969, 'policy_logps/chosen': -331.2682189941406, 'referece_logps/rejected': -480.2537841796875, 'referece_logps/chosen': -328.397705078125, 'logits/rejected': -0.2018170803785324, 'logits/chosen': -0.08100911974906921, 'epoch': 0.15}


  3%|▎         | 269/10740 [1:19:21<54:11:32, 18.63s/it]
{'loss': 0.6035, 'learning_rate': 1.6656346749226005e-06, 'rewards/chosen': -0.16793565452098846, 'rewards/rejected': -0.3908172845840454, 'rewards/accuracies': 0.75, 'rewards/margins': 0.22288161516189575, 'policy_logps/rejected': -344.8621826171875, 'policy_logps/chosen': -251.97683715820312, 'referece_logps/rejected': -340.9539489746094, 'referece_logps/chosen': -250.2974853515625, 'logits/rejected': -0.7627909779548645, 'logits/chosen': -0.825816810131073, 'epoch': 0.15}


  3%|▎         | 271/10740 [1:19:58<54:48:44, 18.85s/it]

  3%|▎         | 272/10740 [1:20:21<57:38:21, 19.82s/it]
{'loss': 0.6235, 'learning_rate': 1.6842105263157893e-06, 'rewards/chosen': -0.0707508996129036, 'rewards/rejected': -0.34498023986816406, 'rewards/accuracies': 0.75, 'rewards/margins': 0.27422937750816345, 'policy_logps/rejected': -367.1800842285156, 'policy_logps/chosen': -458.91571044921875, 'referece_logps/rejected': -363.73028564453125, 'referece_logps/chosen': -458.2082214355469, 'logits/rejected': -0.13682645559310913, 'logits/chosen': -0.30011406540870667, 'epoch': 0.15}
[2024-04-01 20:34:23,336] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  3%|▎         | 274/10740 [1:21:00<57:22:46, 19.74s/it]
{'loss': 0.5996, 'learning_rate': 1.696594427244582e-06, 'rewards/chosen': -0.1834663450717926, 'rewards/rejected': -0.4478429853916168, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2643766403198242, 'policy_logps/rejected': -478.99591064453125, 'policy_logps/chosen': -325.61077880859375, 'referece_logps/rejected': -474.5174560546875, 'referece_logps/chosen': -323.7760925292969, 'logits/rejected': -1.1217232942581177, 'logits/chosen': -1.1273754835128784, 'epoch': 0.15}

  3%|▎         | 275/10740 [1:21:21<58:40:32, 20.18s/it]


  3%|▎         | 277/10740 [1:21:58<54:58:27, 18.91s/it]
{'loss': 0.6271, 'learning_rate': 1.7151702786377708e-06, 'rewards/chosen': -0.20874977111816406, 'rewards/rejected': -0.325636088848114, 'rewards/accuracies': 0.625, 'rewards/margins': 0.11688634753227234, 'policy_logps/rejected': -346.1388244628906, 'policy_logps/chosen': -266.6579895019531, 'referece_logps/rejected': -342.8824768066406, 'referece_logps/chosen': -264.57049560546875, 'logits/rejected': -0.6107405424118042, 'logits/chosen': -0.5421619415283203, 'epoch': 0.15}

  3%|▎         | 278/10740 [1:22:15<53:10:52, 18.30s/it]


  3%|▎         | 280/10740 [1:22:48<50:38:55, 17.43s/it]

  3%|▎         | 281/10740 [1:23:09<53:15:05, 18.33s/it]

  3%|▎         | 282/10740 [1:23:30<56:07:08, 19.32s/it]

  3%|▎         | 283/10740 [1:23:42<49:23:27, 17.00s/it]
{'loss': 0.6282, 'learning_rate': 1.7523219814241485e-06, 'rewards/chosen': -0.09874678403139114, 'rewards/rejected': -0.5393946170806885, 'rewards/accuracies': 0.875, 'rewards/margins': 0.44064781069755554, 'policy_logps/rejected': -337.44439697265625, 'policy_logps/chosen': -220.70277404785156, 'referece_logps/rejected': -332.0504455566406, 'referece_logps/chosen': -219.7152862548828, 'logits/rejected': -1.4430898427963257, 'logits/chosen': -1.3910493850708008, 'epoch': 0.16}


  3%|▎         | 285/10740 [1:24:18<50:54:03, 17.53s/it]

  3%|▎         | 286/10740 [1:24:36<51:06:39, 17.60s/it]
[2024-04-01 20:38:20,000] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6157, 'learning_rate': 1.7708978328173375e-06, 'rewards/chosen': -0.4088749885559082, 'rewards/rejected': -0.6640821695327759, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2552071809768677, 'policy_logps/rejected': -355.4347839355469, 'policy_logps/chosen': -310.52691650390625, 'referece_logps/rejected': -348.7939453125, 'referece_logps/chosen': -306.43817138671875, 'logits/rejected': -0.767174482345581, 'logits/chosen': -0.7094069719314575, 'epoch': 0.16}

  3%|▎         | 287/10740 [1:24:57<53:54:27, 18.57s/it]

  3%|▎         | 288/10740 [1:25:13<51:49:11, 17.85s/it]


  3%|▎         | 290/10740 [1:25:51<53:58:43, 18.60s/it]
[2024-04-01 20:39:34,698] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 291/10740 [1:26:14<58:07:01, 20.02s/it]
{'loss': 0.6051, 'learning_rate': 1.8018575851393188e-06, 'rewards/chosen': -0.030160676687955856, 'rewards/rejected': -0.3477350175380707, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3175743818283081, 'policy_logps/rejected': -256.814453125, 'policy_logps/chosen': -257.33026123046875, 'referece_logps/rejected': -253.33709716796875, 'referece_logps/chosen': -257.0286560058594, 'logits/rejected': -0.31988877058029175, 'logits/chosen': -0.36425262689590454, 'epoch': 0.16}


  3%|▎         | 293/10740 [1:26:48<52:32:13, 18.10s/it]

  3%|▎         | 294/10740 [1:27:10<55:38:52, 19.18s/it]

  3%|▎         | 295/10740 [1:27:21<48:16:19, 16.64s/it]
{'loss': 0.6853, 'learning_rate': 1.826625386996904e-06, 'rewards/chosen': -0.5210645794868469, 'rewards/rejected': -0.1562356948852539, 'rewards/accuracies': 0.125, 'rewards/margins': -0.3648289144039154, 'policy_logps/rejected': -448.14581298828125, 'policy_logps/chosen': -396.8751220703125, 'referece_logps/rejected': -446.58349609375, 'referece_logps/chosen': -391.6645202636719, 'logits/rejected': -0.4585115909576416, 'logits/chosen': -0.46201568841934204, 'epoch': 0.16}


  3%|▎         | 297/10740 [1:27:53<48:28:26, 16.71s/it]
{'loss': 0.5879, 'learning_rate': 1.8390092879256965e-06, 'rewards/chosen': -0.03132476657629013, 'rewards/rejected': -0.2998773753643036, 'rewards/accuracies': 0.625, 'rewards/margins': 0.26855260133743286, 'policy_logps/rejected': -313.35699462890625, 'policy_logps/chosen': -355.7526550292969, 'referece_logps/rejected': -310.3582458496094, 'referece_logps/chosen': -355.43939208984375, 'logits/rejected': -1.7277600765228271, 'logits/chosen': -1.803009271621704, 'epoch': 0.17}

  3%|▎         | 298/10740 [1:28:06<44:51:26, 15.47s/it]

  3%|▎         | 299/10740 [1:28:26<49:02:48, 16.91s/it]

  3%|▎         | 300/10740 [1:28:43<49:28:29, 17.06s/it]


  3%|▎         | 302/10740 [1:29:25<55:03:36, 18.99s/it]
{'loss': 0.6219, 'learning_rate': 1.869969040247678e-06, 'rewards/chosen': -0.20911207795143127, 'rewards/rejected': -0.44540244340896606, 'rewards/accuracies': 0.625, 'rewards/margins': 0.23629038035869598, 'policy_logps/rejected': -366.38922119140625, 'policy_logps/chosen': -256.05572509765625, 'referece_logps/rejected': -361.9352111816406, 'referece_logps/chosen': -253.964599609375, 'logits/rejected': -0.22777870297431946, 'logits/chosen': -0.20328062772750854, 'epoch': 0.17}


  3%|▎         | 304/10740 [1:29:59<52:04:28, 17.96s/it]

  3%|▎         | 305/10740 [1:30:16<50:44:23, 17.50s/it]

  3%|▎         | 306/10740 [1:30:37<54:20:30, 18.75s/it]
{'loss': 0.6334, 'learning_rate': 1.894736842105263e-06, 'rewards/chosen': -0.2896014153957367, 'rewards/rejected': -0.52373206615448, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2341306358575821, 'policy_logps/rejected': -488.8284912109375, 'policy_logps/chosen': -347.4743957519531, 'referece_logps/rejected': -483.59112548828125, 'referece_logps/chosen': -344.5783996582031, 'logits/rejected': -0.7436354756355286, 'logits/chosen': -1.0256950855255127, 'epoch': 0.17}
[2024-04-01 20:44:42,341] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  3%|▎         | 308/10740 [1:31:21<59:22:52, 20.49s/it]
[2024-04-01 20:45:05,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6403, 'learning_rate': 1.9071207430340557e-06, 'rewards/chosen': -0.4695731997489929, 'rewards/rejected': -0.6306343078613281, 'rewards/accuracies': 0.625, 'rewards/margins': 0.1610611081123352, 'policy_logps/rejected': -438.11126708984375, 'policy_logps/chosen': -331.888916015625, 'referece_logps/rejected': -431.8049011230469, 'referece_logps/chosen': -327.19317626953125, 'logits/rejected': -0.8106379508972168, 'logits/chosen': -0.7008465528488159, 'epoch': 0.17}
[2024-04-01 20:45:20,128] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 309/10740 [1:31:36<54:41:16, 18.87s/it]

  3%|▎         | 310/10740 [1:31:58<57:21:25, 19.80s/it]


  3%|▎         | 312/10740 [1:32:32<52:12:10, 18.02s/it]
{'loss': 0.5795, 'learning_rate': 1.9318885448916407e-06, 'rewards/chosen': 0.002566147595643997, 'rewards/rejected': -0.5424282550811768, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5449943542480469, 'policy_logps/rejected': -375.0707702636719, 'policy_logps/chosen': -346.9737548828125, 'referece_logps/rejected': -369.646484375, 'referece_logps/chosen': -346.9994201660156, 'logits/rejected': -0.4038439691066742, 'logits/chosen': -0.35472527146339417, 'epoch': 0.17}


  3%|▎         | 314/10740 [1:33:03<49:04:55, 16.95s/it]

  3%|▎         | 315/10740 [1:33:20<48:39:57, 16.81s/it]

  3%|▎         | 316/10740 [1:33:41<52:46:07, 18.22s/it]
[2024-04-01 20:47:24,808] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6344, 'learning_rate': 1.9566563467492257e-06, 'rewards/chosen': -0.49444419145584106, 'rewards/rejected': -0.5447480082511902, 'rewards/accuracies': 0.75, 'rewards/margins': 0.05030381679534912, 'policy_logps/rejected': -249.0012664794922, 'policy_logps/chosen': -265.93829345703125, 'referece_logps/rejected': -243.55380249023438, 'referece_logps/chosen': -260.9938659667969, 'logits/rejected': -0.3458608388900757, 'logits/chosen': -0.48713165521621704, 'epoch': 0.18}


  3%|▎         | 318/10740 [1:34:14<49:32:52, 17.12s/it]

  3%|▎         | 319/10740 [1:34:34<52:06:11, 18.00s/it]

  3%|▎         | 320/10740 [1:34:50<50:16:03, 17.37s/it]

  3%|▎         | 321/10740 [1:35:11<54:12:47, 18.73s/it]
{'loss': 0.5848, 'learning_rate': 1.9876160990712074e-06, 'rewards/chosen': -0.7398462295532227, 'rewards/rejected': -1.0560885667800903, 'rewards/accuracies': 0.375, 'rewards/margins': 0.3162423074245453, 'policy_logps/rejected': -314.45367431640625, 'policy_logps/chosen': -313.23333740234375, 'referece_logps/rejected': -303.89276123046875, 'referece_logps/chosen': -305.8348693847656, 'logits/rejected': -0.6617113351821899, 'logits/chosen': -0.6956802010536194, 'epoch': 0.18}

  3%|▎         | 322/10740 [1:35:27<51:02:42, 17.64s/it]


  3%|▎         | 324/10740 [1:36:03<51:45:16, 17.89s/it]
{'loss': 0.5527, 'learning_rate': 1.9999999545237734e-06, 'rewards/chosen': -0.20168356597423553, 'rewards/rejected': -0.5781574249267578, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3764738440513611, 'policy_logps/rejected': -277.3412780761719, 'policy_logps/chosen': -281.85333251953125, 'referece_logps/rejected': -271.5596923828125, 'referece_logps/chosen': -279.83648681640625, 'logits/rejected': -0.8088057041168213, 'logits/chosen': -0.881366491317749, 'epoch': 0.18}

  3%|▎         | 325/10740 [1:36:20<50:55:34, 17.60s/it]


  3%|▎         | 327/10740 [1:36:51<47:39:34, 16.48s/it]

  3%|▎         | 328/10740 [1:37:10<49:28:34, 17.11s/it]

  3%|▎         | 329/10740 [1:37:29<51:41:18, 17.87s/it]
{'loss': 0.6209, 'learning_rate': 1.9999983628562907e-06, 'rewards/chosen': -0.03818178176879883, 'rewards/rejected': -0.35798096656799316, 'rewards/accuracies': 0.625, 'rewards/margins': 0.31979915499687195, 'policy_logps/rejected': -447.9229736328125, 'policy_logps/chosen': -304.031005859375, 'referece_logps/rejected': -444.34320068359375, 'referece_logps/chosen': -303.6492004394531, 'logits/rejected': -0.6649552583694458, 'logits/chosen': -0.6858587265014648, 'epoch': 0.18}

  3%|▎         | 330/10740 [1:37:43<47:46:21, 16.52s/it]
[2024-04-01 20:51:47,016] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 331/10740 [1:38:03<51:10:31, 17.70s/it]

  3%|▎         | 332/10740 [1:38:26<55:32:41, 19.21s/it]

  3%|▎         | 333/10740 [1:38:40<51:02:30, 17.66s/it]


  3%|▎         | 335/10740 [1:39:12<49:23:05, 17.09s/it]
{'loss': 0.5505, 'learning_rate': 1.999993451430523e-06, 'rewards/chosen': -0.8278727531433105, 'rewards/rejected': -1.1815756559371948, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3537028431892395, 'policy_logps/rejected': -452.4778137207031, 'policy_logps/chosen': -354.92877197265625, 'referece_logps/rejected': -440.66204833984375, 'referece_logps/chosen': -346.6500244140625, 'logits/rejected': -1.4493627548217773, 'logits/chosen': -1.3682332038879395, 'epoch': 0.19}


  3%|▎         | 337/10740 [1:39:38<43:38:07, 15.10s/it]
{'loss': 0.5528, 'learning_rate': 1.9999910866728364e-06, 'rewards/chosen': -0.97261643409729, 'rewards/rejected': -1.346287488937378, 'rewards/accuracies': 0.875, 'rewards/margins': 0.3736709654331207, 'policy_logps/rejected': -352.325927734375, 'policy_logps/chosen': -329.0877380371094, 'referece_logps/rejected': -338.8630676269531, 'referece_logps/chosen': -319.361572265625, 'logits/rejected': -0.5505890846252441, 'logits/chosen': -0.5528086423873901, 'epoch': 0.19}


  3%|▎         | 339/10740 [1:40:18<51:45:43, 17.92s/it]
[2024-04-01 20:54:02,049] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5504, 'learning_rate': 1.9999883581085907e-06, 'rewards/chosen': -0.26925960183143616, 'rewards/rejected': -1.2491743564605713, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9799148440361023, 'policy_logps/rejected': -373.1499938964844, 'policy_logps/chosen': -258.90338134765625, 'referece_logps/rejected': -360.6582946777344, 'referece_logps/chosen': -256.2107849121094, 'logits/rejected': 0.09644117951393127, 'logits/chosen': -0.07416636496782303, 'epoch': 0.19}


  3%|▎         | 341/10740 [1:40:58<54:11:33, 18.76s/it]
{'loss': 0.544, 'learning_rate': 1.9999852657387793e-06, 'rewards/chosen': -0.3745199143886566, 'rewards/rejected': -0.8341654539108276, 'rewards/accuracies': 0.875, 'rewards/margins': 0.459645539522171, 'policy_logps/rejected': -397.1711730957031, 'policy_logps/chosen': -389.396240234375, 'referece_logps/rejected': -388.8294677734375, 'referece_logps/chosen': -385.65106201171875, 'logits/rejected': -0.7720534205436707, 'logits/chosen': -0.9041877388954163, 'epoch': 0.19}

  3%|▎         | 342/10740 [1:41:15<52:37:51, 18.22s/it]

  3%|▎         | 343/10740 [1:41:33<52:47:41, 18.28s/it]


  3%|▎         | 345/10740 [1:42:02<46:07:08, 15.97s/it]
{'loss': 0.6562, 'learning_rate': 1.999977989587091e-06, 'rewards/chosen': -0.47753870487213135, 'rewards/rejected': -0.6922623515129089, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2147235870361328, 'policy_logps/rejected': -423.1296081542969, 'policy_logps/chosen': -511.542724609375, 'referece_logps/rejected': -416.20703125, 'referece_logps/chosen': -506.7672119140625, 'logits/rejected': -0.3913189470767975, 'logits/chosen': -0.4431295394897461, 'epoch': 0.19}


  3%|▎         | 347/10740 [1:42:38<50:28:23, 17.48s/it]
{'loss': 0.6372, 'learning_rate': 1.999973805807861e-06, 'rewards/chosen': -0.6394199132919312, 'rewards/rejected': -1.094319224357605, 'rewards/accuracies': 0.875, 'rewards/margins': 0.45489925146102905, 'policy_logps/rejected': -517.59228515625, 'policy_logps/chosen': -627.6678466796875, 'referece_logps/rejected': -506.64910888671875, 'referece_logps/chosen': -621.2736206054688, 'logits/rejected': -1.0303049087524414, 'logits/chosen': -0.9875219464302063, 'epoch': 0.19}


  3%|▎         | 349/10740 [1:43:20<55:51:17, 19.35s/it]

  3%|▎         | 350/10740 [1:43:38<54:30:25, 18.89s/it]

  3%|▎         | 351/10740 [1:43:51<49:03:31, 17.00s/it]
{'loss': 0.4761, 'learning_rate': 1.9999643468502406e-06, 'rewards/chosen': -0.5750881433486938, 'rewards/rejected': -0.9168635010719299, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3417752981185913, 'policy_logps/rejected': -454.2731018066406, 'policy_logps/chosen': -378.156982421875, 'referece_logps/rejected': -445.10443115234375, 'referece_logps/chosen': -372.4060974121094, 'logits/rejected': -0.6396780014038086, 'logits/chosen': -0.5965338945388794, 'epoch': 0.2}


  3%|▎         | 353/10740 [1:44:30<52:07:19, 18.06s/it]

  3%|▎         | 354/10740 [1:44:50<54:19:10, 18.83s/it]
{'loss': 0.5826, 'learning_rate': 1.9999562976646e-06, 'rewards/chosen': -0.3508984446525574, 'rewards/rejected': -0.7825462818145752, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4316478371620178, 'policy_logps/rejected': -358.81396484375, 'policy_logps/chosen': -291.6714782714844, 'referece_logps/rejected': -350.9884338378906, 'referece_logps/chosen': -288.1625061035156, 'logits/rejected': 0.34974390268325806, 'logits/chosen': 0.19941754639148712, 'epoch': 0.2}


  3%|▎         | 356/10740 [1:45:28<55:09:42, 19.12s/it]

  3%|▎         | 357/10740 [1:45:48<55:26:46, 19.22s/it]

  3%|▎         | 358/10740 [1:46:03<51:33:41, 17.88s/it]
{'loss': 0.6198, 'learning_rate': 1.9999442921397052e-06, 'rewards/chosen': -0.5200546383857727, 'rewards/rejected': -1.1858173608779907, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6657627820968628, 'policy_logps/rejected': -416.654541015625, 'policy_logps/chosen': -350.99127197265625, 'referece_logps/rejected': -404.79638671875, 'referece_logps/chosen': -345.79071044921875, 'logits/rejected': -1.0443609952926636, 'logits/chosen': -0.9940663576126099, 'epoch': 0.2}

  3%|▎         | 359/10740 [1:46:28<57:42:19, 20.01s/it]


  3%|▎         | 361/10740 [1:47:11<59:40:02, 20.70s/it]
{'loss': 0.5701, 'learning_rate': 1.9999343330475584e-06, 'rewards/chosen': -0.2569907307624817, 'rewards/rejected': -1.2071948051452637, 'rewards/accuracies': 0.75, 'rewards/margins': 0.950204074382782, 'policy_logps/rejected': -370.45037841796875, 'policy_logps/chosen': -459.61676025390625, 'referece_logps/rejected': -358.3784484863281, 'referece_logps/chosen': -457.0468444824219, 'logits/rejected': -0.6494467258453369, 'logits/chosen': -0.7669313549995422, 'epoch': 0.2}


  3%|▎         | 363/10740 [1:47:49<56:54:12, 19.74s/it]

  3%|▎         | 364/10740 [1:48:13<60:27:09, 20.97s/it]
[2024-04-01 21:01:56,504] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6164, 'learning_rate': 1.9999235554371424e-06, 'rewards/chosen': -1.2478679418563843, 'rewards/rejected': -1.7040820121765137, 'rewards/accuracies': 0.875, 'rewards/margins': 0.4562140703201294, 'policy_logps/rejected': -540.096923828125, 'policy_logps/chosen': -482.7372131347656, 'referece_logps/rejected': -523.0560913085938, 'referece_logps/chosen': -470.258544921875, 'logits/rejected': -0.4483640789985657, 'logits/chosen': -0.5090214014053345, 'epoch': 0.2}
[2024-04-01 21:02:14,938] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 365/10740 [1:48:31<58:15:01, 20.21s/it]

  3%|▎         | 366/10740 [1:48:46<53:28:38, 18.56s/it]

  3%|▎         | 367/10740 [1:48:59<49:01:19, 17.01s/it]

  3%|▎         | 368/10740 [1:49:19<51:01:42, 17.71s/it]

  3%|▎         | 369/10740 [1:49:35<49:47:43, 17.29s/it]

  3%|▎         | 370/10740 [1:49:53<50:30:29, 17.53s/it]

  3%|▎         | 371/10740 [1:50:08<48:17:14, 16.76s/it]

  3%|▎         | 372/10740 [1:50:28<50:46:57, 17.63s/it]

  3%|▎         | 373/10740 [1:50:47<52:38:07, 18.28s/it]

  3%|▎         | 374/10740 [1:51:07<53:58:08, 18.74s/it]


  4%|▎         | 376/10740 [1:51:34<45:49:30, 15.92s/it]
{'loss': 0.5712, 'learning_rate': 1.9998722599992837e-06, 'rewards/chosen': -0.4103418290615082, 'rewards/rejected': -0.8699478507041931, 'rewards/accuracies': 0.875, 'rewards/margins': 0.45960596203804016, 'policy_logps/rejected': -285.15899658203125, 'policy_logps/chosen': -264.5282287597656, 'referece_logps/rejected': -276.45953369140625, 'referece_logps/chosen': -260.42486572265625, 'logits/rejected': -0.902509331703186, 'logits/chosen': -0.9277213215827942, 'epoch': 0.21}

  4%|▎         | 377/10740 [1:51:51<46:43:10, 16.23s/it]
[2024-04-01 21:05:57,654] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▎         | 378/10740 [1:52:14<52:21:58, 18.19s/it]

  4%|▎         | 379/10740 [1:52:36<55:58:25, 19.45s/it]
{'loss': 0.6663, 'learning_rate': 1.9998573899432568e-06, 'rewards/chosen': -0.4267718195915222, 'rewards/rejected': -0.6290187835693359, 'rewards/accuracies': 0.75, 'rewards/margins': 0.20224691927433014, 'policy_logps/rejected': -308.84613037109375, 'policy_logps/chosen': -284.049072265625, 'referece_logps/rejected': -302.55596923828125, 'referece_logps/chosen': -279.7813720703125, 'logits/rejected': -0.2593649923801422, 'logits/chosen': -0.3902340531349182, 'epoch': 0.21}

  4%|▎         | 380/10740 [1:52:55<55:22:27, 19.24s/it]

  4%|▎         | 381/10740 [1:53:08<49:31:23, 17.21s/it]

  4%|▎         | 382/10740 [1:53:20<45:35:21, 15.84s/it]


  4%|▎         | 384/10740 [1:53:58<50:26:03, 17.53s/it]

  4%|▎         | 385/10740 [1:54:21<54:33:04, 18.97s/it]
{'loss': 0.5611, 'learning_rate': 1.999825194478188e-06, 'rewards/chosen': -0.511149525642395, 'rewards/rejected': -1.5264983177185059, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0153487920761108, 'policy_logps/rejected': -358.5266418457031, 'policy_logps/chosen': -229.03286743164062, 'referece_logps/rejected': -343.2616882324219, 'referece_logps/chosen': -223.92138671875, 'logits/rejected': -0.5762598514556885, 'logits/chosen': -0.5967965126037598, 'epoch': 0.22}

  4%|▎         | 386/10740 [1:54:40<54:44:53, 19.04s/it]


  4%|▎         | 388/10740 [1:55:17<54:50:29, 19.07s/it]
{'loss': 0.5936, 'learning_rate': 1.9998078690955004e-06, 'rewards/chosen': -0.5453382730484009, 'rewards/rejected': -0.8955498933792114, 'rewards/accuracies': 0.625, 'rewards/margins': 0.35021156072616577, 'policy_logps/rejected': -462.2449951171875, 'policy_logps/chosen': -414.84088134765625, 'referece_logps/rejected': -453.2895202636719, 'referece_logps/chosen': -409.38751220703125, 'logits/rejected': -1.194291114807129, 'logits/chosen': -1.2650113105773926, 'epoch': 0.22}

  4%|▎         | 389/10740 [1:55:38<56:34:30, 19.68s/it]
[2024-04-01 21:09:39,716] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▎         | 390/10740 [1:55:56<55:17:31, 19.23s/it]


  4%|▎         | 392/10740 [1:56:39<58:14:53, 20.26s/it]
{'loss': 0.5825, 'learning_rate': 1.9997834954981886e-06, 'rewards/chosen': -0.3233330249786377, 'rewards/rejected': -1.0658475160598755, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7425145506858826, 'policy_logps/rejected': -376.6304016113281, 'policy_logps/chosen': -347.4854431152344, 'referece_logps/rejected': -365.9719543457031, 'referece_logps/chosen': -344.2521057128906, 'logits/rejected': -0.9382312297821045, 'logits/chosen': -1.0275126695632935, 'epoch': 0.22}

  4%|▎         | 393/10740 [1:56:56<55:38:03, 19.36s/it]


  4%|▎         | 395/10740 [1:57:29<52:20:12, 18.21s/it]

  4%|▎         | 396/10740 [1:57:49<53:19:03, 18.56s/it]
{'loss': 0.5674, 'learning_rate': 1.999757666976869e-06, 'rewards/chosen': -0.49615928530693054, 'rewards/rejected': -1.2259913682937622, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7298319935798645, 'policy_logps/rejected': -275.1953430175781, 'policy_logps/chosen': -292.31976318359375, 'referece_logps/rejected': -262.9354248046875, 'referece_logps/chosen': -287.358154296875, 'logits/rejected': -1.5876563787460327, 'logits/chosen': -1.5543490648269653, 'epoch': 0.22}

  4%|▎         | 397/10740 [1:58:04<50:32:31, 17.59s/it]

  4%|▎         | 398/10740 [1:58:24<52:27:20, 18.26s/it]

  4%|▎         | 399/10740 [1:58:42<52:18:49, 18.21s/it]


  4%|▎         | 401/10740 [1:59:27<58:33:35, 20.39s/it]
[2024-04-01 21:13:11,115] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5013, 'learning_rate': 1.9997233353957966e-06, 'rewards/chosen': -0.6513799428939819, 'rewards/rejected': -1.9009437561035156, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2495636940002441, 'policy_logps/rejected': -369.9145812988281, 'policy_logps/chosen': -291.0882873535156, 'referece_logps/rejected': -350.9051513671875, 'referece_logps/chosen': -284.5744934082031, 'logits/rejected': -0.23176340758800507, 'logits/chosen': -0.20436741411685944, 'epoch': 0.22}
[2024-04-01 21:13:30,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▎         | 402/10740 [1:59:46<57:21:14, 19.97s/it]

  4%|▍         | 403/10740 [1:59:58<50:31:27, 17.60s/it]


  4%|▍         | 405/10740 [2:00:40<55:15:46, 19.25s/it]
{'loss': 0.5565, 'learning_rate': 1.9996942334361125e-06, 'rewards/chosen': -0.13057394325733185, 'rewards/rejected': -0.616080105304718, 'rewards/accuracies': 0.875, 'rewards/margins': 0.48550617694854736, 'policy_logps/rejected': -305.935791015625, 'policy_logps/chosen': -283.7103271484375, 'referece_logps/rejected': -299.77496337890625, 'referece_logps/chosen': -282.40460205078125, 'logits/rejected': -0.3059821128845215, 'logits/chosen': -0.2991662919521332, 'epoch': 0.23}

  4%|▍         | 406/10740 [2:00:52<49:14:23, 17.15s/it]

  4%|▍         | 407/10740 [2:01:14<53:23:50, 18.60s/it]


  4%|▍         | 409/10740 [2:01:47<50:33:55, 17.62s/it]
{'loss': 0.507, 'learning_rate': 1.999663676682318e-06, 'rewards/chosen': -0.6526108980178833, 'rewards/rejected': -0.9109237790107727, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2583129107952118, 'policy_logps/rejected': -407.1558532714844, 'policy_logps/chosen': -364.6406555175781, 'referece_logps/rejected': -398.0466003417969, 'referece_logps/chosen': -358.114501953125, 'logits/rejected': -0.006869867444038391, 'logits/chosen': -0.024691253900527954, 'epoch': 0.23}
[2024-04-01 21:15:54,530] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 410/10740 [2:02:11<55:26:47, 19.32s/it]
[2024-04-01 21:16:15,708] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 411/10740 [2:02:32<57:02:17, 19.88s/it]

  4%|▍         | 412/10740 [2:02:55<59:47:55, 20.84s/it]

  4%|▍         | 413/10740 [2:03:17<60:35:40, 21.12s/it]

  4%|▍         | 414/10740 [2:03:37<59:46:13, 20.84s/it]

  4%|▍         | 415/10740 [2:03:55<57:37:04, 20.09s/it]
[2024-04-01 21:18:00,351] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 417/10740 [2:04:34<55:44:03, 19.44s/it]
[2024-04-01 21:18:17,446] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 418/10740 [2:04:48<51:14:50, 17.87s/it]
{'loss': 0.5764, 'learning_rate': 1.9995896051290702e-06, 'rewards/chosen': -1.3050082921981812, 'rewards/rejected': -1.3754249811172485, 'rewards/accuracies': 0.5, 'rewards/margins': 0.07041662931442261, 'policy_logps/rejected': -386.5504455566406, 'policy_logps/chosen': -363.9514465332031, 'referece_logps/rejected': -372.7961730957031, 'referece_logps/chosen': -350.9013671875, 'logits/rejected': -0.5961962342262268, 'logits/chosen': -0.5025146007537842, 'epoch': 0.23}

  4%|▍         | 419/10740 [2:05:01<46:42:34, 16.29s/it]

  4%|▍         | 420/10740 [2:05:14<44:37:59, 15.57s/it]


  4%|▍         | 422/10740 [2:05:50<47:35:15, 16.60s/it]
{'loss': 0.5877, 'learning_rate': 1.9995543206124876e-06, 'rewards/chosen': -0.7668776512145996, 'rewards/rejected': -1.1740546226501465, 'rewards/accuracies': 0.625, 'rewards/margins': 0.40717703104019165, 'policy_logps/rejected': -256.947265625, 'policy_logps/chosen': -321.97918701171875, 'referece_logps/rejected': -245.2067413330078, 'referece_logps/chosen': -314.31036376953125, 'logits/rejected': -0.44628503918647766, 'logits/chosen': -0.46501395106315613, 'epoch': 0.24}
[2024-04-01 21:19:54,202] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 423/10740 [2:06:10<51:04:34, 17.82s/it]
[2024-04-01 21:20:15,139] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 424/10740 [2:06:31<53:44:52, 18.76s/it]

  4%|▍         | 425/10740 [2:06:48<52:06:42, 18.19s/it]
[2024-04-01 21:20:52,187] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 426/10740 [2:07:08<53:49:36, 18.79s/it]


  4%|▍         | 428/10740 [2:07:42<52:01:03, 18.16s/it]

  4%|▍         | 429/10740 [2:08:04<54:56:11, 19.18s/it]
{'loss': 0.5639, 'learning_rate': 1.999489072632151e-06, 'rewards/chosen': -0.7452355027198792, 'rewards/rejected': -1.3649739027023315, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6197383999824524, 'policy_logps/rejected': -560.9981689453125, 'policy_logps/chosen': -416.5904846191406, 'referece_logps/rejected': -547.3484497070312, 'referece_logps/chosen': -409.13812255859375, 'logits/rejected': -1.0513007640838623, 'logits/chosen': -1.0682790279388428, 'epoch': 0.24}
[2024-04-01 21:22:08,783] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 431/10740 [2:08:46<57:40:20, 20.14s/it]
[2024-04-01 21:22:29,714] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6763, 'learning_rate': 1.9994696121852986e-06, 'rewards/chosen': -0.8125503659248352, 'rewards/rejected': -1.917301893234253, 'rewards/accuracies': 0.625, 'rewards/margins': 1.104751467704773, 'policy_logps/rejected': -545.3248901367188, 'policy_logps/chosen': -517.2208251953125, 'referece_logps/rejected': -526.15185546875, 'referece_logps/chosen': -509.09527587890625, 'logits/rejected': -0.3988852798938751, 'logits/chosen': -0.409894198179245, 'epoch': 0.24}

  4%|▍         | 432/10740 [2:09:07<58:24:42, 20.40s/it]

  4%|▍         | 433/10740 [2:09:18<50:11:08, 17.53s/it]

  4%|▍         | 434/10740 [2:09:31<46:35:42, 16.28s/it]

  4%|▍         | 435/10740 [2:09:43<43:12:06, 15.09s/it]

  4%|▍         | 436/10740 [2:10:04<47:33:43, 16.62s/it]

  4%|▍         | 437/10740 [2:10:27<53:10:53, 18.58s/it]

  4%|▍         | 438/10740 [2:10:45<52:43:01, 18.42s/it]


  4%|▍         | 440/10740 [2:11:20<51:49:11, 18.11s/it]

  4%|▍         | 441/10740 [2:11:40<53:22:16, 18.66s/it]
{'loss': 0.6242, 'learning_rate': 1.999366855844649e-06, 'rewards/chosen': -0.6442546844482422, 'rewards/rejected': -0.84103924036026, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1967845857143402, 'policy_logps/rejected': -392.3916931152344, 'policy_logps/chosen': -448.66448974609375, 'referece_logps/rejected': -383.9813232421875, 'referece_logps/chosen': -442.2218933105469, 'logits/rejected': -0.3029574751853943, 'logits/chosen': -0.5321035981178284, 'epoch': 0.25}


  4%|▍         | 443/10740 [2:12:18<53:09:43, 18.59s/it]
{'loss': 0.5844, 'learning_rate': 1.9993452138075786e-06, 'rewards/chosen': -0.7661440372467041, 'rewards/rejected': -1.1191858053207397, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3530418276786804, 'policy_logps/rejected': -463.00665283203125, 'policy_logps/chosen': -504.3957824707031, 'referece_logps/rejected': -451.8148193359375, 'referece_logps/chosen': -496.734375, 'logits/rejected': -0.33103224635124207, 'logits/chosen': -0.31072062253952026, 'epoch': 0.25}


  4%|▍         | 445/10740 [2:12:48<46:53:38, 16.40s/it]
{'loss': 0.6197, 'learning_rate': 1.9993232081989244e-06, 'rewards/chosen': -0.8781745433807373, 'rewards/rejected': -0.8416218161582947, 'rewards/accuracies': 0.375, 'rewards/margins': -0.03655282407999039, 'policy_logps/rejected': -391.22235107421875, 'policy_logps/chosen': -331.31378173828125, 'referece_logps/rejected': -382.80615234375, 'referece_logps/chosen': -322.53204345703125, 'logits/rejected': -1.0369995832443237, 'logits/chosen': -1.06650972366333, 'epoch': 0.25}

  4%|▍         | 446/10740 [2:13:01<43:47:59, 15.32s/it]

  4%|▍         | 447/10740 [2:13:12<39:43:09, 13.89s/it]

  4%|▍         | 448/10740 [2:13:24<38:23:24, 13.43s/it]

  4%|▍         | 449/10740 [2:13:43<43:19:02, 15.15s/it]

  4%|▍         | 450/10740 [2:14:01<45:22:43, 15.88s/it]

  4%|▍         | 451/10740 [2:14:12<41:06:59, 14.39s/it]


  4%|▍         | 453/10740 [2:14:49<47:51:23, 16.75s/it]
{'loss': 0.524, 'learning_rate': 1.999231550210571e-06, 'rewards/chosen': -1.5528841018676758, 'rewards/rejected': -1.7040048837661743, 'rewards/accuracies': 0.5, 'rewards/margins': 0.15112073719501495, 'policy_logps/rejected': -484.0096435546875, 'policy_logps/chosen': -519.8258056640625, 'referece_logps/rejected': -466.96954345703125, 'referece_logps/chosen': -504.2969970703125, 'logits/rejected': 0.43808484077453613, 'logits/chosen': 0.41896846890449524, 'epoch': 0.25}

  4%|▍         | 454/10740 [2:15:09<50:58:57, 17.84s/it]


  4%|▍         | 456/10740 [2:15:48<53:45:25, 18.82s/it]
{'loss': 0.671, 'learning_rate': 1.999195678873677e-06, 'rewards/chosen': -1.0972312688827515, 'rewards/rejected': -1.0729807615280151, 'rewards/accuracies': 0.375, 'rewards/margins': -0.024250507354736328, 'policy_logps/rejected': -423.3081970214844, 'policy_logps/chosen': -431.6576843261719, 'referece_logps/rejected': -412.57830810546875, 'referece_logps/chosen': -420.68536376953125, 'logits/rejected': -1.0447969436645508, 'logits/chosen': -1.0437062978744507, 'epoch': 0.25}

  4%|▍         | 457/10740 [2:16:06<52:36:04, 18.42s/it]

  4%|▍         | 458/10740 [2:16:26<53:44:40, 18.82s/it]

  4%|▍         | 459/10740 [2:16:46<55:14:26, 19.34s/it]


  4%|▍         | 461/10740 [2:17:20<52:26:41, 18.37s/it]
{'loss': 0.4952, 'learning_rate': 1.9991340757411527e-06, 'rewards/chosen': -0.28743037581443787, 'rewards/rejected': -1.3142378330230713, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0268075466156006, 'policy_logps/rejected': -355.42706298828125, 'policy_logps/chosen': -414.2618713378906, 'referece_logps/rejected': -342.2846984863281, 'referece_logps/chosen': -411.3875427246094, 'logits/rejected': -0.5311688780784607, 'logits/chosen': -0.5701541900634766, 'epoch': 0.26}

  4%|▍         | 462/10740 [2:17:40<53:42:55, 18.81s/it]


  4%|▍         | 464/10740 [2:18:21<55:49:55, 19.56s/it]

  4%|▍         | 465/10740 [2:18:37<52:39:05, 18.45s/it]

  4%|▍         | 466/10740 [2:18:53<50:42:39, 17.77s/it]
{'loss': 0.5283, 'learning_rate': 1.999070200766679e-06, 'rewards/chosen': -0.3959878981113434, 'rewards/rejected': -1.5055381059646606, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1095502376556396, 'policy_logps/rejected': -325.51666259765625, 'policy_logps/chosen': -336.4781494140625, 'referece_logps/rejected': -310.4612731933594, 'referece_logps/chosen': -332.5182189941406, 'logits/rejected': -0.4604310691356659, 'logits/chosen': -0.39000606536865234, 'epoch': 0.26}

  4%|▍         | 467/10740 [2:19:15<54:56:44, 19.25s/it]

  4%|▍         | 468/10740 [2:19:28<48:56:35, 17.15s/it]


  4%|▍         | 470/10740 [2:20:05<50:24:55, 17.67s/it]
{'loss': 0.618, 'learning_rate': 1.999017465158325e-06, 'rewards/chosen': -0.9591729640960693, 'rewards/rejected': -1.2621228694915771, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3029499650001526, 'policy_logps/rejected': -362.63116455078125, 'policy_logps/chosen': -316.53704833984375, 'referece_logps/rejected': -350.0099182128906, 'referece_logps/chosen': -306.9453125, 'logits/rejected': -1.0897607803344727, 'logits/chosen': -1.0435510873794556, 'epoch': 0.26}

  4%|▍         | 471/10740 [2:20:26<53:06:37, 18.62s/it]

  4%|▍         | 472/10740 [2:20:47<55:06:26, 19.32s/it]


  4%|▍         | 474/10740 [2:21:23<51:57:15, 18.22s/it]
{'loss': 0.5827, 'learning_rate': 1.9989632757407203e-06, 'rewards/chosen': -0.7783828973770142, 'rewards/rejected': -1.1669328212738037, 'rewards/accuracies': 0.5, 'rewards/margins': 0.38854992389678955, 'policy_logps/rejected': -493.99725341796875, 'policy_logps/chosen': -351.935546875, 'referece_logps/rejected': -482.327880859375, 'referece_logps/chosen': -344.1517333984375, 'logits/rejected': -0.05382724106311798, 'logits/chosen': -0.042733825743198395, 'epoch': 0.26}

  4%|▍         | 475/10740 [2:21:42<53:05:42, 18.62s/it]

  4%|▍         | 476/10740 [2:22:05<56:14:25, 19.73s/it]

  4%|▍         | 477/10740 [2:22:26<57:24:29, 20.14s/it]


  4%|▍         | 479/10740 [2:23:03<55:38:41, 19.52s/it]

  4%|▍         | 480/10740 [2:23:15<49:09:52, 17.25s/it]
{'loss': 0.5373, 'learning_rate': 1.998879265895051e-06, 'rewards/chosen': -0.3264034390449524, 'rewards/rejected': -0.7768049240112305, 'rewards/accuracies': 0.75, 'rewards/margins': 0.45040154457092285, 'policy_logps/rejected': -288.3359069824219, 'policy_logps/chosen': -232.23641967773438, 'referece_logps/rejected': -280.5678405761719, 'referece_logps/chosen': -228.97238159179688, 'logits/rejected': -0.37613949179649353, 'logits/chosen': -0.3671794831752777, 'epoch': 0.27}

  4%|▍         | 481/10740 [2:23:34<50:33:03, 17.74s/it]

  4%|▍         | 482/10740 [2:23:49<47:59:20, 16.84s/it]

  4%|▍         | 483/10740 [2:24:02<45:21:23, 15.92s/it]

  5%|▍         | 484/10740 [2:24:23<48:52:03, 17.15s/it]

  5%|▍         | 485/10740 [2:24:40<48:50:07, 17.14s/it]

  5%|▍         | 486/10740 [2:24:52<44:25:11, 15.60s/it]

  5%|▍         | 487/10740 [2:25:07<44:34:51, 15.65s/it]


  5%|▍         | 489/10740 [2:25:39<44:37:30, 15.67s/it]
{'loss': 0.5494, 'learning_rate': 1.998747118807036e-06, 'rewards/chosen': -0.9412739872932434, 'rewards/rejected': -1.523170828819275, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5818967223167419, 'policy_logps/rejected': -324.4286804199219, 'policy_logps/chosen': -325.8131103515625, 'referece_logps/rejected': -309.1969909667969, 'referece_logps/chosen': -316.4003601074219, 'logits/rejected': -0.7434782981872559, 'logits/chosen': -0.8024415969848633, 'epoch': 0.27}

  5%|▍         | 490/10740 [2:25:55<44:50:25, 15.75s/it]

  5%|▍         | 491/10740 [2:26:16<49:36:26, 17.42s/it]


  5%|▍         | 493/10740 [2:26:59<55:20:46, 19.44s/it]
{'loss': 0.4505, 'learning_rate': 1.9986860249085163e-06, 'rewards/chosen': -0.6262432932853699, 'rewards/rejected': -0.866820216178894, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24057689309120178, 'policy_logps/rejected': -189.95068359375, 'policy_logps/chosen': -179.01243591308594, 'referece_logps/rejected': -181.28248596191406, 'referece_logps/chosen': -172.75, 'logits/rejected': -1.091183066368103, 'logits/chosen': -1.160280704498291, 'epoch': 0.28}

  5%|▍         | 494/10740 [2:27:16<52:42:51, 18.52s/it]

  5%|▍         | 495/10740 [2:27:37<55:13:14, 19.40s/it]

  5%|▍         | 496/10740 [2:28:00<58:02:21, 20.40s/it]

  5%|▍         | 497/10740 [2:28:18<56:25:05, 19.83s/it]


  5%|▍         | 499/10740 [2:28:56<55:24:54, 19.48s/it]

  5%|▍         | 500/10740 [2:29:14<54:19:20, 19.10s/it]
  5%|▍         | 500/10740 [2:29:14<54:19:20, 19.10s/it]/mnt/petrelfs/songmingyang/anaconda3/envs/vcd_origin/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
  5%|▍         | 501/10740 [2:29:47<66:03:39, 23.23s/it]

  5%|▍         | 502/10740 [2:30:10<65:42:20, 23.10s/it]

  5%|▍         | 503/10740 [2:30:30<62:48:13, 22.09s/it]

  5%|▍         | 504/10740 [2:30:53<63:38:13, 22.38s/it]

  5%|▍         | 505/10740 [2:31:12<61:12:53, 21.53s/it]

  5%|▍         | 506/10740 [2:31:32<59:49:57, 21.05s/it]

  5%|▍         | 507/10740 [2:31:54<60:39:00, 21.34s/it]

  5%|▍         | 508/10740 [2:32:13<58:10:09, 20.47s/it]

  5%|▍         | 509/10740 [2:32:31<56:18:51, 19.82s/it]

  5%|▍         | 510/10740 [2:32:51<56:39:26, 19.94s/it]
[2024-04-01 21:46:35,086] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 511/10740 [2:33:08<54:13:45, 19.09s/it]
[2024-04-01 21:46:52,183] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 512/10740 [2:33:27<54:09:13, 19.06s/it]

  5%|▍         | 513/10740 [2:33:49<56:28:43, 19.88s/it]

  5%|▍         | 514/10740 [2:34:07<55:01:24, 19.37s/it]

  5%|▍         | 515/10740 [2:34:21<50:01:48, 17.61s/it]

  5%|▍         | 516/10740 [2:34:41<52:24:10, 18.45s/it]
[2024-04-01 21:48:25,082] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 517/10740 [2:34:55<47:55:25, 16.88s/it]

  5%|▍         | 518/10740 [2:35:07<44:07:30, 15.54s/it]

  5%|▍         | 519/10740 [2:35:28<48:45:02, 17.17s/it]

  5%|▍         | 520/10740 [2:35:45<48:53:18, 17.22s/it]

  5%|▍         | 521/10740 [2:36:04<50:08:39, 17.67s/it]

  5%|▍         | 522/10740 [2:36:25<53:07:48, 18.72s/it]
[2024-04-01 21:50:08,897] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 523/10740 [2:36:44<53:02:36, 18.69s/it]
[2024-04-01 21:50:27,521] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 524/10740 [2:36:55<46:21:20, 16.34s/it]

  5%|▍         | 525/10740 [2:37:15<49:32:34, 17.46s/it]
[2024-04-01 21:50:58,446] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 526/10740 [2:37:29<46:43:30, 16.47s/it]

  5%|▍         | 527/10740 [2:37:49<49:30:25, 17.45s/it]

  5%|▍         | 528/10740 [2:38:09<51:47:06, 18.26s/it]

  5%|▍         | 529/10740 [2:38:32<55:40:10, 19.63s/it]

  5%|▍         | 530/10740 [2:38:47<52:31:13, 18.52s/it]

  5%|▍         | 531/10740 [2:39:08<53:53:54, 19.01s/it]

  5%|▍         | 532/10740 [2:39:24<51:20:26, 18.11s/it]

  5%|▍         | 533/10740 [2:39:43<51:58:34, 18.33s/it]

  5%|▍         | 534/10740 [2:39:55<47:04:53, 16.61s/it]

  5%|▍         | 535/10740 [2:40:17<51:54:24, 18.31s/it]
[2024-04-01 21:54:01,115] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 536/10740 [2:40:34<50:17:45, 17.74s/it]
[2024-04-01 21:54:17,538] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5222, 'learning_rate': 1.9979374984530587e-06, 'rewards/chosen': -0.5670026540756226, 'rewards/rejected': -1.3307522535324097, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7637495994567871, 'policy_logps/rejected': -333.95819091796875, 'policy_logps/chosen': -339.00799560546875, 'referece_logps/rejected': -320.65069580078125, 'referece_logps/chosen': -333.33795166015625, 'logits/rejected': 0.18306635320186615, 'logits/chosen': 0.08815999329090118, 'epoch': 0.3}


  5%|▌         | 538/10740 [2:41:09<50:34:10, 17.84s/it]

  5%|▌         | 539/10740 [2:41:29<52:41:57, 18.60s/it]
[2024-04-01 21:55:12,760] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 540/10740 [2:41:51<55:19:19, 19.53s/it]
[2024-04-01 21:55:34,449] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 541/10740 [2:42:06<51:49:47, 18.29s/it]

  5%|▌         | 542/10740 [2:42:19<47:19:15, 16.70s/it]

  5%|▌         | 543/10740 [2:42:36<47:41:22, 16.84s/it]

  5%|▌         | 544/10740 [2:42:56<50:20:52, 17.78s/it]

  5%|▌         | 545/10740 [2:43:17<52:37:27, 18.58s/it]
[2024-04-01 21:57:00,444] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 546/10740 [2:43:38<55:15:11, 19.51s/it]

  5%|▌         | 547/10740 [2:43:58<55:28:06, 19.59s/it]

  5%|▌         | 548/10740 [2:44:21<58:12:16, 20.56s/it]

  5%|▌         | 549/10740 [2:44:41<57:36:34, 20.35s/it]

  5%|▌         | 550/10740 [2:45:02<58:01:07, 20.50s/it]

  5%|▌         | 551/10740 [2:45:22<57:36:52, 20.36s/it]

  5%|▌         | 552/10740 [2:45:41<56:33:10, 19.98s/it]

  5%|▌         | 553/10740 [2:45:57<53:36:21, 18.94s/it]

  5%|▌         | 554/10740 [2:46:19<56:15:50, 19.89s/it]

  5%|▌         | 555/10740 [2:46:40<56:32:28, 19.99s/it]

  5%|▌         | 556/10740 [2:47:00<56:50:24, 20.09s/it]

  5%|▌         | 557/10740 [2:47:18<55:17:27, 19.55s/it]

  5%|▌         | 558/10740 [2:47:40<57:25:23, 20.30s/it]

  5%|▌         | 559/10740 [2:48:02<58:58:20, 20.85s/it]
[2024-04-01 22:01:46,201] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 560/10740 [2:48:23<58:40:45, 20.75s/it]

  5%|▌         | 561/10740 [2:48:35<51:12:45, 18.11s/it]

  5%|▌         | 562/10740 [2:48:54<52:00:23, 18.39s/it]
{'loss': 0.6062, 'learning_rate': 1.9974034768969897e-06, 'rewards/chosen': -0.8906822204589844, 'rewards/rejected': -1.3974655866622925, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5067833662033081, 'policy_logps/rejected': -324.2816162109375, 'policy_logps/chosen': -385.0805969238281, 'referece_logps/rejected': -310.30694580078125, 'referece_logps/chosen': -376.17376708984375, 'logits/rejected': -0.21817490458488464, 'logits/chosen': -0.3160099983215332, 'epoch': 0.31}


  5%|▌         | 564/10740 [2:49:32<52:04:31, 18.42s/it]

  5%|▌         | 565/10740 [2:49:51<53:00:00, 18.75s/it]

  5%|▌         | 566/10740 [2:50:03<47:22:30, 16.76s/it]
[2024-04-01 22:03:47,016] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 567/10740 [2:50:23<50:03:25, 17.71s/it]

  5%|▌         | 568/10740 [2:50:45<53:17:49, 18.86s/it]

  5%|▌         | 569/10740 [2:51:04<53:46:40, 19.03s/it]

  5%|▌         | 570/10740 [2:51:25<55:35:57, 19.68s/it]

  5%|▌         | 571/10740 [2:51:44<54:49:46, 19.41s/it]

  5%|▌         | 572/10740 [2:52:06<57:09:02, 20.23s/it]

  5%|▌         | 573/10740 [2:52:26<56:46:25, 20.10s/it]

  5%|▌         | 574/10740 [2:52:46<56:50:00, 20.13s/it]

  5%|▌         | 575/10740 [2:52:59<50:33:17, 17.90s/it]

  5%|▌         | 576/10740 [2:53:19<52:12:01, 18.49s/it]

  5%|▌         | 577/10740 [2:53:33<48:18:25, 17.11s/it]

  5%|▌         | 578/10740 [2:53:53<50:40:00, 17.95s/it]

  5%|▌         | 579/10740 [2:54:10<50:10:40, 17.78s/it]

  5%|▌         | 580/10740 [2:54:32<53:58:17, 19.12s/it]
[2024-04-01 22:08:16,044] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 581/10740 [2:54:54<55:50:49, 19.79s/it]
{'loss': 0.4828, 'learning_rate': 1.9969744473491803e-06, 'rewards/chosen': -0.1975807249546051, 'rewards/rejected': -1.3224767446517944, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1248960494995117, 'policy_logps/rejected': -290.1932678222656, 'policy_logps/chosen': -264.66705322265625, 'referece_logps/rejected': -276.968505859375, 'referece_logps/chosen': -262.6912536621094, 'logits/rejected': -0.5645643472671509, 'logits/chosen': -0.5646845698356628, 'epoch': 0.32}

  5%|▌         | 582/10740 [2:55:13<55:47:05, 19.77s/it]

  5%|▌         | 583/10740 [2:55:35<57:31:09, 20.39s/it]

  5%|▌         | 584/10740 [2:55:59<60:01:59, 21.28s/it]

  5%|▌         | 585/10740 [2:56:16<56:52:22, 20.16s/it]

  5%|▌         | 586/10740 [2:56:36<57:02:46, 20.23s/it]

  5%|▌         | 587/10740 [2:56:54<55:03:31, 19.52s/it]

  5%|▌         | 588/10740 [2:57:16<56:49:47, 20.15s/it]

  5%|▌         | 589/10740 [2:57:33<53:50:34, 19.10s/it]

  5%|▌         | 590/10740 [2:57:53<55:06:50, 19.55s/it]
[2024-04-01 22:11:36,966] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 591/10740 [2:58:07<50:10:00, 17.79s/it]

  6%|▌         | 592/10740 [2:58:30<54:25:41, 19.31s/it]

  6%|▌         | 593/10740 [2:58:49<54:34:43, 19.36s/it]

  6%|▌         | 594/10740 [2:59:10<55:28:47, 19.69s/it]

  6%|▌         | 595/10740 [2:59:31<56:25:12, 20.02s/it]

  6%|▌         | 596/10740 [2:59:48<54:30:29, 19.34s/it]
[2024-04-01 22:13:32,008] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 597/10740 [3:00:11<57:23:42, 20.37s/it]

  6%|▌         | 598/10740 [3:00:25<51:42:35, 18.35s/it]

  6%|▌         | 599/10740 [3:00:45<53:27:17, 18.98s/it]
[2024-04-01 22:14:28,852] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 600/10740 [3:01:08<57:02:09, 20.25s/it]
[2024-04-01 22:14:52,073] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 601/10740 [3:01:25<54:17:01, 19.27s/it]
[2024-04-01 22:15:09,071] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 602/10740 [3:01:40<50:03:05, 17.77s/it]

  6%|▌         | 603/10740 [3:02:01<53:30:13, 19.00s/it]
[2024-04-01 22:15:45,208] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6262, 'learning_rate': 1.996436781938229e-06, 'rewards/chosen': -1.4244390726089478, 'rewards/rejected': -1.7454204559326172, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3209814131259918, 'policy_logps/rejected': -543.2780151367188, 'policy_logps/chosen': -478.39093017578125, 'referece_logps/rejected': -525.8238525390625, 'referece_logps/chosen': -464.1465759277344, 'logits/rejected': -0.7714635729789734, 'logits/chosen': -0.8475526571273804, 'epoch': 0.34}


  6%|▌         | 605/10740 [3:02:39<54:03:31, 19.20s/it]

  6%|▌         | 606/10740 [3:02:57<53:05:05, 18.86s/it]

  6%|▌         | 607/10740 [3:03:16<52:30:56, 18.66s/it]

  6%|▌         | 608/10740 [3:03:27<46:42:58, 16.60s/it]

  6%|▌         | 609/10740 [3:03:49<50:43:50, 18.03s/it]

  6%|▌         | 610/10740 [3:04:07<50:38:03, 17.99s/it]
{'loss': 0.4704, 'learning_rate': 1.9962565066529987e-06, 'rewards/chosen': -0.7896555662155151, 'rewards/rejected': -2.217617988586426, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4279625415802002, 'policy_logps/rejected': -342.249267578125, 'policy_logps/chosen': -331.2887268066406, 'referece_logps/rejected': -320.0731201171875, 'referece_logps/chosen': -323.3921813964844, 'logits/rejected': -0.7412405014038086, 'logits/chosen': -0.7569683790206909, 'epoch': 0.34}


  6%|▌         | 612/10740 [3:04:47<53:00:34, 18.84s/it]

  6%|▌         | 613/10740 [3:05:03<50:48:11, 18.06s/it]

  6%|▌         | 614/10740 [3:05:19<48:58:17, 17.41s/it]
{'loss': 0.451, 'learning_rate': 1.9961514986892782e-06, 'rewards/chosen': -0.4415226876735687, 'rewards/rejected': -0.6795356869697571, 'rewards/accuracies': 0.625, 'rewards/margins': 0.23801301419734955, 'policy_logps/rejected': -267.42181396484375, 'policy_logps/chosen': -218.9144287109375, 'referece_logps/rejected': -260.62646484375, 'referece_logps/chosen': -214.4991912841797, 'logits/rejected': -0.18536029756069183, 'logits/chosen': -0.30766892433166504, 'epoch': 0.34}


  6%|▌         | 616/10740 [3:05:51<47:42:17, 16.96s/it]

  6%|▌         | 617/10740 [3:06:09<48:55:45, 17.40s/it]

  6%|▌         | 618/10740 [3:06:29<50:43:08, 18.04s/it]

  6%|▌         | 619/10740 [3:06:48<51:28:38, 18.31s/it]

  6%|▌         | 620/10740 [3:07:03<49:16:23, 17.53s/it]

  6%|▌         | 621/10740 [3:07:22<49:58:35, 17.78s/it]

  6%|▌         | 622/10740 [3:07:42<51:36:52, 18.36s/it]

  6%|▌         | 623/10740 [3:07:55<47:52:17, 17.03s/it]

  6%|▌         | 624/10740 [3:08:14<48:53:08, 17.40s/it]

  6%|▌         | 625/10740 [3:08:35<52:08:51, 18.56s/it]
{'loss': 0.6298, 'learning_rate': 1.9958552525572614e-06, 'rewards/chosen': -0.9767069220542908, 'rewards/rejected': -1.418045997619629, 'rewards/accuracies': 0.75, 'rewards/margins': 0.44133904576301575, 'policy_logps/rejected': -266.17047119140625, 'policy_logps/chosen': -270.08038330078125, 'referece_logps/rejected': -251.9900360107422, 'referece_logps/chosen': -260.3133239746094, 'logits/rejected': -1.005659580230713, 'logits/chosen': -1.0061439275741577, 'epoch': 0.35}


  6%|▌         | 627/10740 [3:09:12<51:04:42, 18.18s/it]

  6%|▌         | 628/10740 [3:09:26<47:38:41, 16.96s/it]

  6%|▌         | 629/10740 [3:09:45<49:50:59, 17.75s/it]
{'loss': 0.7137, 'learning_rate': 1.9957448092540083e-06, 'rewards/chosen': -1.1613030433654785, 'rewards/rejected': -1.105365514755249, 'rewards/accuracies': 0.5, 'rewards/margins': -0.05593748763203621, 'policy_logps/rejected': -355.72039794921875, 'policy_logps/chosen': -408.7286682128906, 'referece_logps/rejected': -344.666748046875, 'referece_logps/chosen': -397.1156311035156, 'logits/rejected': -0.6946816444396973, 'logits/chosen': -0.642978847026825, 'epoch': 0.35}


  6%|▌         | 631/10740 [3:10:21<49:18:38, 17.56s/it]

  6%|▌         | 632/10740 [3:10:34<44:54:02, 15.99s/it]

  6%|▌         | 633/10740 [3:10:44<40:18:14, 14.36s/it]

  6%|▌         | 634/10740 [3:11:03<44:03:48, 15.70s/it]

  6%|▌         | 635/10740 [3:11:21<46:18:37, 16.50s/it]
{'loss': 0.629, 'learning_rate': 1.995576427387268e-06, 'rewards/chosen': -0.6284140348434448, 'rewards/rejected': -1.907097578048706, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2786835432052612, 'policy_logps/rejected': -401.7885437011719, 'policy_logps/chosen': -431.33392333984375, 'referece_logps/rejected': -382.7175598144531, 'referece_logps/chosen': -425.0498046875, 'logits/rejected': -0.3352896571159363, 'logits/chosen': -0.4448659420013428, 'epoch': 0.35}


  6%|▌         | 637/10740 [3:11:52<44:43:09, 15.93s/it]

  6%|▌         | 638/10740 [3:12:16<51:37:34, 18.40s/it]
[2024-04-01 22:25:59,388] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 639/10740 [3:12:29<47:42:01, 17.00s/it]
[2024-04-01 22:26:13,128] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 640/10740 [3:12:51<51:36:45, 18.40s/it]

  6%|▌         | 641/10740 [3:13:06<48:46:52, 17.39s/it]

  6%|▌         | 642/10740 [3:13:24<48:55:33, 17.44s/it]

  6%|▌         | 643/10740 [3:13:41<49:08:44, 17.52s/it]
{'loss': 0.5368, 'learning_rate': 1.995346847523201e-06, 'rewards/chosen': -0.3235451281070709, 'rewards/rejected': -1.5408165454864502, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2172715663909912, 'policy_logps/rejected': -226.79380798339844, 'policy_logps/chosen': -192.2606964111328, 'referece_logps/rejected': -211.3856201171875, 'referece_logps/chosen': -189.02525329589844, 'logits/rejected': -0.7985331416130066, 'logits/chosen': -0.9146779775619507, 'epoch': 0.36}

  6%|▌         | 644/10740 [3:14:01<50:51:50, 18.14s/it]

  6%|▌         | 645/10740 [3:14:19<50:51:25, 18.14s/it]


  6%|▌         | 647/10740 [3:14:57<52:39:14, 18.78s/it]
[2024-04-01 22:28:41,114] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 648/10740 [3:15:20<55:48:25, 19.91s/it]
[2024-04-01 22:29:03,650] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 649/10740 [3:15:32<48:52:43, 17.44s/it]

  6%|▌         | 650/10740 [3:15:52<51:03:07, 18.21s/it]

  6%|▌         | 651/10740 [3:16:14<54:30:50, 19.45s/it]

  6%|▌         | 652/10740 [3:16:32<53:20:31, 19.04s/it]

  6%|▌         | 653/10740 [3:16:54<56:07:12, 20.03s/it]
[2024-04-01 22:30:38,102] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 654/10740 [3:17:12<53:45:47, 19.19s/it]

  6%|▌         | 655/10740 [3:17:32<54:30:58, 19.46s/it]

  6%|▌         | 656/10740 [3:17:50<53:30:51, 19.10s/it]

  6%|▌         | 657/10740 [3:18:05<49:43:08, 17.75s/it]

  6%|▌         | 658/10740 [3:18:18<45:53:00, 16.38s/it]

  6%|▌         | 659/10740 [3:18:34<45:46:22, 16.35s/it]

  6%|▌         | 660/10740 [3:18:54<48:59:54, 17.50s/it]

  6%|▌         | 661/10740 [3:19:11<48:10:02, 17.20s/it]

  6%|▌         | 662/10740 [3:19:31<50:21:35, 17.99s/it]

  6%|▌         | 663/10740 [3:19:51<52:07:58, 18.62s/it]

  6%|▌         | 664/10740 [3:20:13<55:05:28, 19.68s/it]
{'loss': 0.4304, 'learning_rate': 1.9947166377851117e-06, 'rewards/chosen': -1.170914649963379, 'rewards/rejected': -2.4211745262145996, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2502601146697998, 'policy_logps/rejected': -384.6916198730469, 'policy_logps/chosen': -363.2803649902344, 'referece_logps/rejected': -360.4798278808594, 'referece_logps/chosen': -351.57122802734375, 'logits/rejected': -0.4707866311073303, 'logits/chosen': -0.5115132331848145, 'epoch': 0.37}
[2024-04-01 22:34:18,932] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 665/10740 [3:20:35<57:22:00, 20.50s/it]
[2024-04-01 22:34:37,291] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  6%|▌         | 667/10740 [3:21:07<49:57:04, 17.85s/it]

  6%|▌         | 668/10740 [3:21:28<52:34:51, 18.79s/it]
{'loss': 0.6488, 'learning_rate': 1.9945920734540332e-06, 'rewards/chosen': -0.7520207166671753, 'rewards/rejected': -1.1530003547668457, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4009796380996704, 'policy_logps/rejected': -237.84332275390625, 'policy_logps/chosen': -289.00286865234375, 'referece_logps/rejected': -226.31332397460938, 'referece_logps/chosen': -281.4826354980469, 'logits/rejected': -0.8242983818054199, 'logits/chosen': -0.7446262240409851, 'epoch': 0.37}


  6%|▌         | 670/10740 [3:22:10<55:58:58, 20.01s/it]
{'loss': 0.5524, 'learning_rate': 1.9945292485136144e-06, 'rewards/chosen': -0.861103355884552, 'rewards/rejected': -1.8222171068191528, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9611138105392456, 'policy_logps/rejected': -321.2411804199219, 'policy_logps/chosen': -358.96929931640625, 'referece_logps/rejected': -303.0190124511719, 'referece_logps/chosen': -350.3583068847656, 'logits/rejected': -0.786994457244873, 'logits/chosen': -1.0631438493728638, 'epoch': 0.37}


  6%|▋         | 672/10740 [3:22:52<57:20:10, 20.50s/it]

  6%|▋         | 673/10740 [3:23:13<57:28:50, 20.56s/it]

  6%|▋         | 674/10740 [3:23:31<55:26:13, 19.83s/it]
{'loss': 0.5532, 'learning_rate': 1.9944025131973003e-06, 'rewards/chosen': -0.7793878316879272, 'rewards/rejected': -0.8725042939186096, 'rewards/accuracies': 0.375, 'rewards/margins': 0.0931163802742958, 'policy_logps/rejected': -344.4457092285156, 'policy_logps/chosen': -419.4871826171875, 'referece_logps/rejected': -335.72064208984375, 'referece_logps/chosen': -411.69329833984375, 'logits/rejected': 0.12817522883415222, 'logits/chosen': 0.06503653526306152, 'epoch': 0.38}


  6%|▋         | 676/10740 [3:24:06<52:41:20, 18.85s/it]

  6%|▋         | 677/10740 [3:24:25<52:31:46, 18.79s/it]

  6%|▋         | 678/10740 [3:24:44<52:49:43, 18.90s/it]

  6%|▋         | 679/10740 [3:24:57<47:48:23, 17.11s/it]

  6%|▋         | 680/10740 [3:25:10<44:38:19, 15.97s/it]

  6%|▋         | 681/10740 [3:25:28<46:10:46, 16.53s/it]

  6%|▋         | 682/10740 [3:25:45<46:16:43, 16.56s/it]

  6%|▋         | 683/10740 [3:26:06<50:24:51, 18.05s/it]

  6%|▋         | 684/10740 [3:26:20<46:41:46, 16.72s/it]
{'loss': 0.5971, 'learning_rate': 1.9940793442815484e-06, 'rewards/chosen': -0.520770788192749, 'rewards/rejected': -1.0632611513137817, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5424904227256775, 'policy_logps/rejected': -708.4251708984375, 'policy_logps/chosen': -465.4771423339844, 'referece_logps/rejected': -697.7926025390625, 'referece_logps/chosen': -460.2694091796875, 'logits/rejected': -1.1064060926437378, 'logits/chosen': -0.8372133374214172, 'epoch': 0.38}


  6%|▋         | 686/10740 [3:26:48<42:22:40, 15.17s/it]

  6%|▋         | 687/10740 [3:27:09<47:00:56, 16.84s/it]

  6%|▋         | 688/10740 [3:27:30<50:51:26, 18.21s/it]
[2024-04-01 22:41:13,901] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▋         | 689/10740 [3:27:44<47:11:17, 16.90s/it]

  6%|▋         | 690/10740 [3:27:58<45:07:38, 16.17s/it]

  6%|▋         | 691/10740 [3:28:19<48:38:44, 17.43s/it]
{'loss': 0.3984, 'learning_rate': 1.9938477462360885e-06, 'rewards/chosen': -0.6463512182235718, 'rewards/rejected': -2.1298234462738037, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4834723472595215, 'policy_logps/rejected': -506.9794006347656, 'policy_logps/chosen': -371.560791015625, 'referece_logps/rejected': -485.68115234375, 'referece_logps/chosen': -365.0972900390625, 'logits/rejected': -0.029649965465068817, 'logits/chosen': -0.02765624225139618, 'epoch': 0.39}


  6%|▋         | 693/10740 [3:28:52<48:42:26, 17.45s/it]

  6%|▋         | 694/10740 [3:29:05<44:18:11, 15.88s/it]
{'loss': 0.5441, 'learning_rate': 1.993747133977154e-06, 'rewards/chosen': -0.7405559420585632, 'rewards/rejected': -0.9873842597007751, 'rewards/accuracies': 0.375, 'rewards/margins': 0.24682828783988953, 'policy_logps/rejected': -433.9854736328125, 'policy_logps/chosen': -401.46429443359375, 'referece_logps/rejected': -424.1116027832031, 'referece_logps/chosen': -394.0587463378906, 'logits/rejected': -0.4134559631347656, 'logits/chosen': -0.4107344150543213, 'epoch': 0.39}


  6%|▋         | 696/10740 [3:29:39<45:01:40, 16.14s/it]

  6%|▋         | 697/10740 [3:29:55<44:40:51, 16.02s/it]

  6%|▋         | 698/10740 [3:30:14<47:45:13, 17.12s/it]

  7%|▋         | 699/10740 [3:30:33<49:05:36, 17.60s/it]
{'loss': 0.4927, 'learning_rate': 1.9935776392452336e-06, 'rewards/chosen': -0.262098491191864, 'rewards/rejected': -1.8806480169296265, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6185495853424072, 'policy_logps/rejected': -351.9539794921875, 'policy_logps/chosen': -231.69467163085938, 'referece_logps/rejected': -333.14752197265625, 'referece_logps/chosen': -229.07369995117188, 'logits/rejected': -0.15798580646514893, 'logits/chosen': -0.2783249616622925, 'epoch': 0.39}
[2024-04-01 22:44:39,525] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 701/10740 [3:31:21<58:22:59, 20.94s/it]
{'loss': 0.5071, 'learning_rate': 1.993509208752565e-06, 'rewards/chosen': -0.5753105282783508, 'rewards/rejected': -1.0136704444885254, 'rewards/accuracies': 0.625, 'rewards/margins': 0.43835994601249695, 'policy_logps/rejected': -407.7218933105469, 'policy_logps/chosen': -347.3018798828125, 'referece_logps/rejected': -397.5852355957031, 'referece_logps/chosen': -341.54876708984375, 'logits/rejected': 0.5677075386047363, 'logits/chosen': 0.5878093838691711, 'epoch': 0.39}
[2024-04-01 22:45:21,592] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 703/10740 [3:31:53<51:29:28, 18.47s/it]
{'loss': 0.5164, 'learning_rate': 1.993440416811508e-06, 'rewards/chosen': -0.8218957781791687, 'rewards/rejected': -2.1354193687438965, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3135234117507935, 'policy_logps/rejected': -482.3904724121094, 'policy_logps/chosen': -421.2131652832031, 'referece_logps/rejected': -461.0362548828125, 'referece_logps/chosen': -412.99420166015625, 'logits/rejected': 0.5735834240913391, 'logits/chosen': 0.5714215040206909, 'epoch': 0.39}


  7%|▋         | 705/10740 [3:32:25<47:13:40, 16.94s/it]

  7%|▋         | 706/10740 [3:32:43<47:59:29, 17.22s/it]

  7%|▋         | 707/10740 [3:32:57<44:56:53, 16.13s/it]

  7%|▋         | 708/10740 [3:33:13<45:18:39, 16.26s/it]

  7%|▋         | 709/10740 [3:33:35<49:59:35, 17.94s/it]

  7%|▋         | 710/10740 [3:33:49<46:54:08, 16.83s/it]

  7%|▋         | 711/10740 [3:34:10<49:44:22, 17.85s/it]
[2024-04-01 22:47:53,290] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.604, 'learning_rate': 1.993161635065918e-06, 'rewards/chosen': -1.1167106628417969, 'rewards/rejected': -1.4158469438552856, 'rewards/accuracies': 0.625, 'rewards/margins': 0.29913634061813354, 'policy_logps/rejected': -434.8669128417969, 'policy_logps/chosen': -438.12322998046875, 'referece_logps/rejected': -420.70843505859375, 'referece_logps/chosen': -426.95611572265625, 'logits/rejected': -0.4829499423503876, 'logits/chosen': -0.4814436137676239, 'epoch': 0.4}

  7%|▋         | 712/10740 [3:34:33<54:11:03, 19.45s/it]


  7%|▋         | 714/10740 [3:35:12<54:16:07, 19.49s/it]
{'loss': 0.4608, 'learning_rate': 1.993055601370774e-06, 'rewards/chosen': 0.0356912687420845, 'rewards/rejected': -1.8023970127105713, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8380882740020752, 'policy_logps/rejected': -309.1778564453125, 'policy_logps/chosen': -309.5909118652344, 'referece_logps/rejected': -291.15386962890625, 'referece_logps/chosen': -309.94781494140625, 'logits/rejected': -0.08429054915904999, 'logits/chosen': -0.1407548487186432, 'epoch': 0.4}

  7%|▋         | 715/10740 [3:35:25<48:53:40, 17.56s/it]


  7%|▋         | 717/10740 [3:36:07<54:14:05, 19.48s/it]

  7%|▋         | 718/10740 [3:36:24<51:48:22, 18.61s/it]
{'loss': 0.554, 'learning_rate': 1.9929129586339637e-06, 'rewards/chosen': -1.342645525932312, 'rewards/rejected': -1.340795636177063, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0018499195575714111, 'policy_logps/rejected': -342.69732666015625, 'policy_logps/chosen': -326.0086364746094, 'referece_logps/rejected': -329.28936767578125, 'referece_logps/chosen': -312.5821533203125, 'logits/rejected': -0.969933807849884, 'logits/chosen': -0.9197404384613037, 'epoch': 0.4}

  7%|▋         | 719/10740 [3:36:34<45:08:39, 16.22s/it]


  7%|▋         | 721/10740 [3:37:12<49:27:11, 17.77s/it]
{'loss': 0.5446, 'learning_rate': 1.9928050283374124e-06, 'rewards/chosen': -0.9102537631988525, 'rewards/rejected': -1.5268348455429077, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6165810823440552, 'policy_logps/rejected': -330.4677734375, 'policy_logps/chosen': -258.51531982421875, 'referece_logps/rejected': -315.1994323730469, 'referece_logps/chosen': -249.4127960205078, 'logits/rejected': -0.9474928379058838, 'logits/chosen': -0.9985688924789429, 'epoch': 0.4}

  7%|▋         | 722/10740 [3:37:35<53:38:06, 19.27s/it]

  7%|▋         | 723/10740 [3:37:51<51:20:24, 18.45s/it]

  7%|▋         | 724/10740 [3:38:04<47:09:29, 16.95s/it]


  7%|▋         | 726/10740 [3:38:42<50:09:39, 18.03s/it]
{'loss': 0.524, 'learning_rate': 1.9926233385928243e-06, 'rewards/chosen': -0.8863210082054138, 'rewards/rejected': -1.5326220989227295, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6463009715080261, 'policy_logps/rejected': -464.6704406738281, 'policy_logps/chosen': -538.911376953125, 'referece_logps/rejected': -449.3442077636719, 'referece_logps/chosen': -530.0482177734375, 'logits/rejected': -0.5916284918785095, 'logits/chosen': -0.6049389839172363, 'epoch': 0.41}
[2024-04-01 22:52:46,848] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 728/10740 [3:39:18<49:21:52, 17.75s/it]
{'loss': 0.5676, 'learning_rate': 1.992550030701108e-06, 'rewards/chosen': -1.2037817239761353, 'rewards/rejected': -1.8242661952972412, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6204845905303955, 'policy_logps/rejected': -482.4215087890625, 'policy_logps/chosen': -364.2984924316406, 'referece_logps/rejected': -464.1788024902344, 'referece_logps/chosen': -352.26068115234375, 'logits/rejected': -0.40135425329208374, 'logits/chosen': -0.3556864857673645, 'epoch': 0.41}

  7%|▋         | 729/10740 [3:39:39<51:54:33, 18.67s/it]


  7%|▋         | 731/10740 [3:40:11<49:15:01, 17.71s/it]
{'loss': 0.6481, 'learning_rate': 1.9924393918104725e-06, 'rewards/chosen': -1.3302297592163086, 'rewards/rejected': -1.1833217144012451, 'rewards/accuracies': 0.625, 'rewards/margins': -0.1469079852104187, 'policy_logps/rejected': -439.844482421875, 'policy_logps/chosen': -417.28466796875, 'referece_logps/rejected': -428.01123046875, 'referece_logps/chosen': -403.9823913574219, 'logits/rejected': -0.7886192202568054, 'logits/chosen': -0.753143846988678, 'epoch': 0.41}

  7%|▋         | 732/10740 [3:40:27<47:06:37, 16.95s/it]

  7%|▋         | 733/10740 [3:40:44<47:40:15, 17.15s/it]

  7%|▋         | 734/10740 [3:40:59<45:59:49, 16.55s/it]

  7%|▋         | 735/10740 [3:41:19<48:33:19, 17.47s/it]


  7%|▋         | 737/10740 [3:41:48<43:51:54, 15.79s/it]
{'loss': 0.4538, 'learning_rate': 1.9922156769710736e-06, 'rewards/chosen': -0.38404160737991333, 'rewards/rejected': -1.775743007659912, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3917014598846436, 'policy_logps/rejected': -653.8037109375, 'policy_logps/chosen': -401.8145446777344, 'referece_logps/rejected': -636.0462036132812, 'referece_logps/chosen': -397.9741516113281, 'logits/rejected': 0.10735113173723221, 'logits/chosen': 0.20206411182880402, 'epoch': 0.41}


  7%|▋         | 739/10740 [3:42:26<47:18:11, 17.03s/it]
{'loss': 0.543, 'learning_rate': 1.9921403833660593e-06, 'rewards/chosen': -0.9491367340087891, 'rewards/rejected': -2.7320470809936523, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7829103469848633, 'policy_logps/rejected': -532.1091918945312, 'policy_logps/chosen': -602.7808227539062, 'referece_logps/rejected': -504.7886962890625, 'referece_logps/chosen': -593.2894897460938, 'logits/rejected': -0.3000756502151489, 'logits/chosen': -0.36632129549980164, 'epoch': 0.41}

  7%|▋         | 740/10740 [3:42:41<45:39:59, 16.44s/it]

  7%|▋         | 741/10740 [3:43:03<50:05:38, 18.04s/it]
[2024-04-01 22:57:06,855] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 742/10740 [3:43:23<52:04:20, 18.75s/it]
[2024-04-01 22:57:30,286] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 743/10740 [3:43:47<55:58:01, 20.15s/it]


  7%|▋         | 745/10740 [3:44:28<56:47:12, 20.45s/it]
{'loss': 0.4123, 'learning_rate': 1.991912336958868e-06, 'rewards/chosen': -0.07087327539920807, 'rewards/rejected': -0.8687229156494141, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7978495955467224, 'policy_logps/rejected': -414.1365661621094, 'policy_logps/chosen': -433.4765930175781, 'referece_logps/rejected': -405.4493408203125, 'referece_logps/chosen': -432.76788330078125, 'logits/rejected': -0.3460565507411957, 'logits/chosen': -0.34743019938468933, 'epoch': 0.42}
[2024-04-01 22:58:33,192] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 746/10740 [3:44:49<57:37:17, 20.76s/it]
[2024-04-01 22:58:50,512] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 748/10740 [3:45:26<54:22:47, 19.59s/it]
{'loss': 0.5475, 'learning_rate': 1.991797095780907e-06, 'rewards/chosen': -0.7508670687675476, 'rewards/rejected': -1.863842487335205, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1129753589630127, 'policy_logps/rejected': -386.07830810546875, 'policy_logps/chosen': -366.4801940917969, 'referece_logps/rejected': -367.43988037109375, 'referece_logps/chosen': -358.9715270996094, 'logits/rejected': -0.3459588289260864, 'logits/chosen': -0.36175936460494995, 'epoch': 0.42}


  7%|▋         | 750/10740 [3:45:58<49:42:30, 17.91s/it]

  7%|▋         | 751/10740 [3:46:10<44:29:52, 16.04s/it]
{'loss': 0.5736, 'learning_rate': 1.9916810427455923e-06, 'rewards/chosen': -0.7286925315856934, 'rewards/rejected': -1.05753755569458, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3288450837135315, 'policy_logps/rejected': -333.4537658691406, 'policy_logps/chosen': -377.03643798828125, 'referece_logps/rejected': -322.87835693359375, 'referece_logps/chosen': -369.7495422363281, 'logits/rejected': -0.0803704559803009, 'logits/chosen': -0.1868029534816742, 'epoch': 0.42}


  7%|▋         | 753/10740 [3:46:44<44:50:08, 16.16s/it]
{'loss': 0.6441, 'learning_rate': 1.9916032230704033e-06, 'rewards/chosen': -0.4407346844673157, 'rewards/rejected': -1.2205034494400024, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7797687649726868, 'policy_logps/rejected': -541.5391235351562, 'policy_logps/chosen': -493.7601623535156, 'referece_logps/rejected': -529.3341064453125, 'referece_logps/chosen': -489.352783203125, 'logits/rejected': -1.2948648929595947, 'logits/chosen': -1.3058477640151978, 'epoch': 0.42}


  7%|▋         | 755/10740 [3:47:14<42:31:58, 15.33s/it]
{'loss': 0.5695, 'learning_rate': 1.9915250426402433e-06, 'rewards/chosen': -0.8127851486206055, 'rewards/rejected': -1.4010075330734253, 'rewards/accuracies': 0.625, 'rewards/margins': 0.588222324848175, 'policy_logps/rejected': -332.3619689941406, 'policy_logps/chosen': -357.27978515625, 'referece_logps/rejected': -318.3519287109375, 'referece_logps/chosen': -349.15191650390625, 'logits/rejected': -0.9087343811988831, 'logits/chosen': -0.8238341212272644, 'epoch': 0.42}

  7%|▋         | 756/10740 [3:47:36<47:48:24, 17.24s/it]


  7%|▋         | 758/10740 [3:48:17<52:16:07, 18.85s/it]
{'loss': 0.6169, 'learning_rate': 1.9914070956416856e-06, 'rewards/chosen': -1.1507911682128906, 'rewards/rejected': -1.425691843032837, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2749006152153015, 'policy_logps/rejected': -382.29974365234375, 'policy_logps/chosen': -353.439453125, 'referece_logps/rejected': -368.04278564453125, 'referece_logps/chosen': -341.9315185546875, 'logits/rejected': -1.189140796661377, 'logits/chosen': -1.1938135623931885, 'epoch': 0.42}


  7%|▋         | 760/10740 [3:48:50<50:02:01, 18.05s/it]
{'loss': 0.587, 'learning_rate': 1.9913280134488205e-06, 'rewards/chosen': -0.9687356352806091, 'rewards/rejected': -1.859480619430542, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8907448649406433, 'policy_logps/rejected': -281.0915222167969, 'policy_logps/chosen': -337.9418029785156, 'referece_logps/rejected': -262.4967041015625, 'referece_logps/chosen': -328.2544250488281, 'logits/rejected': -0.552338182926178, 'logits/chosen': -0.596159815788269, 'epoch': 0.42}


  7%|▋         | 762/10740 [3:49:24<47:52:29, 17.27s/it]
{'loss': 0.5703, 'learning_rate': 1.991248570601108e-06, 'rewards/chosen': -0.4123443365097046, 'rewards/rejected': -1.1707501411437988, 'rewards/accuracies': 0.875, 'rewards/margins': 0.758405864238739, 'policy_logps/rejected': -328.96832275390625, 'policy_logps/chosen': -300.46112060546875, 'referece_logps/rejected': -317.26080322265625, 'referece_logps/chosen': -296.3376770019531, 'logits/rejected': -0.7556331157684326, 'logits/chosen': -0.9057707786560059, 'epoch': 0.43}


  7%|▋         | 764/10740 [3:50:00<48:57:11, 17.67s/it]
{'loss': 0.4999, 'learning_rate': 1.9911687671274496e-06, 'rewards/chosen': -0.7878809571266174, 'rewards/rejected': -1.0308634042739868, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24298235774040222, 'policy_logps/rejected': -440.42535400390625, 'policy_logps/chosen': -447.67236328125, 'referece_logps/rejected': -430.11669921875, 'referece_logps/chosen': -439.7934875488281, 'logits/rejected': -0.2785015404224396, 'logits/chosen': -0.34758177399635315, 'epoch': 0.43}


  7%|▋         | 766/10740 [3:50:41<53:01:50, 19.14s/it]
{'loss': 0.479, 'learning_rate': 1.991088603056879e-06, 'rewards/chosen': -0.9214256405830383, 'rewards/rejected': -1.4670898914337158, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5456643104553223, 'policy_logps/rejected': -339.36212158203125, 'policy_logps/chosen': -353.8273010253906, 'referece_logps/rejected': -324.69122314453125, 'referece_logps/chosen': -344.6130676269531, 'logits/rejected': -0.9073432683944702, 'logits/chosen': -0.9074768424034119, 'epoch': 0.43}

  7%|▋         | 767/10740 [3:51:02<54:30:29, 19.68s/it]


  7%|▋         | 769/10740 [3:51:39<52:34:30, 18.98s/it]

  7%|▋         | 770/10740 [3:51:52<48:13:20, 17.41s/it]
{'loss': 0.5127, 'learning_rate': 1.9909271932417895e-06, 'rewards/chosen': -1.024544358253479, 'rewards/rejected': -1.8988933563232422, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8743489384651184, 'policy_logps/rejected': -308.0708312988281, 'policy_logps/chosen': -301.1203918457031, 'referece_logps/rejected': -289.0819091796875, 'referece_logps/chosen': -290.8749694824219, 'logits/rejected': -1.3603360652923584, 'logits/chosen': -1.4465917348861694, 'epoch': 0.43}

  7%|▋         | 771/10740 [3:52:14<51:16:47, 18.52s/it]


  7%|▋         | 773/10740 [3:52:46<47:19:37, 17.09s/it]
{'loss': 0.5396, 'learning_rate': 1.990805189531443e-06, 'rewards/chosen': -0.4991720914840698, 'rewards/rejected': -1.0055123567581177, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5063402056694031, 'policy_logps/rejected': -515.2222290039062, 'policy_logps/chosen': -560.9274291992188, 'referece_logps/rejected': -505.1671142578125, 'referece_logps/chosen': -555.9356689453125, 'logits/rejected': -1.3750591278076172, 'logits/chosen': -1.2927634716033936, 'epoch': 0.43}
[2024-04-01 23:06:50,637] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 775/10740 [3:53:27<51:46:08, 18.70s/it]
[2024-04-01 23:07:10,543] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 776/10740 [3:53:47<52:50:10, 19.09s/it]

  7%|▋         | 777/10740 [3:54:05<51:54:36, 18.76s/it]

  7%|▋         | 778/10740 [3:54:21<49:31:59, 17.90s/it]
{'loss': 0.543, 'learning_rate': 1.9906000477406883e-06, 'rewards/chosen': -1.0587457418441772, 'rewards/rejected': -1.4536579847335815, 'rewards/accuracies': 0.5, 'rewards/margins': 0.39491239190101624, 'policy_logps/rejected': -334.6304016113281, 'policy_logps/chosen': -373.828857421875, 'referece_logps/rejected': -320.09381103515625, 'referece_logps/chosen': -363.2413635253906, 'logits/rejected': -1.0689860582351685, 'logits/chosen': -0.9401084184646606, 'epoch': 0.43}

  7%|▋         | 779/10740 [3:54:41<51:41:15, 18.68s/it]

  7%|▋         | 780/10740 [3:54:58<49:55:20, 18.04s/it]

  7%|▋         | 781/10740 [3:55:17<51:08:41, 18.49s/it]


  7%|▋         | 783/10740 [3:55:49<47:16:15, 17.09s/it]

  7%|▋         | 784/10740 [3:56:00<42:39:50, 15.43s/it]

  7%|▋         | 785/10740 [3:56:11<38:41:50, 13.99s/it]
{'loss': 0.6173, 'learning_rate': 1.9903090652446256e-06, 'rewards/chosen': -0.9061205387115479, 'rewards/rejected': -1.1837373971939087, 'rewards/accuracies': 0.5, 'rewards/margins': 0.27761679887771606, 'policy_logps/rejected': -318.3660888671875, 'policy_logps/chosen': -560.9149169921875, 'referece_logps/rejected': -306.5287170410156, 'referece_logps/chosen': -551.8536987304688, 'logits/rejected': -0.8705552816390991, 'logits/chosen': -1.1058076620101929, 'epoch': 0.44}

  7%|▋         | 786/10740 [3:56:28<41:24:27, 14.98s/it]

  7%|▋         | 787/10740 [3:56:48<45:15:05, 16.37s/it]


  7%|▋         | 789/10740 [3:57:23<48:13:36, 17.45s/it]
{'loss': 0.446, 'learning_rate': 1.990140807886483e-06, 'rewards/chosen': -0.7513568997383118, 'rewards/rejected': -2.010991096496582, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2596341371536255, 'policy_logps/rejected': -388.6600036621094, 'policy_logps/chosen': -445.87408447265625, 'referece_logps/rejected': -368.55010986328125, 'referece_logps/chosen': -438.36053466796875, 'logits/rejected': -1.0809462070465088, 'logits/chosen': -1.1438713073730469, 'epoch': 0.44}


  7%|▋         | 791/10740 [3:58:07<54:44:10, 19.81s/it]

  7%|▋         | 792/10740 [3:58:27<54:59:49, 19.90s/it]
[2024-04-01 23:12:10,643] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4705, 'learning_rate': 1.9900136692692997e-06, 'rewards/chosen': -0.8885911703109741, 'rewards/rejected': -2.054269552230835, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1656785011291504, 'policy_logps/rejected': -394.39508056640625, 'policy_logps/chosen': -331.0911560058594, 'referece_logps/rejected': -373.85235595703125, 'referece_logps/chosen': -322.2052917480469, 'logits/rejected': -0.6976863145828247, 'logits/chosen': -0.7397722005844116, 'epoch': 0.44}

  7%|▋         | 793/10740 [3:58:40<49:35:09, 17.95s/it]

  7%|▋         | 794/10740 [3:59:00<51:00:33, 18.46s/it]
[2024-04-01 23:13:03,851] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 795/10740 [3:59:20<52:24:31, 18.97s/it]


  7%|▋         | 797/10740 [3:59:57<52:25:58, 18.98s/it]
{'loss': 0.5551, 'learning_rate': 1.989799970742374e-06, 'rewards/chosen': -1.5330214500427246, 'rewards/rejected': -2.180263042449951, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6472415924072266, 'policy_logps/rejected': -383.827392578125, 'policy_logps/chosen': -420.3817138671875, 'referece_logps/rejected': -362.0247802734375, 'referece_logps/chosen': -405.051513671875, 'logits/rejected': 0.5241203904151917, 'logits/chosen': 0.5786356925964355, 'epoch': 0.45}

  7%|▋         | 798/10740 [4:00:16<52:28:00, 19.00s/it]

  7%|▋         | 799/10740 [4:00:33<50:24:11, 18.25s/it]

  7%|▋         | 800/10740 [4:00:54<53:26:36, 19.36s/it]

  7%|▋         | 801/10740 [4:01:07<47:28:23, 17.20s/it]


  7%|▋         | 803/10740 [4:01:51<54:36:24, 19.78s/it]

  7%|▋         | 804/10740 [4:02:09<52:55:52, 19.18s/it]
{'loss': 0.5123, 'learning_rate': 1.9894970118763255e-06, 'rewards/chosen': -0.9946249127388, 'rewards/rejected': -2.8189103603363037, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8242852687835693, 'policy_logps/rejected': -376.4490051269531, 'policy_logps/chosen': -390.343505859375, 'referece_logps/rejected': -348.2599182128906, 'referece_logps/chosen': -380.39727783203125, 'logits/rejected': 0.21248182654380798, 'logits/chosen': 0.35161590576171875, 'epoch': 0.45}


  8%|▊         | 806/10740 [4:02:33<43:22:27, 15.72s/it]
{'loss': 0.4724, 'learning_rate': 1.98940964216645e-06, 'rewards/chosen': -1.305989146232605, 'rewards/rejected': -2.2434399127960205, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9374508857727051, 'policy_logps/rejected': -369.9555969238281, 'policy_logps/chosen': -346.9686279296875, 'referece_logps/rejected': -347.521240234375, 'referece_logps/chosen': -333.90875244140625, 'logits/rejected': 0.048563241958618164, 'logits/chosen': -0.07801993191242218, 'epoch': 0.45}

  8%|▊         | 807/10740 [4:02:48<42:08:11, 15.27s/it]

  8%|▊         | 808/10740 [4:03:08<46:22:34, 16.81s/it]

  8%|▊         | 809/10740 [4:03:27<47:53:54, 17.36s/it]

  8%|▊         | 810/10740 [4:03:42<46:13:25, 16.76s/it]

  8%|▊         | 811/10740 [4:04:02<48:35:06, 17.62s/it]

  8%|▊         | 812/10740 [4:04:16<45:38:06, 16.55s/it]
[2024-04-01 23:18:22,409] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 813/10740 [4:04:39<50:55:11, 18.47s/it]

  8%|▊         | 814/10740 [4:04:59<52:12:49, 18.94s/it]

  8%|▊         | 815/10740 [4:05:13<48:01:48, 17.42s/it]

  8%|▊         | 816/10740 [4:05:30<48:14:32, 17.50s/it]
[2024-04-01 23:19:37,674] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 817/10740 [4:05:54<53:19:52, 19.35s/it]

  8%|▊         | 818/10740 [4:06:16<55:24:13, 20.10s/it]


  8%|▊         | 820/10740 [4:06:56<55:26:26, 20.12s/it]
[2024-04-01 23:20:39,270] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4852, 'learning_rate': 1.988787977199921e-06, 'rewards/chosen': -0.6136585474014282, 'rewards/rejected': -2.0454673767089844, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4318087100982666, 'policy_logps/rejected': -539.5341796875, 'policy_logps/chosen': -373.45281982421875, 'referece_logps/rejected': -519.0794677734375, 'referece_logps/chosen': -367.31622314453125, 'logits/rejected': -0.36596623063087463, 'logits/chosen': -0.41666311025619507, 'epoch': 0.46}
[2024-04-01 23:20:56,302] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  8%|▊         | 822/10740 [4:07:34<54:32:18, 19.80s/it]
{'loss': 0.5931, 'learning_rate': 1.988697728735648e-06, 'rewards/chosen': -0.5115324258804321, 'rewards/rejected': -1.20332932472229, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6917970180511475, 'policy_logps/rejected': -306.4830322265625, 'policy_logps/chosen': -289.17901611328125, 'referece_logps/rejected': -294.44970703125, 'referece_logps/chosen': -284.0636901855469, 'logits/rejected': -0.9151578545570374, 'logits/chosen': -0.8665047883987427, 'epoch': 0.46}


  8%|▊         | 824/10740 [4:08:10<51:50:13, 18.82s/it]
{'loss': 0.5825, 'learning_rate': 1.98860712057345e-06, 'rewards/chosen': -0.7824766635894775, 'rewards/rejected': -1.6259713172912598, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8434947729110718, 'policy_logps/rejected': -351.04937744140625, 'policy_logps/chosen': -412.6888122558594, 'referece_logps/rejected': -334.7896728515625, 'referece_logps/chosen': -404.864013671875, 'logits/rejected': -0.4716140925884247, 'logits/chosen': -0.5150966644287109, 'epoch': 0.46}

  8%|▊         | 825/10740 [4:08:22<46:47:18, 16.99s/it]

  8%|▊         | 826/10740 [4:08:36<43:54:34, 15.94s/it]

  8%|▊         | 827/10740 [4:08:57<48:06:47, 17.47s/it]

  8%|▊         | 828/10740 [4:09:17<50:06:15, 18.20s/it]

  8%|▊         | 829/10740 [4:09:33<48:40:53, 17.68s/it]

  8%|▊         | 830/10740 [4:09:44<42:59:46, 15.62s/it]

  8%|▊         | 831/10740 [4:10:05<47:33:11, 17.28s/it]

  8%|▊         | 832/10740 [4:10:18<43:23:09, 15.76s/it]

  8%|▊         | 833/10740 [4:10:37<46:35:23, 16.93s/it]

  8%|▊         | 834/10740 [4:10:56<48:16:25, 17.54s/it]

  8%|▊         | 835/10740 [4:11:17<50:46:51, 18.46s/it]


  8%|▊         | 837/10740 [4:11:48<47:43:21, 17.35s/it]
{'loss': 0.5059, 'learning_rate': 1.988009402171269e-06, 'rewards/chosen': -0.9671274423599243, 'rewards/rejected': -2.528566837310791, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5614395141601562, 'policy_logps/rejected': -461.0865783691406, 'policy_logps/chosen': -418.9056396484375, 'referece_logps/rejected': -435.8009033203125, 'referece_logps/chosen': -409.2344055175781, 'logits/rejected': -1.098597764968872, 'logits/chosen': -1.0636770725250244, 'epoch': 0.47}

  8%|▊         | 838/10740 [4:12:03<46:04:17, 16.75s/it]

  8%|▊         | 839/10740 [4:12:16<42:57:44, 15.62s/it]

  8%|▊         | 840/10740 [4:12:31<42:05:23, 15.31s/it]

  8%|▊         | 841/10740 [4:12:43<39:40:10, 14.43s/it]

  8%|▊         | 842/10740 [4:13:01<42:01:34, 15.29s/it]
[2024-04-01 23:27:02,244] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 843/10740 [4:13:19<44:13:25, 16.09s/it]

  8%|▊         | 844/10740 [4:13:31<40:50:54, 14.86s/it]

  8%|▊         | 845/10740 [4:13:53<47:25:27, 17.25s/it]

  8%|▊         | 846/10740 [4:14:13<49:39:10, 18.07s/it]

  8%|▊         | 847/10740 [4:14:35<52:36:23, 19.14s/it]

  8%|▊         | 848/10740 [4:14:50<49:09:28, 17.89s/it]

  8%|▊         | 849/10740 [4:15:08<49:09:05, 17.89s/it]

  8%|▊         | 850/10740 [4:15:25<48:11:55, 17.54s/it]

  8%|▊         | 851/10740 [4:15:44<50:05:04, 18.23s/it]

  8%|▊         | 852/10740 [4:15:59<47:08:02, 17.16s/it]

  8%|▊         | 853/10740 [4:16:14<45:17:54, 16.49s/it]

  8%|▊         | 854/10740 [4:16:33<47:45:04, 17.39s/it]

  8%|▊         | 855/10740 [4:16:53<49:15:41, 17.94s/it]

  8%|▊         | 856/10740 [4:17:11<49:12:15, 17.92s/it]

  8%|▊         | 857/10740 [4:17:27<47:47:55, 17.41s/it]

  8%|▊         | 858/10740 [4:17:39<43:48:26, 15.96s/it]


  8%|▊         | 860/10740 [4:18:16<47:03:01, 17.14s/it]

  8%|▊         | 861/10740 [4:18:29<43:50:01, 15.97s/it]

  8%|▊         | 862/10740 [4:18:47<45:27:19, 16.57s/it]

  8%|▊         | 863/10740 [4:18:58<40:50:17, 14.88s/it]

  8%|▊         | 864/10740 [4:19:21<47:25:58, 17.29s/it]
[2024-04-01 23:33:04,521] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 865/10740 [4:19:38<47:08:09, 17.18s/it]

  8%|▊         | 866/10740 [4:20:01<51:56:07, 18.94s/it]

  8%|▊         | 867/10740 [4:20:18<50:29:38, 18.41s/it]

  8%|▊         | 868/10740 [4:20:39<52:32:40, 19.16s/it]

  8%|▊         | 869/10740 [4:20:59<53:10:22, 19.39s/it]

  8%|▊         | 870/10740 [4:21:20<54:40:25, 19.94s/it]
[2024-04-01 23:35:03,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 871/10740 [4:21:40<54:47:54, 19.99s/it]

  8%|▊         | 872/10740 [4:21:53<48:57:43, 17.86s/it]
[2024-04-01 23:35:36,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 873/10740 [4:22:14<51:20:08, 18.73s/it]

  8%|▊         | 874/10740 [4:22:35<53:00:23, 19.34s/it]

  8%|▊         | 875/10740 [4:22:49<48:42:03, 17.77s/it]

  8%|▊         | 876/10740 [4:23:08<50:10:21, 18.31s/it]

  8%|▊         | 877/10740 [4:23:29<51:56:52, 18.96s/it]

  8%|▊         | 878/10740 [4:23:51<54:55:01, 20.05s/it]

  8%|▊         | 879/10740 [4:24:11<54:52:40, 20.03s/it]
[2024-04-01 23:37:54,999] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 880/10740 [4:24:24<49:04:17, 17.92s/it]

  8%|▊         | 881/10740 [4:24:39<46:44:17, 17.07s/it]

  8%|▊         | 882/10740 [4:24:52<42:43:50, 15.60s/it]

  8%|▊         | 883/10740 [4:25:13<47:31:13, 17.36s/it]

  8%|▊         | 884/10740 [4:25:27<44:31:22, 16.26s/it]

  8%|▊         | 885/10740 [4:25:41<42:45:25, 15.62s/it]

  8%|▊         | 886/10740 [4:25:54<41:06:17, 15.02s/it]

  8%|▊         | 887/10740 [4:26:05<37:28:06, 13.69s/it]

  8%|▊         | 888/10740 [4:26:21<39:00:48, 14.26s/it]

  8%|▊         | 889/10740 [4:26:38<41:26:34, 15.15s/it]

  8%|▊         | 890/10740 [4:26:51<40:15:11, 14.71s/it]

  8%|▊         | 891/10740 [4:27:09<42:39:42, 15.59s/it]

  8%|▊         | 892/10740 [4:27:27<44:19:02, 16.20s/it]

  8%|▊         | 893/10740 [4:27:47<47:28:38, 17.36s/it]

  8%|▊         | 894/10740 [4:28:07<50:02:57, 18.30s/it]

  8%|▊         | 895/10740 [4:28:23<48:12:33, 17.63s/it]

  8%|▊         | 896/10740 [4:28:43<49:46:04, 18.20s/it]

  8%|▊         | 897/10740 [4:29:00<48:59:01, 17.92s/it]

  8%|▊         | 898/10740 [4:29:18<48:41:06, 17.81s/it]

  8%|▊         | 899/10740 [4:29:39<51:17:21, 18.76s/it]
[2024-04-01 23:43:22,441] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 900/10740 [4:30:00<53:19:10, 19.51s/it]
[2024-04-01 23:43:43,686] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 901/10740 [4:30:21<54:49:44, 20.06s/it]

  8%|▊         | 902/10740 [4:30:37<51:10:59, 18.73s/it]

  8%|▊         | 903/10740 [4:30:55<50:18:24, 18.41s/it]

  8%|▊         | 904/10740 [4:31:14<51:21:05, 18.79s/it]
[2024-04-01 23:44:58,020] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 905/10740 [4:31:36<53:41:45, 19.65s/it]

  8%|▊         | 906/10740 [4:31:54<52:10:07, 19.10s/it]

  8%|▊         | 907/10740 [4:32:10<50:01:48, 18.32s/it]

  8%|▊         | 908/10740 [4:32:25<47:03:30, 17.23s/it]

  8%|▊         | 909/10740 [4:32:37<42:31:51, 15.57s/it]

  8%|▊         | 910/10740 [4:32:49<39:35:16, 14.50s/it]

  8%|▊         | 911/10740 [4:33:02<39:04:14, 14.31s/it]

  8%|▊         | 912/10740 [4:33:22<43:19:18, 15.87s/it]

  9%|▊         | 913/10740 [4:33:43<47:47:00, 17.50s/it]

  9%|▊         | 914/10740 [4:34:00<46:54:33, 17.19s/it]

  9%|▊         | 915/10740 [4:34:19<48:49:07, 17.89s/it]

  9%|▊         | 916/10740 [4:34:39<50:19:41, 18.44s/it]

  9%|▊         | 917/10740 [4:35:01<53:21:58, 19.56s/it]

  9%|▊         | 918/10740 [4:35:23<55:26:31, 20.32s/it]

  9%|▊         | 919/10740 [4:35:43<55:18:09, 20.27s/it]

  9%|▊         | 920/10740 [4:35:56<48:39:32, 17.84s/it]

  9%|▊         | 921/10740 [4:36:13<48:07:46, 17.65s/it]

  9%|▊         | 922/10740 [4:36:34<50:58:50, 18.69s/it]
[2024-04-01 23:50:17,684] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▊         | 923/10740 [4:36:54<52:13:31, 19.15s/it]

  9%|▊         | 924/10740 [4:37:15<53:13:21, 19.52s/it]

  9%|▊         | 925/10740 [4:37:28<48:25:39, 17.76s/it]

  9%|▊         | 926/10740 [4:37:51<52:15:41, 19.17s/it]
{'loss': 0.5581, 'learning_rate': 1.9835099553519654e-06, 'rewards/chosen': -1.210831642150879, 'rewards/rejected': -1.3942443132400513, 'rewards/accuracies': 0.5, 'rewards/margins': 0.18341262638568878, 'policy_logps/rejected': -361.049560546875, 'policy_logps/chosen': -466.0035705566406, 'referece_logps/rejected': -347.1071472167969, 'referece_logps/chosen': -453.895263671875, 'logits/rejected': -0.6345517039299011, 'logits/chosen': -0.6491231918334961, 'epoch': 0.52}


  9%|▊         | 928/10740 [4:38:27<50:18:24, 18.46s/it]

  9%|▊         | 929/10740 [4:38:47<51:42:57, 18.98s/it]

  9%|▊         | 930/10740 [4:39:09<53:49:24, 19.75s/it]

  9%|▊         | 931/10740 [4:39:27<52:17:09, 19.19s/it]
{'loss': 0.5161, 'learning_rate': 1.983236124624002e-06, 'rewards/chosen': -1.5128593444824219, 'rewards/rejected': -2.515007734298706, 'rewards/accuracies': 0.625, 'rewards/margins': 1.002148151397705, 'policy_logps/rejected': -583.0831298828125, 'policy_logps/chosen': -438.1817932128906, 'referece_logps/rejected': -557.93310546875, 'referece_logps/chosen': -423.0531921386719, 'logits/rejected': -0.38743114471435547, 'logits/chosen': -0.38632625341415405, 'epoch': 0.52}


  9%|▊         | 933/10740 [4:40:08<54:15:45, 19.92s/it]

  9%|▊         | 934/10740 [4:40:21<48:46:22, 17.91s/it]

  9%|▊         | 935/10740 [4:40:35<45:43:57, 16.79s/it]

  9%|▊         | 936/10740 [4:40:49<43:37:27, 16.02s/it]
{'loss': 0.5803, 'learning_rate': 1.982960058203024e-06, 'rewards/chosen': -1.2663309574127197, 'rewards/rejected': -1.7814925909042358, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5151616334915161, 'policy_logps/rejected': -237.7643280029297, 'policy_logps/chosen': -302.255126953125, 'referece_logps/rejected': -219.94937133789062, 'referece_logps/chosen': -289.5918273925781, 'logits/rejected': -1.423661708831787, 'logits/chosen': -1.4517021179199219, 'epoch': 0.52}


  9%|▊         | 938/10740 [4:41:22<44:46:56, 16.45s/it]
{'loss': 0.4969, 'learning_rate': 1.982849005781028e-06, 'rewards/chosen': -1.0505506992340088, 'rewards/rejected': -1.6126985549926758, 'rewards/accuracies': 0.625, 'rewards/margins': 0.562147855758667, 'policy_logps/rejected': -318.6569519042969, 'policy_logps/chosen': -370.8629455566406, 'referece_logps/rejected': -302.52996826171875, 'referece_logps/chosen': -360.357421875, 'logits/rejected': -0.9375056624412537, 'logits/chosen': -0.9308673739433289, 'epoch': 0.52}

  9%|▊         | 939/10740 [4:41:36<43:15:24, 15.89s/it]
[2024-04-01 23:55:43,459] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  9%|▉         | 941/10740 [4:42:19<50:11:48, 18.44s/it]
[2024-04-01 23:56:02,634] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 942/10740 [4:42:37<49:55:45, 18.35s/it]

  9%|▉         | 943/10740 [4:42:50<45:23:03, 16.68s/it]
{'loss': 0.4188, 'learning_rate': 1.9825698104455426e-06, 'rewards/chosen': -1.3184077739715576, 'rewards/rejected': -2.216461658477783, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8980541229248047, 'policy_logps/rejected': -487.461181640625, 'policy_logps/chosen': -542.973388671875, 'referece_logps/rejected': -465.2966003417969, 'referece_logps/chosen': -529.789306640625, 'logits/rejected': -0.5713098049163818, 'logits/chosen': -0.1643143594264984, 'epoch': 0.53}
[2024-04-01 23:56:53,796] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  9%|▉         | 945/10740 [4:43:27<47:42:40, 17.54s/it]

  9%|▉         | 946/10740 [4:43:43<46:38:19, 17.14s/it]
[2024-04-01 23:57:27,057] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 947/10740 [4:43:57<44:05:55, 16.21s/it]

  9%|▉         | 948/10740 [4:44:17<46:53:03, 17.24s/it]

  9%|▉         | 949/10740 [4:44:35<47:35:40, 17.50s/it]
{'loss': 0.5502, 'learning_rate': 1.9822318269842996e-06, 'rewards/chosen': -1.2810168266296387, 'rewards/rejected': -2.1851086616516113, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9040918350219727, 'policy_logps/rejected': -465.2646484375, 'policy_logps/chosen': -512.981689453125, 'referece_logps/rejected': -443.41351318359375, 'referece_logps/chosen': -500.1716003417969, 'logits/rejected': -1.0197430849075317, 'logits/chosen': -0.984289288520813, 'epoch': 0.53}
[2024-04-01 23:58:36,214] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  9%|▉         | 951/10740 [4:45:06<44:13:58, 16.27s/it]
{'loss': 0.5296, 'learning_rate': 1.9821184510846408e-06, 'rewards/chosen': -1.0535813570022583, 'rewards/rejected': -1.8846149444580078, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8310335278511047, 'policy_logps/rejected': -245.56443786621094, 'policy_logps/chosen': -265.6506652832031, 'referece_logps/rejected': -226.71824645996094, 'referece_logps/chosen': -255.11485290527344, 'logits/rejected': -1.1951079368591309, 'logits/chosen': -1.3739310503005981, 'epoch': 0.53}

  9%|▉         | 952/10740 [4:45:24<46:01:06, 16.93s/it]


  9%|▉         | 954/10740 [4:45:58<45:10:35, 16.62s/it]
{'loss': 0.4686, 'learning_rate': 1.98194771730248e-06, 'rewards/chosen': -0.38698017597198486, 'rewards/rejected': -0.7572583556175232, 'rewards/accuracies': 0.625, 'rewards/margins': 0.37027817964553833, 'policy_logps/rejected': -499.4496154785156, 'policy_logps/chosen': -465.22625732421875, 'referece_logps/rejected': -491.8770751953125, 'referece_logps/chosen': -461.3564453125, 'logits/rejected': -1.2009819746017456, 'logits/chosen': -0.995908796787262, 'epoch': 0.53}


  9%|▉         | 956/10740 [4:46:34<47:17:15, 17.40s/it]

  9%|▉         | 957/10740 [4:46:51<46:53:16, 17.25s/it]

  9%|▉         | 958/10740 [4:47:07<46:16:14, 17.03s/it]
{'loss': 0.4598, 'learning_rate': 1.9817188219361827e-06, 'rewards/chosen': -0.5511500835418701, 'rewards/rejected': -1.793255090713501, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2421050071716309, 'policy_logps/rejected': -240.58609008789062, 'policy_logps/chosen': -234.09097290039062, 'referece_logps/rejected': -222.6535186767578, 'referece_logps/chosen': -228.5794677734375, 'logits/rejected': -1.8319324254989624, 'logits/chosen': -1.8959851264953613, 'epoch': 0.54}


  9%|▉         | 960/10740 [4:47:48<51:41:45, 19.03s/it]

  9%|▉         | 961/10740 [4:48:10<54:14:37, 19.97s/it]
[2024-04-02 00:01:53,934] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 962/10740 [4:48:33<56:39:00, 20.86s/it]
{'loss': 0.5168, 'learning_rate': 1.981488497934297e-06, 'rewards/chosen': -1.0163441896438599, 'rewards/rejected': -1.5217604637145996, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5054164528846741, 'policy_logps/rejected': -446.03509521484375, 'policy_logps/chosen': -503.0974426269531, 'referece_logps/rejected': -430.8175354003906, 'referece_logps/chosen': -492.93402099609375, 'logits/rejected': -0.5402268767356873, 'logits/chosen': -0.5122390985488892, 'epoch': 0.54}


  9%|▉         | 964/10740 [4:49:15<56:51:40, 20.94s/it]

  9%|▉         | 965/10740 [4:49:36<56:33:16, 20.83s/it]

  9%|▉         | 966/10740 [4:49:54<54:41:31, 20.14s/it]

  9%|▉         | 967/10740 [4:50:09<50:13:41, 18.50s/it]

  9%|▉         | 968/10740 [4:50:28<50:23:20, 18.56s/it]

  9%|▉         | 969/10740 [4:50:48<51:20:15, 18.91s/it]

  9%|▉         | 970/10740 [4:51:00<46:18:02, 17.06s/it]

  9%|▉         | 971/10740 [4:51:19<47:49:06, 17.62s/it]

  9%|▉         | 972/10740 [4:51:33<44:45:36, 16.50s/it]

  9%|▉         | 973/10740 [4:51:54<48:45:30, 17.97s/it]
{'loss': 0.4801, 'learning_rate': 1.9808477432706823e-06, 'rewards/chosen': -1.5419505834579468, 'rewards/rejected': -2.467555522918701, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9256048202514648, 'policy_logps/rejected': -332.4111022949219, 'policy_logps/chosen': -410.01947021484375, 'referece_logps/rejected': -307.7355041503906, 'referece_logps/chosen': -394.5999755859375, 'logits/rejected': -0.54286128282547, 'logits/chosen': -0.6604229807853699, 'epoch': 0.54}


  9%|▉         | 975/10740 [4:52:27<47:22:18, 17.46s/it]

  9%|▉         | 976/10740 [4:52:46<48:15:57, 17.80s/it]

  9%|▉         | 977/10740 [4:52:59<44:52:18, 16.55s/it]

  9%|▉         | 978/10740 [4:53:12<41:46:37, 15.41s/it]

  9%|▉         | 979/10740 [4:53:23<38:25:57, 14.17s/it]

  9%|▉         | 980/10740 [4:53:43<43:16:32, 15.96s/it]

  9%|▉         | 981/10740 [4:54:07<49:02:42, 18.09s/it]

  9%|▉         | 982/10740 [4:54:28<51:51:20, 19.13s/it]
{'loss': 0.515, 'learning_rate': 1.9803154601949528e-06, 'rewards/chosen': -0.48661765456199646, 'rewards/rejected': -1.5833535194396973, 'rewards/accuracies': 0.75, 'rewards/margins': 1.096735954284668, 'policy_logps/rejected': -364.14544677734375, 'policy_logps/chosen': -314.94708251953125, 'referece_logps/rejected': -348.3119201660156, 'referece_logps/chosen': -310.08087158203125, 'logits/rejected': -0.11243152618408203, 'logits/chosen': -0.10693211853504181, 'epoch': 0.55}


  9%|▉         | 984/10740 [4:55:05<49:29:05, 18.26s/it]

  9%|▉         | 985/10740 [4:55:18<45:22:39, 16.75s/it]

  9%|▉         | 986/10740 [4:55:40<50:08:36, 18.51s/it]

  9%|▉         | 987/10740 [4:55:58<49:21:37, 18.22s/it]

  9%|▉         | 988/10740 [4:56:09<43:52:24, 16.20s/it]
{'loss': 0.6252, 'learning_rate': 1.9799565922750275e-06, 'rewards/chosen': -1.2002661228179932, 'rewards/rejected': -1.9192272424697876, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7189611196517944, 'policy_logps/rejected': -352.0517578125, 'policy_logps/chosen': -393.54071044921875, 'referece_logps/rejected': -332.8594970703125, 'referece_logps/chosen': -381.5381164550781, 'logits/rejected': 0.004184424877166748, 'logits/chosen': -0.07222098112106323, 'epoch': 0.55}


  9%|▉         | 990/10740 [4:56:36<39:09:33, 14.46s/it]

  9%|▉         | 991/10740 [4:56:52<40:34:39, 14.98s/it]

  9%|▉         | 992/10740 [4:57:07<40:15:30, 14.87s/it]

  9%|▉         | 993/10740 [4:57:28<45:20:08, 16.74s/it]
{'loss': 0.6253, 'learning_rate': 1.979655084565536e-06, 'rewards/chosen': -1.477033257484436, 'rewards/rejected': -2.1798558235168457, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7028225064277649, 'policy_logps/rejected': -321.0059509277344, 'policy_logps/chosen': -338.4371643066406, 'referece_logps/rejected': -299.2074279785156, 'referece_logps/chosen': -323.6668395996094, 'logits/rejected': -0.7684313654899597, 'logits/chosen': -0.791477620601654, 'epoch': 0.55}


  9%|▉         | 995/10740 [4:58:04<47:58:32, 17.72s/it]
{'loss': 0.5145, 'learning_rate': 1.979533857729064e-06, 'rewards/chosen': -0.7023387551307678, 'rewards/rejected': -1.3658089637756348, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6634703278541565, 'policy_logps/rejected': -428.6924133300781, 'policy_logps/chosen': -389.9722900390625, 'referece_logps/rejected': -415.0343017578125, 'referece_logps/chosen': -382.94891357421875, 'logits/rejected': 0.06697013974189758, 'logits/chosen': -0.0675395280122757, 'epoch': 0.56}

  9%|▉         | 996/10740 [4:58:15<42:51:56, 15.84s/it]

  9%|▉         | 997/10740 [4:58:37<47:56:06, 17.71s/it]


  9%|▉         | 999/10740 [4:59:18<51:52:44, 19.17s/it]

  9%|▉         | 1000/10740 [4:59:39<53:10:48, 19.66s/it]

  9%|▉         | 1001/10740 [5:00:13<64:50:24, 23.97s/it]

  9%|▉         | 1002/10740 [5:00:29<58:20:50, 21.57s/it]

  9%|▉         | 1003/10740 [5:00:40<50:01:46, 18.50s/it]

  9%|▉         | 1004/10740 [5:00:57<48:20:20, 17.87s/it]

  9%|▉         | 1005/10740 [5:01:17<50:08:28, 18.54s/it]

  9%|▉         | 1006/10740 [5:01:39<53:16:49, 19.71s/it]

  9%|▉         | 1007/10740 [5:02:00<54:07:04, 20.02s/it]
{'loss': 0.585, 'learning_rate': 1.9787990146185416e-06, 'rewards/chosen': -1.2441837787628174, 'rewards/rejected': -1.963409423828125, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7192257046699524, 'policy_logps/rejected': -448.8391418457031, 'policy_logps/chosen': -488.2392578125, 'referece_logps/rejected': -429.2050476074219, 'referece_logps/chosen': -475.7973937988281, 'logits/rejected': -0.2929796874523163, 'logits/chosen': -0.3726869523525238, 'epoch': 0.56}


  9%|▉         | 1009/10740 [5:02:41<55:19:33, 20.47s/it]
[2024-04-02 00:16:24,495] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 1010/10740 [5:02:57<51:39:48, 19.11s/it]
[2024-04-02 00:16:40,452] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5686, 'learning_rate': 1.978613300419598e-06, 'rewards/chosen': -0.7341739535331726, 'rewards/rejected': -1.5653018951416016, 'rewards/accuracies': 0.625, 'rewards/margins': 0.831127941608429, 'policy_logps/rejected': -490.52764892578125, 'policy_logps/chosen': -354.22796630859375, 'referece_logps/rejected': -474.8746337890625, 'referece_logps/chosen': -346.8862609863281, 'logits/rejected': -0.7482287287712097, 'logits/chosen': -0.7184011340141296, 'epoch': 0.56}


  9%|▉         | 1012/10740 [5:03:23<43:46:21, 16.20s/it]

  9%|▉         | 1013/10740 [5:03:39<43:57:23, 16.27s/it]
{'loss': 0.5558, 'learning_rate': 1.9784267851551862e-06, 'rewards/chosen': -0.5367255210876465, 'rewards/rejected': -1.3764303922653198, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8397048711776733, 'policy_logps/rejected': -376.5299987792969, 'policy_logps/chosen': -452.6642761230469, 'referece_logps/rejected': -362.76568603515625, 'referece_logps/chosen': -447.29705810546875, 'logits/rejected': -0.686598539352417, 'logits/chosen': -0.6515169143676758, 'epoch': 0.57}

  9%|▉         | 1014/10740 [5:03:54<42:35:52, 15.77s/it]
[2024-04-02 00:17:57,652] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 1015/10740 [5:04:14<46:08:15, 17.08s/it]

  9%|▉         | 1016/10740 [5:04:32<46:55:26, 17.37s/it]

  9%|▉         | 1017/10740 [5:04:49<46:51:10, 17.35s/it]
{'loss': 0.5713, 'learning_rate': 1.97817685229811e-06, 'rewards/chosen': -0.8260588645935059, 'rewards/rejected': -1.7533379793167114, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9272791147232056, 'policy_logps/rejected': -262.98309326171875, 'policy_logps/chosen': -317.48468017578125, 'referece_logps/rejected': -245.44970703125, 'referece_logps/chosen': -309.2240905761719, 'logits/rejected': -1.251471757888794, 'logits/chosen': -1.2924610376358032, 'epoch': 0.57}


  9%|▉         | 1019/10740 [5:05:21<43:46:29, 16.21s/it]

  9%|▉         | 1020/10740 [5:05:41<46:41:10, 17.29s/it]
{'loss': 0.5841, 'learning_rate': 1.977988468475813e-06, 'rewards/chosen': -0.7706140279769897, 'rewards/rejected': -1.7796669006347656, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0090528726577759, 'policy_logps/rejected': -393.56719970703125, 'policy_logps/chosen': -372.8218994140625, 'referece_logps/rejected': -375.7705078125, 'referece_logps/chosen': -365.11578369140625, 'logits/rejected': -0.34189003705978394, 'logits/chosen': -0.4808058738708496, 'epoch': 0.57}


 10%|▉         | 1022/10740 [5:06:19<48:39:53, 18.03s/it]
{'loss': 0.6686, 'learning_rate': 1.977862434499181e-06, 'rewards/chosen': -1.5385456085205078, 'rewards/rejected': -1.9586262702941895, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4200805425643921, 'policy_logps/rejected': -338.9963684082031, 'policy_logps/chosen': -305.7982482910156, 'referece_logps/rejected': -319.4101257324219, 'referece_logps/chosen': -290.41278076171875, 'logits/rejected': -0.2472815215587616, 'logits/chosen': -0.31483474373817444, 'epoch': 0.57}

 10%|▉         | 1023/10740 [5:06:38<49:22:31, 18.29s/it]


 10%|▉         | 1025/10740 [5:07:11<46:29:54, 17.23s/it]
{'loss': 0.5765, 'learning_rate': 1.9776727165062134e-06, 'rewards/chosen': -0.24063968658447266, 'rewards/rejected': -1.2756036520004272, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0349639654159546, 'policy_logps/rejected': -422.9850769042969, 'policy_logps/chosen': -380.9491271972656, 'referece_logps/rejected': -410.2290344238281, 'referece_logps/chosen': -378.542724609375, 'logits/rejected': -0.0568513497710228, 'logits/chosen': -0.20254075527191162, 'epoch': 0.57}


 10%|▉         | 1027/10740 [5:07:48<48:12:31, 17.87s/it]

 10%|▉         | 1028/10740 [5:08:09<50:47:37, 18.83s/it]

 10%|▉         | 1029/10740 [5:08:31<53:06:39, 19.69s/it]

 10%|▉         | 1030/10740 [5:08:53<54:34:26, 20.23s/it]
{'loss': 0.506, 'learning_rate': 1.9773547414937376e-06, 'rewards/chosen': -0.5148677229881287, 'rewards/rejected': -1.2788591384887695, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7639913558959961, 'policy_logps/rejected': -435.70965576171875, 'policy_logps/chosen': -446.40435791015625, 'referece_logps/rejected': -422.9210205078125, 'referece_logps/chosen': -441.2556457519531, 'logits/rejected': -1.0419138669967651, 'logits/chosen': -1.0483709573745728, 'epoch': 0.58}

 10%|▉         | 1031/10740 [5:09:16<56:46:47, 21.05s/it]


 10%|▉         | 1033/10740 [5:09:51<52:44:01, 19.56s/it]

 10%|▉         | 1034/10740 [5:10:09<52:02:31, 19.30s/it]

 10%|▉         | 1035/10740 [5:10:29<51:56:05, 19.26s/it]

 10%|▉         | 1036/10740 [5:10:48<51:44:26, 19.19s/it]
{'loss': 0.448, 'learning_rate': 1.9769702380805305e-06, 'rewards/chosen': -0.7725991010665894, 'rewards/rejected': -1.5490188598632812, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7764197587966919, 'policy_logps/rejected': -420.24395751953125, 'policy_logps/chosen': -435.02032470703125, 'referece_logps/rejected': -404.7537841796875, 'referece_logps/chosen': -427.29437255859375, 'logits/rejected': -1.3322027921676636, 'logits/chosen': -1.2907938957214355, 'epoch': 0.58}


 10%|▉         | 1038/10740 [5:11:27<52:39:48, 19.54s/it]
[2024-04-02 00:25:10,865] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4378, 'learning_rate': 1.976841359351131e-06, 'rewards/chosen': -0.30600136518478394, 'rewards/rejected': -1.6064754724502563, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3004741668701172, 'policy_logps/rejected': -442.97576904296875, 'policy_logps/chosen': -402.48095703125, 'referece_logps/rejected': -426.9110107421875, 'referece_logps/chosen': -399.42095947265625, 'logits/rejected': -0.9067190289497375, 'logits/chosen': -0.9536147117614746, 'epoch': 0.58}

 10%|▉         | 1039/10740 [5:11:50<55:44:35, 20.69s/it]

 10%|▉         | 1040/10740 [5:12:12<56:29:42, 20.97s/it]


 10%|▉         | 1042/10740 [5:12:45<50:16:00, 18.66s/it]
{'loss': 0.4485, 'learning_rate': 1.9765825357859645e-06, 'rewards/chosen': -0.020295238122344017, 'rewards/rejected': -1.143752932548523, 'rewards/accuracies': 0.875, 'rewards/margins': 1.123457670211792, 'policy_logps/rejected': -363.6830139160156, 'policy_logps/chosen': -312.5843505859375, 'referece_logps/rejected': -352.2454833984375, 'referece_logps/chosen': -312.38134765625, 'logits/rejected': -0.3832252621650696, 'logits/chosen': -0.4785788059234619, 'epoch': 0.58}

 10%|▉         | 1043/10740 [5:13:05<51:02:19, 18.95s/it]

 10%|▉         | 1044/10740 [5:13:25<51:52:32, 19.26s/it]

 10%|▉         | 1045/10740 [5:13:40<49:01:33, 18.20s/it]

 10%|▉         | 1046/10740 [5:14:01<51:03:42, 18.96s/it]

 10%|▉         | 1047/10740 [5:14:21<51:34:43, 19.16s/it]


 10%|▉         | 1049/10740 [5:14:58<50:33:19, 18.78s/it]
{'loss': 0.5295, 'learning_rate': 1.976126175105858e-06, 'rewards/chosen': -0.6179359555244446, 'rewards/rejected': -1.2724248170852661, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6544888615608215, 'policy_logps/rejected': -379.49273681640625, 'policy_logps/chosen': -318.05389404296875, 'referece_logps/rejected': -366.76849365234375, 'referece_logps/chosen': -311.8745422363281, 'logits/rejected': 0.23644669353961945, 'logits/chosen': 0.26588791608810425, 'epoch': 0.59}

 10%|▉         | 1050/10740 [5:15:12<47:06:29, 17.50s/it]

 10%|▉         | 1051/10740 [5:15:30<47:24:01, 17.61s/it]

 10%|▉         | 1052/10740 [5:15:53<51:06:06, 18.99s/it]

 10%|▉         | 1053/10740 [5:16:14<53:25:28, 19.85s/it]

 10%|▉         | 1054/10740 [5:16:33<51:58:23, 19.32s/it]

 10%|▉         | 1055/10740 [5:16:49<49:29:22, 18.40s/it]

 10%|▉         | 1056/10740 [5:17:04<47:07:35, 17.52s/it]

 10%|▉         | 1057/10740 [5:17:20<45:57:29, 17.09s/it]


 10%|▉         | 1059/10740 [5:17:58<47:47:08, 17.77s/it]

 10%|▉         | 1060/10740 [5:18:20<51:16:26, 19.07s/it]
{'loss': 0.59, 'learning_rate': 1.975400248360947e-06, 'rewards/chosen': -1.2001914978027344, 'rewards/rejected': -3.3320376873016357, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1318459510803223, 'policy_logps/rejected': -341.99017333984375, 'policy_logps/chosen': -376.2961730957031, 'referece_logps/rejected': -308.6697998046875, 'referece_logps/chosen': -364.2942810058594, 'logits/rejected': -0.6327205300331116, 'logits/chosen': -0.6557490229606628, 'epoch': 0.59}

 10%|▉         | 1061/10740 [5:18:42<54:06:53, 20.13s/it]


 10%|▉         | 1063/10740 [5:19:24<54:57:57, 20.45s/it]
{'loss': 0.6298, 'learning_rate': 1.9752004049864126e-06, 'rewards/chosen': -0.9279670119285583, 'rewards/rejected': -1.2080684900283813, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2801014482975006, 'policy_logps/rejected': -424.61279296875, 'policy_logps/chosen': -386.3935852050781, 'referece_logps/rejected': -412.5321044921875, 'referece_logps/chosen': -377.1138916015625, 'logits/rejected': -0.45703446865081787, 'logits/chosen': -0.4329732656478882, 'epoch': 0.59}


 10%|▉         | 1065/10740 [5:20:00<50:38:29, 18.84s/it]
{'loss': 0.4892, 'learning_rate': 1.9750667325756165e-06, 'rewards/chosen': -0.455405592918396, 'rewards/rejected': -1.9673206806182861, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5119149684906006, 'policy_logps/rejected': -326.9495544433594, 'policy_logps/chosen': -271.77142333984375, 'referece_logps/rejected': -307.2763366699219, 'referece_logps/chosen': -267.2173767089844, 'logits/rejected': -1.6230093240737915, 'logits/chosen': -1.7276264429092407, 'epoch': 0.59}

 10%|▉         | 1066/10740 [5:20:23<54:00:13, 20.10s/it]

 10%|▉         | 1067/10740 [5:20:37<49:23:46, 18.38s/it]


 10%|▉         | 1069/10740 [5:21:12<48:23:33, 18.01s/it]
{'loss': 0.5434, 'learning_rate': 1.9747983235862816e-06, 'rewards/chosen': -0.7493615746498108, 'rewards/rejected': -1.5257000923156738, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7763384580612183, 'policy_logps/rejected': -559.6129150390625, 'policy_logps/chosen': -283.5893249511719, 'referece_logps/rejected': -544.35595703125, 'referece_logps/chosen': -276.0957336425781, 'logits/rejected': -0.33978915214538574, 'logits/chosen': -0.08716040104627609, 'epoch': 0.6}


 10%|▉         | 1071/10740 [5:21:54<52:30:33, 19.55s/it]
[2024-04-02 00:35:37,446] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4965, 'learning_rate': 1.974663587105392e-06, 'rewards/chosen': -1.5310325622558594, 'rewards/rejected': -2.5946950912475586, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0636624097824097, 'policy_logps/rejected': -560.7536010742188, 'policy_logps/chosen': -557.524658203125, 'referece_logps/rejected': -534.8067016601562, 'referece_logps/chosen': -542.21435546875, 'logits/rejected': -0.5662883520126343, 'logits/chosen': -0.6084468364715576, 'epoch': 0.6}
[2024-04-02 00:35:58,886] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|▉         | 1072/10740 [5:22:15<54:01:33, 20.12s/it]
[2024-04-02 00:36:10,362] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 10%|█         | 1074/10740 [5:22:46<48:45:40, 18.16s/it]

 10%|█         | 1075/10740 [5:23:04<48:27:49, 18.05s/it]

 10%|█         | 1076/10740 [5:23:20<46:47:55, 17.43s/it]

 10%|█         | 1077/10740 [5:23:32<42:42:25, 15.91s/it]
{'loss': 0.5627, 'learning_rate': 1.9742572503064477e-06, 'rewards/chosen': -1.0596694946289062, 'rewards/rejected': -2.113715410232544, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0540459156036377, 'policy_logps/rejected': -399.7303161621094, 'policy_logps/chosen': -370.3936462402344, 'referece_logps/rejected': -378.5931701660156, 'referece_logps/chosen': -359.7969665527344, 'logits/rejected': -0.341268390417099, 'logits/chosen': -0.33768153190612793, 'epoch': 0.6}


 10%|█         | 1079/10740 [5:24:10<46:13:14, 17.22s/it]
{'loss': 0.669, 'learning_rate': 1.974121095752295e-06, 'rewards/chosen': -1.2222774028778076, 'rewards/rejected': -0.9076132774353027, 'rewards/accuracies': 0.25, 'rewards/margins': -0.31466418504714966, 'policy_logps/rejected': -357.50628662109375, 'policy_logps/chosen': -395.25213623046875, 'referece_logps/rejected': -348.43017578125, 'referece_logps/chosen': -383.0293884277344, 'logits/rejected': 0.23958832025527954, 'logits/chosen': 0.1579425185918808, 'epoch': 0.6}


 10%|█         | 1081/10740 [5:24:46<48:34:38, 18.11s/it]
[2024-04-02 00:38:30,114] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|█         | 1082/10740 [5:25:06<50:05:28, 18.67s/it]
{'loss': 0.4254, 'learning_rate': 1.9739161994463265e-06, 'rewards/chosen': -0.972790002822876, 'rewards/rejected': -1.8075463771820068, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8347563743591309, 'policy_logps/rejected': -277.93304443359375, 'policy_logps/chosen': -371.79730224609375, 'referece_logps/rejected': -259.8575744628906, 'referece_logps/chosen': -362.0694274902344, 'logits/rejected': -0.9118509292602539, 'logits/chosen': -1.1988096237182617, 'epoch': 0.6}


 10%|█         | 1084/10740 [5:25:39<45:47:51, 17.07s/it]

 10%|█         | 1085/10740 [5:26:00<49:38:55, 18.51s/it]
{'loss': 0.5709, 'learning_rate': 1.973710505919805e-06, 'rewards/chosen': -1.1351161003112793, 'rewards/rejected': -1.5214945077896118, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3863784670829773, 'policy_logps/rejected': -471.6911926269531, 'policy_logps/chosen': -520.92041015625, 'referece_logps/rejected': -456.4762268066406, 'referece_logps/chosen': -509.56927490234375, 'logits/rejected': -0.41575536131858826, 'logits/chosen': -0.4077475666999817, 'epoch': 0.61}

 10%|█         | 1086/10740 [5:26:21<51:10:15, 19.08s/it]


 10%|█         | 1088/10740 [5:26:50<46:01:24, 17.17s/it]
{'loss': 0.5077, 'learning_rate': 1.9735040153411058e-06, 'rewards/chosen': -0.717728316783905, 'rewards/rejected': -1.6004788875579834, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8827506303787231, 'policy_logps/rejected': -370.8050842285156, 'policy_logps/chosen': -313.8405456542969, 'referece_logps/rejected': -354.80029296875, 'referece_logps/chosen': -306.66326904296875, 'logits/rejected': -0.32309800386428833, 'logits/chosen': -0.32874825596809387, 'epoch': 0.61}

 10%|█         | 1089/10740 [5:27:04<43:21:47, 16.18s/it]


 10%|█         | 1091/10740 [5:27:41<45:44:59, 17.07s/it]
{'loss': 0.5329, 'learning_rate': 1.973296727879256e-06, 'rewards/chosen': -2.289904832839966, 'rewards/rejected': -3.139991521835327, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8500865697860718, 'policy_logps/rejected': -492.1994934082031, 'policy_logps/chosen': -427.9179992675781, 'referece_logps/rejected': -460.7995910644531, 'referece_logps/chosen': -405.0189208984375, 'logits/rejected': -0.19515538215637207, 'logits/chosen': -0.24678918719291687, 'epoch': 0.61}

 10%|█         | 1092/10740 [5:27:54<42:23:26, 15.82s/it]

 10%|█         | 1093/10740 [5:28:15<46:56:56, 17.52s/it]

 10%|█         | 1094/10740 [5:28:36<49:38:19, 18.53s/it]


 10%|█         | 1096/10740 [5:29:10<48:38:56, 18.16s/it]

 10%|█         | 1097/10740 [5:29:23<44:17:37, 16.54s/it]
{'loss': 0.5173, 'learning_rate': 1.972879762985477e-06, 'rewards/chosen': -0.6590176820755005, 'rewards/rejected': -1.864000916481018, 'rewards/accuracies': 0.75, 'rewards/margins': 1.204983115196228, 'policy_logps/rejected': -321.3091125488281, 'policy_logps/chosen': -370.4858093261719, 'referece_logps/rejected': -302.66912841796875, 'referece_logps/chosen': -363.8956604003906, 'logits/rejected': -1.0286860466003418, 'logits/chosen': -1.0703251361846924, 'epoch': 0.61}

 10%|█         | 1098/10740 [5:29:45<48:45:01, 18.20s/it]

 10%|█         | 1099/10740 [5:30:04<49:28:07, 18.47s/it]

 10%|█         | 1100/10740 [5:30:24<50:21:13, 18.80s/it]


 10%|█         | 1102/10740 [5:30:59<49:04:07, 18.33s/it]

 10%|█         | 1103/10740 [5:31:15<46:53:20, 17.52s/it]

 10%|█         | 1104/10740 [5:31:34<48:42:39, 18.20s/it]

 10%|█         | 1105/10740 [5:31:53<49:06:18, 18.35s/it]

 10%|█         | 1106/10740 [5:32:10<48:12:28, 18.01s/it]

 10%|█         | 1107/10740 [5:32:31<50:10:33, 18.75s/it]

 10%|█         | 1108/10740 [5:32:44<46:09:06, 17.25s/it]

 10%|█         | 1109/10740 [5:33:01<45:30:46, 17.01s/it]
{'loss': 0.5046, 'learning_rate': 1.9720362781097073e-06, 'rewards/chosen': -0.8947940468788147, 'rewards/rejected': -1.8209501504898071, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9261560440063477, 'policy_logps/rejected': -401.4270935058594, 'policy_logps/chosen': -418.0035095214844, 'referece_logps/rejected': -383.2175598144531, 'referece_logps/chosen': -409.0555725097656, 'logits/rejected': -0.19656315445899963, 'logits/chosen': -0.20236679911613464, 'epoch': 0.62}

 10%|█         | 1110/10740 [5:33:14<42:16:28, 15.80s/it]

 10%|█         | 1111/10740 [5:33:33<45:09:13, 16.88s/it]

 10%|█         | 1112/10740 [5:33:50<44:35:56, 16.68s/it]

 10%|█         | 1113/10740 [5:34:04<42:53:40, 16.04s/it]

 10%|█         | 1114/10740 [5:34:22<44:26:13, 16.62s/it]

 10%|█         | 1115/10740 [5:34:35<41:49:58, 15.65s/it]

 10%|█         | 1116/10740 [5:34:56<45:32:31, 17.04s/it]

 10%|█         | 1117/10740 [5:35:09<42:53:55, 16.05s/it]

 10%|█         | 1118/10740 [5:35:21<39:36:55, 14.82s/it]


 10%|█         | 1120/10740 [5:35:53<41:13:34, 15.43s/it]
{'loss': 0.6221, 'learning_rate': 1.97125189963079e-06, 'rewards/chosen': -1.4641555547714233, 'rewards/rejected': -1.650073766708374, 'rewards/accuracies': 0.625, 'rewards/margins': 0.18591804802417755, 'policy_logps/rejected': -219.1496124267578, 'policy_logps/chosen': -254.7145233154297, 'referece_logps/rejected': -202.64886474609375, 'referece_logps/chosen': -240.07298278808594, 'logits/rejected': -1.596790075302124, 'logits/chosen': -1.7109215259552002, 'epoch': 0.63}


 10%|█         | 1122/10740 [5:36:29<44:56:34, 16.82s/it]
{'loss': 0.5181, 'learning_rate': 1.9711081367174745e-06, 'rewards/chosen': -1.1575747728347778, 'rewards/rejected': -1.6128214597702026, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4552465081214905, 'policy_logps/rejected': -531.6434326171875, 'policy_logps/chosen': -360.8147888183594, 'referece_logps/rejected': -515.5152587890625, 'referece_logps/chosen': -349.2390441894531, 'logits/rejected': -0.504738986492157, 'logits/chosen': -0.5172369480133057, 'epoch': 0.63}


 10%|█         | 1124/10740 [5:37:03<46:29:28, 17.41s/it]
{'loss': 0.5711, 'learning_rate': 1.9709640205055006e-06, 'rewards/chosen': -1.055349588394165, 'rewards/rejected': -1.3630762100219727, 'rewards/accuracies': 0.625, 'rewards/margins': 0.30772650241851807, 'policy_logps/rejected': -551.6599731445312, 'policy_logps/chosen': -420.3834228515625, 'referece_logps/rejected': -538.0292358398438, 'referece_logps/chosen': -409.8299255371094, 'logits/rejected': 0.09481817483901978, 'logits/chosen': 0.28400757908821106, 'epoch': 0.63}


 10%|█         | 1126/10740 [5:37:45<50:59:23, 19.09s/it]

 10%|█         | 1127/10740 [5:38:03<49:49:48, 18.66s/it]

 11%|█         | 1128/10740 [5:38:25<52:44:29, 19.75s/it]
{'loss': 0.4215, 'learning_rate': 1.9706747283954304e-06, 'rewards/chosen': -0.7146385908126831, 'rewards/rejected': -1.9873826503753662, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2727439403533936, 'policy_logps/rejected': -365.4151916503906, 'policy_logps/chosen': -301.9053039550781, 'referece_logps/rejected': -345.54132080078125, 'referece_logps/chosen': -294.7589111328125, 'logits/rejected': -0.6655778884887695, 'logits/chosen': -0.8197158575057983, 'epoch': 0.63}


 11%|█         | 1130/10740 [5:38:51<43:48:58, 16.41s/it]
{'loss': 0.5569, 'learning_rate': 1.970529552602581e-06, 'rewards/chosen': -0.5812565684318542, 'rewards/rejected': -1.687307357788086, 'rewards/accuracies': 0.875, 'rewards/margins': 1.106050729751587, 'policy_logps/rejected': -350.4548034667969, 'policy_logps/chosen': -333.1986083984375, 'referece_logps/rejected': -333.5817565917969, 'referece_logps/chosen': -327.3860168457031, 'logits/rejected': -1.0328694581985474, 'logits/chosen': -0.9688943028450012, 'epoch': 0.63}


 11%|█         | 1132/10740 [5:39:29<47:19:23, 17.73s/it]

 11%|█         | 1133/10740 [5:39:47<47:46:52, 17.90s/it]

 11%|█         | 1134/10740 [5:40:03<45:56:14, 17.22s/it]
{'loss': 0.5444, 'learning_rate': 1.9702381418053356e-06, 'rewards/chosen': -1.42427396774292, 'rewards/rejected': -1.5855721235275269, 'rewards/accuracies': 0.625, 'rewards/margins': 0.1612982302904129, 'policy_logps/rejected': -434.4162902832031, 'policy_logps/chosen': -438.4302062988281, 'referece_logps/rejected': -418.5605773925781, 'referece_logps/chosen': -424.18743896484375, 'logits/rejected': 0.06438057124614716, 'logits/chosen': 0.10793117433786392, 'epoch': 0.63}

 11%|█         | 1135/10740 [5:40:21<46:36:38, 17.47s/it]


 11%|█         | 1137/10740 [5:40:54<45:46:39, 17.16s/it]
{'loss': 0.5639, 'learning_rate': 1.9700186571060845e-06, 'rewards/chosen': -1.8504983186721802, 'rewards/rejected': -2.4568004608154297, 'rewards/accuracies': 0.75, 'rewards/margins': 0.60630202293396, 'policy_logps/rejected': -386.8907165527344, 'policy_logps/chosen': -485.950927734375, 'referece_logps/rejected': -362.32275390625, 'referece_logps/chosen': -467.4459228515625, 'logits/rejected': -0.29618674516677856, 'logits/chosen': -0.3989361524581909, 'epoch': 0.64}

 11%|█         | 1138/10740 [5:41:09<44:24:31, 16.65s/it]

 11%|█         | 1139/10740 [5:41:27<45:09:33, 16.93s/it]


 11%|█         | 1141/10740 [5:42:10<50:49:29, 19.06s/it]

 11%|█         | 1142/10740 [5:42:34<54:50:51, 20.57s/it]
{'loss': 0.4921, 'learning_rate': 1.969651084851609e-06, 'rewards/chosen': -0.9501854181289673, 'rewards/rejected': -1.6911784410476685, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7409931421279907, 'policy_logps/rejected': -548.1991577148438, 'policy_logps/chosen': -425.7191467285156, 'referece_logps/rejected': -531.287353515625, 'referece_logps/chosen': -416.21728515625, 'logits/rejected': -1.6129990816116333, 'logits/chosen': -1.6226909160614014, 'epoch': 0.64}

 11%|█         | 1143/10740 [5:42:55<55:27:13, 20.80s/it]

 11%|█         | 1144/10740 [5:43:13<53:22:14, 20.02s/it]

 11%|█         | 1145/10740 [5:43:26<47:28:48, 17.81s/it]

 11%|█         | 1146/10740 [5:43:39<43:23:23, 16.28s/it]

 11%|█         | 1147/10740 [5:43:57<44:48:48, 16.82s/it]

 11%|█         | 1148/10740 [5:44:18<48:17:25, 18.12s/it]

 11%|█         | 1149/10740 [5:44:40<51:05:58, 19.18s/it]

 11%|█         | 1150/10740 [5:44:55<48:04:44, 18.05s/it]

 11%|█         | 1151/10740 [5:45:15<50:00:15, 18.77s/it]
[2024-04-02 00:59:21,297] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 11%|█         | 1152/10740 [5:45:38<52:43:27, 19.80s/it]

 11%|█         | 1153/10740 [5:45:59<53:39:47, 20.15s/it]
[2024-04-02 01:00:01,921] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 11%|█         | 1155/10740 [5:46:38<52:57:58, 19.89s/it]
{'loss': 0.5211, 'learning_rate': 1.968685080611503e-06, 'rewards/chosen': -0.5460695028305054, 'rewards/rejected': -1.1205648183822632, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5744953155517578, 'policy_logps/rejected': -410.99267578125, 'policy_logps/chosen': -424.1706848144531, 'referece_logps/rejected': -399.7870178222656, 'referece_logps/chosen': -418.7099609375, 'logits/rejected': -0.647303581237793, 'logits/chosen': -0.8852900862693787, 'epoch': 0.65}
[2024-04-02 01:00:41,342] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 11%|█         | 1157/10740 [5:47:20<54:48:25, 20.59s/it]
{'loss': 0.5115, 'learning_rate': 1.968535142637188e-06, 'rewards/chosen': -0.8544283509254456, 'rewards/rejected': -1.7478734254837036, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8934451341629028, 'policy_logps/rejected': -458.8125, 'policy_logps/chosen': -409.98663330078125, 'referece_logps/rejected': -441.333740234375, 'referece_logps/chosen': -401.4423828125, 'logits/rejected': -0.3595399856567383, 'logits/chosen': -0.38795384764671326, 'epoch': 0.65}

 11%|█         | 1158/10740 [5:47:36<51:28:55, 19.34s/it]


 11%|█         | 1160/10740 [5:48:12<48:35:00, 18.26s/it]
{'loss': 0.4387, 'learning_rate': 1.968309575012963e-06, 'rewards/chosen': -1.0343098640441895, 'rewards/rejected': -2.0372767448425293, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0029667615890503, 'policy_logps/rejected': -360.39239501953125, 'policy_logps/chosen': -335.76702880859375, 'referece_logps/rejected': -340.01959228515625, 'referece_logps/chosen': -325.42388916015625, 'logits/rejected': -1.2816654443740845, 'logits/chosen': -1.4012973308563232, 'epoch': 0.65}

 11%|█         | 1161/10740 [5:48:28<47:03:31, 17.69s/it]

 11%|█         | 1162/10740 [5:48:48<48:32:19, 18.24s/it]

 11%|█         | 1163/10740 [5:49:05<48:01:02, 18.05s/it]

 11%|█         | 1164/10740 [5:49:22<47:03:28, 17.69s/it]


 11%|█         | 1166/10740 [5:49:54<44:43:50, 16.82s/it]
{'loss': 0.4951, 'learning_rate': 1.9678560620564246e-06, 'rewards/chosen': -0.4435560703277588, 'rewards/rejected': -1.4766591787338257, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0331032276153564, 'policy_logps/rejected': -447.701904296875, 'policy_logps/chosen': -504.4759216308594, 'referece_logps/rejected': -432.9353332519531, 'referece_logps/chosen': -500.04034423828125, 'logits/rejected': 0.22612497210502625, 'logits/chosen': 0.04595386981964111, 'epoch': 0.65}
[2024-04-02 01:03:57,258] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 11%|█         | 1167/10740 [5:50:14<46:56:55, 17.66s/it]

 11%|█         | 1168/10740 [5:50:32<47:16:38, 17.78s/it]

 11%|█         | 1169/10740 [5:50:47<45:38:24, 17.17s/it]


 11%|█         | 1171/10740 [5:51:22<46:30:17, 17.50s/it]
{'loss': 0.4595, 'learning_rate': 1.967475713735688e-06, 'rewards/chosen': -0.6776712536811829, 'rewards/rejected': -1.7227190732955933, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0450479984283447, 'policy_logps/rejected': -491.3859558105469, 'policy_logps/chosen': -355.4387512207031, 'referece_logps/rejected': -474.1587829589844, 'referece_logps/chosen': -348.66204833984375, 'logits/rejected': 0.48773282766342163, 'logits/chosen': 0.4844236671924591, 'epoch': 0.65}

 11%|█         | 1172/10740 [5:51:41<47:26:08, 17.85s/it]


 11%|█         | 1174/10740 [5:52:16<47:12:49, 17.77s/it]
{'loss': 0.5403, 'learning_rate': 1.9672464487564147e-06, 'rewards/chosen': -0.7287468910217285, 'rewards/rejected': -1.2237520217895508, 'rewards/accuracies': 0.625, 'rewards/margins': 0.49500519037246704, 'policy_logps/rejected': -476.57574462890625, 'policy_logps/chosen': -469.1471252441406, 'referece_logps/rejected': -464.3382263183594, 'referece_logps/chosen': -461.8596496582031, 'logits/rejected': -0.32533836364746094, 'logits/chosen': -0.4231925904750824, 'epoch': 0.66}

 11%|█         | 1175/10740 [5:52:35<47:42:37, 17.96s/it]


 11%|█         | 1177/10740 [5:53:00<40:38:55, 15.30s/it]
{'loss': 0.5711, 'learning_rate': 1.9670163920162594e-06, 'rewards/chosen': -0.9555677175521851, 'rewards/rejected': -2.2347943782806396, 'rewards/accuracies': 0.5, 'rewards/margins': 1.279226541519165, 'policy_logps/rejected': -326.35699462890625, 'policy_logps/chosen': -313.0262145996094, 'referece_logps/rejected': -304.009033203125, 'referece_logps/chosen': -303.47052001953125, 'logits/rejected': -0.8371682167053223, 'logits/chosen': -0.9247832298278809, 'epoch': 0.66}

 11%|█         | 1178/10740 [5:53:14<39:53:03, 15.02s/it]

 11%|█         | 1179/10740 [5:53:29<39:36:17, 14.91s/it]

 11%|█         | 1180/10740 [5:53:46<40:46:05, 15.35s/it]

 11%|█         | 1181/10740 [5:54:03<42:04:27, 15.85s/it]

 11%|█         | 1182/10740 [5:54:18<41:38:15, 15.68s/it]

 11%|█         | 1183/10740 [5:54:37<44:13:29, 16.66s/it]

 11%|█         | 1184/10740 [5:54:54<44:27:47, 16.75s/it]

 11%|█         | 1185/10740 [5:55:13<46:38:45, 17.57s/it]

 11%|█         | 1186/10740 [5:55:30<45:43:27, 17.23s/it]

 11%|█         | 1187/10740 [5:55:48<46:26:01, 17.50s/it]

 11%|█         | 1188/10740 [5:56:07<47:33:07, 17.92s/it]

 11%|█         | 1189/10740 [5:56:30<51:32:43, 19.43s/it]

 11%|█         | 1190/10740 [5:56:50<52:21:33, 19.74s/it]

 11%|█         | 1191/10740 [5:57:05<48:21:59, 18.23s/it]

 11%|█         | 1192/10740 [5:57:24<49:19:18, 18.60s/it]

 11%|█         | 1193/10740 [5:57:44<49:57:30, 18.84s/it]

 11%|█         | 1194/10740 [5:58:05<52:07:17, 19.66s/it]

 11%|█         | 1195/10740 [5:58:20<48:15:40, 18.20s/it]

 11%|█         | 1196/10740 [5:58:40<49:53:35, 18.82s/it]

 11%|█         | 1197/10740 [5:58:59<50:07:06, 18.91s/it]

 11%|█         | 1198/10740 [5:59:14<46:23:33, 17.50s/it]


 11%|█         | 1200/10740 [5:59:47<44:43:59, 16.88s/it]
{'loss': 0.5021, 'learning_rate': 1.965226339894021e-06, 'rewards/chosen': -1.1718086004257202, 'rewards/rejected': -1.8395845890045166, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6677759885787964, 'policy_logps/rejected': -339.4544372558594, 'policy_logps/chosen': -344.32183837890625, 'referece_logps/rejected': -321.0586242675781, 'referece_logps/chosen': -332.6037292480469, 'logits/rejected': -0.6966124176979065, 'logits/chosen': -0.7251959443092346, 'epoch': 0.67}

 11%|█         | 1201/10740 [5:59:59<41:20:49, 15.60s/it]

 11%|█         | 1202/10740 [6:00:17<43:08:12, 16.28s/it]

 11%|█         | 1203/10740 [6:00:33<42:22:16, 15.99s/it]

 11%|█         | 1204/10740 [6:00:53<46:14:54, 17.46s/it]


 11%|█         | 1206/10740 [6:01:35<50:20:05, 19.01s/it]
{'loss': 0.514, 'learning_rate': 1.96475172856062e-06, 'rewards/chosen': -1.0968425273895264, 'rewards/rejected': -2.4722864627838135, 'rewards/accuracies': 0.75, 'rewards/margins': 1.375443935394287, 'policy_logps/rejected': -282.56365966796875, 'policy_logps/chosen': -293.4786071777344, 'referece_logps/rejected': -257.8408203125, 'referece_logps/chosen': -282.51019287109375, 'logits/rejected': -0.0628620982170105, 'logits/chosen': -0.15367068350315094, 'epoch': 0.67}

 11%|█         | 1207/10740 [6:01:47<45:02:13, 17.01s/it]

 11%|█         | 1208/10740 [6:02:06<46:22:47, 17.52s/it]

 11%|█▏        | 1209/10740 [6:02:27<48:52:37, 18.46s/it]

 11%|█▏        | 1210/10740 [6:02:42<46:24:03, 17.53s/it]

 11%|█▏        | 1211/10740 [6:03:02<48:36:06, 18.36s/it]

 11%|█▏        | 1212/10740 [6:03:18<46:37:09, 17.61s/it]

 11%|█▏        | 1213/10740 [6:03:41<50:41:42, 19.16s/it]


 11%|█▏        | 1215/10740 [6:04:13<47:33:11, 17.97s/it]
{'loss': 0.4331, 'learning_rate': 1.964033889159388e-06, 'rewards/chosen': -0.4141004681587219, 'rewards/rejected': -1.752833604812622, 'rewards/accuracies': 1.0, 'rewards/margins': 1.338733196258545, 'policy_logps/rejected': -507.7969970703125, 'policy_logps/chosen': -469.65399169921875, 'referece_logps/rejected': -490.2685852050781, 'referece_logps/chosen': -465.51300048828125, 'logits/rejected': -0.027427345514297485, 'logits/chosen': -0.03442230820655823, 'epoch': 0.68}

 11%|█▏        | 1216/10740 [6:04:33<48:55:59, 18.50s/it]

 11%|█▏        | 1217/10740 [6:04:48<46:06:19, 17.43s/it]


 11%|█▏        | 1219/10740 [6:05:16<42:20:28, 16.01s/it]

 11%|█▏        | 1220/10740 [6:05:37<46:31:17, 17.59s/it]

 11%|█▏        | 1221/10740 [6:05:50<42:53:20, 16.22s/it]

 11%|█▏        | 1222/10740 [6:06:10<45:36:42, 17.25s/it]

 11%|█▏        | 1223/10740 [6:06:25<43:52:52, 16.60s/it]

 11%|█▏        | 1224/10740 [6:06:37<39:53:13, 15.09s/it]

 11%|█▏        | 1225/10740 [6:06:56<43:32:20, 16.47s/it]

 11%|█▏        | 1226/10740 [6:07:16<45:58:20, 17.40s/it]

 11%|█▏        | 1227/10740 [6:07:30<43:37:42, 16.51s/it]

 11%|█▏        | 1228/10740 [6:07:49<45:43:18, 17.30s/it]

 11%|█▏        | 1229/10740 [6:08:03<42:51:45, 16.22s/it]

 11%|█▏        | 1230/10740 [6:08:23<45:56:50, 17.39s/it]

 11%|█▏        | 1231/10740 [6:08:40<45:41:31, 17.30s/it]

 11%|█▏        | 1232/10740 [6:08:52<41:28:05, 15.70s/it]

 11%|█▏        | 1233/10740 [6:09:12<44:34:50, 16.88s/it]

 11%|█▏        | 1234/10740 [6:09:30<45:40:26, 17.30s/it]

 11%|█▏        | 1235/10740 [6:09:47<45:16:03, 17.14s/it]

 12%|█▏        | 1236/10740 [6:10:05<45:59:24, 17.42s/it]

 12%|█▏        | 1237/10740 [6:10:26<49:08:02, 18.61s/it]

 12%|█▏        | 1238/10740 [6:10:47<51:04:47, 19.35s/it]

 12%|█▏        | 1239/10740 [6:11:06<50:40:27, 19.20s/it]

 12%|█▏        | 1240/10740 [6:11:26<51:07:25, 19.37s/it]

 12%|█▏        | 1241/10740 [6:11:47<52:01:10, 19.71s/it]

 12%|█▏        | 1242/10740 [6:12:06<51:49:36, 19.64s/it]

 12%|█▏        | 1243/10740 [6:12:22<49:06:00, 18.61s/it]

 12%|█▏        | 1244/10740 [6:12:41<49:20:00, 18.70s/it]

 12%|█▏        | 1245/10740 [6:13:00<49:09:10, 18.64s/it]

 12%|█▏        | 1246/10740 [6:13:14<45:29:01, 17.25s/it]

 12%|█▏        | 1247/10740 [6:13:32<46:18:20, 17.56s/it]

 12%|█▏        | 1248/10740 [6:13:54<49:29:42, 18.77s/it]

 12%|█▏        | 1249/10740 [6:14:10<47:48:56, 18.14s/it]

 12%|█▏        | 1250/10740 [6:14:27<46:35:41, 17.68s/it]

 12%|█▏        | 1251/10740 [6:14:47<48:49:15, 18.52s/it]

 12%|█▏        | 1252/10740 [6:15:05<48:17:58, 18.33s/it]

 12%|█▏        | 1253/10740 [6:15:23<47:55:15, 18.18s/it]

 12%|█▏        | 1254/10740 [6:15:44<49:55:47, 18.95s/it]

 12%|█▏        | 1255/10740 [6:16:05<51:30:21, 19.55s/it]

 12%|█▏        | 1256/10740 [6:16:21<49:15:05, 18.70s/it]

 12%|█▏        | 1257/10740 [6:16:39<48:04:34, 18.25s/it]

 12%|█▏        | 1258/10740 [6:16:58<49:18:59, 18.72s/it]

 12%|█▏        | 1259/10740 [6:17:17<48:51:08, 18.55s/it]

 12%|█▏        | 1260/10740 [6:17:33<47:23:25, 18.00s/it]
{'loss': 0.4154, 'learning_rate': 1.9603382671023996e-06, 'rewards/chosen': -0.46486997604370117, 'rewards/rejected': -2.9877843856811523, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5229148864746094, 'policy_logps/rejected': -454.7500305175781, 'policy_logps/chosen': -398.47283935546875, 'referece_logps/rejected': -424.87213134765625, 'referece_logps/chosen': -393.82415771484375, 'logits/rejected': -0.9036973118782043, 'logits/chosen': -0.7628686428070068, 'epoch': 0.7}


 12%|█▏        | 1262/10740 [6:18:09<47:14:57, 17.95s/it]

 12%|█▏        | 1263/10740 [6:18:27<47:47:23, 18.15s/it]

 12%|█▏        | 1264/10740 [6:18:42<44:40:24, 16.97s/it]

 12%|█▏        | 1265/10740 [6:18:57<43:34:48, 16.56s/it]

 12%|█▏        | 1266/10740 [6:19:11<41:32:59, 15.79s/it]

 12%|█▏        | 1267/10740 [6:19:23<38:12:38, 14.52s/it]

 12%|█▏        | 1268/10740 [6:19:43<42:55:57, 16.32s/it]

 12%|█▏        | 1269/10740 [6:20:05<47:20:00, 17.99s/it]

 12%|█▏        | 1270/10740 [6:20:29<51:35:14, 19.61s/it]

 12%|█▏        | 1271/10740 [6:20:51<53:30:48, 20.35s/it]

 12%|█▏        | 1272/10740 [6:21:10<52:51:35, 20.10s/it]

 12%|█▏        | 1273/10740 [6:21:30<52:28:20, 19.95s/it]

 12%|█▏        | 1274/10740 [6:21:45<48:45:58, 18.55s/it]

 12%|█▏        | 1275/10740 [6:22:08<52:17:35, 19.89s/it]
[2024-04-02 01:35:51,821] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1276/10740 [6:22:29<52:47:02, 20.08s/it]

 12%|█▏        | 1277/10740 [6:22:43<48:04:17, 18.29s/it]

 12%|█▏        | 1278/10740 [6:22:59<46:52:45, 17.84s/it]

 12%|█▏        | 1279/10740 [6:23:13<43:18:23, 16.48s/it]

 12%|█▏        | 1280/10740 [6:23:29<43:14:53, 16.46s/it]

 12%|█▏        | 1281/10740 [6:23:50<46:48:22, 17.81s/it]

 12%|█▏        | 1282/10740 [6:24:01<41:28:31, 15.79s/it]

 12%|█▏        | 1283/10740 [6:24:13<38:14:17, 14.56s/it]

 12%|█▏        | 1284/10740 [6:24:27<37:37:51, 14.33s/it]

 12%|█▏        | 1285/10740 [6:24:45<40:59:15, 15.61s/it]

 12%|█▏        | 1286/10740 [6:25:05<44:06:54, 16.80s/it]

 12%|█▏        | 1287/10740 [6:25:21<43:42:40, 16.65s/it]

 12%|█▏        | 1288/10740 [6:25:40<45:24:59, 17.30s/it]

 12%|█▏        | 1289/10740 [6:25:57<45:28:06, 17.32s/it]

 12%|█▏        | 1290/10740 [6:26:19<49:10:41, 18.73s/it]

 12%|█▏        | 1291/10740 [6:26:39<50:08:55, 19.11s/it]
{'loss': 0.5465, 'learning_rate': 1.9576894607723117e-06, 'rewards/chosen': -1.039921522140503, 'rewards/rejected': -2.0292344093322754, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9893128871917725, 'policy_logps/rejected': -290.3480224609375, 'policy_logps/chosen': -341.35894775390625, 'referece_logps/rejected': -270.0556945800781, 'referece_logps/chosen': -330.959716796875, 'logits/rejected': -0.7585057616233826, 'logits/chosen': -0.7400407195091248, 'epoch': 0.72}


 12%|█▏        | 1293/10740 [6:27:13<46:06:15, 17.57s/it]

 12%|█▏        | 1294/10740 [6:27:35<50:17:35, 19.17s/it]

 12%|█▏        | 1295/10740 [6:27:50<46:45:29, 17.82s/it]

 12%|█▏        | 1296/10740 [6:28:10<48:10:03, 18.36s/it]

 12%|█▏        | 1297/10740 [6:28:31<50:42:47, 19.33s/it]
{'loss': 0.4048, 'learning_rate': 1.9571671127697173e-06, 'rewards/chosen': -0.751247227191925, 'rewards/rejected': -2.1549301147460938, 'rewards/accuracies': 0.875, 'rewards/margins': 1.403682827949524, 'policy_logps/rejected': -542.8515014648438, 'policy_logps/chosen': -519.8486328125, 'referece_logps/rejected': -521.30224609375, 'referece_logps/chosen': -512.336181640625, 'logits/rejected': -0.9347082376480103, 'logits/chosen': -0.835468590259552, 'epoch': 0.72}


 12%|█▏        | 1299/10740 [6:29:08<49:26:31, 18.85s/it]

 12%|█▏        | 1300/10740 [6:29:22<46:08:21, 17.60s/it]

 12%|█▏        | 1301/10740 [6:29:39<44:56:41, 17.14s/it]
{'loss': 0.4209, 'learning_rate': 1.956817139528526e-06, 'rewards/chosen': -1.6207702159881592, 'rewards/rejected': -3.050126075744629, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4293557405471802, 'policy_logps/rejected': -597.760009765625, 'policy_logps/chosen': -583.4483642578125, 'referece_logps/rejected': -567.2586669921875, 'referece_logps/chosen': -567.2406005859375, 'logits/rejected': 0.16193190217018127, 'logits/chosen': 0.20141251385211945, 'epoch': 0.73}


 12%|█▏        | 1303/10740 [6:30:16<47:28:47, 18.11s/it]

 12%|█▏        | 1304/10740 [6:30:32<45:33:54, 17.38s/it]

 12%|█▏        | 1305/10740 [6:30:46<43:08:35, 16.46s/it]

 12%|█▏        | 1306/10740 [6:30:58<38:53:46, 14.84s/it]

 12%|█▏        | 1307/10740 [6:31:18<43:36:17, 16.64s/it]

 12%|█▏        | 1308/10740 [6:31:30<40:00:22, 15.27s/it]
[2024-04-02 01:45:14,163] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1309/10740 [6:31:43<38:13:15, 14.59s/it]

 12%|█▏        | 1310/10740 [6:32:05<43:33:47, 16.63s/it]

 12%|█▏        | 1311/10740 [6:32:20<42:22:13, 16.18s/it]

 12%|█▏        | 1312/10740 [6:32:35<41:12:22, 15.73s/it]

 12%|█▏        | 1313/10740 [6:32:52<42:38:28, 16.28s/it]
{'loss': 0.6058, 'learning_rate': 1.955758867466138e-06, 'rewards/chosen': -1.3429268598556519, 'rewards/rejected': -1.5562995672225952, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2133728563785553, 'policy_logps/rejected': -412.25360107421875, 'policy_logps/chosen': -313.21600341796875, 'referece_logps/rejected': -396.690673828125, 'referece_logps/chosen': -299.7867431640625, 'logits/rejected': -0.5407721996307373, 'logits/chosen': -0.18446026742458344, 'epoch': 0.73}

 12%|█▏        | 1314/10740 [6:33:13<46:16:19, 17.67s/it]

 12%|█▏        | 1315/10740 [6:33:31<46:26:25, 17.74s/it]


 12%|█▏        | 1317/10740 [6:34:02<43:10:08, 16.49s/it]

 12%|█▏        | 1318/10740 [6:34:22<45:42:02, 17.46s/it]

 12%|█▏        | 1319/10740 [6:34:44<48:57:19, 18.71s/it]

 12%|█▏        | 1320/10740 [6:35:00<46:38:00, 17.82s/it]
{'loss': 0.5406, 'learning_rate': 1.955135760454127e-06, 'rewards/chosen': -1.1089600324630737, 'rewards/rejected': -1.3679957389831543, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2590356767177582, 'policy_logps/rejected': -398.50616455078125, 'policy_logps/chosen': -353.6195068359375, 'referece_logps/rejected': -384.8262023925781, 'referece_logps/chosen': -342.5299072265625, 'logits/rejected': 0.5169181227684021, 'logits/chosen': 0.5803957581520081, 'epoch': 0.74}


 12%|█▏        | 1322/10740 [6:35:39<49:00:29, 18.73s/it]

 12%|█▏        | 1323/10740 [6:35:58<49:40:18, 18.99s/it]
[2024-04-02 01:49:42,099] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1324/10740 [6:36:20<51:26:37, 19.67s/it]
[2024-04-02 01:50:03,353] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1325/10740 [6:36:36<48:41:22, 18.62s/it]

 12%|█▏        | 1326/10740 [6:36:58<51:15:15, 19.60s/it]

 12%|█▏        | 1327/10740 [6:37:19<52:30:27, 20.08s/it]

 12%|█▏        | 1328/10740 [6:37:37<50:45:16, 19.41s/it]
{'loss': 0.4834, 'learning_rate': 1.9544184260029862e-06, 'rewards/chosen': -0.5957466959953308, 'rewards/rejected': -1.6604424715042114, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0646958351135254, 'policy_logps/rejected': -457.37469482421875, 'policy_logps/chosen': -458.5900573730469, 'referece_logps/rejected': -440.770263671875, 'referece_logps/chosen': -452.63262939453125, 'logits/rejected': 0.41080984473228455, 'logits/chosen': 0.45183998346328735, 'epoch': 0.74}

 12%|█▏        | 1329/10740 [6:37:47<43:51:30, 16.78s/it]


 12%|█▏        | 1331/10740 [6:38:19<43:35:26, 16.68s/it]

 12%|█▏        | 1332/10740 [6:38:30<38:54:22, 14.89s/it]

 12%|█▏        | 1333/10740 [6:38:43<37:16:26, 14.26s/it]

 12%|█▏        | 1334/10740 [6:39:03<42:02:10, 16.09s/it]

 12%|█▏        | 1335/10740 [6:39:23<45:08:16, 17.28s/it]

 12%|█▏        | 1336/10740 [6:39:45<48:29:58, 18.57s/it]

 12%|█▏        | 1337/10740 [6:40:06<50:27:42, 19.32s/it]
{'loss': 0.3892, 'learning_rate': 1.9536047842455644e-06, 'rewards/chosen': -0.9790400862693787, 'rewards/rejected': -1.937570333480835, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9585304260253906, 'policy_logps/rejected': -360.9532165527344, 'policy_logps/chosen': -349.2475280761719, 'referece_logps/rejected': -341.5775146484375, 'referece_logps/chosen': -339.4571228027344, 'logits/rejected': -0.47152280807495117, 'logits/chosen': -0.535503625869751, 'epoch': 0.75}

 12%|█▏        | 1338/10740 [6:40:26<50:57:35, 19.51s/it]


 12%|█▏        | 1340/10740 [6:41:01<47:58:11, 18.37s/it]

 12%|█▏        | 1341/10740 [6:41:23<50:25:09, 19.31s/it]

 12%|█▏        | 1342/10740 [6:41:37<46:37:31, 17.86s/it]

 13%|█▎        | 1343/10740 [6:41:57<47:57:48, 18.37s/it]

 13%|█▎        | 1344/10740 [6:42:14<47:19:10, 18.13s/it]
{'loss': 0.4597, 'learning_rate': 1.9529670944308484e-06, 'rewards/chosen': -0.47654247283935547, 'rewards/rejected': -2.0748729705810547, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5983306169509888, 'policy_logps/rejected': -274.51007080078125, 'policy_logps/chosen': -238.43338012695312, 'referece_logps/rejected': -253.76138305664062, 'referece_logps/chosen': -233.66795349121094, 'logits/rejected': -0.30028659105300903, 'logits/chosen': -0.5082029700279236, 'epoch': 0.75}
[2024-04-02 01:56:19,430] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1346/10740 [6:42:55<50:22:20, 19.30s/it]
[2024-04-02 01:56:39,080] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1347/10740 [6:43:12<48:22:37, 18.54s/it]

 13%|█▎        | 1348/10740 [6:43:33<50:09:58, 19.23s/it]

 13%|█▎        | 1349/10740 [6:43:52<50:23:25, 19.32s/it]

 13%|█▎        | 1350/10740 [6:44:11<49:35:52, 19.02s/it]

 13%|█▎        | 1351/10740 [6:44:33<51:54:26, 19.90s/it]
{'loss': 0.607, 'learning_rate': 1.952325157557657e-06, 'rewards/chosen': -1.4027243852615356, 'rewards/rejected': -2.111905574798584, 'rewards/accuracies': 0.625, 'rewards/margins': 0.709181010723114, 'policy_logps/rejected': -335.946533203125, 'policy_logps/chosen': -390.04693603515625, 'referece_logps/rejected': -314.82745361328125, 'referece_logps/chosen': -376.01971435546875, 'logits/rejected': 0.0031978050246834755, 'logits/chosen': 0.08511698246002197, 'epoch': 0.75}

 13%|█▎        | 1352/10740 [6:44:50<49:43:12, 19.07s/it]

 13%|█▎        | 1353/10740 [6:45:04<45:47:56, 17.56s/it]


 13%|█▎        | 1355/10740 [6:45:33<42:22:45, 16.26s/it]

 13%|█▎        | 1356/10740 [6:45:55<46:44:42, 17.93s/it]

 13%|█▎        | 1357/10740 [6:46:14<47:20:51, 18.17s/it]

 13%|█▎        | 1358/10740 [6:46:33<48:24:58, 18.58s/it]
{'loss': 0.5663, 'learning_rate': 1.9516789764868884e-06, 'rewards/chosen': -1.208127737045288, 'rewards/rejected': -2.00412917137146, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7960012555122375, 'policy_logps/rejected': -494.46466064453125, 'policy_logps/chosen': -425.22406005859375, 'referece_logps/rejected': -474.42340087890625, 'referece_logps/chosen': -413.1427917480469, 'logits/rejected': -1.2631878852844238, 'logits/chosen': -1.2577509880065918, 'epoch': 0.76}


 13%|█▎        | 1360/10740 [6:47:11<49:07:27, 18.85s/it]
[2024-04-02 02:00:54,582] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5163, 'learning_rate': 1.951493574179852e-06, 'rewards/chosen': -1.1835826635360718, 'rewards/rejected': -2.5304675102233887, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3468849658966064, 'policy_logps/rejected': -454.8370056152344, 'policy_logps/chosen': -396.87890625, 'referece_logps/rejected': -429.5323486328125, 'referece_logps/chosen': -385.04315185546875, 'logits/rejected': -0.2181396782398224, 'logits/chosen': -0.21112707257270813, 'epoch': 0.76}


 13%|█▎        | 1362/10740 [6:47:53<52:10:47, 20.03s/it]
[2024-04-02 02:01:36,993] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1363/10740 [6:48:09<48:48:30, 18.74s/it]

 13%|█▎        | 1364/10740 [6:48:27<48:27:44, 18.61s/it]

 13%|█▎        | 1365/10740 [6:48:45<47:50:34, 18.37s/it]
[2024-04-02 02:02:28,842] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.478, 'learning_rate': 1.9510285540983584e-06, 'rewards/chosen': -0.19024387001991272, 'rewards/rejected': -1.0619992017745972, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8717553615570068, 'policy_logps/rejected': -268.8790588378906, 'policy_logps/chosen': -398.7609558105469, 'referece_logps/rejected': -258.2590637207031, 'referece_logps/chosen': -396.8585510253906, 'logits/rejected': -0.8943396806716919, 'logits/chosen': -0.988422155380249, 'epoch': 0.76}

 13%|█▎        | 1366/10740 [6:49:04<48:26:40, 18.60s/it]


 13%|█▎        | 1368/10740 [6:49:39<47:28:29, 18.24s/it]

 13%|█▎        | 1369/10740 [6:49:56<46:20:03, 17.80s/it]
{'loss': 0.446, 'learning_rate': 1.9506549810117e-06, 'rewards/chosen': -1.8890177011489868, 'rewards/rejected': -2.0043692588806152, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11535154283046722, 'policy_logps/rejected': -258.3690490722656, 'policy_logps/chosen': -302.244873046875, 'referece_logps/rejected': -238.3253631591797, 'referece_logps/chosen': -283.35467529296875, 'logits/rejected': -1.428359031677246, 'logits/chosen': -1.4267778396606445, 'epoch': 0.76}


 13%|█▎        | 1371/10740 [6:50:33<48:10:11, 18.51s/it]

 13%|█▎        | 1372/10740 [6:50:52<48:08:36, 18.50s/it]

 13%|█▎        | 1373/10740 [6:51:14<51:05:01, 19.63s/it]

 13%|█▎        | 1374/10740 [6:51:30<48:09:07, 18.51s/it]
[2024-04-02 02:05:13,688] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1375/10740 [6:51:44<44:28:21, 17.10s/it]

 13%|█▎        | 1376/10740 [6:52:05<47:58:51, 18.45s/it]
[2024-04-02 02:05:49,085] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3948, 'learning_rate': 1.9499978995591206e-06, 'rewards/chosen': -1.186499834060669, 'rewards/rejected': -3.4987382888793945, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3122384548187256, 'policy_logps/rejected': -559.291748046875, 'policy_logps/chosen': -389.7969970703125, 'referece_logps/rejected': -524.3043212890625, 'referece_logps/chosen': -377.9320068359375, 'logits/rejected': -0.8190754652023315, 'logits/chosen': -0.7821135520935059, 'epoch': 0.77}
[2024-04-02 02:06:08,520] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1378/10740 [6:52:45<50:13:34, 19.31s/it]

 13%|█▎        | 1379/10740 [6:53:02<48:08:35, 18.51s/it]
{'loss': 0.4069, 'learning_rate': 1.9497149969817704e-06, 'rewards/chosen': -0.7597445249557495, 'rewards/rejected': -1.2890784740447998, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5293338298797607, 'policy_logps/rejected': -323.1731872558594, 'policy_logps/chosen': -364.9669189453125, 'referece_logps/rejected': -310.28240966796875, 'referece_logps/chosen': -357.3695068359375, 'logits/rejected': 0.004470929503440857, 'logits/chosen': 0.005574882961809635, 'epoch': 0.77}


 13%|█▎        | 1381/10740 [6:53:39<48:36:43, 18.70s/it]

 13%|█▎        | 1382/10740 [6:53:58<48:45:09, 18.75s/it]
{'loss': 0.4661, 'learning_rate': 1.949431316994295e-06, 'rewards/chosen': -0.6583994626998901, 'rewards/rejected': -1.6923807859420776, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0339813232421875, 'policy_logps/rejected': -314.94012451171875, 'policy_logps/chosen': -330.1753234863281, 'referece_logps/rejected': -298.01629638671875, 'referece_logps/chosen': -323.5913391113281, 'logits/rejected': -1.609873652458191, 'logits/chosen': -1.6025707721710205, 'epoch': 0.77}


 13%|█▎        | 1384/10740 [6:54:40<51:24:32, 19.78s/it]
{'loss': 0.4222, 'learning_rate': 1.949241765222778e-06, 'rewards/chosen': -1.03852379322052, 'rewards/rejected': -2.310962677001953, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2724390029907227, 'policy_logps/rejected': -368.9432678222656, 'policy_logps/chosen': -306.5669860839844, 'referece_logps/rejected': -345.83367919921875, 'referece_logps/chosen': -296.1817626953125, 'logits/rejected': -0.26839709281921387, 'logits/chosen': -0.2856237292289734, 'epoch': 0.77}


 13%|█▎        | 1386/10740 [6:55:18<50:04:27, 19.27s/it]

 13%|█▎        | 1387/10740 [6:55:38<50:42:29, 19.52s/it]
{'loss': 0.4546, 'learning_rate': 1.9489567900681015e-06, 'rewards/chosen': -0.7497437000274658, 'rewards/rejected': -1.9647644758224487, 'rewards/accuracies': 0.75, 'rewards/margins': 1.215020775794983, 'policy_logps/rejected': -327.7273254394531, 'policy_logps/chosen': -335.62091064453125, 'referece_logps/rejected': -308.0796813964844, 'referece_logps/chosen': -328.1235046386719, 'logits/rejected': -0.9223073720932007, 'logits/chosen': -0.7427363395690918, 'epoch': 0.77}


 13%|█▎        | 1389/10740 [6:56:22<54:06:46, 20.83s/it]

 13%|█▎        | 1390/10740 [6:56:42<53:51:14, 20.74s/it]
[2024-04-02 02:10:26,096] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5419, 'learning_rate': 1.948671038123947e-06, 'rewards/chosen': -0.9175763726234436, 'rewards/rejected': -1.8984932899475098, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9809167981147766, 'policy_logps/rejected': -481.89788818359375, 'policy_logps/chosen': -465.1170654296875, 'referece_logps/rejected': -462.9129638671875, 'referece_logps/chosen': -455.94122314453125, 'logits/rejected': 0.4189493954181671, 'logits/chosen': 0.47373220324516296, 'epoch': 0.78}


 13%|█▎        | 1392/10740 [6:57:24<53:41:05, 20.67s/it]
{'loss': 0.4409, 'learning_rate': 1.948480105393618e-06, 'rewards/chosen': -1.466822624206543, 'rewards/rejected': -2.512424945831299, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0456024408340454, 'policy_logps/rejected': -406.4644470214844, 'policy_logps/chosen': -496.10357666015625, 'referece_logps/rejected': -381.3401794433594, 'referece_logps/chosen': -481.435302734375, 'logits/rejected': -0.3979892134666443, 'logits/chosen': -0.6255812644958496, 'epoch': 0.78}

 13%|█▎        | 1393/10740 [6:57:43<51:59:41, 20.03s/it]

 13%|█▎        | 1394/10740 [6:58:01<50:51:25, 19.59s/it]


 13%|█▎        | 1396/10740 [6:58:36<46:46:07, 18.02s/it]

 13%|█▎        | 1397/10740 [6:58:56<48:32:56, 18.71s/it]

 13%|█▎        | 1398/10740 [6:59:12<46:28:18, 17.91s/it]

 13%|█▎        | 1399/10740 [6:59:28<44:50:17, 17.28s/it]
{'loss': 0.563, 'learning_rate': 1.9478091238968757e-06, 'rewards/chosen': -1.213384985923767, 'rewards/rejected': -1.5120482444763184, 'rewards/accuracies': 0.5, 'rewards/margins': 0.29866334795951843, 'policy_logps/rejected': -359.5523376464844, 'policy_logps/chosen': -418.02435302734375, 'referece_logps/rejected': -344.431884765625, 'referece_logps/chosen': -405.8904724121094, 'logits/rejected': -1.1667765378952026, 'logits/chosen': -1.2444450855255127, 'epoch': 0.78}


 13%|█▎        | 1401/10740 [6:59:58<42:25:55, 16.36s/it]

 13%|█▎        | 1402/10740 [7:00:18<45:05:21, 17.38s/it]
{'loss': 0.4932, 'learning_rate': 1.947520267140248e-06, 'rewards/chosen': -1.0084943771362305, 'rewards/rejected': -1.9542149305343628, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9457205533981323, 'policy_logps/rejected': -288.50732421875, 'policy_logps/chosen': -314.5959167480469, 'referece_logps/rejected': -268.96514892578125, 'referece_logps/chosen': -304.510986328125, 'logits/rejected': -1.2114980220794678, 'logits/chosen': -1.3031989336013794, 'epoch': 0.78}

 13%|█▎        | 1403/10740 [7:00:37<46:16:08, 17.84s/it]


 13%|█▎        | 1405/10740 [7:01:06<41:44:12, 16.10s/it]

 13%|█▎        | 1406/10740 [7:01:27<44:58:41, 17.35s/it]

 13%|█▎        | 1407/10740 [7:01:44<44:52:57, 17.31s/it]
{'loss': 0.5731, 'learning_rate': 1.9470371157438527e-06, 'rewards/chosen': -0.996434211730957, 'rewards/rejected': -1.7747167348861694, 'rewards/accuracies': 0.75, 'rewards/margins': 0.778282642364502, 'policy_logps/rejected': -218.5557098388672, 'policy_logps/chosen': -311.49420166015625, 'referece_logps/rejected': -200.80853271484375, 'referece_logps/chosen': -301.5299072265625, 'logits/rejected': -0.7753027081489563, 'logits/chosen': -0.9386172890663147, 'epoch': 0.79}


 13%|█▎        | 1409/10740 [7:02:22<47:14:35, 18.23s/it]
{'loss': 0.5291, 'learning_rate': 1.9468432521762806e-06, 'rewards/chosen': -0.9646795392036438, 'rewards/rejected': -1.4303081035614014, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4656285047531128, 'policy_logps/rejected': -558.832275390625, 'policy_logps/chosen': -487.7142028808594, 'referece_logps/rejected': -544.5291748046875, 'referece_logps/chosen': -478.0674133300781, 'logits/rejected': -0.2786538898944855, 'logits/chosen': -0.2602492868900299, 'epoch': 0.79}


 13%|█▎        | 1411/10740 [7:03:00<48:35:19, 18.75s/it]

 13%|█▎        | 1412/10740 [7:03:20<49:47:11, 19.21s/it]
{'loss': 0.462, 'learning_rate': 1.9465518109641433e-06, 'rewards/chosen': -1.7579724788665771, 'rewards/rejected': -2.608822822570801, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8508501648902893, 'policy_logps/rejected': -527.5803833007812, 'policy_logps/chosen': -390.69879150390625, 'referece_logps/rejected': -501.49212646484375, 'referece_logps/chosen': -373.1190490722656, 'logits/rejected': -1.49705171585083, 'logits/chosen': -1.242790937423706, 'epoch': 0.79}

 13%|█▎        | 1413/10740 [7:03:40<49:54:28, 19.26s/it]


 13%|█▎        | 1415/10740 [7:04:18<49:55:50, 19.28s/it]

 13%|█▎        | 1416/10740 [7:04:37<49:14:41, 19.01s/it]

 13%|█▎        | 1417/10740 [7:04:50<44:36:29, 17.23s/it]
{'loss': 0.5725, 'learning_rate': 1.9460643539046113e-06, 'rewards/chosen': -0.9055829048156738, 'rewards/rejected': -1.48062264919281, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5750398635864258, 'policy_logps/rejected': -423.6727294921875, 'policy_logps/chosen': -364.0162048339844, 'referece_logps/rejected': -408.86651611328125, 'referece_logps/chosen': -354.9604187011719, 'logits/rejected': -0.9788742661476135, 'logits/chosen': -0.9618750214576721, 'epoch': 0.79}
[2024-04-02 02:18:55,287] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1419/10740 [7:05:25<43:55:54, 16.97s/it]
[2024-04-02 02:19:08,529] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1420/10740 [7:05:47<47:58:36, 18.53s/it]
[2024-04-02 02:19:30,710] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4268, 'learning_rate': 1.9457708470354626e-06, 'rewards/chosen': -0.6579871773719788, 'rewards/rejected': -1.576758623123169, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9187712669372559, 'policy_logps/rejected': -432.4909362792969, 'policy_logps/chosen': -464.4821472167969, 'referece_logps/rejected': -416.723388671875, 'referece_logps/chosen': -457.9022521972656, 'logits/rejected': -0.3760538697242737, 'logits/chosen': -0.27583885192871094, 'epoch': 0.79}

 13%|█▎        | 1421/10740 [7:06:02<45:09:01, 17.44s/it]


 13%|█▎        | 1423/10740 [7:06:43<48:56:22, 18.91s/it]
{'loss': 0.5676, 'learning_rate': 1.945476565984759e-06, 'rewards/chosen': -1.4305813312530518, 'rewards/rejected': -2.7317399978637695, 'rewards/accuracies': 0.5, 'rewards/margins': 1.3011583089828491, 'policy_logps/rejected': -578.9618530273438, 'policy_logps/chosen': -454.5889587402344, 'referece_logps/rejected': -551.64453125, 'referece_logps/chosen': -440.28314208984375, 'logits/rejected': 0.10915755480527878, 'logits/chosen': -0.08546045422554016, 'epoch': 0.79}

 13%|█▎        | 1424/10740 [7:07:03<50:05:04, 19.35s/it]


 13%|█▎        | 1426/10740 [7:07:42<49:41:08, 19.20s/it]
{'loss': 0.5432, 'learning_rate': 1.945181510993391e-06, 'rewards/chosen': -0.6167236566543579, 'rewards/rejected': -2.124866485595703, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5081427097320557, 'policy_logps/rejected': -500.73883056640625, 'policy_logps/chosen': -569.632080078125, 'referece_logps/rejected': -479.49017333984375, 'referece_logps/chosen': -563.4649047851562, 'logits/rejected': 0.269059419631958, 'logits/chosen': 0.16875538229942322, 'epoch': 0.8}

 13%|█▎        | 1427/10740 [7:08:04<51:21:07, 19.85s/it]
[2024-04-02 02:22:09,309] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1429/10740 [7:08:45<52:05:18, 20.14s/it]
[2024-04-02 02:22:28,718] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1430/10740 [7:09:06<52:55:35, 20.47s/it]
{'loss': 0.5307, 'learning_rate': 1.944786900848073e-06, 'rewards/chosen': -1.272281527519226, 'rewards/rejected': -1.5614110231399536, 'rewards/accuracies': 0.625, 'rewards/margins': 0.28912946581840515, 'policy_logps/rejected': -325.3365478515625, 'policy_logps/chosen': -367.8973693847656, 'referece_logps/rejected': -309.7224426269531, 'referece_logps/chosen': -355.174560546875, 'logits/rejected': -0.9539783596992493, 'logits/chosen': -1.110485315322876, 'epoch': 0.8}

 13%|█▎        | 1431/10740 [7:09:25<51:51:11, 20.05s/it]

 13%|█▎        | 1432/10740 [7:09:42<49:23:03, 19.10s/it]

 13%|█▎        | 1433/10740 [7:10:00<48:21:05, 18.70s/it]

 13%|█▎        | 1434/10740 [7:10:20<49:03:44, 18.98s/it]


 13%|█▎        | 1436/10740 [7:11:03<52:30:36, 20.32s/it]

 13%|█▎        | 1437/10740 [7:11:17<47:57:08, 18.56s/it]
{'loss': 0.5008, 'learning_rate': 1.9440930251093216e-06, 'rewards/chosen': -1.3831008672714233, 'rewards/rejected': -3.1426548957824707, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7595542669296265, 'policy_logps/rejected': -463.3436584472656, 'policy_logps/chosen': -474.26123046875, 'referece_logps/rejected': -431.9171142578125, 'referece_logps/chosen': -460.43017578125, 'logits/rejected': 0.23396247625350952, 'logits/chosen': 0.31717926263809204, 'epoch': 0.8}

 13%|█▎        | 1438/10740 [7:11:36<47:47:17, 18.49s/it]

 13%|█▎        | 1439/10740 [7:11:48<43:08:13, 16.70s/it]


 13%|█▎        | 1441/10740 [7:12:25<45:40:16, 17.68s/it]

 13%|█▎        | 1442/10740 [7:12:49<50:08:26, 19.41s/it]

 13%|█▎        | 1443/10740 [7:13:09<50:43:54, 19.64s/it]

 13%|█▎        | 1444/10740 [7:13:25<48:12:21, 18.67s/it]
[2024-04-02 02:27:08,922] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5362, 'learning_rate': 1.94339494186088e-06, 'rewards/chosen': -1.2803102731704712, 'rewards/rejected': -3.211151123046875, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9308406114578247, 'policy_logps/rejected': -645.54736328125, 'policy_logps/chosen': -398.021484375, 'referece_logps/rejected': -613.4359130859375, 'referece_logps/chosen': -385.2183837890625, 'logits/rejected': -0.4326859414577484, 'logits/chosen': -0.23341697454452515, 'epoch': 0.81}
[2024-04-02 02:27:29,600] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1446/10740 [7:14:05<49:42:28, 19.25s/it]

 13%|█▎        | 1447/10740 [7:14:21<47:28:39, 18.39s/it]

 13%|█▎        | 1448/10740 [7:14:43<50:17:32, 19.48s/it]

 13%|█▎        | 1449/10740 [7:15:03<50:19:46, 19.50s/it]
{'loss': 0.4386, 'learning_rate': 1.9428937366654166e-06, 'rewards/chosen': -1.7576919794082642, 'rewards/rejected': -2.1978490352630615, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4401569962501526, 'policy_logps/rejected': -348.2471008300781, 'policy_logps/chosen': -378.15313720703125, 'referece_logps/rejected': -326.26861572265625, 'referece_logps/chosen': -360.5762634277344, 'logits/rejected': -0.32446444034576416, 'logits/chosen': -0.3550347089767456, 'epoch': 0.81}


 14%|█▎        | 1451/10740 [7:15:43<51:23:48, 19.92s/it]
{'loss': 0.406, 'learning_rate': 1.9426926542138727e-06, 'rewards/chosen': -1.0924439430236816, 'rewards/rejected': -2.579071521759033, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4866275787353516, 'policy_logps/rejected': -346.42108154296875, 'policy_logps/chosen': -273.0621337890625, 'referece_logps/rejected': -320.6303405761719, 'referece_logps/chosen': -262.1376953125, 'logits/rejected': -0.9920089840888977, 'logits/chosen': -1.0129201412200928, 'epoch': 0.81}

 14%|█▎        | 1452/10740 [7:16:04<52:00:43, 20.16s/it]

 14%|█▎        | 1453/10740 [7:16:22<50:16:09, 19.49s/it]

 14%|█▎        | 1454/10740 [7:16:39<48:19:47, 18.74s/it]

 14%|█▎        | 1455/10740 [7:16:59<49:07:32, 19.05s/it]


 14%|█▎        | 1457/10740 [7:17:39<50:53:14, 19.73s/it]
{'loss': 0.4107, 'learning_rate': 1.942087349387524e-06, 'rewards/chosen': -1.2803187370300293, 'rewards/rejected': -2.3393092155456543, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0589905977249146, 'policy_logps/rejected': -516.8956298828125, 'policy_logps/chosen': -510.59136962890625, 'referece_logps/rejected': -493.5025329589844, 'referece_logps/chosen': -497.7882080078125, 'logits/rejected': 0.5032118558883667, 'logits/chosen': 0.4117678701877594, 'epoch': 0.81}

 14%|█▎        | 1458/10740 [7:17:59<50:50:26, 19.72s/it]

 14%|█▎        | 1459/10740 [7:18:20<51:47:26, 20.09s/it]


 14%|█▎        | 1461/10740 [7:18:57<49:37:59, 19.26s/it]

 14%|█▎        | 1462/10740 [7:19:18<50:23:12, 19.55s/it]
{'loss': 0.4387, 'learning_rate': 1.941580572273353e-06, 'rewards/chosen': -0.8774286508560181, 'rewards/rejected': -2.2863080501556396, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4088793992996216, 'policy_logps/rejected': -300.7875061035156, 'policy_logps/chosen': -292.5267333984375, 'referece_logps/rejected': -277.9244384765625, 'referece_logps/chosen': -283.75244140625, 'logits/rejected': -1.3310694694519043, 'logits/chosen': -1.3740664720535278, 'epoch': 0.82}

 14%|█▎        | 1463/10740 [7:19:37<50:24:59, 19.56s/it]

 14%|█▎        | 1464/10740 [7:19:57<50:31:34, 19.61s/it]


 14%|█▎        | 1466/10740 [7:20:22<40:53:59, 15.88s/it]
{'loss': 0.5867, 'learning_rate': 1.941173609023703e-06, 'rewards/chosen': -0.820132851600647, 'rewards/rejected': -1.595401406288147, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7752684950828552, 'policy_logps/rejected': -352.175048828125, 'policy_logps/chosen': -319.45947265625, 'referece_logps/rejected': -336.22100830078125, 'referece_logps/chosen': -311.2581481933594, 'logits/rejected': -0.5625145435333252, 'logits/chosen': -0.6006648540496826, 'epoch': 0.82}


 14%|█▎        | 1468/10740 [7:20:53<41:18:50, 16.04s/it]

 14%|█▎        | 1469/10740 [7:21:12<42:59:14, 16.69s/it]
{'loss': 0.5795, 'learning_rate': 1.940867487732602e-06, 'rewards/chosen': -1.2098726034164429, 'rewards/rejected': -1.7462093830108643, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5363366603851318, 'policy_logps/rejected': -481.61773681640625, 'policy_logps/chosen': -456.34735107421875, 'referece_logps/rejected': -464.15557861328125, 'referece_logps/chosen': -444.2486572265625, 'logits/rejected': -1.3580063581466675, 'logits/chosen': -1.5054200887680054, 'epoch': 0.82}

 14%|█▎        | 1470/10740 [7:21:27<42:00:35, 16.31s/it]

 14%|█▎        | 1471/10740 [7:21:46<44:10:28, 17.16s/it]

 14%|█▎        | 1472/10740 [7:22:06<46:13:04, 17.95s/it]

 14%|█▎        | 1473/10740 [7:22:19<42:32:11, 16.52s/it]


 14%|█▎        | 1475/10740 [7:22:54<42:34:23, 16.54s/it]

 14%|█▎        | 1476/10740 [7:23:10<42:04:51, 16.35s/it]

 14%|█▍        | 1477/10740 [7:23:26<42:07:05, 16.37s/it]
{'loss': 0.5596, 'learning_rate': 1.940047399707448e-06, 'rewards/chosen': -1.1373544931411743, 'rewards/rejected': -1.5329468250274658, 'rewards/accuracies': 0.5, 'rewards/margins': 0.39559245109558105, 'policy_logps/rejected': -306.99664306640625, 'policy_logps/chosen': -296.41851806640625, 'referece_logps/rejected': -291.66717529296875, 'referece_logps/chosen': -285.0449523925781, 'logits/rejected': -0.8264254331588745, 'logits/chosen': -0.7564341425895691, 'epoch': 0.83}

 14%|█▍        | 1478/10740 [7:23:41<40:40:38, 15.81s/it]


 14%|█▍        | 1480/10740 [7:24:20<46:27:34, 18.06s/it]
{'loss': 0.5569, 'learning_rate': 1.9397384556976024e-06, 'rewards/chosen': -0.4583309292793274, 'rewards/rejected': -2.062059164047241, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6037280559539795, 'policy_logps/rejected': -457.7975769042969, 'policy_logps/chosen': -241.0254669189453, 'referece_logps/rejected': -437.177001953125, 'referece_logps/chosen': -236.44216918945312, 'logits/rejected': -1.7343664169311523, 'logits/chosen': -1.7276556491851807, 'epoch': 0.83}

 14%|█▍        | 1481/10740 [7:24:39<46:47:47, 18.20s/it]


 14%|█▍        | 1483/10740 [7:25:08<41:49:14, 16.26s/it]
{'loss': 0.5195, 'learning_rate': 1.9394287424441492e-06, 'rewards/chosen': -1.0272150039672852, 'rewards/rejected': -2.6010916233062744, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5738766193389893, 'policy_logps/rejected': -384.087158203125, 'policy_logps/chosen': -440.6808776855469, 'referece_logps/rejected': -358.0762939453125, 'referece_logps/chosen': -430.4087219238281, 'logits/rejected': -0.5916004776954651, 'logits/chosen': -0.5694762468338013, 'epoch': 0.83}

 14%|█▍        | 1484/10740 [7:25:28<44:27:45, 17.29s/it]

 14%|█▍        | 1485/10740 [7:25:47<45:46:26, 17.81s/it]


 14%|█▍        | 1487/10740 [7:26:28<49:26:27, 19.24s/it]

 14%|█▍        | 1488/10740 [7:26:46<48:35:38, 18.91s/it]

 14%|█▍        | 1489/10740 [7:26:58<43:16:31, 16.84s/it]
{'loss': 0.6577, 'learning_rate': 1.9388070092211386e-06, 'rewards/chosen': -1.2263096570968628, 'rewards/rejected': -2.112884998321533, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8865753412246704, 'policy_logps/rejected': -550.9268798828125, 'policy_logps/chosen': -403.2498474121094, 'referece_logps/rejected': -529.7979736328125, 'referece_logps/chosen': -390.9867858886719, 'logits/rejected': 0.386738657951355, 'logits/chosen': 0.36186015605926514, 'epoch': 0.83}


 14%|█▍        | 1491/10740 [7:27:34<44:48:02, 17.44s/it]
{'loss': 0.5198, 'learning_rate': 1.9385990816184305e-06, 'rewards/chosen': -1.381995439529419, 'rewards/rejected': -2.113101005554199, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7311054468154907, 'policy_logps/rejected': -459.2393798828125, 'policy_logps/chosen': -439.5107421875, 'referece_logps/rejected': -438.1083679199219, 'referece_logps/chosen': -425.6907958984375, 'logits/rejected': -0.6940207481384277, 'logits/chosen': -0.6635357141494751, 'epoch': 0.83}

 14%|█▍        | 1492/10740 [7:27:46<40:20:23, 15.70s/it]

 14%|█▍        | 1493/10740 [7:28:06<43:32:10, 16.95s/it]


 14%|█▍        | 1495/10740 [7:28:46<48:06:25, 18.73s/it]

 14%|█▍        | 1496/10740 [7:29:06<48:57:01, 19.06s/it]

 14%|█▍        | 1497/10740 [7:29:28<51:17:00, 19.97s/it]
{'loss': 0.571, 'learning_rate': 1.9379732502842398e-06, 'rewards/chosen': -0.8911948800086975, 'rewards/rejected': -1.8380630016326904, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9468681812286377, 'policy_logps/rejected': -243.82159423828125, 'policy_logps/chosen': -316.5102844238281, 'referece_logps/rejected': -225.44097900390625, 'referece_logps/chosen': -307.5983581542969, 'logits/rejected': -1.2430131435394287, 'logits/chosen': -1.3172823190689087, 'epoch': 0.84}

 14%|█▍        | 1498/10740 [7:29:48<51:11:23, 19.94s/it]


 14%|█▍        | 1500/10740 [7:30:26<50:12:36, 19.56s/it]
{'loss': 0.4154, 'learning_rate': 1.937659182790925e-06, 'rewards/chosen': -0.8232885003089905, 'rewards/rejected': -1.5943026542663574, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7710141539573669, 'policy_logps/rejected': -424.5137939453125, 'policy_logps/chosen': -306.3341979980469, 'referece_logps/rejected': -408.5707702636719, 'referece_logps/chosen': -298.101318359375, 'logits/rejected': -0.230735182762146, 'logits/chosen': -0.22546610236167908, 'epoch': 0.84}

 14%|█▍        | 1501/10740 [7:31:01<62:03:51, 24.18s/it]

 14%|█▍        | 1502/10740 [7:31:23<60:01:17, 23.39s/it]

 14%|█▍        | 1503/10740 [7:31:46<59:39:41, 23.25s/it]

 14%|█▍        | 1504/10740 [7:32:04<55:52:32, 21.78s/it]

 14%|█▍        | 1505/10740 [7:32:25<55:06:09, 21.48s/it]

 14%|█▍        | 1506/10740 [7:32:43<52:26:43, 20.45s/it]

 14%|█▍        | 1507/10740 [7:33:04<52:44:38, 20.57s/it]

 14%|█▍        | 1508/10740 [7:33:22<51:03:31, 19.91s/it]

 14%|█▍        | 1509/10740 [7:33:42<50:53:38, 19.85s/it]


 14%|█▍        | 1511/10740 [7:34:19<48:31:25, 18.93s/it]
{'loss': 0.4604, 'learning_rate': 1.9365010371994295e-06, 'rewards/chosen': -0.308336079120636, 'rewards/rejected': -1.644729495048523, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3363935947418213, 'policy_logps/rejected': -363.95587158203125, 'policy_logps/chosen': -328.9482116699219, 'referece_logps/rejected': -347.50860595703125, 'referece_logps/chosen': -325.8648376464844, 'logits/rejected': -0.4195520281791687, 'logits/chosen': -0.44247594475746155, 'epoch': 0.84}


 14%|█▍        | 1513/10740 [7:34:49<42:45:49, 16.68s/it]
{'loss': 0.5492, 'learning_rate': 1.936289357597574e-06, 'rewards/chosen': -1.2564107179641724, 'rewards/rejected': -1.9790221452713013, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7226113080978394, 'policy_logps/rejected': -354.28521728515625, 'policy_logps/chosen': -294.10821533203125, 'referece_logps/rejected': -334.4949645996094, 'referece_logps/chosen': -281.5440673828125, 'logits/rejected': -0.3461492359638214, 'logits/chosen': -0.2858119010925293, 'epoch': 0.85}

 14%|█▍        | 1514/10740 [7:35:07<44:15:51, 17.27s/it]


 14%|█▍        | 1516/10740 [7:35:37<40:08:15, 15.67s/it]
{'loss': 0.6223, 'learning_rate': 1.9359711995353058e-06, 'rewards/chosen': -1.0124540328979492, 'rewards/rejected': -1.6850203275680542, 'rewards/accuracies': 0.75, 'rewards/margins': 0.672566294670105, 'policy_logps/rejected': -228.63916015625, 'policy_logps/chosen': -273.99676513671875, 'referece_logps/rejected': -211.78895568847656, 'referece_logps/chosen': -263.87225341796875, 'logits/rejected': -0.45774024724960327, 'logits/chosen': -0.445747047662735, 'epoch': 0.85}

 14%|█▍        | 1517/10740 [7:35:56<42:36:39, 16.63s/it]

 14%|█▍        | 1518/10740 [7:36:06<38:03:27, 14.86s/it]

 14%|█▍        | 1519/10740 [7:36:17<34:54:05, 13.63s/it]

 14%|█▍        | 1520/10740 [7:36:36<39:14:20, 15.32s/it]

 14%|█▍        | 1521/10740 [7:36:50<37:48:25, 14.76s/it]

 14%|█▍        | 1522/10740 [7:37:10<41:34:50, 16.24s/it]


 14%|█▍        | 1524/10740 [7:37:45<44:06:23, 17.23s/it]
{'loss': 0.5833, 'learning_rate': 1.935119033074949e-06, 'rewards/chosen': -1.2119247913360596, 'rewards/rejected': -2.2751781940460205, 'rewards/accuracies': 0.75, 'rewards/margins': 1.063253402709961, 'policy_logps/rejected': -359.0623779296875, 'policy_logps/chosen': -343.9438781738281, 'referece_logps/rejected': -336.3106384277344, 'referece_logps/chosen': -331.82464599609375, 'logits/rejected': -1.279321312904358, 'logits/chosen': -1.4560306072235107, 'epoch': 0.85}

 14%|█▍        | 1525/10740 [7:38:04<45:41:25, 17.85s/it]

 14%|█▍        | 1526/10740 [7:38:21<45:13:03, 17.67s/it]

 14%|█▍        | 1527/10740 [7:38:42<47:33:02, 18.58s/it]

 14%|█▍        | 1528/10740 [7:38:58<45:08:25, 17.64s/it]

 14%|█▍        | 1529/10740 [7:39:12<43:08:30, 16.86s/it]

 14%|█▍        | 1530/10740 [7:39:30<44:02:39, 17.22s/it]

 14%|█▍        | 1531/10740 [7:39:48<44:36:50, 17.44s/it]


 14%|█▍        | 1533/10740 [7:40:23<43:35:10, 17.04s/it]
[2024-04-02 02:54:06,479] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5786, 'learning_rate': 1.9341538396286835e-06, 'rewards/chosen': -0.46607816219329834, 'rewards/rejected': -2.4195685386657715, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9534903764724731, 'policy_logps/rejected': -397.1931457519531, 'policy_logps/chosen': -389.3872985839844, 'referece_logps/rejected': -372.9974670410156, 'referece_logps/chosen': -384.72650146484375, 'logits/rejected': -0.2674176096916199, 'logits/chosen': -0.2101859450340271, 'epoch': 0.86}

 14%|█▍        | 1534/10740 [7:40:35<39:39:35, 15.51s/it]

 14%|█▍        | 1535/10740 [7:40:58<45:32:02, 17.81s/it]


 14%|█▍        | 1537/10740 [7:41:35<47:20:57, 18.52s/it]
[2024-04-02 02:55:18,899] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4152, 'learning_rate': 1.9337226552873487e-06, 'rewards/chosen': 0.10004670917987823, 'rewards/rejected': -1.7766258716583252, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8766725063323975, 'policy_logps/rejected': -407.5653076171875, 'policy_logps/chosen': -494.39068603515625, 'referece_logps/rejected': -389.7990417480469, 'referece_logps/chosen': -495.39117431640625, 'logits/rejected': 0.004879981279373169, 'logits/chosen': -0.06490662693977356, 'epoch': 0.86}

 14%|█▍        | 1538/10740 [7:41:48<42:54:07, 16.78s/it]

 14%|█▍        | 1539/10740 [7:42:04<42:10:46, 16.50s/it]


 14%|█▍        | 1541/10740 [7:42:41<44:26:50, 17.39s/it]
{'loss': 0.5851, 'learning_rate': 1.9332901121563215e-06, 'rewards/chosen': -1.5479917526245117, 'rewards/rejected': -1.724787950515747, 'rewards/accuracies': 0.5, 'rewards/margins': 0.17679619789123535, 'policy_logps/rejected': -451.7927551269531, 'policy_logps/chosen': -503.1667175292969, 'referece_logps/rejected': -434.5448913574219, 'referece_logps/chosen': -487.686767578125, 'logits/rejected': -0.5280147790908813, 'logits/chosen': -0.5763735175132751, 'epoch': 0.86}

 14%|█▍        | 1542/10740 [7:42:57<43:38:09, 17.08s/it]


 14%|█▍        | 1544/10740 [7:43:29<42:32:57, 16.66s/it]
{'loss': 0.5588, 'learning_rate': 1.932964813480898e-06, 'rewards/chosen': -1.7037385702133179, 'rewards/rejected': -2.18403959274292, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4803010821342468, 'policy_logps/rejected': -299.5802001953125, 'policy_logps/chosen': -271.9302673339844, 'referece_logps/rejected': -277.73980712890625, 'referece_logps/chosen': -254.89288330078125, 'logits/rejected': -1.0641472339630127, 'logits/chosen': -0.9483325481414795, 'epoch': 0.86}

 14%|█▍        | 1545/10740 [7:43:47<43:14:36, 16.93s/it]

 14%|█▍        | 1546/10740 [7:44:03<42:22:30, 16.59s/it]

 14%|█▍        | 1547/10740 [7:44:20<42:45:05, 16.74s/it]

 14%|█▍        | 1548/10740 [7:44:41<46:05:56, 18.05s/it]

 14%|█▍        | 1549/10740 [7:44:59<46:05:46, 18.06s/it]

 14%|█▍        | 1550/10740 [7:45:20<48:19:43, 18.93s/it]

 14%|█▍        | 1551/10740 [7:45:36<45:52:21, 17.97s/it]

 14%|█▍        | 1552/10740 [7:45:57<48:33:34, 19.03s/it]

 14%|█▍        | 1553/10740 [7:46:15<47:59:57, 18.81s/it]
[2024-04-02 03:00:20,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 1554/10740 [7:46:36<49:36:10, 19.44s/it]

 14%|█▍        | 1555/10740 [7:46:49<44:37:17, 17.49s/it]

 14%|█▍        | 1556/10740 [7:47:01<40:05:47, 15.72s/it]

 14%|█▍        | 1557/10740 [7:47:22<44:18:21, 17.37s/it]

 15%|█▍        | 1558/10740 [7:47:37<42:30:56, 16.67s/it]

 15%|█▍        | 1559/10740 [7:47:54<42:20:47, 16.60s/it]

 15%|█▍        | 1560/10740 [7:48:15<46:03:34, 18.06s/it]

 15%|█▍        | 1561/10740 [7:48:35<47:51:34, 18.77s/it]

 15%|█▍        | 1562/10740 [7:48:51<45:17:16, 17.76s/it]

 15%|█▍        | 1563/10740 [7:49:12<47:55:03, 18.80s/it]

 15%|█▍        | 1564/10740 [7:49:33<49:42:29, 19.50s/it]

 15%|█▍        | 1565/10740 [7:49:46<44:52:38, 17.61s/it]

 15%|█▍        | 1566/10740 [7:50:03<44:06:43, 17.31s/it]

 15%|█▍        | 1567/10740 [7:50:23<46:25:36, 18.22s/it]
[2024-04-02 03:04:28,406] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▍        | 1568/10740 [7:50:45<48:45:17, 19.14s/it]

 15%|█▍        | 1569/10740 [7:50:58<44:22:04, 17.42s/it]
[2024-04-02 03:05:04,362] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▍        | 1570/10740 [7:51:21<48:17:16, 18.96s/it]

 15%|█▍        | 1571/10740 [7:51:34<44:17:49, 17.39s/it]

 15%|█▍        | 1572/10740 [7:51:54<46:23:07, 18.21s/it]

 15%|█▍        | 1573/10740 [7:52:07<41:39:49, 16.36s/it]

 15%|█▍        | 1574/10740 [7:52:24<42:38:58, 16.75s/it]

 15%|█▍        | 1575/10740 [7:52:40<41:37:10, 16.35s/it]

 15%|█▍        | 1576/10740 [7:52:52<38:54:20, 15.28s/it]

 15%|█▍        | 1577/10740 [7:53:10<40:54:35, 16.07s/it]


 15%|█▍        | 1579/10740 [7:53:44<41:05:21, 16.15s/it]

 15%|█▍        | 1580/10740 [7:54:06<45:05:23, 17.72s/it]

 15%|█▍        | 1581/10740 [7:54:25<46:30:05, 18.28s/it]

 15%|█▍        | 1582/10740 [7:54:45<48:00:38, 18.87s/it]

 15%|█▍        | 1583/10740 [7:55:02<46:31:10, 18.29s/it]

 15%|█▍        | 1584/10740 [7:55:25<50:06:20, 19.70s/it]
[2024-04-02 03:09:09,053] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▍        | 1585/10740 [7:55:46<51:10:40, 20.12s/it]

 15%|█▍        | 1586/10740 [7:56:05<50:07:32, 19.71s/it]

 15%|█▍        | 1587/10740 [7:56:25<50:23:07, 19.82s/it]

 15%|█▍        | 1588/10740 [7:56:44<49:56:49, 19.65s/it]

 15%|█▍        | 1589/10740 [7:57:05<50:16:10, 19.78s/it]

 15%|█▍        | 1590/10740 [7:57:26<51:15:33, 20.17s/it]

 15%|█▍        | 1591/10740 [7:57:41<47:45:25, 18.79s/it]

 15%|█▍        | 1592/10740 [7:58:03<50:10:43, 19.75s/it]

 15%|█▍        | 1593/10740 [7:58:23<50:25:31, 19.85s/it]

 15%|█▍        | 1594/10740 [7:58:41<49:05:22, 19.32s/it]

 15%|█▍        | 1595/10740 [7:59:02<50:07:36, 19.73s/it]
{'loss': 0.5682, 'learning_rate': 1.927318109773216e-06, 'rewards/chosen': -1.1811119318008423, 'rewards/rejected': -1.8959102630615234, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7147983908653259, 'policy_logps/rejected': -325.7157897949219, 'policy_logps/chosen': -358.57720947265625, 'referece_logps/rejected': -306.7566833496094, 'referece_logps/chosen': -346.76611328125, 'logits/rejected': -0.48912811279296875, 'logits/chosen': -0.420433908700943, 'epoch': 0.89}


 15%|█▍        | 1597/10740 [7:59:35<46:31:35, 18.32s/it]

 15%|█▍        | 1598/10740 [7:59:46<41:26:09, 16.32s/it]
{'loss': 0.5382, 'learning_rate': 1.9269791058635114e-06, 'rewards/chosen': -1.2290663719177246, 'rewards/rejected': -1.5813336372375488, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3522670865058899, 'policy_logps/rejected': -448.1810607910156, 'policy_logps/chosen': -506.47607421875, 'referece_logps/rejected': -432.36773681640625, 'referece_logps/chosen': -494.18536376953125, 'logits/rejected': -0.22507387399673462, 'logits/chosen': -0.2610059380531311, 'epoch': 0.89}


 15%|█▍        | 1600/10740 [8:00:22<43:33:59, 17.16s/it]

 15%|█▍        | 1601/10740 [8:00:42<45:15:17, 17.83s/it]

 15%|█▍        | 1602/10740 [8:01:04<48:29:09, 19.10s/it]

 15%|█▍        | 1603/10740 [8:01:19<45:44:39, 18.02s/it]

 15%|█▍        | 1604/10740 [8:01:36<44:38:54, 17.59s/it]

 15%|█▍        | 1605/10740 [8:01:52<43:39:43, 17.21s/it]

 15%|█▍        | 1606/10740 [8:02:07<41:54:39, 16.52s/it]

 15%|█▍        | 1607/10740 [8:02:29<46:01:32, 18.14s/it]

 15%|█▍        | 1608/10740 [8:02:44<43:39:53, 17.21s/it]

 15%|█▍        | 1609/10740 [8:03:03<44:40:10, 17.61s/it]

 15%|█▍        | 1610/10740 [8:03:23<46:32:18, 18.35s/it]

 15%|█▌        | 1611/10740 [8:03:41<46:54:30, 18.50s/it]

 15%|█▌        | 1612/10740 [8:04:00<46:37:55, 18.39s/it]

 15%|█▌        | 1613/10740 [8:04:18<46:21:27, 18.29s/it]

 15%|█▌        | 1614/10740 [8:04:33<44:18:11, 17.48s/it]

 15%|█▌        | 1615/10740 [8:04:55<47:10:54, 18.61s/it]

 15%|█▌        | 1616/10740 [8:05:16<49:13:02, 19.42s/it]

 15%|█▌        | 1617/10740 [8:05:38<51:04:18, 20.15s/it]

 15%|█▌        | 1618/10740 [8:06:01<53:13:27, 21.01s/it]
[2024-04-02 03:19:44,408] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1619/10740 [8:06:22<53:11:55, 21.00s/it]
[2024-04-02 03:20:05,386] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1620/10740 [8:06:38<49:17:13, 19.46s/it]

 15%|█▌        | 1621/10740 [8:06:54<46:59:20, 18.55s/it]

 15%|█▌        | 1622/10740 [8:07:11<46:12:09, 18.24s/it]

 15%|█▌        | 1623/10740 [8:07:30<46:31:39, 18.37s/it]

 15%|█▌        | 1624/10740 [8:07:51<48:32:17, 19.17s/it]

 15%|█▌        | 1625/10740 [8:08:08<46:50:31, 18.50s/it]

 15%|█▌        | 1626/10740 [8:08:26<46:02:42, 18.19s/it]

 15%|█▌        | 1627/10740 [8:08:36<40:18:48, 15.93s/it]

 15%|█▌        | 1628/10740 [8:08:51<39:48:32, 15.73s/it]

 15%|█▌        | 1629/10740 [8:09:06<38:32:31, 15.23s/it]

 15%|█▌        | 1630/10740 [8:09:26<42:20:44, 16.73s/it]

 15%|█▌        | 1631/10740 [8:09:38<39:03:08, 15.43s/it]

 15%|█▌        | 1632/10740 [8:09:50<36:20:26, 14.36s/it]

 15%|█▌        | 1633/10740 [8:10:10<40:43:53, 16.10s/it]

 15%|█▌        | 1634/10740 [8:10:23<37:57:38, 15.01s/it]

 15%|█▌        | 1635/10740 [8:10:41<40:15:19, 15.92s/it]

 15%|█▌        | 1636/10740 [8:11:02<44:36:56, 17.64s/it]

 15%|█▌        | 1637/10740 [8:11:22<46:16:07, 18.30s/it]

 15%|█▌        | 1638/10740 [8:11:39<45:26:28, 17.97s/it]

 15%|█▌        | 1639/10740 [8:11:56<44:36:32, 17.65s/it]

 15%|█▌        | 1640/10740 [8:12:15<45:26:41, 17.98s/it]

 15%|█▌        | 1641/10740 [8:12:29<42:44:59, 16.91s/it]

 15%|█▌        | 1642/10740 [8:12:53<47:33:01, 18.82s/it]
[2024-04-02 03:26:36,476] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1643/10740 [8:13:12<48:01:19, 19.00s/it]
[2024-04-02 03:26:55,921] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1644/10740 [8:13:26<44:00:47, 17.42s/it]

 15%|█▌        | 1645/10740 [8:13:47<46:59:54, 18.60s/it]
[2024-04-02 03:27:31,008] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1646/10740 [8:14:07<47:43:19, 18.89s/it]

 15%|█▌        | 1647/10740 [8:14:29<50:16:05, 19.90s/it]
{'loss': 0.4455, 'learning_rate': 1.9213348325980526e-06, 'rewards/chosen': -0.25187093019485474, 'rewards/rejected': -2.1984755992889404, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9466047286987305, 'policy_logps/rejected': -328.0790100097656, 'policy_logps/chosen': -309.3360290527344, 'referece_logps/rejected': -306.0942687988281, 'referece_logps/chosen': -306.8173828125, 'logits/rejected': -0.27958443760871887, 'logits/chosen': -0.4184635579586029, 'epoch': 0.92}


 15%|█▌        | 1649/10740 [8:15:02<45:07:14, 17.87s/it]

 15%|█▌        | 1650/10740 [8:15:19<44:28:44, 17.62s/it]

 15%|█▌        | 1651/10740 [8:15:35<43:41:25, 17.31s/it]

 15%|█▌        | 1652/10740 [8:15:55<45:22:47, 17.98s/it]

 15%|█▌        | 1653/10740 [8:16:17<48:21:20, 19.16s/it]

 15%|█▌        | 1654/10740 [8:16:33<45:52:58, 18.18s/it]
{'loss': 0.477, 'learning_rate': 1.9205120545048524e-06, 'rewards/chosen': -1.0583299398422241, 'rewards/rejected': -2.275634765625, 'rewards/accuracies': 0.5, 'rewards/margins': 1.2173048257827759, 'policy_logps/rejected': -432.513427734375, 'policy_logps/chosen': -376.18804931640625, 'referece_logps/rejected': -409.757080078125, 'referece_logps/chosen': -365.604736328125, 'logits/rejected': -0.29833513498306274, 'logits/chosen': -0.4210011661052704, 'epoch': 0.92}


 15%|█▌        | 1656/10740 [8:17:16<50:11:02, 19.89s/it]

 15%|█▌        | 1657/10740 [8:17:31<46:54:19, 18.59s/it]

 15%|█▌        | 1658/10740 [8:17:53<49:43:09, 19.71s/it]
[2024-04-02 03:31:37,128] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1659/10740 [8:18:13<49:33:52, 19.65s/it]
{'loss': 0.4675, 'learning_rate': 1.9199218439683668e-06, 'rewards/chosen': -2.0486674308776855, 'rewards/rejected': -2.593017339706421, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5443499684333801, 'policy_logps/rejected': -415.2197265625, 'policy_logps/chosen': -472.3741455078125, 'referece_logps/rejected': -389.28955078125, 'referece_logps/chosen': -451.8874816894531, 'logits/rejected': -0.35034802556037903, 'logits/chosen': -0.3564918339252472, 'epoch': 0.93}


 15%|█▌        | 1661/10740 [8:18:54<50:07:25, 19.87s/it]

 15%|█▌        | 1662/10740 [8:19:13<49:49:45, 19.76s/it]

 15%|█▌        | 1663/10740 [8:19:30<47:41:35, 18.92s/it]

 15%|█▌        | 1664/10740 [8:19:48<46:40:19, 18.51s/it]

 16%|█▌        | 1665/10740 [8:20:11<50:10:53, 19.91s/it]

 16%|█▌        | 1666/10740 [8:20:29<48:51:36, 19.38s/it]

 16%|█▌        | 1667/10740 [8:20:43<44:31:18, 17.67s/it]
{'loss': 0.4653, 'learning_rate': 1.9189731568751627e-06, 'rewards/chosen': -0.7931529879570007, 'rewards/rejected': -1.499078392982483, 'rewards/accuracies': 0.75, 'rewards/margins': 0.705925464630127, 'policy_logps/rejected': -356.9178161621094, 'policy_logps/chosen': -389.02264404296875, 'referece_logps/rejected': -341.927001953125, 'referece_logps/chosen': -381.0910949707031, 'logits/rejected': -1.1033085584640503, 'logits/chosen': -1.1542136669158936, 'epoch': 0.93}


 16%|█▌        | 1669/10740 [8:21:15<42:50:37, 17.00s/it]

 16%|█▌        | 1670/10740 [8:21:29<41:04:55, 16.31s/it]

 16%|█▌        | 1671/10740 [8:21:44<39:32:27, 15.70s/it]
{'loss': 0.546, 'learning_rate': 1.9184968069944645e-06, 'rewards/chosen': -0.6503536701202393, 'rewards/rejected': -1.9469521045684814, 'rewards/accuracies': 0.875, 'rewards/margins': 1.296598196029663, 'policy_logps/rejected': -313.66998291015625, 'policy_logps/chosen': -302.67779541015625, 'referece_logps/rejected': -294.2004699707031, 'referece_logps/chosen': -296.1742248535156, 'logits/rejected': -1.572703242301941, 'logits/chosen': -1.588428020477295, 'epoch': 0.93}

 16%|█▌        | 1672/10740 [8:22:02<41:41:33, 16.55s/it]

 16%|█▌        | 1673/10740 [8:22:20<42:57:04, 17.05s/it]


 16%|█▌        | 1675/10740 [8:22:45<36:39:06, 14.56s/it]

 16%|█▌        | 1676/10740 [8:23:04<39:32:05, 15.70s/it]

 16%|█▌        | 1677/10740 [8:23:19<39:05:52, 15.53s/it]
{'loss': 0.5726, 'learning_rate': 1.9177797762047456e-06, 'rewards/chosen': -1.0780060291290283, 'rewards/rejected': -1.424870252609253, 'rewards/accuracies': 0.625, 'rewards/margins': 0.34686434268951416, 'policy_logps/rejected': -385.6968078613281, 'policy_logps/chosen': -320.4205627441406, 'referece_logps/rejected': -371.4481201171875, 'referece_logps/chosen': -309.6405029296875, 'logits/rejected': -1.8223421573638916, 'logits/chosen': -1.8105672597885132, 'epoch': 0.94}


 16%|█▌        | 1679/10740 [8:23:57<44:02:49, 17.50s/it]

 16%|█▌        | 1680/10740 [8:24:18<46:29:37, 18.47s/it]
{'loss': 0.6286, 'learning_rate': 1.917420133759726e-06, 'rewards/chosen': -1.610586404800415, 'rewards/rejected': -2.4980432987213135, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8874568939208984, 'policy_logps/rejected': -375.30157470703125, 'policy_logps/chosen': -335.3115234375, 'referece_logps/rejected': -350.3211364746094, 'referece_logps/chosen': -319.20562744140625, 'logits/rejected': -0.8215834498405457, 'logits/chosen': -0.7965282201766968, 'epoch': 0.94}


 16%|█▌        | 1682/10740 [8:24:50<42:58:30, 17.08s/it]

 16%|█▌        | 1683/10740 [8:25:10<44:53:47, 17.85s/it]
{'loss': 0.5203, 'learning_rate': 1.9170597403402526e-06, 'rewards/chosen': -0.9465798735618591, 'rewards/rejected': -1.0828207731246948, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1362408548593521, 'policy_logps/rejected': -333.5202331542969, 'policy_logps/chosen': -282.3251953125, 'referece_logps/rejected': -322.6920166015625, 'referece_logps/chosen': -272.8594055175781, 'logits/rejected': -1.2138928174972534, 'logits/chosen': -1.1837104558944702, 'epoch': 0.94}

 16%|█▌        | 1684/10740 [8:25:26<44:02:04, 17.50s/it]


 16%|█▌        | 1686/10740 [8:26:04<45:09:05, 17.95s/it]

 16%|█▌        | 1687/10740 [8:26:21<45:06:32, 17.94s/it]

 16%|█▌        | 1688/10740 [8:26:44<48:16:57, 19.20s/it]

 16%|█▌        | 1689/10740 [8:26:57<43:55:49, 17.47s/it]

 16%|█▌        | 1690/10740 [8:27:18<46:17:35, 18.41s/it]
{'loss': 0.5293, 'learning_rate': 1.916215903563565e-06, 'rewards/chosen': -0.763679027557373, 'rewards/rejected': -2.1904919147491455, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4268128871917725, 'policy_logps/rejected': -370.7221984863281, 'policy_logps/chosen': -446.2313232421875, 'referece_logps/rejected': -348.8172912597656, 'referece_logps/chosen': -438.59454345703125, 'logits/rejected': -0.4099135398864746, 'logits/chosen': -0.6117174625396729, 'epoch': 0.94}


 16%|█▌        | 1692/10740 [8:27:52<44:54:38, 17.87s/it]

 16%|█▌        | 1693/10740 [8:28:13<47:34:05, 18.93s/it]

 16%|█▌        | 1694/10740 [8:28:31<46:54:15, 18.67s/it]

 16%|█▌        | 1695/10740 [8:28:52<48:37:55, 19.36s/it]

 16%|█▌        | 1696/10740 [8:29:07<45:19:42, 18.04s/it]

 16%|█▌        | 1697/10740 [8:29:19<41:00:32, 16.33s/it]
{'loss': 0.5082, 'learning_rate': 1.9153679835162775e-06, 'rewards/chosen': -1.0482949018478394, 'rewards/rejected': -1.3447730541229248, 'rewards/accuracies': 0.625, 'rewards/margins': 0.29647815227508545, 'policy_logps/rejected': -382.4992980957031, 'policy_logps/chosen': -407.8833923339844, 'referece_logps/rejected': -369.05157470703125, 'referece_logps/chosen': -397.400390625, 'logits/rejected': -0.9153305292129517, 'logits/chosen': -1.01999831199646, 'epoch': 0.95}

 16%|█▌        | 1698/10740 [8:29:35<40:13:04, 16.01s/it]

 16%|█▌        | 1699/10740 [8:29:51<40:30:59, 16.13s/it]

 16%|█▌        | 1700/10740 [8:30:10<42:56:41, 17.10s/it]


 16%|█▌        | 1702/10740 [8:30:44<42:39:21, 16.99s/it]

 16%|█▌        | 1703/10740 [8:30:58<40:35:35, 16.17s/it]

 16%|█▌        | 1704/10740 [8:31:20<44:26:28, 17.71s/it]
{'loss': 0.5626, 'learning_rate': 1.9145159839772885e-06, 'rewards/chosen': -1.273718237876892, 'rewards/rejected': -2.079758882522583, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8060407042503357, 'policy_logps/rejected': -422.4083251953125, 'policy_logps/chosen': -320.4946594238281, 'referece_logps/rejected': -401.6107177734375, 'referece_logps/chosen': -307.7574768066406, 'logits/rejected': -0.26453229784965515, 'logits/chosen': -0.20414140820503235, 'epoch': 0.95}

 16%|█▌        | 1705/10740 [8:31:40<46:49:43, 18.66s/it]
[2024-04-02 03:45:44,200] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 16%|█▌        | 1707/10740 [8:32:12<41:48:58, 16.67s/it]

 16%|█▌        | 1708/10740 [8:32:32<44:53:25, 17.89s/it]

 16%|█▌        | 1709/10740 [8:32:52<45:56:48, 18.32s/it]
{'loss': 0.4268, 'learning_rate': 1.91390491733265e-06, 'rewards/chosen': -1.059861421585083, 'rewards/rejected': -2.052539110183716, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9926778674125671, 'policy_logps/rejected': -500.1893005371094, 'policy_logps/chosen': -549.228515625, 'referece_logps/rejected': -479.66387939453125, 'referece_logps/chosen': -538.6299438476562, 'logits/rejected': -0.3642948567867279, 'logits/chosen': -0.3681989312171936, 'epoch': 0.95}


 16%|█▌        | 1711/10740 [8:33:28<46:21:10, 18.48s/it]

 16%|█▌        | 1712/10740 [8:33:50<48:56:55, 19.52s/it]

 16%|█▌        | 1713/10740 [8:34:02<43:17:19, 17.26s/it]

 16%|█▌        | 1714/10740 [8:34:22<45:03:26, 17.97s/it]

 16%|█▌        | 1715/10740 [8:34:40<45:17:39, 18.07s/it]

 16%|█▌        | 1716/10740 [8:34:54<42:09:40, 16.82s/it]
{'loss': 0.4064, 'learning_rate': 1.9130459332234005e-06, 'rewards/chosen': -1.152172565460205, 'rewards/rejected': -2.0311286449432373, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8789563775062561, 'policy_logps/rejected': -425.14959716796875, 'policy_logps/chosen': -384.3050842285156, 'referece_logps/rejected': -404.83831787109375, 'referece_logps/chosen': -372.78338623046875, 'logits/rejected': -0.9171956777572632, 'logits/chosen': -0.8401163816452026, 'epoch': 0.96}


 16%|█▌        | 1718/10740 [8:35:30<44:07:49, 17.61s/it]
{'loss': 0.5713, 'learning_rate': 1.9127997616306893e-06, 'rewards/chosen': -0.5267607569694519, 'rewards/rejected': -1.7396481037139893, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2128872871398926, 'policy_logps/rejected': -328.1242980957031, 'policy_logps/chosen': -363.62310791015625, 'referece_logps/rejected': -310.7278137207031, 'referece_logps/chosen': -358.35546875, 'logits/rejected': -1.2462581396102905, 'logits/chosen': -1.323089361190796, 'epoch': 0.96}


 16%|█▌        | 1720/10740 [8:36:04<42:46:11, 17.07s/it]

 16%|█▌        | 1721/10740 [8:36:24<45:06:23, 18.00s/it]
{'loss': 0.4911, 'learning_rate': 1.9124298816093305e-06, 'rewards/chosen': -0.7132849097251892, 'rewards/rejected': -2.590402126312256, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8771171569824219, 'policy_logps/rejected': -463.7352294921875, 'policy_logps/chosen': -481.5877380371094, 'referece_logps/rejected': -437.83123779296875, 'referece_logps/chosen': -474.4548645019531, 'logits/rejected': -0.037173930555582047, 'logits/chosen': 0.08458445221185684, 'epoch': 0.96}

 16%|█▌        | 1722/10740 [8:36:45<47:48:22, 19.08s/it]


 16%|█▌        | 1724/10740 [8:37:23<47:30:38, 18.97s/it]

 16%|█▌        | 1725/10740 [8:37:44<49:06:37, 19.61s/it]

 16%|█▌        | 1726/10740 [8:38:00<46:10:15, 18.44s/it]

 16%|█▌        | 1727/10740 [8:38:12<41:26:12, 16.55s/it]

 16%|█▌        | 1728/10740 [8:38:28<41:31:32, 16.59s/it]

 16%|█▌        | 1729/10740 [8:38:50<45:11:45, 18.06s/it]

 16%|█▌        | 1730/10740 [8:39:07<44:24:10, 17.74s/it]

 16%|█▌        | 1731/10740 [8:39:21<41:23:54, 16.54s/it]

 16%|█▌        | 1732/10740 [8:39:34<39:20:10, 15.72s/it]
{'loss': 0.6225, 'learning_rate': 1.9110672671182697e-06, 'rewards/chosen': -1.2953494787216187, 'rewards/rejected': -2.046846866607666, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7514972686767578, 'policy_logps/rejected': -340.7989807128906, 'policy_logps/chosen': -268.2970275878906, 'referece_logps/rejected': -320.33056640625, 'referece_logps/chosen': -255.34352111816406, 'logits/rejected': -0.7014771103858948, 'logits/chosen': -0.677508533000946, 'epoch': 0.97}


 16%|█▌        | 1734/10740 [8:40:14<44:35:15, 17.82s/it]

 16%|█▌        | 1735/10740 [8:40:34<46:04:46, 18.42s/it]

 16%|█▌        | 1736/10740 [8:40:56<48:50:17, 19.53s/it]

 16%|█▌        | 1737/10740 [8:41:10<44:37:38, 17.85s/it]

 16%|█▌        | 1738/10740 [8:41:28<44:53:37, 17.95s/it]
{'loss': 0.4949, 'learning_rate': 1.9103197958381955e-06, 'rewards/chosen': -1.0270557403564453, 'rewards/rejected': -1.7548359632492065, 'rewards/accuracies': 0.375, 'rewards/margins': 0.7277800440788269, 'policy_logps/rejected': -348.9615478515625, 'policy_logps/chosen': -466.3229675292969, 'referece_logps/rejected': -331.4132080078125, 'referece_logps/chosen': -456.0523681640625, 'logits/rejected': 0.19184641540050507, 'logits/chosen': 0.20678997039794922, 'epoch': 0.97}


 16%|█▌        | 1740/10740 [8:42:01<42:47:05, 17.11s/it]

 16%|█▌        | 1741/10740 [8:42:14<40:04:24, 16.03s/it]

 16%|█▌        | 1742/10740 [8:42:34<42:48:59, 17.13s/it]

 16%|█▌        | 1743/10740 [8:42:52<43:51:28, 17.55s/it]

 16%|█▌        | 1744/10740 [8:43:05<40:03:24, 16.03s/it]

 16%|█▌        | 1745/10740 [8:43:25<42:57:50, 17.20s/it]
{'loss': 0.5827, 'learning_rate': 1.9094439789751187e-06, 'rewards/chosen': -1.6782419681549072, 'rewards/rejected': -2.4517524242401123, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7735102772712708, 'policy_logps/rejected': -402.9132080078125, 'policy_logps/chosen': -521.5131225585938, 'referece_logps/rejected': -378.3956604003906, 'referece_logps/chosen': -504.7306823730469, 'logits/rejected': -0.7001015543937683, 'logits/chosen': -0.6643612384796143, 'epoch': 0.97}


 16%|█▋        | 1747/10740 [8:43:49<35:59:53, 14.41s/it]

 16%|█▋        | 1748/10740 [8:44:06<38:14:59, 15.31s/it]

 16%|█▋        | 1749/10740 [8:44:25<40:48:46, 16.34s/it]

 16%|█▋        | 1750/10740 [8:44:39<39:10:12, 15.69s/it]

 16%|█▋        | 1751/10740 [8:44:59<42:31:05, 17.03s/it]

 16%|█▋        | 1752/10740 [8:45:11<38:35:36, 15.46s/it]

 16%|█▋        | 1753/10740 [8:45:27<38:51:38, 15.57s/it]

 16%|█▋        | 1754/10740 [8:45:43<39:25:33, 15.79s/it]
{'loss': 0.5045, 'learning_rate': 1.9083119737095224e-06, 'rewards/chosen': -0.8866880536079407, 'rewards/rejected': -1.9269973039627075, 'rewards/accuracies': 0.875, 'rewards/margins': 1.040309190750122, 'policy_logps/rejected': -378.3883056640625, 'policy_logps/chosen': -292.9303283691406, 'referece_logps/rejected': -359.11834716796875, 'referece_logps/chosen': -284.0634460449219, 'logits/rejected': -0.21608442068099976, 'logits/chosen': -0.1552577167749405, 'epoch': 0.98}


 16%|█▋        | 1756/10740 [8:46:21<43:12:30, 17.31s/it]

 16%|█▋        | 1757/10740 [8:46:33<39:38:38, 15.89s/it]
{'loss': 0.5546, 'learning_rate': 1.907933151171009e-06, 'rewards/chosen': -0.5706606507301331, 'rewards/rejected': -2.002854585647583, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4321939945220947, 'policy_logps/rejected': -265.4165954589844, 'policy_logps/chosen': -279.0014953613281, 'referece_logps/rejected': -245.3880615234375, 'referece_logps/chosen': -273.2948913574219, 'logits/rejected': -0.6473392844200134, 'logits/chosen': -0.8152589201927185, 'epoch': 0.98}


 16%|█▋        | 1759/10740 [8:47:07<40:40:31, 16.30s/it]

 16%|█▋        | 1760/10740 [8:47:24<41:48:34, 16.76s/it]

 16%|█▋        | 1761/10740 [8:47:41<41:59:34, 16.84s/it]

 16%|█▋        | 1762/10740 [8:48:03<45:26:43, 18.22s/it]

 16%|█▋        | 1763/10740 [8:48:25<48:21:43, 19.39s/it]

 16%|█▋        | 1764/10740 [8:48:43<47:03:40, 18.87s/it]
{'loss': 0.4852, 'learning_rate': 1.9070463421956192e-06, 'rewards/chosen': -1.0610969066619873, 'rewards/rejected': -2.488455057144165, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4273580312728882, 'policy_logps/rejected': -547.86181640625, 'policy_logps/chosen': -479.3455505371094, 'referece_logps/rejected': -522.977294921875, 'referece_logps/chosen': -468.7345886230469, 'logits/rejected': -0.051797062158584595, 'logits/chosen': -0.08306902647018433, 'epoch': 0.99}


 16%|█▋        | 1766/10740 [8:49:18<45:05:08, 18.09s/it]

 16%|█▋        | 1767/10740 [8:49:35<44:01:39, 17.66s/it]

 16%|█▋        | 1768/10740 [8:49:49<41:03:19, 16.47s/it]

 16%|█▋        | 1769/10740 [8:50:07<42:14:57, 16.95s/it]

 16%|█▋        | 1770/10740 [8:50:27<44:22:25, 17.81s/it]

 16%|█▋        | 1771/10740 [8:50:41<42:02:20, 16.87s/it]
{'loss': 0.5938, 'learning_rate': 1.9061554908153257e-06, 'rewards/chosen': -0.666543185710907, 'rewards/rejected': -1.451960802078247, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7854175567626953, 'policy_logps/rejected': -308.27349853515625, 'policy_logps/chosen': -266.09808349609375, 'referece_logps/rejected': -293.75390625, 'referece_logps/chosen': -259.4326477050781, 'logits/rejected': -1.3158106803894043, 'logits/chosen': -1.2803698778152466, 'epoch': 0.99}


 17%|█▋        | 1773/10740 [8:51:05<36:00:31, 14.46s/it]
{'loss': 0.5442, 'learning_rate': 1.9059002199221447e-06, 'rewards/chosen': -0.7081655859947205, 'rewards/rejected': -1.99871027469635, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2905447483062744, 'policy_logps/rejected': -294.5824890136719, 'policy_logps/chosen': -311.0290222167969, 'referece_logps/rejected': -274.5953674316406, 'referece_logps/chosen': -303.9473571777344, 'logits/rejected': -1.2268552780151367, 'logits/chosen': -1.4456042051315308, 'epoch': 0.99}

 17%|█▋        | 1774/10740 [8:51:24<39:17:00, 15.77s/it]

 17%|█▋        | 1775/10740 [8:51:42<41:06:41, 16.51s/it]


 17%|█▋        | 1777/10740 [8:52:20<44:01:13, 17.68s/it]
{'loss': 0.4717, 'learning_rate': 1.9053886895026357e-06, 'rewards/chosen': -0.8640465140342712, 'rewards/rejected': -2.1179254055023193, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2538789510726929, 'policy_logps/rejected': -283.57763671875, 'policy_logps/chosen': -260.6316833496094, 'referece_logps/rejected': -262.39837646484375, 'referece_logps/chosen': -251.99122619628906, 'logits/rejected': -0.6094602942466736, 'logits/chosen': -0.6820699572563171, 'epoch': 0.99}

 17%|█▋        | 1778/10740 [8:52:36<43:02:18, 17.29s/it]


 17%|█▋        | 1780/10740 [8:53:13<45:29:32, 18.28s/it]

 17%|█▋        | 1781/10740 [8:53:34<47:07:03, 18.93s/it]
{'loss': 0.43, 'learning_rate': 1.9048758415261287e-06, 'rewards/chosen': -0.7188382744789124, 'rewards/rejected': -2.7721989154815674, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0533607006073, 'policy_logps/rejected': -280.1195068359375, 'policy_logps/chosen': -281.9991149902344, 'referece_logps/rejected': -252.3975067138672, 'referece_logps/chosen': -274.8106994628906, 'logits/rejected': -0.7664042115211487, 'logits/chosen': -0.8087131977081299, 'epoch': 0.99}


 17%|█▋        | 1783/10740 [8:54:04<41:36:25, 16.72s/it]
{'loss': 0.4362, 'learning_rate': 1.9046189236871505e-06, 'rewards/chosen': -0.9168353080749512, 'rewards/rejected': -2.180940866470337, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2641055583953857, 'policy_logps/rejected': -275.9672546386719, 'policy_logps/chosen': -246.86155700683594, 'referece_logps/rejected': -254.1578369140625, 'referece_logps/chosen': -237.6931915283203, 'logits/rejected': -1.1607671976089478, 'logits/chosen': -1.1376854181289673, 'epoch': 1.0}

 17%|█▋        | 1784/10740 [8:54:25<44:58:49, 18.08s/it]

 17%|█▋        | 1785/10740 [8:54:44<46:06:54, 18.54s/it]


 17%|█▋        | 1787/10740 [8:55:26<48:49:01, 19.63s/it]

 17%|█▋        | 1788/10740 [8:55:46<49:04:29, 19.74s/it]

 17%|█▋        | 1789/10740 [8:56:00<45:04:27, 18.13s/it]
{'loss': 0.5112, 'learning_rate': 1.9038461958893052e-06, 'rewards/chosen': -0.6624706387519836, 'rewards/rejected': -1.08381187915802, 'rewards/accuracies': 0.875, 'rewards/margins': 0.421341210603714, 'policy_logps/rejected': -457.18414306640625, 'policy_logps/chosen': -422.8976135253906, 'referece_logps/rejected': -446.3460388183594, 'referece_logps/chosen': -416.27294921875, 'logits/rejected': 0.08024203777313232, 'logits/chosen': 0.04680490493774414, 'epoch': 1.0}

 17%|█▋        | 1790/10740 [8:56:23<48:51:15, 19.65s/it]

 17%|█▋        | 1791/10740 [8:56:44<50:04:38, 20.15s/it]

 17%|█▋        | 1792/10740 [8:56:59<46:07:34, 18.56s/it]


 17%|█▋        | 1794/10740 [8:57:39<47:46:10, 19.22s/it]
{'loss': 0.5123, 'learning_rate': 1.9031999952574278e-06, 'rewards/chosen': -1.1316969394683838, 'rewards/rejected': -1.642916202545166, 'rewards/accuracies': 0.625, 'rewards/margins': 0.511219322681427, 'policy_logps/rejected': -347.33367919921875, 'policy_logps/chosen': -382.169189453125, 'referece_logps/rejected': -330.904541015625, 'referece_logps/chosen': -370.85223388671875, 'logits/rejected': -0.27640044689178467, 'logits/chosen': -0.4091808497905731, 'epoch': 1.0}

 17%|█▋        | 1795/10740 [8:58:01<49:14:40, 19.82s/it]

 17%|█▋        | 1796/10740 [8:58:19<48:20:45, 19.46s/it]


 17%|█▋        | 1798/10740 [8:58:50<43:29:33, 17.51s/it]
{'loss': 0.5606, 'learning_rate': 1.9026815560129268e-06, 'rewards/chosen': -1.309301733970642, 'rewards/rejected': -2.9932236671447754, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6839219331741333, 'policy_logps/rejected': -324.0773620605469, 'policy_logps/chosen': -233.78921508789062, 'referece_logps/rejected': -294.1451110839844, 'referece_logps/chosen': -220.69619750976562, 'logits/rejected': -0.8152490854263306, 'logits/chosen': -0.8880876302719116, 'epoch': 1.0}


 17%|█▋        | 1800/10740 [8:59:24<42:31:04, 17.12s/it]
{'loss': 0.5402, 'learning_rate': 1.9024218437368966e-06, 'rewards/chosen': -0.6625527143478394, 'rewards/rejected': -0.8218162059783936, 'rewards/accuracies': 0.75, 'rewards/margins': 0.15926343202590942, 'policy_logps/rejected': -365.1839294433594, 'policy_logps/chosen': -379.1681823730469, 'referece_logps/rejected': -356.96575927734375, 'referece_logps/chosen': -372.54266357421875, 'logits/rejected': -0.5718071460723877, 'logits/chosen': -0.5650086998939514, 'epoch': 1.01}


 17%|█▋        | 1802/10740 [9:00:04<46:18:47, 18.65s/it]
{'loss': 0.4453, 'learning_rate': 1.902161803150955e-06, 'rewards/chosen': -1.4215190410614014, 'rewards/rejected': -2.3741936683654785, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9526748657226562, 'policy_logps/rejected': -452.6142578125, 'policy_logps/chosen': -415.3532409667969, 'referece_logps/rejected': -428.872314453125, 'referece_logps/chosen': -401.13800048828125, 'logits/rejected': -0.5668087005615234, 'logits/chosen': -0.6016438603401184, 'epoch': 1.01}

 17%|█▋        | 1803/10740 [9:00:17<42:05:13, 16.95s/it]


 17%|█▋        | 1805/10740 [9:00:50<42:20:57, 17.06s/it]
{'loss': 0.5115, 'learning_rate': 1.9017711268979392e-06, 'rewards/chosen': -1.1355633735656738, 'rewards/rejected': -2.4446542263031006, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3090909719467163, 'policy_logps/rejected': -400.4934387207031, 'policy_logps/chosen': -424.0435485839844, 'referece_logps/rejected': -376.04693603515625, 'referece_logps/chosen': -412.6879577636719, 'logits/rejected': 0.4660726487636566, 'logits/chosen': 0.43225234746932983, 'epoch': 1.01}

 17%|█▋        | 1806/10740 [9:01:06<41:59:35, 16.92s/it]

 17%|█▋        | 1807/10740 [9:01:27<44:23:05, 17.89s/it]

 17%|█▋        | 1808/10740 [9:01:39<40:33:56, 16.35s/it]

 17%|█▋        | 1809/10740 [9:02:00<44:03:15, 17.76s/it]


 17%|█▋        | 1811/10740 [9:02:34<41:59:06, 16.93s/it]
{'loss': 0.5855, 'learning_rate': 1.9009875602184647e-06, 'rewards/chosen': -0.8354765772819519, 'rewards/rejected': -1.5274090766906738, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6919326782226562, 'policy_logps/rejected': -515.7857055664062, 'policy_logps/chosen': -321.7571716308594, 'referece_logps/rejected': -500.51165771484375, 'referece_logps/chosen': -313.40240478515625, 'logits/rejected': -0.4971749186515808, 'logits/chosen': -0.6143507957458496, 'epoch': 1.01}


 17%|█▋        | 1813/10740 [9:03:06<40:32:12, 16.35s/it]
{'loss': 0.4458, 'learning_rate': 1.900725715622279e-06, 'rewards/chosen': -0.9675718545913696, 'rewards/rejected': -1.2801693677902222, 'rewards/accuracies': 0.375, 'rewards/margins': 0.31259751319885254, 'policy_logps/rejected': -404.0538330078125, 'policy_logps/chosen': -387.9745788574219, 'referece_logps/rejected': -391.25213623046875, 'referece_logps/chosen': -378.2988586425781, 'logits/rejected': -0.6878385543823242, 'logits/chosen': -0.8227844834327698, 'epoch': 1.01}


 17%|█▋        | 1815/10740 [9:03:36<38:07:28, 15.38s/it]

 17%|█▋        | 1816/10740 [9:03:48<35:29:58, 14.32s/it]

 17%|█▋        | 1817/10740 [9:04:06<38:14:47, 15.43s/it]

 17%|█▋        | 1818/10740 [9:04:22<39:01:10, 15.74s/it]
{'loss': 0.5669, 'learning_rate': 1.9000696706843015e-06, 'rewards/chosen': -1.6568102836608887, 'rewards/rejected': -2.238229990005493, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5814197659492493, 'policy_logps/rejected': -450.77716064453125, 'policy_logps/chosen': -501.48956298828125, 'referece_logps/rejected': -428.39483642578125, 'referece_logps/chosen': -484.9214172363281, 'logits/rejected': -0.8321609497070312, 'logits/chosen': -0.8418943881988525, 'epoch': 1.02}


 17%|█▋        | 1820/10740 [9:04:56<41:14:48, 16.65s/it]

 17%|█▋        | 1821/10740 [9:05:12<41:00:33, 16.55s/it]
{'loss': 0.5398, 'learning_rate': 1.8996750612635031e-06, 'rewards/chosen': -1.183171033859253, 'rewards/rejected': -1.712920904159546, 'rewards/accuracies': 0.875, 'rewards/margins': 0.529749870300293, 'policy_logps/rejected': -297.53302001953125, 'policy_logps/chosen': -429.3807067871094, 'referece_logps/rejected': -280.40380859375, 'referece_logps/chosen': -417.5490417480469, 'logits/rejected': -0.8721503615379333, 'logits/chosen': -1.0351741313934326, 'epoch': 1.02}


 17%|█▋        | 1823/10740 [9:05:48<42:24:09, 17.12s/it]

 17%|█▋        | 1824/10740 [9:06:08<44:34:36, 18.00s/it]
{'loss': 0.3849, 'learning_rate': 1.8992797153938707e-06, 'rewards/chosen': -0.7372815608978271, 'rewards/rejected': -2.5620315074920654, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8247499465942383, 'policy_logps/rejected': -386.48541259765625, 'policy_logps/chosen': -380.0633239746094, 'referece_logps/rejected': -360.8651428222656, 'referece_logps/chosen': -372.69049072265625, 'logits/rejected': 0.4241396188735962, 'logits/chosen': 0.36736342310905457, 'epoch': 1.02}

 17%|█▋        | 1825/10740 [9:06:29<46:40:17, 18.85s/it]


 17%|█▋        | 1827/10740 [9:07:00<42:16:10, 17.07s/it]

 17%|█▋        | 1828/10740 [9:07:22<45:43:06, 18.47s/it]
{'loss': 0.5262, 'learning_rate': 1.898751442540047e-06, 'rewards/chosen': -0.952319324016571, 'rewards/rejected': -2.9223544597625732, 'rewards/accuracies': 0.875, 'rewards/margins': 1.970035195350647, 'policy_logps/rejected': -345.0320129394531, 'policy_logps/chosen': -268.10302734375, 'referece_logps/rejected': -315.8084716796875, 'referece_logps/chosen': -258.5798034667969, 'logits/rejected': -0.9701751470565796, 'logits/chosen': -1.0908639430999756, 'epoch': 1.02}


 17%|█▋        | 1830/10740 [9:07:54<43:25:25, 17.54s/it]
{'loss': 0.4668, 'learning_rate': 1.898486815603182e-06, 'rewards/chosen': -1.5220279693603516, 'rewards/rejected': -2.1491539478302, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6271260976791382, 'policy_logps/rejected': -591.5474853515625, 'policy_logps/chosen': -520.9092407226562, 'referece_logps/rejected': -570.0559692382812, 'referece_logps/chosen': -505.68896484375, 'logits/rejected': 0.08047573268413544, 'logits/chosen': 0.044002264738082886, 'epoch': 1.02}

 17%|█▋        | 1831/10740 [9:08:10<41:50:25, 16.91s/it]

 17%|█▋        | 1832/10740 [9:08:30<43:56:25, 17.76s/it]

 17%|█▋        | 1833/10740 [9:08:43<40:32:32, 16.39s/it]

 17%|█▋        | 1834/10740 [9:09:04<43:55:39, 17.76s/it]


 17%|█▋        | 1836/10740 [9:09:37<42:26:21, 17.16s/it]
{'loss': 0.4867, 'learning_rate': 1.897690973908417e-06, 'rewards/chosen': -1.3157715797424316, 'rewards/rejected': -1.1545641422271729, 'rewards/accuracies': 0.5, 'rewards/margins': -0.1612074077129364, 'policy_logps/rejected': -276.48382568359375, 'policy_logps/chosen': -312.9519958496094, 'referece_logps/rejected': -264.93817138671875, 'referece_logps/chosen': -299.7943115234375, 'logits/rejected': -0.5405832529067993, 'logits/chosen': -0.5950197577476501, 'epoch': 1.03}

 17%|█▋        | 1837/10740 [9:09:56<43:57:32, 17.78s/it]

 17%|█▋        | 1838/10740 [9:10:17<46:33:03, 18.83s/it]


 17%|█▋        | 1840/10740 [9:10:53<45:11:47, 18.28s/it]
{'loss': 0.5902, 'learning_rate': 1.8971587796738461e-06, 'rewards/chosen': -1.2852855920791626, 'rewards/rejected': -1.9921770095825195, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7068914175033569, 'policy_logps/rejected': -502.60931396484375, 'policy_logps/chosen': -427.056640625, 'referece_logps/rejected': -482.6875305175781, 'referece_logps/chosen': -414.2037048339844, 'logits/rejected': -0.7621409893035889, 'logits/chosen': -1.0531214475631714, 'epoch': 1.03}


 17%|█▋        | 1842/10740 [9:11:15<36:04:10, 14.59s/it]
{'loss': 0.5972, 'learning_rate': 1.8968921929153905e-06, 'rewards/chosen': -1.3673884868621826, 'rewards/rejected': -1.4442559480667114, 'rewards/accuracies': 0.5, 'rewards/margins': 0.07686731219291687, 'policy_logps/rejected': -377.92547607421875, 'policy_logps/chosen': -425.3816833496094, 'referece_logps/rejected': -363.48291015625, 'referece_logps/chosen': -411.70782470703125, 'logits/rejected': -0.7008336186408997, 'logits/chosen': -0.6469473838806152, 'epoch': 1.03}

 17%|█▋        | 1843/10740 [9:11:31<37:21:33, 15.12s/it]


 17%|█▋        | 1845/10740 [9:11:59<35:49:25, 14.50s/it]
{'loss': 0.508, 'learning_rate': 1.896491700998978e-06, 'rewards/chosen': -1.0494358539581299, 'rewards/rejected': -2.8230972290039062, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7736610174179077, 'policy_logps/rejected': -479.29205322265625, 'policy_logps/chosen': -321.8942565917969, 'referece_logps/rejected': -451.06109619140625, 'referece_logps/chosen': -311.3998718261719, 'logits/rejected': -1.0609567165374756, 'logits/chosen': -1.1665669679641724, 'epoch': 1.03}


 17%|█▋        | 1847/10740 [9:12:30<38:12:48, 15.47s/it]
{'loss': 0.3933, 'learning_rate': 1.8962242986772058e-06, 'rewards/chosen': -1.493145227432251, 'rewards/rejected': -2.212846040725708, 'rewards/accuracies': 0.625, 'rewards/margins': 0.719700813293457, 'policy_logps/rejected': -399.0478515625, 'policy_logps/chosen': -485.4753723144531, 'referece_logps/rejected': -376.91943359375, 'referece_logps/chosen': -470.5439758300781, 'logits/rejected': -0.21936586499214172, 'logits/chosen': -0.28550058603286743, 'epoch': 1.03}

 17%|█▋        | 1848/10740 [9:12:46<38:22:51, 15.54s/it]


 17%|█▋        | 1850/10740 [9:13:25<44:02:02, 17.83s/it]
[2024-04-02 04:27:08,489] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 1851/10740 [9:13:47<47:13:24, 19.13s/it]
[2024-04-02 04:27:30,633] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 1852/10740 [9:14:07<47:43:35, 19.33s/it]

 17%|█▋        | 1853/10740 [9:14:19<42:17:11, 17.13s/it]
{'loss': 0.4744, 'learning_rate': 1.8954201357705124e-06, 'rewards/chosen': -0.9388957619667053, 'rewards/rejected': -1.386169672012329, 'rewards/accuracies': 0.75, 'rewards/margins': 0.447273850440979, 'policy_logps/rejected': -211.8055419921875, 'policy_logps/chosen': -190.7631378173828, 'referece_logps/rejected': -197.94383239746094, 'referece_logps/chosen': -181.3741912841797, 'logits/rejected': -1.1875256299972534, 'logits/chosen': -1.3667117357254028, 'epoch': 1.04}


 17%|█▋        | 1855/10740 [9:14:53<42:17:13, 17.13s/it]

 17%|█▋        | 1856/10740 [9:15:07<39:58:34, 16.20s/it]
{'loss': 0.4965, 'learning_rate': 1.8950169547036423e-06, 'rewards/chosen': -1.3347491025924683, 'rewards/rejected': -2.891636371612549, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5568875074386597, 'policy_logps/rejected': -437.7244567871094, 'policy_logps/chosen': -476.71575927734375, 'referece_logps/rejected': -408.80810546875, 'referece_logps/chosen': -463.3682861328125, 'logits/rejected': -0.7010198831558228, 'logits/chosen': -0.7210571765899658, 'epoch': 1.04}

 17%|█▋        | 1857/10740 [9:15:27<43:04:31, 17.46s/it]
[2024-04-02 04:29:31,840] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 1858/10740 [9:15:48<45:35:55, 18.48s/it]
[2024-04-02 04:29:53,458] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 1859/10740 [9:16:10<47:54:50, 19.42s/it]
[2024-04-02 04:30:13,607] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 17%|█▋        | 1861/10740 [9:16:51<49:23:51, 20.03s/it]

 17%|█▋        | 1862/10740 [9:17:09<48:11:58, 19.54s/it]

 17%|█▋        | 1863/10740 [9:17:29<48:23:02, 19.62s/it]

 17%|█▋        | 1864/10740 [9:17:51<50:17:19, 20.40s/it]

 17%|█▋        | 1865/10740 [9:18:09<48:33:21, 19.70s/it]

 17%|█▋        | 1866/10740 [9:18:31<49:54:56, 20.25s/it]
{'loss': 0.3827, 'learning_rate': 1.893667728413621e-06, 'rewards/chosen': -0.7755199670791626, 'rewards/rejected': -1.8479673862457275, 'rewards/accuracies': 0.875, 'rewards/margins': 1.072447419166565, 'policy_logps/rejected': -234.58935546875, 'policy_logps/chosen': -444.4525146484375, 'referece_logps/rejected': -216.1096954345703, 'referece_logps/chosen': -436.6973571777344, 'logits/rejected': -0.8479408025741577, 'logits/chosen': -0.8445099592208862, 'epoch': 1.04}


 17%|█▋        | 1868/10740 [9:19:05<45:54:55, 18.63s/it]
{'loss': 0.4666, 'learning_rate': 1.8933969073870992e-06, 'rewards/chosen': -0.528594434261322, 'rewards/rejected': -1.4912728071212769, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9626782536506653, 'policy_logps/rejected': -259.8641662597656, 'policy_logps/chosen': -244.70132446289062, 'referece_logps/rejected': -244.951416015625, 'referece_logps/chosen': -239.41537475585938, 'logits/rejected': -0.35022395849227905, 'logits/chosen': -0.3111468553543091, 'epoch': 1.04}

 17%|█▋        | 1869/10740 [9:19:18<41:53:59, 17.00s/it]


 17%|█▋        | 1871/10740 [9:19:59<46:07:16, 18.72s/it]

 17%|█▋        | 1872/10740 [9:20:15<44:07:30, 17.91s/it]
{'loss': 0.4971, 'learning_rate': 1.8928542903530466e-06, 'rewards/chosen': -0.8761417269706726, 'rewards/rejected': -2.2229111194610596, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3467694520950317, 'policy_logps/rejected': -281.6523742675781, 'policy_logps/chosen': -348.6767272949219, 'referece_logps/rejected': -259.42327880859375, 'referece_logps/chosen': -339.915283203125, 'logits/rejected': -0.5506267547607422, 'logits/chosen': -0.6017975211143494, 'epoch': 1.05}

 17%|█▋        | 1873/10740 [9:20:34<45:10:47, 18.34s/it]

 17%|█▋        | 1874/10740 [9:20:54<46:04:25, 18.71s/it]

 17%|█▋        | 1875/10740 [9:21:17<49:22:35, 20.05s/it]

 17%|█▋        | 1876/10740 [9:21:35<47:32:55, 19.31s/it]

 17%|█▋        | 1877/10740 [9:21:47<42:25:16, 17.23s/it]

 17%|█▋        | 1878/10740 [9:22:08<45:29:27, 18.48s/it]

 17%|█▋        | 1879/10740 [9:22:30<47:38:41, 19.36s/it]

 18%|█▊        | 1880/10740 [9:22:43<43:27:28, 17.66s/it]

 18%|█▊        | 1881/10740 [9:22:57<40:04:30, 16.29s/it]

 18%|█▊        | 1882/10740 [9:23:13<40:04:04, 16.28s/it]

 18%|█▊        | 1883/10740 [9:23:35<44:28:56, 18.08s/it]

 18%|█▊        | 1884/10740 [9:23:53<44:32:13, 18.10s/it]

 18%|█▊        | 1885/10740 [9:24:13<45:42:47, 18.58s/it]

 18%|█▊        | 1886/10740 [9:24:34<47:45:37, 19.42s/it]

 18%|█▊        | 1887/10740 [9:24:46<42:07:36, 17.13s/it]

 18%|█▊        | 1888/10740 [9:24:57<37:25:08, 15.22s/it]

 18%|█▊        | 1889/10740 [9:25:07<34:03:05, 13.85s/it]

 18%|█▊        | 1890/10740 [9:25:21<33:34:14, 13.66s/it]

 18%|█▊        | 1891/10740 [9:25:35<34:23:34, 13.99s/it]

 18%|█▊        | 1892/10740 [9:25:51<35:35:43, 14.48s/it]

 18%|█▊        | 1893/10740 [9:26:05<35:11:25, 14.32s/it]

 18%|█▊        | 1894/10740 [9:26:26<40:18:16, 16.40s/it]

 18%|█▊        | 1895/10740 [9:26:47<43:45:01, 17.81s/it]

 18%|█▊        | 1896/10740 [9:27:09<46:41:03, 19.00s/it]

 18%|█▊        | 1897/10740 [9:27:26<44:43:36, 18.21s/it]

 18%|█▊        | 1898/10740 [9:27:46<46:04:35, 18.76s/it]

 18%|█▊        | 1899/10740 [9:28:02<44:28:50, 18.11s/it]

 18%|█▊        | 1900/10740 [9:28:23<46:17:06, 18.85s/it]

 18%|█▊        | 1901/10740 [9:28:46<49:22:46, 20.11s/it]

 18%|█▊        | 1902/10740 [9:29:07<50:19:20, 20.50s/it]

 18%|█▊        | 1903/10740 [9:29:20<44:24:16, 18.09s/it]

 18%|█▊        | 1904/10740 [9:29:32<40:24:32, 16.46s/it]

 18%|█▊        | 1905/10740 [9:29:48<39:28:38, 16.09s/it]

 18%|█▊        | 1906/10740 [9:30:07<42:04:45, 17.15s/it]

 18%|█▊        | 1907/10740 [9:30:30<45:57:55, 18.73s/it]

 18%|█▊        | 1908/10740 [9:30:48<45:54:57, 18.72s/it]

 18%|█▊        | 1909/10740 [9:31:10<48:27:40, 19.76s/it]

 18%|█▊        | 1910/10740 [9:31:30<48:28:01, 19.76s/it]

 18%|█▊        | 1911/10740 [9:31:50<48:28:13, 19.76s/it]

 18%|█▊        | 1912/10740 [9:32:10<48:16:13, 19.68s/it]

 18%|█▊        | 1913/10740 [9:32:30<48:42:21, 19.86s/it]

 18%|█▊        | 1914/10740 [9:32:49<48:33:32, 19.81s/it]

 18%|█▊        | 1915/10740 [9:33:12<50:19:16, 20.53s/it]

 18%|█▊        | 1916/10740 [9:33:25<45:16:02, 18.47s/it]

 18%|█▊        | 1917/10740 [9:33:48<47:59:58, 19.58s/it]

 18%|█▊        | 1918/10740 [9:34:09<49:40:32, 20.27s/it]

 18%|█▊        | 1919/10740 [9:34:29<49:19:24, 20.13s/it]

 18%|█▊        | 1920/10740 [9:34:40<42:19:42, 17.28s/it]

 18%|█▊        | 1921/10740 [9:34:55<40:40:57, 16.61s/it]

 18%|█▊        | 1922/10740 [9:35:09<38:36:16, 15.76s/it]

 18%|█▊        | 1923/10740 [9:35:25<39:13:05, 16.01s/it]

 18%|█▊        | 1924/10740 [9:35:47<43:39:35, 17.83s/it]

 18%|█▊        | 1925/10740 [9:36:05<43:54:12, 17.93s/it]

 18%|█▊        | 1926/10740 [9:36:23<43:17:17, 17.68s/it]

 18%|█▊        | 1927/10740 [9:36:41<43:39:17, 17.83s/it]

 18%|█▊        | 1928/10740 [9:36:59<44:17:33, 18.10s/it]

 18%|█▊        | 1929/10740 [9:37:14<41:26:08, 16.93s/it]

 18%|█▊        | 1930/10740 [9:37:33<43:05:20, 17.61s/it]

 18%|█▊        | 1931/10740 [9:37:46<40:02:08, 16.36s/it]

 18%|█▊        | 1932/10740 [9:37:57<35:50:48, 14.65s/it]

 18%|█▊        | 1933/10740 [9:38:13<36:48:11, 15.04s/it]

 18%|█▊        | 1934/10740 [9:38:24<33:44:55, 13.80s/it]

 18%|█▊        | 1935/10740 [9:38:42<36:55:07, 15.09s/it]

 18%|█▊        | 1936/10740 [9:38:58<37:17:34, 15.25s/it]


 18%|█▊        | 1938/10740 [9:39:35<41:03:19, 16.79s/it]
{'loss': 0.5211, 'learning_rate': 1.883714116010585e-06, 'rewards/chosen': -1.250051498413086, 'rewards/rejected': -1.53623366355896, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2861820161342621, 'policy_logps/rejected': -320.5467224121094, 'policy_logps/chosen': -298.2336120605469, 'referece_logps/rejected': -305.18438720703125, 'referece_logps/chosen': -285.73309326171875, 'logits/rejected': 0.1132267564535141, 'logits/chosen': 0.073356494307518, 'epoch': 1.08}

 18%|█▊        | 1939/10740 [9:39:51<40:49:45, 16.70s/it]

 18%|█▊        | 1940/10740 [9:40:08<40:54:14, 16.73s/it]

 18%|█▊        | 1941/10740 [9:40:24<40:25:15, 16.54s/it]


 18%|█▊        | 1943/10740 [9:41:01<41:55:20, 17.16s/it]
{'loss': 0.4578, 'learning_rate': 1.8830073659312983e-06, 'rewards/chosen': -1.3997489213943481, 'rewards/rejected': -2.889198064804077, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4894492626190186, 'policy_logps/rejected': -406.334716796875, 'policy_logps/chosen': -302.009765625, 'referece_logps/rejected': -377.4427490234375, 'referece_logps/chosen': -288.0122985839844, 'logits/rejected': -0.010567612946033478, 'logits/chosen': -0.04545867443084717, 'epoch': 1.09}

 18%|█▊        | 1944/10740 [9:41:21<44:38:33, 18.27s/it]

 18%|█▊        | 1945/10740 [9:41:38<43:32:39, 17.82s/it]

 18%|█▊        | 1946/10740 [9:41:58<44:48:25, 18.34s/it]

 18%|█▊        | 1947/10740 [9:42:15<43:55:29, 17.98s/it]


 18%|█▊        | 1949/10740 [9:42:47<41:11:43, 16.87s/it]

 18%|█▊        | 1950/10740 [9:43:04<41:08:25, 16.85s/it]

 18%|█▊        | 1951/10740 [9:43:21<41:03:12, 16.82s/it]

 18%|█▊        | 1952/10740 [9:43:39<42:08:18, 17.26s/it]

 18%|█▊        | 1953/10740 [9:43:51<37:56:01, 15.54s/it]

 18%|█▊        | 1954/10740 [9:44:12<42:00:55, 17.22s/it]

 18%|█▊        | 1955/10740 [9:44:33<45:11:48, 18.52s/it]

 18%|█▊        | 1956/10740 [9:44:54<46:24:07, 19.02s/it]

 18%|█▊        | 1957/10740 [9:45:10<44:44:19, 18.34s/it]

 18%|█▊        | 1958/10740 [9:45:26<42:57:22, 17.61s/it]

 18%|█▊        | 1959/10740 [9:45:46<44:18:40, 18.17s/it]

 18%|█▊        | 1960/10740 [9:45:57<39:15:40, 16.10s/it]

 18%|█▊        | 1961/10740 [9:46:19<43:33:46, 17.86s/it]

 18%|█▊        | 1962/10740 [9:46:36<43:11:54, 17.72s/it]

 18%|█▊        | 1963/10740 [9:46:53<42:43:02, 17.52s/it]

 18%|█▊        | 1964/10740 [9:47:13<44:15:25, 18.15s/it]

 18%|█▊        | 1965/10740 [9:47:31<44:24:13, 18.22s/it]

 18%|█▊        | 1966/10740 [9:47:54<47:28:37, 19.48s/it]

 18%|█▊        | 1967/10740 [9:48:05<41:39:29, 17.09s/it]

 18%|█▊        | 1968/10740 [9:48:24<42:45:03, 17.54s/it]

 18%|█▊        | 1969/10740 [9:48:40<41:59:04, 17.23s/it]

 18%|█▊        | 1970/10740 [9:49:01<44:41:39, 18.35s/it]

 18%|█▊        | 1971/10740 [9:49:17<42:27:52, 17.43s/it]

 18%|█▊        | 1972/10740 [9:49:37<44:10:11, 18.14s/it]

 18%|█▊        | 1973/10740 [9:49:55<44:17:25, 18.19s/it]

 18%|█▊        | 1974/10740 [9:50:15<45:51:08, 18.83s/it]

 18%|█▊        | 1975/10740 [9:50:36<47:03:25, 19.33s/it]

 18%|█▊        | 1976/10740 [9:50:58<49:14:15, 20.23s/it]

 18%|█▊        | 1977/10740 [9:51:19<49:52:52, 20.49s/it]

 18%|█▊        | 1978/10740 [9:51:39<49:08:29, 20.19s/it]

 18%|█▊        | 1979/10740 [9:51:58<48:40:34, 20.00s/it]

 18%|█▊        | 1980/10740 [9:52:11<43:28:24, 17.87s/it]

 18%|█▊        | 1981/10740 [9:52:30<44:23:22, 18.24s/it]

 18%|█▊        | 1982/10740 [9:52:41<38:59:54, 16.03s/it]
{'loss': 0.5899, 'learning_rate': 1.8774259337815789e-06, 'rewards/chosen': -1.1463533639907837, 'rewards/rejected': -1.6189745664596558, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4726211726665497, 'policy_logps/rejected': -461.0478515625, 'policy_logps/chosen': -484.2421569824219, 'referece_logps/rejected': -444.85809326171875, 'referece_logps/chosen': -472.7786560058594, 'logits/rejected': -0.2409995049238205, 'logits/chosen': -0.21517068147659302, 'epoch': 1.11}


 18%|█▊        | 1984/10740 [9:53:20<44:09:50, 18.16s/it]

 18%|█▊        | 1985/10740 [9:53:41<46:15:15, 19.02s/it]

 18%|█▊        | 1986/10740 [9:53:53<40:51:55, 16.81s/it]

 19%|█▊        | 1987/10740 [9:54:13<42:50:34, 17.62s/it]

 19%|█▊        | 1988/10740 [9:54:25<39:18:01, 16.17s/it]

 19%|█▊        | 1989/10740 [9:54:41<39:17:21, 16.16s/it]
{'loss': 0.5438, 'learning_rate': 1.8764112673460564e-06, 'rewards/chosen': -1.0017529726028442, 'rewards/rejected': -1.3933550119400024, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3916019797325134, 'policy_logps/rejected': -358.4820556640625, 'policy_logps/chosen': -377.5262451171875, 'referece_logps/rejected': -344.5484924316406, 'referece_logps/chosen': -367.50872802734375, 'logits/rejected': -0.2824605405330658, 'logits/chosen': -0.32714414596557617, 'epoch': 1.11}


 19%|█▊        | 1991/10740 [9:55:10<36:34:37, 15.05s/it]

 19%|█▊        | 1992/10740 [9:55:30<39:45:49, 16.36s/it]

 19%|█▊        | 1993/10740 [9:55:41<35:51:33, 14.76s/it]

 19%|█▊        | 1994/10740 [9:55:53<33:56:00, 13.97s/it]
{'loss': 0.4426, 'learning_rate': 1.8756841139902806e-06, 'rewards/chosen': -1.3014857769012451, 'rewards/rejected': -2.0957601070404053, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7942741513252258, 'policy_logps/rejected': -327.8782653808594, 'policy_logps/chosen': -316.42681884765625, 'referece_logps/rejected': -306.920654296875, 'referece_logps/chosen': -303.4119873046875, 'logits/rejected': -0.8680011630058289, 'logits/chosen': -0.972693681716919, 'epoch': 1.11}


 19%|█▊        | 1996/10740 [9:56:28<38:45:03, 15.95s/it]

 19%|█▊        | 1997/10740 [9:56:47<40:47:59, 16.80s/it]

 19%|█▊        | 1998/10740 [9:57:05<41:28:16, 17.08s/it]

 19%|█▊        | 1999/10740 [9:57:19<39:10:11, 16.13s/it]
{'loss': 0.5198, 'learning_rate': 1.8749549694944242e-06, 'rewards/chosen': -1.2953407764434814, 'rewards/rejected': -1.76959228515625, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4742514491081238, 'policy_logps/rejected': -544.9252319335938, 'policy_logps/chosen': -413.8680725097656, 'referece_logps/rejected': -527.229248046875, 'referece_logps/chosen': -400.9146728515625, 'logits/rejected': -0.01607324182987213, 'logits/chosen': -0.08079483360052109, 'epoch': 1.12}


 19%|█▊        | 2001/10740 [9:58:05<49:59:30, 20.59s/it]
{'loss': 0.5295, 'learning_rate': 1.8746627545480845e-06, 'rewards/chosen': -1.291877031326294, 'rewards/rejected': -2.391357421875, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0994805097579956, 'policy_logps/rejected': -337.8836364746094, 'policy_logps/chosen': -409.8410339355469, 'referece_logps/rejected': -313.97003173828125, 'referece_logps/chosen': -396.92230224609375, 'logits/rejected': -1.5114784240722656, 'logits/chosen': -1.720226764678955, 'epoch': 1.12}

 19%|█▊        | 2002/10740 [9:58:24<48:58:23, 20.18s/it]


 19%|█▊        | 2004/10740 [9:59:01<46:54:28, 19.33s/it]

 19%|█▊        | 2005/10740 [9:59:20<46:54:28, 19.33s/it]

 19%|█▊        | 2006/10740 [9:59:38<45:17:19, 18.67s/it]
{'loss': 0.526, 'learning_rate': 1.8739308252424624e-06, 'rewards/chosen': -1.9717528820037842, 'rewards/rejected': -2.269268751144409, 'rewards/accuracies': 0.5, 'rewards/margins': 0.29751595854759216, 'policy_logps/rejected': -192.709228515625, 'policy_logps/chosen': -247.6111602783203, 'referece_logps/rejected': -170.01654052734375, 'referece_logps/chosen': -227.89364624023438, 'logits/rejected': -0.8009015917778015, 'logits/chosen': -0.8763495683670044, 'epoch': 1.12}


 19%|█▊        | 2008/10740 [10:00:10<42:43:07, 17.61s/it]

 19%|█▊        | 2009/10740 [10:00:27<42:46:36, 17.64s/it]

 19%|█▊        | 2010/10740 [10:00:47<44:09:32, 18.21s/it]

 19%|█▊        | 2011/10740 [10:01:01<41:07:30, 16.96s/it]
{'loss': 0.4288, 'learning_rate': 1.8731969087834067e-06, 'rewards/chosen': -1.7001240253448486, 'rewards/rejected': -3.2901105880737305, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5899865627288818, 'policy_logps/rejected': -569.5643310546875, 'policy_logps/chosen': -517.6640625, 'referece_logps/rejected': -536.6632080078125, 'referece_logps/chosen': -500.6628112792969, 'logits/rejected': 0.0946093201637268, 'logits/chosen': 0.03313577175140381, 'epoch': 1.12}


 19%|█▊        | 2013/10740 [10:01:39<43:52:35, 18.10s/it]

 19%|█▉        | 2014/10740 [10:01:56<42:35:31, 17.57s/it]

 19%|█▉        | 2015/10740 [10:02:11<41:05:43, 16.96s/it]

 19%|█▉        | 2016/10740 [10:02:24<37:46:03, 15.58s/it]

 19%|█▉        | 2017/10740 [10:02:43<40:42:25, 16.80s/it]

 19%|█▉        | 2018/10740 [10:03:01<41:27:59, 17.12s/it]

 19%|█▉        | 2019/10740 [10:03:21<43:25:41, 17.93s/it]

 19%|█▉        | 2020/10740 [10:03:34<39:40:16, 16.38s/it]

 19%|█▉        | 2021/10740 [10:03:53<42:03:05, 17.36s/it]

 19%|█▉        | 2022/10740 [10:04:11<42:35:46, 17.59s/it]

 19%|█▉        | 2023/10740 [10:04:31<44:13:18, 18.26s/it]

 19%|█▉        | 2024/10740 [10:04:51<45:09:27, 18.65s/it]

 19%|█▉        | 2025/10740 [10:05:08<44:06:42, 18.22s/it]

 19%|█▉        | 2026/10740 [10:05:29<46:09:50, 19.07s/it]

 19%|█▉        | 2027/10740 [10:05:47<45:21:46, 18.74s/it]

 19%|█▉        | 2028/10740 [10:06:04<43:58:32, 18.17s/it]

 19%|█▉        | 2029/10740 [10:06:20<42:38:58, 17.63s/it]
{'loss': 0.4499, 'learning_rate': 1.8705383817384945e-06, 'rewards/chosen': -0.3736807107925415, 'rewards/rejected': -1.4688794612884521, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0951987504959106, 'policy_logps/rejected': -309.04052734375, 'policy_logps/chosen': -266.7751770019531, 'referece_logps/rejected': -294.35174560546875, 'referece_logps/chosen': -263.038330078125, 'logits/rejected': -0.5353939533233643, 'logits/chosen': -0.4997040331363678, 'epoch': 1.13}


 19%|█▉        | 2031/10740 [10:06:51<40:05:06, 16.57s/it]

 19%|█▉        | 2032/10740 [10:07:05<38:21:35, 15.86s/it]
{'loss': 0.4462, 'learning_rate': 1.8700927976825463e-06, 'rewards/chosen': -1.2163450717926025, 'rewards/rejected': -2.507699489593506, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2913544178009033, 'policy_logps/rejected': -385.6407470703125, 'policy_logps/chosen': -269.59051513671875, 'referece_logps/rejected': -360.5637512207031, 'referece_logps/chosen': -257.42706298828125, 'logits/rejected': -0.6819208860397339, 'logits/chosen': -0.7383302450180054, 'epoch': 1.14}

 19%|█▉        | 2033/10740 [10:07:25<40:52:13, 16.90s/it]


 19%|█▉        | 2035/10740 [10:07:54<37:22:56, 15.46s/it]

 19%|█▉        | 2036/10740 [10:08:11<38:45:44, 16.03s/it]
{'loss': 0.4298, 'learning_rate': 1.8694975777521088e-06, 'rewards/chosen': -1.3552765846252441, 'rewards/rejected': -2.2068512439727783, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8515747785568237, 'policy_logps/rejected': -451.88775634765625, 'policy_logps/chosen': -487.019287109375, 'referece_logps/rejected': -429.8192138671875, 'referece_logps/chosen': -473.46649169921875, 'logits/rejected': 0.12030217051506042, 'logits/chosen': 0.12437454611063004, 'epoch': 1.14}


 19%|█▉        | 2038/10740 [10:08:52<44:13:50, 18.30s/it]
{'loss': 0.4767, 'learning_rate': 1.8691994932351126e-06, 'rewards/chosen': -1.272344946861267, 'rewards/rejected': -2.404663562774658, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1323186159133911, 'policy_logps/rejected': -281.7613220214844, 'policy_logps/chosen': -344.59539794921875, 'referece_logps/rejected': -257.7146911621094, 'referece_logps/chosen': -331.8719482421875, 'logits/rejected': -1.006874918937683, 'logits/chosen': -1.0864677429199219, 'epoch': 1.14}


 19%|█▉        | 2040/10740 [10:09:21<40:04:42, 16.58s/it]

 19%|█▉        | 2041/10740 [10:09:34<37:33:29, 15.54s/it]

 19%|█▉        | 2042/10740 [10:09:53<40:02:46, 16.57s/it]

 19%|█▉        | 2043/10740 [10:10:14<43:07:52, 17.85s/it]

 19%|█▉        | 2044/10740 [10:10:33<44:08:38, 18.27s/it]

 19%|█▉        | 2045/10740 [10:10:56<47:10:00, 19.53s/it]

 19%|█▉        | 2046/10740 [10:11:16<47:18:36, 19.59s/it]

 19%|█▉        | 2047/10740 [10:11:30<43:57:13, 18.20s/it]

 19%|█▉        | 2048/10740 [10:11:50<44:59:36, 18.64s/it]

 19%|█▉        | 2049/10740 [10:12:10<45:41:37, 18.93s/it]

 19%|█▉        | 2050/10740 [10:12:30<46:34:39, 19.30s/it]

 19%|█▉        | 2051/10740 [10:12:48<45:58:40, 19.05s/it]

 19%|█▉        | 2052/10740 [10:13:08<46:34:54, 19.30s/it]

 19%|█▉        | 2053/10740 [10:13:21<41:37:57, 17.25s/it]

 19%|█▉        | 2054/10740 [10:13:38<41:43:23, 17.29s/it]

 19%|█▉        | 2055/10740 [10:13:56<42:22:36, 17.57s/it]

 19%|█▉        | 2056/10740 [10:14:18<45:30:38, 18.87s/it]

 19%|█▉        | 2057/10740 [10:14:40<47:41:07, 19.77s/it]

 19%|█▉        | 2058/10740 [10:14:58<46:04:24, 19.10s/it]

 19%|█▉        | 2059/10740 [10:15:17<46:36:15, 19.33s/it]

 19%|█▉        | 2060/10740 [10:15:37<46:40:52, 19.36s/it]

 19%|█▉        | 2061/10740 [10:15:58<48:04:04, 19.94s/it]

 19%|█▉        | 2062/10740 [10:16:16<46:23:41, 19.25s/it]

 19%|█▉        | 2063/10740 [10:16:30<42:24:14, 17.59s/it]

 19%|█▉        | 2064/10740 [10:16:51<45:04:25, 18.70s/it]

 19%|█▉        | 2065/10740 [10:17:02<39:43:25, 16.48s/it]

 19%|█▉        | 2066/10740 [10:17:14<36:07:21, 14.99s/it]

 19%|█▉        | 2067/10740 [10:17:32<38:23:09, 15.93s/it]

 19%|█▉        | 2068/10740 [10:17:48<38:23:20, 15.94s/it]

 19%|█▉        | 2069/10740 [10:18:04<38:54:36, 16.15s/it]

 19%|█▉        | 2070/10740 [10:18:25<42:12:48, 17.53s/it]
{'loss': 0.4297, 'learning_rate': 1.8643872086900816e-06, 'rewards/chosen': -0.9486666917800903, 'rewards/rejected': -2.164954900741577, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2162880897521973, 'policy_logps/rejected': -529.0987548828125, 'policy_logps/chosen': -492.788818359375, 'referece_logps/rejected': -507.4491882324219, 'referece_logps/chosen': -483.30218505859375, 'logits/rejected': -0.19395270943641663, 'logits/chosen': -0.08142882585525513, 'epoch': 1.16}


 19%|█▉        | 2072/10740 [10:19:01<42:43:27, 17.74s/it]

 19%|█▉        | 2073/10740 [10:19:15<39:40:43, 16.48s/it]
{'loss': 0.6109, 'learning_rate': 1.8639319225914884e-06, 'rewards/chosen': -1.6361370086669922, 'rewards/rejected': -2.1812903881073, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5451532602310181, 'policy_logps/rejected': -354.64825439453125, 'policy_logps/chosen': -494.51416015625, 'referece_logps/rejected': -332.8353576660156, 'referece_logps/chosen': -478.1528015136719, 'logits/rejected': -1.3209285736083984, 'logits/chosen': -1.331751823425293, 'epoch': 1.16}

 19%|█▉        | 2074/10740 [10:19:36<42:53:43, 17.82s/it]


 19%|█▉        | 2076/10740 [10:20:13<43:56:50, 18.26s/it]

 19%|█▉        | 2077/10740 [10:20:26<40:15:05, 16.73s/it]

 19%|█▉        | 2078/10740 [10:20:46<42:16:41, 17.57s/it]

 19%|█▉        | 2079/10740 [10:21:05<43:29:43, 18.08s/it]

 19%|█▉        | 2080/10740 [10:21:21<41:45:48, 17.36s/it]

 19%|█▉        | 2081/10740 [10:21:40<43:03:13, 17.90s/it]
{'loss': 0.4036, 'learning_rate': 1.8627143699668554e-06, 'rewards/chosen': -1.0130491256713867, 'rewards/rejected': -2.3199493885040283, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3069006204605103, 'policy_logps/rejected': -391.54962158203125, 'policy_logps/chosen': -293.53961181640625, 'referece_logps/rejected': -368.3500671386719, 'referece_logps/chosen': -283.4090881347656, 'logits/rejected': 0.28338009119033813, 'logits/chosen': 0.42267662286758423, 'epoch': 1.16}


 19%|█▉        | 2083/10740 [10:22:10<38:48:21, 16.14s/it]
{'loss': 0.5553, 'learning_rate': 1.862409196873704e-06, 'rewards/chosen': -1.251621127128601, 'rewards/rejected': -1.2417572736740112, 'rewards/accuracies': 0.375, 'rewards/margins': -0.009863842278718948, 'policy_logps/rejected': -477.20184326171875, 'policy_logps/chosen': -555.9971313476562, 'referece_logps/rejected': -464.7842712402344, 'referece_logps/chosen': -543.48095703125, 'logits/rejected': -0.8400179147720337, 'logits/chosen': -1.0545121431350708, 'epoch': 1.16}


 19%|█▉        | 2085/10740 [10:22:43<39:29:50, 16.43s/it]

 19%|█▉        | 2086/10740 [10:22:59<39:08:46, 16.28s/it]

 19%|█▉        | 2087/10740 [10:23:15<39:18:06, 16.35s/it]

 19%|█▉        | 2088/10740 [10:23:35<41:42:08, 17.35s/it]
{'loss': 0.5511, 'learning_rate': 1.8616448917149542e-06, 'rewards/chosen': -2.0081117153167725, 'rewards/rejected': -3.2913527488708496, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2832410335540771, 'policy_logps/rejected': -603.5955200195312, 'policy_logps/chosen': -619.3433227539062, 'referece_logps/rejected': -570.6820068359375, 'referece_logps/chosen': -599.26220703125, 'logits/rejected': -0.08210620284080505, 'logits/chosen': -0.21918289363384247, 'epoch': 1.17}

 19%|█▉        | 2089/10740 [10:23:50<40:04:07, 16.67s/it]

 19%|█▉        | 2090/10740 [10:24:12<44:05:42, 18.35s/it]

 19%|█▉        | 2091/10740 [10:24:28<42:15:15, 17.59s/it]


 19%|█▉        | 2093/10740 [10:25:07<44:55:32, 18.70s/it]
{'loss': 0.4935, 'learning_rate': 1.8608786273386606e-06, 'rewards/chosen': -1.2241325378417969, 'rewards/rejected': -2.2811810970306396, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0570485591888428, 'policy_logps/rejected': -351.9310302734375, 'policy_logps/chosen': -303.23736572265625, 'referece_logps/rejected': -329.1192626953125, 'referece_logps/chosen': -290.9960021972656, 'logits/rejected': -0.5692973732948303, 'logits/chosen': -0.5114409923553467, 'epoch': 1.17}

 19%|█▉        | 2094/10740 [10:25:26<44:56:21, 18.71s/it]


 20%|█▉        | 2096/10740 [10:25:56<40:49:48, 17.00s/it]

 20%|█▉        | 2097/10740 [10:26:11<39:51:36, 16.60s/it]

 20%|█▉        | 2098/10740 [10:26:35<44:54:09, 18.71s/it]

 20%|█▉        | 2099/10740 [10:26:55<45:51:36, 19.11s/it]
{'loss': 0.4517, 'learning_rate': 1.8599565263734785e-06, 'rewards/chosen': -1.3637783527374268, 'rewards/rejected': -2.4969711303710938, 'rewards/accuracies': 0.75, 'rewards/margins': 1.133192777633667, 'policy_logps/rejected': -517.96630859375, 'policy_logps/chosen': -485.8431091308594, 'referece_logps/rejected': -492.99664306640625, 'referece_logps/chosen': -472.2052917480469, 'logits/rejected': -0.383754163980484, 'logits/chosen': -0.295184463262558, 'epoch': 1.17}

 20%|█▉        | 2100/10740 [10:27:16<47:19:45, 19.72s/it]


 20%|█▉        | 2102/10740 [10:27:56<46:46:43, 19.50s/it]
{'loss': 0.5094, 'learning_rate': 1.8594944197975124e-06, 'rewards/chosen': -1.137991189956665, 'rewards/rejected': -1.988119125366211, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8501280546188354, 'policy_logps/rejected': -460.5420837402344, 'policy_logps/chosen': -374.3428955078125, 'referece_logps/rejected': -440.660888671875, 'referece_logps/chosen': -362.9629821777344, 'logits/rejected': -0.4259491562843323, 'logits/chosen': -0.4325576424598694, 'epoch': 1.17}

 20%|█▉        | 2103/10740 [10:28:13<45:03:44, 18.78s/it]


 20%|█▉        | 2105/10740 [10:28:51<45:16:58, 18.88s/it]
{'loss': 0.4401, 'learning_rate': 1.8590316096634613e-06, 'rewards/chosen': -1.4208958148956299, 'rewards/rejected': -1.9261854887008667, 'rewards/accuracies': 0.375, 'rewards/margins': 0.5052894353866577, 'policy_logps/rejected': -339.2052001953125, 'policy_logps/chosen': -592.411376953125, 'referece_logps/rejected': -319.94329833984375, 'referece_logps/chosen': -578.2024536132812, 'logits/rejected': -1.3846226930618286, 'logits/chosen': -1.32085120677948, 'epoch': 1.18}


 20%|█▉        | 2107/10740 [10:29:29<44:57:32, 18.75s/it]

 20%|█▉        | 2108/10740 [10:29:43<41:33:20, 17.33s/it]

 20%|█▉        | 2109/10740 [10:30:05<44:49:45, 18.70s/it]
{'loss': 0.4971, 'learning_rate': 1.8584134357159125e-06, 'rewards/chosen': -1.8699004650115967, 'rewards/rejected': -1.889660358428955, 'rewards/accuracies': 0.5, 'rewards/margins': 0.019759997725486755, 'policy_logps/rejected': -331.004638671875, 'policy_logps/chosen': -395.4192810058594, 'referece_logps/rejected': -312.1080627441406, 'referece_logps/chosen': -376.7203369140625, 'logits/rejected': -0.06422284245491028, 'logits/chosen': -0.06537787616252899, 'epoch': 1.18}


 20%|█▉        | 2111/10740 [10:30:42<44:19:58, 18.50s/it]

 20%|█▉        | 2112/10740 [10:31:01<44:55:21, 18.74s/it]
{'loss': 0.4754, 'learning_rate': 1.8579489854206041e-06, 'rewards/chosen': -0.6805635690689087, 'rewards/rejected': -2.0383739471435547, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3578104972839355, 'policy_logps/rejected': -276.35333251953125, 'policy_logps/chosen': -322.40582275390625, 'referece_logps/rejected': -255.96957397460938, 'referece_logps/chosen': -315.6002197265625, 'logits/rejected': -1.1898752450942993, 'logits/chosen': -1.2063055038452148, 'epoch': 1.18}

 20%|█▉        | 2113/10740 [10:31:22<46:39:07, 19.47s/it]

 20%|█▉        | 2114/10740 [10:31:38<44:08:16, 18.42s/it]

 20%|█▉        | 2115/10740 [10:31:54<42:11:27, 17.61s/it]


 20%|█▉        | 2117/10740 [10:32:30<42:56:09, 17.93s/it]

 20%|█▉        | 2118/10740 [10:32:50<44:09:14, 18.44s/it]
{'loss': 0.4823, 'learning_rate': 1.85701797833164e-06, 'rewards/chosen': -1.6431916952133179, 'rewards/rejected': -2.0667483806610107, 'rewards/accuracies': 0.875, 'rewards/margins': 0.42355650663375854, 'policy_logps/rejected': -380.952392578125, 'policy_logps/chosen': -456.22137451171875, 'referece_logps/rejected': -360.284912109375, 'referece_logps/chosen': -439.78948974609375, 'logits/rejected': -0.6770728826522827, 'logits/chosen': -0.7719753384590149, 'epoch': 1.18}

 20%|█▉        | 2119/10740 [10:33:05<41:43:30, 17.42s/it]


 20%|█▉        | 2121/10740 [10:33:47<46:27:03, 19.40s/it]
{'loss': 0.3452, 'learning_rate': 1.856551422300081e-06, 'rewards/chosen': -0.19615668058395386, 'rewards/rejected': -1.9902524948120117, 'rewards/accuracies': 1.0, 'rewards/margins': 1.794095754623413, 'policy_logps/rejected': -477.26348876953125, 'policy_logps/chosen': -495.7557678222656, 'referece_logps/rejected': -457.3609619140625, 'referece_logps/chosen': -493.794189453125, 'logits/rejected': -0.6447440981864929, 'logits/chosen': -0.7114210724830627, 'epoch': 1.18}

 20%|█▉        | 2122/10740 [10:34:05<45:07:18, 18.85s/it]


 20%|█▉        | 2124/10740 [10:34:34<38:53:33, 16.25s/it]
{'loss': 0.5863, 'learning_rate': 1.8560841651194924e-06, 'rewards/chosen': -1.2025519609451294, 'rewards/rejected': -1.9467953443527222, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7442435026168823, 'policy_logps/rejected': -398.409423828125, 'policy_logps/chosen': -401.69976806640625, 'referece_logps/rejected': -378.94146728515625, 'referece_logps/chosen': -389.67425537109375, 'logits/rejected': -0.4042852520942688, 'logits/chosen': -0.2966434955596924, 'epoch': 1.19}

 20%|█▉        | 2125/10740 [10:34:55<42:16:33, 17.67s/it]

 20%|█▉        | 2126/10740 [10:35:08<39:25:59, 16.48s/it]


 20%|█▉        | 2128/10740 [10:35:50<44:29:33, 18.60s/it]

 20%|█▉        | 2129/10740 [10:36:10<45:37:35, 19.08s/it]

 20%|█▉        | 2130/10740 [10:36:30<46:40:54, 19.52s/it]
{'loss': 0.4149, 'learning_rate': 1.8551475488417334e-06, 'rewards/chosen': -1.7791920900344849, 'rewards/rejected': -2.1016993522644043, 'rewards/accuracies': 0.25, 'rewards/margins': 0.32250717282295227, 'policy_logps/rejected': -326.19873046875, 'policy_logps/chosen': -427.0301818847656, 'referece_logps/rejected': -305.1817321777344, 'referece_logps/chosen': -409.23828125, 'logits/rejected': -0.5200129747390747, 'logits/chosen': -0.5940349102020264, 'epoch': 1.19}


 20%|█▉        | 2132/10740 [10:37:00<40:41:11, 17.02s/it]

 20%|█▉        | 2133/10740 [10:37:16<39:52:56, 16.68s/it]

 20%|█▉        | 2134/10740 [10:37:28<36:33:17, 15.29s/it]
{'loss': 0.4583, 'learning_rate': 1.8545215822452888e-06, 'rewards/chosen': -1.1995947360992432, 'rewards/rejected': -2.034487724304199, 'rewards/accuracies': 0.625, 'rewards/margins': 0.834892988204956, 'policy_logps/rejected': -275.948486328125, 'policy_logps/chosen': -350.7961120605469, 'referece_logps/rejected': -255.60360717773438, 'referece_logps/chosen': -338.8001708984375, 'logits/rejected': -0.8045225143432617, 'logits/chosen': -0.9290908575057983, 'epoch': 1.19}


 20%|█▉        | 2136/10740 [10:37:58<36:25:32, 15.24s/it]

 20%|█▉        | 2137/10740 [10:38:12<35:30:31, 14.86s/it]

 20%|█▉        | 2138/10740 [10:38:32<39:45:44, 16.64s/it]

 20%|█▉        | 2139/10740 [10:38:52<41:55:36, 17.55s/it]
{'loss': 0.3686, 'learning_rate': 1.8537373753880991e-06, 'rewards/chosen': -1.2703330516815186, 'rewards/rejected': -2.6305925846099854, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3602594137191772, 'policy_logps/rejected': -433.9197998046875, 'policy_logps/chosen': -427.70782470703125, 'referece_logps/rejected': -407.61383056640625, 'referece_logps/chosen': -415.0044860839844, 'logits/rejected': -0.451965868473053, 'logits/chosen': -0.46996423602104187, 'epoch': 1.19}

 20%|█▉        | 2140/10740 [10:39:11<42:40:19, 17.86s/it]


 20%|█▉        | 2142/10740 [10:39:46<41:51:25, 17.53s/it]
{'loss': 0.3599, 'learning_rate': 1.8532659193655543e-06, 'rewards/chosen': -0.9750059247016907, 'rewards/rejected': -2.204157829284668, 'rewards/accuracies': 0.75, 'rewards/margins': 1.229151964187622, 'policy_logps/rejected': -230.4779510498047, 'policy_logps/chosen': -199.164794921875, 'referece_logps/rejected': -208.43637084960938, 'referece_logps/chosen': -189.41473388671875, 'logits/rejected': -0.8132002949714661, 'logits/chosen': -0.7076370120048523, 'epoch': 1.2}

 20%|█▉        | 2143/10740 [10:40:07<44:12:43, 18.51s/it]

 20%|█▉        | 2144/10740 [10:40:27<45:15:38, 18.96s/it]


 20%|█▉        | 2146/10740 [10:40:58<40:38:27, 17.02s/it]
{'loss': 0.4927, 'learning_rate': 1.8526362249095535e-06, 'rewards/chosen': -1.7664616107940674, 'rewards/rejected': -2.1697165966033936, 'rewards/accuracies': 0.5, 'rewards/margins': 0.40325525403022766, 'policy_logps/rejected': -418.7392883300781, 'policy_logps/chosen': -449.5595703125, 'referece_logps/rejected': -397.0421447753906, 'referece_logps/chosen': -431.89495849609375, 'logits/rejected': -0.7681904435157776, 'logits/chosen': -0.8777666091918945, 'epoch': 1.2}


 20%|██        | 2148/10740 [10:41:34<43:01:16, 18.03s/it]
{'loss': 0.4539, 'learning_rate': 1.8523209123281292e-06, 'rewards/chosen': -1.5926594734191895, 'rewards/rejected': -3.1561906337738037, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5635311603546143, 'policy_logps/rejected': -609.7176513671875, 'policy_logps/chosen': -513.8670654296875, 'referece_logps/rejected': -578.15576171875, 'referece_logps/chosen': -497.9404602050781, 'logits/rejected': -1.0653247833251953, 'logits/chosen': -1.0971977710723877, 'epoch': 1.2}

 20%|██        | 2149/10740 [10:41:55<45:17:29, 18.98s/it]

 20%|██        | 2150/10740 [10:42:18<47:39:43, 19.97s/it]

 20%|██        | 2151/10740 [10:42:33<44:28:06, 18.64s/it]


 20%|██        | 2153/10740 [10:43:10<44:17:32, 18.57s/it]
{'loss': 0.4703, 'learning_rate': 1.8515312745139621e-06, 'rewards/chosen': -0.8262719511985779, 'rewards/rejected': -1.6134436130523682, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7871716022491455, 'policy_logps/rejected': -370.1461181640625, 'policy_logps/chosen': -327.983154296875, 'referece_logps/rejected': -354.01165771484375, 'referece_logps/chosen': -319.7204284667969, 'logits/rejected': -0.3324330151081085, 'logits/chosen': -0.36768919229507446, 'epoch': 1.2}

 20%|██        | 2154/10740 [10:43:26<42:08:05, 17.67s/it]

 20%|██        | 2155/10740 [10:43:37<37:49:59, 15.86s/it]

 20%|██        | 2156/10740 [10:43:51<36:26:37, 15.28s/it]

 20%|██        | 2157/10740 [10:44:11<39:35:18, 16.60s/it]

 20%|██        | 2158/10740 [10:44:21<35:17:14, 14.80s/it]

 20%|██        | 2159/10740 [10:44:34<33:26:08, 14.03s/it]


 20%|██        | 2161/10740 [10:45:09<38:11:13, 16.02s/it]
{'loss': 0.4608, 'learning_rate': 1.8502638274199965e-06, 'rewards/chosen': -0.3967720568180084, 'rewards/rejected': -2.0016589164733887, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6048868894577026, 'policy_logps/rejected': -564.1458129882812, 'policy_logps/chosen': -529.4227294921875, 'referece_logps/rejected': -544.1292724609375, 'referece_logps/chosen': -525.4550170898438, 'logits/rejected': 0.03218832612037659, 'logits/chosen': 0.020502641797065735, 'epoch': 1.21}

 20%|██        | 2162/10740 [10:45:30<41:48:40, 17.55s/it]

 20%|██        | 2163/10740 [10:45:50<43:29:11, 18.25s/it]


 20%|██        | 2165/10740 [10:46:25<42:35:22, 17.88s/it]

 20%|██        | 2166/10740 [10:46:47<45:28:12, 19.09s/it]
{'loss': 0.4878, 'learning_rate': 1.8494691591765929e-06, 'rewards/chosen': -1.673564076423645, 'rewards/rejected': -2.782259464263916, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1086955070495605, 'policy_logps/rejected': -469.81964111328125, 'policy_logps/chosen': -562.430419921875, 'referece_logps/rejected': -441.9970397949219, 'referece_logps/chosen': -545.69482421875, 'logits/rejected': -1.1388754844665527, 'logits/chosen': -1.28794264793396, 'epoch': 1.21}

 20%|██        | 2167/10740 [10:47:05<45:07:41, 18.95s/it]


 20%|██        | 2169/10740 [10:47:41<42:31:35, 17.86s/it]
{'loss': 0.4741, 'learning_rate': 1.8489914309792542e-06, 'rewards/chosen': -0.684170126914978, 'rewards/rejected': -1.5168335437774658, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8326635360717773, 'policy_logps/rejected': -272.96685791015625, 'policy_logps/chosen': -370.67041015625, 'referece_logps/rejected': -257.79852294921875, 'referece_logps/chosen': -363.8287048339844, 'logits/rejected': -0.44852057099342346, 'logits/chosen': -0.46929502487182617, 'epoch': 1.21}

 20%|██        | 2170/10740 [10:47:59<42:52:48, 18.01s/it]

 20%|██        | 2171/10740 [10:48:20<44:41:20, 18.77s/it]


 20%|██        | 2173/10740 [10:49:02<47:57:39, 20.15s/it]
{'loss': 0.4275, 'learning_rate': 1.8483533790672681e-06, 'rewards/chosen': -1.900851845741272, 'rewards/rejected': -2.789644718170166, 'rewards/accuracies': 0.625, 'rewards/margins': 0.888792872428894, 'policy_logps/rejected': -506.66705322265625, 'policy_logps/chosen': -484.52301025390625, 'referece_logps/rejected': -478.7705993652344, 'referece_logps/chosen': -465.5144958496094, 'logits/rejected': -0.08465854823589325, 'logits/chosen': -0.10166183114051819, 'epoch': 1.21}


 20%|██        | 2175/10740 [10:49:39<45:44:22, 19.23s/it]

 20%|██        | 2176/10740 [10:49:58<45:41:07, 19.20s/it]
{'loss': 0.4908, 'learning_rate': 1.8478740299044114e-06, 'rewards/chosen': -1.6160573959350586, 'rewards/rejected': -2.8593456745147705, 'rewards/accuracies': 0.75, 'rewards/margins': 1.243288278579712, 'policy_logps/rejected': -361.67022705078125, 'policy_logps/chosen': -359.4798278808594, 'referece_logps/rejected': -333.0767517089844, 'referece_logps/chosen': -343.31927490234375, 'logits/rejected': -0.37928247451782227, 'logits/chosen': -0.36374783515930176, 'epoch': 1.22}


 20%|██        | 2178/10740 [10:50:33<42:30:48, 17.88s/it]
{'loss': 0.5552, 'learning_rate': 1.847554078190507e-06, 'rewards/chosen': -1.2734647989273071, 'rewards/rejected': -2.528911828994751, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2554469108581543, 'policy_logps/rejected': -410.5115051269531, 'policy_logps/chosen': -357.982421875, 'referece_logps/rejected': -385.2224426269531, 'referece_logps/chosen': -345.247802734375, 'logits/rejected': 0.2397787719964981, 'logits/chosen': 0.2454214096069336, 'epoch': 1.22}

 20%|██        | 2179/10740 [10:50:49<41:36:03, 17.49s/it]

 20%|██        | 2180/10740 [10:51:10<43:51:37, 18.45s/it]


 20%|██        | 2182/10740 [10:51:51<46:10:52, 19.43s/it]
{'loss': 0.4851, 'learning_rate': 1.8469132498337698e-06, 'rewards/chosen': -0.774531364440918, 'rewards/rejected': -2.1135029792785645, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3389716148376465, 'policy_logps/rejected': -281.1451416015625, 'policy_logps/chosen': -238.7085418701172, 'referece_logps/rejected': -260.0101318359375, 'referece_logps/chosen': -230.96322631835938, 'logits/rejected': -0.4350340664386749, 'logits/chosen': -0.45211994647979736, 'epoch': 1.22}

 20%|██        | 2183/10740 [10:52:10<46:19:48, 19.49s/it]

 20%|██        | 2184/10740 [10:52:31<47:28:48, 19.98s/it]


 20%|██        | 2186/10740 [10:53:07<44:59:15, 18.93s/it]

 20%|██        | 2187/10740 [10:53:29<46:57:36, 19.77s/it]
{'loss': 0.3492, 'learning_rate': 1.8461104813486663e-06, 'rewards/chosen': -0.8377444744110107, 'rewards/rejected': -1.8659285306930542, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0281840562820435, 'policy_logps/rejected': -327.7881164550781, 'policy_logps/chosen': -303.71661376953125, 'referece_logps/rejected': -309.12884521484375, 'referece_logps/chosen': -295.3391418457031, 'logits/rejected': -0.6777467727661133, 'logits/chosen': -0.7502374649047852, 'epoch': 1.22}


 20%|██        | 2189/10740 [10:54:09<47:39:34, 20.06s/it]
{'loss': 0.3545, 'learning_rate': 1.8457888351615936e-06, 'rewards/chosen': -0.5658546090126038, 'rewards/rejected': -1.3150461912155151, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7491915225982666, 'policy_logps/rejected': -527.6160278320312, 'policy_logps/chosen': -485.0234069824219, 'referece_logps/rejected': -514.465576171875, 'referece_logps/chosen': -479.3648681640625, 'logits/rejected': -1.1017796993255615, 'logits/chosen': -1.0080769062042236, 'epoch': 1.22}


 20%|██        | 2191/10740 [10:54:39<42:48:32, 18.03s/it]

 20%|██        | 2192/10740 [10:54:57<42:54:23, 18.07s/it]
{'loss': 0.462, 'learning_rate': 1.8453057889683304e-06, 'rewards/chosen': -1.3935143947601318, 'rewards/rejected': -2.263167142868042, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8696528673171997, 'policy_logps/rejected': -315.9241638183594, 'policy_logps/chosen': -224.26815795898438, 'referece_logps/rejected': -293.2925109863281, 'referece_logps/chosen': -210.33303833007812, 'logits/rejected': -0.5018147826194763, 'logits/chosen': -0.44355690479278564, 'epoch': 1.22}

 20%|██        | 2193/10740 [10:55:20<46:02:39, 19.39s/it]

 20%|██        | 2194/10740 [10:55:35<42:49:16, 18.04s/it]


 20%|██        | 2196/10740 [10:56:13<43:42:08, 18.41s/it]

 20%|██        | 2197/10740 [10:56:35<46:09:52, 19.45s/it]
{'loss': 0.4263, 'learning_rate': 1.8444991745224803e-06, 'rewards/chosen': -1.3818639516830444, 'rewards/rejected': -4.191400051116943, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8095366954803467, 'policy_logps/rejected': -533.4232177734375, 'policy_logps/chosen': -508.4638366699219, 'referece_logps/rejected': -491.5091552734375, 'referece_logps/chosen': -494.6451416015625, 'logits/rejected': 0.2471068799495697, 'logits/chosen': 0.3385322093963623, 'epoch': 1.23}


 20%|██        | 2199/10740 [10:57:07<42:18:27, 17.83s/it]

 20%|██        | 2200/10740 [10:57:22<39:40:48, 16.73s/it]
{'loss': 0.4313, 'learning_rate': 1.8440142840263228e-06, 'rewards/chosen': -0.8740297555923462, 'rewards/rejected': -3.0897648334503174, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2157351970672607, 'policy_logps/rejected': -378.2233581542969, 'policy_logps/chosen': -447.2611999511719, 'referece_logps/rejected': -347.3257141113281, 'referece_logps/chosen': -438.52093505859375, 'logits/rejected': -0.7723535895347595, 'logits/chosen': -0.8818939924240112, 'epoch': 1.23}

 20%|██        | 2201/10740 [10:57:39<40:08:26, 16.92s/it]

 21%|██        | 2202/10740 [10:57:57<41:02:51, 17.31s/it]


 21%|██        | 2204/10740 [10:58:38<44:36:17, 18.81s/it]
{'loss': 0.4611, 'learning_rate': 1.8433666887211627e-06, 'rewards/chosen': -1.3360490798950195, 'rewards/rejected': -2.5045390129089355, 'rewards/accuracies': 0.875, 'rewards/margins': 1.168489933013916, 'policy_logps/rejected': -310.5809631347656, 'policy_logps/chosen': -340.0888671875, 'referece_logps/rejected': -285.53558349609375, 'referece_logps/chosen': -326.7283630371094, 'logits/rejected': 0.2394176870584488, 'logits/chosen': 0.21455593407154083, 'epoch': 1.23}


 21%|██        | 2206/10740 [10:59:18<46:23:27, 19.57s/it]
{'loss': 0.531, 'learning_rate': 1.8430424307720531e-06, 'rewards/chosen': -0.813975989818573, 'rewards/rejected': -2.3907976150512695, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5768216848373413, 'policy_logps/rejected': -396.2920227050781, 'policy_logps/chosen': -292.4209899902344, 'referece_logps/rejected': -372.384033203125, 'referece_logps/chosen': -284.28125, 'logits/rejected': -1.128430962562561, 'logits/chosen': -1.1629266738891602, 'epoch': 1.23}

 21%|██        | 2207/10740 [10:59:39<47:21:25, 19.98s/it]

 21%|██        | 2208/10740 [10:59:52<42:36:07, 17.98s/it]

 21%|██        | 2209/10740 [11:00:08<41:26:03, 17.48s/it]

 21%|██        | 2210/10740 [11:00:29<43:49:33, 18.50s/it]

 21%|██        | 2211/10740 [11:00:51<45:51:45, 19.36s/it]

 21%|██        | 2212/10740 [11:01:05<42:40:19, 18.01s/it]


 21%|██        | 2214/10740 [11:01:50<47:55:01, 20.23s/it]

 21%|██        | 2215/10740 [11:02:10<47:41:26, 20.14s/it]

 21%|██        | 2216/10740 [11:02:32<49:09:55, 20.76s/it]

 21%|██        | 2217/10740 [11:02:52<48:42:12, 20.57s/it]

 21%|██        | 2218/10740 [11:03:06<43:58:19, 18.58s/it]

 21%|██        | 2219/10740 [11:03:20<40:42:44, 17.20s/it]
{'loss': 0.4583, 'learning_rate': 1.8409272833995918e-06, 'rewards/chosen': -0.9750590324401855, 'rewards/rejected': -2.4244418144226074, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4493827819824219, 'policy_logps/rejected': -409.03118896484375, 'policy_logps/chosen': -389.3562316894531, 'referece_logps/rejected': -384.7867126464844, 'referece_logps/chosen': -379.6056213378906, 'logits/rejected': -0.7159755825996399, 'logits/chosen': -0.6735130548477173, 'epoch': 1.24}


 21%|██        | 2221/10740 [11:03:56<41:32:22, 17.55s/it]
{'loss': 0.4566, 'learning_rate': 1.8406007280304152e-06, 'rewards/chosen': -1.0854719877243042, 'rewards/rejected': -2.587779998779297, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5023083686828613, 'policy_logps/rejected': -325.91552734375, 'policy_logps/chosen': -301.88311767578125, 'referece_logps/rejected': -300.0377197265625, 'referece_logps/chosen': -291.02838134765625, 'logits/rejected': -0.7847741842269897, 'logits/chosen': -0.6489435434341431, 'epoch': 1.24}

 21%|██        | 2222/10740 [11:04:08<37:59:35, 16.06s/it]


 21%|██        | 2224/10740 [11:04:48<42:06:17, 17.80s/it]
{'loss': 0.6446, 'learning_rate': 1.8401103216035864e-06, 'rewards/chosen': -1.3854707479476929, 'rewards/rejected': -1.703277587890625, 'rewards/accuracies': 0.625, 'rewards/margins': 0.31780683994293213, 'policy_logps/rejected': -413.2055358886719, 'policy_logps/chosen': -403.6182861328125, 'referece_logps/rejected': -396.1728210449219, 'referece_logps/chosen': -389.7635498046875, 'logits/rejected': 0.11080653965473175, 'logits/chosen': 0.01737678050994873, 'epoch': 1.24}

 21%|██        | 2225/10740 [11:05:09<44:17:15, 18.72s/it]

 21%|██        | 2226/10740 [11:05:27<43:57:09, 18.58s/it]


 21%|██        | 2228/10740 [11:06:08<46:23:38, 19.62s/it]
{'loss': 0.4331, 'learning_rate': 1.8394553766960276e-06, 'rewards/chosen': -0.4704839885234833, 'rewards/rejected': -1.904576063156128, 'rewards/accuracies': 0.5, 'rewards/margins': 1.4340920448303223, 'policy_logps/rejected': -337.775146484375, 'policy_logps/chosen': -328.7129821777344, 'referece_logps/rejected': -318.7293701171875, 'referece_logps/chosen': -324.0081481933594, 'logits/rejected': -0.7029845714569092, 'logits/chosen': -0.8460612297058105, 'epoch': 1.24}

 21%|██        | 2229/10740 [11:06:25<44:45:02, 18.93s/it]

 21%|██        | 2230/10740 [11:06:41<42:21:53, 17.92s/it]

 21%|██        | 2231/10740 [11:06:55<40:00:36, 16.93s/it]

 21%|██        | 2232/10740 [11:07:15<41:39:54, 17.63s/it]

 21%|██        | 2233/10740 [11:07:34<43:11:29, 18.28s/it]

 21%|██        | 2234/10740 [11:07:55<44:41:16, 18.91s/it]

 21%|██        | 2235/10740 [11:08:07<39:55:08, 16.90s/it]

 21%|██        | 2236/10740 [11:08:21<37:56:24, 16.06s/it]

 21%|██        | 2237/10740 [11:08:39<38:51:38, 16.45s/it]

 21%|██        | 2238/10740 [11:08:49<34:46:11, 14.72s/it]

 21%|██        | 2239/10740 [11:09:09<38:17:39, 16.22s/it]

 21%|██        | 2240/10740 [11:09:23<37:02:49, 15.69s/it]

 21%|██        | 2241/10740 [11:09:44<40:17:04, 17.06s/it]

 21%|██        | 2242/10740 [11:10:04<42:23:25, 17.96s/it]

 21%|██        | 2243/10740 [11:10:16<38:03:59, 16.13s/it]

 21%|██        | 2244/10740 [11:10:31<37:27:32, 15.87s/it]


 21%|██        | 2246/10740 [11:11:00<36:41:04, 15.55s/it]

 21%|██        | 2247/10740 [11:11:16<36:49:17, 15.61s/it]
{'loss': 0.4589, 'learning_rate': 1.8363277221181879e-06, 'rewards/chosen': -1.077484130859375, 'rewards/rejected': -2.804598569869995, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7271144390106201, 'policy_logps/rejected': -475.2262268066406, 'policy_logps/chosen': -354.35809326171875, 'referece_logps/rejected': -447.1802673339844, 'referece_logps/chosen': -343.5832214355469, 'logits/rejected': -0.13600340485572815, 'logits/chosen': -0.3126942217350006, 'epoch': 1.26}

 21%|██        | 2248/10740 [11:11:33<37:38:13, 15.96s/it]

 21%|██        | 2249/10740 [11:11:50<38:15:40, 16.22s/it]

 21%|██        | 2250/10740 [11:12:01<34:32:01, 14.64s/it]

 21%|██        | 2251/10740 [11:12:21<38:29:37, 16.32s/it]

 21%|██        | 2252/10740 [11:12:39<40:02:20, 16.98s/it]


 21%|██        | 2254/10740 [11:13:13<40:07:59, 17.03s/it]
{'loss': 0.4091, 'learning_rate': 1.8351685008558646e-06, 'rewards/chosen': -1.102704405784607, 'rewards/rejected': -1.776388168334961, 'rewards/accuracies': 0.75, 'rewards/margins': 0.673683762550354, 'policy_logps/rejected': -451.1061706542969, 'policy_logps/chosen': -413.808349609375, 'referece_logps/rejected': -433.34228515625, 'referece_logps/chosen': -402.78131103515625, 'logits/rejected': 0.09968331456184387, 'logits/chosen': 0.2446497529745102, 'epoch': 1.26}

 21%|██        | 2255/10740 [11:13:28<39:05:08, 16.58s/it]

 21%|██        | 2256/10740 [11:13:50<42:59:59, 18.25s/it]


 21%|██        | 2258/10740 [11:14:25<42:31:49, 18.05s/it]

 21%|██        | 2259/10740 [11:14:43<42:35:19, 18.08s/it]
{'loss': 0.4823, 'learning_rate': 1.834338206548723e-06, 'rewards/chosen': -0.8392745852470398, 'rewards/rejected': -1.5558334589004517, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7165589332580566, 'policy_logps/rejected': -295.9315185546875, 'policy_logps/chosen': -343.270751953125, 'referece_logps/rejected': -280.3731689453125, 'referece_logps/chosen': -334.8780212402344, 'logits/rejected': -0.8364468812942505, 'logits/chosen': -0.6619279980659485, 'epoch': 1.26}

 21%|██        | 2260/10740 [11:14:55<38:21:43, 16.29s/it]

 21%|██        | 2261/10740 [11:15:09<37:07:12, 15.76s/it]

 21%|██        | 2262/10740 [11:15:28<38:59:03, 16.55s/it]

 21%|██        | 2263/10740 [11:15:48<41:36:17, 17.67s/it]

 21%|██        | 2264/10740 [11:16:06<41:44:44, 17.73s/it]

 21%|██        | 2265/10740 [11:16:24<42:02:38, 17.86s/it]

 21%|██        | 2266/10740 [11:16:41<41:30:37, 17.63s/it]

 21%|██        | 2267/10740 [11:16:58<40:38:29, 17.27s/it]


 21%|██        | 2269/10740 [11:17:37<43:17:55, 18.40s/it]
{'loss': 0.3641, 'learning_rate': 1.832671928444769e-06, 'rewards/chosen': -1.134408712387085, 'rewards/rejected': -2.474128007888794, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3397191762924194, 'policy_logps/rejected': -406.84600830078125, 'policy_logps/chosen': -359.07879638671875, 'referece_logps/rejected': -382.104736328125, 'referece_logps/chosen': -347.73468017578125, 'logits/rejected': 0.2956409454345703, 'logits/chosen': 0.24177441000938416, 'epoch': 1.27}

 21%|██        | 2270/10740 [11:18:00<46:43:59, 19.86s/it]

 21%|██        | 2271/10740 [11:18:20<46:47:18, 19.89s/it]


 21%|██        | 2273/10740 [11:18:49<40:50:15, 17.36s/it]
{'loss': 0.5118, 'learning_rate': 1.8320032958142606e-06, 'rewards/chosen': -1.4298721551895142, 'rewards/rejected': -1.9601202011108398, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5302480459213257, 'policy_logps/rejected': -541.7047119140625, 'policy_logps/chosen': -500.3456726074219, 'referece_logps/rejected': -522.103515625, 'referece_logps/chosen': -486.04693603515625, 'logits/rejected': 0.49956855177879333, 'logits/chosen': 0.4767010807991028, 'epoch': 1.27}

 21%|██        | 2274/10740 [11:19:08<42:19:38, 18.00s/it]

 21%|██        | 2275/10740 [11:19:22<39:11:51, 16.67s/it]

 21%|██        | 2276/10740 [11:19:39<39:11:39, 16.67s/it]

 21%|██        | 2277/10740 [11:19:56<39:43:45, 16.90s/it]

 21%|██        | 2278/10740 [11:20:10<37:19:44, 15.88s/it]

 21%|██        | 2279/10740 [11:20:31<41:31:23, 17.67s/it]

 21%|██        | 2280/10740 [11:20:44<38:07:57, 16.23s/it]

 21%|██        | 2281/10740 [11:21:04<40:37:05, 17.29s/it]

 21%|██        | 2282/10740 [11:21:27<44:17:48, 18.85s/it]

 21%|██▏       | 2283/10740 [11:21:48<46:11:51, 19.67s/it]

 21%|██▏       | 2284/10740 [11:22:12<49:07:03, 20.91s/it]

 21%|██▏       | 2285/10740 [11:22:32<48:29:56, 20.65s/it]

 21%|██▏       | 2286/10740 [11:22:51<47:34:28, 20.26s/it]

 21%|██▏       | 2287/10740 [11:23:13<48:16:20, 20.56s/it]


 21%|██▏       | 2289/10740 [11:23:55<48:55:59, 20.84s/it]
{'loss': 0.4109, 'learning_rate': 1.8293166674118003e-06, 'rewards/chosen': -0.09693166613578796, 'rewards/rejected': -1.7641234397888184, 'rewards/accuracies': 0.75, 'rewards/margins': 1.667191743850708, 'policy_logps/rejected': -431.29327392578125, 'policy_logps/chosen': -376.6087951660156, 'referece_logps/rejected': -413.65203857421875, 'referece_logps/chosen': -375.6394958496094, 'logits/rejected': -0.5718668699264526, 'logits/chosen': -0.6418839693069458, 'epoch': 1.28}


 21%|██▏       | 2291/10740 [11:24:29<43:47:38, 18.66s/it]
{'loss': 0.5488, 'learning_rate': 1.8289794798654237e-06, 'rewards/chosen': -0.4997653067111969, 'rewards/rejected': -1.682206153869629, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1824407577514648, 'policy_logps/rejected': -290.7702941894531, 'policy_logps/chosen': -318.9361877441406, 'referece_logps/rejected': -273.9482421875, 'referece_logps/chosen': -313.93853759765625, 'logits/rejected': -0.21534408628940582, 'logits/chosen': -0.2599157392978668, 'epoch': 1.28}

 21%|██▏       | 2292/10740 [11:24:42<39:30:46, 16.84s/it]

 21%|██▏       | 2293/10740 [11:25:00<40:33:18, 17.28s/it]

 21%|██▏       | 2294/10740 [11:25:16<39:19:56, 16.76s/it]

 21%|██▏       | 2295/10740 [11:25:31<38:12:17, 16.29s/it]

 21%|██▏       | 2296/10740 [11:25:52<41:46:41, 17.81s/it]


 21%|██▏       | 2298/10740 [11:26:27<41:02:38, 17.50s/it]

 21%|██▏       | 2299/10740 [11:26:47<42:54:32, 18.30s/it]
{'loss': 0.4435, 'learning_rate': 1.8276277149996853e-06, 'rewards/chosen': -1.059977650642395, 'rewards/rejected': -2.0588793754577637, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9989018440246582, 'policy_logps/rejected': -316.7447814941406, 'policy_logps/chosen': -288.3985900878906, 'referece_logps/rejected': -296.1559753417969, 'referece_logps/chosen': -277.798828125, 'logits/rejected': -0.8267055749893188, 'logits/chosen': -0.7974585890769958, 'epoch': 1.28}


 21%|██▏       | 2301/10740 [11:27:23<42:47:26, 18.25s/it]

 21%|██▏       | 2302/10740 [11:27:45<45:30:14, 19.41s/it]
{'loss': 0.3923, 'learning_rate': 1.8271195607183924e-06, 'rewards/chosen': -0.9707022905349731, 'rewards/rejected': -1.6495360136032104, 'rewards/accuracies': 1.0, 'rewards/margins': 0.6788336634635925, 'policy_logps/rejected': -394.1297607421875, 'policy_logps/chosen': -348.9549255371094, 'referece_logps/rejected': -377.6343688964844, 'referece_logps/chosen': -339.2478942871094, 'logits/rejected': 0.1608702540397644, 'logits/chosen': 0.12755101919174194, 'epoch': 1.29}

 21%|██▏       | 2303/10740 [11:28:00<42:07:56, 17.98s/it]


 21%|██▏       | 2305/10740 [11:28:39<44:55:28, 19.17s/it]
{'loss': 0.4493, 'learning_rate': 1.8266107293801682e-06, 'rewards/chosen': -0.9952719211578369, 'rewards/rejected': -1.8773174285888672, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8820452690124512, 'policy_logps/rejected': -392.2725830078125, 'policy_logps/chosen': -425.6517028808594, 'referece_logps/rejected': -373.4994201660156, 'referece_logps/chosen': -415.698974609375, 'logits/rejected': -0.30930230021476746, 'logits/chosen': -0.3059656023979187, 'epoch': 1.29}


 21%|██▏       | 2307/10740 [11:29:24<48:15:24, 20.60s/it]

 21%|██▏       | 2308/10740 [11:29:44<47:48:26, 20.41s/it]
{'loss': 0.4751, 'learning_rate': 1.8261012214015282e-06, 'rewards/chosen': -1.2401800155639648, 'rewards/rejected': -2.273989677429199, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0338094234466553, 'policy_logps/rejected': -467.8634948730469, 'policy_logps/chosen': -491.9785461425781, 'referece_logps/rejected': -445.12359619140625, 'referece_logps/chosen': -479.57672119140625, 'logits/rejected': 0.02378019690513611, 'logits/chosen': 0.08813512325286865, 'epoch': 1.29}

 21%|██▏       | 2309/10740 [11:30:00<45:17:04, 19.34s/it]

 22%|██▏       | 2310/10740 [11:30:14<41:08:36, 17.57s/it]

 22%|██▏       | 2311/10740 [11:30:34<42:58:58, 18.36s/it]

 22%|██▏       | 2312/10740 [11:30:50<41:18:32, 17.65s/it]
[2024-04-02 06:44:56,252] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2313/10740 [11:31:13<44:42:47, 19.10s/it]

 22%|██▏       | 2314/10740 [11:31:27<41:39:56, 17.80s/it]

 22%|██▏       | 2315/10740 [11:31:45<41:32:56, 17.75s/it]

 22%|██▏       | 2316/10740 [11:32:06<44:02:00, 18.82s/it]

 22%|██▏       | 2317/10740 [11:32:23<42:27:32, 18.15s/it]

 22%|██▏       | 2318/10740 [11:32:37<39:50:50, 17.03s/it]

 22%|██▏       | 2319/10740 [11:32:57<41:49:05, 17.88s/it]

 22%|██▏       | 2320/10740 [11:33:13<40:32:30, 17.33s/it]

 22%|██▏       | 2321/10740 [11:33:26<37:36:03, 16.08s/it]

 22%|██▏       | 2322/10740 [11:33:45<39:20:45, 16.83s/it]

 22%|██▏       | 2323/10740 [11:34:05<41:49:02, 17.89s/it]
[2024-04-02 06:48:12,254] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2324/10740 [11:34:29<45:35:56, 19.51s/it]
[2024-04-02 06:48:32,006] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2325/10740 [11:34:48<45:46:00, 19.58s/it]


 22%|██▏       | 2327/10740 [11:35:26<45:05:03, 19.29s/it]

 22%|██▏       | 2328/10740 [11:35:48<46:50:05, 20.04s/it]

 22%|██▏       | 2329/10740 [11:36:05<44:50:59, 19.20s/it]
{'loss': 0.4784, 'learning_rate': 1.8225157547228866e-06, 'rewards/chosen': -1.1444412469863892, 'rewards/rejected': -2.732306718826294, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5878652334213257, 'policy_logps/rejected': -332.80670166015625, 'policy_logps/chosen': -304.8919982910156, 'referece_logps/rejected': -305.4836120605469, 'referece_logps/chosen': -293.4475402832031, 'logits/rejected': -0.7872405648231506, 'logits/chosen': -0.810648500919342, 'epoch': 1.3}
[2024-04-02 06:50:09,931] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 22%|██▏       | 2331/10740 [11:36:48<48:00:47, 20.56s/it]
[2024-04-02 06:50:32,213] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2332/10740 [11:37:08<47:18:52, 20.26s/it]

 22%|██▏       | 2333/10740 [11:37:22<42:48:33, 18.33s/it]

 22%|██▏       | 2334/10740 [11:37:42<43:53:40, 18.80s/it]

 22%|██▏       | 2335/10740 [11:38:03<45:41:34, 19.57s/it]
[2024-04-02 06:51:46,877] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2336/10740 [11:38:21<44:48:50, 19.20s/it]
[2024-04-02 06:52:05,202] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2337/10740 [11:38:39<43:52:02, 18.79s/it]

 22%|██▏       | 2338/10740 [11:38:57<43:23:53, 18.59s/it]

 22%|██▏       | 2339/10740 [11:39:11<40:06:39, 17.19s/it]

 22%|██▏       | 2340/10740 [11:39:29<40:04:42, 17.18s/it]

 22%|██▏       | 2341/10740 [11:39:48<41:48:24, 17.92s/it]

 22%|██▏       | 2342/10740 [11:40:09<43:42:21, 18.74s/it]
[2024-04-02 06:53:52,534] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4833, 'learning_rate': 1.8202796379892268e-06, 'rewards/chosen': -0.6668967008590698, 'rewards/rejected': -1.50703763961792, 'rewards/accuracies': 1.0, 'rewards/margins': 0.8401408791542053, 'policy_logps/rejected': -451.3233642578125, 'policy_logps/chosen': -401.7849426269531, 'referece_logps/rejected': -436.2529602050781, 'referece_logps/chosen': -395.115966796875, 'logits/rejected': -0.7373877763748169, 'logits/chosen': -0.8131121397018433, 'epoch': 1.31}
[2024-04-02 06:54:14,162] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 22%|██▏       | 2344/10740 [11:40:45<42:23:28, 18.18s/it]

 22%|██▏       | 2345/10740 [11:41:06<43:53:33, 18.82s/it]
[2024-04-02 06:54:49,339] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2346/10740 [11:41:23<42:47:08, 18.35s/it]

 22%|██▏       | 2347/10740 [11:41:44<44:44:28, 19.19s/it]

 22%|██▏       | 2348/10740 [11:42:03<44:21:41, 19.03s/it]
{'loss': 0.4261, 'learning_rate': 1.8192433294622575e-06, 'rewards/chosen': -0.5398339629173279, 'rewards/rejected': -2.46903133392334, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9291976690292358, 'policy_logps/rejected': -309.41204833984375, 'policy_logps/chosen': -431.9787292480469, 'referece_logps/rejected': -284.72174072265625, 'referece_logps/chosen': -426.5804138183594, 'logits/rejected': -1.1986503601074219, 'logits/chosen': -1.3531461954116821, 'epoch': 1.31}


 22%|██▏       | 2350/10740 [11:42:40<43:47:21, 18.79s/it]

 22%|██▏       | 2351/10740 [11:42:59<44:17:35, 19.01s/it]

 22%|██▏       | 2352/10740 [11:43:19<45:03:26, 19.34s/it]

 22%|██▏       | 2353/10740 [11:43:38<44:49:31, 19.24s/it]

 22%|██▏       | 2354/10740 [11:43:52<40:42:12, 17.47s/it]

 22%|██▏       | 2355/10740 [11:44:09<40:46:47, 17.51s/it]

 22%|██▏       | 2356/10740 [11:44:23<38:29:52, 16.53s/it]

 22%|██▏       | 2357/10740 [11:44:38<37:16:14, 16.01s/it]
{'loss': 0.4446, 'learning_rate': 1.8176838381630843e-06, 'rewards/chosen': -1.0802907943725586, 'rewards/rejected': -1.474982738494873, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3946920931339264, 'policy_logps/rejected': -381.73388671875, 'policy_logps/chosen': -398.29779052734375, 'referece_logps/rejected': -366.98406982421875, 'referece_logps/chosen': -387.494873046875, 'logits/rejected': -0.9744246006011963, 'logits/chosen': -1.073411464691162, 'epoch': 1.32}


 22%|██▏       | 2359/10740 [11:45:15<40:17:30, 17.31s/it]

 22%|██▏       | 2360/10740 [11:45:35<42:15:44, 18.16s/it]

 22%|██▏       | 2361/10740 [11:45:56<44:12:48, 19.00s/it]

 22%|██▏       | 2362/10740 [11:46:18<46:02:34, 19.78s/it]

 22%|██▏       | 2363/10740 [11:46:33<42:24:28, 18.22s/it]

 22%|██▏       | 2364/10740 [11:46:49<41:15:16, 17.73s/it]

 22%|██▏       | 2365/10740 [11:47:09<42:46:36, 18.39s/it]

 22%|██▏       | 2366/10740 [11:47:27<42:42:05, 18.36s/it]
{'loss': 0.4618, 'learning_rate': 1.8161183228691825e-06, 'rewards/chosen': -1.0168323516845703, 'rewards/rejected': -2.8571126461029053, 'rewards/accuracies': 0.875, 'rewards/margins': 1.840280294418335, 'policy_logps/rejected': -346.1591796875, 'policy_logps/chosen': -307.05859375, 'referece_logps/rejected': -317.5880126953125, 'referece_logps/chosen': -296.8902587890625, 'logits/rejected': 0.12171629071235657, 'logits/chosen': 0.06401577591896057, 'epoch': 1.32}

 22%|██▏       | 2367/10740 [11:47:49<44:52:38, 19.30s/it]


 22%|██▏       | 2369/10740 [11:48:26<43:28:41, 18.70s/it]

 22%|██▏       | 2370/10740 [11:48:46<44:46:05, 19.26s/it]

 22%|██▏       | 2371/10740 [11:49:01<41:16:42, 17.76s/it]

 22%|██▏       | 2372/10740 [11:49:20<42:44:11, 18.39s/it]

 22%|██▏       | 2373/10740 [11:49:39<42:38:23, 18.35s/it]

 22%|██▏       | 2374/10740 [11:50:01<45:19:36, 19.50s/it]

 22%|██▏       | 2375/10740 [11:50:16<42:28:25, 18.28s/it]

 22%|██▏       | 2376/10740 [11:50:37<44:06:17, 18.98s/it]

 22%|██▏       | 2377/10740 [11:50:52<41:19:40, 17.79s/it]

 22%|██▏       | 2378/10740 [11:51:10<41:51:22, 18.02s/it]

 22%|██▏       | 2379/10740 [11:51:30<43:12:17, 18.60s/it]
[2024-04-02 07:05:14,176] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2380/10740 [11:51:49<43:00:17, 18.52s/it]

 22%|██▏       | 2381/10740 [11:52:10<44:51:31, 19.32s/it]

 22%|██▏       | 2382/10740 [11:52:31<45:47:11, 19.72s/it]

 22%|██▏       | 2383/10740 [11:52:46<42:29:14, 18.30s/it]

 22%|██▏       | 2384/10740 [11:53:04<42:30:58, 18.32s/it]

 22%|██▏       | 2385/10740 [11:53:25<44:27:28, 19.16s/it]
[2024-04-02 07:07:08,801] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2386/10740 [11:53:39<40:35:55, 17.50s/it]
[2024-04-02 07:07:22,421] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2387/10740 [11:53:57<41:13:38, 17.77s/it]

 22%|██▏       | 2388/10740 [11:54:15<41:18:25, 17.80s/it]

 22%|██▏       | 2389/10740 [11:54:32<40:38:30, 17.52s/it]

 22%|██▏       | 2390/10740 [11:54:51<41:36:17, 17.94s/it]

 22%|██▏       | 2391/10740 [11:55:10<42:17:53, 18.24s/it]

 22%|██▏       | 2392/10740 [11:55:22<38:06:20, 16.43s/it]

 22%|██▏       | 2393/10740 [11:55:37<37:08:51, 16.02s/it]

 22%|██▏       | 2394/10740 [11:55:48<33:27:48, 14.43s/it]

 22%|██▏       | 2395/10740 [11:56:08<37:15:06, 16.07s/it]

 22%|██▏       | 2396/10740 [11:56:30<41:27:00, 17.88s/it]
{'loss': 0.4455, 'learning_rate': 1.8108565803420289e-06, 'rewards/chosen': -1.0227043628692627, 'rewards/rejected': -2.1455237865448, 'rewards/accuracies': 0.875, 'rewards/margins': 1.122819423675537, 'policy_logps/rejected': -366.6224670410156, 'policy_logps/chosen': -340.94036865234375, 'referece_logps/rejected': -345.1672058105469, 'referece_logps/chosen': -330.7132873535156, 'logits/rejected': -0.5675593614578247, 'logits/chosen': -0.5699073076248169, 'epoch': 1.34}


 22%|██▏       | 2398/10740 [11:57:06<41:30:09, 17.91s/it]

 22%|██▏       | 2399/10740 [11:57:26<42:43:09, 18.44s/it]

 22%|██▏       | 2400/10740 [11:57:44<42:50:34, 18.49s/it]

 22%|██▏       | 2401/10740 [11:58:00<41:00:49, 17.71s/it]

 22%|██▏       | 2402/10740 [11:58:17<40:35:28, 17.53s/it]
[2024-04-02 07:12:01,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2403/10740 [11:58:35<40:36:55, 17.54s/it]

 22%|██▏       | 2404/10740 [11:58:53<41:10:16, 17.78s/it]
{'loss': 0.4294, 'learning_rate': 1.8094422212611207e-06, 'rewards/chosen': -1.4016196727752686, 'rewards/rejected': -2.147261381149292, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7456417083740234, 'policy_logps/rejected': -335.3931579589844, 'policy_logps/chosen': -317.99822998046875, 'referece_logps/rejected': -313.9205627441406, 'referece_logps/chosen': -303.9820556640625, 'logits/rejected': -0.0787201076745987, 'logits/chosen': -0.189143568277359, 'epoch': 1.34}

 22%|██▏       | 2405/10740 [11:59:14<42:59:12, 18.57s/it]

 22%|██▏       | 2406/10740 [11:59:34<44:01:45, 19.02s/it]


 22%|██▏       | 2408/10740 [12:00:17<47:10:42, 20.38s/it]

 22%|██▏       | 2409/10740 [12:00:37<46:40:04, 20.17s/it]

 22%|██▏       | 2410/10740 [12:00:58<47:31:39, 20.54s/it]
[2024-04-02 07:14:42,025] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2411/10740 [12:01:17<46:34:57, 20.13s/it]

 22%|██▏       | 2412/10740 [12:01:30<41:22:24, 17.88s/it]

 22%|██▏       | 2413/10740 [12:01:50<42:28:44, 18.36s/it]

 22%|██▏       | 2414/10740 [12:02:09<43:19:08, 18.73s/it]

 22%|██▏       | 2415/10740 [12:02:31<45:45:41, 19.79s/it]

 22%|██▏       | 2416/10740 [12:02:49<44:03:56, 19.06s/it]

 23%|██▎       | 2417/10740 [12:03:01<38:59:49, 16.87s/it]

 23%|██▎       | 2418/10740 [12:03:19<40:25:13, 17.49s/it]

 23%|██▎       | 2419/10740 [12:03:31<36:29:16, 15.79s/it]

 23%|██▎       | 2420/10740 [12:03:52<39:43:05, 17.19s/it]
{'loss': 0.4904, 'learning_rate': 1.8065993761814178e-06, 'rewards/chosen': -0.7330890893936157, 'rewards/rejected': -1.9692027568817139, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2361137866973877, 'policy_logps/rejected': -434.62164306640625, 'policy_logps/chosen': -525.4757080078125, 'referece_logps/rejected': -414.9295959472656, 'referece_logps/chosen': -518.1448364257812, 'logits/rejected': 0.8642759323120117, 'logits/chosen': 0.9115569591522217, 'epoch': 1.35}


 23%|██▎       | 2422/10740 [12:04:19<35:36:19, 15.41s/it]
{'loss': 0.5471, 'learning_rate': 1.806242698667335e-06, 'rewards/chosen': -0.5167180895805359, 'rewards/rejected': -1.799503207206726, 'rewards/accuracies': 0.875, 'rewards/margins': 1.282785177230835, 'policy_logps/rejected': -538.9515380859375, 'policy_logps/chosen': -366.0486755371094, 'referece_logps/rejected': -520.9564819335938, 'referece_logps/chosen': -360.88153076171875, 'logits/rejected': -0.0493321418762207, 'logits/chosen': 0.003134012222290039, 'epoch': 1.35}


 23%|██▎       | 2424/10740 [12:04:49<34:15:20, 14.83s/it]

 23%|██▎       | 2425/10740 [12:05:10<38:14:07, 16.55s/it]
{'loss': 0.5084, 'learning_rate': 1.8057071324636728e-06, 'rewards/chosen': -1.3547393083572388, 'rewards/rejected': -3.2008965015411377, 'rewards/accuracies': 0.875, 'rewards/margins': 1.846157193183899, 'policy_logps/rejected': -375.05096435546875, 'policy_logps/chosen': -378.45587158203125, 'referece_logps/rejected': -343.0419921875, 'referece_logps/chosen': -364.9084777832031, 'logits/rejected': -1.0265469551086426, 'logits/chosen': -1.017143964767456, 'epoch': 1.35}

 23%|██▎       | 2426/10740 [12:05:26<38:09:38, 16.52s/it]


 23%|██▎       | 2428/10740 [12:06:09<44:05:24, 19.10s/it]

 23%|██▎       | 2429/10740 [12:06:31<45:55:05, 19.89s/it]

 23%|██▎       | 2430/10740 [12:06:50<45:11:04, 19.57s/it]

 23%|██▎       | 2431/10740 [12:07:07<43:24:00, 18.80s/it]

 23%|██▎       | 2432/10740 [12:07:27<44:37:48, 19.34s/it]

 23%|██▎       | 2433/10740 [12:07:43<42:08:25, 18.26s/it]

 23%|██▎       | 2434/10740 [12:08:01<42:08:48, 18.27s/it]

 23%|██▎       | 2435/10740 [12:08:21<43:21:21, 18.79s/it]

 23%|██▎       | 2436/10740 [12:08:38<41:44:09, 18.09s/it]
{'loss': 0.4921, 'learning_rate': 1.8037377504170188e-06, 'rewards/chosen': -1.028100848197937, 'rewards/rejected': -3.204411745071411, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1763107776641846, 'policy_logps/rejected': -320.50762939453125, 'policy_logps/chosen': -336.0777893066406, 'referece_logps/rejected': -288.4635009765625, 'referece_logps/chosen': -325.79681396484375, 'logits/rejected': -0.5771307349205017, 'logits/chosen': -0.5176019072532654, 'epoch': 1.36}


 23%|██▎       | 2438/10740 [12:09:15<42:46:58, 18.55s/it]
[2024-04-02 07:22:58,982] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2439/10740 [12:09:30<40:08:24, 17.41s/it]

 23%|██▎       | 2440/10740 [12:09:48<40:13:20, 17.45s/it]
[2024-04-02 07:23:31,255] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2441/10740 [12:10:09<43:21:04, 18.81s/it]

 23%|██▎       | 2442/10740 [12:10:26<41:33:22, 18.03s/it]

 23%|██▎       | 2443/10740 [12:10:45<42:35:36, 18.48s/it]
{'loss': 0.4801, 'learning_rate': 1.8024799005078037e-06, 'rewards/chosen': -0.1018407940864563, 'rewards/rejected': -1.8503960371017456, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7485551834106445, 'policy_logps/rejected': -513.816162109375, 'policy_logps/chosen': -544.8187866210938, 'referece_logps/rejected': -495.3121337890625, 'referece_logps/chosen': -543.8004150390625, 'logits/rejected': 0.28958582878112793, 'logits/chosen': 0.14871250092983246, 'epoch': 1.36}


 23%|██▎       | 2445/10740 [12:11:10<35:32:48, 15.43s/it]
{'loss': 0.5372, 'learning_rate': 1.80211985768651e-06, 'rewards/chosen': -1.063876748085022, 'rewards/rejected': -1.345794916152954, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2819182276725769, 'policy_logps/rejected': -535.0989990234375, 'policy_logps/chosen': -444.8182067871094, 'referece_logps/rejected': -521.6409912109375, 'referece_logps/chosen': -434.1794128417969, 'logits/rejected': -0.3403246998786926, 'logits/chosen': -0.3120356500148773, 'epoch': 1.37}


 23%|██▎       | 2447/10740 [12:11:45<38:54:18, 16.89s/it]

 23%|██▎       | 2448/10740 [12:12:00<37:06:25, 16.11s/it]
{'loss': 0.4242, 'learning_rate': 1.8015792463347825e-06, 'rewards/chosen': -1.545488953590393, 'rewards/rejected': -2.787970781326294, 'rewards/accuracies': 0.5, 'rewards/margins': 1.2424818277359009, 'policy_logps/rejected': -404.7734375, 'policy_logps/chosen': -376.43951416015625, 'referece_logps/rejected': -376.8937072753906, 'referece_logps/chosen': -360.9846496582031, 'logits/rejected': -1.021230697631836, 'logits/chosen': -1.046088695526123, 'epoch': 1.37}
[2024-04-02 07:26:04,283] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2450/10740 [12:12:38<40:12:10, 17.46s/it]

 23%|██▎       | 2451/10740 [12:12:58<41:50:19, 18.17s/it]

 23%|██▎       | 2452/10740 [12:13:15<41:26:03, 18.00s/it]
{'loss': 0.484, 'learning_rate': 1.8008574105973961e-06, 'rewards/chosen': -0.22773334383964539, 'rewards/rejected': -1.8427314758300781, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6149981021881104, 'policy_logps/rejected': -317.9883728027344, 'policy_logps/chosen': -292.7283935546875, 'referece_logps/rejected': -299.5610656738281, 'referece_logps/chosen': -290.4510803222656, 'logits/rejected': -2.0478665828704834, 'logits/chosen': -2.0413191318511963, 'epoch': 1.37}

 23%|██▎       | 2453/10740 [12:13:35<42:48:26, 18.60s/it]

 23%|██▎       | 2454/10740 [12:13:54<42:38:01, 18.52s/it]
{'loss': 0.4589, 'learning_rate': 1.8004960556233605e-06, 'rewards/chosen': -0.8474797010421753, 'rewards/rejected': -2.6721720695495605, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8246926069259644, 'policy_logps/rejected': -400.5382995605469, 'policy_logps/chosen': -349.30419921875, 'referece_logps/rejected': -373.8165588378906, 'referece_logps/chosen': -340.82940673828125, 'logits/rejected': -0.20730091631412506, 'logits/chosen': -0.23772665858268738, 'epoch': 1.37}

 23%|██▎       | 2455/10740 [12:14:11<41:51:48, 18.19s/it]


 23%|██▎       | 2457/10740 [12:14:52<44:53:29, 19.51s/it]
{'loss': 0.4141, 'learning_rate': 1.7999534771503355e-06, 'rewards/chosen': -1.7833433151245117, 'rewards/rejected': -2.2346930503845215, 'rewards/accuracies': 0.5, 'rewards/margins': 0.45134973526000977, 'policy_logps/rejected': -363.9668884277344, 'policy_logps/chosen': -346.7389831542969, 'referece_logps/rejected': -341.6199645996094, 'referece_logps/chosen': -328.9055480957031, 'logits/rejected': -0.54244065284729, 'logits/chosen': -0.4984261393547058, 'epoch': 1.37}


 23%|██▎       | 2459/10740 [12:15:22<38:29:21, 16.73s/it]
{'loss': 0.5873, 'learning_rate': 1.7995913943522499e-06, 'rewards/chosen': -1.6814358234405518, 'rewards/rejected': -3.5328822135925293, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8514463901519775, 'policy_logps/rejected': -415.7711486816406, 'policy_logps/chosen': -391.0979919433594, 'referece_logps/rejected': -380.4423828125, 'referece_logps/chosen': -374.28363037109375, 'logits/rejected': -1.3943071365356445, 'logits/chosen': -1.3805721998214722, 'epoch': 1.37}


 23%|██▎       | 2461/10740 [12:15:58<39:47:12, 17.30s/it]

 23%|██▎       | 2462/10740 [12:16:14<38:49:37, 16.89s/it]

 23%|██▎       | 2463/10740 [12:16:26<35:25:01, 15.40s/it]

 23%|██▎       | 2464/10740 [12:16:44<37:37:45, 16.37s/it]
[2024-04-02 07:30:28,205] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2465/10740 [12:17:05<40:10:16, 17.48s/it]
{'loss': 0.491, 'learning_rate': 1.798503401090321e-06, 'rewards/chosen': -1.1516491174697876, 'rewards/rejected': -1.6224098205566406, 'rewards/accuracies': 0.625, 'rewards/margins': 0.470760703086853, 'policy_logps/rejected': -472.05682373046875, 'policy_logps/chosen': -430.3831481933594, 'referece_logps/rejected': -455.832763671875, 'referece_logps/chosen': -418.86669921875, 'logits/rejected': -0.419956773519516, 'logits/chosen': -0.4991876184940338, 'epoch': 1.38}
[2024-04-02 07:31:08,810] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2467/10740 [12:17:38<38:18:29, 16.67s/it]
{'loss': 0.4281, 'learning_rate': 1.7981401554869235e-06, 'rewards/chosen': -1.5275325775146484, 'rewards/rejected': -2.7531275749206543, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2255949974060059, 'policy_logps/rejected': -365.0565185546875, 'policy_logps/chosen': -326.00006103515625, 'referece_logps/rejected': -337.5252685546875, 'referece_logps/chosen': -310.7247619628906, 'logits/rejected': -0.2757349908351898, 'logits/chosen': -0.294425904750824, 'epoch': 1.38}

 23%|██▎       | 2468/10740 [12:17:53<37:19:06, 16.24s/it]

 23%|██▎       | 2469/10740 [12:18:11<38:27:43, 16.74s/it]


 23%|██▎       | 2471/10740 [12:18:46<38:37:27, 16.82s/it]
{'loss': 0.5423, 'learning_rate': 1.7974127932987522e-06, 'rewards/chosen': -1.867723822593689, 'rewards/rejected': -2.1643903255462646, 'rewards/accuracies': 0.625, 'rewards/margins': 0.29666653275489807, 'policy_logps/rejected': -411.821533203125, 'policy_logps/chosen': -442.98431396484375, 'referece_logps/rejected': -390.1776123046875, 'referece_logps/chosen': -424.30712890625, 'logits/rejected': -0.9214911460876465, 'logits/chosen': -0.935725748538971, 'epoch': 1.38}


 23%|██▎       | 2473/10740 [12:19:16<37:03:30, 16.14s/it]

 23%|██▎       | 2474/10740 [12:19:32<37:19:01, 16.25s/it]
{'loss': 0.4513, 'learning_rate': 1.796866510069945e-06, 'rewards/chosen': -1.1238596439361572, 'rewards/rejected': -1.393348217010498, 'rewards/accuracies': 0.625, 'rewards/margins': 0.26948851346969604, 'policy_logps/rejected': -370.551025390625, 'policy_logps/chosen': -462.93218994140625, 'referece_logps/rejected': -356.6175842285156, 'referece_logps/chosen': -451.693603515625, 'logits/rejected': -0.9564310312271118, 'logits/chosen': -1.0103646516799927, 'epoch': 1.38}


 23%|██▎       | 2476/10740 [12:20:14<42:27:04, 18.49s/it]
{'loss': 0.309, 'learning_rate': 1.79650195883832e-06, 'rewards/chosen': -0.9410085082054138, 'rewards/rejected': -2.9916412830352783, 'rewards/accuracies': 1.0, 'rewards/margins': 2.050632953643799, 'policy_logps/rejected': -341.08184814453125, 'policy_logps/chosen': -230.40328979492188, 'referece_logps/rejected': -311.16546630859375, 'referece_logps/chosen': -220.99319458007812, 'logits/rejected': -0.8208141326904297, 'logits/chosen': -0.8431702256202698, 'epoch': 1.38}

 23%|██▎       | 2477/10740 [12:20:34<43:25:03, 18.92s/it]

 23%|██▎       | 2478/10740 [12:20:52<42:43:22, 18.62s/it]


 23%|██▎       | 2480/10740 [12:21:31<43:46:27, 19.08s/it]

 23%|██▎       | 2481/10740 [12:21:54<46:55:48, 20.46s/it]
[2024-04-02 07:35:38,028] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2482/10740 [12:22:15<46:54:48, 20.45s/it]

 23%|██▎       | 2483/10740 [12:22:35<46:40:17, 20.35s/it]
[2024-04-02 07:36:18,577] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2484/10740 [12:22:51<43:28:27, 18.96s/it]

 23%|██▎       | 2485/10740 [12:23:10<43:47:21, 19.10s/it]

 23%|██▎       | 2486/10740 [12:23:32<45:53:13, 20.01s/it]
{'loss': 0.4937, 'learning_rate': 1.7946748587081388e-06, 'rewards/chosen': -1.970762014389038, 'rewards/rejected': -4.1321258544921875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1613638401031494, 'policy_logps/rejected': -423.0984191894531, 'policy_logps/chosen': -475.6663818359375, 'referece_logps/rejected': -381.7771911621094, 'referece_logps/chosen': -455.95880126953125, 'logits/rejected': -0.5582431554794312, 'logits/chosen': -0.5191624164581299, 'epoch': 1.39}
[2024-04-02 07:37:37,368] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2488/10740 [12:24:16<48:18:44, 21.08s/it]
[2024-04-02 07:37:59,881] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3379, 'learning_rate': 1.7943085708182947e-06, 'rewards/chosen': -1.9824284315109253, 'rewards/rejected': -3.5380260944366455, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5555976629257202, 'policy_logps/rejected': -621.3225708007812, 'policy_logps/chosen': -487.3416442871094, 'referece_logps/rejected': -585.9422607421875, 'referece_logps/chosen': -467.5173645019531, 'logits/rejected': 0.6501423120498657, 'logits/chosen': 0.72422194480896, 'epoch': 1.39}
[2024-04-02 07:38:15,289] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2489/10740 [12:24:32<44:24:32, 19.38s/it]
[2024-04-02 07:38:37,181] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2491/10740 [12:25:13<45:47:43, 19.99s/it]
{'loss': 0.4478, 'learning_rate': 1.79375859719287e-06, 'rewards/chosen': 0.047664955258369446, 'rewards/rejected': -1.7576334476470947, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8052984476089478, 'policy_logps/rejected': -243.58297729492188, 'policy_logps/chosen': -268.7474060058594, 'referece_logps/rejected': -226.00662231445312, 'referece_logps/chosen': -269.22406005859375, 'logits/rejected': -0.08233489096164703, 'logits/chosen': -0.0929618775844574, 'epoch': 1.39}


 23%|██▎       | 2493/10740 [12:25:51<44:18:29, 19.34s/it]
{'loss': 0.4673, 'learning_rate': 1.7933915871100083e-06, 'rewards/chosen': -1.676139235496521, 'rewards/rejected': -2.537292242050171, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8611529469490051, 'policy_logps/rejected': -433.743896484375, 'policy_logps/chosen': -514.7366943359375, 'referece_logps/rejected': -408.3710021972656, 'referece_logps/chosen': -497.97528076171875, 'logits/rejected': -0.35042572021484375, 'logits/chosen': -0.4267647862434387, 'epoch': 1.39}

 23%|██▎       | 2494/10740 [12:26:05<40:48:19, 17.81s/it]


 23%|██▎       | 2496/10740 [12:26:43<42:09:34, 18.41s/it]

 23%|██▎       | 2497/10740 [12:27:01<41:59:49, 18.34s/it]
{'loss': 0.3805, 'learning_rate': 1.7926567011470049e-06, 'rewards/chosen': -1.0932259559631348, 'rewards/rejected': -2.479557752609253, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3863316774368286, 'policy_logps/rejected': -309.30682373046875, 'policy_logps/chosen': -220.130126953125, 'referece_logps/rejected': -284.5112609863281, 'referece_logps/chosen': -209.19786071777344, 'logits/rejected': -0.18684372305870056, 'logits/chosen': -0.3309684097766876, 'epoch': 1.39}

 23%|██▎       | 2498/10740 [12:27:18<40:56:41, 17.88s/it]


 23%|██▎       | 2500/10740 [12:27:59<43:57:39, 19.21s/it]
[2024-04-02 07:41:42,404] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5183, 'learning_rate': 1.792104779628546e-06, 'rewards/chosen': -0.68245929479599, 'rewards/rejected': -2.31466007232666, 'rewards/accuracies': 0.875, 'rewards/margins': 1.632200837135315, 'policy_logps/rejected': -508.97003173828125, 'policy_logps/chosen': -441.7832336425781, 'referece_logps/rejected': -485.82342529296875, 'referece_logps/chosen': -434.9586486816406, 'logits/rejected': 0.002551339566707611, 'logits/chosen': -0.04243338480591774, 'epoch': 1.4}
[2024-04-02 07:42:11,787] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2502/10740 [12:28:45<47:01:34, 20.55s/it]

 23%|██▎       | 2503/10740 [12:29:03<45:13:23, 19.76s/it]

 23%|██▎       | 2504/10740 [12:29:25<46:53:28, 20.50s/it]

 23%|██▎       | 2505/10740 [12:29:45<46:38:36, 20.39s/it]

 23%|██▎       | 2506/10740 [12:29:59<42:08:44, 18.43s/it]
{'loss': 0.3409, 'learning_rate': 1.790998991859513e-06, 'rewards/chosen': -1.333666443824768, 'rewards/rejected': -3.2789413928985596, 'rewards/accuracies': 0.75, 'rewards/margins': 1.945274829864502, 'policy_logps/rejected': -422.9190979003906, 'policy_logps/chosen': -515.7655639648438, 'referece_logps/rejected': -390.12969970703125, 'referece_logps/chosen': -502.42889404296875, 'logits/rejected': -1.444032073020935, 'logits/chosen': -1.5985095500946045, 'epoch': 1.4}


 23%|██▎       | 2508/10740 [12:30:39<43:56:12, 19.21s/it]

 23%|██▎       | 2509/10740 [12:30:55<41:27:29, 18.13s/it]

 23%|██▎       | 2510/10740 [12:31:14<42:34:49, 18.63s/it]
{'loss': 0.4964, 'learning_rate': 1.7902603609240622e-06, 'rewards/chosen': -1.0478160381317139, 'rewards/rejected': -1.8964741230010986, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8486580848693848, 'policy_logps/rejected': -377.35125732421875, 'policy_logps/chosen': -413.24591064453125, 'referece_logps/rejected': -358.3865051269531, 'referece_logps/chosen': -402.7677307128906, 'logits/rejected': -0.5295960903167725, 'logits/chosen': -0.5756692886352539, 'epoch': 1.4}

 23%|██▎       | 2511/10740 [12:31:34<43:10:54, 18.89s/it]


 23%|██▎       | 2513/10740 [12:32:07<40:18:42, 17.64s/it]
{'loss': 0.4529, 'learning_rate': 1.7897056329644765e-06, 'rewards/chosen': -1.084064245223999, 'rewards/rejected': -1.4465595483779907, 'rewards/accuracies': 0.75, 'rewards/margins': 0.36249518394470215, 'policy_logps/rejected': -608.3178100585938, 'policy_logps/chosen': -459.70989990234375, 'referece_logps/rejected': -593.8521728515625, 'referece_logps/chosen': -448.8692321777344, 'logits/rejected': -0.37358763813972473, 'logits/chosen': -0.47235584259033203, 'epoch': 1.4}


 23%|██▎       | 2515/10740 [12:32:41<38:44:42, 16.96s/it]
{'loss': 0.5628, 'learning_rate': 1.7893354551684026e-06, 'rewards/chosen': -1.5141738653182983, 'rewards/rejected': -2.4259912967681885, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9118176698684692, 'policy_logps/rejected': -403.42425537109375, 'policy_logps/chosen': -380.99700927734375, 'referece_logps/rejected': -379.164306640625, 'referece_logps/chosen': -365.8553161621094, 'logits/rejected': -0.44023841619491577, 'logits/chosen': -0.4023360013961792, 'epoch': 1.41}


 23%|██▎       | 2517/10740 [12:33:13<37:07:55, 16.26s/it]

 23%|██▎       | 2518/10740 [12:33:32<38:28:13, 16.84s/it]
{'loss': 0.5581, 'learning_rate': 1.7887796500764528e-06, 'rewards/chosen': -1.4267534017562866, 'rewards/rejected': -2.2006070613861084, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7738537192344666, 'policy_logps/rejected': -487.9808349609375, 'policy_logps/chosen': -488.9917907714844, 'referece_logps/rejected': -465.9747314453125, 'referece_logps/chosen': -474.7242736816406, 'logits/rejected': -0.5836902856826782, 'logits/chosen': -0.5847286581993103, 'epoch': 1.41}

 23%|██▎       | 2519/10740 [12:33:48<38:23:08, 16.81s/it]


 23%|██▎       | 2521/10740 [12:34:23<39:12:45, 17.18s/it]
{'loss': 0.4725, 'learning_rate': 1.7882231993115498e-06, 'rewards/chosen': -1.0576905012130737, 'rewards/rejected': -2.765143632888794, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7074532508850098, 'policy_logps/rejected': -367.54864501953125, 'policy_logps/chosen': -271.7046813964844, 'referece_logps/rejected': -339.897216796875, 'referece_logps/chosen': -261.1277770996094, 'logits/rejected': -0.682530403137207, 'logits/chosen': -0.724468469619751, 'epoch': 1.41}

 23%|██▎       | 2522/10740 [12:34:40<38:50:07, 17.01s/it]


 24%|██▎       | 2524/10740 [12:35:16<38:54:20, 17.05s/it]
{'loss': 0.4961, 'learning_rate': 1.7876661033291884e-06, 'rewards/chosen': -1.564332127571106, 'rewards/rejected': -3.0673627853393555, 'rewards/accuracies': 0.75, 'rewards/margins': 1.50303053855896, 'policy_logps/rejected': -423.42474365234375, 'policy_logps/chosen': -396.14678955078125, 'referece_logps/rejected': -392.7511291503906, 'referece_logps/chosen': -380.50347900390625, 'logits/rejected': -1.3037422895431519, 'logits/chosen': -1.314839243888855, 'epoch': 1.41}

 24%|██▎       | 2525/10740 [12:35:34<39:38:30, 17.37s/it]

 24%|██▎       | 2526/10740 [12:35:53<40:58:42, 17.96s/it]


 24%|██▎       | 2528/10740 [12:36:33<43:15:47, 18.97s/it]
{'loss': 0.4486, 'learning_rate': 1.7869223058027027e-06, 'rewards/chosen': -1.3053959608078003, 'rewards/rejected': -2.2042174339294434, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8988215327262878, 'policy_logps/rejected': -326.5653076171875, 'policy_logps/chosen': -310.2449035644531, 'referece_logps/rejected': -304.5231628417969, 'referece_logps/chosen': -297.1909484863281, 'logits/rejected': -0.910763680934906, 'logits/chosen': -1.0518975257873535, 'epoch': 1.41}

 24%|██▎       | 2529/10740 [12:36:54<44:17:56, 19.42s/it]


 24%|██▎       | 2531/10740 [12:37:34<44:39:04, 19.58s/it]
{'loss': 0.4422, 'learning_rate': 1.7863637060872747e-06, 'rewards/chosen': -1.4420762062072754, 'rewards/rejected': -4.828182697296143, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3861067295074463, 'policy_logps/rejected': -293.7728576660156, 'policy_logps/chosen': -367.89581298828125, 'referece_logps/rejected': -245.49107360839844, 'referece_logps/chosen': -353.4750671386719, 'logits/rejected': -0.11861807107925415, 'logits/chosen': -0.0781126618385315, 'epoch': 1.41}

 24%|██▎       | 2532/10740 [12:37:49<41:40:01, 18.28s/it]

 24%|██▎       | 2533/10740 [12:38:10<43:48:02, 19.21s/it]

 24%|██▎       | 2534/10740 [12:38:31<44:35:06, 19.56s/it]


 24%|██▎       | 2536/10740 [12:39:10<44:11:39, 19.39s/it]

 24%|██▎       | 2537/10740 [12:39:30<44:38:23, 19.59s/it]

 24%|██▎       | 2538/10740 [12:39:50<44:41:49, 19.62s/it]
{'loss': 0.3752, 'learning_rate': 1.7850578042829654e-06, 'rewards/chosen': -0.893197238445282, 'rewards/rejected': -1.6951932907104492, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8019960522651672, 'policy_logps/rejected': -573.4456787109375, 'policy_logps/chosen': -515.38134765625, 'referece_logps/rejected': -556.4937744140625, 'referece_logps/chosen': -506.4493408203125, 'logits/rejected': 0.3957928717136383, 'logits/chosen': 0.23549270629882812, 'epoch': 1.42}

 24%|██▎       | 2539/10740 [12:40:09<44:23:22, 19.49s/it]

 24%|██▎       | 2540/10740 [12:40:24<41:45:36, 18.33s/it]


 24%|██▎       | 2542/10740 [12:41:00<41:05:36, 18.05s/it]
{'loss': 0.4756, 'learning_rate': 1.7843100034426215e-06, 'rewards/chosen': -1.434705138206482, 'rewards/rejected': -2.4271953105926514, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9924901723861694, 'policy_logps/rejected': -384.9598083496094, 'policy_logps/chosen': -357.25286865234375, 'referece_logps/rejected': -360.6878356933594, 'referece_logps/chosen': -342.90582275390625, 'logits/rejected': -0.052083343267440796, 'logits/chosen': -0.024244368076324463, 'epoch': 1.42}

 24%|██▎       | 2543/10740 [12:41:10<36:04:25, 15.84s/it]

 24%|██▎       | 2544/10740 [12:41:23<33:47:42, 14.84s/it]

 24%|██▎       | 2545/10740 [12:41:43<37:36:02, 16.52s/it]

 24%|██▎       | 2546/10740 [12:42:03<40:05:49, 17.62s/it]

 24%|██▎       | 2547/10740 [12:42:23<41:29:03, 18.23s/it]

 24%|██▎       | 2548/10740 [12:42:43<42:20:03, 18.60s/it]

 24%|██▎       | 2549/10740 [12:43:01<42:16:53, 18.58s/it]


 24%|██▍       | 2551/10740 [12:43:36<40:50:50, 17.96s/it]
{'loss': 0.4111, 'learning_rate': 1.7826232801201407e-06, 'rewards/chosen': -0.8328177332878113, 'rewards/rejected': -2.970242500305176, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1374247074127197, 'policy_logps/rejected': -428.6099548339844, 'policy_logps/chosen': -261.2637023925781, 'referece_logps/rejected': -398.9075012207031, 'referece_logps/chosen': -252.93551635742188, 'logits/rejected': -1.3757569789886475, 'logits/chosen': -1.345647931098938, 'epoch': 1.43}

 24%|██▍       | 2552/10740 [12:43:54<41:10:13, 18.10s/it]

 24%|██▍       | 2553/10740 [12:44:14<42:27:49, 18.67s/it]

 24%|██▍       | 2554/10740 [12:44:29<39:55:27, 17.56s/it]

 24%|██▍       | 2555/10740 [12:44:49<41:04:57, 18.07s/it]

 24%|██▍       | 2556/10740 [12:45:02<37:40:28, 16.57s/it]

 24%|██▍       | 2557/10740 [12:45:23<40:57:03, 18.02s/it]

 24%|██▍       | 2558/10740 [12:45:41<40:53:30, 17.99s/it]


 24%|██▍       | 2560/10740 [12:46:22<44:20:49, 19.52s/it]
{'loss': 0.3981, 'learning_rate': 1.7809307910991174e-06, 'rewards/chosen': -1.0056651830673218, 'rewards/rejected': -2.4627983570098877, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4571330547332764, 'policy_logps/rejected': -363.21551513671875, 'policy_logps/chosen': -341.8778076171875, 'referece_logps/rejected': -338.5875244140625, 'referece_logps/chosen': -331.8211364746094, 'logits/rejected': -0.9268279075622559, 'logits/chosen': -0.9240704774856567, 'epoch': 1.43}

 24%|██▍       | 2561/10740 [12:46:41<44:01:53, 19.38s/it]


 24%|██▍       | 2563/10740 [12:47:20<44:23:21, 19.54s/it]
{'loss': 0.3867, 'learning_rate': 1.780365348979648e-06, 'rewards/chosen': -0.957560122013092, 'rewards/rejected': -2.4474732875823975, 'rewards/accuracies': 0.875, 'rewards/margins': 1.489912986755371, 'policy_logps/rejected': -244.03692626953125, 'policy_logps/chosen': -332.638916015625, 'referece_logps/rejected': -219.5622100830078, 'referece_logps/chosen': -323.0633544921875, 'logits/rejected': -1.6233158111572266, 'logits/chosen': -1.6573657989501953, 'epoch': 1.43}


 24%|██▍       | 2565/10740 [12:47:56<42:20:39, 18.65s/it]

 24%|██▍       | 2566/10740 [12:48:17<43:35:33, 19.20s/it]

 24%|██▍       | 2567/10740 [12:48:41<46:51:18, 20.64s/it]

 24%|██▍       | 2568/10740 [12:49:01<46:24:40, 20.45s/it]
{'loss': 0.4052, 'learning_rate': 1.779421526153247e-06, 'rewards/chosen': -1.7833521366119385, 'rewards/rejected': -2.7038822174072266, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9205300807952881, 'policy_logps/rejected': -275.5997619628906, 'policy_logps/chosen': -362.4708557128906, 'referece_logps/rejected': -248.5609130859375, 'referece_logps/chosen': -344.6373291015625, 'logits/rejected': -1.398427963256836, 'logits/chosen': -1.4437929391860962, 'epoch': 1.43}


 24%|██▍       | 2570/10740 [12:49:41<45:53:14, 20.22s/it]

 24%|██▍       | 2571/10740 [12:50:00<45:26:08, 20.02s/it]
{'loss': 0.5925, 'learning_rate': 1.778854381636453e-06, 'rewards/chosen': -1.5094759464263916, 'rewards/rejected': -2.5899291038513184, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0804531574249268, 'policy_logps/rejected': -488.998779296875, 'policy_logps/chosen': -293.1705017089844, 'referece_logps/rejected': -463.0994873046875, 'referece_logps/chosen': -278.07574462890625, 'logits/rejected': -1.4258477687835693, 'logits/chosen': -1.265714168548584, 'epoch': 1.44}

 24%|██▍       | 2572/10740 [12:50:14<40:59:56, 18.07s/it]

 24%|██▍       | 2573/10740 [12:50:35<43:02:00, 18.97s/it]

 24%|██▍       | 2574/10740 [12:50:54<43:08:39, 19.02s/it]

 24%|██▍       | 2575/10740 [12:51:14<43:32:37, 19.20s/it]


 24%|██▍       | 2577/10740 [12:51:37<34:28:26, 15.20s/it]
{'loss': 0.5365, 'learning_rate': 1.777718180422418e-06, 'rewards/chosen': -1.5976506471633911, 'rewards/rejected': -1.6022498607635498, 'rewards/accuracies': 0.5, 'rewards/margins': 0.004599258303642273, 'policy_logps/rejected': -356.5500183105469, 'policy_logps/chosen': -323.7365417480469, 'referece_logps/rejected': -340.5275573730469, 'referece_logps/chosen': -307.7600402832031, 'logits/rejected': -0.2820588946342468, 'logits/chosen': -0.241925448179245, 'epoch': 1.44}

 24%|██▍       | 2578/10740 [12:51:52<34:49:52, 15.36s/it]

 24%|██▍       | 2579/10740 [12:52:07<34:40:06, 15.29s/it]
[2024-04-02 08:06:11,972] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2580/10740 [12:52:28<38:26:39, 16.96s/it]

 24%|██▍       | 2581/10740 [12:52:48<40:18:17, 17.78s/it]

 24%|██▍       | 2582/10740 [12:53:04<39:09:09, 17.28s/it]

 24%|██▍       | 2583/10740 [12:53:19<37:50:20, 16.70s/it]

 24%|██▍       | 2584/10740 [12:53:39<39:50:23, 17.58s/it]

 24%|██▍       | 2585/10740 [12:53:52<36:39:59, 16.19s/it]
[2024-04-02 08:07:55,195] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2586/10740 [12:54:11<38:54:43, 17.18s/it]

 24%|██▍       | 2587/10740 [12:54:28<38:31:08, 17.01s/it]


 24%|██▍       | 2589/10740 [12:55:03<39:29:20, 17.44s/it]
{'loss': 0.4534, 'learning_rate': 1.7754381423043744e-06, 'rewards/chosen': -1.0593883991241455, 'rewards/rejected': -1.9007813930511475, 'rewards/accuracies': 0.75, 'rewards/margins': 0.841392993927002, 'policy_logps/rejected': -499.77783203125, 'policy_logps/chosen': -447.72076416015625, 'referece_logps/rejected': -480.7700500488281, 'referece_logps/chosen': -437.1268310546875, 'logits/rejected': -1.0170552730560303, 'logits/chosen': -0.8629635572433472, 'epoch': 1.45}

 24%|██▍       | 2590/10740 [12:55:17<37:22:43, 16.51s/it]

 24%|██▍       | 2591/10740 [12:55:35<38:24:57, 16.97s/it]
[2024-04-02 08:09:42,129] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 24%|██▍       | 2593/10740 [12:56:19<43:33:12, 19.25s/it]
{'loss': 0.4376, 'learning_rate': 1.774675871226281e-06, 'rewards/chosen': -1.764385461807251, 'rewards/rejected': -2.2515392303466797, 'rewards/accuracies': 0.625, 'rewards/margins': 0.48715344071388245, 'policy_logps/rejected': -415.00274658203125, 'policy_logps/chosen': -327.0452575683594, 'referece_logps/rejected': -392.48736572265625, 'referece_logps/chosen': -309.40142822265625, 'logits/rejected': -0.8411613702774048, 'logits/chosen': -0.9345390200614929, 'epoch': 1.45}

 24%|██▍       | 2594/10740 [12:56:40<45:08:50, 19.95s/it]
[2024-04-02 08:10:35,985] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2595/10740 [12:56:52<39:49:28, 17.60s/it]

 24%|██▍       | 2596/10740 [12:57:10<40:04:21, 17.71s/it]

 24%|██▍       | 2597/10740 [12:57:30<41:17:08, 18.25s/it]

 24%|██▍       | 2598/10740 [12:57:43<37:35:01, 16.62s/it]

 24%|██▍       | 2599/10740 [12:58:00<38:05:08, 16.84s/it]

 24%|██▍       | 2600/10740 [12:58:20<40:12:42, 17.78s/it]

 24%|██▍       | 2601/10740 [12:58:33<36:45:19, 16.26s/it]


 24%|██▍       | 2603/10740 [12:59:13<41:23:16, 18.31s/it]
[2024-04-02 08:12:56,708] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4514, 'learning_rate': 1.7727652638552884e-06, 'rewards/chosen': -1.2798466682434082, 'rewards/rejected': -2.661200523376465, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3813539743423462, 'policy_logps/rejected': -322.9678649902344, 'policy_logps/chosen': -471.24835205078125, 'referece_logps/rejected': -296.3558349609375, 'referece_logps/chosen': -458.44989013671875, 'logits/rejected': -0.4438320994377136, 'logits/chosen': -0.3733856678009033, 'epoch': 1.45}
[2024-04-02 08:13:19,965] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2604/10740 [12:59:36<44:44:12, 19.80s/it]
[2024-04-02 08:13:42,163] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2605/10740 [12:59:58<46:21:37, 20.52s/it]

 24%|██▍       | 2606/10740 [13:00:17<44:49:16, 19.84s/it]

 24%|██▍       | 2607/10740 [13:00:39<46:45:57, 20.70s/it]


 24%|██▍       | 2609/10740 [13:01:07<39:17:26, 17.40s/it]
{'loss': 0.4736, 'learning_rate': 1.7716155246454045e-06, 'rewards/chosen': -2.208223819732666, 'rewards/rejected': -2.2710442543029785, 'rewards/accuracies': 0.75, 'rewards/margins': 0.06282046437263489, 'policy_logps/rejected': -455.03631591796875, 'policy_logps/chosen': -532.7115478515625, 'referece_logps/rejected': -432.32586669921875, 'referece_logps/chosen': -510.6292724609375, 'logits/rejected': 0.2986331284046173, 'logits/chosen': 0.15610072016716003, 'epoch': 1.46}

 24%|██▍       | 2610/10740 [13:01:23<37:56:08, 16.80s/it]

 24%|██▍       | 2611/10740 [13:01:38<37:08:39, 16.45s/it]

 24%|██▍       | 2612/10740 [13:01:53<35:50:12, 15.87s/it]

 24%|██▍       | 2613/10740 [13:02:06<34:15:47, 15.18s/it]
[2024-04-02 08:16:10,392] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 24%|██▍       | 2615/10740 [13:02:41<36:24:54, 16.13s/it]

 24%|██▍       | 2616/10740 [13:02:59<37:30:58, 16.62s/it]
{'loss': 0.4982, 'learning_rate': 1.7702709692947588e-06, 'rewards/chosen': -1.4807878732681274, 'rewards/rejected': -2.4093644618988037, 'rewards/accuracies': 0.875, 'rewards/margins': 0.928576648235321, 'policy_logps/rejected': -367.3260192871094, 'policy_logps/chosen': -459.1707763671875, 'referece_logps/rejected': -343.2323913574219, 'referece_logps/chosen': -444.3628845214844, 'logits/rejected': -0.8636946678161621, 'logits/chosen': -0.9170164465904236, 'epoch': 1.46}
[2024-04-02 08:17:01,585] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2617/10740 [13:03:18<38:51:45, 17.22s/it]
[2024-04-02 08:17:22,735] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2618/10740 [13:03:39<41:30:55, 18.40s/it]

 24%|██▍       | 2619/10740 [13:03:50<36:44:50, 16.29s/it]

 24%|██▍       | 2620/10740 [13:04:02<33:54:47, 15.04s/it]

 24%|██▍       | 2621/10740 [13:04:22<36:51:51, 16.35s/it]

 24%|██▍       | 2622/10740 [13:04:33<33:19:37, 14.78s/it]

 24%|██▍       | 2623/10740 [13:04:45<31:26:48, 13.95s/it]


 24%|██▍       | 2625/10740 [13:05:14<31:31:03, 13.98s/it]

 24%|██▍       | 2626/10740 [13:05:33<35:20:50, 15.68s/it]

 24%|██▍       | 2627/10740 [13:05:51<36:58:18, 16.41s/it]
{'loss': 0.3988, 'learning_rate': 1.7681511631693838e-06, 'rewards/chosen': -1.0856982469558716, 'rewards/rejected': -3.747143268585205, 'rewards/accuracies': 0.75, 'rewards/margins': 2.661445379257202, 'policy_logps/rejected': -341.4535827636719, 'policy_logps/chosen': -301.9715576171875, 'referece_logps/rejected': -303.98211669921875, 'referece_logps/chosen': -291.11456298828125, 'logits/rejected': -0.9934045672416687, 'logits/chosen': -1.0395631790161133, 'epoch': 1.47}


 24%|██▍       | 2629/10740 [13:06:36<43:30:37, 19.31s/it]
[2024-04-02 08:20:19,474] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4953, 'learning_rate': 1.7677648349416124e-06, 'rewards/chosen': -0.5011307597160339, 'rewards/rejected': -1.4011330604553223, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9000023007392883, 'policy_logps/rejected': -418.605224609375, 'policy_logps/chosen': -452.4967956542969, 'referece_logps/rejected': -404.59381103515625, 'referece_logps/chosen': -447.4855041503906, 'logits/rejected': -0.9012272953987122, 'logits/chosen': -0.971672534942627, 'epoch': 1.47}
[2024-04-02 08:20:37,671] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2630/10740 [13:06:54<42:45:05, 18.98s/it]

 24%|██▍       | 2631/10740 [13:07:12<42:24:11, 18.82s/it]

 25%|██▍       | 2632/10740 [13:07:32<42:53:53, 19.05s/it]

 25%|██▍       | 2633/10740 [13:07:48<40:54:04, 18.16s/it]

 25%|██▍       | 2634/10740 [13:07:59<35:50:02, 15.91s/it]

 25%|██▍       | 2635/10740 [13:08:18<38:22:02, 17.04s/it]

 25%|██▍       | 2636/10740 [13:08:34<37:23:46, 16.61s/it]
[2024-04-02 08:22:39,492] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2637/10740 [13:08:56<40:51:05, 18.15s/it]

 25%|██▍       | 2638/10740 [13:09:15<41:31:12, 18.45s/it]

 25%|██▍       | 2639/10740 [13:09:35<42:23:50, 18.84s/it]

 25%|██▍       | 2640/10740 [13:09:55<43:28:26, 19.32s/it]

 25%|██▍       | 2641/10740 [13:10:15<43:45:31, 19.45s/it]

 25%|██▍       | 2642/10740 [13:10:36<44:38:22, 19.84s/it]

 25%|██▍       | 2643/10740 [13:10:51<41:47:05, 18.58s/it]

 25%|██▍       | 2644/10740 [13:11:12<43:24:15, 19.30s/it]

 25%|██▍       | 2645/10740 [13:11:32<43:33:20, 19.37s/it]

 25%|██▍       | 2646/10740 [13:11:46<39:51:29, 17.73s/it]

 25%|██▍       | 2647/10740 [13:11:58<36:00:00, 16.01s/it]

 25%|██▍       | 2648/10740 [13:12:14<36:01:22, 16.03s/it]

 25%|██▍       | 2649/10740 [13:12:28<34:59:31, 15.57s/it]

 25%|██▍       | 2650/10740 [13:12:39<31:41:54, 14.11s/it]

 25%|██▍       | 2651/10740 [13:12:54<32:13:55, 14.34s/it]

 25%|██▍       | 2652/10740 [13:13:11<33:53:25, 15.08s/it]

 25%|██▍       | 2653/10740 [13:13:25<33:10:50, 14.77s/it]

 25%|██▍       | 2654/10740 [13:13:44<36:11:22, 16.11s/it]

 25%|██▍       | 2655/10740 [13:13:58<34:56:50, 15.56s/it]

 25%|██▍       | 2656/10740 [13:14:15<35:58:10, 16.02s/it]

 25%|██▍       | 2657/10740 [13:14:28<33:59:54, 15.14s/it]

 25%|██▍       | 2658/10740 [13:14:43<33:25:07, 14.89s/it]

 25%|██▍       | 2659/10740 [13:15:01<35:26:54, 15.79s/it]

 25%|██▍       | 2660/10740 [13:15:17<35:47:04, 15.94s/it]
[2024-04-02 08:29:22,124] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2661/10740 [13:15:38<39:32:17, 17.62s/it]
[2024-04-02 08:29:39,380] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2662/10740 [13:15:56<39:17:21, 17.51s/it]

 25%|██▍       | 2663/10740 [13:16:15<40:40:59, 18.13s/it]

 25%|██▍       | 2664/10740 [13:16:33<40:06:58, 17.88s/it]

 25%|██▍       | 2665/10740 [13:16:48<38:18:25, 17.08s/it]

 25%|██▍       | 2666/10740 [13:17:08<40:18:45, 17.97s/it]

 25%|██▍       | 2667/10740 [13:17:24<39:11:12, 17.47s/it]

 25%|██▍       | 2668/10740 [13:17:43<40:23:39, 18.02s/it]

 25%|██▍       | 2669/10740 [13:18:06<43:13:02, 19.28s/it]

 25%|██▍       | 2670/10740 [13:18:21<40:53:21, 18.24s/it]
[2024-04-02 08:32:26,118] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2671/10740 [13:18:42<42:42:40, 19.06s/it]

 25%|██▍       | 2672/10740 [13:19:01<42:07:34, 18.80s/it]

 25%|██▍       | 2673/10740 [13:19:19<41:51:35, 18.68s/it]

 25%|██▍       | 2674/10740 [13:19:33<38:36:03, 17.23s/it]

 25%|██▍       | 2675/10740 [13:19:51<39:20:49, 17.56s/it]

 25%|██▍       | 2676/10740 [13:20:11<40:44:41, 18.19s/it]

 25%|██▍       | 2677/10740 [13:20:24<37:13:20, 16.62s/it]

 25%|██▍       | 2678/10740 [13:20:43<38:53:31, 17.37s/it]

 25%|██▍       | 2679/10740 [13:21:00<39:02:38, 17.44s/it]

 25%|██▍       | 2680/10740 [13:21:13<35:41:27, 15.94s/it]

 25%|██▍       | 2681/10740 [13:21:28<34:48:40, 15.55s/it]

 25%|██▍       | 2682/10740 [13:21:46<37:04:08, 16.56s/it]

 25%|██▍       | 2683/10740 [13:22:08<40:37:47, 18.15s/it]

 25%|██▍       | 2684/10740 [13:22:25<39:33:25, 17.68s/it]

 25%|██▌       | 2685/10740 [13:22:41<38:18:43, 17.12s/it]

 25%|██▌       | 2686/10740 [13:22:59<39:19:43, 17.58s/it]
[2024-04-02 08:37:07,617] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▌       | 2687/10740 [13:23:24<43:57:06, 19.65s/it]
[2024-04-02 08:37:28,434] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 25%|██▌       | 2689/10740 [13:24:05<44:51:38, 20.06s/it]

 25%|██▌       | 2690/10740 [13:24:26<45:30:42, 20.35s/it]

 25%|██▌       | 2691/10740 [13:24:48<46:21:02, 20.73s/it]

 25%|██▌       | 2692/10740 [13:25:11<48:29:16, 21.69s/it]

 25%|██▌       | 2693/10740 [13:25:29<45:38:32, 20.42s/it]

 25%|██▌       | 2694/10740 [13:25:49<45:13:18, 20.23s/it]

 25%|██▌       | 2695/10740 [13:26:06<43:31:27, 19.48s/it]

 25%|██▌       | 2696/10740 [13:26:27<43:54:58, 19.65s/it]
[2024-04-02 08:40:10,246] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▌       | 2697/10740 [13:26:45<43:18:13, 19.38s/it]

 25%|██▌       | 2698/10740 [13:27:03<42:18:13, 18.94s/it]

 25%|██▌       | 2699/10740 [13:27:20<40:46:13, 18.25s/it]

 25%|██▌       | 2700/10740 [13:27:33<37:14:32, 16.68s/it]

 25%|██▌       | 2701/10740 [13:27:53<39:46:29, 17.81s/it]

 25%|██▌       | 2702/10740 [13:28:05<35:33:58, 15.93s/it]

 25%|██▌       | 2703/10740 [13:28:27<39:32:05, 17.71s/it]
[2024-04-02 08:42:10,405] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▌       | 2704/10740 [13:28:46<40:30:18, 18.15s/it]

 25%|██▌       | 2705/10740 [13:29:04<40:26:17, 18.12s/it]

 25%|██▌       | 2706/10740 [13:29:22<40:27:53, 18.13s/it]

 25%|██▌       | 2707/10740 [13:29:43<42:09:28, 18.89s/it]

 25%|██▌       | 2708/10740 [13:29:55<37:50:21, 16.96s/it]

 25%|██▌       | 2709/10740 [13:30:09<35:31:05, 15.92s/it]

 25%|██▌       | 2710/10740 [13:30:29<38:28:48, 17.25s/it]

 25%|██▌       | 2711/10740 [13:30:48<39:30:01, 17.71s/it]

 25%|██▌       | 2712/10740 [13:31:08<41:29:02, 18.60s/it]

 25%|██▌       | 2713/10740 [13:31:25<40:01:08, 17.95s/it]

 25%|██▌       | 2714/10740 [13:31:44<40:58:18, 18.38s/it]

 25%|██▌       | 2715/10740 [13:32:02<40:11:31, 18.03s/it]

 25%|██▌       | 2716/10740 [13:32:17<38:09:30, 17.12s/it]

 25%|██▌       | 2717/10740 [13:32:35<39:06:21, 17.55s/it]

 25%|██▌       | 2718/10740 [13:32:55<40:45:08, 18.29s/it]

 25%|██▌       | 2719/10740 [13:33:11<39:27:16, 17.71s/it]

 25%|██▌       | 2720/10740 [13:33:33<42:18:41, 18.99s/it]
[2024-04-02 08:47:17,148] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▌       | 2721/10740 [13:33:46<38:00:46, 17.07s/it]
[2024-04-02 08:47:29,716] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▌       | 2722/10740 [13:34:01<36:37:26, 16.44s/it]

 25%|██▌       | 2723/10740 [13:34:17<36:40:04, 16.47s/it]

 25%|██▌       | 2724/10740 [13:34:33<36:03:19, 16.19s/it]

 25%|██▌       | 2725/10740 [13:34:54<39:20:29, 17.67s/it]

 25%|██▌       | 2726/10740 [13:35:06<35:12:19, 15.81s/it]

 25%|██▌       | 2727/10740 [13:35:25<37:31:00, 16.86s/it]

 25%|██▌       | 2728/10740 [13:35:45<39:20:00, 17.67s/it]

 25%|██▌       | 2729/10740 [13:36:01<38:48:34, 17.44s/it]

 25%|██▌       | 2730/10740 [13:36:18<38:15:59, 17.20s/it]
{'loss': 0.4661, 'learning_rate': 1.7478950823540576e-06, 'rewards/chosen': -0.9789441823959351, 'rewards/rejected': -2.676776885986328, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6978325843811035, 'policy_logps/rejected': -401.4721984863281, 'policy_logps/chosen': -364.39727783203125, 'referece_logps/rejected': -374.70440673828125, 'referece_logps/chosen': -354.60784912109375, 'logits/rejected': -0.11342164874076843, 'logits/chosen': -0.3052315413951874, 'epoch': 1.53}


 25%|██▌       | 2732/10740 [13:36:57<40:38:31, 18.27s/it]

 25%|██▌       | 2733/10740 [13:37:14<39:19:00, 17.68s/it]

 25%|██▌       | 2734/10740 [13:37:33<40:38:51, 18.28s/it]
{'loss': 0.5078, 'learning_rate': 1.7470937540857409e-06, 'rewards/chosen': -1.1440136432647705, 'rewards/rejected': -2.148139476776123, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0041258335113525, 'policy_logps/rejected': -396.62274169921875, 'policy_logps/chosen': -319.7758483886719, 'referece_logps/rejected': -375.1413269042969, 'referece_logps/chosen': -308.335693359375, 'logits/rejected': -0.6098916530609131, 'logits/chosen': -0.5446534752845764, 'epoch': 1.53}
[2024-04-02 08:51:35,497] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 25%|██▌       | 2736/10740 [13:38:12<41:50:55, 18.82s/it]

 25%|██▌       | 2737/10740 [13:38:24<37:10:40, 16.72s/it]

 25%|██▌       | 2738/10740 [13:38:37<35:00:53, 15.75s/it]

 26%|██▌       | 2739/10740 [13:38:54<35:57:02, 16.18s/it]

 26%|██▌       | 2740/10740 [13:39:14<38:17:00, 17.23s/it]

 26%|██▌       | 2741/10740 [13:39:30<37:11:57, 16.74s/it]

 26%|██▌       | 2742/10740 [13:39:47<37:37:54, 16.94s/it]

 26%|██▌       | 2743/10740 [13:40:07<39:26:56, 17.76s/it]
{'loss': 0.414, 'learning_rate': 1.745286792186025e-06, 'rewards/chosen': -0.9123884439468384, 'rewards/rejected': -3.64255428314209, 'rewards/accuracies': 0.875, 'rewards/margins': 2.730165719985962, 'policy_logps/rejected': -463.88592529296875, 'policy_logps/chosen': -355.51287841796875, 'referece_logps/rejected': -427.46038818359375, 'referece_logps/chosen': -346.38897705078125, 'logits/rejected': -1.2578752040863037, 'logits/chosen': -1.2982381582260132, 'epoch': 1.53}

 26%|██▌       | 2744/10740 [13:40:26<40:34:49, 18.27s/it]


 26%|██▌       | 2746/10740 [13:41:01<40:32:48, 18.26s/it]

 26%|██▌       | 2747/10740 [13:41:19<39:57:52, 18.00s/it]
{'loss': 0.4261, 'learning_rate': 1.7444819347894695e-06, 'rewards/chosen': -1.1267130374908447, 'rewards/rejected': -2.550058364868164, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4233454465866089, 'policy_logps/rejected': -417.5815734863281, 'policy_logps/chosen': -263.1309814453125, 'referece_logps/rejected': -392.08099365234375, 'referece_logps/chosen': -251.86387634277344, 'logits/rejected': -1.0034573078155518, 'logits/chosen': -0.9993671178817749, 'epoch': 1.53}


 26%|██▌       | 2749/10740 [13:41:57<40:44:05, 18.35s/it]

 26%|██▌       | 2750/10740 [13:42:18<42:16:45, 19.05s/it]

 26%|██▌       | 2751/10740 [13:42:35<41:19:14, 18.62s/it]

 26%|██▌       | 2752/10740 [13:42:54<41:17:15, 18.61s/it]

 26%|██▌       | 2753/10740 [13:43:16<43:21:12, 19.54s/it]
[2024-04-02 08:56:59,296] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2754/10740 [13:43:32<41:01:07, 18.49s/it]
[2024-04-02 08:57:15,337] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2755/10740 [13:43:45<37:26:58, 16.88s/it]

 26%|██▌       | 2756/10740 [13:44:05<39:38:36, 17.88s/it]

 26%|██▌       | 2757/10740 [13:44:26<41:45:34, 18.83s/it]
{'loss': 0.4408, 'learning_rate': 1.7424650539925877e-06, 'rewards/chosen': -0.33836597204208374, 'rewards/rejected': -1.4330713748931885, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0947054624557495, 'policy_logps/rejected': -450.54705810546875, 'policy_logps/chosen': -404.370361328125, 'referece_logps/rejected': -436.2163391113281, 'referece_logps/chosen': -400.9866943359375, 'logits/rejected': -0.03883802890777588, 'logits/chosen': -0.09215791523456573, 'epoch': 1.54}

 26%|██▌       | 2758/10740 [13:44:46<42:40:12, 19.24s/it]

 26%|██▌       | 2759/10740 [13:45:04<41:55:35, 18.91s/it]


 26%|██▌       | 2761/10740 [13:45:43<42:53:13, 19.35s/it]

 26%|██▌       | 2762/10740 [13:45:58<39:28:47, 17.81s/it]

 26%|██▌       | 2763/10740 [13:46:19<41:45:28, 18.85s/it]
[2024-04-02 09:00:02,672] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2764/10740 [13:46:39<42:32:20, 19.20s/it]
[2024-04-02 09:00:22,700] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4648, 'learning_rate': 1.7410492183673071e-06, 'rewards/chosen': -1.1510238647460938, 'rewards/rejected': -2.5892879962921143, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4382641315460205, 'policy_logps/rejected': -391.60760498046875, 'policy_logps/chosen': -507.7549743652344, 'referece_logps/rejected': -365.71478271484375, 'referece_logps/chosen': -496.2447814941406, 'logits/rejected': -0.7190777659416199, 'logits/chosen': -0.6632511615753174, 'epoch': 1.54}


 26%|██▌       | 2766/10740 [13:47:11<39:34:47, 17.87s/it]
{'loss': 0.5888, 'learning_rate': 1.7406440870246563e-06, 'rewards/chosen': -1.7224829196929932, 'rewards/rejected': -2.4181082248687744, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6956251859664917, 'policy_logps/rejected': -448.42633056640625, 'policy_logps/chosen': -504.46343994140625, 'referece_logps/rejected': -424.2452392578125, 'referece_logps/chosen': -487.23858642578125, 'logits/rejected': 0.22352388501167297, 'logits/chosen': 0.24348574876785278, 'epoch': 1.55}
[2024-04-02 09:01:14,116] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2767/10740 [13:47:30<40:24:08, 18.24s/it]

 26%|██▌       | 2768/10740 [13:47:49<40:33:30, 18.32s/it]


 26%|██▌       | 2770/10740 [13:48:24<39:17:18, 17.75s/it]
{'loss': 0.5276, 'learning_rate': 1.7398330161261088e-06, 'rewards/chosen': -1.964815616607666, 'rewards/rejected': -3.071347713470459, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1065322160720825, 'policy_logps/rejected': -406.0971374511719, 'policy_logps/chosen': -408.19873046875, 'referece_logps/rejected': -375.38360595703125, 'referece_logps/chosen': -388.5505676269531, 'logits/rejected': -0.9968479871749878, 'logits/chosen': -1.107939600944519, 'epoch': 1.55}


 26%|██▌       | 2772/10740 [13:49:02<40:36:00, 18.34s/it]

 26%|██▌       | 2773/10740 [13:49:24<42:37:13, 19.26s/it]
[2024-04-02 09:03:07,531] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2774/10740 [13:49:38<39:10:13, 17.70s/it]
[2024-04-02 09:03:21,600] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4474, 'learning_rate': 1.7390208685936484e-06, 'rewards/chosen': -1.3339354991912842, 'rewards/rejected': -2.1468887329101562, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8129534125328064, 'policy_logps/rejected': -431.5793762207031, 'policy_logps/chosen': -355.895751953125, 'referece_logps/rejected': -410.11041259765625, 'referece_logps/chosen': -342.556396484375, 'logits/rejected': 0.19902996718883514, 'logits/chosen': 0.1456265151500702, 'epoch': 1.55}


 26%|██▌       | 2776/10740 [13:50:12<38:04:58, 17.21s/it]
{'loss': 0.5381, 'learning_rate': 1.7386143914589739e-06, 'rewards/chosen': -1.3260319232940674, 'rewards/rejected': -1.846096396446228, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5200644731521606, 'policy_logps/rejected': -281.96990966796875, 'policy_logps/chosen': -337.3611145019531, 'referece_logps/rejected': -263.50897216796875, 'referece_logps/chosen': -324.1007995605469, 'logits/rejected': -0.4714500904083252, 'logits/chosen': -0.49010536074638367, 'epoch': 1.55}

 26%|██▌       | 2777/10740 [13:50:27<36:32:03, 16.52s/it]
[2024-04-02 09:04:30,425] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 26%|██▌       | 2779/10740 [13:51:01<36:39:54, 16.58s/it]

 26%|██▌       | 2780/10740 [13:51:12<32:44:59, 14.81s/it]
{'loss': 0.585, 'learning_rate': 1.7378006311921375e-06, 'rewards/chosen': -2.263309955596924, 'rewards/rejected': -2.642378330230713, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3790683448314667, 'policy_logps/rejected': -466.72039794921875, 'policy_logps/chosen': -384.82122802734375, 'referece_logps/rejected': -440.2966003417969, 'referece_logps/chosen': -362.1881408691406, 'logits/rejected': 0.5837787985801697, 'logits/chosen': 0.6439879536628723, 'epoch': 1.55}


 26%|██▌       | 2782/10740 [13:51:36<29:38:29, 13.41s/it]

 26%|██▌       | 2783/10740 [13:51:56<33:50:56, 15.31s/it]
{'loss': 0.4574, 'learning_rate': 1.7371896063271136e-06, 'rewards/chosen': -1.3261899948120117, 'rewards/rejected': -2.809676170349121, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4834861755371094, 'policy_logps/rejected': -390.94342041015625, 'policy_logps/chosen': -385.1589660644531, 'referece_logps/rejected': -362.84661865234375, 'referece_logps/chosen': -371.8970947265625, 'logits/rejected': -0.7205169796943665, 'logits/chosen': -0.7987657785415649, 'epoch': 1.55}


 26%|██▌       | 2785/10740 [13:52:32<37:12:08, 16.84s/it]

 26%|██▌       | 2786/10740 [13:52:49<37:12:27, 16.84s/it]

 26%|██▌       | 2787/10740 [13:53:06<37:47:51, 17.11s/it]

 26%|██▌       | 2788/10740 [13:53:30<42:14:57, 19.13s/it]

 26%|██▌       | 2789/10740 [13:53:46<40:10:39, 18.19s/it]

 26%|██▌       | 2790/10740 [13:54:04<39:58:14, 18.10s/it]

 26%|██▌       | 2791/10740 [13:54:23<40:17:13, 18.25s/it]
{'loss': 0.3755, 'learning_rate': 1.73555725788236e-06, 'rewards/chosen': -1.3905633687973022, 'rewards/rejected': -3.3791701793670654, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9886066913604736, 'policy_logps/rejected': -410.1082458496094, 'policy_logps/chosen': -331.7580261230469, 'referece_logps/rejected': -376.3165283203125, 'referece_logps/chosen': -317.8524169921875, 'logits/rejected': -0.29319122433662415, 'logits/chosen': -0.27628767490386963, 'epoch': 1.56}


 26%|██▌       | 2793/10740 [13:54:55<38:24:57, 17.40s/it]

 26%|██▌       | 2794/10740 [13:55:14<39:52:45, 18.07s/it]
{'loss': 0.4467, 'learning_rate': 1.7349440228428045e-06, 'rewards/chosen': -1.339016318321228, 'rewards/rejected': -2.3675906658172607, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0285742282867432, 'policy_logps/rejected': -427.3140563964844, 'policy_logps/chosen': -363.070556640625, 'referece_logps/rejected': -403.6381530761719, 'referece_logps/chosen': -349.680419921875, 'logits/rejected': -0.7016118764877319, 'logits/chosen': -0.6993094682693481, 'epoch': 1.56}
[2024-04-02 09:09:20,863] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 26%|██▌       | 2796/10740 [13:55:54<41:34:56, 18.84s/it]
{'loss': 0.6189, 'learning_rate': 1.7345348652273083e-06, 'rewards/chosen': -1.0670099258422852, 'rewards/rejected': -1.7941792011260986, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7271692752838135, 'policy_logps/rejected': -497.05865478515625, 'policy_logps/chosen': -541.4894409179688, 'referece_logps/rejected': -479.11688232421875, 'referece_logps/chosen': -530.8193359375, 'logits/rejected': -0.41294705867767334, 'logits/chosen': -0.5219373106956482, 'epoch': 1.56}

 26%|██▌       | 2797/10740 [13:56:13<41:32:07, 18.83s/it]


 26%|██▌       | 2799/10740 [13:56:46<39:21:35, 17.84s/it]
{'loss': 0.3752, 'learning_rate': 1.7339206277925143e-06, 'rewards/chosen': -0.9247041940689087, 'rewards/rejected': -2.283449172973633, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3587449789047241, 'policy_logps/rejected': -482.8346252441406, 'policy_logps/chosen': -360.1858215332031, 'referece_logps/rejected': -460.0000915527344, 'referece_logps/chosen': -350.9386901855469, 'logits/rejected': -0.7257918119430542, 'logits/chosen': -0.9669945240020752, 'epoch': 1.56}


 26%|██▌       | 2801/10740 [13:57:18<37:05:14, 16.82s/it]

 26%|██▌       | 2802/10740 [13:57:36<37:41:20, 17.09s/it]

 26%|██▌       | 2803/10740 [13:57:56<39:31:52, 17.93s/it]
{'loss': 0.4029, 'learning_rate': 1.7331007101068191e-06, 'rewards/chosen': -1.021557092666626, 'rewards/rejected': -2.458846092224121, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4372889995574951, 'policy_logps/rejected': -309.3055419921875, 'policy_logps/chosen': -298.3168029785156, 'referece_logps/rejected': -284.7170715332031, 'referece_logps/chosen': -288.1012268066406, 'logits/rejected': -0.36177706718444824, 'logits/chosen': -0.2721555829048157, 'epoch': 1.57}

 26%|██▌       | 2804/10740 [13:58:13<39:08:05, 17.75s/it]


 26%|██▌       | 2806/10740 [13:58:41<35:00:59, 15.89s/it]
{'loss': 0.4477, 'learning_rate': 1.732485071665589e-06, 'rewards/chosen': -2.2534995079040527, 'rewards/rejected': -4.201888084411621, 'rewards/accuracies': 1.0, 'rewards/margins': 1.948388695716858, 'policy_logps/rejected': -452.2898254394531, 'policy_logps/chosen': -360.1591796875, 'referece_logps/rejected': -410.2709655761719, 'referece_logps/chosen': -337.6241760253906, 'logits/rejected': -0.9448500871658325, 'logits/chosen': -0.9824365973472595, 'epoch': 1.57}


 26%|██▌       | 2808/10740 [13:59:12<34:22:30, 15.60s/it]

 26%|██▌       | 2809/10740 [13:59:32<37:02:26, 16.81s/it]
{'loss': 0.5825, 'learning_rate': 1.7318688336325723e-06, 'rewards/chosen': -2.0466887950897217, 'rewards/rejected': -3.299226999282837, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2525382041931152, 'policy_logps/rejected': -505.705810546875, 'policy_logps/chosen': -469.00714111328125, 'referece_logps/rejected': -472.71356201171875, 'referece_logps/chosen': -448.5402526855469, 'logits/rejected': 0.09574931114912033, 'logits/chosen': 0.10908366739749908, 'epoch': 1.57}
[2024-04-02 09:13:37,426] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 26%|██▌       | 2811/10740 [14:00:15<42:08:45, 19.14s/it]
[2024-04-02 09:13:58,595] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3933, 'learning_rate': 1.7314576754197639e-06, 'rewards/chosen': -1.924054741859436, 'rewards/rejected': -3.155336380004883, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2312817573547363, 'policy_logps/rejected': -467.6748962402344, 'policy_logps/chosen': -421.293701171875, 'referece_logps/rejected': -436.1215515136719, 'referece_logps/chosen': -402.05316162109375, 'logits/rejected': -0.45844805240631104, 'logits/chosen': -0.31936565041542053, 'epoch': 1.57}


 26%|██▌       | 2813/10740 [14:00:50<40:27:01, 18.37s/it]

 26%|██▌       | 2814/10740 [14:01:07<39:03:21, 17.74s/it]

 26%|██▌       | 2815/10740 [14:01:19<35:18:45, 16.04s/it]
{'loss': 0.5858, 'learning_rate': 1.7306345608094116e-06, 'rewards/chosen': -2.034350872039795, 'rewards/rejected': -3.059648275375366, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0252974033355713, 'policy_logps/rejected': -297.8539123535156, 'policy_logps/chosen': -315.86102294921875, 'referece_logps/rejected': -267.2574157714844, 'referece_logps/chosen': -295.51751708984375, 'logits/rejected': -0.8346114754676819, 'logits/chosen': -0.7912160158157349, 'epoch': 1.57}


 26%|██▌       | 2817/10740 [14:01:50<34:30:08, 15.68s/it]

 26%|██▌       | 2818/10740 [14:02:07<34:49:17, 15.82s/it]

 26%|██▌       | 2819/10740 [14:02:19<32:33:19, 14.80s/it]

 26%|██▋       | 2820/10740 [14:02:39<35:48:07, 16.27s/it]

 26%|██▋       | 2821/10740 [14:02:59<38:08:22, 17.34s/it]
{'loss': 0.4586, 'learning_rate': 1.7293978956787008e-06, 'rewards/chosen': -1.1868568658828735, 'rewards/rejected': -3.00425386428833, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8173969984054565, 'policy_logps/rejected': -350.5449523925781, 'policy_logps/chosen': -298.2842712402344, 'referece_logps/rejected': -320.50238037109375, 'referece_logps/chosen': -286.4156494140625, 'logits/rejected': 0.07186123728752136, 'logits/chosen': -0.06218068674206734, 'epoch': 1.58}


 26%|██▋       | 2823/10740 [14:03:33<37:51:53, 17.22s/it]
[2024-04-02 09:17:16,367] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2824/10740 [14:03:47<35:39:55, 16.22s/it]

 26%|██▋       | 2825/10740 [14:04:00<33:52:13, 15.41s/it]

 26%|██▋       | 2826/10740 [14:04:21<37:29:29, 17.05s/it]
[2024-04-02 09:18:04,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2827/10740 [14:04:39<38:24:28, 17.47s/it]
{'loss': 0.4284, 'learning_rate': 1.7281588422896372e-06, 'rewards/chosen': -1.205112099647522, 'rewards/rejected': -3.8128433227539062, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6077311038970947, 'policy_logps/rejected': -498.8404541015625, 'policy_logps/chosen': -414.61639404296875, 'referece_logps/rejected': -460.7120361328125, 'referece_logps/chosen': -402.56524658203125, 'logits/rejected': -0.13158416748046875, 'logits/chosen': -0.11809197068214417, 'epoch': 1.58}

 26%|██▋       | 2828/10740 [14:05:02<41:35:12, 18.92s/it]


 26%|██▋       | 2830/10740 [14:05:36<39:40:28, 18.06s/it]

 26%|██▋       | 2831/10740 [14:05:55<39:45:24, 18.10s/it]

 26%|██▋       | 2832/10740 [14:06:14<40:51:36, 18.60s/it]

 26%|██▋       | 2833/10740 [14:06:27<36:58:16, 16.83s/it]

 26%|██▋       | 2834/10740 [14:06:47<39:08:19, 17.82s/it]

 26%|██▋       | 2835/10740 [14:07:07<40:31:54, 18.46s/it]

 26%|██▋       | 2836/10740 [14:07:28<42:18:59, 19.27s/it]

 26%|██▋       | 2837/10740 [14:07:49<43:10:05, 19.66s/it]

 26%|██▋       | 2838/10740 [14:08:03<39:12:30, 17.86s/it]

 26%|██▋       | 2839/10740 [14:08:15<35:43:51, 16.28s/it]

 26%|██▋       | 2840/10740 [14:08:35<38:02:42, 17.34s/it]

 26%|██▋       | 2841/10740 [14:08:53<38:14:26, 17.43s/it]

 26%|██▋       | 2842/10740 [14:09:09<37:11:08, 16.95s/it]

 26%|██▋       | 2843/10740 [14:09:28<39:06:34, 17.83s/it]

 26%|██▋       | 2844/10740 [14:09:49<41:01:36, 18.71s/it]
[2024-04-02 09:23:32,878] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4984, 'learning_rate': 1.72463525697802e-06, 'rewards/chosen': -1.4633004665374756, 'rewards/rejected': -2.339895725250244, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8765952587127686, 'policy_logps/rejected': -488.7459411621094, 'policy_logps/chosen': -499.72576904296875, 'referece_logps/rejected': -465.34698486328125, 'referece_logps/chosen': -485.0927734375, 'logits/rejected': 0.4552626311779022, 'logits/chosen': 0.4708634614944458, 'epoch': 1.59}

 26%|██▋       | 2845/10740 [14:10:11<42:47:53, 19.52s/it]
[2024-04-02 09:24:11,742] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 27%|██▋       | 2847/10740 [14:10:45<40:20:03, 18.40s/it]

 27%|██▋       | 2848/10740 [14:11:04<40:20:37, 18.40s/it]
[2024-04-02 09:24:47,386] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2849/10740 [14:11:22<40:10:52, 18.33s/it]
{'loss': 0.3401, 'learning_rate': 1.7235952793001714e-06, 'rewards/chosen': -1.2627687454223633, 'rewards/rejected': -2.3055050373077393, 'rewards/accuracies': 0.875, 'rewards/margins': 1.042736291885376, 'policy_logps/rejected': -321.8535461425781, 'policy_logps/chosen': -292.04437255859375, 'referece_logps/rejected': -298.7984924316406, 'referece_logps/chosen': -279.4166259765625, 'logits/rejected': -0.39630866050720215, 'logits/chosen': -0.3757142126560211, 'epoch': 1.59}

 27%|██▋       | 2850/10740 [14:11:42<41:36:38, 18.99s/it]

 27%|██▋       | 2851/10740 [14:12:00<41:03:25, 18.74s/it]

 27%|██▋       | 2852/10740 [14:12:18<40:24:07, 18.44s/it]

 27%|██▋       | 2853/10740 [14:12:34<38:57:04, 17.78s/it]

 27%|██▋       | 2854/10740 [14:12:52<39:05:40, 17.85s/it]

 27%|██▋       | 2855/10740 [14:13:15<42:00:03, 19.18s/it]
[2024-04-02 09:27:21,808] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2856/10740 [14:13:38<44:43:14, 20.42s/it]


 27%|██▋       | 2858/10740 [14:14:15<42:38:00, 19.47s/it]

 27%|██▋       | 2859/10740 [14:14:36<43:24:10, 19.83s/it]
{'loss': 0.3714, 'learning_rate': 1.721510390356435e-06, 'rewards/chosen': -1.0879679918289185, 'rewards/rejected': -2.2208642959594727, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1328961849212646, 'policy_logps/rejected': -177.69711303710938, 'policy_logps/chosen': -206.07305908203125, 'referece_logps/rejected': -155.48846435546875, 'referece_logps/chosen': -195.19337463378906, 'logits/rejected': -1.51488196849823, 'logits/chosen': -1.5774307250976562, 'epoch': 1.6}

 27%|██▋       | 2860/10740 [14:14:53<41:26:23, 18.93s/it]


 27%|██▋       | 2862/10740 [14:15:22<36:23:46, 16.63s/it]
{'loss': 0.5142, 'learning_rate': 1.7208836431586952e-06, 'rewards/chosen': -1.5491015911102295, 'rewards/rejected': -2.67403507232666, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1249334812164307, 'policy_logps/rejected': -399.22613525390625, 'policy_logps/chosen': -358.579345703125, 'referece_logps/rejected': -372.4858093261719, 'referece_logps/chosen': -343.08831787109375, 'logits/rejected': 0.13241492211818695, 'logits/chosen': 0.13893388211727142, 'epoch': 1.6}


 27%|██▋       | 2864/10740 [14:16:00<38:56:13, 17.80s/it]
{'loss': 0.5099, 'learning_rate': 1.72046548383119e-06, 'rewards/chosen': -2.0541937351226807, 'rewards/rejected': -3.2333688735961914, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1791753768920898, 'policy_logps/rejected': -533.6240234375, 'policy_logps/chosen': -408.18280029296875, 'referece_logps/rejected': -501.29034423828125, 'referece_logps/chosen': -387.640869140625, 'logits/rejected': 0.32433614134788513, 'logits/chosen': 0.3589975833892822, 'epoch': 1.6}

 27%|██▋       | 2865/10740 [14:16:16<38:15:03, 17.49s/it]


 27%|██▋       | 2867/10740 [14:16:54<39:44:24, 18.17s/it]
{'loss': 0.4653, 'learning_rate': 1.7198377534267408e-06, 'rewards/chosen': -1.0190120935440063, 'rewards/rejected': -2.502730369567871, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4837182760238647, 'policy_logps/rejected': -510.38873291015625, 'policy_logps/chosen': -432.5141906738281, 'referece_logps/rejected': -485.3614807128906, 'referece_logps/chosen': -422.3240966796875, 'logits/rejected': -0.43882083892822266, 'logits/chosen': -0.4001200497150421, 'epoch': 1.6}

 27%|██▋       | 2868/10740 [14:17:17<43:17:07, 19.80s/it]

 27%|██▋       | 2869/10740 [14:17:37<43:10:58, 19.75s/it]

 27%|██▋       | 2870/10740 [14:17:57<43:17:43, 19.80s/it]


 27%|██▋       | 2872/10740 [14:18:36<43:02:46, 19.70s/it]

 27%|██▋       | 2873/10740 [14:18:50<39:05:45, 17.89s/it]
{'loss': 0.5037, 'learning_rate': 1.7185805254150306e-06, 'rewards/chosen': -2.424142599105835, 'rewards/rejected': -3.0411791801452637, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6170364618301392, 'policy_logps/rejected': -425.1671142578125, 'policy_logps/chosen': -351.3326416015625, 'referece_logps/rejected': -394.7553405761719, 'referece_logps/chosen': -327.09124755859375, 'logits/rejected': -0.21423503756523132, 'logits/chosen': -0.1042163223028183, 'epoch': 1.61}

 27%|██▋       | 2874/10740 [14:19:07<38:26:31, 17.59s/it]

 27%|██▋       | 2875/10740 [14:19:22<37:16:07, 17.06s/it]

 27%|██▋       | 2876/10740 [14:19:41<38:06:18, 17.44s/it]

 27%|██▋       | 2877/10740 [14:19:59<38:20:24, 17.55s/it]


 27%|██▋       | 2879/10740 [14:20:32<37:13:24, 17.05s/it]
{'loss': 0.5377, 'learning_rate': 1.7173209445641468e-06, 'rewards/chosen': -1.7830740213394165, 'rewards/rejected': -2.995030641555786, 'rewards/accuracies': 0.75, 'rewards/margins': 1.21195650100708, 'policy_logps/rejected': -287.0120849609375, 'policy_logps/chosen': -439.30780029296875, 'referece_logps/rejected': -257.061767578125, 'referece_logps/chosen': -421.47705078125, 'logits/rejected': -0.8946545124053955, 'logits/chosen': -0.8440693020820618, 'epoch': 1.61}

 27%|██▋       | 2880/10740 [14:20:51<38:07:22, 17.46s/it]

 27%|██▋       | 2881/10740 [14:21:09<38:24:00, 17.59s/it]


 27%|██▋       | 2883/10740 [14:21:40<35:39:31, 16.34s/it]
{'loss': 0.4848, 'learning_rate': 1.7164799189003566e-06, 'rewards/chosen': -1.489101767539978, 'rewards/rejected': -2.671837329864502, 'rewards/accuracies': 0.875, 'rewards/margins': 1.182735562324524, 'policy_logps/rejected': -331.3811340332031, 'policy_logps/chosen': -353.1108703613281, 'referece_logps/rejected': -304.6627502441406, 'referece_logps/chosen': -338.2198181152344, 'logits/rejected': -0.27751919627189636, 'logits/chosen': -0.23077738285064697, 'epoch': 1.61}

 27%|██▋       | 2884/10740 [14:21:59<37:40:39, 17.27s/it]


 27%|██▋       | 2886/10740 [14:22:40<41:12:08, 18.89s/it]
{'loss': 0.5346, 'learning_rate': 1.7158484653467432e-06, 'rewards/chosen': -1.0506255626678467, 'rewards/rejected': -2.0698585510253906, 'rewards/accuracies': 0.75, 'rewards/margins': 1.019233226776123, 'policy_logps/rejected': -399.9564514160156, 'policy_logps/chosen': -428.67852783203125, 'referece_logps/rejected': -379.25787353515625, 'referece_logps/chosen': -418.17230224609375, 'logits/rejected': -0.8899623155593872, 'logits/chosen': -0.7961223721504211, 'epoch': 1.61}
[2024-04-02 09:36:47,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 27%|██▋       | 2888/10740 [14:23:24<44:34:44, 20.44s/it]
[2024-04-02 09:37:07,997] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2889/10740 [14:23:38<40:09:16, 18.41s/it]

 27%|██▋       | 2890/10740 [14:23:54<38:51:02, 17.82s/it]
{'loss': 0.5057, 'learning_rate': 1.715005615850293e-06, 'rewards/chosen': -1.359912633895874, 'rewards/rejected': -2.426705837249756, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0667930841445923, 'policy_logps/rejected': -446.3788757324219, 'policy_logps/chosen': -559.7307739257812, 'referece_logps/rejected': -422.1117858886719, 'referece_logps/chosen': -546.1317138671875, 'logits/rejected': 0.43153291940689087, 'logits/chosen': 0.3806184232234955, 'epoch': 1.61}


 27%|██▋       | 2892/10740 [14:24:26<36:33:35, 16.77s/it]
{'loss': 0.4054, 'learning_rate': 1.714583800836308e-06, 'rewards/chosen': -1.2985105514526367, 'rewards/rejected': -2.7460947036743164, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4475841522216797, 'policy_logps/rejected': -454.8294677734375, 'policy_logps/chosen': -363.4866027832031, 'referece_logps/rejected': -427.3685302734375, 'referece_logps/chosen': -350.50146484375, 'logits/rejected': -0.44595515727996826, 'logits/chosen': -0.6580568552017212, 'epoch': 1.62}


 27%|██▋       | 2894/10740 [14:25:05<39:39:09, 18.19s/it]
[2024-04-02 09:38:48,360] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4528, 'learning_rate': 1.7141617258497329e-06, 'rewards/chosen': -1.814969539642334, 'rewards/rejected': -3.158163547515869, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3431938886642456, 'policy_logps/rejected': -385.3293151855469, 'policy_logps/chosen': -342.4013366699219, 'referece_logps/rejected': -353.7476501464844, 'referece_logps/chosen': -324.2516784667969, 'logits/rejected': -1.3341792821884155, 'logits/chosen': -1.1552953720092773, 'epoch': 1.62}


 27%|██▋       | 2896/10740 [14:25:42<40:40:52, 18.67s/it]
{'loss': 0.5463, 'learning_rate': 1.7137393910441218e-06, 'rewards/chosen': -1.6026045083999634, 'rewards/rejected': -2.913935661315918, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3113312721252441, 'policy_logps/rejected': -309.0695495605469, 'policy_logps/chosen': -340.8864440917969, 'referece_logps/rejected': -279.9302062988281, 'referece_logps/chosen': -324.86041259765625, 'logits/rejected': -1.4621071815490723, 'logits/chosen': -1.4301421642303467, 'epoch': 1.62}


 27%|██▋       | 2898/10740 [14:26:11<36:15:18, 16.64s/it]
{'loss': 0.6469, 'learning_rate': 1.7133167965731246e-06, 'rewards/chosen': -2.0778563022613525, 'rewards/rejected': -2.5362069606781006, 'rewards/accuracies': 0.5, 'rewards/margins': 0.45835036039352417, 'policy_logps/rejected': -475.768310546875, 'policy_logps/chosen': -484.32672119140625, 'referece_logps/rejected': -450.40625, 'referece_logps/chosen': -463.54815673828125, 'logits/rejected': -1.7948353290557861, 'logits/chosen': -1.7478363513946533, 'epoch': 1.62}
[2024-04-02 09:40:15,688] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 27%|██▋       | 2900/10740 [14:26:53<40:58:34, 18.82s/it]
[2024-04-02 09:40:36,317] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4746, 'learning_rate': 1.7128939425904853e-06, 'rewards/chosen': -0.7747522592544556, 'rewards/rejected': -2.124967098236084, 'rewards/accuracies': 0.875, 'rewards/margins': 1.350214958190918, 'policy_logps/rejected': -375.2815246582031, 'policy_logps/chosen': -339.4349365234375, 'referece_logps/rejected': -354.0318603515625, 'referece_logps/chosen': -331.6873779296875, 'logits/rejected': 0.8123680353164673, 'logits/chosen': 0.8450496792793274, 'epoch': 1.62}

 27%|██▋       | 2901/10740 [14:27:06<37:28:39, 17.21s/it]


 27%|██▋       | 2903/10740 [14:27:38<36:24:26, 16.72s/it]
{'loss': 0.5793, 'learning_rate': 1.7122591753687447e-06, 'rewards/chosen': -1.506935954093933, 'rewards/rejected': -2.8780295848846436, 'rewards/accuracies': 0.75, 'rewards/margins': 1.371093988418579, 'policy_logps/rejected': -426.15081787109375, 'policy_logps/chosen': -414.4664306640625, 'referece_logps/rejected': -397.37054443359375, 'referece_logps/chosen': -399.3970947265625, 'logits/rejected': -0.8061934113502502, 'logits/chosen': -0.824822187423706, 'epoch': 1.62}

 27%|██▋       | 2904/10740 [14:27:51<33:46:12, 15.51s/it]

 27%|██▋       | 2905/10740 [14:28:07<34:06:24, 15.67s/it]


 27%|██▋       | 2907/10740 [14:28:43<36:48:47, 16.92s/it]

 27%|██▋       | 2908/10740 [14:29:02<38:38:27, 17.76s/it]

 27%|██▋       | 2909/10740 [14:29:15<35:08:20, 16.15s/it]
{'loss': 0.6119, 'learning_rate': 1.7109878923390433e-06, 'rewards/chosen': -1.9643152952194214, 'rewards/rejected': -2.6290769577026367, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6647619009017944, 'policy_logps/rejected': -244.922607421875, 'policy_logps/chosen': -299.4213562011719, 'referece_logps/rejected': -218.6318359375, 'referece_logps/chosen': -279.7781982421875, 'logits/rejected': -1.3332747220993042, 'logits/chosen': -1.4090361595153809, 'epoch': 1.63}


 27%|██▋       | 2911/10740 [14:29:46<35:15:49, 16.22s/it]
{'loss': 0.4934, 'learning_rate': 1.710563613794763e-06, 'rewards/chosen': -1.7779020071029663, 'rewards/rejected': -2.5867726802825928, 'rewards/accuracies': 0.75, 'rewards/margins': 0.808870792388916, 'policy_logps/rejected': -427.9775695800781, 'policy_logps/chosen': -514.5220336914062, 'referece_logps/rejected': -402.10980224609375, 'referece_logps/chosen': -496.7430419921875, 'logits/rejected': -0.4793020188808441, 'logits/chosen': -0.5512619614601135, 'epoch': 1.63}

 27%|██▋       | 2912/10740 [14:30:01<34:23:33, 15.82s/it]

 27%|██▋       | 2913/10740 [14:30:21<37:07:02, 17.07s/it]

 27%|██▋       | 2914/10740 [14:30:40<37:55:30, 17.45s/it]
[2024-04-02 09:44:43,068] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2915/10740 [14:30:59<39:27:37, 18.15s/it]

 27%|██▋       | 2916/10740 [14:31:21<41:54:40, 19.28s/it]
[2024-04-02 09:45:25,919] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 27%|██▋       | 2918/10740 [14:32:03<43:29:19, 20.02s/it]

 27%|██▋       | 2919/10740 [14:32:15<38:26:40, 17.70s/it]
{'loss': 0.5364, 'learning_rate': 1.7088639160625466e-06, 'rewards/chosen': -0.965434193611145, 'rewards/rejected': -2.432023048400879, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4665889739990234, 'policy_logps/rejected': -477.42437744140625, 'policy_logps/chosen': -397.6506652832031, 'referece_logps/rejected': -453.1041564941406, 'referece_logps/chosen': -387.9963073730469, 'logits/rejected': -0.793143630027771, 'logits/chosen': -0.9017353653907776, 'epoch': 1.63}

 27%|██▋       | 2920/10740 [14:32:32<37:58:49, 17.48s/it]

 27%|██▋       | 2921/10740 [14:32:50<38:06:35, 17.55s/it]

 27%|██▋       | 2922/10740 [14:33:10<39:40:10, 18.27s/it]


 27%|██▋       | 2924/10740 [14:33:33<32:19:46, 14.89s/it]
{'loss': 0.4973, 'learning_rate': 1.7077995089822555e-06, 'rewards/chosen': -1.690324306488037, 'rewards/rejected': -2.753523349761963, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0631990432739258, 'policy_logps/rejected': -310.4552001953125, 'policy_logps/chosen': -299.6062316894531, 'referece_logps/rejected': -282.9199523925781, 'referece_logps/chosen': -282.7029724121094, 'logits/rejected': -1.9089219570159912, 'logits/chosen': -2.0670645236968994, 'epoch': 1.63}

 27%|██▋       | 2925/10740 [14:33:48<32:29:25, 14.97s/it]

 27%|██▋       | 2926/10740 [14:34:01<31:20:42, 14.44s/it]


 27%|██▋       | 2928/10740 [14:34:37<34:58:36, 16.12s/it]

 27%|██▋       | 2929/10740 [14:34:49<32:09:30, 14.82s/it]

 27%|██▋       | 2930/10740 [14:35:11<36:56:55, 17.03s/it]

 27%|██▋       | 2931/10740 [14:35:33<39:59:47, 18.44s/it]

 27%|██▋       | 2932/10740 [14:35:51<39:46:30, 18.34s/it]
{'loss': 0.4048, 'learning_rate': 1.7060931111053363e-06, 'rewards/chosen': -1.0271096229553223, 'rewards/rejected': -2.4969825744628906, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4698727130889893, 'policy_logps/rejected': -257.6151428222656, 'policy_logps/chosen': -236.14718627929688, 'referece_logps/rejected': -232.6453094482422, 'referece_logps/chosen': -225.87606811523438, 'logits/rejected': -1.4350199699401855, 'logits/chosen': -1.4477747678756714, 'epoch': 1.64}


 27%|██▋       | 2934/10740 [14:36:31<41:52:27, 19.31s/it]
{'loss': 0.4604, 'learning_rate': 1.7056658690388837e-06, 'rewards/chosen': -1.0523595809936523, 'rewards/rejected': -1.5147415399551392, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4623820185661316, 'policy_logps/rejected': -286.4637145996094, 'policy_logps/chosen': -278.6540222167969, 'referece_logps/rejected': -271.3163146972656, 'referece_logps/chosen': -268.13043212890625, 'logits/rejected': -0.5513381958007812, 'logits/chosen': -0.629799485206604, 'epoch': 1.64}

 27%|██▋       | 2935/10740 [14:36:52<42:57:31, 19.81s/it]


 27%|██▋       | 2937/10740 [14:37:27<41:00:20, 18.92s/it]
[2024-04-02 09:51:10,917] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2938/10740 [14:37:39<36:36:55, 16.90s/it]
[2024-04-02 09:51:23,091] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5106, 'learning_rate': 1.7048106148770276e-06, 'rewards/chosen': -1.699157476425171, 'rewards/rejected': -1.78244948387146, 'rewards/accuracies': 0.5, 'rewards/margins': 0.08329206705093384, 'policy_logps/rejected': -554.9898071289062, 'policy_logps/chosen': -394.57904052734375, 'referece_logps/rejected': -537.1654052734375, 'referece_logps/chosen': -377.5874938964844, 'logits/rejected': -0.933281660079956, 'logits/chosen': -0.7691373229026794, 'epoch': 1.64}

 27%|██▋       | 2939/10740 [14:38:01<39:24:11, 18.18s/it]

 27%|██▋       | 2940/10740 [14:38:22<41:43:13, 19.26s/it]

 27%|██▋       | 2941/10740 [14:38:40<40:34:05, 18.73s/it]

 27%|██▋       | 2942/10740 [14:39:00<41:15:49, 19.05s/it]


 27%|██▋       | 2944/10740 [14:39:43<44:26:32, 20.52s/it]
[2024-04-02 09:53:26,887] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4714, 'learning_rate': 1.70352581089619e-06, 'rewards/chosen': -2.046398401260376, 'rewards/rejected': -3.4806156158447266, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4342174530029297, 'policy_logps/rejected': -355.05621337890625, 'policy_logps/chosen': -419.1158752441406, 'referece_logps/rejected': -320.25006103515625, 'referece_logps/chosen': -398.65191650390625, 'logits/rejected': 0.2648138999938965, 'logits/chosen': 0.2006230652332306, 'epoch': 1.64}

 27%|██▋       | 2945/10740 [14:39:58<41:00:12, 18.94s/it]

 27%|██▋       | 2946/10740 [14:40:16<40:17:14, 18.61s/it]

 27%|██▋       | 2947/10740 [14:40:32<38:14:15, 17.66s/it]

 27%|██▋       | 2948/10740 [14:40:51<39:08:01, 18.08s/it]

 27%|██▋       | 2949/10740 [14:41:10<40:11:11, 18.57s/it]

 27%|██▋       | 2950/10740 [14:41:31<41:30:50, 19.18s/it]

 27%|██▋       | 2951/10740 [14:41:42<36:15:47, 16.76s/it]

 27%|██▋       | 2952/10740 [14:42:02<38:15:08, 17.68s/it]

 27%|██▋       | 2953/10740 [14:42:20<38:43:51, 17.91s/it]

 28%|██▊       | 2954/10740 [14:42:43<41:28:46, 19.18s/it]

 28%|██▊       | 2955/10740 [14:43:01<41:02:10, 18.98s/it]

 28%|██▊       | 2956/10740 [14:43:19<40:34:31, 18.77s/it]

 28%|██▊       | 2957/10740 [14:43:33<37:05:27, 17.16s/it]

 28%|██▊       | 2958/10740 [14:43:43<32:52:09, 15.21s/it]

 28%|██▊       | 2959/10740 [14:44:05<36:57:26, 17.10s/it]
[2024-04-02 09:58:10,116] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2960/10740 [14:44:26<39:46:09, 18.40s/it]
[2024-04-02 09:58:25,104] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2961/10740 [14:44:41<37:33:01, 17.38s/it]


 28%|██▊       | 2963/10740 [14:45:12<35:16:56, 16.33s/it]
{'loss': 0.5029, 'learning_rate': 1.6994420880443714e-06, 'rewards/chosen': -1.0447319746017456, 'rewards/rejected': -2.5351383686065674, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4904062747955322, 'policy_logps/rejected': -361.423583984375, 'policy_logps/chosen': -447.7408752441406, 'referece_logps/rejected': -336.07220458984375, 'referece_logps/chosen': -437.2935791015625, 'logits/rejected': -0.9799619913101196, 'logits/chosen': -0.9794052243232727, 'epoch': 1.66}

 28%|██▊       | 2964/10740 [14:45:28<35:22:23, 16.38s/it]

 28%|██▊       | 2965/10740 [14:45:47<36:39:23, 16.97s/it]

 28%|██▊       | 2966/10740 [14:46:05<37:30:41, 17.37s/it]

 28%|██▊       | 2967/10740 [14:46:18<34:56:10, 16.18s/it]

 28%|██▊       | 2968/10740 [14:46:36<35:34:38, 16.48s/it]

 28%|██▊       | 2969/10740 [14:46:55<37:37:51, 17.43s/it]

 28%|██▊       | 2970/10740 [14:47:11<36:42:43, 17.01s/it]

 28%|██▊       | 2971/10740 [14:47:29<37:00:18, 17.15s/it]

 28%|██▊       | 2972/10740 [14:47:42<34:33:18, 16.01s/it]

 28%|██▊       | 2973/10740 [14:48:03<37:37:36, 17.44s/it]


 28%|██▊       | 2975/10740 [14:48:34<34:35:53, 16.04s/it]
{'loss': 0.4748, 'learning_rate': 1.6968510536100035e-06, 'rewards/chosen': -1.4172035455703735, 'rewards/rejected': -2.436244249343872, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0190407037734985, 'policy_logps/rejected': -419.3846740722656, 'policy_logps/chosen': -359.61309814453125, 'referece_logps/rejected': -395.0222473144531, 'referece_logps/chosen': -345.44110107421875, 'logits/rejected': -0.6226927042007446, 'logits/chosen': -0.653637707233429, 'epoch': 1.66}

 28%|██▊       | 2976/10740 [14:48:47<32:21:14, 15.00s/it]

 28%|██▊       | 2977/10740 [14:49:08<36:26:37, 16.90s/it]

 28%|██▊       | 2978/10740 [14:49:25<36:34:10, 16.96s/it]

 28%|██▊       | 2979/10740 [14:49:45<38:24:37, 17.82s/it]

 28%|██▊       | 2980/10740 [14:50:04<39:32:36, 18.34s/it]

 28%|██▊       | 2981/10740 [14:50:27<42:21:54, 19.66s/it]

 28%|██▊       | 2982/10740 [14:50:47<42:39:21, 19.79s/it]

 28%|██▊       | 2983/10740 [14:51:08<42:54:45, 19.92s/it]

 28%|██▊       | 2984/10740 [14:51:23<40:22:11, 18.74s/it]


 28%|██▊       | 2986/10740 [14:52:00<40:10:12, 18.65s/it]

 28%|██▊       | 2987/10740 [14:52:22<42:22:36, 19.68s/it]
{'loss': 0.5769, 'learning_rate': 1.694250892420557e-06, 'rewards/chosen': -0.7269529104232788, 'rewards/rejected': -1.9317083358764648, 'rewards/accuracies': 0.75, 'rewards/margins': 1.204755425453186, 'policy_logps/rejected': -348.1080627441406, 'policy_logps/chosen': -415.8919372558594, 'referece_logps/rejected': -328.79095458984375, 'referece_logps/chosen': -408.6224365234375, 'logits/rejected': 0.2471962720155716, 'logits/chosen': 0.2840862274169922, 'epoch': 1.67}

 28%|██▊       | 2988/10740 [14:52:41<41:52:48, 19.45s/it]
[2024-04-02 10:06:42,661] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2989/10740 [14:52:59<40:40:20, 18.89s/it]

 28%|██▊       | 2990/10740 [14:53:15<38:55:50, 18.08s/it]

 28%|██▊       | 2991/10740 [14:53:34<39:34:35, 18.39s/it]

 28%|██▊       | 2992/10740 [14:53:50<37:38:59, 17.49s/it]

 28%|██▊       | 2993/10740 [14:54:04<35:38:12, 16.56s/it]

 28%|██▊       | 2994/10740 [14:54:24<37:37:12, 17.48s/it]

 28%|██▊       | 2995/10740 [14:54:40<36:35:13, 17.01s/it]

 28%|██▊       | 2996/10740 [14:54:53<34:12:57, 15.91s/it]

 28%|██▊       | 2997/10740 [14:55:12<36:13:31, 16.84s/it]

 28%|██▊       | 2998/10740 [14:55:25<33:33:28, 15.60s/it]

 28%|██▊       | 2999/10740 [14:55:35<30:25:45, 14.15s/it]


 28%|██▊       | 3001/10740 [14:56:29<45:47:45, 21.30s/it]
{'loss': 0.4973, 'learning_rate': 1.6912058812676642e-06, 'rewards/chosen': -1.418073058128357, 'rewards/rejected': -2.556159257888794, 'rewards/accuracies': 0.75, 'rewards/margins': 1.138085961341858, 'policy_logps/rejected': -466.4107360839844, 'policy_logps/chosen': -425.15960693359375, 'referece_logps/rejected': -440.8491516113281, 'referece_logps/chosen': -410.9788513183594, 'logits/rejected': 0.3146839439868927, 'logits/chosen': 0.21725696325302124, 'epoch': 1.68}
[2024-04-02 10:10:33,509] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3002/10740 [14:56:50<45:43:10, 21.27s/it]
[2024-04-02 10:10:51,426] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3003/10740 [14:57:08<43:33:06, 20.26s/it]

 28%|██▊       | 3004/10740 [14:57:26<42:11:09, 19.63s/it]

 28%|██▊       | 3005/10740 [14:57:47<43:21:38, 20.18s/it]

 28%|██▊       | 3006/10740 [14:58:05<41:59:30, 19.55s/it]

 28%|██▊       | 3007/10740 [14:58:22<40:07:14, 18.68s/it]

 28%|██▊       | 3008/10740 [14:58:37<37:38:37, 17.53s/it]

 28%|██▊       | 3009/10740 [14:58:52<35:47:32, 16.67s/it]

 28%|██▊       | 3010/10740 [14:59:08<35:51:30, 16.70s/it]

 28%|██▊       | 3011/10740 [14:59:28<37:42:13, 17.56s/it]

 28%|██▊       | 3012/10740 [14:59:39<33:51:55, 15.78s/it]

 28%|██▊       | 3013/10740 [14:59:53<32:31:46, 15.16s/it]

 28%|██▊       | 3014/10740 [15:00:10<33:46:50, 15.74s/it]

 28%|██▊       | 3015/10740 [15:00:28<35:13:14, 16.41s/it]

 28%|██▊       | 3016/10740 [15:00:41<33:04:02, 15.41s/it]

 28%|██▊       | 3017/10740 [15:00:58<33:44:22, 15.73s/it]

 28%|██▊       | 3018/10740 [15:01:15<34:51:32, 16.25s/it]
[2024-04-02 10:15:20,168] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3019/10740 [15:01:36<37:59:56, 17.72s/it]
[2024-04-02 10:15:33,746] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 28%|██▊       | 3021/10740 [15:02:07<35:37:35, 16.62s/it]
{'loss': 0.4108, 'learning_rate': 1.6868345041293202e-06, 'rewards/chosen': -1.5458157062530518, 'rewards/rejected': -3.665605306625366, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1197896003723145, 'policy_logps/rejected': -380.30804443359375, 'policy_logps/chosen': -357.7608642578125, 'referece_logps/rejected': -343.6519775390625, 'referece_logps/chosen': -342.302734375, 'logits/rejected': 0.1385299265384674, 'logits/chosen': 0.12200335413217545, 'epoch': 1.69}

 28%|██▊       | 3022/10740 [15:02:26<37:07:36, 17.32s/it]

 28%|██▊       | 3023/10740 [15:02:46<38:41:10, 18.05s/it]


 28%|██▊       | 3025/10740 [15:03:25<40:25:09, 18.86s/it]
{'loss': 0.4295, 'learning_rate': 1.685957225080016e-06, 'rewards/chosen': -1.0423144102096558, 'rewards/rejected': -1.3917030096054077, 'rewards/accuracies': 0.5, 'rewards/margins': 0.34938859939575195, 'policy_logps/rejected': -309.47332763671875, 'policy_logps/chosen': -294.5118103027344, 'referece_logps/rejected': -295.5562744140625, 'referece_logps/chosen': -284.08868408203125, 'logits/rejected': -1.1644855737686157, 'logits/chosen': -1.1966724395751953, 'epoch': 1.69}

 28%|██▊       | 3026/10740 [15:03:45<41:00:37, 19.14s/it]
[2024-04-02 10:17:50,055] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3027/10740 [15:04:06<42:28:21, 19.82s/it]
[2024-04-02 10:18:05,737] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3028/10740 [15:04:22<39:48:21, 18.58s/it]

 28%|██▊       | 3029/10740 [15:04:45<42:52:26, 20.02s/it]
[2024-04-02 10:18:51,510] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3030/10740 [15:05:08<44:24:16, 20.73s/it]

 28%|██▊       | 3031/10740 [15:05:30<45:39:42, 21.32s/it]


 28%|██▊       | 3033/10740 [15:05:59<37:45:56, 17.64s/it]
{'loss': 0.5133, 'learning_rate': 1.6841996735642411e-06, 'rewards/chosen': -1.1225285530090332, 'rewards/rejected': -2.5681066513061523, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4455779790878296, 'policy_logps/rejected': -286.95684814453125, 'policy_logps/chosen': -249.08485412597656, 'referece_logps/rejected': -261.2757568359375, 'referece_logps/chosen': -237.85955810546875, 'logits/rejected': -0.5080161094665527, 'logits/chosen': -0.539889395236969, 'epoch': 1.69}

 28%|██▊       | 3034/10740 [15:06:19<39:07:38, 18.28s/it]
[2024-04-02 10:20:20,657] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3035/10740 [15:06:37<38:55:07, 18.18s/it]

 28%|██▊       | 3036/10740 [15:06:54<37:59:26, 17.75s/it]

 28%|██▊       | 3037/10740 [15:07:11<37:59:03, 17.75s/it]

 28%|██▊       | 3038/10740 [15:07:32<40:05:02, 18.74s/it]
[2024-04-02 10:21:36,785] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3039/10740 [15:07:53<41:16:29, 19.29s/it]

 28%|██▊       | 3040/10740 [15:08:06<37:00:28, 17.30s/it]

 28%|██▊       | 3041/10740 [15:08:29<40:57:26, 19.15s/it]


 28%|██▊       | 3043/10740 [15:09:09<41:56:24, 19.62s/it]

 28%|██▊       | 3044/10740 [15:09:29<42:14:13, 19.76s/it]
[2024-04-02 10:23:13,230] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3194, 'learning_rate': 1.681776539328039e-06, 'rewards/chosen': -0.7376255989074707, 'rewards/rejected': -4.084344387054443, 'rewards/accuracies': 1.0, 'rewards/margins': 3.346719264984131, 'policy_logps/rejected': -364.35791015625, 'policy_logps/chosen': -337.09515380859375, 'referece_logps/rejected': -323.5144348144531, 'referece_logps/chosen': -329.7189025878906, 'logits/rejected': -0.9612898826599121, 'logits/chosen': -0.9784631729125977, 'epoch': 1.7}


 28%|██▊       | 3046/10740 [15:10:13<44:24:31, 20.78s/it]

 28%|██▊       | 3047/10740 [15:10:30<42:20:29, 19.81s/it]

 28%|██▊       | 3048/10740 [15:10:51<42:44:03, 20.00s/it]

 28%|██▊       | 3049/10740 [15:11:11<42:42:28, 19.99s/it]

 28%|██▊       | 3050/10740 [15:11:26<39:35:11, 18.53s/it]

 28%|██▊       | 3051/10740 [15:11:37<35:04:06, 16.42s/it]

 28%|██▊       | 3052/10740 [15:11:55<35:56:32, 16.83s/it]

 28%|██▊       | 3053/10740 [15:12:08<33:47:11, 15.82s/it]

 28%|██▊       | 3054/10740 [15:12:20<31:11:51, 14.61s/it]

 28%|██▊       | 3055/10740 [15:12:42<35:30:09, 16.63s/it]
{'loss': 0.4832, 'learning_rate': 1.6793459019796223e-06, 'rewards/chosen': -1.0835249423980713, 'rewards/rejected': -1.7878944873809814, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7043694853782654, 'policy_logps/rejected': -435.7970275878906, 'policy_logps/chosen': -431.3723449707031, 'referece_logps/rejected': -417.9180908203125, 'referece_logps/chosen': -420.537109375, 'logits/rejected': -0.1511593759059906, 'logits/chosen': -0.14414368569850922, 'epoch': 1.71}

 28%|██▊       | 3056/10740 [15:12:54<32:47:29, 15.36s/it]


 28%|██▊       | 3058/10740 [15:13:29<36:01:25, 16.88s/it]
[2024-04-02 10:27:13,002] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3059/10740 [15:13:48<37:30:11, 17.58s/it]
{'loss': 0.3636, 'learning_rate': 1.6784601787999532e-06, 'rewards/chosen': -0.418313592672348, 'rewards/rejected': -2.00948429107666, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5911705493927002, 'policy_logps/rejected': -583.5115356445312, 'policy_logps/chosen': -338.7705383300781, 'referece_logps/rejected': -563.416748046875, 'referece_logps/chosen': -334.5873718261719, 'logits/rejected': -0.05165122449398041, 'logits/chosen': -0.09149345755577087, 'epoch': 1.71}


 29%|██▊       | 3061/10740 [15:14:19<35:33:37, 16.67s/it]
{'loss': 0.4459, 'learning_rate': 1.678016946883843e-06, 'rewards/chosen': -0.8799835443496704, 'rewards/rejected': -2.0514779090881348, 'rewards/accuracies': 0.875, 'rewards/margins': 1.171494483947754, 'policy_logps/rejected': -592.8399658203125, 'policy_logps/chosen': -445.85797119140625, 'referece_logps/rejected': -572.3251953125, 'referece_logps/chosen': -437.05816650390625, 'logits/rejected': 0.2959062457084656, 'logits/chosen': 0.267365425825119, 'epoch': 1.71}


 29%|██▊       | 3063/10740 [15:14:52<35:45:22, 16.77s/it]

 29%|██▊       | 3064/10740 [15:15:14<38:57:04, 18.27s/it]

 29%|██▊       | 3065/10740 [15:15:32<39:19:26, 18.45s/it]

 29%|██▊       | 3066/10740 [15:15:52<39:47:11, 18.66s/it]

 29%|██▊       | 3067/10740 [15:16:12<41:06:16, 19.29s/it]

 29%|██▊       | 3068/10740 [15:16:27<38:24:22, 18.02s/it]

 29%|██▊       | 3069/10740 [15:16:48<39:48:01, 18.68s/it]

 29%|██▊       | 3070/10740 [15:17:05<39:07:37, 18.36s/it]

 29%|██▊       | 3071/10740 [15:17:19<35:56:51, 16.87s/it]

 29%|██▊       | 3072/10740 [15:17:31<32:47:19, 15.39s/it]

 29%|██▊       | 3073/10740 [15:17:52<36:30:10, 17.14s/it]
[2024-04-02 10:31:35,545] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▊       | 3074/10740 [15:18:05<34:01:35, 15.98s/it]

 29%|██▊       | 3075/10740 [15:18:21<34:09:25, 16.04s/it]

 29%|██▊       | 3076/10740 [15:18:37<34:12:47, 16.07s/it]

 29%|██▊       | 3077/10740 [15:18:56<35:42:31, 16.78s/it]

 29%|██▊       | 3078/10740 [15:19:18<38:51:10, 18.26s/it]

 29%|██▊       | 3079/10740 [15:19:32<36:27:56, 17.14s/it]
{'loss': 0.4524, 'learning_rate': 1.6740167789042073e-06, 'rewards/chosen': -1.0877808332443237, 'rewards/rejected': -3.208876132965088, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1210954189300537, 'policy_logps/rejected': -360.09429931640625, 'policy_logps/chosen': -352.5723876953125, 'referece_logps/rejected': -328.00555419921875, 'referece_logps/chosen': -341.6946105957031, 'logits/rejected': -1.5802454948425293, 'logits/chosen': -1.5382484197616577, 'epoch': 1.72}


 29%|██▊       | 3081/10740 [15:20:09<37:20:21, 17.55s/it]

 29%|██▊       | 3082/10740 [15:20:25<36:47:12, 17.29s/it]
{'loss': 0.5746, 'learning_rate': 1.6733481499940306e-06, 'rewards/chosen': -1.3393940925598145, 'rewards/rejected': -1.7414277791976929, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4020337760448456, 'policy_logps/rejected': -321.8602600097656, 'policy_logps/chosen': -295.3138122558594, 'referece_logps/rejected': -304.4460144042969, 'referece_logps/chosen': -281.9198913574219, 'logits/rejected': -0.45584845542907715, 'logits/chosen': -0.5807524919509888, 'epoch': 1.72}


 29%|██▊       | 3084/10740 [15:21:05<40:06:01, 18.86s/it]

 29%|██▊       | 3085/10740 [15:21:27<41:50:32, 19.68s/it]

 29%|██▊       | 3086/10740 [15:21:46<41:21:58, 19.46s/it]

 29%|██▊       | 3087/10740 [15:22:04<40:38:04, 19.11s/it]
{'loss': 0.4631, 'learning_rate': 1.6722325438943186e-06, 'rewards/chosen': -1.5774693489074707, 'rewards/rejected': -2.5086472034454346, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9311777949333191, 'policy_logps/rejected': -442.83551025390625, 'policy_logps/chosen': -411.51470947265625, 'referece_logps/rejected': -417.7490539550781, 'referece_logps/chosen': -395.7400207519531, 'logits/rejected': -0.8663638830184937, 'logits/chosen': -0.8883060216903687, 'epoch': 1.72}


 29%|██▉       | 3089/10740 [15:22:40<39:30:40, 18.59s/it]

 29%|██▉       | 3090/10740 [15:22:58<39:17:01, 18.49s/it]

 29%|██▉       | 3091/10740 [15:23:13<37:00:38, 17.42s/it]

 29%|██▉       | 3092/10740 [15:23:34<39:39:01, 18.66s/it]

 29%|██▉       | 3093/10740 [15:23:54<40:23:06, 19.01s/it]

 29%|██▉       | 3094/10740 [15:24:15<41:38:31, 19.61s/it]

 29%|██▉       | 3095/10740 [15:24:28<37:20:29, 17.58s/it]

 29%|██▉       | 3096/10740 [15:24:52<41:14:04, 19.42s/it]
{'loss': 0.4248, 'learning_rate': 1.6702206027269003e-06, 'rewards/chosen': -1.7908068895339966, 'rewards/rejected': -2.2652666568756104, 'rewards/accuracies': 0.75, 'rewards/margins': 0.47445976734161377, 'policy_logps/rejected': -450.6073303222656, 'policy_logps/chosen': -327.1235656738281, 'referece_logps/rejected': -427.95465087890625, 'referece_logps/chosen': -309.2154846191406, 'logits/rejected': -0.8713489770889282, 'logits/chosen': -0.8844295740127563, 'epoch': 1.73}

 29%|██▉       | 3097/10740 [15:25:07<38:23:54, 18.09s/it]


 29%|██▉       | 3099/10740 [15:25:37<36:07:43, 17.02s/it]

 29%|██▉       | 3100/10740 [15:25:53<35:28:58, 16.72s/it]

 29%|██▉       | 3101/10740 [15:26:05<32:14:09, 15.19s/it]

 29%|██▉       | 3102/10740 [15:26:25<35:16:40, 16.63s/it]

 29%|██▉       | 3103/10740 [15:26:36<31:25:40, 14.81s/it]

 29%|██▉       | 3104/10740 [15:26:50<31:14:37, 14.73s/it]

 29%|██▉       | 3105/10740 [15:27:08<33:16:39, 15.69s/it]

 29%|██▉       | 3106/10740 [15:27:28<35:51:48, 16.91s/it]

 29%|██▉       | 3107/10740 [15:27:44<35:06:10, 16.56s/it]

 29%|██▉       | 3108/10740 [15:27:55<31:54:36, 15.05s/it]

 29%|██▉       | 3109/10740 [15:28:11<32:05:24, 15.14s/it]

 29%|██▉       | 3110/10740 [15:28:22<29:52:20, 14.09s/it]

 29%|██▉       | 3111/10740 [15:28:42<33:27:27, 15.79s/it]

 29%|██▉       | 3112/10740 [15:28:57<33:06:23, 15.62s/it]

 29%|██▉       | 3113/10740 [15:29:14<33:50:49, 15.98s/it]

 29%|██▉       | 3114/10740 [15:29:30<34:07:46, 16.11s/it]

 29%|██▉       | 3115/10740 [15:29:50<36:32:57, 17.26s/it]

 29%|██▉       | 3116/10740 [15:30:12<39:28:37, 18.64s/it]
{'loss': 0.4447, 'learning_rate': 1.665731966161441e-06, 'rewards/chosen': -1.8056193590164185, 'rewards/rejected': -2.60146427154541, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7958449125289917, 'policy_logps/rejected': -424.4134521484375, 'policy_logps/chosen': -424.7296142578125, 'referece_logps/rejected': -398.3988342285156, 'referece_logps/chosen': -406.67340087890625, 'logits/rejected': -1.8348207473754883, 'logits/chosen': -1.8369159698486328, 'epoch': 1.74}


 29%|██▉       | 3118/10740 [15:30:49<39:19:31, 18.57s/it]
{'loss': 0.3988, 'learning_rate': 1.6652817677078032e-06, 'rewards/chosen': -2.151136875152588, 'rewards/rejected': -2.9716203212738037, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8204835653305054, 'policy_logps/rejected': -409.4921875, 'policy_logps/chosen': -356.7917785644531, 'referece_logps/rejected': -379.7759704589844, 'referece_logps/chosen': -335.2804260253906, 'logits/rejected': -0.6382502317428589, 'logits/chosen': -0.6712180972099304, 'epoch': 1.74}

 29%|██▉       | 3119/10740 [15:31:07<39:08:28, 18.49s/it]


 29%|██▉       | 3121/10740 [15:31:42<37:44:29, 17.83s/it]

 29%|██▉       | 3122/10740 [15:31:58<36:47:56, 17.39s/it]

 29%|██▉       | 3123/10740 [15:32:16<36:52:06, 17.43s/it]

 29%|██▉       | 3124/10740 [15:32:34<37:28:53, 17.72s/it]

 29%|██▉       | 3125/10740 [15:32:50<36:38:43, 17.32s/it]

 29%|██▉       | 3126/10740 [15:33:08<36:32:47, 17.28s/it]

 29%|██▉       | 3127/10740 [15:33:29<39:02:47, 18.46s/it]

 29%|██▉       | 3128/10740 [15:33:48<39:39:51, 18.76s/it]

 29%|██▉       | 3129/10740 [15:34:08<40:31:48, 19.17s/it]

 29%|██▉       | 3130/10740 [15:34:26<39:27:25, 18.67s/it]
{'loss': 0.4542, 'learning_rate': 1.6625754999680845e-06, 'rewards/chosen': -1.5323072671890259, 'rewards/rejected': -3.4817235469818115, 'rewards/accuracies': 1.0, 'rewards/margins': 1.949415922164917, 'policy_logps/rejected': -301.80792236328125, 'policy_logps/chosen': -303.77374267578125, 'referece_logps/rejected': -266.9906921386719, 'referece_logps/chosen': -288.45062255859375, 'logits/rejected': -0.08804690092802048, 'logits/chosen': -0.034197740256786346, 'epoch': 1.75}


 29%|██▉       | 3132/10740 [15:35:06<40:52:34, 19.34s/it]

 29%|██▉       | 3133/10740 [15:35:29<43:14:01, 20.46s/it]
{'loss': 0.4419, 'learning_rate': 1.661897575733079e-06, 'rewards/chosen': -1.1596002578735352, 'rewards/rejected': -2.4335103034973145, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2739102840423584, 'policy_logps/rejected': -297.9264831542969, 'policy_logps/chosen': -276.2728271484375, 'referece_logps/rejected': -273.59136962890625, 'referece_logps/chosen': -264.6768493652344, 'logits/rejected': -0.8809947371482849, 'logits/chosen': -0.9861711263656616, 'epoch': 1.75}


 29%|██▉       | 3135/10740 [15:36:02<38:13:02, 18.09s/it]

 29%|██▉       | 3136/10740 [15:36:20<38:21:04, 18.16s/it]

 29%|██▉       | 3137/10740 [15:36:43<41:07:29, 19.47s/it]

 29%|██▉       | 3138/10740 [15:37:02<40:54:57, 19.38s/it]

 29%|██▉       | 3139/10740 [15:37:22<41:19:26, 19.57s/it]

 29%|██▉       | 3140/10740 [15:37:33<35:55:41, 17.02s/it]

 29%|██▉       | 3141/10740 [15:37:48<34:32:52, 16.37s/it]
{'loss': 0.5993, 'learning_rate': 1.6600871304287044e-06, 'rewards/chosen': -1.0873560905456543, 'rewards/rejected': -1.790879487991333, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7035233378410339, 'policy_logps/rejected': -460.3089904785156, 'policy_logps/chosen': -428.34613037109375, 'referece_logps/rejected': -442.4002380371094, 'referece_logps/chosen': -417.47259521484375, 'logits/rejected': -0.8772073984146118, 'logits/chosen': -0.8210360407829285, 'epoch': 1.75}


 29%|██▉       | 3143/10740 [15:38:18<33:37:16, 15.93s/it]

 29%|██▉       | 3144/10740 [15:38:35<34:14:27, 16.23s/it]

 29%|██▉       | 3145/10740 [15:38:46<31:03:16, 14.72s/it]

 29%|██▉       | 3146/10740 [15:39:03<32:09:27, 15.24s/it]

 29%|██▉       | 3147/10740 [15:39:22<34:55:41, 16.56s/it]

 29%|██▉       | 3148/10740 [15:39:38<34:16:25, 16.25s/it]

 29%|██▉       | 3149/10740 [15:40:00<37:51:13, 17.95s/it]
{'loss': 0.4179, 'learning_rate': 1.658272842787392e-06, 'rewards/chosen': -1.4470771551132202, 'rewards/rejected': -2.586883068084717, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1398060321807861, 'policy_logps/rejected': -537.1144409179688, 'policy_logps/chosen': -642.1930541992188, 'referece_logps/rejected': -511.24560546875, 'referece_logps/chosen': -627.7222900390625, 'logits/rejected': -0.8133550882339478, 'logits/chosen': -0.9512366652488708, 'epoch': 1.76}


 29%|██▉       | 3151/10740 [15:40:37<38:46:20, 18.39s/it]

 29%|██▉       | 3152/10740 [15:40:56<39:08:05, 18.57s/it]

 29%|██▉       | 3153/10740 [15:41:16<40:05:40, 19.02s/it]

 29%|██▉       | 3154/10740 [15:41:38<41:48:43, 19.84s/it]

 29%|██▉       | 3155/10740 [15:41:58<42:04:35, 19.97s/it]

 29%|██▉       | 3156/10740 [15:42:19<42:37:03, 20.23s/it]

 29%|██▉       | 3157/10740 [15:42:39<42:32:25, 20.20s/it]

 29%|██▉       | 3158/10740 [15:42:57<40:53:51, 19.42s/it]

 29%|██▉       | 3159/10740 [15:43:16<41:02:29, 19.49s/it]

 29%|██▉       | 3160/10740 [15:43:37<41:46:18, 19.84s/it]

 29%|██▉       | 3161/10740 [15:43:59<43:10:26, 20.51s/it]

 29%|██▉       | 3162/10740 [15:44:19<42:36:39, 20.24s/it]

 29%|██▉       | 3163/10740 [15:44:31<37:24:04, 17.77s/it]
{'loss': 0.4481, 'learning_rate': 1.6550886255695816e-06, 'rewards/chosen': -1.1589065790176392, 'rewards/rejected': -2.644242286682129, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4853357076644897, 'policy_logps/rejected': -287.8831787109375, 'policy_logps/chosen': -252.8197784423828, 'referece_logps/rejected': -261.44073486328125, 'referece_logps/chosen': -241.23074340820312, 'logits/rejected': -1.1969408988952637, 'logits/chosen': -1.2010972499847412, 'epoch': 1.77}


 29%|██▉       | 3165/10740 [15:45:08<37:53:18, 18.01s/it]
{'loss': 0.4224, 'learning_rate': 1.6546327827597986e-06, 'rewards/chosen': -1.5417661666870117, 'rewards/rejected': -2.500166177749634, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9583999514579773, 'policy_logps/rejected': -367.8662414550781, 'policy_logps/chosen': -246.66424560546875, 'referece_logps/rejected': -342.86456298828125, 'referece_logps/chosen': -231.24656677246094, 'logits/rejected': -0.618566632270813, 'logits/chosen': -0.5192564129829407, 'epoch': 1.77}

 29%|██▉       | 3166/10740 [15:45:26<38:20:29, 18.22s/it]


 29%|██▉       | 3168/10740 [15:46:03<38:13:36, 18.17s/it]
{'loss': 0.4096, 'learning_rate': 1.6539485720435555e-06, 'rewards/chosen': -1.8557502031326294, 'rewards/rejected': -1.9667580127716064, 'rewards/accuracies': 0.625, 'rewards/margins': 0.11100790649652481, 'policy_logps/rejected': -252.161376953125, 'policy_logps/chosen': -253.36581420898438, 'referece_logps/rejected': -232.4938201904297, 'referece_logps/chosen': -234.80831909179688, 'logits/rejected': -1.0221508741378784, 'logits/chosen': -1.0782099962234497, 'epoch': 1.77}

 30%|██▉       | 3169/10740 [15:46:22<38:48:28, 18.45s/it]


 30%|██▉       | 3171/10740 [15:47:04<41:21:35, 19.67s/it]

 30%|██▉       | 3172/10740 [15:47:23<41:24:34, 19.70s/it]

 30%|██▉       | 3173/10740 [15:47:42<40:30:31, 19.27s/it]

 30%|██▉       | 3174/10740 [15:47:56<37:14:48, 17.72s/it]
{'loss': 0.3574, 'learning_rate': 1.6525785452595682e-06, 'rewards/chosen': -1.9771270751953125, 'rewards/rejected': -3.111671209335327, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1345443725585938, 'policy_logps/rejected': -384.0087585449219, 'policy_logps/chosen': -498.7524108886719, 'referece_logps/rejected': -352.89202880859375, 'referece_logps/chosen': -478.98114013671875, 'logits/rejected': -0.7321345806121826, 'logits/chosen': -0.6699872612953186, 'epoch': 1.77}

 30%|██▉       | 3175/10740 [15:48:12<36:34:28, 17.41s/it]

 30%|██▉       | 3176/10740 [15:48:28<35:32:50, 16.92s/it]


 30%|██▉       | 3178/10740 [15:49:03<36:49:22, 17.53s/it]

 30%|██▉       | 3179/10740 [15:49:26<39:55:13, 19.01s/it]

 30%|██▉       | 3180/10740 [15:49:41<37:43:08, 17.96s/it]
{'loss': 0.4803, 'learning_rate': 1.6512063817458603e-06, 'rewards/chosen': -1.3321884870529175, 'rewards/rejected': -2.382993221282959, 'rewards/accuracies': 0.75, 'rewards/margins': 1.050804615020752, 'policy_logps/rejected': -367.5703430175781, 'policy_logps/chosen': -357.0433654785156, 'referece_logps/rejected': -343.7403869628906, 'referece_logps/chosen': -343.7215270996094, 'logits/rejected': -0.11931955814361572, 'logits/chosen': -0.14752672612667084, 'epoch': 1.78}

 30%|██▉       | 3181/10740 [15:49:55<34:47:48, 16.57s/it]

 30%|██▉       | 3182/10740 [15:50:15<37:03:01, 17.65s/it]

 30%|██▉       | 3183/10740 [15:50:34<38:23:47, 18.29s/it]


 30%|██▉       | 3185/10740 [15:51:12<38:40:55, 18.43s/it]
{'loss': 0.3768, 'learning_rate': 1.6500612831681768e-06, 'rewards/chosen': -1.1005408763885498, 'rewards/rejected': -2.806319236755371, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7057783603668213, 'policy_logps/rejected': -509.2596130371094, 'policy_logps/chosen': -515.9244995117188, 'referece_logps/rejected': -481.1964111328125, 'referece_logps/chosen': -504.9190673828125, 'logits/rejected': -0.2236587405204773, 'logits/chosen': -0.3058028817176819, 'epoch': 1.78}


 30%|██▉       | 3187/10740 [15:51:49<39:31:48, 18.84s/it]
{'loss': 0.424, 'learning_rate': 1.649602829718581e-06, 'rewards/chosen': -1.485166311264038, 'rewards/rejected': -3.315221071243286, 'rewards/accuracies': 0.875, 'rewards/margins': 1.830054759979248, 'policy_logps/rejected': -412.6710205078125, 'policy_logps/chosen': -375.4591064453125, 'referece_logps/rejected': -379.5188293457031, 'referece_logps/chosen': -360.6074523925781, 'logits/rejected': -0.7434487342834473, 'logits/chosen': -0.9024328589439392, 'epoch': 1.78}

 30%|██▉       | 3188/10740 [15:52:08<39:37:24, 18.89s/it]


 30%|██▉       | 3190/10740 [15:52:41<38:18:11, 18.26s/it]
{'loss': 0.4092, 'learning_rate': 1.648914706474064e-06, 'rewards/chosen': -1.819770336151123, 'rewards/rejected': -3.042229652404785, 'rewards/accuracies': 0.75, 'rewards/margins': 1.222459316253662, 'policy_logps/rejected': -373.93096923828125, 'policy_logps/chosen': -328.82183837890625, 'referece_logps/rejected': -343.5086975097656, 'referece_logps/chosen': -310.6241455078125, 'logits/rejected': -0.7916524410247803, 'logits/chosen': -0.8028358817100525, 'epoch': 1.78}

 30%|██▉       | 3191/10740 [15:52:57<36:28:37, 17.40s/it]


 30%|██▉       | 3193/10740 [15:53:30<35:01:26, 16.71s/it]

 30%|██▉       | 3194/10740 [15:53:51<38:03:06, 18.15s/it]

 30%|██▉       | 3195/10740 [15:54:11<39:09:22, 18.68s/it]

 30%|██▉       | 3196/10740 [15:54:31<39:54:02, 19.04s/it]
{'loss': 0.4489, 'learning_rate': 1.6475368669984731e-06, 'rewards/chosen': -0.8648569583892822, 'rewards/rejected': -1.7791424989700317, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9142854809761047, 'policy_logps/rejected': -286.2674865722656, 'policy_logps/chosen': -261.1313171386719, 'referece_logps/rejected': -268.47607421875, 'referece_logps/chosen': -252.48275756835938, 'logits/rejected': -0.9520964622497559, 'logits/chosen': -1.0569555759429932, 'epoch': 1.79}


 30%|██▉       | 3198/10740 [15:55:10<40:20:45, 19.26s/it]

 30%|██▉       | 3199/10740 [15:55:22<35:34:07, 16.98s/it]

 30%|██▉       | 3200/10740 [15:55:38<34:59:29, 16.71s/it]
{'loss': 0.4954, 'learning_rate': 1.6466171291683e-06, 'rewards/chosen': -1.2012478113174438, 'rewards/rejected': -1.2084684371948242, 'rewards/accuracies': 0.25, 'rewards/margins': 0.007220812141895294, 'policy_logps/rejected': -331.2608337402344, 'policy_logps/chosen': -332.8997802734375, 'referece_logps/rejected': -319.1761779785156, 'referece_logps/chosen': -320.8872985839844, 'logits/rejected': -0.8615546822547913, 'logits/chosen': -0.9581480026245117, 'epoch': 1.79}


 30%|██▉       | 3202/10740 [15:56:18<38:44:48, 18.50s/it]

 30%|██▉       | 3203/10740 [15:56:37<39:21:08, 18.80s/it]
{'loss': 0.583, 'learning_rate': 1.6459267082026496e-06, 'rewards/chosen': -1.519088625907898, 'rewards/rejected': -2.1330082416534424, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6139196157455444, 'policy_logps/rejected': -381.1834716796875, 'policy_logps/chosen': -433.7152404785156, 'referece_logps/rejected': -359.8533935546875, 'referece_logps/chosen': -418.5243225097656, 'logits/rejected': -0.4314776062965393, 'logits/chosen': -0.3314227759838104, 'epoch': 1.79}


 30%|██▉       | 3205/10740 [15:57:22<43:01:45, 20.56s/it]
{'loss': 0.4527, 'learning_rate': 1.6454661337809053e-06, 'rewards/chosen': -1.7010071277618408, 'rewards/rejected': -3.40360426902771, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7025971412658691, 'policy_logps/rejected': -467.1495666503906, 'policy_logps/chosen': -347.994873046875, 'referece_logps/rejected': -433.1135559082031, 'referece_logps/chosen': -330.9847717285156, 'logits/rejected': 0.034973278641700745, 'logits/chosen': -0.06893026828765869, 'epoch': 1.79}


 30%|██▉       | 3207/10740 [15:57:58<40:03:58, 19.15s/it]
{'loss': 0.5079, 'learning_rate': 1.6450053245322545e-06, 'rewards/chosen': -1.6888436079025269, 'rewards/rejected': -2.555837392807007, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8669939041137695, 'policy_logps/rejected': -452.5590515136719, 'policy_logps/chosen': -376.824951171875, 'referece_logps/rejected': -427.0006408691406, 'referece_logps/chosen': -359.9365539550781, 'logits/rejected': -0.31552594900131226, 'logits/chosen': -0.3164207339286804, 'epoch': 1.79}


 30%|██▉       | 3209/10740 [15:58:32<38:09:08, 18.24s/it]

 30%|██▉       | 3210/10740 [15:58:46<35:39:09, 17.05s/it]

 30%|██▉       | 3211/10740 [15:59:00<33:17:36, 15.92s/it]

 30%|██▉       | 3212/10740 [15:59:20<35:45:09, 17.10s/it]

 30%|██▉       | 3213/10740 [15:59:40<37:34:47, 17.97s/it]

 30%|██▉       | 3214/10740 [15:59:56<36:49:46, 17.62s/it]
{'loss': 0.5007, 'learning_rate': 1.6433906453212667e-06, 'rewards/chosen': -1.4680795669555664, 'rewards/rejected': -1.8005378246307373, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3324581980705261, 'policy_logps/rejected': -523.1382446289062, 'policy_logps/chosen': -478.7144775390625, 'referece_logps/rejected': -505.1328125, 'referece_logps/chosen': -464.03363037109375, 'logits/rejected': -1.313531517982483, 'logits/chosen': -1.3536806106567383, 'epoch': 1.8}


 30%|██▉       | 3216/10740 [16:00:24<32:26:45, 15.52s/it]

 30%|██▉       | 3217/10740 [16:00:42<33:34:36, 16.07s/it]

 30%|██▉       | 3218/10740 [16:00:54<31:27:16, 15.05s/it]

 30%|██▉       | 3219/10740 [16:01:14<34:18:08, 16.42s/it]

 30%|██▉       | 3220/10740 [16:01:32<35:32:36, 17.02s/it]

 30%|██▉       | 3221/10740 [16:01:50<36:10:34, 17.32s/it]

 30%|███       | 3222/10740 [16:02:07<35:28:13, 16.99s/it]

 30%|███       | 3223/10740 [16:02:28<38:29:01, 18.43s/it]

 30%|███       | 3224/10740 [16:02:50<40:24:21, 19.35s/it]
{'loss': 0.4113, 'learning_rate': 1.6410789884956033e-06, 'rewards/chosen': -0.48986345529556274, 'rewards/rejected': -1.8641948699951172, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3743313550949097, 'policy_logps/rejected': -377.8506774902344, 'policy_logps/chosen': -318.8697204589844, 'referece_logps/rejected': -359.208740234375, 'referece_logps/chosen': -313.9710693359375, 'logits/rejected': 0.3032904863357544, 'logits/chosen': 0.29584774374961853, 'epoch': 1.8}

 30%|███       | 3225/10740 [16:03:03<36:21:25, 17.42s/it]


 30%|███       | 3227/10740 [16:03:38<36:19:21, 17.40s/it]

 30%|███       | 3228/10740 [16:04:00<39:13:13, 18.80s/it]

 30%|███       | 3229/10740 [16:04:14<36:14:13, 17.37s/it]
{'loss': 0.4379, 'learning_rate': 1.639920972229289e-06, 'rewards/chosen': -0.04789291322231293, 'rewards/rejected': -1.1486536264419556, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1007606983184814, 'policy_logps/rejected': -351.33349609375, 'policy_logps/chosen': -338.24969482421875, 'referece_logps/rejected': -339.84698486328125, 'referece_logps/chosen': -337.770751953125, 'logits/rejected': 0.20384719967842102, 'logits/chosen': 0.23734739422798157, 'epoch': 1.8}

 30%|███       | 3230/10740 [16:04:34<37:46:26, 18.11s/it]


 30%|███       | 3232/10740 [16:05:10<37:59:31, 18.22s/it]

 30%|███       | 3233/10740 [16:05:32<40:32:33, 19.44s/it]
{'loss': 0.5178, 'learning_rate': 1.638993511447056e-06, 'rewards/chosen': -1.4786497354507446, 'rewards/rejected': -1.8150279521942139, 'rewards/accuracies': 0.75, 'rewards/margins': 0.33637839555740356, 'policy_logps/rejected': -284.5028381347656, 'policy_logps/chosen': -266.116455078125, 'referece_logps/rejected': -266.3525390625, 'referece_logps/chosen': -251.32997131347656, 'logits/rejected': -0.9341120719909668, 'logits/chosen': -0.994171679019928, 'epoch': 1.81}

 30%|███       | 3234/10740 [16:05:47<37:38:20, 18.05s/it]

 30%|███       | 3235/10740 [16:06:08<39:10:59, 18.80s/it]

 30%|███       | 3236/10740 [16:06:24<37:30:42, 18.00s/it]
[2024-04-02 11:20:29,326] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 3238/10740 [16:07:07<41:13:38, 19.78s/it]
[2024-04-02 11:20:50,721] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3239/10740 [16:07:27<41:21:54, 19.85s/it]
{'loss': 0.4109, 'learning_rate': 1.63760057715524e-06, 'rewards/chosen': -1.7230788469314575, 'rewards/rejected': -2.6060523986816406, 'rewards/accuracies': 0.75, 'rewards/margins': 0.882973849773407, 'policy_logps/rejected': -351.7615051269531, 'policy_logps/chosen': -304.49658203125, 'referece_logps/rejected': -325.7009582519531, 'referece_logps/chosen': -287.26580810546875, 'logits/rejected': -0.8982431888580322, 'logits/chosen': -0.6374929547309875, 'epoch': 1.81}

 30%|███       | 3240/10740 [16:07:48<41:52:08, 20.10s/it]

 30%|███       | 3241/10740 [16:08:02<38:05:15, 18.28s/it]


 30%|███       | 3243/10740 [16:08:37<36:46:08, 17.66s/it]
{'loss': 0.4108, 'learning_rate': 1.6366707941857976e-06, 'rewards/chosen': -1.442962646484375, 'rewards/rejected': -3.0524845123291016, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6095221042633057, 'policy_logps/rejected': -335.4667663574219, 'policy_logps/chosen': -348.8590087890625, 'referece_logps/rejected': -304.9419250488281, 'referece_logps/chosen': -334.429443359375, 'logits/rejected': 0.29580995440483093, 'logits/chosen': 0.25079357624053955, 'epoch': 1.81}


 30%|███       | 3245/10740 [16:09:17<39:18:56, 18.88s/it]
{'loss': 0.4364, 'learning_rate': 1.6362055551758765e-06, 'rewards/chosen': -0.9105924367904663, 'rewards/rejected': -2.4517171382904053, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5411245822906494, 'policy_logps/rejected': -335.8658447265625, 'policy_logps/chosen': -341.2815856933594, 'referece_logps/rejected': -311.34869384765625, 'referece_logps/chosen': -332.1756591796875, 'logits/rejected': -1.1670938730239868, 'logits/chosen': -1.1402661800384521, 'epoch': 1.81}


 30%|███       | 3247/10740 [16:09:53<37:57:42, 18.24s/it]
{'loss': 0.3461, 'learning_rate': 1.6357400847081386e-06, 'rewards/chosen': -0.9879359006881714, 'rewards/rejected': -2.7669503688812256, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7790144681930542, 'policy_logps/rejected': -406.6838684082031, 'policy_logps/chosen': -332.6502380371094, 'referece_logps/rejected': -379.01434326171875, 'referece_logps/chosen': -322.7708740234375, 'logits/rejected': -0.06639397144317627, 'logits/chosen': -0.1335963010787964, 'epoch': 1.81}


 30%|███       | 3249/10740 [16:10:35<40:44:09, 19.58s/it]

 30%|███       | 3250/10740 [16:10:51<38:02:24, 18.28s/it]

 30%|███       | 3251/10740 [16:11:09<38:04:49, 18.31s/it]

 30%|███       | 3252/10740 [16:11:23<35:38:57, 17.14s/it]
{'loss': 0.4893, 'learning_rate': 1.6345753970223646e-06, 'rewards/chosen': -1.3044092655181885, 'rewards/rejected': -1.8376796245574951, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5332704186439514, 'policy_logps/rejected': -575.5482788085938, 'policy_logps/chosen': -416.9990539550781, 'referece_logps/rejected': -557.1715087890625, 'referece_logps/chosen': -403.9549560546875, 'logits/rejected': -0.3335188329219818, 'logits/chosen': -0.25434449315071106, 'epoch': 1.82}

 30%|███       | 3253/10740 [16:11:40<35:10:16, 16.91s/it]
[2024-04-02 11:25:43,669] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3254/10740 [16:12:00<37:15:35, 17.92s/it]
[2024-04-02 11:26:03,623] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3255/10740 [16:12:20<38:31:30, 18.53s/it]
[2024-04-02 11:26:21,821] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3256/10740 [16:12:38<38:18:52, 18.43s/it]

 30%|███       | 3257/10740 [16:12:56<38:15:00, 18.40s/it]

 30%|███       | 3258/10740 [16:13:20<41:35:52, 20.02s/it]


 30%|███       | 3260/10740 [16:13:45<33:55:48, 16.33s/it]

 30%|███       | 3261/10740 [16:14:03<34:25:08, 16.57s/it]
{'loss': 0.5023, 'learning_rate': 1.632475324850489e-06, 'rewards/chosen': -1.3633407354354858, 'rewards/rejected': -1.6130707263946533, 'rewards/accuracies': 0.625, 'rewards/margins': 0.24972999095916748, 'policy_logps/rejected': -475.45782470703125, 'policy_logps/chosen': -390.8778381347656, 'referece_logps/rejected': -459.3271484375, 'referece_logps/chosen': -377.24444580078125, 'logits/rejected': 0.16578343510627747, 'logits/chosen': 0.18909382820129395, 'epoch': 1.82}


 30%|███       | 3263/10740 [16:14:35<34:00:45, 16.38s/it]
{'loss': 0.4589, 'learning_rate': 1.6320080088236163e-06, 'rewards/chosen': -1.1525951623916626, 'rewards/rejected': -2.1013097763061523, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9487147927284241, 'policy_logps/rejected': -422.0317687988281, 'policy_logps/chosen': -405.4150085449219, 'referece_logps/rejected': -401.0187072753906, 'referece_logps/chosen': -393.8890380859375, 'logits/rejected': -1.1730321645736694, 'logits/chosen': -1.1008152961730957, 'epoch': 1.82}


 30%|███       | 3265/10740 [16:15:11<36:36:26, 17.63s/it]

 30%|███       | 3266/10740 [16:15:29<36:39:00, 17.65s/it]
{'loss': 0.4165, 'learning_rate': 1.6313066037163812e-06, 'rewards/chosen': -1.4033602476119995, 'rewards/rejected': -2.519042491912842, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1156818866729736, 'policy_logps/rejected': -632.5435180664062, 'policy_logps/chosen': -480.2982482910156, 'referece_logps/rejected': -607.3530883789062, 'referece_logps/chosen': -466.2646179199219, 'logits/rejected': -0.057229384779930115, 'logits/chosen': 0.11891747266054153, 'epoch': 1.82}

 30%|███       | 3267/10740 [16:15:50<39:01:38, 18.80s/it]
[2024-04-02 11:29:55,465] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 3269/10740 [16:16:23<35:39:47, 17.18s/it]
[2024-04-02 11:30:07,142] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4289, 'learning_rate': 1.6306046818392227e-06, 'rewards/chosen': -0.990927517414093, 'rewards/rejected': -2.0129313468933105, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0220037698745728, 'policy_logps/rejected': -403.02392578125, 'policy_logps/chosen': -447.793212890625, 'referece_logps/rejected': -382.8945617675781, 'referece_logps/chosen': -437.8839111328125, 'logits/rejected': -0.8990195989608765, 'logits/chosen': -0.9522836208343506, 'epoch': 1.83}


 30%|███       | 3271/10740 [16:16:57<35:45:48, 17.24s/it]
{'loss': 0.54, 'learning_rate': 1.6301364471104379e-06, 'rewards/chosen': -1.610358715057373, 'rewards/rejected': -2.07517409324646, 'rewards/accuracies': 0.625, 'rewards/margins': 0.46481549739837646, 'policy_logps/rejected': -348.92474365234375, 'policy_logps/chosen': -395.7900695800781, 'referece_logps/rejected': -328.1729736328125, 'referece_logps/chosen': -379.6864929199219, 'logits/rejected': -0.7128298282623291, 'logits/chosen': -0.8181472420692444, 'epoch': 1.83}

 30%|███       | 3272/10740 [16:17:10<32:57:18, 15.89s/it]

 30%|███       | 3273/10740 [16:17:30<35:28:00, 17.10s/it]


 30%|███       | 3275/10740 [16:17:59<32:36:03, 15.72s/it]

 31%|███       | 3276/10740 [16:18:16<32:53:13, 15.86s/it]
{'loss': 0.429, 'learning_rate': 1.6289648576933747e-06, 'rewards/chosen': -1.0269676446914673, 'rewards/rejected': -3.9734222888946533, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9464545249938965, 'policy_logps/rejected': -466.34515380859375, 'policy_logps/chosen': -463.8394775390625, 'referece_logps/rejected': -426.6109313964844, 'referece_logps/chosen': -453.5697326660156, 'logits/rejected': -0.7994874715805054, 'logits/chosen': -0.7509245872497559, 'epoch': 1.83}


 31%|███       | 3278/10740 [16:18:56<37:06:32, 17.90s/it]
{'loss': 0.4278, 'learning_rate': 1.6284958213360567e-06, 'rewards/chosen': -1.154028058052063, 'rewards/rejected': -2.354870557785034, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2008424997329712, 'policy_logps/rejected': -386.7279052734375, 'policy_logps/chosen': -286.88421630859375, 'referece_logps/rejected': -363.1791687011719, 'referece_logps/chosen': -275.34393310546875, 'logits/rejected': -0.8547666668891907, 'logits/chosen': -0.8769888877868652, 'epoch': 1.83}


 31%|███       | 3280/10740 [16:19:30<36:53:25, 17.80s/it]
{'loss': 0.5445, 'learning_rate': 1.6280265563257984e-06, 'rewards/chosen': -1.5931216478347778, 'rewards/rejected': -2.624258279800415, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0311366319656372, 'policy_logps/rejected': -378.2908020019531, 'policy_logps/chosen': -431.08087158203125, 'referece_logps/rejected': -352.0482177734375, 'referece_logps/chosen': -415.149658203125, 'logits/rejected': -0.2465045005083084, 'logits/chosen': -0.22980085015296936, 'epoch': 1.83}

 31%|███       | 3281/10740 [16:19:49<37:45:50, 18.23s/it]

 31%|███       | 3282/10740 [16:20:07<37:44:15, 18.22s/it]

 31%|███       | 3283/10740 [16:20:30<40:52:35, 19.73s/it]

 31%|███       | 3284/10740 [16:20:48<39:46:07, 19.20s/it]

 31%|███       | 3285/10740 [16:21:05<38:25:56, 18.56s/it]

 31%|███       | 3286/10740 [16:21:25<39:13:07, 18.94s/it]

 31%|███       | 3287/10740 [16:21:39<35:54:59, 17.35s/it]

 31%|███       | 3288/10740 [16:21:58<37:17:46, 18.02s/it]

 31%|███       | 3289/10740 [16:22:18<38:26:24, 18.57s/it]


 31%|███       | 3291/10740 [16:22:48<34:07:26, 16.49s/it]
{'loss': 0.4761, 'learning_rate': 1.6254415192314472e-06, 'rewards/chosen': -1.2860643863677979, 'rewards/rejected': -2.0776827335357666, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7916184067726135, 'policy_logps/rejected': -461.652099609375, 'policy_logps/chosen': -326.65960693359375, 'referece_logps/rejected': -440.875244140625, 'referece_logps/chosen': -313.7989501953125, 'logits/rejected': -0.041898056864738464, 'logits/chosen': -0.20670276880264282, 'epoch': 1.84}

 31%|███       | 3292/10740 [16:23:03<33:33:09, 16.22s/it]

 31%|███       | 3293/10740 [16:23:24<36:02:07, 17.42s/it]

 31%|███       | 3294/10740 [16:23:43<37:30:48, 18.14s/it]

 31%|███       | 3295/10740 [16:24:01<37:25:30, 18.10s/it]


 31%|███       | 3297/10740 [16:24:46<41:50:14, 20.24s/it]
{'loss': 0.403, 'learning_rate': 1.6240285960234806e-06, 'rewards/chosen': -1.2742745876312256, 'rewards/rejected': -2.376403570175171, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1021288633346558, 'policy_logps/rejected': -565.2200317382812, 'policy_logps/chosen': -484.52239990234375, 'referece_logps/rejected': -541.4559936523438, 'referece_logps/chosen': -471.7796325683594, 'logits/rejected': -0.8752185106277466, 'logits/chosen': -0.8492196798324585, 'epoch': 1.84}

 31%|███       | 3298/10740 [16:25:04<40:37:23, 19.65s/it]

 31%|███       | 3299/10740 [16:25:25<41:05:16, 19.88s/it]
[2024-04-02 11:39:28,883] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3300/10740 [16:25:45<41:18:38, 19.99s/it]


 31%|███       | 3302/10740 [16:26:24<40:51:58, 19.78s/it]
{'loss': 0.4266, 'learning_rate': 1.622849599005114e-06, 'rewards/chosen': -1.1616045236587524, 'rewards/rejected': -2.250434160232544, 'rewards/accuracies': 0.75, 'rewards/margins': 1.088829517364502, 'policy_logps/rejected': -436.43902587890625, 'policy_logps/chosen': -399.1612243652344, 'referece_logps/rejected': -413.9346618652344, 'referece_logps/chosen': -387.5451965332031, 'logits/rejected': -0.7653689384460449, 'logits/chosen': -0.722846269607544, 'epoch': 1.84}


 31%|███       | 3304/10740 [16:26:54<36:18:16, 17.58s/it]
{'loss': 0.5211, 'learning_rate': 1.6223776034997157e-06, 'rewards/chosen': -1.8162139654159546, 'rewards/rejected': -2.2932939529418945, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4770801365375519, 'policy_logps/rejected': -444.3123474121094, 'policy_logps/chosen': -439.33795166015625, 'referece_logps/rejected': -421.3794250488281, 'referece_logps/chosen': -421.1758117675781, 'logits/rejected': 0.40105998516082764, 'logits/chosen': 0.45383840799331665, 'epoch': 1.85}

 31%|███       | 3305/10740 [16:27:15<38:33:35, 18.67s/it]


 31%|███       | 3307/10740 [16:27:54<39:22:04, 19.07s/it]
{'loss': 0.4743, 'learning_rate': 1.6216691857445417e-06, 'rewards/chosen': -0.6865006685256958, 'rewards/rejected': -2.3498189449310303, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6633182764053345, 'policy_logps/rejected': -453.19696044921875, 'policy_logps/chosen': -447.6046142578125, 'referece_logps/rejected': -429.69879150390625, 'referece_logps/chosen': -440.7396240234375, 'logits/rejected': -0.019972827285528183, 'logits/chosen': -0.17865782976150513, 'epoch': 1.85}

 31%|███       | 3308/10740 [16:28:14<39:41:40, 19.23s/it]

 31%|███       | 3309/10740 [16:28:35<40:54:23, 19.82s/it]

 31%|███       | 3310/10740 [16:28:51<38:40:58, 18.74s/it]

 31%|███       | 3311/10740 [16:29:11<39:33:35, 19.17s/it]

 31%|███       | 3312/10740 [16:29:32<40:27:57, 19.61s/it]
{'loss': 0.4928, 'learning_rate': 1.6204873589258007e-06, 'rewards/chosen': -2.2471797466278076, 'rewards/rejected': -3.0034799575805664, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7563003301620483, 'policy_logps/rejected': -472.2075500488281, 'policy_logps/chosen': -364.86102294921875, 'referece_logps/rejected': -442.1727294921875, 'referece_logps/chosen': -342.38922119140625, 'logits/rejected': -0.7443365454673767, 'logits/chosen': -0.820841908454895, 'epoch': 1.85}

 31%|███       | 3313/10740 [16:29:43<34:58:25, 16.95s/it]
[2024-04-02 11:43:45,420] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 31%|███       | 3315/10740 [16:30:16<34:19:57, 16.65s/it]
{'loss': 0.4914, 'learning_rate': 1.6197775854443982e-06, 'rewards/chosen': -0.6527198553085327, 'rewards/rejected': -2.748685121536255, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0959653854370117, 'policy_logps/rejected': -486.87091064453125, 'policy_logps/chosen': -302.6741943359375, 'referece_logps/rejected': -459.3840637207031, 'referece_logps/chosen': -296.14697265625, 'logits/rejected': -0.34939706325531006, 'logits/chosen': -0.26968201994895935, 'epoch': 1.85}

 31%|███       | 3316/10740 [16:30:35<35:31:26, 17.23s/it]

 31%|███       | 3317/10740 [16:30:56<38:01:17, 18.44s/it]

 31%|███       | 3318/10740 [16:31:13<36:58:57, 17.94s/it]
[2024-04-02 11:45:13,118] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3319/10740 [16:31:29<36:04:58, 17.50s/it]

 31%|███       | 3320/10740 [16:31:43<33:52:31, 16.44s/it]

 31%|███       | 3321/10740 [16:32:01<34:43:42, 16.85s/it]

 31%|███       | 3322/10740 [16:32:16<33:23:17, 16.20s/it]

 31%|███       | 3323/10740 [16:32:29<31:25:22, 15.25s/it]


 31%|███       | 3325/10740 [16:32:59<31:20:25, 15.22s/it]
[2024-04-02 11:46:42,314] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5251, 'learning_rate': 1.6174080130377621e-06, 'rewards/chosen': -1.974990963935852, 'rewards/rejected': -1.9442061185836792, 'rewards/accuracies': 0.375, 'rewards/margins': -0.03078487515449524, 'policy_logps/rejected': -428.1220703125, 'policy_logps/chosen': -491.5964050292969, 'referece_logps/rejected': -408.67999267578125, 'referece_logps/chosen': -471.84649658203125, 'logits/rejected': -0.8775814771652222, 'logits/chosen': -0.8343294858932495, 'epoch': 1.86}

 31%|███       | 3326/10740 [16:33:21<35:42:07, 17.34s/it]

 31%|███       | 3327/10740 [16:33:36<34:02:45, 16.53s/it]

 31%|███       | 3328/10740 [16:33:50<32:56:33, 16.00s/it]

 31%|███       | 3329/10740 [16:34:11<36:02:08, 17.50s/it]

 31%|███       | 3330/10740 [16:34:24<33:01:46, 16.05s/it]

 31%|███       | 3331/10740 [16:34:40<32:58:56, 16.03s/it]
[2024-04-02 11:48:42,166] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3332/10740 [16:34:58<34:30:50, 16.77s/it]

 31%|███       | 3333/10740 [16:35:13<33:16:26, 16.17s/it]

 31%|███       | 3334/10740 [16:35:26<31:17:25, 15.21s/it]

 31%|███       | 3335/10740 [16:35:48<35:08:45, 17.09s/it]

 31%|███       | 3336/10740 [16:36:05<35:37:15, 17.32s/it]

 31%|███       | 3337/10740 [16:36:24<36:38:51, 17.82s/it]

 31%|███       | 3338/10740 [16:36:35<32:12:23, 15.66s/it]

 31%|███       | 3339/10740 [16:36:56<35:29:25, 17.26s/it]

 31%|███       | 3340/10740 [16:37:13<35:31:11, 17.28s/it]
[2024-04-02 11:51:17,065] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 31%|███       | 3342/10740 [16:37:47<34:14:18, 16.66s/it]
{'loss': 0.4329, 'learning_rate': 1.6133668640020707e-06, 'rewards/chosen': -1.985642433166504, 'rewards/rejected': -2.709824562072754, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7241820693016052, 'policy_logps/rejected': -422.08770751953125, 'policy_logps/chosen': -350.620849609375, 'referece_logps/rejected': -394.9894714355469, 'referece_logps/chosen': -330.7644348144531, 'logits/rejected': -0.503044605255127, 'logits/chosen': -0.558421790599823, 'epoch': 1.87}

 31%|███       | 3343/10740 [16:37:59<31:42:30, 15.43s/it]

 31%|███       | 3344/10740 [16:38:20<34:54:29, 16.99s/it]

 31%|███       | 3345/10740 [16:38:39<36:28:57, 17.76s/it]

 31%|███       | 3346/10740 [16:38:58<36:48:45, 17.92s/it]


 31%|███       | 3348/10740 [16:39:27<32:45:56, 15.96s/it]
{'loss': 0.5005, 'learning_rate': 1.6119367213144105e-06, 'rewards/chosen': -1.8260236978530884, 'rewards/rejected': -2.5288119316101074, 'rewards/accuracies': 0.875, 'rewards/margins': 0.702788233757019, 'policy_logps/rejected': -386.43450927734375, 'policy_logps/chosen': -315.8910827636719, 'referece_logps/rejected': -361.1463623046875, 'referece_logps/chosen': -297.630859375, 'logits/rejected': -0.36740946769714355, 'logits/chosen': -0.4254164397716522, 'epoch': 1.87}


 31%|███       | 3350/10740 [16:40:01<33:23:48, 16.27s/it]
{'loss': 0.5141, 'learning_rate': 1.6114595615967105e-06, 'rewards/chosen': -1.0714038610458374, 'rewards/rejected': -1.5720223188400269, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5006185173988342, 'policy_logps/rejected': -323.177001953125, 'policy_logps/chosen': -402.75579833984375, 'referece_logps/rejected': -307.456787109375, 'referece_logps/chosen': -392.0417785644531, 'logits/rejected': -1.3711950778961182, 'logits/chosen': -1.3679790496826172, 'epoch': 1.87}

 31%|███       | 3351/10740 [16:40:22<36:34:35, 17.82s/it]

 31%|███       | 3352/10740 [16:40:42<37:40:04, 18.35s/it]


 31%|███       | 3354/10740 [16:41:19<38:27:43, 18.75s/it]

 31%|███       | 3355/10740 [16:41:33<35:34:06, 17.34s/it]

 31%|███       | 3356/10740 [16:41:47<33:19:17, 16.25s/it]
{'loss': 0.4554, 'learning_rate': 1.6100267484085071e-06, 'rewards/chosen': -0.9350320100784302, 'rewards/rejected': -2.410806179046631, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4757741689682007, 'policy_logps/rejected': -347.7125549316406, 'policy_logps/chosen': -422.1285095214844, 'referece_logps/rejected': -323.6045227050781, 'referece_logps/chosen': -412.7781677246094, 'logits/rejected': 0.25725382566452026, 'logits/chosen': 0.31952130794525146, 'epoch': 1.87}

 31%|███▏      | 3357/10740 [16:42:07<35:27:16, 17.29s/it]
[2024-04-02 11:56:09,522] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3358/10740 [16:42:26<36:36:19, 17.85s/it]


 31%|███▏      | 3360/10740 [16:42:51<30:56:50, 15.10s/it]
{'loss': 0.4244, 'learning_rate': 1.609070429658225e-06, 'rewards/chosen': -1.31689453125, 'rewards/rejected': -1.9675809144973755, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6506864428520203, 'policy_logps/rejected': -462.4305114746094, 'policy_logps/chosen': -527.0319213867188, 'referece_logps/rejected': -442.75469970703125, 'referece_logps/chosen': -513.8629760742188, 'logits/rejected': 0.2738609313964844, 'logits/chosen': 0.15606693923473358, 'epoch': 1.88}
[2024-04-02 11:56:56,319] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3361/10740 [16:43:13<34:44:58, 16.95s/it]

 31%|███▏      | 3362/10740 [16:43:32<36:16:39, 17.70s/it]

 31%|███▏      | 3363/10740 [16:43:47<34:26:55, 16.81s/it]

 31%|███▏      | 3364/10740 [16:44:00<32:08:15, 15.69s/it]

 31%|███▏      | 3365/10740 [16:44:20<34:50:41, 17.01s/it]

 31%|███▏      | 3366/10740 [16:44:38<35:38:22, 17.40s/it]

 31%|███▏      | 3367/10740 [16:44:56<35:57:55, 17.56s/it]

 31%|███▏      | 3368/10740 [16:45:19<38:58:06, 19.03s/it]

 31%|███▏      | 3369/10740 [16:45:36<38:13:11, 18.67s/it]

 31%|███▏      | 3370/10740 [16:45:53<36:39:06, 17.90s/it]

 31%|███▏      | 3371/10740 [16:46:15<39:07:44, 19.12s/it]

 31%|███▏      | 3372/10740 [16:46:35<39:39:50, 19.38s/it]
[2024-04-02 12:00:39,514] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3373/10740 [16:46:56<40:49:10, 19.95s/it]
[2024-04-02 12:01:02,064] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3374/10740 [16:47:18<42:24:42, 20.73s/it]

 31%|███▏      | 3375/10740 [16:47:35<39:56:42, 19.53s/it]

 31%|███▏      | 3376/10740 [16:47:55<40:19:55, 19.72s/it]

 31%|███▏      | 3377/10740 [16:48:11<37:51:57, 18.51s/it]
[2024-04-02 12:02:15,378] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3378/10740 [16:48:32<39:13:06, 19.18s/it]

 31%|███▏      | 3379/10740 [16:48:51<39:36:20, 19.37s/it]
[2024-04-02 12:02:54,680] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3380/10740 [16:49:11<39:40:09, 19.40s/it]
[2024-04-02 12:03:11,948] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3381/10740 [16:49:28<38:21:20, 18.76s/it]
[2024-04-02 12:03:34,065] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3382/10740 [16:49:50<40:24:20, 19.77s/it]

 31%|███▏      | 3383/10740 [16:50:10<40:19:30, 19.73s/it]

 32%|███▏      | 3384/10740 [16:50:33<42:04:54, 20.59s/it]

 32%|███▏      | 3385/10740 [16:50:51<40:40:16, 19.91s/it]

 32%|███▏      | 3386/10740 [16:51:03<36:05:19, 17.67s/it]

 32%|███▏      | 3387/10740 [16:51:21<36:12:54, 17.73s/it]

 32%|███▏      | 3388/10740 [16:51:37<35:11:53, 17.24s/it]

 32%|███▏      | 3389/10740 [16:51:54<35:06:28, 17.19s/it]

 32%|███▏      | 3390/10740 [16:52:12<35:15:16, 17.27s/it]

 32%|███▏      | 3391/10740 [16:52:26<33:23:58, 16.36s/it]

 32%|███▏      | 3392/10740 [16:52:38<30:48:51, 15.10s/it]

 32%|███▏      | 3393/10740 [16:52:58<33:34:52, 16.45s/it]

 32%|███▏      | 3394/10740 [16:53:15<34:05:28, 16.71s/it]

 32%|███▏      | 3395/10740 [16:53:29<32:06:31, 15.74s/it]

 32%|███▏      | 3396/10740 [16:53:48<34:25:57, 16.88s/it]

 32%|███▏      | 3397/10740 [16:54:08<36:27:00, 17.87s/it]

 32%|███▏      | 3398/10740 [16:54:28<37:38:03, 18.45s/it]

 32%|███▏      | 3399/10740 [16:54:40<33:51:09, 16.60s/it]

 32%|███▏      | 3400/10740 [16:54:57<34:06:54, 16.73s/it]

 32%|███▏      | 3401/10740 [16:55:13<33:31:29, 16.44s/it]

 32%|███▏      | 3402/10740 [16:55:31<34:03:05, 16.71s/it]

 32%|███▏      | 3403/10740 [16:55:50<35:55:52, 17.63s/it]

 32%|███▏      | 3404/10740 [16:56:07<35:14:35, 17.29s/it]

 32%|███▏      | 3405/10740 [16:56:20<32:39:34, 16.03s/it]

 32%|███▏      | 3406/10740 [16:56:43<36:51:46, 18.09s/it]

 32%|███▏      | 3407/10740 [16:56:59<35:26:22, 17.40s/it]

 32%|███▏      | 3408/10740 [16:57:16<35:43:09, 17.54s/it]

 32%|███▏      | 3409/10740 [16:57:38<38:01:01, 18.67s/it]

 32%|███▏      | 3410/10740 [16:58:00<40:05:37, 19.69s/it]

 32%|███▏      | 3411/10740 [16:58:15<37:03:17, 18.20s/it]

 32%|███▏      | 3412/10740 [16:58:37<39:21:26, 19.34s/it]

 32%|███▏      | 3413/10740 [16:58:56<39:32:38, 19.43s/it]

 32%|███▏      | 3414/10740 [16:59:16<39:54:38, 19.61s/it]

 32%|███▏      | 3415/10740 [16:59:33<38:17:11, 18.82s/it]


 32%|███▏      | 3417/10740 [17:00:03<33:51:21, 16.64s/it]

 32%|███▏      | 3418/10740 [17:00:22<35:15:28, 17.34s/it]

 32%|███▏      | 3419/10740 [17:00:32<31:11:32, 15.34s/it]

 32%|███▏      | 3420/10740 [17:00:47<30:52:24, 15.18s/it]
{'loss': 0.5839, 'learning_rate': 1.594620069627286e-06, 'rewards/chosen': -1.1944366693496704, 'rewards/rejected': -1.224255084991455, 'rewards/accuracies': 0.625, 'rewards/margins': 0.029818259179592133, 'policy_logps/rejected': -391.53607177734375, 'policy_logps/chosen': -398.0421142578125, 'referece_logps/rejected': -379.29351806640625, 'referece_logps/chosen': -386.0977783203125, 'logits/rejected': -1.203535556793213, 'logits/chosen': -1.0194205045700073, 'epoch': 1.91}


 32%|███▏      | 3422/10740 [17:01:21<31:44:42, 15.62s/it]

 32%|███▏      | 3423/10740 [17:01:40<34:05:10, 16.77s/it]
{'loss': 0.3669, 'learning_rate': 1.5938924013982524e-06, 'rewards/chosen': -1.4697637557983398, 'rewards/rejected': -1.877777338027954, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4080135226249695, 'policy_logps/rejected': -524.0213623046875, 'policy_logps/chosen': -365.162353515625, 'referece_logps/rejected': -505.24365234375, 'referece_logps/chosen': -350.46466064453125, 'logits/rejected': -1.2812895774841309, 'logits/chosen': -1.2366321086883545, 'epoch': 1.91}


 32%|███▏      | 3425/10740 [17:02:15<33:56:53, 16.71s/it]

 32%|███▏      | 3426/10740 [17:02:34<35:38:47, 17.55s/it]

 32%|███▏      | 3427/10740 [17:02:54<36:54:10, 18.17s/it]

 32%|███▏      | 3428/10740 [17:03:13<37:12:05, 18.32s/it]
{'loss': 0.4495, 'learning_rate': 1.5926785409915153e-06, 'rewards/chosen': -1.1272525787353516, 'rewards/rejected': -2.3282976150512695, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2010447978973389, 'policy_logps/rejected': -460.9749755859375, 'policy_logps/chosen': -441.30078125, 'referece_logps/rejected': -437.6919860839844, 'referece_logps/chosen': -430.0282897949219, 'logits/rejected': -0.620542049407959, 'logits/chosen': -0.37743228673934937, 'epoch': 1.92}


 32%|███▏      | 3430/10740 [17:03:52<39:00:52, 19.21s/it]
[2024-04-02 12:17:35,881] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3431/10740 [17:04:11<38:52:03, 19.14s/it]
[2024-04-02 12:17:54,863] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3432/10740 [17:04:24<35:00:21, 17.24s/it]

 32%|███▏      | 3433/10740 [17:04:40<34:31:45, 17.01s/it]

 32%|███▏      | 3434/10740 [17:04:54<32:29:02, 16.01s/it]

 32%|███▏      | 3435/10740 [17:05:15<35:44:05, 17.61s/it]

 32%|███▏      | 3436/10740 [17:05:32<35:22:46, 17.44s/it]

 32%|███▏      | 3437/10740 [17:05:48<34:05:38, 16.81s/it]

 32%|███▏      | 3438/10740 [17:06:10<37:30:39, 18.49s/it]
[2024-04-02 12:19:53,956] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3439/10740 [17:06:30<38:12:26, 18.84s/it]

 32%|███▏      | 3440/10740 [17:06:43<34:49:05, 17.17s/it]

 32%|███▏      | 3441/10740 [17:07:02<35:47:36, 17.65s/it]
[2024-04-02 12:20:45,661] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3442/10740 [17:07:22<36:59:15, 18.25s/it]

 32%|███▏      | 3443/10740 [17:07:38<35:50:14, 17.68s/it]
{'loss': 0.5255, 'learning_rate': 1.589028884993426e-06, 'rewards/chosen': -1.3748815059661865, 'rewards/rejected': -2.306942939758301, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9320612549781799, 'policy_logps/rejected': -377.28668212890625, 'policy_logps/chosen': -333.7623596191406, 'referece_logps/rejected': -354.21728515625, 'referece_logps/chosen': -320.0135192871094, 'logits/rejected': -0.04812844097614288, 'logits/chosen': -0.05807715654373169, 'epoch': 1.92}


 32%|███▏      | 3445/10740 [17:08:19<38:25:25, 18.96s/it]

 32%|███▏      | 3446/10740 [17:08:38<38:57:05, 19.22s/it]

 32%|███▏      | 3447/10740 [17:08:53<35:57:47, 17.75s/it]

 32%|███▏      | 3448/10740 [17:09:12<36:49:20, 18.18s/it]

 32%|███▏      | 3449/10740 [17:09:25<33:37:20, 16.60s/it]

 32%|███▏      | 3450/10740 [17:09:45<35:36:54, 17.59s/it]

 32%|███▏      | 3451/10740 [17:09:56<32:02:53, 15.83s/it]

 32%|███▏      | 3452/10740 [17:10:18<35:24:04, 17.49s/it]
{'loss': 0.4601, 'learning_rate': 1.586833300662977e-06, 'rewards/chosen': -1.0075182914733887, 'rewards/rejected': -3.081610679626465, 'rewards/accuracies': 1.0, 'rewards/margins': 2.074092149734497, 'policy_logps/rejected': -461.39422607421875, 'policy_logps/chosen': -235.68638610839844, 'referece_logps/rejected': -430.578125, 'referece_logps/chosen': -225.6112060546875, 'logits/rejected': 0.0926048532128334, 'logits/chosen': 0.08726875483989716, 'epoch': 1.93}


 32%|███▏      | 3454/10740 [17:10:59<38:22:25, 18.96s/it]

 32%|███▏      | 3455/10740 [17:11:15<36:55:14, 18.24s/it]
{'loss': 0.5145, 'learning_rate': 1.5861004776896818e-06, 'rewards/chosen': -1.2170008420944214, 'rewards/rejected': -1.549745798110962, 'rewards/accuracies': 0.625, 'rewards/margins': 0.33274489641189575, 'policy_logps/rejected': -368.8525695800781, 'policy_logps/chosen': -280.596923828125, 'referece_logps/rejected': -353.3551330566406, 'referece_logps/chosen': -268.42694091796875, 'logits/rejected': -0.3268875479698181, 'logits/chosen': -0.5002110004425049, 'epoch': 1.93}


 32%|███▏      | 3457/10740 [17:11:55<38:22:26, 18.97s/it]
[2024-04-02 12:25:38,527] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3458/10740 [17:12:09<35:21:51, 17.48s/it]

 32%|███▏      | 3459/10740 [17:12:27<35:41:00, 17.64s/it]
{'loss': 0.4539, 'learning_rate': 1.5851226341938761e-06, 'rewards/chosen': -0.5630748867988586, 'rewards/rejected': -1.0965025424957275, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5334277153015137, 'policy_logps/rejected': -481.7174987792969, 'policy_logps/chosen': -456.8069763183594, 'referece_logps/rejected': -470.75250244140625, 'referece_logps/chosen': -451.17620849609375, 'logits/rejected': -1.1406848430633545, 'logits/chosen': -0.9253302812576294, 'epoch': 1.93}


 32%|███▏      | 3461/10740 [17:13:04<36:15:49, 17.94s/it]

 32%|███▏      | 3462/10740 [17:13:22<36:22:30, 17.99s/it]

 32%|███▏      | 3463/10740 [17:13:35<33:11:52, 16.42s/it]

 32%|███▏      | 3464/10740 [17:13:51<33:00:26, 16.33s/it]

 32%|███▏      | 3465/10740 [17:14:09<33:59:41, 16.82s/it]

 32%|███▏      | 3466/10740 [17:14:26<34:15:40, 16.96s/it]

 32%|███▏      | 3467/10740 [17:14:38<31:01:35, 15.36s/it]

 32%|███▏      | 3468/10740 [17:14:50<28:52:42, 14.30s/it]

 32%|███▏      | 3469/10740 [17:15:03<28:22:41, 14.05s/it]

 32%|███▏      | 3470/10740 [17:15:21<30:25:03, 15.06s/it]

 32%|███▏      | 3471/10740 [17:15:42<34:28:57, 17.08s/it]
[2024-04-02 12:29:26,040] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3472/10740 [17:16:02<36:00:23, 17.83s/it]
[2024-04-02 12:29:45,640] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4937, 'learning_rate': 1.581938769585863e-06, 'rewards/chosen': -0.8597374558448792, 'rewards/rejected': -2.2463715076446533, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3866338729858398, 'policy_logps/rejected': -461.85784912109375, 'policy_logps/chosen': -470.6678466796875, 'referece_logps/rejected': -439.39410400390625, 'referece_logps/chosen': -462.070556640625, 'logits/rejected': -1.660485863685608, 'logits/chosen': -1.5966747999191284, 'epoch': 1.94}


 32%|███▏      | 3474/10740 [17:16:42<38:12:19, 18.93s/it]

 32%|███▏      | 3475/10740 [17:16:55<34:34:05, 17.13s/it]

 32%|███▏      | 3476/10740 [17:17:17<37:03:25, 18.37s/it]

 32%|███▏      | 3477/10740 [17:17:37<38:11:53, 18.93s/it]

 32%|███▏      | 3478/10740 [17:17:53<36:30:23, 18.10s/it]

 32%|███▏      | 3479/10740 [17:18:09<35:03:05, 17.38s/it]

 32%|███▏      | 3480/10740 [17:18:23<33:05:55, 16.41s/it]

 32%|███▏      | 3481/10740 [17:18:40<33:32:13, 16.63s/it]

 32%|███▏      | 3482/10740 [17:19:01<36:04:00, 17.89s/it]
[2024-04-02 12:32:44,510] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3483/10740 [17:19:17<35:19:47, 17.53s/it]
{'loss': 0.4059, 'learning_rate': 1.5792377417508096e-06, 'rewards/chosen': -1.413847804069519, 'rewards/rejected': -2.6560983657836914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2422504425048828, 'policy_logps/rejected': -436.14227294921875, 'policy_logps/chosen': -382.3267517089844, 'referece_logps/rejected': -409.581298828125, 'referece_logps/chosen': -368.18829345703125, 'logits/rejected': 0.5003955364227295, 'logits/chosen': 0.4287700355052948, 'epoch': 1.95}


 32%|███▏      | 3485/10740 [17:19:52<35:17:36, 17.51s/it]

 32%|███▏      | 3486/10740 [17:20:11<36:17:27, 18.01s/it]
{'loss': 0.5324, 'learning_rate': 1.578499990199508e-06, 'rewards/chosen': -1.4399787187576294, 'rewards/rejected': -3.5758512020111084, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1358726024627686, 'policy_logps/rejected': -361.20086669921875, 'policy_logps/chosen': -390.5604553222656, 'referece_logps/rejected': -325.4423522949219, 'referece_logps/chosen': -376.16070556640625, 'logits/rejected': -0.47876161336898804, 'logits/chosen': -0.5520734786987305, 'epoch': 1.95}

 32%|███▏      | 3487/10740 [17:20:30<37:05:12, 18.41s/it]


 32%|███▏      | 3489/10740 [17:21:15<41:09:37, 20.44s/it]

 32%|███▏      | 3490/10740 [17:21:33<39:56:20, 19.83s/it]

 33%|███▎      | 3491/10740 [17:21:55<41:23:11, 20.55s/it]

 33%|███▎      | 3492/10740 [17:22:11<38:12:02, 18.97s/it]

 33%|███▎      | 3493/10740 [17:22:23<33:58:22, 16.88s/it]

 33%|███▎      | 3494/10740 [17:22:39<33:33:54, 16.68s/it]

 33%|███▎      | 3495/10740 [17:22:53<32:09:38, 15.98s/it]

 33%|███▎      | 3496/10740 [17:23:13<34:26:51, 17.12s/it]
{'loss': 0.488, 'learning_rate': 1.5760374017177038e-06, 'rewards/chosen': -1.6177181005477905, 'rewards/rejected': -2.415739059448242, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7980210185050964, 'policy_logps/rejected': -321.563720703125, 'policy_logps/chosen': -344.200927734375, 'referece_logps/rejected': -297.4062805175781, 'referece_logps/chosen': -328.0237731933594, 'logits/rejected': -0.2498977780342102, 'logits/chosen': -0.23100316524505615, 'epoch': 1.95}

 33%|███▎      | 3497/10740 [17:23:33<35:57:15, 17.87s/it]


 33%|███▎      | 3499/10740 [17:24:05<33:25:15, 16.62s/it]

 33%|███▎      | 3500/10740 [17:24:22<33:39:02, 16.73s/it]

 33%|███▎      | 3501/10740 [17:24:55<43:37:56, 21.70s/it]
{'loss': 0.4995, 'learning_rate': 1.5748041413760094e-06, 'rewards/chosen': -1.003430962562561, 'rewards/rejected': -3.133957624435425, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1305267810821533, 'policy_logps/rejected': -628.338623046875, 'policy_logps/chosen': -566.08251953125, 'referece_logps/rejected': -596.9990844726562, 'referece_logps/chosen': -556.0482788085938, 'logits/rejected': -0.3208363354206085, 'logits/chosen': -0.24441106617450714, 'epoch': 1.96}


 33%|███▎      | 3503/10740 [17:25:31<39:31:48, 19.66s/it]
{'loss': 0.506, 'learning_rate': 1.5743104711233385e-06, 'rewards/chosen': -1.6639885902404785, 'rewards/rejected': -2.70043683052063, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0364481210708618, 'policy_logps/rejected': -438.6619567871094, 'policy_logps/chosen': -369.3592529296875, 'referece_logps/rejected': -411.6575927734375, 'referece_logps/chosen': -352.7193603515625, 'logits/rejected': -0.3659270405769348, 'logits/chosen': -0.3483423590660095, 'epoch': 1.96}
[2024-04-02 12:39:34,046] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 33%|███▎      | 3505/10740 [17:26:10<39:24:51, 19.61s/it]

 33%|███▎      | 3506/10740 [17:26:30<39:26:51, 19.63s/it]

 33%|███▎      | 3507/10740 [17:26:44<36:13:18, 18.03s/it]
{'loss': 0.5021, 'learning_rate': 1.5733225039783398e-06, 'rewards/chosen': -0.9961681962013245, 'rewards/rejected': -1.3701480627059937, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3739798963069916, 'policy_logps/rejected': -308.6463317871094, 'policy_logps/chosen': -289.24969482421875, 'referece_logps/rejected': -294.94488525390625, 'referece_logps/chosen': -279.28802490234375, 'logits/rejected': -0.6041303873062134, 'logits/chosen': -0.7371695041656494, 'epoch': 1.96}


 33%|███▎      | 3509/10740 [17:27:20<36:28:59, 18.16s/it]

 33%|███▎      | 3510/10740 [17:27:40<37:22:03, 18.61s/it]

 33%|███▎      | 3511/10740 [17:27:56<36:05:03, 17.97s/it]
{'loss': 0.3185, 'learning_rate': 1.5723337025120317e-06, 'rewards/chosen': -1.0589690208435059, 'rewards/rejected': -2.830341100692749, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7713719606399536, 'policy_logps/rejected': -351.38720703125, 'policy_logps/chosen': -388.8984375, 'referece_logps/rejected': -323.083740234375, 'referece_logps/chosen': -378.3088073730469, 'logits/rejected': 0.2962114214897156, 'logits/chosen': 0.3070959448814392, 'epoch': 1.96}


 33%|███▎      | 3513/10740 [17:28:32<35:29:08, 17.68s/it]

 33%|███▎      | 3514/10740 [17:28:52<36:47:17, 18.33s/it]

 33%|███▎      | 3515/10740 [17:29:12<37:58:00, 18.92s/it]

 33%|███▎      | 3516/10740 [17:29:29<37:06:29, 18.49s/it]

 33%|███▎      | 3517/10740 [17:29:42<33:39:42, 16.78s/it]

 33%|███▎      | 3518/10740 [17:29:56<31:35:05, 15.74s/it]

 33%|███▎      | 3519/10740 [17:30:12<32:02:19, 15.97s/it]
{'loss': 0.4964, 'learning_rate': 1.5703536023724693e-06, 'rewards/chosen': -1.8484413623809814, 'rewards/rejected': -2.2159204483032227, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3674790859222412, 'policy_logps/rejected': -396.9455261230469, 'policy_logps/chosen': -339.870849609375, 'referece_logps/rejected': -374.78631591796875, 'referece_logps/chosen': -321.38641357421875, 'logits/rejected': -1.043501853942871, 'logits/chosen': -1.0431102514266968, 'epoch': 1.97}

 33%|███▎      | 3520/10740 [17:30:23<29:03:23, 14.49s/it]

 33%|███▎      | 3521/10740 [17:30:41<31:03:54, 15.49s/it]

 33%|███▎      | 3522/10740 [17:30:57<31:27:48, 15.69s/it]


 33%|███▎      | 3524/10740 [17:31:31<33:07:29, 16.53s/it]

 33%|███▎      | 3525/10740 [17:31:44<31:02:20, 15.49s/it]

 33%|███▎      | 3526/10740 [17:31:58<29:44:30, 14.84s/it]

 33%|███▎      | 3527/10740 [17:32:11<29:07:14, 14.53s/it]

 33%|███▎      | 3528/10740 [17:32:33<33:33:26, 16.75s/it]

 33%|███▎      | 3529/10740 [17:32:50<33:42:07, 16.83s/it]

 33%|███▎      | 3530/10740 [17:33:10<35:04:52, 17.52s/it]
{'loss': 0.4765, 'learning_rate': 1.5676255460953255e-06, 'rewards/chosen': -0.9096424579620361, 'rewards/rejected': -2.2296857833862305, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3200433254241943, 'policy_logps/rejected': -383.9715270996094, 'policy_logps/chosen': -361.5404968261719, 'referece_logps/rejected': -361.67462158203125, 'referece_logps/chosen': -352.444091796875, 'logits/rejected': -0.4139525890350342, 'logits/chosen': -0.5781885385513306, 'epoch': 1.97}

 33%|███▎      | 3531/10740 [17:33:27<35:05:24, 17.52s/it]


 33%|███▎      | 3533/10740 [17:34:04<35:49:22, 17.89s/it]

 33%|███▎      | 3534/10740 [17:34:20<35:10:31, 17.57s/it]
{'loss': 0.5026, 'learning_rate': 1.566631975243148e-06, 'rewards/chosen': -1.622192621231079, 'rewards/rejected': -2.2196221351623535, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5974293947219849, 'policy_logps/rejected': -393.30328369140625, 'policy_logps/chosen': -566.912109375, 'referece_logps/rejected': -371.1070861816406, 'referece_logps/chosen': -550.6902465820312, 'logits/rejected': -0.31818053126335144, 'logits/chosen': -0.3731313943862915, 'epoch': 1.97}


 33%|███▎      | 3536/10740 [17:34:50<31:40:16, 15.83s/it]

 33%|███▎      | 3537/10740 [17:35:00<28:39:22, 14.32s/it]

 33%|███▎      | 3538/10740 [17:35:20<31:56:26, 15.97s/it]

 33%|███▎      | 3539/10740 [17:35:42<35:35:29, 17.79s/it]
{'loss': 0.4952, 'learning_rate': 1.5653888522748377e-06, 'rewards/chosen': -1.4825721979141235, 'rewards/rejected': -2.5192739963531494, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0367016792297363, 'policy_logps/rejected': -435.9452819824219, 'policy_logps/chosen': -386.3060302734375, 'referece_logps/rejected': -410.7525329589844, 'referece_logps/chosen': -371.48028564453125, 'logits/rejected': -0.10847832262516022, 'logits/chosen': -0.17467230558395386, 'epoch': 1.98}

 33%|███▎      | 3540/10740 [17:36:03<37:23:11, 18.69s/it]


 33%|███▎      | 3542/10740 [17:36:38<35:48:53, 17.91s/it]

 33%|███▎      | 3543/10740 [17:36:58<36:54:30, 18.46s/it]

 33%|███▎      | 3544/10740 [17:37:12<34:02:16, 17.03s/it]

 33%|███▎      | 3545/10740 [17:37:32<35:45:48, 17.89s/it]

 33%|███▎      | 3546/10740 [17:37:53<37:35:16, 18.81s/it]
{'loss': 0.4005, 'learning_rate': 1.5636463209662035e-06, 'rewards/chosen': -0.9321399927139282, 'rewards/rejected': -2.367664337158203, 'rewards/accuracies': 1.0, 'rewards/margins': 1.435524344444275, 'policy_logps/rejected': -302.0152587890625, 'policy_logps/chosen': -219.7896728515625, 'referece_logps/rejected': -278.3385925292969, 'referece_logps/chosen': -210.46829223632812, 'logits/rejected': -0.4530256986618042, 'logits/chosen': -0.3984408378601074, 'epoch': 1.98}

 33%|███▎      | 3547/10740 [17:38:10<36:27:35, 18.25s/it]


 33%|███▎      | 3549/10740 [17:38:48<37:34:36, 18.81s/it]
[2024-04-02 12:52:31,515] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3550/10740 [17:39:02<34:40:23, 17.36s/it]
{'loss': 0.4724, 'learning_rate': 1.5626494604613525e-06, 'rewards/chosen': -1.7592763900756836, 'rewards/rejected': -1.9684677124023438, 'rewards/accuracies': 0.375, 'rewards/margins': 0.20919154584407806, 'policy_logps/rejected': -373.01544189453125, 'policy_logps/chosen': -386.3476257324219, 'referece_logps/rejected': -353.3307189941406, 'referece_logps/chosen': -368.7548828125, 'logits/rejected': -0.09027586877346039, 'logits/chosen': -0.08571111410856247, 'epoch': 1.98}


 33%|███▎      | 3552/10740 [17:39:39<36:39:33, 18.36s/it]

 33%|███▎      | 3553/10740 [17:39:52<33:41:14, 16.87s/it]
{'loss': 0.5112, 'learning_rate': 1.5619012776727395e-06, 'rewards/chosen': -1.439489483833313, 'rewards/rejected': -1.4517419338226318, 'rewards/accuracies': 0.5, 'rewards/margins': 0.012252375483512878, 'policy_logps/rejected': -622.6043701171875, 'policy_logps/chosen': -575.9298706054688, 'referece_logps/rejected': -608.0869750976562, 'referece_logps/chosen': -561.534912109375, 'logits/rejected': 0.1787845492362976, 'logits/chosen': 0.19866608083248138, 'epoch': 1.98}
[2024-04-02 12:53:57,244] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3554/10740 [17:40:14<36:24:12, 18.24s/it]

 33%|███▎      | 3555/10740 [17:40:33<37:16:49, 18.68s/it]


 33%|███▎      | 3557/10740 [17:41:15<39:06:47, 19.60s/it]

 33%|███▎      | 3558/10740 [17:41:36<40:17:38, 20.20s/it]
{'loss': 0.436, 'learning_rate': 1.5606532845350733e-06, 'rewards/chosen': -1.2320795059204102, 'rewards/rejected': -2.906644582748413, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6745651960372925, 'policy_logps/rejected': -321.3675537109375, 'policy_logps/chosen': -272.1670227050781, 'referece_logps/rejected': -292.30108642578125, 'referece_logps/chosen': -259.8462219238281, 'logits/rejected': -0.4765584468841553, 'logits/chosen': -0.5892488956451416, 'epoch': 1.99}


 33%|███▎      | 3560/10740 [17:42:12<38:29:43, 19.30s/it]

 33%|███▎      | 3561/10740 [17:42:33<39:09:56, 19.64s/it]
{'loss': 0.4806, 'learning_rate': 1.559903876557357e-06, 'rewards/chosen': -1.315937876701355, 'rewards/rejected': -2.639777660369873, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3238399028778076, 'policy_logps/rejected': -322.78399658203125, 'policy_logps/chosen': -350.71173095703125, 'referece_logps/rejected': -296.38623046875, 'referece_logps/chosen': -337.5523986816406, 'logits/rejected': -0.8897209763526917, 'logits/chosen': -0.7577558755874634, 'epoch': 1.99}

 33%|███▎      | 3562/10740 [17:42:49<37:31:12, 18.82s/it]


 33%|███▎      | 3564/10740 [17:43:29<38:40:51, 19.41s/it]

 33%|███▎      | 3565/10740 [17:43:50<39:50:34, 19.99s/it]
{'loss': 0.3873, 'learning_rate': 1.558903953081697e-06, 'rewards/chosen': -1.3164479732513428, 'rewards/rejected': -1.8946444988250732, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5781964063644409, 'policy_logps/rejected': -362.00079345703125, 'policy_logps/chosen': -284.8320007324219, 'referece_logps/rejected': -343.05438232421875, 'referece_logps/chosen': -271.66754150390625, 'logits/rejected': -0.8756259679794312, 'logits/chosen': -0.9045145511627197, 'epoch': 1.99}


 33%|███▎      | 3567/10740 [17:44:29<39:06:33, 19.63s/it]
{'loss': 0.4499, 'learning_rate': 1.5584036862507986e-06, 'rewards/chosen': -1.1560148000717163, 'rewards/rejected': -2.7604691982269287, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6044543981552124, 'policy_logps/rejected': -322.22906494140625, 'policy_logps/chosen': -314.49267578125, 'referece_logps/rejected': -294.6244201660156, 'referece_logps/chosen': -302.93255615234375, 'logits/rejected': 0.18691852688789368, 'logits/chosen': 0.027091562747955322, 'epoch': 1.99}


 33%|███▎      | 3569/10740 [17:45:11<40:25:08, 20.29s/it]
{'loss': 0.4852, 'learning_rate': 1.5579032162671662e-06, 'rewards/chosen': -1.2719615697860718, 'rewards/rejected': -1.6177195310592651, 'rewards/accuracies': 0.375, 'rewards/margins': 0.3457581102848053, 'policy_logps/rejected': -252.4540557861328, 'policy_logps/chosen': -306.7308654785156, 'referece_logps/rejected': -236.27684020996094, 'referece_logps/chosen': -294.0112609863281, 'logits/rejected': -0.856817364692688, 'logits/chosen': -0.6546274423599243, 'epoch': 1.99}


 33%|███▎      | 3571/10740 [17:45:45<37:27:31, 18.81s/it]

 33%|███▎      | 3572/10740 [17:46:06<39:13:04, 19.70s/it]
[2024-04-02 12:59:50,144] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.44, 'learning_rate': 1.557152130778652e-06, 'rewards/chosen': -1.302628755569458, 'rewards/rejected': -3.4576611518859863, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1550326347351074, 'policy_logps/rejected': -483.87823486328125, 'policy_logps/chosen': -445.6664733886719, 'referece_logps/rejected': -449.3016357421875, 'referece_logps/chosen': -432.6402282714844, 'logits/rejected': 0.04223528504371643, 'logits/chosen': 0.06522031128406525, 'epoch': 2.0}
[2024-04-02 13:00:07,535] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3573/10740 [17:46:24<37:50:06, 19.00s/it]

 33%|███▎      | 3574/10740 [17:46:44<38:16:28, 19.23s/it]


 33%|███▎      | 3576/10740 [17:47:11<32:04:24, 16.12s/it]

 33%|███▎      | 3577/10740 [17:47:30<34:10:02, 17.17s/it]
{'loss': 0.4465, 'learning_rate': 1.555899308447919e-06, 'rewards/chosen': -1.0694748163223267, 'rewards/rejected': -3.2410197257995605, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1715447902679443, 'policy_logps/rejected': -372.020751953125, 'policy_logps/chosen': -340.7225646972656, 'referece_logps/rejected': -339.61053466796875, 'referece_logps/chosen': -330.02783203125, 'logits/rejected': -0.4935415983200073, 'logits/chosen': -0.47317445278167725, 'epoch': 2.0}

 33%|███▎      | 3578/10740 [17:47:50<35:41:51, 17.94s/it]


 33%|███▎      | 3580/10740 [17:48:22<33:35:05, 16.89s/it]

 33%|███▎      | 3581/10740 [17:48:43<35:57:14, 18.08s/it]
[2024-04-02 13:02:27,085] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3582/10740 [17:48:55<32:03:53, 16.13s/it]

 33%|███▎      | 3583/10740 [17:49:13<32:56:47, 16.57s/it]

 33%|███▎      | 3584/10740 [17:49:28<32:33:05, 16.38s/it]

 33%|███▎      | 3585/10740 [17:49:47<33:53:40, 17.05s/it]

 33%|███▎      | 3586/10740 [17:50:00<31:42:55, 15.96s/it]

 33%|███▎      | 3587/10740 [17:50:17<31:57:12, 16.08s/it]
{'loss': 0.4129, 'learning_rate': 1.5533898746082935e-06, 'rewards/chosen': -1.0118792057037354, 'rewards/rejected': -3.5844831466674805, 'rewards/accuracies': 1.0, 'rewards/margins': 2.572603940963745, 'policy_logps/rejected': -449.8981018066406, 'policy_logps/chosen': -498.28082275390625, 'referece_logps/rejected': -414.0533142089844, 'referece_logps/chosen': -488.1619873046875, 'logits/rejected': 0.30282363295555115, 'logits/chosen': 0.08981728553771973, 'epoch': 2.0}


 33%|███▎      | 3589/10740 [17:50:55<34:52:07, 17.55s/it]

 33%|███▎      | 3590/10740 [17:51:11<33:39:00, 16.94s/it]
{'loss': 0.4715, 'learning_rate': 1.5526360619402134e-06, 'rewards/chosen': -1.1025145053863525, 'rewards/rejected': -2.333940267562866, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2314257621765137, 'policy_logps/rejected': -399.20904541015625, 'policy_logps/chosen': -376.3006286621094, 'referece_logps/rejected': -375.8696594238281, 'referece_logps/chosen': -365.27545166015625, 'logits/rejected': -0.26069769263267517, 'logits/chosen': -0.3176236152648926, 'epoch': 2.01}


 33%|███▎      | 3592/10740 [17:51:49<35:57:34, 18.11s/it]
[2024-04-02 13:05:32,648] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4201, 'learning_rate': 1.5521332688053783e-06, 'rewards/chosen': -1.9823999404907227, 'rewards/rejected': -2.817359685897827, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8349595069885254, 'policy_logps/rejected': -378.3565368652344, 'policy_logps/chosen': -395.6088562011719, 'referece_logps/rejected': -350.1829833984375, 'referece_logps/chosen': -375.7848205566406, 'logits/rejected': 0.08867356181144714, 'logits/chosen': -0.1441044956445694, 'epoch': 2.01}


 33%|███▎      | 3594/10740 [17:52:29<37:32:32, 18.91s/it]
[2024-04-02 13:06:12,503] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3595/10740 [17:52:39<32:31:38, 16.39s/it]
{'loss': 0.4826, 'learning_rate': 1.5513787025262556e-06, 'rewards/chosen': -1.738638162612915, 'rewards/rejected': -2.557962417602539, 'rewards/accuracies': 0.5, 'rewards/margins': 0.819324254989624, 'policy_logps/rejected': -389.0167541503906, 'policy_logps/chosen': -379.03436279296875, 'referece_logps/rejected': -363.4371337890625, 'referece_logps/chosen': -361.6479797363281, 'logits/rejected': -0.5306451916694641, 'logits/chosen': -0.5210483074188232, 'epoch': 2.01}


 33%|███▎      | 3597/10740 [17:53:21<37:12:05, 18.75s/it]

 34%|███▎      | 3598/10740 [17:53:43<38:46:09, 19.54s/it]
{'loss': 0.457, 'learning_rate': 1.5506236849039533e-06, 'rewards/chosen': -1.3010739088058472, 'rewards/rejected': -2.010962963104248, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7098891735076904, 'policy_logps/rejected': -365.1353454589844, 'policy_logps/chosen': -461.95391845703125, 'referece_logps/rejected': -345.02569580078125, 'referece_logps/chosen': -448.94317626953125, 'logits/rejected': -0.10626808553934097, 'logits/chosen': -0.07919637858867645, 'epoch': 2.01}

 34%|███▎      | 3599/10740 [17:54:02<38:40:17, 19.50s/it]


 34%|███▎      | 3601/10740 [17:54:42<38:53:12, 19.61s/it]

 34%|███▎      | 3602/10740 [17:54:57<36:34:46, 18.45s/it]
{'loss': 0.5352, 'learning_rate': 1.5496162937197574e-06, 'rewards/chosen': -1.8438646793365479, 'rewards/rejected': -2.1197948455810547, 'rewards/accuracies': 0.625, 'rewards/margins': 0.27592992782592773, 'policy_logps/rejected': -403.3003234863281, 'policy_logps/chosen': -392.2170715332031, 'referece_logps/rejected': -382.1024169921875, 'referece_logps/chosen': -373.7784118652344, 'logits/rejected': -0.7799487709999084, 'logits/chosen': -0.8809197545051575, 'epoch': 2.01}
[2024-04-02 13:08:58,978] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▎      | 3603/10740 [17:55:15<36:16:00, 18.29s/it]


 34%|███▎      | 3605/10740 [17:55:46<32:36:01, 16.45s/it]
{'loss': 0.4383, 'learning_rate': 1.5488602253674818e-06, 'rewards/chosen': -1.0175673961639404, 'rewards/rejected': -2.8747990131378174, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8572319746017456, 'policy_logps/rejected': -406.4385070800781, 'policy_logps/chosen': -407.29638671875, 'referece_logps/rejected': -377.6905212402344, 'referece_logps/chosen': -397.12066650390625, 'logits/rejected': -0.7972526550292969, 'logits/chosen': -0.8124685287475586, 'epoch': 2.01}

 34%|███▎      | 3606/10740 [17:55:58<30:21:11, 15.32s/it]

 34%|███▎      | 3607/10740 [17:56:17<32:31:51, 16.42s/it]


 34%|███▎      | 3609/10740 [17:56:54<34:09:32, 17.24s/it]
[2024-04-02 13:10:37,582] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5049, 'learning_rate': 1.5478514354556099e-06, 'rewards/chosen': -1.386738657951355, 'rewards/rejected': -3.213963031768799, 'rewards/accuracies': 0.5, 'rewards/margins': 1.8272240161895752, 'policy_logps/rejected': -509.9340515136719, 'policy_logps/chosen': -490.72357177734375, 'referece_logps/rejected': -477.79437255859375, 'referece_logps/chosen': -476.856201171875, 'logits/rejected': -0.6734445095062256, 'logits/chosen': -0.5784071087837219, 'epoch': 2.02}

 34%|███▎      | 3610/10740 [17:57:14<36:08:47, 18.25s/it]

 34%|███▎      | 3611/10740 [17:57:38<39:21:34, 19.88s/it]
[2024-04-02 13:11:44,082] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▎      | 3612/10740 [17:58:00<40:45:18, 20.58s/it]

 34%|███▎      | 3613/10740 [17:58:20<40:23:34, 20.40s/it]
[2024-04-02 13:12:25,219] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▎      | 3614/10740 [17:58:41<40:49:58, 20.63s/it]
[2024-04-02 13:12:44,745] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 34%|███▎      | 3616/10740 [17:59:18<38:11:37, 19.30s/it]
{'loss': 0.3758, 'learning_rate': 1.5460841355992015e-06, 'rewards/chosen': -1.1819595098495483, 'rewards/rejected': -2.367229461669922, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1852697134017944, 'policy_logps/rejected': -447.4612121582031, 'policy_logps/chosen': -480.71673583984375, 'referece_logps/rejected': -423.78887939453125, 'referece_logps/chosen': -468.8971252441406, 'logits/rejected': -1.0326581001281738, 'logits/chosen': -1.0631040334701538, 'epoch': 2.02}

 34%|███▎      | 3617/10740 [17:59:31<34:36:23, 17.49s/it]
[2024-04-02 13:13:37,276] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 34%|███▎      | 3619/10740 [18:00:14<38:09:29, 19.29s/it]

 34%|███▎      | 3620/10740 [18:00:32<37:41:13, 19.06s/it]

 34%|███▎      | 3621/10740 [18:00:52<37:59:47, 19.21s/it]
{'loss': 0.4931, 'learning_rate': 1.5448202880690238e-06, 'rewards/chosen': -2.1308677196502686, 'rewards/rejected': -2.7412216663360596, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6103543043136597, 'policy_logps/rejected': -278.08551025390625, 'policy_logps/chosen': -276.57843017578125, 'referece_logps/rejected': -250.67330932617188, 'referece_logps/chosen': -255.26976013183594, 'logits/rejected': -1.3057677745819092, 'logits/chosen': -1.3814860582351685, 'epoch': 2.02}


 34%|███▎      | 3623/10740 [18:01:22<33:51:12, 17.12s/it]
{'loss': 0.4278, 'learning_rate': 1.544314402026802e-06, 'rewards/chosen': -1.0041781663894653, 'rewards/rejected': -3.5786192417144775, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5744409561157227, 'policy_logps/rejected': -309.7757568359375, 'policy_logps/chosen': -341.6694030761719, 'referece_logps/rejected': -273.98956298828125, 'referece_logps/chosen': -331.6275939941406, 'logits/rejected': -0.44755107164382935, 'logits/chosen': -0.4825461208820343, 'epoch': 2.02}

 34%|███▎      | 3624/10740 [18:01:33<30:23:38, 15.38s/it]

 34%|███▍      | 3625/10740 [18:01:49<30:35:33, 15.48s/it]


 34%|███▍      | 3627/10740 [18:02:24<32:05:58, 16.25s/it]
{'loss': 0.429, 'learning_rate': 1.5433020360457327e-06, 'rewards/chosen': -1.954909086227417, 'rewards/rejected': -3.677877187728882, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7229679822921753, 'policy_logps/rejected': -293.2958068847656, 'policy_logps/chosen': -276.9588623046875, 'referece_logps/rejected': -256.51702880859375, 'referece_logps/chosen': -257.4097900390625, 'logits/rejected': -0.5826845169067383, 'logits/chosen': -0.6782790422439575, 'epoch': 2.03}


 34%|███▍      | 3629/10740 [18:02:50<28:26:39, 14.40s/it]
{'loss': 0.5018, 'learning_rate': 1.5427955564751942e-06, 'rewards/chosen': -1.3068372011184692, 'rewards/rejected': -2.964707136154175, 'rewards/accuracies': 1.0, 'rewards/margins': 1.657869815826416, 'policy_logps/rejected': -563.9622802734375, 'policy_logps/chosen': -371.6145324707031, 'referece_logps/rejected': -534.3151245117188, 'referece_logps/chosen': -358.54620361328125, 'logits/rejected': -0.528456449508667, 'logits/chosen': -0.5374954342842102, 'epoch': 2.03}

 34%|███▍      | 3630/10740 [18:03:05<28:36:34, 14.49s/it]

 34%|███▍      | 3631/10740 [18:03:16<26:39:56, 13.50s/it]

 34%|███▍      | 3632/10740 [18:03:27<25:31:27, 12.93s/it]

 34%|███▍      | 3633/10740 [18:03:45<28:28:20, 14.42s/it]


 34%|███▍      | 3635/10740 [18:04:24<32:55:48, 16.69s/it]
{'loss': 0.4658, 'learning_rate': 1.5412749336549248e-06, 'rewards/chosen': -2.2048161029815674, 'rewards/rejected': -3.3428502082824707, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1380341053009033, 'policy_logps/rejected': -530.2987060546875, 'policy_logps/chosen': -358.97265625, 'referece_logps/rejected': -496.8702392578125, 'referece_logps/chosen': -336.9244384765625, 'logits/rejected': 0.0031281039118766785, 'logits/chosen': 0.020650628954172134, 'epoch': 2.03}

 34%|███▍      | 3636/10740 [18:04:43<34:25:00, 17.44s/it]

 34%|███▍      | 3637/10740 [18:05:03<36:06:37, 18.30s/it]


 34%|███▍      | 3639/10740 [18:05:34<33:05:53, 16.78s/it]
{'loss': 0.5204, 'learning_rate': 1.5402602001951053e-06, 'rewards/chosen': -1.8563438653945923, 'rewards/rejected': -2.2022452354431152, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3459014892578125, 'policy_logps/rejected': -247.1912078857422, 'policy_logps/chosen': -283.83013916015625, 'referece_logps/rejected': -225.1687774658203, 'referece_logps/chosen': -265.26666259765625, 'logits/rejected': -0.49297261238098145, 'logits/chosen': -0.6654393672943115, 'epoch': 2.03}


 34%|███▍      | 3641/10740 [18:06:08<32:56:20, 16.70s/it]

 34%|███▍      | 3642/10740 [18:06:27<33:54:41, 17.20s/it]
{'loss': 0.4576, 'learning_rate': 1.5394986340706035e-06, 'rewards/chosen': -1.7789570093154907, 'rewards/rejected': -3.664095640182495, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8851386308670044, 'policy_logps/rejected': -388.556396484375, 'policy_logps/chosen': -409.60498046875, 'referece_logps/rejected': -351.9154052734375, 'referece_logps/chosen': -391.8154296875, 'logits/rejected': -0.6188966035842896, 'logits/chosen': -0.5765265226364136, 'epoch': 2.03}

 34%|███▍      | 3643/10740 [18:06:46<35:12:43, 17.86s/it]


 34%|███▍      | 3645/10740 [18:07:26<37:12:06, 18.88s/it]
{'loss': 0.3654, 'learning_rate': 1.5387366263276138e-06, 'rewards/chosen': -0.597615659236908, 'rewards/rejected': -3.3733010292053223, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7756853103637695, 'policy_logps/rejected': -450.967041015625, 'policy_logps/chosen': -263.6921081542969, 'referece_logps/rejected': -417.2340087890625, 'referece_logps/chosen': -257.7159729003906, 'logits/rejected': -0.2266463041305542, 'logits/chosen': -0.2683590054512024, 'epoch': 2.04}

 34%|███▍      | 3646/10740 [18:07:42<35:10:04, 17.85s/it]


 34%|███▍      | 3648/10740 [18:08:19<35:55:51, 18.24s/it]
{'loss': 0.4251, 'learning_rate': 1.5379741775898947e-06, 'rewards/chosen': -1.0245013236999512, 'rewards/rejected': -2.79685378074646, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7723522186279297, 'policy_logps/rejected': -397.8876647949219, 'policy_logps/chosen': -480.9297790527344, 'referece_logps/rejected': -369.91912841796875, 'referece_logps/chosen': -470.6847839355469, 'logits/rejected': 0.02621825598180294, 'logits/chosen': -0.08560025691986084, 'epoch': 2.04}

 34%|███▍      | 3649/10740 [18:08:39<37:20:39, 18.96s/it]


 34%|███▍      | 3651/10740 [18:09:19<37:46:32, 19.18s/it]

 34%|███▍      | 3652/10740 [18:09:41<39:24:28, 20.02s/it]
{'loss': 0.351, 'learning_rate': 1.5369568943599102e-06, 'rewards/chosen': -0.8060638904571533, 'rewards/rejected': -2.264639139175415, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4585752487182617, 'policy_logps/rejected': -328.46136474609375, 'policy_logps/chosen': -285.46417236328125, 'referece_logps/rejected': -305.8149719238281, 'referece_logps/chosen': -277.4035339355469, 'logits/rejected': -0.8932666182518005, 'logits/chosen': -0.8433367013931274, 'epoch': 2.04}

 34%|███▍      | 3653/10740 [18:10:01<39:50:57, 20.24s/it]

 34%|███▍      | 3654/10740 [18:10:22<39:54:52, 20.28s/it]

 34%|███▍      | 3655/10740 [18:10:41<39:27:06, 20.05s/it]


 34%|███▍      | 3657/10740 [18:11:20<38:57:51, 19.80s/it]
{'loss': 0.3774, 'learning_rate': 1.535684191651353e-06, 'rewards/chosen': -0.9283153414726257, 'rewards/rejected': -1.9524562358856201, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0241410732269287, 'policy_logps/rejected': -256.6923828125, 'policy_logps/chosen': -300.7891540527344, 'referece_logps/rejected': -237.16783142089844, 'referece_logps/chosen': -291.5060119628906, 'logits/rejected': -0.5857011079788208, 'logits/chosen': -0.6313522458076477, 'epoch': 2.04}

 34%|███▍      | 3658/10740 [18:11:40<38:50:17, 19.74s/it]

 34%|███▍      | 3659/10740 [18:11:58<37:49:51, 19.23s/it]

 34%|███▍      | 3660/10740 [18:12:20<39:15:28, 19.96s/it]
[2024-04-02 13:26:26,893] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 34%|███▍      | 3662/10740 [18:12:57<37:00:52, 18.83s/it]
[2024-04-02 13:26:40,673] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▍      | 3663/10740 [18:13:17<37:36:10, 19.13s/it]
{'loss': 0.4498, 'learning_rate': 1.534155340837132e-06, 'rewards/chosen': -0.8929807543754578, 'rewards/rejected': -1.865135669708252, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9721549153327942, 'policy_logps/rejected': -491.8757629394531, 'policy_logps/chosen': -409.4686279296875, 'referece_logps/rejected': -473.224365234375, 'referece_logps/chosen': -400.53875732421875, 'logits/rejected': -0.11295226216316223, 'logits/chosen': -0.06702315807342529, 'epoch': 2.05}

 34%|███▍      | 3664/10740 [18:13:31<34:55:13, 17.77s/it]

 34%|███▍      | 3665/10740 [18:13:45<32:42:29, 16.64s/it]

 34%|███▍      | 3666/10740 [18:13:58<30:13:08, 15.38s/it]


 34%|███▍      | 3668/10740 [18:14:37<34:37:04, 17.62s/it]

 34%|███▍      | 3669/10740 [18:14:59<37:18:12, 18.99s/it]
{'loss': 0.4233, 'learning_rate': 1.532624741044799e-06, 'rewards/chosen': -1.5641655921936035, 'rewards/rejected': -3.206982374191284, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6428170204162598, 'policy_logps/rejected': -272.5792541503906, 'policy_logps/chosen': -337.404296875, 'referece_logps/rejected': -240.50942993164062, 'referece_logps/chosen': -321.76263427734375, 'logits/rejected': -0.4529193043708801, 'logits/chosen': -0.5097190141677856, 'epoch': 2.05}

 34%|███▍      | 3670/10740 [18:15:16<36:04:38, 18.37s/it]

 34%|███▍      | 3671/10740 [18:15:30<33:44:18, 17.18s/it]

 34%|███▍      | 3672/10740 [18:15:51<35:57:36, 18.32s/it]

 34%|███▍      | 3673/10740 [18:16:13<38:12:51, 19.47s/it]
[2024-04-02 13:30:17,319] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 34%|███▍      | 3675/10740 [18:16:49<36:02:14, 18.36s/it]

 34%|███▍      | 3676/10740 [18:17:03<33:41:11, 17.17s/it]

 34%|███▍      | 3677/10740 [18:17:23<35:04:53, 17.88s/it]
{'loss': 0.5401, 'learning_rate': 1.530581229351958e-06, 'rewards/chosen': -1.821166753768921, 'rewards/rejected': -2.121697187423706, 'rewards/accuracies': 0.5, 'rewards/margins': 0.30053043365478516, 'policy_logps/rejected': -305.13189697265625, 'policy_logps/chosen': -328.5343933105469, 'referece_logps/rejected': -283.9149169921875, 'referece_logps/chosen': -310.32269287109375, 'logits/rejected': -0.4199630618095398, 'logits/chosen': -0.48423025012016296, 'epoch': 2.05}

 34%|███▍      | 3678/10740 [18:17:36<32:16:39, 16.45s/it]
[2024-04-02 13:31:37,650] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▍      | 3679/10740 [18:17:54<33:11:10, 16.92s/it]

 34%|███▍      | 3680/10740 [18:18:12<33:48:43, 17.24s/it]

 34%|███▍      | 3681/10740 [18:18:30<34:33:15, 17.62s/it]

 34%|███▍      | 3682/10740 [18:18:45<32:30:44, 16.58s/it]

 34%|███▍      | 3683/10740 [18:19:00<31:45:04, 16.20s/it]
[2024-04-02 13:33:03,509] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▍      | 3684/10740 [18:19:20<33:55:29, 17.31s/it]


 34%|███▍      | 3686/10740 [18:19:55<34:54:56, 17.82s/it]

 34%|███▍      | 3687/10740 [18:20:15<36:14:46, 18.50s/it]
{'loss': 0.4434, 'learning_rate': 1.528022497943899e-06, 'rewards/chosen': -1.673504114151001, 'rewards/rejected': -3.193809747695923, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5203053951263428, 'policy_logps/rejected': -438.3415222167969, 'policy_logps/chosen': -417.467529296875, 'referece_logps/rejected': -406.40338134765625, 'referece_logps/chosen': -400.7325134277344, 'logits/rejected': -0.2907819449901581, 'logits/chosen': -0.3796162009239197, 'epoch': 2.06}

 34%|███▍      | 3688/10740 [18:20:37<37:57:04, 19.37s/it]


 34%|███▍      | 3690/10740 [18:21:09<34:15:15, 17.49s/it]
{'loss': 0.4058, 'learning_rate': 1.5272539409757989e-06, 'rewards/chosen': -1.4748731851577759, 'rewards/rejected': -2.7703745365142822, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2955012321472168, 'policy_logps/rejected': -255.18399047851562, 'policy_logps/chosen': -321.18707275390625, 'referece_logps/rejected': -227.48025512695312, 'referece_logps/chosen': -306.4383239746094, 'logits/rejected': -1.1872605085372925, 'logits/chosen': -1.225932240486145, 'epoch': 2.06}


 34%|███▍      | 3692/10740 [18:21:51<37:39:18, 19.23s/it]
{'loss': 0.2985, 'learning_rate': 1.526741329849703e-06, 'rewards/chosen': -0.8241364359855652, 'rewards/rejected': -2.047067642211914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2229313850402832, 'policy_logps/rejected': -358.3385009765625, 'policy_logps/chosen': -339.759033203125, 'referece_logps/rejected': -337.8677978515625, 'referece_logps/chosen': -331.5176696777344, 'logits/rejected': 0.10750345885753632, 'logits/chosen': 0.0168512761592865, 'epoch': 2.06}

 34%|███▍      | 3693/10740 [18:22:04<34:01:20, 17.38s/it]

 34%|███▍      | 3694/10740 [18:22:16<30:52:06, 15.77s/it]

 34%|███▍      | 3695/10740 [18:22:32<30:58:33, 15.83s/it]

 34%|███▍      | 3696/10740 [18:22:53<33:43:33, 17.24s/it]

 34%|███▍      | 3697/10740 [18:23:08<32:49:34, 16.78s/it]


 34%|███▍      | 3699/10740 [18:23:53<38:16:04, 19.57s/it]
{'loss': 0.3439, 'learning_rate': 1.524945683017794e-06, 'rewards/chosen': -2.440725326538086, 'rewards/rejected': -3.1733174324035645, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7325922250747681, 'policy_logps/rejected': -424.0000305175781, 'policy_logps/chosen': -301.04473876953125, 'referece_logps/rejected': -392.2668151855469, 'referece_logps/chosen': -276.63751220703125, 'logits/rejected': -1.0422924757003784, 'logits/chosen': -1.0046741962432861, 'epoch': 2.07}

 34%|███▍      | 3700/10740 [18:24:12<37:31:22, 19.19s/it]
[2024-04-02 13:38:18,041] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▍      | 3701/10740 [18:24:34<39:36:07, 20.25s/it]
[2024-04-02 13:38:35,093] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▍      | 3702/10740 [18:24:51<37:43:09, 19.29s/it]

 34%|███▍      | 3703/10740 [18:25:14<39:39:10, 20.29s/it]

 34%|███▍      | 3704/10740 [18:25:36<40:38:59, 20.80s/it]

 34%|███▍      | 3705/10740 [18:25:54<39:15:50, 20.09s/it]

 35%|███▍      | 3706/10740 [18:26:14<39:05:20, 20.01s/it]

 35%|███▍      | 3707/10740 [18:26:35<39:47:10, 20.37s/it]

 35%|███▍      | 3708/10740 [18:26:51<37:10:15, 19.03s/it]
[2024-04-02 13:40:53,718] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 3709/10740 [18:27:10<36:57:01, 18.92s/it]

 35%|███▍      | 3710/10740 [18:27:30<37:38:30, 19.28s/it]

 35%|███▍      | 3711/10740 [18:27:51<38:40:36, 19.81s/it]

 35%|███▍      | 3712/10740 [18:28:04<34:42:42, 17.78s/it]

 35%|███▍      | 3713/10740 [18:28:24<35:45:23, 18.32s/it]

 35%|███▍      | 3714/10740 [18:28:35<31:30:35, 16.15s/it]

 35%|███▍      | 3715/10740 [18:28:49<30:16:42, 15.52s/it]

 35%|███▍      | 3716/10740 [18:29:08<32:36:17, 16.71s/it]
[2024-04-02 13:43:08,333] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 3717/10740 [18:29:25<32:18:28, 16.56s/it]
[2024-04-02 13:43:29,285] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 3718/10740 [18:29:46<34:52:20, 17.88s/it]

 35%|███▍      | 3719/10740 [18:30:05<35:54:50, 18.41s/it]

 35%|███▍      | 3720/10740 [18:30:22<34:51:44, 17.88s/it]

 35%|███▍      | 3721/10740 [18:30:45<38:10:30, 19.58s/it]

 35%|███▍      | 3722/10740 [18:30:59<34:30:02, 17.70s/it]

 35%|███▍      | 3723/10740 [18:31:11<31:16:40, 16.05s/it]

 35%|███▍      | 3724/10740 [18:31:30<33:18:07, 17.09s/it]
[2024-04-02 13:45:35,441] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 3725/10740 [18:31:52<35:45:27, 18.35s/it]

 35%|███▍      | 3726/10740 [18:32:11<36:32:03, 18.75s/it]

 35%|███▍      | 3727/10740 [18:32:31<36:44:57, 18.86s/it]

 35%|███▍      | 3728/10740 [18:32:53<39:05:53, 20.07s/it]

 35%|███▍      | 3729/10740 [18:33:15<39:51:33, 20.47s/it]

 35%|███▍      | 3730/10740 [18:33:30<36:29:27, 18.74s/it]

 35%|███▍      | 3731/10740 [18:33:44<34:17:13, 17.61s/it]

 35%|███▍      | 3732/10740 [18:33:58<31:57:26, 16.42s/it]

 35%|███▍      | 3733/10740 [18:34:16<32:46:25, 16.84s/it]

 35%|███▍      | 3734/10740 [18:34:33<33:06:22, 17.01s/it]
[2024-04-02 13:48:38,570] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 3735/10740 [18:34:55<35:42:35, 18.35s/it]

 35%|███▍      | 3736/10740 [18:35:13<35:43:27, 18.36s/it]

 35%|███▍      | 3737/10740 [18:35:31<35:35:16, 18.29s/it]

 35%|███▍      | 3738/10740 [18:35:42<31:21:18, 16.12s/it]

 35%|███▍      | 3739/10740 [18:36:05<34:53:58, 17.95s/it]

 35%|███▍      | 3740/10740 [18:36:25<36:28:21, 18.76s/it]

 35%|███▍      | 3741/10740 [18:36:41<34:59:02, 17.99s/it]

 35%|███▍      | 3742/10740 [18:36:52<30:42:14, 15.80s/it]

 35%|███▍      | 3743/10740 [18:37:14<34:12:39, 17.60s/it]

 35%|███▍      | 3744/10740 [18:37:33<35:14:54, 18.14s/it]

 35%|███▍      | 3745/10740 [18:37:55<37:30:18, 19.30s/it]

 35%|███▍      | 3746/10740 [18:38:15<37:46:35, 19.44s/it]

 35%|███▍      | 3747/10740 [18:38:35<37:48:53, 19.47s/it]

 35%|███▍      | 3748/10740 [18:38:58<40:20:49, 20.77s/it]

 35%|███▍      | 3749/10740 [18:39:19<40:13:27, 20.71s/it]

 35%|███▍      | 3750/10740 [18:39:39<39:45:27, 20.48s/it]

 35%|███▍      | 3751/10740 [18:39:57<38:06:02, 19.63s/it]

 35%|███▍      | 3752/10740 [18:40:16<38:05:37, 19.62s/it]

 35%|███▍      | 3753/10740 [18:40:36<38:25:45, 19.80s/it]

 35%|███▍      | 3754/10740 [18:40:57<39:01:35, 20.11s/it]

 35%|███▍      | 3755/10740 [18:41:13<36:44:22, 18.94s/it]

 35%|███▍      | 3756/10740 [18:41:34<37:33:40, 19.36s/it]

 35%|███▍      | 3757/10740 [18:41:50<35:58:58, 18.55s/it]

 35%|███▍      | 3758/10740 [18:42:10<36:17:49, 18.72s/it]

 35%|███▌      | 3759/10740 [18:42:28<36:14:59, 18.69s/it]

 35%|███▌      | 3760/10740 [18:42:47<36:30:47, 18.83s/it]

 35%|███▌      | 3761/10740 [18:42:59<32:16:51, 16.65s/it]

 35%|███▌      | 3762/10740 [18:43:13<31:02:43, 16.02s/it]

 35%|███▌      | 3763/10740 [18:43:31<31:43:55, 16.37s/it]

 35%|███▌      | 3764/10740 [18:43:44<30:11:10, 15.58s/it]

 35%|███▌      | 3765/10740 [18:44:02<31:12:09, 16.10s/it]

 35%|███▌      | 3766/10740 [18:44:21<33:11:30, 17.13s/it]
[2024-04-02 13:58:26,863] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 3767/10740 [18:44:43<35:55:19, 18.55s/it]
[2024-04-02 13:58:45,849] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 3768/10740 [18:45:02<36:10:22, 18.68s/it]

 35%|███▌      | 3769/10740 [18:45:21<36:30:33, 18.85s/it]

 35%|███▌      | 3770/10740 [18:45:33<32:34:32, 16.83s/it]

 35%|███▌      | 3771/10740 [18:45:44<28:57:39, 14.96s/it]

 35%|███▌      | 3772/10740 [18:45:55<26:33:41, 13.72s/it]


 35%|███▌      | 3774/10740 [18:46:35<32:43:29, 16.91s/it]

 35%|███▌      | 3775/10740 [18:46:51<32:33:45, 16.83s/it]

 35%|███▌      | 3776/10740 [18:47:04<29:56:02, 15.47s/it]

 35%|███▌      | 3777/10740 [18:47:25<33:05:30, 17.11s/it]
[2024-04-02 14:01:08,440] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 3778/10740 [18:47:44<34:20:18, 17.76s/it]
[2024-04-02 14:01:27,706] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 3779/10740 [18:47:56<31:15:39, 16.17s/it]

 35%|███▌      | 3780/10740 [18:48:13<31:28:45, 16.28s/it]

 35%|███▌      | 3781/10740 [18:48:27<30:10:23, 15.61s/it]

 35%|███▌      | 3782/10740 [18:48:41<29:10:34, 15.10s/it]

 35%|███▌      | 3783/10740 [18:49:00<31:23:47, 16.25s/it]

 35%|███▌      | 3784/10740 [18:49:18<32:39:39, 16.90s/it]

 35%|███▌      | 3785/10740 [18:49:31<30:22:14, 15.72s/it]

 35%|███▌      | 3786/10740 [18:49:42<27:37:08, 14.30s/it]

 35%|███▌      | 3787/10740 [18:50:04<32:11:41, 16.67s/it]

 35%|███▌      | 3788/10740 [18:50:18<30:35:07, 15.84s/it]

 35%|███▌      | 3789/10740 [18:50:35<30:57:05, 16.03s/it]

 35%|███▌      | 3790/10740 [18:50:56<33:40:52, 17.45s/it]
[2024-04-02 14:04:39,290] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 3791/10740 [18:51:11<32:29:23, 16.83s/it]

 35%|███▌      | 3792/10740 [18:51:28<32:28:56, 16.83s/it]

 35%|███▌      | 3793/10740 [18:51:41<30:26:56, 15.78s/it]
{'loss': 0.4409, 'learning_rate': 1.5006092932383506e-06, 'rewards/chosen': -1.1514917612075806, 'rewards/rejected': -1.6008844375610352, 'rewards/accuracies': 0.375, 'rewards/margins': 0.4493926167488098, 'policy_logps/rejected': -294.66357421875, 'policy_logps/chosen': -237.81378173828125, 'referece_logps/rejected': -278.6547546386719, 'referece_logps/chosen': -226.29885864257812, 'logits/rejected': -0.5648656487464905, 'logits/chosen': -0.6675618290901184, 'epoch': 2.12}


 35%|███▌      | 3795/10740 [18:52:19<33:56:23, 17.59s/it]

 35%|███▌      | 3796/10740 [18:52:39<35:11:43, 18.25s/it]

 35%|███▌      | 3797/10740 [18:52:58<36:00:52, 18.67s/it]

 35%|███▌      | 3798/10740 [18:53:14<34:04:19, 17.67s/it]

 35%|███▌      | 3799/10740 [18:53:32<34:19:10, 17.80s/it]

 35%|███▌      | 3800/10740 [18:53:51<34:56:20, 18.12s/it]

 35%|███▌      | 3801/10740 [18:54:08<34:40:22, 17.99s/it]

 35%|███▌      | 3802/10740 [18:54:23<32:45:40, 17.00s/it]

 35%|███▌      | 3803/10740 [18:54:40<32:35:46, 16.92s/it]

 35%|███▌      | 3804/10740 [18:55:01<35:08:43, 18.24s/it]

 35%|███▌      | 3805/10740 [18:55:23<37:26:02, 19.43s/it]
[2024-04-02 14:09:07,015] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 3806/10740 [18:55:41<36:10:09, 18.78s/it]

 35%|███▌      | 3807/10740 [18:56:00<36:36:31, 19.01s/it]

 35%|███▌      | 3808/10740 [18:56:18<35:59:58, 18.70s/it]

 35%|███▌      | 3809/10740 [18:56:41<38:22:40, 19.93s/it]

 35%|███▌      | 3810/10740 [18:57:01<38:22:20, 19.93s/it]

 35%|███▌      | 3811/10740 [18:57:18<36:57:01, 19.20s/it]

 35%|███▌      | 3812/10740 [18:57:30<32:22:24, 16.82s/it]

 36%|███▌      | 3813/10740 [18:57:51<35:01:03, 18.20s/it]
[2024-04-02 14:11:34,707] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3814/10740 [18:58:08<34:15:31, 17.81s/it]
{'loss': 0.37, 'learning_rate': 1.4951167661227757e-06, 'rewards/chosen': -1.0378283262252808, 'rewards/rejected': -2.912053346633911, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8742250204086304, 'policy_logps/rejected': -332.03271484375, 'policy_logps/chosen': -264.1851501464844, 'referece_logps/rejected': -302.91217041015625, 'referece_logps/chosen': -253.80686950683594, 'logits/rejected': -0.6651510000228882, 'logits/chosen': -0.5820238590240479, 'epoch': 2.13}

 36%|███▌      | 3815/10740 [18:58:26<34:26:23, 17.90s/it]


 36%|███▌      | 3817/10740 [18:59:05<36:27:53, 18.96s/it]

 36%|███▌      | 3818/10740 [18:59:26<37:16:28, 19.39s/it]

 36%|███▌      | 3819/10740 [18:59:43<36:02:47, 18.75s/it]

 36%|███▌      | 3820/10740 [19:00:05<38:05:59, 19.82s/it]

 36%|███▌      | 3821/10740 [19:00:25<38:04:37, 19.81s/it]
{'loss': 0.2867, 'learning_rate': 1.4932814997191158e-06, 'rewards/chosen': -0.5880798697471619, 'rewards/rejected': -2.9169609546661377, 'rewards/accuracies': 0.875, 'rewards/margins': 2.32888126373291, 'policy_logps/rejected': -299.5018310546875, 'policy_logps/chosen': -303.03887939453125, 'referece_logps/rejected': -270.33221435546875, 'referece_logps/chosen': -297.1580810546875, 'logits/rejected': -0.29054561257362366, 'logits/chosen': -0.4224735200405121, 'epoch': 2.13}

 36%|███▌      | 3822/10740 [19:00:41<35:29:57, 18.47s/it]


 36%|███▌      | 3824/10740 [19:01:21<36:58:55, 19.25s/it]

 36%|███▌      | 3825/10740 [19:01:32<32:05:52, 16.71s/it]

 36%|███▌      | 3826/10740 [19:01:43<28:58:50, 15.09s/it]

 36%|███▌      | 3827/10740 [19:02:05<33:04:21, 17.22s/it]

 36%|███▌      | 3828/10740 [19:02:20<31:39:34, 16.49s/it]
{'loss': 0.4949, 'learning_rate': 1.491444034923312e-06, 'rewards/chosen': -1.7801510095596313, 'rewards/rejected': -2.6427319049835205, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8625810146331787, 'policy_logps/rejected': -439.01812744140625, 'policy_logps/chosen': -420.5746765136719, 'referece_logps/rejected': -412.5908508300781, 'referece_logps/chosen': -402.7731018066406, 'logits/rejected': -0.0438004732131958, 'logits/chosen': -0.22321027517318726, 'epoch': 2.14}


 36%|███▌      | 3830/10740 [19:02:59<35:21:31, 18.42s/it]

 36%|███▌      | 3831/10740 [19:03:14<33:12:00, 17.30s/it]

 36%|███▌      | 3832/10740 [19:03:35<35:23:31, 18.44s/it]

 36%|███▌      | 3833/10740 [19:03:55<36:13:22, 18.88s/it]
{'loss': 0.3323, 'learning_rate': 1.4901302186505705e-06, 'rewards/chosen': -2.068943500518799, 'rewards/rejected': -3.3561267852783203, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2871829271316528, 'policy_logps/rejected': -416.46002197265625, 'policy_logps/chosen': -467.8894348144531, 'referece_logps/rejected': -382.8988037109375, 'referece_logps/chosen': -447.20001220703125, 'logits/rejected': -0.7570345997810364, 'logits/chosen': -0.7055217623710632, 'epoch': 2.14}


 36%|███▌      | 3835/10740 [19:04:30<34:25:08, 17.94s/it]

 36%|███▌      | 3836/10740 [19:04:51<36:23:59, 18.98s/it]

 36%|███▌      | 3837/10740 [19:05:11<36:56:25, 19.26s/it]
{'loss': 0.3631, 'learning_rate': 1.4890783630752267e-06, 'rewards/chosen': -0.9243053197860718, 'rewards/rejected': -3.3427059650421143, 'rewards/accuracies': 1.0, 'rewards/margins': 2.418400287628174, 'policy_logps/rejected': -393.94476318359375, 'policy_logps/chosen': -388.07513427734375, 'referece_logps/rejected': -360.51763916015625, 'referece_logps/chosen': -378.83209228515625, 'logits/rejected': -0.1250210851430893, 'logits/chosen': -0.17510005831718445, 'epoch': 2.14}


 36%|███▌      | 3839/10740 [19:05:50<36:56:23, 19.27s/it]
{'loss': 0.5145, 'learning_rate': 1.488552168294617e-06, 'rewards/chosen': -1.7356336116790771, 'rewards/rejected': -3.5319926738739014, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7963590621948242, 'policy_logps/rejected': -506.62054443359375, 'policy_logps/chosen': -517.943359375, 'referece_logps/rejected': -471.3006286621094, 'referece_logps/chosen': -500.5870361328125, 'logits/rejected': 1.2716786861419678, 'logits/chosen': 1.3048466444015503, 'epoch': 2.14}

 36%|███▌      | 3840/10740 [19:06:09<36:32:53, 19.07s/it]

 36%|███▌      | 3841/10740 [19:06:27<36:10:53, 18.88s/it]


 36%|███▌      | 3843/10740 [19:07:09<37:46:40, 19.72s/it]

 36%|███▌      | 3844/10740 [19:07:28<37:24:18, 19.53s/it]

 36%|███▌      | 3845/10740 [19:07:52<39:38:05, 20.69s/it]

 36%|███▌      | 3846/10740 [19:08:02<33:48:47, 17.66s/it]
{'loss': 0.4386, 'learning_rate': 1.486709088116326e-06, 'rewards/chosen': -1.1802765130996704, 'rewards/rejected': -1.7731634378433228, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5928869843482971, 'policy_logps/rejected': -411.4460144042969, 'policy_logps/chosen': -361.0980224609375, 'referece_logps/rejected': -393.7143859863281, 'referece_logps/chosen': -349.2952575683594, 'logits/rejected': 0.459710955619812, 'logits/chosen': 0.45305752754211426, 'epoch': 2.15}


 36%|███▌      | 3848/10740 [19:08:38<34:34:21, 18.06s/it]
[2024-04-02 14:22:21,589] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3849/10740 [19:09:00<36:40:58, 19.16s/it]
[2024-04-02 14:22:43,331] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3850/10740 [19:09:21<37:43:11, 19.71s/it]
[2024-04-02 14:23:04,311] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3782, 'learning_rate': 1.4856549250336534e-06, 'rewards/chosen': -1.0305378437042236, 'rewards/rejected': -3.1930580139160156, 'rewards/accuracies': 1.0, 'rewards/margins': 2.162520408630371, 'policy_logps/rejected': -495.7125549316406, 'policy_logps/chosen': -391.7809143066406, 'referece_logps/rejected': -463.781982421875, 'referece_logps/chosen': -381.47552490234375, 'logits/rejected': -0.17914097011089325, 'logits/chosen': -0.05574595183134079, 'epoch': 2.15}
[2024-04-02 14:23:28,429] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3851/10740 [19:09:45<40:14:46, 21.03s/it]


 36%|███▌      | 3853/10740 [19:10:18<35:44:43, 18.68s/it]

 36%|███▌      | 3854/10740 [19:10:33<34:08:52, 17.85s/it]

 36%|███▌      | 3855/10740 [19:10:50<33:16:20, 17.40s/it]
{'loss': 0.5, 'learning_rate': 1.4843362275014002e-06, 'rewards/chosen': -1.96462881565094, 'rewards/rejected': -2.1337130069732666, 'rewards/accuracies': 0.625, 'rewards/margins': 0.16908405721187592, 'policy_logps/rejected': -312.9914245605469, 'policy_logps/chosen': -366.25921630859375, 'referece_logps/rejected': -291.654296875, 'referece_logps/chosen': -346.6129150390625, 'logits/rejected': -0.19016696512699127, 'logits/chosen': -0.1322447657585144, 'epoch': 2.15}

 36%|███▌      | 3856/10740 [19:11:11<35:28:00, 18.55s/it]
[2024-04-02 14:25:11,031] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3857/10740 [19:11:27<34:09:11, 17.86s/it]


 36%|███▌      | 3859/10740 [19:12:09<37:28:27, 19.61s/it]
[2024-04-02 14:25:53,153] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3860/10740 [19:12:32<39:09:00, 20.49s/it]
[2024-04-02 14:26:15,690] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3818, 'learning_rate': 1.4830164286801549e-06, 'rewards/chosen': -0.6982946395874023, 'rewards/rejected': -2.2692389488220215, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5709443092346191, 'policy_logps/rejected': -299.7127380371094, 'policy_logps/chosen': -369.67340087890625, 'referece_logps/rejected': -277.02032470703125, 'referece_logps/chosen': -362.6904602050781, 'logits/rejected': -0.6911348104476929, 'logits/chosen': -0.6451210379600525, 'epoch': 2.16}


 36%|███▌      | 3862/10740 [19:13:08<37:32:43, 19.65s/it]
{'loss': 0.4756, 'learning_rate': 1.4824882014628735e-06, 'rewards/chosen': -1.7039989233016968, 'rewards/rejected': -2.359464168548584, 'rewards/accuracies': 0.375, 'rewards/margins': 0.6554651260375977, 'policy_logps/rejected': -379.11029052734375, 'policy_logps/chosen': -497.4292907714844, 'referece_logps/rejected': -355.5156555175781, 'referece_logps/chosen': -480.3893127441406, 'logits/rejected': -0.10337270796298981, 'logits/chosen': -0.06791182607412338, 'epoch': 2.16}
[2024-04-02 14:27:10,725] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 36%|███▌      | 3864/10740 [19:13:48<38:13:03, 20.01s/it]

 36%|███▌      | 3865/10740 [19:14:08<38:10:21, 19.99s/it]

 36%|███▌      | 3866/10740 [19:14:28<38:17:02, 20.05s/it]

 36%|███▌      | 3867/10740 [19:14:48<37:45:01, 19.77s/it]

 36%|███▌      | 3868/10740 [19:15:02<34:31:07, 18.08s/it]
{'loss': 0.3734, 'learning_rate': 1.4809024673764236e-06, 'rewards/chosen': -0.5366342663764954, 'rewards/rejected': -2.874558210372925, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3379242420196533, 'policy_logps/rejected': -493.0101318359375, 'policy_logps/chosen': -465.67822265625, 'referece_logps/rejected': -464.2645263671875, 'referece_logps/chosen': -460.3118591308594, 'logits/rejected': -0.6337233185768127, 'logits/chosen': -0.5998319983482361, 'epoch': 2.16}


 36%|███▌      | 3870/10740 [19:15:36<33:53:58, 17.76s/it]

 36%|███▌      | 3871/10740 [19:15:58<36:33:58, 19.16s/it]
[2024-04-02 14:29:42,226] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3872/10740 [19:16:19<37:20:16, 19.57s/it]
{'loss': 0.2768, 'learning_rate': 1.479844436213132e-06, 'rewards/chosen': -1.5222845077514648, 'rewards/rejected': -3.16056489944458, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6382802724838257, 'policy_logps/rejected': -237.87913513183594, 'policy_logps/chosen': -166.4303741455078, 'referece_logps/rejected': -206.27349853515625, 'referece_logps/chosen': -151.20755004882812, 'logits/rejected': -0.30612704157829285, 'logits/chosen': -0.4679371118545532, 'epoch': 2.16}
[2024-04-02 14:30:19,201] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3873/10740 [19:16:35<35:32:57, 18.64s/it]


 36%|███▌      | 3875/10740 [19:17:05<32:11:58, 16.89s/it]

 36%|███▌      | 3876/10740 [19:17:16<29:05:27, 15.26s/it]

 36%|███▌      | 3877/10740 [19:17:36<31:47:43, 16.68s/it]

 36%|███▌      | 3878/10740 [19:17:51<30:34:01, 16.04s/it]
{'loss': 0.462, 'learning_rate': 1.4782560806589252e-06, 'rewards/chosen': -1.317150354385376, 'rewards/rejected': -2.4861276149749756, 'rewards/accuracies': 0.75, 'rewards/margins': 1.16897714138031, 'policy_logps/rejected': -340.6368713378906, 'policy_logps/chosen': -399.5522766113281, 'referece_logps/rejected': -315.77557373046875, 'referece_logps/chosen': -386.3807678222656, 'logits/rejected': -0.425555557012558, 'logits/chosen': -0.43585890531539917, 'epoch': 2.17}


 36%|███▌      | 3880/10740 [19:18:25<31:40:30, 16.62s/it]

 36%|███▌      | 3881/10740 [19:18:40<31:00:39, 16.28s/it]

 36%|███▌      | 3882/10740 [19:19:04<35:15:14, 18.51s/it]

 36%|███▌      | 3883/10740 [19:19:19<33:13:33, 17.44s/it]
{'loss': 0.3952, 'learning_rate': 1.476931254599379e-06, 'rewards/chosen': -1.7008494138717651, 'rewards/rejected': -3.7938127517700195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.092963457107544, 'policy_logps/rejected': -398.9874267578125, 'policy_logps/chosen': -431.540771484375, 'referece_logps/rejected': -361.0492858886719, 'referece_logps/chosen': -414.53228759765625, 'logits/rejected': -0.6343512535095215, 'logits/chosen': -0.6538041234016418, 'epoch': 2.17}


 36%|███▌      | 3885/10740 [19:19:51<32:26:32, 17.04s/it]
[2024-04-02 14:33:34,925] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4261, 'learning_rate': 1.4764010203603673e-06, 'rewards/chosen': -1.6210548877716064, 'rewards/rejected': -3.9628193378448486, 'rewards/accuracies': 0.875, 'rewards/margins': 2.341764450073242, 'policy_logps/rejected': -538.0496215820312, 'policy_logps/chosen': -411.6427001953125, 'referece_logps/rejected': -498.42138671875, 'referece_logps/chosen': -395.43218994140625, 'logits/rejected': 0.19213980436325073, 'logits/chosen': 0.04750567674636841, 'epoch': 2.17}


 36%|███▌      | 3887/10740 [19:20:21<30:46:35, 16.17s/it]
{'loss': 0.4601, 'learning_rate': 1.4758706128019952e-06, 'rewards/chosen': -0.957727313041687, 'rewards/rejected': -2.430126190185547, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4723989963531494, 'policy_logps/rejected': -419.1652526855469, 'policy_logps/chosen': -432.1468505859375, 'referece_logps/rejected': -394.8639831542969, 'referece_logps/chosen': -422.5696105957031, 'logits/rejected': -0.19191928207874298, 'logits/chosen': -0.21181261539459229, 'epoch': 2.17}


 36%|███▌      | 3889/10740 [19:20:59<33:21:51, 17.53s/it]

 36%|███▌      | 3890/10740 [19:21:14<32:12:51, 16.93s/it]

 36%|███▌      | 3891/10740 [19:21:28<30:43:49, 16.15s/it]
{'loss': 0.4359, 'learning_rate': 1.4748092784991017e-06, 'rewards/chosen': -1.3522840738296509, 'rewards/rejected': -2.6116607189178467, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2593765258789062, 'policy_logps/rejected': -420.4617919921875, 'policy_logps/chosen': -497.2528076171875, 'referece_logps/rejected': -394.3451843261719, 'referece_logps/chosen': -483.72998046875, 'logits/rejected': -0.48697033524513245, 'logits/chosen': -0.5394290089607239, 'epoch': 2.17}


 36%|███▌      | 3893/10740 [19:21:57<29:06:57, 15.31s/it]

 36%|███▋      | 3894/10740 [19:22:19<32:43:03, 17.20s/it]
{'loss': 0.3826, 'learning_rate': 1.4740128242442637e-06, 'rewards/chosen': -0.8048580884933472, 'rewards/rejected': -2.43953537940979, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6346772909164429, 'policy_logps/rejected': -387.5948181152344, 'policy_logps/chosen': -481.6055908203125, 'referece_logps/rejected': -363.199462890625, 'referece_logps/chosen': -473.5570373535156, 'logits/rejected': 0.16328737139701843, 'logits/chosen': 0.05792364478111267, 'epoch': 2.18}


 36%|███▋      | 3896/10740 [19:23:01<36:22:23, 19.13s/it]

 36%|███▋      | 3897/10740 [19:23:19<36:08:35, 19.01s/it]

 36%|███▋      | 3898/10740 [19:23:41<37:41:35, 19.83s/it]
[2024-04-02 14:37:24,895] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4762, 'learning_rate': 1.472950281773775e-06, 'rewards/chosen': -1.8959336280822754, 'rewards/rejected': -3.402268409729004, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5063350200653076, 'policy_logps/rejected': -411.06341552734375, 'policy_logps/chosen': -394.92474365234375, 'referece_logps/rejected': -377.04071044921875, 'referece_logps/chosen': -375.96539306640625, 'logits/rejected': -0.1437043845653534, 'logits/chosen': -0.20199096202850342, 'epoch': 2.18}
[2024-04-02 14:37:43,805] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▋      | 3899/10740 [19:24:00<37:09:42, 19.56s/it]


 36%|███▋      | 3901/10740 [19:24:38<36:55:28, 19.44s/it]
{'loss': 0.3729, 'learning_rate': 1.4721529231684874e-06, 'rewards/chosen': -0.21473178267478943, 'rewards/rejected': -2.475886344909668, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2611546516418457, 'policy_logps/rejected': -274.9250183105469, 'policy_logps/chosen': -331.2344665527344, 'referece_logps/rejected': -250.16615295410156, 'referece_logps/chosen': -329.087158203125, 'logits/rejected': -0.6129165291786194, 'logits/chosen': -0.6755623817443848, 'epoch': 2.18}


 36%|███▋      | 3903/10740 [19:25:15<35:44:16, 18.82s/it]

 36%|███▋      | 3904/10740 [19:25:34<35:53:27, 18.90s/it]

 36%|███▋      | 3905/10740 [19:25:53<35:57:42, 18.94s/it]
{'loss': 0.4606, 'learning_rate': 1.4710891772658032e-06, 'rewards/chosen': -0.5680389404296875, 'rewards/rejected': -2.438638210296631, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8705997467041016, 'policy_logps/rejected': -338.1819152832031, 'policy_logps/chosen': -270.9520568847656, 'referece_logps/rejected': -313.7955322265625, 'referece_logps/chosen': -265.27166748046875, 'logits/rejected': -0.638329029083252, 'logits/chosen': -0.6895221471786499, 'epoch': 2.18}


 36%|███▋      | 3907/10740 [19:26:23<32:17:46, 17.02s/it]

 36%|███▋      | 3908/10740 [19:26:37<30:06:13, 15.86s/it]
{'loss': 0.4161, 'learning_rate': 1.4702909178636292e-06, 'rewards/chosen': -0.5652451515197754, 'rewards/rejected': -2.4556007385253906, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8903554677963257, 'policy_logps/rejected': -512.1077880859375, 'policy_logps/chosen': -408.1780700683594, 'referece_logps/rejected': -487.55181884765625, 'referece_logps/chosen': -402.5256652832031, 'logits/rejected': -0.26940691471099854, 'logits/chosen': -0.16620315611362457, 'epoch': 2.18}


 36%|███▋      | 3910/10740 [19:27:09<29:49:22, 15.72s/it]
{'loss': 0.3962, 'learning_rate': 1.4697585310179492e-06, 'rewards/chosen': -0.8373771905899048, 'rewards/rejected': -1.738579273223877, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9012022018432617, 'policy_logps/rejected': -342.39471435546875, 'policy_logps/chosen': -254.8680419921875, 'referece_logps/rejected': -325.0089416503906, 'referece_logps/chosen': -246.4942626953125, 'logits/rejected': -1.2379474639892578, 'logits/chosen': -1.2615153789520264, 'epoch': 2.18}


 36%|███▋      | 3912/10740 [19:27:51<34:56:38, 18.42s/it]

 36%|███▋      | 3913/10740 [19:28:12<36:07:59, 19.05s/it]
{'loss': 0.4917, 'learning_rate': 1.4689596303673033e-06, 'rewards/chosen': -1.0720083713531494, 'rewards/rejected': -2.759094715118408, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6870864629745483, 'policy_logps/rejected': -340.80279541015625, 'policy_logps/chosen': -367.8566589355469, 'referece_logps/rejected': -313.21185302734375, 'referece_logps/chosen': -357.13653564453125, 'logits/rejected': -0.3664567172527313, 'logits/chosen': -0.28779155015945435, 'epoch': 2.19}


 36%|███▋      | 3915/10740 [19:28:41<31:21:01, 16.54s/it]
{'loss': 0.4418, 'learning_rate': 1.468426816628029e-06, 'rewards/chosen': -1.5653762817382812, 'rewards/rejected': -2.9865822792053223, 'rewards/accuracies': 0.75, 'rewards/margins': 1.421205759048462, 'policy_logps/rejected': -429.2939758300781, 'policy_logps/chosen': -480.3212585449219, 'referece_logps/rejected': -399.42816162109375, 'referece_logps/chosen': -464.66748046875, 'logits/rejected': -1.3938689231872559, 'logits/chosen': -1.4634485244750977, 'epoch': 2.19}

 36%|███▋      | 3916/10740 [19:28:56<30:22:40, 16.03s/it]


 36%|███▋      | 3918/10740 [19:29:30<30:57:31, 16.34s/it]
{'loss': 0.3438, 'learning_rate': 1.4676272765454596e-06, 'rewards/chosen': -1.144667148590088, 'rewards/rejected': -2.9755148887634277, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8308477401733398, 'policy_logps/rejected': -426.2622375488281, 'policy_logps/chosen': -491.03009033203125, 'referece_logps/rejected': -396.5071105957031, 'referece_logps/chosen': -479.58343505859375, 'logits/rejected': 0.008055388927459717, 'logits/chosen': -0.06776389479637146, 'epoch': 2.19}

 36%|███▋      | 3919/10740 [19:29:46<31:03:33, 16.39s/it]

 36%|███▋      | 3920/10740 [19:30:01<29:49:57, 15.75s/it]

 37%|███▋      | 3921/10740 [19:30:19<31:19:25, 16.54s/it]

 37%|███▋      | 3922/10740 [19:30:32<29:36:12, 15.63s/it]

 37%|███▋      | 3923/10740 [19:30:51<30:59:27, 16.37s/it]


 37%|███▋      | 3925/10740 [19:31:23<30:36:17, 16.17s/it]
{'loss': 0.5762, 'learning_rate': 1.4657601955366766e-06, 'rewards/chosen': -1.7117446660995483, 'rewards/rejected': -2.5112380981445312, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7994933128356934, 'policy_logps/rejected': -457.1119384765625, 'policy_logps/chosen': -472.60638427734375, 'referece_logps/rejected': -431.9995422363281, 'referece_logps/chosen': -455.4888610839844, 'logits/rejected': -1.2884663343429565, 'logits/chosen': -1.322803020477295, 'epoch': 2.19}

 37%|███▋      | 3926/10740 [19:31:45<33:50:35, 17.88s/it]


 37%|███▋      | 3928/10740 [19:32:09<28:09:16, 14.88s/it]

 37%|███▋      | 3929/10740 [19:32:30<31:23:54, 16.60s/it]

 37%|███▋      | 3930/10740 [19:32:50<33:17:14, 17.60s/it]

 37%|███▋      | 3931/10740 [19:33:12<35:39:10, 18.85s/it]
{'loss': 0.3824, 'learning_rate': 1.464158187952021e-06, 'rewards/chosen': -0.8844544887542725, 'rewards/rejected': -3.0990054607391357, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2145509719848633, 'policy_logps/rejected': -392.00347900390625, 'policy_logps/chosen': -257.51434326171875, 'referece_logps/rejected': -361.013427734375, 'referece_logps/chosen': -248.66976928710938, 'logits/rejected': 0.10145869106054306, 'logits/chosen': 0.05464858561754227, 'epoch': 2.2}

 37%|███▋      | 3932/10740 [19:33:27<33:43:12, 17.83s/it]

 37%|███▋      | 3933/10740 [19:33:45<33:38:05, 17.79s/it]


 37%|███▋      | 3935/10740 [19:34:26<36:05:14, 19.09s/it]
{'loss': 0.3569, 'learning_rate': 1.4630893382453002e-06, 'rewards/chosen': -0.751868724822998, 'rewards/rejected': -3.506150007247925, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7542812824249268, 'policy_logps/rejected': -622.7362060546875, 'policy_logps/chosen': -477.93597412109375, 'referece_logps/rejected': -587.6746826171875, 'referece_logps/chosen': -470.4172668457031, 'logits/rejected': 0.5608193874359131, 'logits/chosen': 0.5953071117401123, 'epoch': 2.2}

 37%|███▋      | 3936/10740 [19:34:48<38:12:15, 20.21s/it]
[2024-04-02 14:48:53,020] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 37%|███▋      | 3938/10740 [19:35:28<37:40:21, 19.94s/it]

 37%|███▋      | 3939/10740 [19:35:50<38:41:21, 20.48s/it]

 37%|███▋      | 3940/10740 [19:36:08<37:26:41, 19.82s/it]
{'loss': 0.381, 'learning_rate': 1.4617523286143805e-06, 'rewards/chosen': -1.4723472595214844, 'rewards/rejected': -3.381350517272949, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9090032577514648, 'policy_logps/rejected': -306.18243408203125, 'policy_logps/chosen': -245.37692260742188, 'referece_logps/rejected': -272.36895751953125, 'referece_logps/chosen': -230.65345764160156, 'logits/rejected': -0.6794453263282776, 'logits/chosen': -0.6399325132369995, 'epoch': 2.2}

 37%|███▋      | 3941/10740 [19:36:25<36:00:26, 19.07s/it]

 37%|███▋      | 3942/10740 [19:36:44<35:57:37, 19.04s/it]


 37%|███▋      | 3944/10740 [19:37:22<35:52:47, 19.01s/it]
{'loss': 0.4809, 'learning_rate': 1.460681964808656e-06, 'rewards/chosen': -1.6753989458084106, 'rewards/rejected': -3.0947072505950928, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4193081855773926, 'policy_logps/rejected': -430.10992431640625, 'policy_logps/chosen': -404.94744873046875, 'referece_logps/rejected': -399.162841796875, 'referece_logps/chosen': -388.1934814453125, 'logits/rejected': -0.757992684841156, 'logits/chosen': -0.6212245225906372, 'epoch': 2.2}

 37%|███▋      | 3945/10740 [19:37:35<32:30:28, 17.22s/it]

 37%|███▋      | 3946/10740 [19:37:46<29:07:10, 15.43s/it]

 37%|███▋      | 3947/10740 [19:38:07<32:00:59, 16.97s/it]

 37%|███▋      | 3948/10740 [19:38:25<32:20:55, 17.15s/it]


 37%|███▋      | 3950/10740 [19:38:56<30:41:27, 16.27s/it]
[2024-04-02 14:52:39,438] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4833, 'learning_rate': 1.4590751625826058e-06, 'rewards/chosen': -1.7298487424850464, 'rewards/rejected': -2.1216704845428467, 'rewards/accuracies': 0.5, 'rewards/margins': 0.39182186126708984, 'policy_logps/rejected': -405.5279541015625, 'policy_logps/chosen': -452.8690185546875, 'referece_logps/rejected': -384.31121826171875, 'referece_logps/chosen': -435.5705261230469, 'logits/rejected': 0.00871141254901886, 'logits/chosen': 0.06136903166770935, 'epoch': 2.21}


 37%|███▋      | 3952/10740 [19:39:26<29:20:05, 15.56s/it]
{'loss': 0.3521, 'learning_rate': 1.458539227548632e-06, 'rewards/chosen': -1.8947491645812988, 'rewards/rejected': -3.9138593673706055, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0191102027893066, 'policy_logps/rejected': -358.56982421875, 'policy_logps/chosen': -329.2400817871094, 'referece_logps/rejected': -319.43121337890625, 'referece_logps/chosen': -310.29254150390625, 'logits/rejected': -0.528192400932312, 'logits/chosen': -0.4811147153377533, 'epoch': 2.21}

 37%|███▋      | 3953/10740 [19:39:45<30:56:45, 16.41s/it]

 37%|███▋      | 3954/10740 [19:40:05<33:20:19, 17.69s/it]

 37%|███▋      | 3955/10740 [19:40:26<34:55:54, 18.53s/it]

 37%|███▋      | 3956/10740 [19:40:46<35:46:14, 18.98s/it]
[2024-04-02 14:54:44,625] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 37%|███▋      | 3958/10740 [19:41:20<34:21:05, 18.23s/it]

 37%|███▋      | 3959/10740 [19:41:41<35:39:59, 18.94s/it]
{'loss': 0.456, 'learning_rate': 1.4566621424939262e-06, 'rewards/chosen': -1.1158794164657593, 'rewards/rejected': -2.2336814403533936, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1178017854690552, 'policy_logps/rejected': -308.7167053222656, 'policy_logps/chosen': -514.41064453125, 'referece_logps/rejected': -286.3799133300781, 'referece_logps/chosen': -503.2518310546875, 'logits/rejected': -1.0321301221847534, 'logits/chosen': -1.2886687517166138, 'epoch': 2.21}
[2024-04-02 14:55:42,876] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 3960/10740 [19:41:59<35:26:06, 18.82s/it]

 37%|███▋      | 3961/10740 [19:42:12<31:53:17, 16.93s/it]


 37%|███▋      | 3963/10740 [19:42:50<34:01:52, 18.08s/it]

 37%|███▋      | 3964/10740 [19:43:08<33:53:39, 18.01s/it]
[2024-04-02 14:56:51,844] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4008, 'learning_rate': 1.4553201209309042e-06, 'rewards/chosen': -0.8151060342788696, 'rewards/rejected': -1.9736493825912476, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1585432291030884, 'policy_logps/rejected': -644.6671142578125, 'policy_logps/chosen': -487.9249572753906, 'referece_logps/rejected': -624.9307250976562, 'referece_logps/chosen': -479.77386474609375, 'logits/rejected': 0.5459665060043335, 'logits/chosen': 0.652391254901886, 'epoch': 2.21}

 37%|███▋      | 3965/10740 [19:43:28<34:52:16, 18.53s/it]


 37%|███▋      | 3967/10740 [19:43:57<31:04:18, 16.52s/it]
{'loss': 0.366, 'learning_rate': 1.4545144108480076e-06, 'rewards/chosen': -0.8994835615158081, 'rewards/rejected': -2.4032986164093018, 'rewards/accuracies': 1.0, 'rewards/margins': 1.503814935684204, 'policy_logps/rejected': -453.95489501953125, 'policy_logps/chosen': -470.4687194824219, 'referece_logps/rejected': -429.9219055175781, 'referece_logps/chosen': -461.473876953125, 'logits/rejected': -0.017285145819187164, 'logits/chosen': -0.004441708326339722, 'epoch': 2.22}


 37%|███▋      | 3969/10740 [19:44:28<31:09:45, 16.57s/it]

 37%|███▋      | 3970/10740 [19:44:51<34:24:46, 18.30s/it]
[2024-04-02 14:58:34,514] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.368, 'learning_rate': 1.4537083287123307e-06, 'rewards/chosen': -1.825121521949768, 'rewards/rejected': -3.7984800338745117, 'rewards/accuracies': 0.75, 'rewards/margins': 1.973358392715454, 'policy_logps/rejected': -427.15692138671875, 'policy_logps/chosen': -450.6121826171875, 'referece_logps/rejected': -389.17218017578125, 'referece_logps/chosen': -432.3609619140625, 'logits/rejected': -0.09398195147514343, 'logits/chosen': -0.19908010959625244, 'epoch': 2.22}


 37%|███▋      | 3972/10740 [19:45:30<35:34:50, 18.93s/it]
{'loss': 0.4298, 'learning_rate': 1.453170734251723e-06, 'rewards/chosen': -1.1031460762023926, 'rewards/rejected': -1.2697709798812866, 'rewards/accuracies': 0.625, 'rewards/margins': 0.1666249930858612, 'policy_logps/rejected': -237.28126525878906, 'policy_logps/chosen': -296.48944091796875, 'referece_logps/rejected': -224.58355712890625, 'referece_logps/chosen': -285.45794677734375, 'logits/rejected': -0.9560413956642151, 'logits/chosen': -1.0917056798934937, 'epoch': 2.22}


 37%|███▋      | 3974/10740 [19:46:09<35:47:20, 19.04s/it]

 37%|███▋      | 3975/10740 [19:46:28<36:10:36, 19.25s/it]
{'loss': 0.3735, 'learning_rate': 1.4523640334945314e-06, 'rewards/chosen': -1.3485811948776245, 'rewards/rejected': -2.2161738872528076, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8675926923751831, 'policy_logps/rejected': -381.007080078125, 'policy_logps/chosen': -300.6476745605469, 'referece_logps/rejected': -358.8453674316406, 'referece_logps/chosen': -287.161865234375, 'logits/rejected': -1.089918851852417, 'logits/chosen': -1.0596528053283691, 'epoch': 2.22}

 37%|███▋      | 3976/10740 [19:46:46<35:14:29, 18.76s/it]


 37%|███▋      | 3978/10740 [19:47:20<34:28:16, 18.35s/it]
{'loss': 0.3771, 'learning_rate': 1.4515569624447985e-06, 'rewards/chosen': -1.4981639385223389, 'rewards/rejected': -3.7168922424316406, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2187280654907227, 'policy_logps/rejected': -485.9573059082031, 'policy_logps/chosen': -259.0957946777344, 'referece_logps/rejected': -448.78839111328125, 'referece_logps/chosen': -244.1141357421875, 'logits/rejected': -1.1748710870742798, 'logits/chosen': -1.1088955402374268, 'epoch': 2.22}

 37%|███▋      | 3979/10740 [19:47:37<33:30:03, 17.84s/it]

 37%|███▋      | 3980/10740 [19:47:51<31:26:59, 16.75s/it]

 37%|███▋      | 3981/10740 [19:48:11<33:15:18, 17.71s/it]

 37%|███▋      | 3982/10740 [19:48:30<33:37:30, 17.91s/it]


 37%|███▋      | 3984/10740 [19:49:07<34:06:38, 18.18s/it]
{'loss': 0.3446, 'learning_rate': 1.4499417121105935e-06, 'rewards/chosen': -1.451073169708252, 'rewards/rejected': -2.737257719039917, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2861849069595337, 'policy_logps/rejected': -387.8171081542969, 'policy_logps/chosen': -436.7915344238281, 'referece_logps/rejected': -360.4444885253906, 'referece_logps/chosen': -422.28076171875, 'logits/rejected': -0.5965378880500793, 'logits/chosen': -0.6225066781044006, 'epoch': 2.23}

 37%|███▋      | 3985/10740 [19:49:22<32:20:28, 17.24s/it]


 37%|███▋      | 3987/10740 [19:49:53<30:23:35, 16.20s/it]

 37%|███▋      | 3988/10740 [19:50:09<30:30:21, 16.27s/it]

 37%|███▋      | 3989/10740 [19:50:27<31:22:13, 16.73s/it]

 37%|███▋      | 3990/10740 [19:50:46<33:03:37, 17.63s/it]
{'loss': 0.3612, 'learning_rate': 1.4483249885379013e-06, 'rewards/chosen': -1.3826452493667603, 'rewards/rejected': -2.231074333190918, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8484289646148682, 'policy_logps/rejected': -544.3770751953125, 'policy_logps/chosen': -456.6573486328125, 'referece_logps/rejected': -522.06640625, 'referece_logps/chosen': -442.83087158203125, 'logits/rejected': 0.13456794619560242, 'logits/chosen': 0.33219802379608154, 'epoch': 2.23}

 37%|███▋      | 3991/10740 [19:51:04<33:09:50, 17.69s/it]

 37%|███▋      | 3992/10740 [19:51:22<33:17:00, 17.76s/it]


 37%|███▋      | 3994/10740 [19:51:53<31:14:25, 16.67s/it]
{'loss': 0.3585, 'learning_rate': 1.4472463569708813e-06, 'rewards/chosen': -1.815215826034546, 'rewards/rejected': -3.5432167053222656, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7280008792877197, 'policy_logps/rejected': -270.19940185546875, 'policy_logps/chosen': -332.6971435546875, 'referece_logps/rejected': -234.7672119140625, 'referece_logps/chosen': -314.5449523925781, 'logits/rejected': -1.4373012781143188, 'logits/chosen': -1.2630882263183594, 'epoch': 2.23}

 37%|███▋      | 3995/10740 [19:52:13<32:57:05, 17.59s/it]


 37%|███▋      | 3997/10740 [19:52:55<36:10:33, 19.31s/it]

 37%|███▋      | 3998/10740 [19:53:15<36:24:50, 19.44s/it]
{'loss': 0.3519, 'learning_rate': 1.4461670745534871e-06, 'rewards/chosen': -1.1459671258926392, 'rewards/rejected': -2.56913685798645, 'rewards/accuracies': 0.875, 'rewards/margins': 1.423169732093811, 'policy_logps/rejected': -506.52642822265625, 'policy_logps/chosen': -452.8868713378906, 'referece_logps/rejected': -480.83502197265625, 'referece_logps/chosen': -441.42718505859375, 'logits/rejected': -0.10882719606161118, 'logits/chosen': -0.27561601996421814, 'epoch': 2.23}

 37%|███▋      | 3999/10740 [19:53:32<35:19:57, 18.87s/it]

 37%|███▋      | 4000/10740 [19:53:50<34:28:07, 18.41s/it]

 37%|███▋      | 4001/10740 [19:54:26<44:44:33, 23.90s/it]


 37%|███▋      | 4003/10740 [19:55:05<40:14:29, 21.50s/it]
{'loss': 0.4186, 'learning_rate': 1.444817058666228e-06, 'rewards/chosen': -1.9315203428268433, 'rewards/rejected': -3.498399496078491, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5668795108795166, 'policy_logps/rejected': -424.5071716308594, 'policy_logps/chosen': -343.542236328125, 'referece_logps/rejected': -389.52313232421875, 'referece_logps/chosen': -324.22705078125, 'logits/rejected': -0.32711946964263916, 'logits/chosen': -0.36136311292648315, 'epoch': 2.24}

 37%|███▋      | 4004/10740 [19:55:27<40:27:38, 21.62s/it]


 37%|███▋      | 4006/10740 [19:56:07<38:57:01, 20.82s/it]
{'loss': 0.3206, 'learning_rate': 1.4440065634509773e-06, 'rewards/chosen': -1.67781400680542, 'rewards/rejected': -3.87209415435791, 'rewards/accuracies': 1.0, 'rewards/margins': 2.194279909133911, 'policy_logps/rejected': -432.68218994140625, 'policy_logps/chosen': -424.8713684082031, 'referece_logps/rejected': -393.9612731933594, 'referece_logps/chosen': -408.0932312011719, 'logits/rejected': 0.1893954575061798, 'logits/chosen': 0.25866013765335083, 'epoch': 2.24}


 37%|███▋      | 4008/10740 [19:56:47<38:23:26, 20.53s/it]
[2024-04-02 15:10:31,043] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3343, 'learning_rate': 1.443466031349094e-06, 'rewards/chosen': -1.9423846006393433, 'rewards/rejected': -3.9807887077331543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0384039878845215, 'policy_logps/rejected': -533.1587524414062, 'policy_logps/chosen': -534.8275146484375, 'referece_logps/rejected': -493.350830078125, 'referece_logps/chosen': -515.4037475585938, 'logits/rejected': 0.45327961444854736, 'logits/chosen': 0.24801181256771088, 'epoch': 2.24}
[2024-04-02 15:10:51,866] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 4009/10740 [19:57:08<38:32:58, 20.62s/it]

 37%|███▋      | 4010/10740 [19:57:31<39:33:07, 21.16s/it]

 37%|███▋      | 4011/10740 [19:57:46<36:33:34, 19.56s/it]
[2024-04-02 15:11:49,718] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 4012/10740 [19:58:06<36:34:47, 19.57s/it]

 37%|███▋      | 4013/10740 [19:58:22<34:42:25, 18.57s/it]

 37%|███▋      | 4014/10740 [19:58:42<35:39:12, 19.08s/it]

 37%|███▋      | 4015/10740 [19:59:02<35:56:51, 19.24s/it]

 37%|███▋      | 4016/10740 [19:59:15<32:12:32, 17.24s/it]

 37%|███▋      | 4017/10740 [19:59:34<33:31:38, 17.95s/it]


 37%|███▋      | 4019/10740 [20:00:12<34:01:18, 18.22s/it]
{'loss': 0.4126, 'learning_rate': 1.4404902261603483e-06, 'rewards/chosen': -0.36362287402153015, 'rewards/rejected': -1.9600167274475098, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5963937044143677, 'policy_logps/rejected': -421.2139587402344, 'policy_logps/chosen': -316.6313171386719, 'referece_logps/rejected': -401.61383056640625, 'referece_logps/chosen': -312.9950866699219, 'logits/rejected': -0.72417813539505, 'logits/chosen': -0.8316717147827148, 'epoch': 2.25}

 37%|███▋      | 4020/10740 [20:00:32<35:06:01, 18.80s/it]

 37%|███▋      | 4021/10740 [20:00:45<31:49:05, 17.05s/it]

 37%|███▋      | 4022/10740 [20:00:58<29:49:13, 15.98s/it]

 37%|███▋      | 4023/10740 [20:01:09<26:50:18, 14.38s/it]

 37%|███▋      | 4024/10740 [20:01:25<27:36:09, 14.80s/it]

 37%|███▋      | 4025/10740 [20:01:44<30:24:01, 16.30s/it]

 37%|███▋      | 4026/10740 [20:01:55<27:17:21, 14.63s/it]

 37%|███▋      | 4027/10740 [20:02:05<24:44:32, 13.27s/it]

 38%|███▊      | 4028/10740 [20:02:16<23:26:57, 12.58s/it]

 38%|███▊      | 4029/10740 [20:02:33<25:32:29, 13.70s/it]

 38%|███▊      | 4030/10740 [20:02:49<27:14:06, 14.61s/it]

 38%|███▊      | 4031/10740 [20:03:11<31:15:13, 16.77s/it]

 38%|███▊      | 4032/10740 [20:03:28<31:07:32, 16.70s/it]

 38%|███▊      | 4033/10740 [20:03:48<33:24:03, 17.93s/it]

 38%|███▊      | 4034/10740 [20:04:03<31:39:59, 17.00s/it]

 38%|███▊      | 4035/10740 [20:04:19<31:11:49, 16.75s/it]

 38%|███▊      | 4036/10740 [20:04:38<32:24:52, 17.41s/it]

 38%|███▊      | 4037/10740 [20:05:00<34:57:50, 18.78s/it]

 38%|███▊      | 4038/10740 [20:05:17<33:51:31, 18.19s/it]

 38%|███▊      | 4039/10740 [20:05:39<35:52:36, 19.27s/it]

 38%|███▊      | 4040/10740 [20:05:56<34:25:58, 18.50s/it]

 38%|███▊      | 4041/10740 [20:06:09<31:40:27, 17.02s/it]

 38%|███▊      | 4042/10740 [20:06:28<32:53:37, 17.68s/it]
[2024-04-02 15:20:35,187] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4043/10740 [20:06:51<35:49:43, 19.26s/it]

 38%|███▊      | 4044/10740 [20:07:05<32:25:49, 17.44s/it]

 38%|███▊      | 4045/10740 [20:07:20<31:25:47, 16.90s/it]

 38%|███▊      | 4046/10740 [20:07:41<33:23:37, 17.96s/it]

 38%|███▊      | 4047/10740 [20:07:52<29:32:38, 15.89s/it]
[2024-04-02 15:21:48,880] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 38%|███▊      | 4049/10740 [20:08:26<31:28:25, 16.93s/it]
{'loss': 0.3871, 'learning_rate': 1.4323498507163629e-06, 'rewards/chosen': -1.7118234634399414, 'rewards/rejected': -3.5868992805480957, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8750755786895752, 'policy_logps/rejected': -591.058349609375, 'policy_logps/chosen': -494.22491455078125, 'referece_logps/rejected': -555.1893310546875, 'referece_logps/chosen': -477.106689453125, 'logits/rejected': -0.39583534002304077, 'logits/chosen': -0.33564263582229614, 'epoch': 2.26}

 38%|███▊      | 4050/10740 [20:08:40<29:24:59, 15.83s/it]

 38%|███▊      | 4051/10740 [20:08:57<30:09:51, 16.23s/it]

 38%|███▊      | 4052/10740 [20:09:14<30:29:03, 16.41s/it]

 38%|███▊      | 4053/10740 [20:09:35<33:33:56, 18.07s/it]

 38%|███▊      | 4054/10740 [20:09:57<35:19:06, 19.02s/it]

 38%|███▊      | 4055/10740 [20:10:13<33:58:11, 18.29s/it]

 38%|███▊      | 4056/10740 [20:10:36<36:11:34, 19.49s/it]

 38%|███▊      | 4057/10740 [20:10:53<35:02:50, 18.88s/it]


 38%|███▊      | 4059/10740 [20:11:28<34:03:50, 18.36s/it]
{'loss': 0.4325, 'learning_rate': 1.4296284946429315e-06, 'rewards/chosen': -0.9477848410606384, 'rewards/rejected': -3.2393598556518555, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2915749549865723, 'policy_logps/rejected': -365.7865905761719, 'policy_logps/chosen': -426.1014709472656, 'referece_logps/rejected': -333.3930358886719, 'referece_logps/chosen': -416.62359619140625, 'logits/rejected': -0.410334050655365, 'logits/chosen': -0.39832690358161926, 'epoch': 2.27}


 38%|███▊      | 4061/10740 [20:12:04<33:52:00, 18.25s/it]
{'loss': 0.5281, 'learning_rate': 1.4290837537267433e-06, 'rewards/chosen': -1.9705700874328613, 'rewards/rejected': -2.95786190032959, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9872917532920837, 'policy_logps/rejected': -351.0087890625, 'policy_logps/chosen': -407.418212890625, 'referece_logps/rejected': -321.43017578125, 'referece_logps/chosen': -387.71246337890625, 'logits/rejected': 0.2915966808795929, 'logits/chosen': 0.3184809386730194, 'epoch': 2.27}


 38%|███▊      | 4063/10740 [20:12:41<33:24:02, 18.01s/it]

 38%|███▊      | 4064/10740 [20:12:55<31:09:32, 16.80s/it]
{'loss': 0.3872, 'learning_rate': 1.4282663497177674e-06, 'rewards/chosen': -1.4101524353027344, 'rewards/rejected': -2.4054605960845947, 'rewards/accuracies': 0.875, 'rewards/margins': 0.995307981967926, 'policy_logps/rejected': -428.7297668457031, 'policy_logps/chosen': -523.9254760742188, 'referece_logps/rejected': -404.6751403808594, 'referece_logps/chosen': -509.8240051269531, 'logits/rejected': -0.12759338319301605, 'logits/chosen': -0.11243242025375366, 'epoch': 2.27}

 38%|███▊      | 4065/10740 [20:13:09<29:44:31, 16.04s/it]

 38%|███▊      | 4066/10740 [20:13:24<29:09:15, 15.73s/it]
[2024-04-02 15:27:26,943] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4067/10740 [20:13:43<31:12:17, 16.83s/it]

 38%|███▊      | 4068/10740 [20:14:01<31:48:08, 17.16s/it]

 38%|███▊      | 4069/10740 [20:14:13<29:02:24, 15.67s/it]

 38%|███▊      | 4070/10740 [20:14:33<31:12:53, 16.85s/it]

 38%|███▊      | 4071/10740 [20:14:51<32:04:41, 17.32s/it]

 38%|███▊      | 4072/10740 [20:15:10<32:40:43, 17.64s/it]

 38%|███▊      | 4073/10740 [20:15:32<35:16:28, 19.05s/it]


 38%|███▊      | 4075/10740 [20:16:11<35:28:53, 19.16s/it]
{'loss': 0.3196, 'learning_rate': 1.4252662074822627e-06, 'rewards/chosen': -1.4392290115356445, 'rewards/rejected': -2.5234742164611816, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0842453241348267, 'policy_logps/rejected': -265.45458984375, 'policy_logps/chosen': -256.8370666503906, 'referece_logps/rejected': -240.2198486328125, 'referece_logps/chosen': -242.4447784423828, 'logits/rejected': -0.9337116479873657, 'logits/chosen': -0.8561645746231079, 'epoch': 2.28}

 38%|███▊      | 4076/10740 [20:16:30<35:31:57, 19.20s/it]


 38%|███▊      | 4078/10740 [20:17:05<33:50:15, 18.29s/it]
{'loss': 0.3992, 'learning_rate': 1.4244471732238329e-06, 'rewards/chosen': -1.3604724407196045, 'rewards/rejected': -2.7081267833709717, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3476544618606567, 'policy_logps/rejected': -422.4136047363281, 'policy_logps/chosen': -311.0656433105469, 'referece_logps/rejected': -395.3323059082031, 'referece_logps/chosen': -297.4609375, 'logits/rejected': 0.1774313896894455, 'logits/chosen': 0.16203787922859192, 'epoch': 2.28}

 38%|███▊      | 4079/10740 [20:17:26<35:13:27, 19.04s/it]


 38%|███▊      | 4081/10740 [20:17:59<32:54:15, 17.79s/it]
{'loss': 0.4022, 'learning_rate': 1.4236277915248222e-06, 'rewards/chosen': -1.4882334470748901, 'rewards/rejected': -2.8362252712249756, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3479915857315063, 'policy_logps/rejected': -387.3128356933594, 'policy_logps/chosen': -250.70980834960938, 'referece_logps/rejected': -358.9505920410156, 'referece_logps/chosen': -235.82748413085938, 'logits/rejected': -0.7047533392906189, 'logits/chosen': -0.6697075963020325, 'epoch': 2.28}

 38%|███▊      | 4082/10740 [20:18:20<34:43:56, 18.78s/it]

 38%|███▊      | 4083/10740 [20:18:37<34:04:34, 18.43s/it]

 38%|███▊      | 4084/10740 [20:18:57<34:41:15, 18.76s/it]

 38%|███▊      | 4085/10740 [20:19:15<34:25:39, 18.62s/it]

 38%|███▊      | 4086/10740 [20:19:31<32:54:14, 17.80s/it]

 38%|███▊      | 4087/10740 [20:19:55<36:21:37, 19.67s/it]

 38%|███▊      | 4088/10740 [20:20:18<38:17:52, 20.73s/it]

 38%|███▊      | 4089/10740 [20:20:35<35:55:18, 19.44s/it]

 38%|███▊      | 4090/10740 [20:20:50<33:41:36, 18.24s/it]

 38%|███▊      | 4091/10740 [20:21:10<34:22:21, 18.61s/it]

 38%|███▊      | 4092/10740 [20:21:29<34:32:39, 18.71s/it]

 38%|███▊      | 4093/10740 [20:21:48<34:52:06, 18.88s/it]

 38%|███▊      | 4094/10740 [20:22:06<34:26:25, 18.66s/it]

 38%|███▊      | 4095/10740 [20:22:21<32:12:39, 17.45s/it]

 38%|███▊      | 4096/10740 [20:22:40<33:21:38, 18.08s/it]

 38%|███▊      | 4097/10740 [20:22:53<30:08:54, 16.34s/it]

 38%|███▊      | 4098/10740 [20:23:07<29:05:13, 15.77s/it]

 38%|███▊      | 4099/10740 [20:23:28<31:59:54, 17.35s/it]

 38%|███▊      | 4100/10740 [20:23:40<28:55:29, 15.68s/it]

 38%|███▊      | 4101/10740 [20:23:57<29:28:56, 15.99s/it]

 38%|███▊      | 4102/10740 [20:24:18<32:18:26, 17.52s/it]

 38%|███▊      | 4103/10740 [20:24:36<32:51:00, 17.82s/it]


 38%|███▊      | 4105/10740 [20:25:15<34:56:31, 18.96s/it]
{'loss': 0.3974, 'learning_rate': 1.4170603106180488e-06, 'rewards/chosen': -1.584385633468628, 'rewards/rejected': -3.194672107696533, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6102863550186157, 'policy_logps/rejected': -367.6668395996094, 'policy_logps/chosen': -434.9742126464844, 'referece_logps/rejected': -335.7200927734375, 'referece_logps/chosen': -419.13037109375, 'logits/rejected': -0.6246037483215332, 'logits/chosen': -0.6989778876304626, 'epoch': 2.29}

 38%|███▊      | 4106/10740 [20:25:31<33:03:56, 17.94s/it]

 38%|███▊      | 4107/10740 [20:25:51<34:23:30, 18.67s/it]

 38%|███▊      | 4108/10740 [20:26:10<34:37:32, 18.80s/it]

 38%|███▊      | 4109/10740 [20:26:30<35:03:24, 19.03s/it]

 38%|███▊      | 4110/10740 [20:26:41<30:45:02, 16.70s/it]

 38%|███▊      | 4111/10740 [20:27:01<32:25:01, 17.60s/it]

 38%|███▊      | 4112/10740 [20:27:20<33:31:02, 18.20s/it]

 38%|███▊      | 4113/10740 [20:27:40<34:24:40, 18.69s/it]

 38%|███▊      | 4114/10740 [20:27:51<29:59:45, 16.30s/it]

 38%|███▊      | 4115/10740 [20:28:03<27:27:16, 14.92s/it]

 38%|███▊      | 4116/10740 [20:28:25<31:36:00, 17.17s/it]

 38%|███▊      | 4117/10740 [20:28:47<34:10:25, 18.58s/it]

 38%|███▊      | 4118/10740 [20:29:07<34:48:56, 18.93s/it]

 38%|███▊      | 4119/10740 [20:29:23<33:17:28, 18.10s/it]

 38%|███▊      | 4120/10740 [20:29:45<35:18:41, 19.20s/it]

 38%|███▊      | 4121/10740 [20:30:05<35:45:05, 19.44s/it]

 38%|███▊      | 4122/10740 [20:30:23<34:56:54, 19.01s/it]

 38%|███▊      | 4123/10740 [20:30:41<34:51:51, 18.97s/it]

 38%|███▊      | 4124/10740 [20:30:59<34:12:29, 18.61s/it]

 38%|███▊      | 4125/10740 [20:31:17<33:40:24, 18.33s/it]

 38%|███▊      | 4126/10740 [20:31:39<35:48:54, 19.49s/it]

 38%|███▊      | 4127/10740 [20:31:55<34:04:30, 18.55s/it]

 38%|███▊      | 4128/10740 [20:32:08<30:57:02, 16.85s/it]

 38%|███▊      | 4129/10740 [20:32:28<32:36:15, 17.75s/it]

 38%|███▊      | 4130/10740 [20:32:44<31:24:47, 17.11s/it]

 38%|███▊      | 4131/10740 [20:33:00<30:49:54, 16.79s/it]

 38%|███▊      | 4132/10740 [20:33:15<30:08:04, 16.42s/it]
[2024-04-02 15:47:20,086] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4133/10740 [20:33:36<32:36:08, 17.76s/it]
[2024-04-02 15:47:44,838] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4134/10740 [20:34:01<36:26:39, 19.86s/it]

 39%|███▊      | 4135/10740 [20:34:23<37:49:29, 20.62s/it]
[2024-04-02 15:48:27,812] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 39%|███▊      | 4137/10740 [20:35:05<38:03:16, 20.75s/it]

 39%|███▊      | 4138/10740 [20:35:25<37:26:22, 20.42s/it]

 39%|███▊      | 4139/10740 [20:35:48<38:42:24, 21.11s/it]
[2024-04-02 15:49:31,251] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4948, 'learning_rate': 1.4077190599289515e-06, 'rewards/chosen': -1.4461945295333862, 'rewards/rejected': -3.6033055782318115, 'rewards/accuracies': 0.625, 'rewards/margins': 2.157111167907715, 'policy_logps/rejected': -407.67987060546875, 'policy_logps/chosen': -345.43707275390625, 'referece_logps/rejected': -371.6468505859375, 'referece_logps/chosen': -330.97509765625, 'logits/rejected': 0.12272503972053528, 'logits/chosen': 0.07196041941642761, 'epoch': 2.31}


 39%|███▊      | 4141/10740 [20:36:29<38:29:46, 21.00s/it]

 39%|███▊      | 4142/10740 [20:36:48<37:17:44, 20.35s/it]

 39%|███▊      | 4143/10740 [20:37:01<33:23:30, 18.22s/it]

 39%|███▊      | 4144/10740 [20:37:23<35:20:39, 19.29s/it]

 39%|███▊      | 4145/10740 [20:37:40<33:56:29, 18.53s/it]

 39%|███▊      | 4146/10740 [20:37:51<29:45:32, 16.25s/it]
{'loss': 0.4403, 'learning_rate': 1.4057905075320696e-06, 'rewards/chosen': -2.1061816215515137, 'rewards/rejected': -2.0506536960601807, 'rewards/accuracies': 0.625, 'rewards/margins': -0.055528074502944946, 'policy_logps/rejected': -498.4303283691406, 'policy_logps/chosen': -375.9864807128906, 'referece_logps/rejected': -477.9237365722656, 'referece_logps/chosen': -354.9246520996094, 'logits/rejected': -0.23279982805252075, 'logits/chosen': -0.29775354266166687, 'epoch': 2.32}


 39%|███▊      | 4148/10740 [20:38:27<31:17:43, 17.09s/it]

 39%|███▊      | 4149/10740 [20:38:45<31:46:26, 17.35s/it]

 39%|███▊      | 4150/10740 [20:39:02<31:28:00, 17.19s/it]

 39%|███▊      | 4151/10740 [20:39:22<33:10:17, 18.12s/it]
[2024-04-02 15:53:05,826] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▊      | 4152/10740 [20:39:44<35:20:15, 19.31s/it]

 39%|███▊      | 4153/10740 [20:40:00<33:13:16, 18.16s/it]

 39%|███▊      | 4154/10740 [20:40:19<34:05:19, 18.63s/it]

 39%|███▊      | 4155/10740 [20:40:36<32:46:44, 17.92s/it]

 39%|███▊      | 4156/10740 [20:40:52<32:02:10, 17.52s/it]
{'loss': 0.4739, 'learning_rate': 1.4030322976626162e-06, 'rewards/chosen': -1.6272090673446655, 'rewards/rejected': -2.2698755264282227, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6426664590835571, 'policy_logps/rejected': -344.9093322753906, 'policy_logps/chosen': -491.8948669433594, 'referece_logps/rejected': -322.2105407714844, 'referece_logps/chosen': -475.62274169921875, 'logits/rejected': -1.0360374450683594, 'logits/chosen': -1.141422986984253, 'epoch': 2.32}


 39%|███▊      | 4158/10740 [20:41:35<35:22:25, 19.35s/it]
{'loss': 0.3035, 'learning_rate': 1.4024802150043862e-06, 'rewards/chosen': -0.7113780379295349, 'rewards/rejected': -2.1466290950775146, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4352513551712036, 'policy_logps/rejected': -397.5946350097656, 'policy_logps/chosen': -297.129638671875, 'referece_logps/rejected': -376.1283264160156, 'referece_logps/chosen': -290.015869140625, 'logits/rejected': -0.5541321635246277, 'logits/chosen': -0.5881723165512085, 'epoch': 2.32}


 39%|███▊      | 4160/10740 [20:42:13<34:55:56, 19.11s/it]
{'loss': 0.3281, 'learning_rate': 1.4019279859199094e-06, 'rewards/chosen': -2.01124906539917, 'rewards/rejected': -4.9646782875061035, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9534287452697754, 'policy_logps/rejected': -414.3086853027344, 'policy_logps/chosen': -326.6831970214844, 'referece_logps/rejected': -364.6618957519531, 'referece_logps/chosen': -306.5707092285156, 'logits/rejected': -0.5698376893997192, 'logits/chosen': -0.5815266966819763, 'epoch': 2.32}


 39%|███▉      | 4162/10740 [20:42:58<37:55:30, 20.76s/it]

 39%|███▉      | 4163/10740 [20:43:18<37:39:16, 20.61s/it]

 39%|███▉      | 4164/10740 [20:43:39<37:55:33, 20.76s/it]

 39%|███▉      | 4165/10740 [20:43:59<37:29:51, 20.53s/it]

 39%|███▉      | 4166/10740 [20:44:18<36:11:18, 19.82s/it]

 39%|███▉      | 4167/10740 [20:44:37<36:03:38, 19.75s/it]

 39%|███▉      | 4168/10740 [20:44:58<36:45:51, 20.14s/it]
{'loss': 0.3169, 'learning_rate': 1.399717609338455e-06, 'rewards/chosen': -1.769917607307434, 'rewards/rejected': -4.379373073577881, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6094553470611572, 'policy_logps/rejected': -438.6583557128906, 'policy_logps/chosen': -522.0953369140625, 'referece_logps/rejected': -394.86468505859375, 'referece_logps/chosen': -504.39617919921875, 'logits/rejected': 0.6059297323226929, 'logits/chosen': 0.739194393157959, 'epoch': 2.33}


 39%|███▉      | 4170/10740 [20:45:36<35:47:43, 19.61s/it]

 39%|███▉      | 4171/10740 [20:45:56<35:58:00, 19.71s/it]

 39%|███▉      | 4172/10740 [20:46:13<34:33:37, 18.94s/it]

 39%|███▉      | 4173/10740 [20:46:32<34:12:50, 18.76s/it]

 39%|███▉      | 4174/10740 [20:46:52<35:01:33, 19.20s/it]

 39%|███▉      | 4175/10740 [20:47:14<36:23:08, 19.95s/it]

 39%|███▉      | 4176/10740 [20:47:26<32:26:51, 17.80s/it]

 39%|███▉      | 4177/10740 [20:47:42<31:13:30, 17.13s/it]

 39%|███▉      | 4178/10740 [20:47:57<30:21:58, 16.66s/it]
{'loss': 0.3795, 'learning_rate': 1.3969513681455678e-06, 'rewards/chosen': -1.6077308654785156, 'rewards/rejected': -2.8828489780426025, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2751179933547974, 'policy_logps/rejected': -553.49951171875, 'policy_logps/chosen': -405.7019958496094, 'referece_logps/rejected': -524.6710815429688, 'referece_logps/chosen': -389.6246337890625, 'logits/rejected': 0.24012506008148193, 'logits/chosen': 0.25041908025741577, 'epoch': 2.33}


 39%|███▉      | 4180/10740 [20:48:37<33:29:36, 18.38s/it]

 39%|███▉      | 4181/10740 [20:48:50<30:15:55, 16.61s/it]

 39%|███▉      | 4182/10740 [20:49:07<30:43:01, 16.86s/it]

 39%|███▉      | 4183/10740 [20:49:28<32:52:41, 18.05s/it]

 39%|███▉      | 4184/10740 [20:49:48<33:40:48, 18.49s/it]

 39%|███▉      | 4185/10740 [20:50:08<34:35:01, 18.99s/it]

 39%|███▉      | 4186/10740 [20:50:28<35:10:29, 19.32s/it]

 39%|███▉      | 4187/10740 [20:50:44<33:32:57, 18.43s/it]

 39%|███▉      | 4188/10740 [20:51:02<33:07:28, 18.20s/it]
{'loss': 0.455, 'learning_rate': 1.3941815165853468e-06, 'rewards/chosen': -0.7552756071090698, 'rewards/rejected': -2.9249863624572754, 'rewards/accuracies': 0.875, 'rewards/margins': 2.169710636138916, 'policy_logps/rejected': -573.1594848632812, 'policy_logps/chosen': -411.0545349121094, 'referece_logps/rejected': -543.90966796875, 'referece_logps/chosen': -403.5017395019531, 'logits/rejected': -0.5289662480354309, 'logits/chosen': -0.6220141649246216, 'epoch': 2.34}

 39%|███▉      | 4189/10740 [20:51:21<33:47:10, 18.57s/it]

 39%|███▉      | 4190/10740 [20:51:37<32:15:03, 17.73s/it]


 39%|███▉      | 4192/10740 [20:52:16<33:55:44, 18.65s/it]
[2024-04-02 16:05:59,531] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4193/10740 [20:52:38<35:46:14, 19.67s/it]
[2024-04-02 16:06:21,570] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3814, 'learning_rate': 1.3927952447888538e-06, 'rewards/chosen': -0.8915070295333862, 'rewards/rejected': -3.6749179363250732, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7834110260009766, 'policy_logps/rejected': -341.13836669921875, 'policy_logps/chosen': -366.04425048828125, 'referece_logps/rejected': -304.38916015625, 'referece_logps/chosen': -357.1291809082031, 'logits/rejected': -0.4658876657485962, 'logits/chosen': -0.587643027305603, 'epoch': 2.34}
[2024-04-02 16:06:39,044] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 39%|███▉      | 4195/10740 [20:53:14<34:38:11, 19.05s/it]

 39%|███▉      | 4196/10740 [20:53:32<33:51:20, 18.62s/it]

 39%|███▉      | 4197/10740 [20:53:49<32:59:32, 18.15s/it]

 39%|███▉      | 4198/10740 [20:54:10<34:40:59, 19.09s/it]
[2024-04-02 16:07:54,134] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4199/10740 [20:54:26<33:02:01, 18.18s/it]

 39%|███▉      | 4200/10740 [20:54:40<30:38:46, 16.87s/it]
{'loss': 0.3702, 'learning_rate': 1.3908529645014839e-06, 'rewards/chosen': -0.5802554488182068, 'rewards/rejected': -2.4489176273345947, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8686621189117432, 'policy_logps/rejected': -454.35760498046875, 'policy_logps/chosen': -334.7706298828125, 'referece_logps/rejected': -429.86846923828125, 'referece_logps/chosen': -328.9681091308594, 'logits/rejected': -0.7953105568885803, 'logits/chosen': -0.8926517367362976, 'epoch': 2.35}


 39%|███▉      | 4202/10740 [20:55:16<31:52:24, 17.55s/it]

 39%|███▉      | 4203/10740 [20:55:29<29:12:58, 16.09s/it]

 39%|███▉      | 4204/10740 [20:55:51<32:30:40, 17.91s/it]

 39%|███▉      | 4205/10740 [20:56:03<28:58:11, 15.96s/it]

 39%|███▉      | 4206/10740 [20:56:22<31:00:45, 17.09s/it]

 39%|███▉      | 4207/10740 [20:56:42<32:26:09, 17.87s/it]

 39%|███▉      | 4208/10740 [20:57:03<34:07:34, 18.81s/it]

 39%|███▉      | 4209/10740 [20:57:18<31:59:02, 17.63s/it]

 39%|███▉      | 4210/10740 [20:57:38<33:31:15, 18.48s/it]

 39%|███▉      | 4211/10740 [20:57:51<30:33:12, 16.85s/it]

 39%|███▉      | 4212/10740 [20:58:02<27:09:43, 14.98s/it]

 39%|███▉      | 4213/10740 [20:58:22<29:58:32, 16.53s/it]

 39%|███▉      | 4214/10740 [20:58:39<30:16:30, 16.70s/it]

 39%|███▉      | 4215/10740 [20:58:51<27:18:52, 15.07s/it]
{'loss': 0.5136, 'learning_rate': 1.3866850808398606e-06, 'rewards/chosen': -0.8534916043281555, 'rewards/rejected': -2.185218572616577, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3317269086837769, 'policy_logps/rejected': -467.6524963378906, 'policy_logps/chosen': -325.4474792480469, 'referece_logps/rejected': -445.80029296875, 'referece_logps/chosen': -316.91259765625, 'logits/rejected': 0.7540320754051208, 'logits/chosen': 0.8503836989402771, 'epoch': 2.35}

 39%|███▉      | 4216/10740 [20:59:08<28:25:29, 15.69s/it]


 39%|███▉      | 4218/10740 [20:59:51<33:45:55, 18.64s/it]
[2024-04-02 16:13:34,467] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4219/10740 [21:00:05<31:24:41, 17.34s/it]
[2024-04-02 16:13:48,782] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4220/10740 [21:00:25<32:56:06, 18.19s/it]

 39%|███▉      | 4221/10740 [21:00:38<30:07:39, 16.64s/it]
{'loss': 0.3789, 'learning_rate': 1.3850157068898683e-06, 'rewards/chosen': -1.516481876373291, 'rewards/rejected': -3.2461767196655273, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7296947240829468, 'policy_logps/rejected': -296.8021240234375, 'policy_logps/chosen': -274.5057067871094, 'referece_logps/rejected': -264.3403625488281, 'referece_logps/chosen': -259.3409118652344, 'logits/rejected': -0.6512212157249451, 'logits/chosen': -0.8162924647331238, 'epoch': 2.36}


 39%|███▉      | 4223/10740 [21:01:13<30:25:07, 16.80s/it]
{'loss': 0.3692, 'learning_rate': 1.3844589684915843e-06, 'rewards/chosen': -1.2851413488388062, 'rewards/rejected': -3.0526111125946045, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7674700021743774, 'policy_logps/rejected': -380.57379150390625, 'policy_logps/chosen': -433.4812316894531, 'referece_logps/rejected': -350.04766845703125, 'referece_logps/chosen': -420.62982177734375, 'logits/rejected': -0.7871434092521667, 'logits/chosen': -0.7460994720458984, 'epoch': 2.36}


 39%|███▉      | 4225/10740 [21:01:47<30:44:38, 16.99s/it]
{'loss': 0.354, 'learning_rate': 1.3839020902233594e-06, 'rewards/chosen': -1.6257230043411255, 'rewards/rejected': -4.172084331512451, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5463614463806152, 'policy_logps/rejected': -389.2862854003906, 'policy_logps/chosen': -362.548095703125, 'referece_logps/rejected': -347.5654602050781, 'referece_logps/chosen': -346.2908630371094, 'logits/rejected': -0.41674870252609253, 'logits/chosen': -0.40194424986839294, 'epoch': 2.36}

 39%|███▉      | 4226/10740 [21:02:04<30:42:03, 16.97s/it]

 39%|███▉      | 4227/10740 [21:02:22<31:10:27, 17.23s/it]

 39%|███▉      | 4228/10740 [21:02:36<29:20:14, 16.22s/it]

 39%|███▉      | 4229/10740 [21:02:56<31:35:54, 17.47s/it]


 39%|███▉      | 4231/10740 [21:03:31<31:15:27, 17.29s/it]
{'loss': 0.4146, 'learning_rate': 1.3822306182252703e-06, 'rewards/chosen': -0.9069108963012695, 'rewards/rejected': -4.8333210945129395, 'rewards/accuracies': 0.875, 'rewards/margins': 3.92641019821167, 'policy_logps/rejected': -460.74334716796875, 'policy_logps/chosen': -452.6985778808594, 'referece_logps/rejected': -412.41009521484375, 'referece_logps/chosen': -443.6294860839844, 'logits/rejected': -0.233048215508461, 'logits/chosen': -0.28055188059806824, 'epoch': 2.36}
[2024-04-02 16:17:33,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4232/10740 [21:03:50<32:03:02, 17.73s/it]


 39%|███▉      | 4234/10740 [21:04:32<34:52:40, 19.30s/it]
{'loss': 0.4565, 'learning_rate': 1.3813944125591712e-06, 'rewards/chosen': -1.4226781129837036, 'rewards/rejected': -2.700869560241699, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2781915664672852, 'policy_logps/rejected': -450.38592529296875, 'policy_logps/chosen': -338.88330078125, 'referece_logps/rejected': -423.37725830078125, 'referece_logps/chosen': -324.65655517578125, 'logits/rejected': 0.6493744850158691, 'logits/chosen': 0.5287410020828247, 'epoch': 2.37}

 39%|███▉      | 4235/10740 [21:04:54<36:32:24, 20.22s/it]


 39%|███▉      | 4237/10740 [21:05:28<32:58:11, 18.25s/it]
{'loss': 0.4099, 'learning_rate': 1.3805578946942768e-06, 'rewards/chosen': -1.5723336935043335, 'rewards/rejected': -2.820077419281006, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2477436065673828, 'policy_logps/rejected': -443.9063720703125, 'policy_logps/chosen': -317.7243957519531, 'referece_logps/rejected': -415.70562744140625, 'referece_logps/chosen': -302.0010986328125, 'logits/rejected': -0.8103430271148682, 'logits/chosen': -0.8730844855308533, 'epoch': 2.37}

 39%|███▉      | 4238/10740 [21:05:43<31:14:51, 17.30s/it]


 39%|███▉      | 4240/10740 [21:06:12<28:32:36, 15.81s/it]
{'loss': 0.4021, 'learning_rate': 1.379721065315337e-06, 'rewards/chosen': -1.7006090879440308, 'rewards/rejected': -2.8940815925598145, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1934726238250732, 'policy_logps/rejected': -363.9717712402344, 'policy_logps/chosen': -325.560791015625, 'referece_logps/rejected': -335.03094482421875, 'referece_logps/chosen': -308.5546875, 'logits/rejected': -1.7784900665283203, 'logits/chosen': -1.743354082107544, 'epoch': 2.37}

 39%|███▉      | 4241/10740 [21:06:32<31:00:57, 17.18s/it]

 39%|███▉      | 4242/10740 [21:06:48<30:16:36, 16.77s/it]


 40%|███▉      | 4244/10740 [21:07:26<32:12:22, 17.85s/it]
{'loss': 0.4043, 'learning_rate': 1.378604809416676e-06, 'rewards/chosen': -1.1557096242904663, 'rewards/rejected': -3.2061612606048584, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0504517555236816, 'policy_logps/rejected': -317.046630859375, 'policy_logps/chosen': -293.56463623046875, 'referece_logps/rejected': -284.9850158691406, 'referece_logps/chosen': -282.0075378417969, 'logits/rejected': -0.7666548490524292, 'logits/chosen': -0.6879581212997437, 'epoch': 2.37}


 40%|███▉      | 4246/10740 [21:07:57<30:56:03, 17.15s/it]

 40%|███▉      | 4247/10740 [21:08:14<30:34:34, 16.95s/it]
{'loss': 0.487, 'learning_rate': 1.3777672558359788e-06, 'rewards/chosen': -1.0191658735275269, 'rewards/rejected': -2.7160580158233643, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6968920230865479, 'policy_logps/rejected': -318.5036315917969, 'policy_logps/chosen': -347.95074462890625, 'referece_logps/rejected': -291.34307861328125, 'referece_logps/chosen': -337.7590637207031, 'logits/rejected': 0.6085067391395569, 'logits/chosen': 0.5723525285720825, 'epoch': 2.37}

 40%|███▉      | 4248/10740 [21:08:33<31:39:25, 17.55s/it]

 40%|███▉      | 4249/10740 [21:08:45<28:33:16, 15.84s/it]

 40%|███▉      | 4250/10740 [21:08:59<27:27:15, 15.23s/it]


 40%|███▉      | 4252/10740 [21:09:38<31:37:27, 17.55s/it]

 40%|███▉      | 4253/10740 [21:09:50<28:37:55, 15.89s/it]
{'loss': 0.4454, 'learning_rate': 1.3760912216713156e-06, 'rewards/chosen': -2.0614962577819824, 'rewards/rejected': -2.6757071018218994, 'rewards/accuracies': 0.625, 'rewards/margins': 0.614210844039917, 'policy_logps/rejected': -445.1919250488281, 'policy_logps/chosen': -457.0497741699219, 'referece_logps/rejected': -418.4348449707031, 'referece_logps/chosen': -436.434814453125, 'logits/rejected': -0.09264697134494781, 'logits/chosen': -0.14133943617343903, 'epoch': 2.38}


 40%|███▉      | 4255/10740 [21:10:26<30:34:22, 16.97s/it]

 40%|███▉      | 4256/10740 [21:10:38<27:49:41, 15.45s/it]

 40%|███▉      | 4257/10740 [21:10:57<30:13:15, 16.78s/it]

 40%|███▉      | 4258/10740 [21:11:12<28:49:56, 16.01s/it]
{'loss': 0.4004, 'learning_rate': 1.3746935856248103e-06, 'rewards/chosen': -1.443394660949707, 'rewards/rejected': -3.272739887237549, 'rewards/accuracies': 0.875, 'rewards/margins': 1.829345464706421, 'policy_logps/rejected': -433.189208984375, 'policy_logps/chosen': -363.39373779296875, 'referece_logps/rejected': -400.4617919921875, 'referece_logps/chosen': -348.9598083496094, 'logits/rejected': -0.7074561715126038, 'logits/chosen': -0.5429073572158813, 'epoch': 2.38}

 40%|███▉      | 4259/10740 [21:11:31<30:41:49, 17.05s/it]


 40%|███▉      | 4261/10740 [21:12:10<32:20:56, 17.97s/it]

 40%|███▉      | 4262/10740 [21:12:21<28:58:39, 16.10s/it]
{'loss': 0.4134, 'learning_rate': 1.3735748632077006e-06, 'rewards/chosen': -1.4365307092666626, 'rewards/rejected': -3.4643027782440186, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0277719497680664, 'policy_logps/rejected': -468.1919860839844, 'policy_logps/chosen': -399.116455078125, 'referece_logps/rejected': -433.5489501953125, 'referece_logps/chosen': -384.7511291503906, 'logits/rejected': -1.2406065464019775, 'logits/chosen': -1.3071894645690918, 'epoch': 2.38}


 40%|███▉      | 4264/10740 [21:12:52<27:21:50, 15.21s/it]

 40%|███▉      | 4265/10740 [21:13:13<30:49:23, 17.14s/it]

 40%|███▉      | 4266/10740 [21:13:26<28:15:52, 15.72s/it]

 40%|███▉      | 4267/10740 [21:13:44<29:40:05, 16.50s/it]

 40%|███▉      | 4268/10740 [21:14:06<32:35:27, 18.13s/it]

 40%|███▉      | 4269/10740 [21:14:26<33:25:58, 18.60s/it]

 40%|███▉      | 4270/10740 [21:14:46<34:27:26, 19.17s/it]

 40%|███▉      | 4271/10740 [21:15:00<31:27:57, 17.51s/it]
{'loss': 0.4296, 'learning_rate': 1.371055752564207e-06, 'rewards/chosen': -0.895669162273407, 'rewards/rejected': -2.5633139610290527, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6676448583602905, 'policy_logps/rejected': -358.37109375, 'policy_logps/chosen': -525.5095825195312, 'referece_logps/rejected': -332.7379455566406, 'referece_logps/chosen': -516.5528564453125, 'logits/rejected': -0.00775662437081337, 'logits/chosen': -0.23012886941432953, 'epoch': 2.39}
[2024-04-02 16:29:06,287] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4272/10740 [21:15:23<34:13:59, 19.05s/it]


 40%|███▉      | 4274/10740 [21:15:52<30:12:08, 16.82s/it]

 40%|███▉      | 4275/10740 [21:16:03<26:50:16, 14.94s/it]

 40%|███▉      | 4276/10740 [21:16:18<27:03:55, 15.07s/it]

 40%|███▉      | 4277/10740 [21:16:36<28:32:55, 15.90s/it]
{'loss': 0.3671, 'learning_rate': 1.3693748256436587e-06, 'rewards/chosen': -1.0014915466308594, 'rewards/rejected': -2.905719757080078, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9042284488677979, 'policy_logps/rejected': -585.7429809570312, 'policy_logps/chosen': -403.0999755859375, 'referece_logps/rejected': -556.6857299804688, 'referece_logps/chosen': -393.0850524902344, 'logits/rejected': -0.5543826222419739, 'logits/chosen': -0.5252415537834167, 'epoch': 2.39}


 40%|███▉      | 4279/10740 [21:17:18<33:15:16, 18.53s/it]
{'loss': 0.3155, 'learning_rate': 1.368814247633935e-06, 'rewards/chosen': -1.6283758878707886, 'rewards/rejected': -4.29301118850708, 'rewards/accuracies': 0.875, 'rewards/margins': 2.664635419845581, 'policy_logps/rejected': -460.8266906738281, 'policy_logps/chosen': -488.90325927734375, 'referece_logps/rejected': -417.8965759277344, 'referece_logps/chosen': -472.6194763183594, 'logits/rejected': 0.575591504573822, 'logits/chosen': 0.67613685131073, 'epoch': 2.39}

 40%|███▉      | 4280/10740 [21:17:39<34:29:32, 19.22s/it]


 40%|███▉      | 4282/10740 [21:18:20<35:34:28, 19.83s/it]
{'loss': 0.3511, 'learning_rate': 1.367973129098899e-06, 'rewards/chosen': -1.6085858345031738, 'rewards/rejected': -3.3162686824798584, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7076826095581055, 'policy_logps/rejected': -465.2445983886719, 'policy_logps/chosen': -424.98870849609375, 'referece_logps/rejected': -432.0819396972656, 'referece_logps/chosen': -408.90283203125, 'logits/rejected': 0.4357401728630066, 'logits/chosen': 0.33953189849853516, 'epoch': 2.39}

 40%|███▉      | 4283/10740 [21:18:37<34:17:19, 19.12s/it]


 40%|███▉      | 4285/10740 [21:19:14<33:36:33, 18.74s/it]
{'loss': 0.4429, 'learning_rate': 1.3671317093513543e-06, 'rewards/chosen': -1.1495617628097534, 'rewards/rejected': -2.761047124862671, 'rewards/accuracies': 0.75, 'rewards/margins': 1.611485481262207, 'policy_logps/rejected': -463.97760009765625, 'policy_logps/chosen': -293.99786376953125, 'referece_logps/rejected': -436.3671569824219, 'referece_logps/chosen': -282.50225830078125, 'logits/rejected': -1.0756182670593262, 'logits/chosen': -1.0601170063018799, 'epoch': 2.39}

 40%|███▉      | 4286/10740 [21:19:27<30:37:14, 17.08s/it]


 40%|███▉      | 4288/10740 [21:20:02<30:31:47, 17.03s/it]
{'loss': 0.4301, 'learning_rate': 1.3662899890800638e-06, 'rewards/chosen': -1.5882307291030884, 'rewards/rejected': -3.5785858631134033, 'rewards/accuracies': 0.875, 'rewards/margins': 1.990355134010315, 'policy_logps/rejected': -526.8355102539062, 'policy_logps/chosen': -319.9369201660156, 'referece_logps/rejected': -491.04962158203125, 'referece_logps/chosen': -304.05462646484375, 'logits/rejected': 0.05108121782541275, 'logits/chosen': 0.07900935411453247, 'epoch': 2.4}


 40%|███▉      | 4290/10740 [21:20:36<30:55:17, 17.26s/it]

 40%|███▉      | 4291/10740 [21:20:53<30:42:56, 17.15s/it]
{'loss': 0.4286, 'learning_rate': 1.3654479689740367e-06, 'rewards/chosen': -1.6489019393920898, 'rewards/rejected': -2.6125686168670654, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9636668562889099, 'policy_logps/rejected': -381.7257995605469, 'policy_logps/chosen': -458.40765380859375, 'referece_logps/rejected': -355.60015869140625, 'referece_logps/chosen': -441.91864013671875, 'logits/rejected': -0.12602820992469788, 'logits/chosen': -0.08350531756877899, 'epoch': 2.4}


 40%|███▉      | 4293/10740 [21:21:35<33:49:07, 18.88s/it]
{'loss': 0.2795, 'learning_rate': 1.3648864560021934e-06, 'rewards/chosen': -1.7328517436981201, 'rewards/rejected': -4.601428031921387, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8685760498046875, 'policy_logps/rejected': -424.85797119140625, 'policy_logps/chosen': -289.1492919921875, 'referece_logps/rejected': -378.84368896484375, 'referece_logps/chosen': -271.82080078125, 'logits/rejected': -0.6399415731430054, 'logits/chosen': -0.6348139047622681, 'epoch': 2.4}


 40%|███▉      | 4295/10740 [21:22:14<35:10:11, 19.64s/it]
{'loss': 0.2688, 'learning_rate': 1.3643248102810811e-06, 'rewards/chosen': -0.7331749200820923, 'rewards/rejected': -4.225702285766602, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4925272464752197, 'policy_logps/rejected': -544.0419311523438, 'policy_logps/chosen': -401.0732727050781, 'referece_logps/rejected': -501.784912109375, 'referece_logps/chosen': -393.74151611328125, 'logits/rejected': -0.8251447081565857, 'logits/chosen': -0.928364634513855, 'epoch': 2.4}

 40%|████      | 4296/10740 [21:22:30<33:01:47, 18.45s/it]


 40%|████      | 4298/10740 [21:23:06<33:15:08, 18.58s/it]
{'loss': 0.4362, 'learning_rate': 1.3634820932415233e-06, 'rewards/chosen': -1.6424732208251953, 'rewards/rejected': -2.865689277648926, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2232160568237305, 'policy_logps/rejected': -524.4369506835938, 'policy_logps/chosen': -427.5146179199219, 'referece_logps/rejected': -495.780029296875, 'referece_logps/chosen': -411.08990478515625, 'logits/rejected': 0.38893359899520874, 'logits/chosen': 0.3647392690181732, 'epoch': 2.4}

 40%|████      | 4299/10740 [21:23:19<30:13:45, 16.90s/it]

 40%|████      | 4300/10740 [21:23:40<31:58:24, 17.87s/it]


 40%|████      | 4302/10740 [21:24:21<34:25:40, 19.25s/it]
{'loss': 0.4032, 'learning_rate': 1.3623580078071755e-06, 'rewards/chosen': -0.9821809530258179, 'rewards/rejected': -2.6816935539245605, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6995123624801636, 'policy_logps/rejected': -351.5312805175781, 'policy_logps/chosen': -374.615966796875, 'referece_logps/rejected': -324.71435546875, 'referece_logps/chosen': -364.7941589355469, 'logits/rejected': -1.6940269470214844, 'logits/chosen': -1.6385505199432373, 'epoch': 2.4}

 40%|████      | 4303/10740 [21:24:37<33:03:15, 18.49s/it]

 40%|████      | 4304/10740 [21:24:55<32:40:30, 18.28s/it]

 40%|████      | 4305/10740 [21:25:12<31:40:52, 17.72s/it]


 40%|████      | 4307/10740 [21:25:49<31:59:32, 17.90s/it]

 40%|████      | 4308/10740 [21:26:09<32:59:34, 18.47s/it]

 40%|████      | 4309/10740 [21:26:21<29:28:11, 16.50s/it]
{'loss': 0.3959, 'learning_rate': 1.3603895904237334e-06, 'rewards/chosen': -0.9366312026977539, 'rewards/rejected': -1.9645081758499146, 'rewards/accuracies': 0.75, 'rewards/margins': 1.027876853942871, 'policy_logps/rejected': -306.1880798339844, 'policy_logps/chosen': -309.901123046875, 'referece_logps/rejected': -286.5429992675781, 'referece_logps/chosen': -300.5347900390625, 'logits/rejected': -0.8367478847503662, 'logits/chosen': -0.8816641569137573, 'epoch': 2.41}

 40%|████      | 4310/10740 [21:26:40<30:51:10, 17.27s/it]

 40%|████      | 4311/10740 [21:27:00<32:30:56, 18.21s/it]

 40%|████      | 4312/10740 [21:27:18<32:07:07, 17.99s/it]


 40%|████      | 4314/10740 [21:27:57<33:51:07, 18.96s/it]
[2024-04-02 16:41:40,751] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|████      | 4315/10740 [21:28:15<33:11:29, 18.60s/it]
[2024-04-02 16:41:58,491] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3898, 'learning_rate': 1.3587010968370568e-06, 'rewards/chosen': -0.6249663233757019, 'rewards/rejected': -3.2077319622039795, 'rewards/accuracies': 0.875, 'rewards/margins': 2.582765579223633, 'policy_logps/rejected': -304.2047424316406, 'policy_logps/chosen': -314.204345703125, 'referece_logps/rejected': -272.12744140625, 'referece_logps/chosen': -307.9546813964844, 'logits/rejected': -1.3120448589324951, 'logits/chosen': -1.4441039562225342, 'epoch': 2.41}

 40%|████      | 4316/10740 [21:28:28<30:16:07, 16.96s/it]

 40%|████      | 4317/10740 [21:28:48<31:47:30, 17.82s/it]

 40%|████      | 4318/10740 [21:29:08<33:15:49, 18.65s/it]
[2024-04-02 16:43:11,756] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 40%|████      | 4320/10740 [21:29:49<34:51:30, 19.55s/it]

 40%|████      | 4321/10740 [21:30:11<35:57:15, 20.16s/it]

 40%|████      | 4322/10740 [21:30:30<35:51:08, 20.11s/it]
{'loss': 0.262, 'learning_rate': 1.3567297036155335e-06, 'rewards/chosen': -1.141517996788025, 'rewards/rejected': -3.2486419677734375, 'rewards/accuracies': 0.875, 'rewards/margins': 2.107123613357544, 'policy_logps/rejected': -302.228759765625, 'policy_logps/chosen': -252.22293090820312, 'referece_logps/rejected': -269.74237060546875, 'referece_logps/chosen': -240.8077392578125, 'logits/rejected': -0.6841822266578674, 'logits/chosen': -0.7659233212471008, 'epoch': 2.41}


 40%|████      | 4324/10740 [21:31:07<34:08:39, 19.16s/it]

 40%|████      | 4325/10740 [21:31:27<34:57:19, 19.62s/it]
{'loss': 0.4936, 'learning_rate': 1.35588433361232e-06, 'rewards/chosen': -0.8766950964927673, 'rewards/rejected': -2.6300525665283203, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7533574104309082, 'policy_logps/rejected': -377.34063720703125, 'policy_logps/chosen': -346.3414306640625, 'referece_logps/rejected': -351.04010009765625, 'referece_logps/chosen': -337.574462890625, 'logits/rejected': 0.022755712270736694, 'logits/chosen': 0.03729981184005737, 'epoch': 2.42}


 40%|████      | 4327/10740 [21:32:03<32:42:34, 18.36s/it]

 40%|████      | 4328/10740 [21:32:21<32:39:12, 18.33s/it]
{'loss': 0.4363, 'learning_rate': 1.3550386722921476e-06, 'rewards/chosen': -1.3447685241699219, 'rewards/rejected': -3.046255111694336, 'rewards/accuracies': 0.875, 'rewards/margins': 1.701486587524414, 'policy_logps/rejected': -330.43133544921875, 'policy_logps/chosen': -375.36102294921875, 'referece_logps/rejected': -299.96881103515625, 'referece_logps/chosen': -361.9133605957031, 'logits/rejected': -0.7573439478874207, 'logits/chosen': -0.7734256982803345, 'epoch': 2.42}

 40%|████      | 4329/10740 [21:32:40<32:54:02, 18.47s/it]

 40%|████      | 4330/10740 [21:32:59<33:03:14, 18.56s/it]

 40%|████      | 4331/10740 [21:33:18<33:44:57, 18.96s/it]

 40%|████      | 4332/10740 [21:33:35<32:10:22, 18.07s/it]

 40%|████      | 4333/10740 [21:33:54<32:50:53, 18.46s/it]

 40%|████      | 4334/10740 [21:34:10<31:41:53, 17.81s/it]

 40%|████      | 4335/10740 [21:34:33<34:18:15, 19.28s/it]

 40%|████      | 4336/10740 [21:34:44<30:09:27, 16.95s/it]


 40%|████      | 4338/10740 [21:35:12<27:23:53, 15.41s/it]

 40%|████      | 4339/10740 [21:35:31<29:40:51, 16.69s/it]
{'loss': 0.4013, 'learning_rate': 1.3519354329291747e-06, 'rewards/chosen': -1.414165735244751, 'rewards/rejected': -3.3242669105529785, 'rewards/accuracies': 0.875, 'rewards/margins': 1.910101294517517, 'policy_logps/rejected': -385.61309814453125, 'policy_logps/chosen': -300.4205017089844, 'referece_logps/rejected': -352.37042236328125, 'referece_logps/chosen': -286.2788391113281, 'logits/rejected': -1.003084659576416, 'logits/chosen': -1.0552440881729126, 'epoch': 2.42}
[2024-04-02 16:49:36,305] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|████      | 4340/10740 [21:35:53<32:02:54, 18.03s/it]
[2024-04-02 16:49:55,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|████      | 4341/10740 [21:36:12<32:47:36, 18.45s/it]

 40%|████      | 4342/10740 [21:36:26<30:24:08, 17.11s/it]

 40%|████      | 4343/10740 [21:36:42<29:56:50, 16.85s/it]
[2024-04-02 16:50:47,805] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 40%|████      | 4345/10740 [21:37:18<30:06:31, 16.95s/it]
[2024-04-02 16:51:01,491] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4369, 'learning_rate': 1.35024112225283e-06, 'rewards/chosen': -2.107450246810913, 'rewards/rejected': -3.5489025115966797, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4414523839950562, 'policy_logps/rejected': -350.6239318847656, 'policy_logps/chosen': -310.87872314453125, 'referece_logps/rejected': -315.1348876953125, 'referece_logps/chosen': -289.8042297363281, 'logits/rejected': -0.35149893164634705, 'logits/chosen': -0.330549418926239, 'epoch': 2.43}

 40%|████      | 4346/10740 [21:37:34<29:51:08, 16.81s/it]

 40%|████      | 4347/10740 [21:37:55<31:59:40, 18.02s/it]

 40%|████      | 4348/10740 [21:38:13<32:11:01, 18.13s/it]


 41%|████      | 4350/10740 [21:38:50<32:45:11, 18.45s/it]

 41%|████      | 4351/10740 [21:39:12<34:33:46, 19.48s/it]
[2024-04-02 16:52:56,018] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3766, 'learning_rate': 1.3485456647863851e-06, 'rewards/chosen': -1.136887788772583, 'rewards/rejected': -2.746633768081665, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6097460985183716, 'policy_logps/rejected': -410.1431884765625, 'policy_logps/chosen': -385.9989318847656, 'referece_logps/rejected': -382.6768798828125, 'referece_logps/chosen': -374.63006591796875, 'logits/rejected': -0.04153094068169594, 'logits/chosen': -0.0968373715877533, 'epoch': 2.43}
[2024-04-02 16:53:14,633] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 41%|████      | 4353/10740 [21:39:48<33:06:13, 18.66s/it]
{'loss': 0.3713, 'learning_rate': 1.3479802584147213e-06, 'rewards/chosen': -1.383652687072754, 'rewards/rejected': -3.3856382369995117, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0019853115081787, 'policy_logps/rejected': -514.5699462890625, 'policy_logps/chosen': -467.46697998046875, 'referece_logps/rejected': -480.7135314941406, 'referece_logps/chosen': -453.6304931640625, 'logits/rejected': -0.6391788721084595, 'logits/chosen': -0.6538792848587036, 'epoch': 2.43}

 41%|████      | 4354/10740 [21:40:07<33:04:23, 18.64s/it]


 41%|████      | 4356/10740 [21:40:46<34:09:41, 19.26s/it]
[2024-04-02 16:54:29,890] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4087, 'learning_rate': 1.3471319115490912e-06, 'rewards/chosen': -1.0699583292007446, 'rewards/rejected': -2.5381014347076416, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4681432247161865, 'policy_logps/rejected': -491.18206787109375, 'policy_logps/chosen': -515.42578125, 'referece_logps/rejected': -465.80108642578125, 'referece_logps/chosen': -504.7262268066406, 'logits/rejected': -0.20001348853111267, 'logits/chosen': -0.19785277545452118, 'epoch': 2.43}

 41%|████      | 4357/10740 [21:41:01<31:46:26, 17.92s/it]


 41%|████      | 4359/10740 [21:41:32<29:09:57, 16.45s/it]
{'loss': 0.4574, 'learning_rate': 1.3462832805309904e-06, 'rewards/chosen': -1.393505573272705, 'rewards/rejected': -2.150742530822754, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7572368383407593, 'policy_logps/rejected': -300.13427734375, 'policy_logps/chosen': -286.491943359375, 'referece_logps/rejected': -278.6268310546875, 'referece_logps/chosen': -272.556884765625, 'logits/rejected': -0.8485367894172668, 'logits/chosen': -0.9000734090805054, 'epoch': 2.44}
[2024-04-02 16:55:35,457] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 41%|████      | 4361/10740 [21:42:11<31:39:10, 17.86s/it]

 41%|████      | 4362/10740 [21:42:26<30:20:13, 17.12s/it]

 41%|████      | 4363/10740 [21:42:41<29:02:32, 16.40s/it]
{'loss': 0.5149, 'learning_rate': 1.3451513316925915e-06, 'rewards/chosen': -2.4189281463623047, 'rewards/rejected': -2.8318095207214355, 'rewards/accuracies': 0.75, 'rewards/margins': 0.41288086771965027, 'policy_logps/rejected': -415.24383544921875, 'policy_logps/chosen': -578.8356323242188, 'referece_logps/rejected': -386.92578125, 'referece_logps/chosen': -554.6464233398438, 'logits/rejected': 0.21265490353107452, 'logits/chosen': 0.08057297766208649, 'epoch': 2.44}
[2024-04-02 16:56:47,142] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 41%|████      | 4365/10740 [21:43:24<33:36:54, 18.98s/it]
[2024-04-02 16:57:07,718] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████      | 4366/10740 [21:43:44<34:17:33, 19.37s/it]
[2024-04-02 16:57:27,986] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5203, 'learning_rate': 1.3443020403539374e-06, 'rewards/chosen': -0.91825932264328, 'rewards/rejected': -1.9425663948059082, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0243070125579834, 'policy_logps/rejected': -367.348876953125, 'policy_logps/chosen': -367.37603759765625, 'referece_logps/rejected': -347.9231872558594, 'referece_logps/chosen': -358.1934509277344, 'logits/rejected': -0.8270354270935059, 'logits/chosen': -0.7690013647079468, 'epoch': 2.44}

 41%|████      | 4367/10740 [21:44:01<33:03:39, 18.68s/it]

 41%|████      | 4368/10740 [21:44:18<31:48:33, 17.97s/it]

 41%|████      | 4369/10740 [21:44:39<33:32:35, 18.95s/it]

 41%|████      | 4370/10740 [21:44:52<30:19:21, 17.14s/it]

 41%|████      | 4371/10740 [21:45:12<31:49:35, 17.99s/it]


 41%|████      | 4373/10740 [21:45:48<31:48:39, 17.99s/it]

 41%|████      | 4374/10740 [21:46:08<33:03:31, 18.69s/it]
[2024-04-02 16:59:52,205] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3296, 'learning_rate': 1.34203588747483e-06, 'rewards/chosen': -1.5667266845703125, 'rewards/rejected': -2.3118412494659424, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7451145648956299, 'policy_logps/rejected': -314.4015808105469, 'policy_logps/chosen': -287.8409423828125, 'referece_logps/rejected': -291.283203125, 'referece_logps/chosen': -272.17364501953125, 'logits/rejected': -0.4953985810279846, 'logits/chosen': -0.5673425793647766, 'epoch': 2.44}

 41%|████      | 4375/10740 [21:46:28<33:31:15, 18.96s/it]
[2024-04-02 17:00:32,765] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████      | 4376/10740 [21:46:49<34:35:21, 19.57s/it]

 41%|████      | 4377/10740 [21:47:08<34:02:51, 19.26s/it]


 41%|████      | 4379/10740 [21:47:39<30:03:21, 17.01s/it]
{'loss': 0.4498, 'learning_rate': 1.3406185300454664e-06, 'rewards/chosen': -1.2847075462341309, 'rewards/rejected': -2.436595916748047, 'rewards/accuracies': 0.625, 'rewards/margins': 1.151888132095337, 'policy_logps/rejected': -308.3365783691406, 'policy_logps/chosen': -262.7787780761719, 'referece_logps/rejected': -283.97064208984375, 'referece_logps/chosen': -249.93167114257812, 'logits/rejected': -1.474901795387268, 'logits/chosen': -1.4358242750167847, 'epoch': 2.45}

 41%|████      | 4380/10740 [21:48:02<33:00:38, 18.69s/it]

 41%|████      | 4381/10740 [21:48:17<31:28:02, 17.81s/it]


 41%|████      | 4383/10740 [21:48:52<31:17:59, 17.73s/it]
{'loss': 0.3629, 'learning_rate': 1.3394840863056916e-06, 'rewards/chosen': -1.2292253971099854, 'rewards/rejected': -3.21766996383667, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9884448051452637, 'policy_logps/rejected': -511.8336181640625, 'policy_logps/chosen': -401.3644104003906, 'referece_logps/rejected': -479.6568603515625, 'referece_logps/chosen': -389.0721435546875, 'logits/rejected': -0.7531699538230896, 'logits/chosen': -0.7747171521186829, 'epoch': 2.45}

 41%|████      | 4384/10740 [21:49:04<28:05:37, 15.91s/it]


 41%|████      | 4386/10740 [21:49:44<31:57:03, 18.10s/it]
{'loss': 0.4441, 'learning_rate': 1.33863292920303e-06, 'rewards/chosen': -1.347609043121338, 'rewards/rejected': -2.626061201095581, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2784521579742432, 'policy_logps/rejected': -556.49853515625, 'policy_logps/chosen': -349.41363525390625, 'referece_logps/rejected': -530.2379760742188, 'referece_logps/chosen': -335.9375305175781, 'logits/rejected': -0.6568636894226074, 'logits/chosen': -0.5662136077880859, 'epoch': 2.45}

 41%|████      | 4387/10740 [21:49:55<28:10:33, 15.97s/it]


 41%|████      | 4389/10740 [21:50:27<28:46:39, 16.31s/it]
{'loss': 0.4127, 'learning_rate': 1.3377814949049274e-06, 'rewards/chosen': -0.7725353837013245, 'rewards/rejected': -1.9553691148757935, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1828337907791138, 'policy_logps/rejected': -351.6733093261719, 'policy_logps/chosen': -343.76458740234375, 'referece_logps/rejected': -332.1195983886719, 'referece_logps/chosen': -336.0392150878906, 'logits/rejected': -0.3124516010284424, 'logits/chosen': -0.291756272315979, 'epoch': 2.45}


 41%|████      | 4391/10740 [21:51:04<31:27:35, 17.84s/it]
{'loss': 0.3982, 'learning_rate': 1.3372137183862261e-06, 'rewards/chosen': -1.1813091039657593, 'rewards/rejected': -2.531710386276245, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3504012823104858, 'policy_logps/rejected': -505.9563293457031, 'policy_logps/chosen': -458.7717590332031, 'referece_logps/rejected': -480.6391906738281, 'referece_logps/chosen': -446.9585876464844, 'logits/rejected': -0.04365530610084534, 'logits/chosen': 0.0730782151222229, 'epoch': 2.45}

 41%|████      | 4392/10740 [21:51:22<31:07:46, 17.65s/it]


 41%|████      | 4394/10740 [21:52:01<32:42:06, 18.55s/it]

 41%|████      | 4395/10740 [21:52:21<33:18:59, 18.90s/it]
{'loss': 0.4377, 'learning_rate': 1.3360777975104638e-06, 'rewards/chosen': -2.5647428035736084, 'rewards/rejected': -4.653929710388184, 'rewards/accuracies': 0.875, 'rewards/margins': 2.089186906814575, 'policy_logps/rejected': -435.0116271972656, 'policy_logps/chosen': -363.2298583984375, 'referece_logps/rejected': -388.4723205566406, 'referece_logps/chosen': -337.58245849609375, 'logits/rejected': -0.5746584534645081, 'logits/chosen': -0.683853805065155, 'epoch': 2.46}


 41%|████      | 4397/10740 [21:53:01<34:43:09, 19.71s/it]
{'loss': 0.3243, 'learning_rate': 1.3355096535666614e-06, 'rewards/chosen': -2.110370397567749, 'rewards/rejected': -4.010141372680664, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8997706174850464, 'policy_logps/rejected': -498.0011291503906, 'policy_logps/chosen': -476.0924072265625, 'referece_logps/rejected': -457.89971923828125, 'referece_logps/chosen': -454.9887390136719, 'logits/rejected': 0.29465246200561523, 'logits/chosen': 0.3544248938560486, 'epoch': 2.46}


 41%|████      | 4399/10740 [21:53:37<33:25:29, 18.98s/it]
{'loss': 0.4027, 'learning_rate': 1.334941387561159e-06, 'rewards/chosen': -1.4965991973876953, 'rewards/rejected': -2.8085684776306152, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3119690418243408, 'policy_logps/rejected': -310.40045166015625, 'policy_logps/chosen': -359.48626708984375, 'referece_logps/rejected': -282.31475830078125, 'referece_logps/chosen': -344.5202941894531, 'logits/rejected': 0.059509385377168655, 'logits/chosen': 0.1997191309928894, 'epoch': 2.46}

 41%|████      | 4400/10740 [21:53:56<33:34:37, 19.07s/it]

 41%|████      | 4401/10740 [21:54:10<30:50:28, 17.52s/it]


 41%|████      | 4403/10740 [21:54:47<32:14:04, 18.31s/it]

 41%|████      | 4404/10740 [21:55:07<33:03:14, 18.78s/it]
{'loss': 0.4801, 'learning_rate': 1.3335201898843078e-06, 'rewards/chosen': -1.2108403444290161, 'rewards/rejected': -2.487891435623169, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2770512104034424, 'policy_logps/rejected': -361.31243896484375, 'policy_logps/chosen': -489.4289855957031, 'referece_logps/rejected': -336.4335021972656, 'referece_logps/chosen': -477.320556640625, 'logits/rejected': -0.9268784523010254, 'logits/chosen': -1.0027344226837158, 'epoch': 2.46}

 41%|████      | 4405/10740 [21:55:18<28:49:27, 16.38s/it]


 41%|████      | 4407/10740 [21:55:47<27:08:45, 15.43s/it]
{'loss': 0.4311, 'learning_rate': 1.332667107057623e-06, 'rewards/chosen': -1.3400192260742188, 'rewards/rejected': -2.3981165885925293, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0580971240997314, 'policy_logps/rejected': -279.03369140625, 'policy_logps/chosen': -349.6903991699219, 'referece_logps/rejected': -255.0525360107422, 'referece_logps/chosen': -336.29022216796875, 'logits/rejected': -1.271209478378296, 'logits/chosen': -1.347322702407837, 'epoch': 2.46}


 41%|████      | 4409/10740 [21:56:19<27:43:45, 15.77s/it]
{'loss': 0.4079, 'learning_rate': 1.3320982338456146e-06, 'rewards/chosen': -0.8532055020332336, 'rewards/rejected': -2.0194015502929688, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1661961078643799, 'policy_logps/rejected': -334.7355651855469, 'policy_logps/chosen': -536.3959350585938, 'referece_logps/rejected': -314.54156494140625, 'referece_logps/chosen': -527.8638916015625, 'logits/rejected': -0.11642506718635559, 'logits/chosen': -0.15343284606933594, 'epoch': 2.46}


 41%|████      | 4411/10740 [21:56:47<25:41:29, 14.61s/it]

 41%|████      | 4412/10740 [21:57:08<28:44:44, 16.35s/it]

 41%|████      | 4413/10740 [21:57:31<32:37:40, 18.56s/it]
[2024-04-02 17:11:15,093] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████      | 4414/10740 [21:57:51<33:15:52, 18.93s/it]
[2024-04-02 17:11:34,875] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4854, 'learning_rate': 1.3306755226783387e-06, 'rewards/chosen': -1.3424071073532104, 'rewards/rejected': -2.329697847366333, 'rewards/accuracies': 0.875, 'rewards/margins': 0.987290620803833, 'policy_logps/rejected': -501.034423828125, 'policy_logps/chosen': -399.8196716308594, 'referece_logps/rejected': -477.7374267578125, 'referece_logps/chosen': -386.3956298828125, 'logits/rejected': -0.45031285285949707, 'logits/chosen': -0.5079054832458496, 'epoch': 2.47}

 41%|████      | 4415/10740 [21:58:11<33:37:27, 19.14s/it]


 41%|████      | 4417/10740 [21:58:47<33:33:39, 19.11s/it]
[2024-04-02 17:12:31,170] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████      | 4418/10740 [21:59:07<33:55:20, 19.32s/it]

 41%|████      | 4419/10740 [21:59:25<33:19:35, 18.98s/it]

 41%|████      | 4420/10740 [21:59:45<33:45:54, 19.23s/it]

 41%|████      | 4421/10740 [22:00:01<31:58:22, 18.22s/it]

 41%|████      | 4422/10740 [22:00:21<33:00:26, 18.81s/it]
{'loss': 0.4397, 'learning_rate': 1.3283976222183851e-06, 'rewards/chosen': -0.5569406151771545, 'rewards/rejected': -1.7919669151306152, 'rewards/accuracies': 0.875, 'rewards/margins': 1.235026240348816, 'policy_logps/rejected': -416.5354919433594, 'policy_logps/chosen': -385.2503356933594, 'referece_logps/rejected': -398.61578369140625, 'referece_logps/chosen': -379.6808776855469, 'logits/rejected': -0.5515381097793579, 'logits/chosen': -0.5441562533378601, 'epoch': 2.47}

 41%|████      | 4423/10740 [22:00:40<33:04:03, 18.84s/it]

 41%|████      | 4424/10740 [22:00:58<32:42:47, 18.65s/it]

 41%|████      | 4425/10740 [22:01:19<33:46:09, 19.25s/it]

 41%|████      | 4426/10740 [22:01:37<32:51:08, 18.73s/it]
[2024-04-02 17:15:40,858] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 41%|████      | 4428/10740 [22:02:18<34:24:16, 19.62s/it]

 41%|████      | 4429/10740 [22:02:37<34:25:41, 19.64s/it]

 41%|████      | 4430/10740 [22:02:54<32:54:32, 18.78s/it]
[2024-04-02 17:16:37,730] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4431/10740 [22:03:14<33:27:40, 19.09s/it]

 41%|████▏     | 4432/10740 [22:03:33<33:42:17, 19.24s/it]
{'loss': 0.4059, 'learning_rate': 1.3255475600284216e-06, 'rewards/chosen': -1.7138508558273315, 'rewards/rejected': -2.998387098312378, 'rewards/accuracies': 0.625, 'rewards/margins': 1.284536361694336, 'policy_logps/rejected': -450.5946044921875, 'policy_logps/chosen': -470.18572998046875, 'referece_logps/rejected': -420.61077880859375, 'referece_logps/chosen': -453.04718017578125, 'logits/rejected': 0.35213348269462585, 'logits/chosen': 0.33477914333343506, 'epoch': 2.48}

 41%|████▏     | 4433/10740 [22:03:47<30:38:48, 17.49s/it]


 41%|████▏     | 4435/10740 [22:04:28<33:17:16, 19.01s/it]
[2024-04-02 17:18:11,347] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4436/10740 [22:04:40<29:47:05, 17.01s/it]
{'loss': 0.3621, 'learning_rate': 1.324406704638748e-06, 'rewards/chosen': -1.2625598907470703, 'rewards/rejected': -2.1760549545288086, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9134953022003174, 'policy_logps/rejected': -429.7513732910156, 'policy_logps/chosen': -343.41436767578125, 'referece_logps/rejected': -407.9908142089844, 'referece_logps/chosen': -330.7887878417969, 'logits/rejected': -1.426841378211975, 'logits/chosen': -1.529665231704712, 'epoch': 2.48}
[2024-04-02 17:18:38,971] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4437/10740 [22:04:55<28:52:10, 16.49s/it]

 41%|████▏     | 4438/10740 [22:05:08<27:02:21, 15.45s/it]


 41%|████▏     | 4440/10740 [22:05:40<27:18:04, 15.60s/it]
{'loss': 0.4899, 'learning_rate': 1.3232653771597627e-06, 'rewards/chosen': -1.276784896850586, 'rewards/rejected': -3.4429218769073486, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1661367416381836, 'policy_logps/rejected': -328.4383239746094, 'policy_logps/chosen': -368.8396301269531, 'referece_logps/rejected': -294.0091247558594, 'referece_logps/chosen': -356.07183837890625, 'logits/rejected': 0.18783865869045258, 'logits/chosen': 0.32570451498031616, 'epoch': 2.48}
[2024-04-02 17:19:42,416] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4441/10740 [22:05:59<28:51:49, 16.50s/it]


 41%|████▏     | 4443/10740 [22:06:28<27:03:09, 15.47s/it]
{'loss': 0.3424, 'learning_rate': 1.3224090727410241e-06, 'rewards/chosen': -2.1013808250427246, 'rewards/rejected': -2.674377679824829, 'rewards/accuracies': 0.75, 'rewards/margins': 0.572996973991394, 'policy_logps/rejected': -385.81085205078125, 'policy_logps/chosen': -370.3739013671875, 'referece_logps/rejected': -359.06707763671875, 'referece_logps/chosen': -349.3601379394531, 'logits/rejected': -0.2574232518672943, 'logits/chosen': -0.2525795102119446, 'epoch': 2.48}

 41%|████▏     | 4444/10740 [22:06:44<27:30:08, 15.73s/it]
[2024-04-02 17:20:49,218] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4445/10740 [22:07:05<30:17:54, 17.33s/it]

 41%|████▏     | 4446/10740 [22:07:28<32:46:47, 18.75s/it]

 41%|████▏     | 4447/10740 [22:07:40<29:31:34, 16.89s/it]
[2024-04-02 17:21:23,841] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4448/10740 [22:08:00<30:56:15, 17.70s/it]
{'loss': 0.385, 'learning_rate': 1.3209813125781564e-06, 'rewards/chosen': -1.894364595413208, 'rewards/rejected': -3.182156562805176, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2877917289733887, 'policy_logps/rejected': -415.797607421875, 'policy_logps/chosen': -406.69049072265625, 'referece_logps/rejected': -383.9759826660156, 'referece_logps/chosen': -387.7468566894531, 'logits/rejected': -1.0919352769851685, 'logits/chosen': -1.0106161832809448, 'epoch': 2.48}


 41%|████▏     | 4450/10740 [22:08:34<30:17:01, 17.33s/it]
{'loss': 0.4603, 'learning_rate': 1.3204100039729263e-06, 'rewards/chosen': -1.8195182085037231, 'rewards/rejected': -2.1334638595581055, 'rewards/accuracies': 0.5, 'rewards/margins': 0.31394538283348083, 'policy_logps/rejected': -441.3747253417969, 'policy_logps/chosen': -644.3616943359375, 'referece_logps/rejected': -420.04010009765625, 'referece_logps/chosen': -626.16650390625, 'logits/rejected': -0.4166618883609772, 'logits/chosen': -0.6632022261619568, 'epoch': 2.49}


 41%|████▏     | 4452/10740 [22:09:10<31:12:59, 17.87s/it]
{'loss': 0.413, 'learning_rate': 1.319838578799396e-06, 'rewards/chosen': -1.9163522720336914, 'rewards/rejected': -3.746729850769043, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8303775787353516, 'policy_logps/rejected': -382.9456787109375, 'policy_logps/chosen': -351.47576904296875, 'referece_logps/rejected': -345.4783630371094, 'referece_logps/chosen': -332.3122863769531, 'logits/rejected': -0.5598599314689636, 'logits/chosen': -0.6407292485237122, 'epoch': 2.49}

 41%|████▏     | 4453/10740 [22:09:31<32:41:33, 18.72s/it]

 41%|████▏     | 4454/10740 [22:09:45<30:24:00, 17.41s/it]


 41%|████▏     | 4456/10740 [22:10:16<28:53:04, 16.55s/it]
[2024-04-02 17:24:00,108] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 41%|████▏     | 4457/10740 [22:10:32<28:34:13, 16.37s/it]
{'loss': 0.3652, 'learning_rate': 1.3184095072436414e-06, 'rewards/chosen': -1.8991461992263794, 'rewards/rejected': -2.613973617553711, 'rewards/accuracies': 0.875, 'rewards/margins': 0.714827299118042, 'policy_logps/rejected': -263.0462951660156, 'policy_logps/chosen': -275.88275146484375, 'referece_logps/rejected': -236.90655517578125, 'referece_logps/chosen': -256.89129638671875, 'logits/rejected': -0.7112338542938232, 'logits/chosen': -0.7765728235244751, 'epoch': 2.49}

 42%|████▏     | 4458/10740 [22:10:53<30:53:46, 17.71s/it]

 42%|████▏     | 4459/10740 [22:11:13<31:55:49, 18.30s/it]

 42%|████▏     | 4460/10740 [22:11:32<32:10:11, 18.44s/it]

 42%|████▏     | 4461/10740 [22:11:46<29:48:37, 17.09s/it]

 42%|████▏     | 4462/10740 [22:12:06<31:22:00, 17.99s/it]

 42%|████▏     | 4463/10740 [22:12:23<31:17:12, 17.94s/it]


 42%|████▏     | 4465/10740 [22:13:03<32:59:49, 18.93s/it]
{'loss': 0.4632, 'learning_rate': 1.316121488180562e-06, 'rewards/chosen': -1.0251903533935547, 'rewards/rejected': -2.9962966442108154, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9711061716079712, 'policy_logps/rejected': -447.8931884765625, 'policy_logps/chosen': -479.17877197265625, 'referece_logps/rejected': -417.9302062988281, 'referece_logps/chosen': -468.9268493652344, 'logits/rejected': 0.40255817770957947, 'logits/chosen': 0.35213950276374817, 'epoch': 2.49}

 42%|████▏     | 4466/10740 [22:13:13<28:40:51, 16.46s/it]


 42%|████▏     | 4468/10740 [22:13:44<28:12:04, 16.19s/it]
{'loss': 0.4618, 'learning_rate': 1.3152630059080383e-06, 'rewards/chosen': -1.835554599761963, 'rewards/rejected': -2.578429937362671, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7428750991821289, 'policy_logps/rejected': -521.2356567382812, 'policy_logps/chosen': -458.21234130859375, 'referece_logps/rejected': -495.45135498046875, 'referece_logps/chosen': -439.85675048828125, 'logits/rejected': -0.8807617425918579, 'logits/chosen': -0.8692008256912231, 'epoch': 2.5}

 42%|████▏     | 4469/10740 [22:13:55<25:18:10, 14.53s/it]

 42%|████▏     | 4470/10740 [22:14:08<24:24:03, 14.01s/it]

 42%|████▏     | 4471/10740 [22:14:28<27:19:53, 15.70s/it]


 42%|████▏     | 4473/10740 [22:15:05<29:36:58, 17.01s/it]
{'loss': 0.3582, 'learning_rate': 1.3138316289887854e-06, 'rewards/chosen': -1.4534868001937866, 'rewards/rejected': -3.2683897018432617, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8149030208587646, 'policy_logps/rejected': -338.1085205078125, 'policy_logps/chosen': -281.3646545410156, 'referece_logps/rejected': -305.4245910644531, 'referece_logps/chosen': -266.82977294921875, 'logits/rejected': -0.25840020179748535, 'logits/chosen': -0.3789333701133728, 'epoch': 2.5}
[2024-04-02 17:29:09,430] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4474/10740 [22:15:26<31:43:44, 18.23s/it]

 42%|████▏     | 4475/10740 [22:15:46<32:46:14, 18.83s/it]

 42%|████▏     | 4476/10740 [22:16:00<30:12:36, 17.36s/it]

 42%|████▏     | 4477/10740 [22:16:18<30:28:26, 17.52s/it]

 42%|████▏     | 4478/10740 [22:16:33<29:33:03, 16.99s/it]

 42%|████▏     | 4479/10740 [22:16:53<31:01:22, 17.84s/it]

 42%|████▏     | 4480/10740 [22:17:12<31:29:33, 18.11s/it]

 42%|████▏     | 4481/10740 [22:17:31<31:43:33, 18.25s/it]
[2024-04-02 17:31:35,831] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4482/10740 [22:17:52<33:23:49, 19.21s/it]
[2024-04-02 17:31:57,437] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4483/10740 [22:18:14<34:38:25, 19.93s/it]

 42%|████▏     | 4484/10740 [22:18:26<30:46:56, 17.71s/it]

 42%|████▏     | 4485/10740 [22:18:43<30:30:14, 17.56s/it]

 42%|████▏     | 4486/10740 [22:19:06<33:01:28, 19.01s/it]
[2024-04-02 17:33:08,965] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4487/10740 [22:19:25<33:13:11, 19.13s/it]

 42%|████▏     | 4488/10740 [22:19:43<32:41:37, 18.83s/it]

 42%|████▏     | 4489/10740 [22:19:59<30:56:28, 17.82s/it]

 42%|████▏     | 4490/10740 [22:20:14<29:21:10, 16.91s/it]

 42%|████▏     | 4491/10740 [22:20:29<28:45:15, 16.57s/it]


 42%|████▏     | 4493/10740 [22:20:59<26:51:09, 15.47s/it]
{'loss': 0.5106, 'learning_rate': 1.3080990179450956e-06, 'rewards/chosen': -2.182640552520752, 'rewards/rejected': -2.0242700576782227, 'rewards/accuracies': 0.5, 'rewards/margins': -0.15837030112743378, 'policy_logps/rejected': -404.5085144042969, 'policy_logps/chosen': -438.0976867675781, 'referece_logps/rejected': -384.26580810546875, 'referece_logps/chosen': -416.2712707519531, 'logits/rejected': -0.5647159218788147, 'logits/chosen': -0.5433464646339417, 'epoch': 2.51}

 42%|████▏     | 4494/10740 [22:21:11<24:59:30, 14.40s/it]


 42%|████▏     | 4496/10740 [22:21:33<21:56:24, 12.65s/it]

 42%|████▏     | 4497/10740 [22:21:55<26:42:09, 15.40s/it]

 42%|████▏     | 4498/10740 [22:22:09<26:10:19, 15.09s/it]

 42%|████▏     | 4499/10740 [22:22:31<29:34:03, 17.06s/it]

 42%|████▏     | 4500/10740 [22:22:52<31:51:11, 18.38s/it]

 42%|████▏     | 4501/10740 [22:23:20<36:36:15, 21.12s/it]

 42%|████▏     | 4502/10740 [22:23:35<33:45:59, 19.49s/it]

 42%|████▏     | 4503/10740 [22:23:57<34:36:51, 19.98s/it]

 42%|████▏     | 4504/10740 [22:24:12<32:26:18, 18.73s/it]

 42%|████▏     | 4505/10740 [22:24:25<29:04:59, 16.79s/it]

 42%|████▏     | 4506/10740 [22:24:38<27:19:03, 15.78s/it]

 42%|████▏     | 4507/10740 [22:24:58<29:19:58, 16.94s/it]
[2024-04-02 17:38:41,457] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4508/10740 [22:25:10<27:07:33, 15.67s/it]

 42%|████▏     | 4509/10740 [22:25:32<29:57:23, 17.31s/it]
[2024-04-02 17:39:15,288] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4510/10740 [22:25:53<31:52:54, 18.42s/it]
[2024-04-02 17:39:36,313] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4511/10740 [22:26:14<33:23:59, 19.30s/it]

 42%|████▏     | 4512/10740 [22:26:33<33:23:28, 19.30s/it]
[2024-04-02 17:40:16,966] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4513/10740 [22:26:54<34:09:29, 19.75s/it]
[2024-04-02 17:40:37,757] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4514/10740 [22:27:14<34:05:51, 19.72s/it]

 42%|████▏     | 4515/10740 [22:27:25<29:37:36, 17.13s/it]

 42%|████▏     | 4516/10740 [22:27:37<27:10:46, 15.72s/it]

 42%|████▏     | 4517/10740 [22:27:55<28:09:04, 16.29s/it]

 42%|████▏     | 4518/10740 [22:28:15<29:55:53, 17.32s/it]

 42%|████▏     | 4519/10740 [22:28:33<30:32:41, 17.68s/it]

 42%|████▏     | 4520/10740 [22:28:54<32:05:36, 18.57s/it]

 42%|████▏     | 4521/10740 [22:29:13<32:37:20, 18.88s/it]

 42%|████▏     | 4522/10740 [22:29:31<32:14:05, 18.66s/it]

 42%|████▏     | 4523/10740 [22:29:51<32:44:33, 18.96s/it]

 42%|████▏     | 4524/10740 [22:30:11<33:10:55, 19.22s/it]

 42%|████▏     | 4525/10740 [22:30:26<31:03:03, 17.99s/it]

 42%|████▏     | 4526/10740 [22:30:47<32:39:37, 18.92s/it]

 42%|████▏     | 4527/10740 [22:31:05<32:13:37, 18.67s/it]

 42%|████▏     | 4528/10740 [22:31:25<32:59:06, 19.12s/it]

 42%|████▏     | 4529/10740 [22:31:47<34:08:10, 19.79s/it]

 42%|████▏     | 4530/10740 [22:32:08<34:46:22, 20.16s/it]

 42%|████▏     | 4531/10740 [22:32:31<36:14:58, 21.02s/it]
[2024-04-02 17:46:14,528] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4532/10740 [22:32:49<34:51:41, 20.22s/it]
[2024-04-02 17:46:32,873] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4533/10740 [22:33:12<36:21:33, 21.09s/it]

 42%|████▏     | 4534/10740 [22:33:31<35:06:05, 20.36s/it]

 42%|████▏     | 4535/10740 [22:33:48<33:18:25, 19.32s/it]
[2024-04-02 17:47:31,565] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4536/10740 [22:34:06<32:46:05, 19.01s/it]

 42%|████▏     | 4537/10740 [22:34:28<34:00:57, 19.74s/it]
[2024-04-02 17:48:11,297] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4538/10740 [22:34:44<32:33:28, 18.90s/it]
[2024-04-02 17:48:28,228] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4539/10740 [22:35:03<32:16:46, 18.74s/it]

 42%|████▏     | 4540/10740 [22:35:16<29:10:03, 16.94s/it]

 42%|████▏     | 4541/10740 [22:35:36<30:45:22, 17.86s/it]

 42%|████▏     | 4542/10740 [22:35:54<31:04:18, 18.05s/it]

 42%|████▏     | 4543/10740 [22:36:11<30:40:25, 17.82s/it]

 42%|████▏     | 4544/10740 [22:36:33<32:48:33, 19.06s/it]

 42%|████▏     | 4545/10740 [22:36:53<33:12:49, 19.30s/it]

 42%|████▏     | 4546/10740 [22:37:13<33:26:21, 19.44s/it]

 42%|████▏     | 4547/10740 [22:37:28<31:21:20, 18.23s/it]

 42%|████▏     | 4548/10740 [22:37:51<33:32:09, 19.50s/it]
[2024-04-02 17:51:34,553] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4549/10740 [22:38:11<33:47:19, 19.65s/it]

 42%|████▏     | 4550/10740 [22:38:23<29:53:19, 17.38s/it]

 42%|████▏     | 4551/10740 [22:38:40<29:56:30, 17.42s/it]

 42%|████▏     | 4552/10740 [22:38:55<28:24:24, 16.53s/it]

 42%|████▏     | 4553/10740 [22:39:10<27:32:35, 16.03s/it]

 42%|████▏     | 4554/10740 [22:39:22<25:22:34, 14.77s/it]

 42%|████▏     | 4555/10740 [22:39:43<28:35:21, 16.64s/it]

 42%|████▏     | 4556/10740 [22:40:01<29:31:28, 17.19s/it]

 42%|████▏     | 4557/10740 [22:40:18<29:09:48, 16.98s/it]

 42%|████▏     | 4558/10740 [22:40:39<31:29:24, 18.34s/it]

 42%|████▏     | 4559/10740 [22:40:59<32:31:09, 18.94s/it]
[2024-04-02 17:54:43,107] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4560/10740 [22:41:22<34:11:17, 19.92s/it]

 42%|████▏     | 4561/10740 [22:41:44<35:31:50, 20.70s/it]

 42%|████▏     | 4562/10740 [22:42:04<35:13:05, 20.52s/it]

 42%|████▏     | 4563/10740 [22:42:16<30:51:54, 17.99s/it]

 42%|████▏     | 4564/10740 [22:42:38<32:39:21, 19.04s/it]

 43%|████▎     | 4565/10740 [22:43:01<34:49:47, 20.31s/it]
[2024-04-02 17:56:44,761] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4566/10740 [22:43:19<33:40:37, 19.64s/it]
[2024-04-02 17:57:02,837] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4567/10740 [22:43:32<30:15:30, 17.65s/it]

 43%|████▎     | 4568/10740 [22:43:52<31:18:06, 18.26s/it]
[2024-04-02 17:57:35,523] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4569/10740 [22:44:03<27:26:15, 16.01s/it]

 43%|████▎     | 4570/10740 [22:44:16<26:03:10, 15.20s/it]

 43%|████▎     | 4571/10740 [22:44:27<23:42:49, 13.84s/it]

 43%|████▎     | 4572/10740 [22:44:50<28:25:11, 16.59s/it]

 43%|████▎     | 4573/10740 [22:45:09<30:09:13, 17.60s/it]

 43%|████▎     | 4574/10740 [22:45:23<28:11:44, 16.46s/it]
{'loss': 0.5168, 'learning_rate': 1.2847694927654144e-06, 'rewards/chosen': -2.0356202125549316, 'rewards/rejected': -4.118954181671143, 'rewards/accuracies': 0.875, 'rewards/margins': 2.083333730697632, 'policy_logps/rejected': -584.01611328125, 'policy_logps/chosen': -477.75579833984375, 'referece_logps/rejected': -542.82666015625, 'referece_logps/chosen': -457.39959716796875, 'logits/rejected': -0.011850185692310333, 'logits/chosen': 0.08964497596025467, 'epoch': 2.56}


 43%|████▎     | 4576/10740 [22:46:06<32:57:33, 19.25s/it]
{'loss': 0.4858, 'learning_rate': 1.2841912480025347e-06, 'rewards/chosen': -1.8166435956954956, 'rewards/rejected': -2.5786070823669434, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7619637250900269, 'policy_logps/rejected': -348.64788818359375, 'policy_logps/chosen': -421.9996643066406, 'referece_logps/rejected': -322.8618469238281, 'referece_logps/chosen': -403.833251953125, 'logits/rejected': -0.6383442878723145, 'logits/chosen': -0.6959044337272644, 'epoch': 2.56}


 43%|████▎     | 4578/10740 [22:46:50<34:52:25, 20.37s/it]
{'loss': 0.3454, 'learning_rate': 1.2836128998480932e-06, 'rewards/chosen': -1.684072494506836, 'rewards/rejected': -4.237743377685547, 'rewards/accuracies': 0.875, 'rewards/margins': 2.553670644760132, 'policy_logps/rejected': -358.2071533203125, 'policy_logps/chosen': -273.9348449707031, 'referece_logps/rejected': -315.8297119140625, 'referece_logps/chosen': -257.0941162109375, 'logits/rejected': -0.8216755390167236, 'logits/chosen': -0.8376235961914062, 'epoch': 2.56}


 43%|████▎     | 4580/10740 [22:47:28<33:23:38, 19.52s/it]

 43%|████▎     | 4581/10740 [22:47:48<33:49:51, 19.77s/it]

 43%|████▎     | 4582/10740 [22:48:06<32:39:15, 19.09s/it]

 43%|████▎     | 4583/10740 [22:48:25<32:58:36, 19.28s/it]

 43%|████▎     | 4584/10740 [22:48:42<31:34:06, 18.46s/it]

 43%|████▎     | 4585/10740 [22:48:57<29:38:18, 17.34s/it]

 43%|████▎     | 4586/10740 [22:49:17<31:05:26, 18.19s/it]

 43%|████▎     | 4587/10740 [22:49:35<30:55:27, 18.09s/it]

 43%|████▎     | 4588/10740 [22:49:57<32:51:26, 19.23s/it]
[2024-04-02 18:03:40,391] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4447, 'learning_rate': 1.2807196155680823e-06, 'rewards/chosen': -1.9960880279541016, 'rewards/rejected': -4.063140869140625, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0670533180236816, 'policy_logps/rejected': -408.7470397949219, 'policy_logps/chosen': -381.8057861328125, 'referece_logps/rejected': -368.11566162109375, 'referece_logps/chosen': -361.8448791503906, 'logits/rejected': -0.16694198548793793, 'logits/chosen': -0.10904109477996826, 'epoch': 2.56}
[2024-04-02 18:03:53,004] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 43%|████▎     | 4590/10740 [22:50:29<30:42:36, 17.98s/it]

 43%|████▎     | 4591/10740 [22:50:50<32:08:19, 18.82s/it]

 43%|████▎     | 4592/10740 [22:51:10<32:37:58, 19.11s/it]

 43%|████▎     | 4593/10740 [22:51:31<33:46:39, 19.78s/it]

 43%|████▎     | 4594/10740 [22:51:45<31:06:30, 18.22s/it]

 43%|████▎     | 4595/10740 [22:52:02<30:07:31, 17.65s/it]

 43%|████▎     | 4596/10740 [22:52:20<30:33:04, 17.90s/it]

 43%|████▎     | 4597/10740 [22:52:37<29:42:15, 17.41s/it]

 43%|████▎     | 4598/10740 [22:52:56<30:55:18, 18.12s/it]
{'loss': 0.5163, 'learning_rate': 1.2778237780762397e-06, 'rewards/chosen': -1.8264861106872559, 'rewards/rejected': -2.6457011699676514, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8192150592803955, 'policy_logps/rejected': -421.903564453125, 'policy_logps/chosen': -348.65740966796875, 'referece_logps/rejected': -395.446533203125, 'referece_logps/chosen': -330.392578125, 'logits/rejected': -0.32936397194862366, 'logits/chosen': -0.1863033026456833, 'epoch': 2.57}


 43%|████▎     | 4600/10740 [22:53:26<28:50:09, 16.91s/it]

 43%|████▎     | 4601/10740 [22:53:46<30:16:37, 17.75s/it]

 43%|████▎     | 4602/10740 [22:53:57<26:45:27, 15.69s/it]

 43%|████▎     | 4603/10740 [22:54:13<26:54:50, 15.79s/it]

 43%|████▎     | 4604/10740 [22:54:33<29:00:52, 17.02s/it]

 43%|████▎     | 4605/10740 [22:54:49<28:32:12, 16.75s/it]

 43%|████▎     | 4606/10740 [22:55:08<29:44:25, 17.45s/it]

 43%|████▎     | 4607/10740 [22:55:26<29:57:59, 17.59s/it]

 43%|████▎     | 4608/10740 [22:55:39<27:33:39, 16.18s/it]
[2024-04-02 18:09:22,669] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4558, 'learning_rate': 1.2749254137108972e-06, 'rewards/chosen': -0.9198040962219238, 'rewards/rejected': -2.3944129943847656, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4746090173721313, 'policy_logps/rejected': -341.93756103515625, 'policy_logps/chosen': -345.2674560546875, 'referece_logps/rejected': -317.993408203125, 'referece_logps/chosen': -336.06939697265625, 'logits/rejected': -1.3163208961486816, 'logits/chosen': -1.3460828065872192, 'epoch': 2.57}


 43%|████▎     | 4610/10740 [22:56:17<30:15:23, 17.77s/it]

 43%|████▎     | 4611/10740 [22:56:41<33:05:36, 19.44s/it]

 43%|████▎     | 4612/10740 [22:56:52<28:59:26, 17.03s/it]

 43%|████▎     | 4613/10740 [22:57:10<29:35:40, 17.39s/it]

 43%|████▎     | 4614/10740 [22:57:32<31:52:40, 18.73s/it]

 43%|████▎     | 4615/10740 [22:57:55<33:48:33, 19.87s/it]

 43%|████▎     | 4616/10740 [22:58:14<33:36:45, 19.76s/it]
[2024-04-02 18:11:57,951] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4617/10740 [22:58:28<30:31:59, 17.95s/it]

 43%|████▎     | 4618/10740 [22:58:43<28:59:10, 17.05s/it]

 43%|████▎     | 4619/10740 [22:59:03<30:35:46, 17.99s/it]

 43%|████▎     | 4620/10740 [22:59:25<32:24:13, 19.06s/it]
[2024-04-02 18:13:08,374] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4527, 'learning_rate': 1.2714440781178018e-06, 'rewards/chosen': -1.719128966331482, 'rewards/rejected': -3.5543668270111084, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8352378606796265, 'policy_logps/rejected': -472.515869140625, 'policy_logps/chosen': -487.4752502441406, 'referece_logps/rejected': -436.9721984863281, 'referece_logps/chosen': -470.2839050292969, 'logits/rejected': -0.34409284591674805, 'logits/chosen': -0.4369729161262512, 'epoch': 2.58}
[2024-04-02 18:13:29,316] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 43%|████▎     | 4622/10740 [23:00:05<33:22:49, 19.64s/it]
{'loss': 0.3934, 'learning_rate': 1.2708635086482171e-06, 'rewards/chosen': -1.1588938236236572, 'rewards/rejected': -2.8061487674713135, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6472548246383667, 'policy_logps/rejected': -300.3402099609375, 'policy_logps/chosen': -321.88433837890625, 'referece_logps/rejected': -272.2787170410156, 'referece_logps/chosen': -310.29541015625, 'logits/rejected': -0.9630076885223389, 'logits/chosen': -1.1273090839385986, 'epoch': 2.58}


 43%|████▎     | 4624/10740 [23:00:43<32:34:53, 19.18s/it]

 43%|████▎     | 4625/10740 [23:01:04<33:32:11, 19.74s/it]
{'loss': 0.4136, 'learning_rate': 1.2699924697421074e-06, 'rewards/chosen': -2.2146525382995605, 'rewards/rejected': -3.9529004096984863, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7382477521896362, 'policy_logps/rejected': -362.4609069824219, 'policy_logps/chosen': -417.6577453613281, 'referece_logps/rejected': -322.9319152832031, 'referece_logps/chosen': -395.5111389160156, 'logits/rejected': -0.018877815455198288, 'logits/chosen': 0.02099784091114998, 'epoch': 2.58}
[2024-04-02 18:15:07,778] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 43%|████▎     | 4627/10740 [23:01:46<34:43:38, 20.45s/it]

 43%|████▎     | 4628/10740 [23:02:03<32:53:02, 19.37s/it]

 43%|████▎     | 4629/10740 [23:02:21<31:55:55, 18.81s/it]

 43%|████▎     | 4630/10740 [23:02:41<32:38:46, 19.24s/it]
[2024-04-02 18:16:24,508] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4631/10740 [23:02:57<31:18:32, 18.45s/it]

 43%|████▎     | 4632/10740 [23:03:18<32:37:07, 19.23s/it]

 43%|████▎     | 4633/10740 [23:03:35<31:26:22, 18.53s/it]

 43%|████▎     | 4634/10740 [23:03:52<30:42:56, 18.11s/it]
{'loss': 0.381, 'learning_rate': 1.2673780298270222e-06, 'rewards/chosen': -2.290863275527954, 'rewards/rejected': -3.2495970726013184, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9587333798408508, 'policy_logps/rejected': -479.9273986816406, 'policy_logps/chosen': -362.3356018066406, 'referece_logps/rejected': -447.4314270019531, 'referece_logps/chosen': -339.4269714355469, 'logits/rejected': 0.2330450564622879, 'logits/chosen': 0.22265949845314026, 'epoch': 2.59}


 43%|████▎     | 4636/10740 [23:04:30<30:58:24, 18.27s/it]

 43%|████▎     | 4637/10740 [23:04:47<30:35:49, 18.05s/it]

 43%|████▎     | 4638/10740 [23:05:10<32:51:15, 19.38s/it]

 43%|████▎     | 4639/10740 [23:05:33<34:53:00, 20.58s/it]
[2024-04-02 18:19:16,848] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4640/10740 [23:05:57<36:18:25, 21.43s/it]
[2024-04-02 18:19:40,243] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4641/10740 [23:06:19<36:45:31, 21.70s/it]
[2024-04-02 18:20:02,570] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4642/10740 [23:06:41<37:03:49, 21.88s/it]
[2024-04-02 18:20:24,880] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4643/10740 [23:06:59<34:56:36, 20.63s/it]

 43%|████▎     | 4644/10740 [23:07:17<33:31:21, 19.80s/it]

 43%|████▎     | 4645/10740 [23:07:36<33:17:04, 19.66s/it]
{'loss': 0.2916, 'learning_rate': 1.2641799301488556e-06, 'rewards/chosen': -1.3613576889038086, 'rewards/rejected': -3.699009418487549, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3376517295837402, 'policy_logps/rejected': -404.0561218261719, 'policy_logps/chosen': -504.0677490234375, 'referece_logps/rejected': -367.0660095214844, 'referece_logps/chosen': -490.45416259765625, 'logits/rejected': -0.6047406792640686, 'logits/chosen': -0.7257634401321411, 'epoch': 2.59}


 43%|████▎     | 4647/10740 [23:08:12<31:47:34, 18.78s/it]

 43%|████▎     | 4648/10740 [23:08:29<30:52:01, 18.24s/it]

 43%|████▎     | 4649/10740 [23:08:41<27:33:48, 16.29s/it]
{'loss': 0.4548, 'learning_rate': 1.2630162621256012e-06, 'rewards/chosen': -1.1722590923309326, 'rewards/rejected': -2.0892229080200195, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9169638156890869, 'policy_logps/rejected': -358.2279968261719, 'policy_logps/chosen': -356.8018493652344, 'referece_logps/rejected': -337.33575439453125, 'referece_logps/chosen': -345.07928466796875, 'logits/rejected': -0.23402172327041626, 'logits/chosen': -0.27192866802215576, 'epoch': 2.6}


 43%|████▎     | 4651/10740 [23:09:21<31:11:44, 18.44s/it]

 43%|████▎     | 4652/10740 [23:09:34<28:16:11, 16.72s/it]
{'loss': 0.4127, 'learning_rate': 1.262143259834831e-06, 'rewards/chosen': -1.8427245616912842, 'rewards/rejected': -3.0522518157958984, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2095272541046143, 'policy_logps/rejected': -410.0268249511719, 'policy_logps/chosen': -254.5567169189453, 'referece_logps/rejected': -379.5042419433594, 'referece_logps/chosen': -236.12948608398438, 'logits/rejected': -0.9337332844734192, 'logits/chosen': -0.9755577445030212, 'epoch': 2.6}

 43%|████▎     | 4653/10740 [23:09:55<30:09:28, 17.84s/it]


 43%|████▎     | 4655/10740 [23:10:34<31:48:23, 18.82s/it]
{'loss': 0.4764, 'learning_rate': 1.261270042960923e-06, 'rewards/chosen': -1.6452807188034058, 'rewards/rejected': -2.8301923274993896, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1849117279052734, 'policy_logps/rejected': -265.9148864746094, 'policy_logps/chosen': -297.5050048828125, 'referece_logps/rejected': -237.61297607421875, 'referece_logps/chosen': -281.05218505859375, 'logits/rejected': -0.5821403861045837, 'logits/chosen': -0.6944396495819092, 'epoch': 2.6}


 43%|████▎     | 4657/10740 [23:11:09<30:17:27, 17.93s/it]

 43%|████▎     | 4658/10740 [23:11:29<31:23:21, 18.58s/it]

 43%|████▎     | 4659/10740 [23:11:49<31:59:59, 18.94s/it]

 43%|████▎     | 4660/10740 [23:12:07<31:25:44, 18.61s/it]
{'loss': 0.4551, 'learning_rate': 1.2598142065944779e-06, 'rewards/chosen': -1.2421401739120483, 'rewards/rejected': -3.0485568046569824, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8064168691635132, 'policy_logps/rejected': -411.1773986816406, 'policy_logps/chosen': -343.5592346191406, 'referece_logps/rejected': -380.69183349609375, 'referece_logps/chosen': -331.1378173828125, 'logits/rejected': -0.8508883714675903, 'logits/chosen': -1.326102614402771, 'epoch': 2.6}

 43%|████▎     | 4661/10740 [23:12:24<30:49:04, 18.25s/it]


 43%|████▎     | 4663/10740 [23:12:57<29:22:34, 17.40s/it]

 43%|████▎     | 4664/10740 [23:13:16<29:50:58, 17.69s/it]
{'loss': 0.4032, 'learning_rate': 1.2586491119891529e-06, 'rewards/chosen': -1.5973951816558838, 'rewards/rejected': -3.926305055618286, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3289098739624023, 'policy_logps/rejected': -472.198974609375, 'policy_logps/chosen': -382.5753173828125, 'referece_logps/rejected': -432.9358825683594, 'referece_logps/chosen': -366.60137939453125, 'logits/rejected': -0.02980019897222519, 'logits/chosen': 0.048674389719963074, 'epoch': 2.61}


 43%|████▎     | 4666/10740 [23:13:52<29:35:25, 17.54s/it]
{'loss': 0.3575, 'learning_rate': 1.2580664234318906e-06, 'rewards/chosen': -1.4608261585235596, 'rewards/rejected': -3.039076805114746, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5782506465911865, 'policy_logps/rejected': -342.0150451660156, 'policy_logps/chosen': -297.418212890625, 'referece_logps/rejected': -311.624267578125, 'referece_logps/chosen': -282.8099670410156, 'logits/rejected': -0.48212340474128723, 'logits/chosen': -0.5729047060012817, 'epoch': 2.61}

 43%|████▎     | 4667/10740 [23:14:11<30:31:48, 18.10s/it]


 43%|████▎     | 4669/10740 [23:14:50<31:45:05, 18.83s/it]

 43%|████▎     | 4670/10740 [23:15:10<32:13:20, 19.11s/it]

 43%|████▎     | 4671/10740 [23:15:32<33:28:35, 19.86s/it]
{'loss': 0.4063, 'learning_rate': 1.2566092917465046e-06, 'rewards/chosen': -1.9474815130233765, 'rewards/rejected': -3.2079384326934814, 'rewards/accuracies': 0.5, 'rewards/margins': 1.2604570388793945, 'policy_logps/rejected': -395.7854919433594, 'policy_logps/chosen': -316.7718200683594, 'referece_logps/rejected': -363.70611572265625, 'referece_logps/chosen': -297.2969970703125, 'logits/rejected': -1.0086809396743774, 'logits/chosen': -1.0808851718902588, 'epoch': 2.61}

 44%|████▎     | 4672/10740 [23:15:49<32:08:35, 19.07s/it]


 44%|████▎     | 4674/10740 [23:16:22<30:00:12, 17.81s/it]

 44%|████▎     | 4675/10740 [23:16:40<29:54:32, 17.75s/it]
{'loss': 0.4346, 'learning_rate': 1.2554431661327927e-06, 'rewards/chosen': -1.0310187339782715, 'rewards/rejected': -2.8165667057037354, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7855479717254639, 'policy_logps/rejected': -348.6227111816406, 'policy_logps/chosen': -339.85650634765625, 'referece_logps/rejected': -320.4570617675781, 'referece_logps/chosen': -329.54632568359375, 'logits/rejected': -0.3358413577079773, 'logits/chosen': -0.46417713165283203, 'epoch': 2.61}


 44%|████▎     | 4677/10740 [23:17:14<29:12:13, 17.34s/it]

 44%|████▎     | 4678/10740 [23:17:34<30:33:06, 18.14s/it]

 44%|████▎     | 4679/10740 [23:17:51<29:36:51, 17.59s/it]

 44%|████▎     | 4680/10740 [23:18:09<29:50:03, 17.72s/it]
{'loss': 0.4947, 'learning_rate': 1.253984986568026e-06, 'rewards/chosen': -1.1359034776687622, 'rewards/rejected': -2.523900270462036, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3879965543746948, 'policy_logps/rejected': -426.81573486328125, 'policy_logps/chosen': -445.2542419433594, 'referece_logps/rejected': -401.57672119140625, 'referece_logps/chosen': -433.89520263671875, 'logits/rejected': -0.543907880783081, 'logits/chosen': -0.3909628093242645, 'epoch': 2.61}


 44%|████▎     | 4682/10740 [23:18:45<29:56:05, 17.79s/it]

 44%|████▎     | 4683/10740 [23:19:03<29:58:41, 17.82s/it]
{'loss': 0.5093, 'learning_rate': 1.2531098014102725e-06, 'rewards/chosen': -1.1581919193267822, 'rewards/rejected': -1.6260915994644165, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4678996205329895, 'policy_logps/rejected': -382.0900573730469, 'policy_logps/chosen': -621.2806396484375, 'referece_logps/rejected': -365.8291931152344, 'referece_logps/chosen': -609.6987915039062, 'logits/rejected': -0.5959773659706116, 'logits/chosen': -0.6476995348930359, 'epoch': 2.62}
[2024-04-02 18:33:06,833] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 44%|████▎     | 4685/10740 [23:19:40<30:28:46, 18.12s/it]
[2024-04-02 18:33:23,741] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5354, 'learning_rate': 1.252526229489429e-06, 'rewards/chosen': -1.4559649229049683, 'rewards/rejected': -2.585848331451416, 'rewards/accuracies': 0.75, 'rewards/margins': 1.129883050918579, 'policy_logps/rejected': -415.72283935546875, 'policy_logps/chosen': -369.32366943359375, 'referece_logps/rejected': -389.8643798828125, 'referece_logps/chosen': -354.76397705078125, 'logits/rejected': -0.523140549659729, 'logits/chosen': -0.4608353078365326, 'epoch': 2.62}

 44%|████▎     | 4686/10740 [23:19:54<28:10:43, 16.76s/it]

 44%|████▎     | 4687/10740 [23:20:09<27:39:49, 16.45s/it]

 44%|████▎     | 4688/10740 [23:20:26<27:40:36, 16.46s/it]


 44%|████▎     | 4690/10740 [23:20:58<27:29:50, 16.36s/it]

 44%|████▎     | 4691/10740 [23:21:16<28:15:45, 16.82s/it]
[2024-04-02 18:34:59,882] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▎     | 4692/10740 [23:21:37<30:11:50, 17.97s/it]

 44%|████▎     | 4693/10740 [23:21:51<28:13:20, 16.80s/it]

 44%|████▎     | 4694/10740 [23:22:11<29:46:10, 17.73s/it]

 44%|████▎     | 4695/10740 [23:22:29<29:57:22, 17.84s/it]
[2024-04-02 18:36:12,604] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▎     | 4696/10740 [23:22:45<28:51:03, 17.18s/it]
{'loss': 0.4141, 'learning_rate': 1.2493149474156327e-06, 'rewards/chosen': -1.14492928981781, 'rewards/rejected': -3.2987775802612305, 'rewards/accuracies': 0.875, 'rewards/margins': 2.15384840965271, 'policy_logps/rejected': -397.93780517578125, 'policy_logps/chosen': -339.86541748046875, 'referece_logps/rejected': -364.9499816894531, 'referece_logps/chosen': -328.4161376953125, 'logits/rejected': 0.45049089193344116, 'logits/chosen': 0.36608320474624634, 'epoch': 2.62}

 44%|████▎     | 4697/10740 [23:22:56<25:54:32, 15.43s/it]

 44%|████▎     | 4698/10740 [23:23:08<24:01:44, 14.32s/it]

 44%|████▍     | 4699/10740 [23:23:20<23:03:53, 13.74s/it]


 44%|████▍     | 4701/10740 [23:24:01<28:45:37, 17.14s/it]

 44%|████▍     | 4702/10740 [23:24:18<28:50:55, 17.20s/it]
{'loss': 0.4735, 'learning_rate': 1.2475621802885299e-06, 'rewards/chosen': -2.087155342102051, 'rewards/rejected': -3.6270852088928223, 'rewards/accuracies': 0.75, 'rewards/margins': 1.539929986000061, 'policy_logps/rejected': -441.1242980957031, 'policy_logps/chosen': -359.6391906738281, 'referece_logps/rejected': -404.85345458984375, 'referece_logps/chosen': -338.76763916015625, 'logits/rejected': -1.0959794521331787, 'logits/chosen': -1.0377581119537354, 'epoch': 2.63}


 44%|████▍     | 4704/10740 [23:25:00<31:58:34, 19.07s/it]
{'loss': 0.4299, 'learning_rate': 1.246977744164957e-06, 'rewards/chosen': -1.7197291851043701, 'rewards/rejected': -1.9079551696777344, 'rewards/accuracies': 0.625, 'rewards/margins': 0.18822619318962097, 'policy_logps/rejected': -410.69171142578125, 'policy_logps/chosen': -434.3470458984375, 'referece_logps/rejected': -391.61212158203125, 'referece_logps/chosen': -417.1497802734375, 'logits/rejected': -1.24033522605896, 'logits/chosen': -1.2458550930023193, 'epoch': 2.63}

 44%|████▍     | 4705/10740 [23:25:22<33:22:14, 19.91s/it]
[2024-04-02 18:39:25,751] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4706/10740 [23:25:42<33:21:58, 19.91s/it]


 44%|████▍     | 4708/10740 [23:26:21<32:59:13, 19.69s/it]
{'loss': 0.3654, 'learning_rate': 1.2458086025716952e-06, 'rewards/chosen': -1.043908715248108, 'rewards/rejected': -2.3674232959747314, 'rewards/accuracies': 0.75, 'rewards/margins': 1.323514461517334, 'policy_logps/rejected': -379.4912109375, 'policy_logps/chosen': -407.75848388671875, 'referece_logps/rejected': -355.8169250488281, 'referece_logps/chosen': -397.3193664550781, 'logits/rejected': 0.18269267678260803, 'logits/chosen': 0.1451408863067627, 'epoch': 2.63}

 44%|████▍     | 4709/10740 [23:26:42<33:37:11, 20.07s/it]


 44%|████▍     | 4711/10740 [23:27:16<30:47:55, 18.39s/it]

 44%|████▍     | 4712/10740 [23:27:33<29:44:31, 17.76s/it]
{'loss': 0.4299, 'learning_rate': 1.2446391032681503e-06, 'rewards/chosen': -2.087602376937866, 'rewards/rejected': -2.92944598197937, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8418434262275696, 'policy_logps/rejected': -495.4342041015625, 'policy_logps/chosen': -316.47625732421875, 'referece_logps/rejected': -466.1397399902344, 'referece_logps/chosen': -295.60015869140625, 'logits/rejected': -0.6858089566230774, 'logits/chosen': -0.8337969183921814, 'epoch': 2.63}
[2024-04-02 18:41:37,503] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4713/10740 [23:27:54<31:24:23, 18.76s/it]
[2024-04-02 18:41:55,321] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4714/10740 [23:28:12<30:55:43, 18.48s/it]


 44%|████▍     | 4716/10740 [23:28:41<27:22:48, 16.36s/it]
{'loss': 0.5332, 'learning_rate': 1.2434692479562237e-06, 'rewards/chosen': -1.9319809675216675, 'rewards/rejected': -2.159637689590454, 'rewards/accuracies': 0.5, 'rewards/margins': 0.22765660285949707, 'policy_logps/rejected': -252.93495178222656, 'policy_logps/chosen': -294.1532287597656, 'referece_logps/rejected': -231.3385467529297, 'referece_logps/chosen': -274.8334045410156, 'logits/rejected': -0.8325971961021423, 'logits/chosen': -0.976288914680481, 'epoch': 2.63}

 44%|████▍     | 4717/10740 [23:29:02<29:39:27, 17.73s/it]


 44%|████▍     | 4719/10740 [23:29:33<28:07:11, 16.81s/it]

 44%|████▍     | 4720/10740 [23:29:53<29:43:43, 17.78s/it]
{'loss': 0.4859, 'learning_rate': 1.242299038338334e-06, 'rewards/chosen': -2.2244648933410645, 'rewards/rejected': -3.712912082672119, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4884475469589233, 'policy_logps/rejected': -500.1084899902344, 'policy_logps/chosen': -483.27508544921875, 'referece_logps/rejected': -462.9793395996094, 'referece_logps/chosen': -461.0304260253906, 'logits/rejected': -0.4650684893131256, 'logits/chosen': -0.35809141397476196, 'epoch': 2.64}

 44%|████▍     | 4721/10740 [23:30:08<28:06:07, 16.81s/it]

 44%|████▍     | 4722/10740 [23:30:28<29:57:00, 17.92s/it]

 44%|████▍     | 4723/10740 [23:30:44<28:55:58, 17.31s/it]

 44%|████▍     | 4724/10740 [23:30:58<26:55:51, 16.12s/it]


 44%|████▍     | 4726/10740 [23:31:35<29:04:07, 17.40s/it]
{'loss': 0.3657, 'learning_rate': 1.2405430633131297e-06, 'rewards/chosen': -1.139494776725769, 'rewards/rejected': -2.7239925861358643, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5844978094100952, 'policy_logps/rejected': -368.5210266113281, 'policy_logps/chosen': -355.3313903808594, 'referece_logps/rejected': -341.2811584472656, 'referece_logps/chosen': -343.93646240234375, 'logits/rejected': -1.1638572216033936, 'logits/chosen': -1.2300193309783936, 'epoch': 2.64}


 44%|████▍     | 4728/10740 [23:32:07<27:45:25, 16.62s/it]
{'loss': 0.3203, 'learning_rate': 1.2399575629969188e-06, 'rewards/chosen': -1.6805956363677979, 'rewards/rejected': -3.865086793899536, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1844911575317383, 'policy_logps/rejected': -507.7991638183594, 'policy_logps/chosen': -466.8379821777344, 'referece_logps/rejected': -469.1483154296875, 'referece_logps/chosen': -450.03204345703125, 'logits/rejected': -0.2858239412307739, 'logits/chosen': -0.13392674922943115, 'epoch': 2.64}


 44%|████▍     | 4730/10740 [23:32:42<28:50:17, 17.27s/it]
{'loss': 0.3708, 'learning_rate': 1.2393719753817944e-06, 'rewards/chosen': -2.2032930850982666, 'rewards/rejected': -3.918879508972168, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7155864238739014, 'policy_logps/rejected': -438.3409423828125, 'policy_logps/chosen': -457.68316650390625, 'referece_logps/rejected': -399.1521301269531, 'referece_logps/chosen': -435.6501770019531, 'logits/rejected': -0.9644178152084351, 'logits/chosen': -0.9932143092155457, 'epoch': 2.64}

 44%|████▍     | 4731/10740 [23:32:59<28:43:15, 17.21s/it]


 44%|████▍     | 4733/10740 [23:33:31<27:49:27, 16.68s/it]

 44%|████▍     | 4734/10740 [23:33:52<29:50:51, 17.89s/it]
{'loss': 0.3724, 'learning_rate': 1.2382005391070078e-06, 'rewards/chosen': -1.978518009185791, 'rewards/rejected': -2.3770883083343506, 'rewards/accuracies': 0.625, 'rewards/margins': 0.39857038855552673, 'policy_logps/rejected': -376.3891906738281, 'policy_logps/chosen': -329.91131591796875, 'referece_logps/rejected': -352.6183166503906, 'referece_logps/chosen': -310.12615966796875, 'logits/rejected': -0.41198089718818665, 'logits/chosen': -0.46126192808151245, 'epoch': 2.64}

 44%|████▍     | 4735/10740 [23:34:12<31:18:26, 18.77s/it]


 44%|████▍     | 4737/10740 [23:34:44<28:33:05, 17.12s/it]

 44%|████▍     | 4738/10740 [23:35:08<31:58:04, 19.17s/it]

 44%|████▍     | 4739/10740 [23:35:25<31:08:47, 18.68s/it]
[2024-04-02 18:49:08,921] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4740/10740 [23:35:43<30:42:13, 18.42s/it]
{'loss': 0.4889, 'learning_rate': 1.2364427352800685e-06, 'rewards/chosen': -1.991886854171753, 'rewards/rejected': -3.3217930793762207, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3299063444137573, 'policy_logps/rejected': -373.3744812011719, 'policy_logps/chosen': -319.5056457519531, 'referece_logps/rejected': -340.15655517578125, 'referece_logps/chosen': -299.5867919921875, 'logits/rejected': -0.5525456070899963, 'logits/chosen': -0.6992738842964172, 'epoch': 2.65}


 44%|████▍     | 4742/10740 [23:36:19<29:46:35, 17.87s/it]
{'loss': 0.4299, 'learning_rate': 1.2358566283464633e-06, 'rewards/chosen': -1.916095495223999, 'rewards/rejected': -2.9439072608947754, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0278117656707764, 'policy_logps/rejected': -551.2232666015625, 'policy_logps/chosen': -455.2917785644531, 'referece_logps/rejected': -521.7842407226562, 'referece_logps/chosen': -436.130859375, 'logits/rejected': -0.671831488609314, 'logits/chosen': -0.7846007943153381, 'epoch': 2.65}


 44%|████▍     | 4744/10740 [23:36:56<30:25:02, 18.26s/it]

 44%|████▍     | 4745/10740 [23:37:12<29:12:23, 17.54s/it]

 44%|████▍     | 4746/10740 [23:37:26<27:19:34, 16.41s/it]

 44%|████▍     | 4747/10740 [23:37:45<28:53:21, 17.35s/it]
{'loss': 0.4439, 'learning_rate': 1.2343909860735507e-06, 'rewards/chosen': -1.8772554397583008, 'rewards/rejected': -2.6630699634552, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7858145236968994, 'policy_logps/rejected': -347.6167907714844, 'policy_logps/chosen': -435.7385559082031, 'referece_logps/rejected': -320.986083984375, 'referece_logps/chosen': -416.9660339355469, 'logits/rejected': -0.7982463836669922, 'logits/chosen': -0.8761336207389832, 'epoch': 2.65}


 44%|████▍     | 4749/10740 [23:38:16<27:18:28, 16.41s/it]
{'loss': 0.3222, 'learning_rate': 1.2338045797487015e-06, 'rewards/chosen': -1.803644061088562, 'rewards/rejected': -3.3058362007141113, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5021922588348389, 'policy_logps/rejected': -251.71241760253906, 'policy_logps/chosen': -301.80914306640625, 'referece_logps/rejected': -218.65402221679688, 'referece_logps/chosen': -283.772705078125, 'logits/rejected': -1.5943987369537354, 'logits/chosen': -1.7377666234970093, 'epoch': 2.65}


 44%|████▍     | 4751/10740 [23:38:49<27:20:54, 16.44s/it]

 44%|████▍     | 4752/10740 [23:39:07<28:02:56, 16.86s/it]

 44%|████▍     | 4753/10740 [23:39:25<28:37:36, 17.21s/it]

 44%|████▍     | 4754/10740 [23:39:46<30:20:12, 18.24s/it]
[2024-04-02 18:53:29,740] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4755/10740 [23:40:02<29:07:04, 17.51s/it]
[2024-04-02 18:53:45,551] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4756/10740 [23:40:17<28:03:38, 16.88s/it]
{'loss': 0.2582, 'learning_rate': 1.2317514891614234e-06, 'rewards/chosen': -1.4010634422302246, 'rewards/rejected': -2.4828643798828125, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0818010568618774, 'policy_logps/rejected': -343.62396240234375, 'policy_logps/chosen': -379.006591796875, 'referece_logps/rejected': -318.79534912109375, 'referece_logps/chosen': -364.9959716796875, 'logits/rejected': -0.9055604338645935, 'logits/chosen': -0.9612973928451538, 'epoch': 2.66}

 44%|████▍     | 4757/10740 [23:40:37<29:29:57, 17.75s/it]

 44%|████▍     | 4758/10740 [23:40:53<28:32:32, 17.18s/it]
[2024-04-02 18:54:58,091] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4759/10740 [23:41:14<30:42:07, 18.48s/it]
[2024-04-02 18:55:18,234] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4760/10740 [23:41:34<31:31:32, 18.98s/it]

 44%|████▍     | 4761/10740 [23:41:48<28:59:18, 17.45s/it]


 44%|████▍     | 4763/10740 [23:42:21<28:12:52, 16.99s/it]

 44%|████▍     | 4764/10740 [23:42:34<25:51:12, 15.57s/it]

 44%|████▍     | 4765/10740 [23:42:52<27:21:04, 16.48s/it]
{'loss': 0.3717, 'learning_rate': 1.2291102849026617e-06, 'rewards/chosen': -1.0386207103729248, 'rewards/rejected': -2.379756450653076, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3411356210708618, 'policy_logps/rejected': -425.1826171875, 'policy_logps/chosen': -428.16326904296875, 'referece_logps/rejected': -401.3850402832031, 'referece_logps/chosen': -417.77703857421875, 'logits/rejected': 0.06880728155374527, 'logits/chosen': 0.23986497521400452, 'epoch': 2.66}

 44%|████▍     | 4766/10740 [23:43:05<25:38:15, 15.45s/it]

 44%|████▍     | 4767/10740 [23:43:17<23:38:24, 14.25s/it]

 44%|████▍     | 4768/10740 [23:43:37<26:38:45, 16.06s/it]
[2024-04-02 18:57:44,388] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4769/10740 [23:44:01<30:22:20, 18.31s/it]


 44%|████▍     | 4771/10740 [23:44:28<26:33:25, 16.02s/it]
{'loss': 0.3455, 'learning_rate': 1.2273485431460201e-06, 'rewards/chosen': -1.8008824586868286, 'rewards/rejected': -3.685366630554199, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8844842910766602, 'policy_logps/rejected': -499.0746765136719, 'policy_logps/chosen': -427.61376953125, 'referece_logps/rejected': -462.2210693359375, 'referece_logps/chosen': -409.60491943359375, 'logits/rejected': -0.5059802532196045, 'logits/chosen': -0.5066848993301392, 'epoch': 2.67}

 44%|████▍     | 4772/10740 [23:44:50<29:17:35, 17.67s/it]

 44%|████▍     | 4773/10740 [23:45:10<30:33:58, 18.44s/it]
[2024-04-02 18:59:14,688] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4774/10740 [23:45:31<31:51:50, 19.23s/it]

 44%|████▍     | 4775/10740 [23:45:54<33:45:16, 20.37s/it]

 44%|████▍     | 4776/10740 [23:46:14<33:38:06, 20.30s/it]

 44%|████▍     | 4777/10740 [23:46:34<33:26:48, 20.19s/it]
[2024-04-02 19:00:31,852] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▍     | 4778/10740 [23:46:48<30:23:11, 18.35s/it]

 44%|████▍     | 4779/10740 [23:47:06<30:06:48, 18.19s/it]
[2024-04-02 19:01:11,189] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 4780/10740 [23:47:27<31:46:05, 19.19s/it]

 45%|████▍     | 4781/10740 [23:47:39<28:10:47, 17.02s/it]

 45%|████▍     | 4782/10740 [23:47:57<28:23:08, 17.15s/it]

 45%|████▍     | 4783/10740 [23:48:17<29:41:13, 17.94s/it]

 45%|████▍     | 4784/10740 [23:48:29<27:02:25, 16.34s/it]

 45%|████▍     | 4785/10740 [23:48:41<24:51:51, 15.03s/it]

 45%|████▍     | 4786/10740 [23:49:00<26:27:27, 16.00s/it]

 45%|████▍     | 4787/10740 [23:49:15<26:26:36, 15.99s/it]

 45%|████▍     | 4788/10740 [23:49:32<26:29:40, 16.02s/it]

 45%|████▍     | 4789/10740 [23:49:50<27:41:48, 16.75s/it]

 45%|████▍     | 4790/10740 [23:50:11<29:41:52, 17.97s/it]

 45%|████▍     | 4791/10740 [23:50:25<28:00:23, 16.95s/it]

 45%|████▍     | 4792/10740 [23:50:42<27:53:35, 16.88s/it]

 45%|████▍     | 4793/10740 [23:51:02<29:12:32, 17.68s/it]

 45%|████▍     | 4794/10740 [23:51:22<30:31:02, 18.48s/it]

 45%|████▍     | 4795/10740 [23:51:35<27:57:50, 16.93s/it]

 45%|████▍     | 4796/10740 [23:51:55<29:18:53, 17.75s/it]

 45%|████▍     | 4797/10740 [23:52:14<29:56:52, 18.14s/it]

 45%|████▍     | 4798/10740 [23:52:35<31:27:55, 19.06s/it]

 45%|████▍     | 4799/10740 [23:52:49<29:02:01, 17.59s/it]

 45%|████▍     | 4800/10740 [23:53:04<27:43:12, 16.80s/it]


 45%|████▍     | 4802/10740 [23:53:43<30:01:00, 18.20s/it]

 45%|████▍     | 4803/10740 [23:53:57<28:02:25, 17.00s/it]
{'loss': 0.5277, 'learning_rate': 1.2179401557762558e-06, 'rewards/chosen': -1.3876018524169922, 'rewards/rejected': -2.1098201274871826, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7222182750701904, 'policy_logps/rejected': -326.12890625, 'policy_logps/chosen': -270.73089599609375, 'referece_logps/rejected': -305.0306701660156, 'referece_logps/chosen': -256.8548889160156, 'logits/rejected': -0.19087716937065125, 'logits/chosen': -0.1768161654472351, 'epoch': 2.68}

 45%|████▍     | 4804/10740 [23:54:16<29:15:51, 17.75s/it]


 45%|████▍     | 4806/10740 [23:54:53<29:29:07, 17.89s/it]
{'loss': 0.4347, 'learning_rate': 1.2170570652167557e-06, 'rewards/chosen': -1.4337307214736938, 'rewards/rejected': -2.813472032546997, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3797414302825928, 'policy_logps/rejected': -399.8116455078125, 'policy_logps/chosen': -351.0048828125, 'referece_logps/rejected': -371.67694091796875, 'referece_logps/chosen': -336.6675720214844, 'logits/rejected': 0.06992428004741669, 'logits/chosen': 0.10135248303413391, 'epoch': 2.68}

 45%|████▍     | 4807/10740 [23:55:09<28:43:18, 17.43s/it]

 45%|████▍     | 4808/10740 [23:55:23<27:10:00, 16.49s/it]


 45%|████▍     | 4810/10740 [23:55:55<26:19:10, 15.98s/it]
{'loss': 0.3457, 'learning_rate': 1.2158793348761875e-06, 'rewards/chosen': -1.0099365711212158, 'rewards/rejected': -2.547821044921875, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5378844738006592, 'policy_logps/rejected': -438.3258361816406, 'policy_logps/chosen': -461.9657287597656, 'referece_logps/rejected': -412.8475646972656, 'referece_logps/chosen': -451.86639404296875, 'logits/rejected': -0.12402777373790741, 'logits/chosen': 0.054650843143463135, 'epoch': 2.69}
[2024-04-02 19:09:57,222] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 4811/10740 [23:56:13<27:37:27, 16.77s/it]

 45%|████▍     | 4812/10740 [23:56:26<25:20:13, 15.39s/it]

 45%|████▍     | 4813/10740 [23:56:45<27:05:00, 16.45s/it]

 45%|████▍     | 4814/10740 [23:57:04<28:19:06, 17.20s/it]
[2024-04-02 19:11:05,114] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 4815/10740 [23:57:21<28:37:53, 17.40s/it]

 45%|████▍     | 4816/10740 [23:57:37<27:52:49, 16.94s/it]

 45%|████▍     | 4817/10740 [23:57:51<26:30:46, 16.11s/it]

 45%|████▍     | 4818/10740 [23:58:10<27:49:59, 16.92s/it]

 45%|████▍     | 4819/10740 [23:58:31<29:36:21, 18.00s/it]

 45%|████▍     | 4820/10740 [23:58:49<29:30:20, 17.94s/it]

 45%|████▍     | 4821/10740 [23:59:03<27:41:05, 16.84s/it]

 45%|████▍     | 4822/10740 [23:59:25<30:06:29, 18.32s/it]
[2024-04-02 19:13:32,020] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 4823/10740 [23:59:48<32:45:09, 19.93s/it]

 45%|████▍     | 4824/10740 [24:00:04<30:36:31, 18.63s/it]
[2024-04-02 19:14:09,461] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 45%|████▍     | 4826/10740 [24:00:45<32:09:45, 19.58s/it]
{'loss': 0.3838, 'learning_rate': 1.2111652890991172e-06, 'rewards/chosen': -0.9291857481002808, 'rewards/rejected': -2.6066250801086426, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6774392127990723, 'policy_logps/rejected': -462.7727355957031, 'policy_logps/chosen': -385.22705078125, 'referece_logps/rejected': -436.7065124511719, 'referece_logps/chosen': -375.9352111816406, 'logits/rejected': 0.04542210325598717, 'logits/chosen': 0.07621901482343674, 'epoch': 2.7}
[2024-04-02 19:14:39,994] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 4827/10740 [24:00:56<27:55:32, 17.00s/it]


 45%|████▍     | 4829/10740 [24:01:33<29:21:02, 17.88s/it]
{'loss': 0.3914, 'learning_rate': 1.2102808548346066e-06, 'rewards/chosen': -1.1998308897018433, 'rewards/rejected': -2.7417402267456055, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5419093370437622, 'policy_logps/rejected': -367.1123046875, 'policy_logps/chosen': -420.6809387207031, 'referece_logps/rejected': -339.6949157714844, 'referece_logps/chosen': -408.6826477050781, 'logits/rejected': -1.3668009042739868, 'logits/chosen': -1.2658140659332275, 'epoch': 2.7}

 45%|████▍     | 4830/10740 [24:01:54<30:46:02, 18.74s/it]

 45%|████▍     | 4831/10740 [24:02:15<31:39:38, 19.29s/it]

 45%|████▍     | 4832/10740 [24:02:33<31:13:27, 19.03s/it]

 45%|████▌     | 4833/10740 [24:02:54<32:20:02, 19.71s/it]

 45%|████▌     | 4834/10740 [24:03:15<32:36:57, 19.88s/it]
[2024-04-02 19:17:17,367] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 4835/10740 [24:03:34<32:11:49, 19.63s/it]

 45%|████▌     | 4836/10740 [24:03:52<31:39:49, 19.31s/it]
[2024-04-02 19:17:56,855] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 4837/10740 [24:04:13<32:27:26, 19.79s/it]

 45%|████▌     | 4838/10740 [24:04:35<33:15:50, 20.29s/it]


 45%|████▌     | 4840/10740 [24:05:16<33:20:31, 20.34s/it]
{'loss': 0.321, 'learning_rate': 1.2070364620373679e-06, 'rewards/chosen': -1.4633846282958984, 'rewards/rejected': -3.115532398223877, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6521475315093994, 'policy_logps/rejected': -449.28106689453125, 'policy_logps/chosen': -414.378173828125, 'referece_logps/rejected': -418.12579345703125, 'referece_logps/chosen': -399.74432373046875, 'logits/rejected': -0.5149527788162231, 'logits/chosen': -0.5050066709518433, 'epoch': 2.7}

 45%|████▌     | 4841/10740 [24:05:29<29:57:03, 18.28s/it]
[2024-04-02 19:19:33,932] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 4842/10740 [24:05:50<31:18:35, 19.11s/it]


 45%|████▌     | 4844/10740 [24:06:16<25:58:20, 15.86s/it]
{'loss': 0.4423, 'learning_rate': 1.2058561160460682e-06, 'rewards/chosen': -1.363063931465149, 'rewards/rejected': -2.48572039604187, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1226561069488525, 'policy_logps/rejected': -375.6186218261719, 'policy_logps/chosen': -463.2364807128906, 'referece_logps/rejected': -350.7613830566406, 'referece_logps/chosen': -449.6058349609375, 'logits/rejected': -0.3219241797924042, 'logits/chosen': -0.28687143325805664, 'epoch': 2.71}

 45%|████▌     | 4845/10740 [24:06:29<24:33:57, 15.00s/it]

 45%|████▌     | 4846/10740 [24:06:45<25:11:53, 15.39s/it]

 45%|████▌     | 4847/10740 [24:06:57<23:17:45, 14.23s/it]


 45%|████▌     | 4849/10740 [24:07:36<27:43:00, 16.94s/it]

 45%|████▌     | 4850/10740 [24:07:54<28:33:29, 17.45s/it]

 45%|████▌     | 4851/10740 [24:08:07<26:07:43, 15.97s/it]

 45%|████▌     | 4852/10740 [24:08:26<27:55:07, 17.07s/it]

 45%|████▌     | 4853/10740 [24:08:49<30:46:03, 18.81s/it]

 45%|████▌     | 4854/10740 [24:09:11<32:10:18, 19.68s/it]

 45%|████▌     | 4855/10740 [24:09:33<33:07:15, 20.26s/it]
[2024-04-02 19:23:16,426] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 4856/10740 [24:09:48<30:53:16, 18.90s/it]
[2024-04-02 19:23:32,145] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 4857/10740 [24:10:05<29:46:32, 18.22s/it]

 45%|████▌     | 4858/10740 [24:10:25<30:32:37, 18.69s/it]

 45%|████▌     | 4859/10740 [24:10:42<29:49:53, 18.26s/it]

 45%|████▌     | 4860/10740 [24:11:01<30:08:22, 18.45s/it]

 45%|████▌     | 4861/10740 [24:11:14<27:15:02, 16.69s/it]

 45%|████▌     | 4862/10740 [24:11:33<28:24:37, 17.40s/it]

 45%|████▌     | 4863/10740 [24:11:56<31:30:57, 19.31s/it]

 45%|████▌     | 4864/10740 [24:12:15<31:12:39, 19.12s/it]

 45%|████▌     | 4865/10740 [24:12:31<29:53:08, 18.31s/it]

 45%|████▌     | 4866/10740 [24:12:48<29:07:03, 17.85s/it]

 45%|████▌     | 4867/10740 [24:13:05<28:24:08, 17.41s/it]

 45%|████▌     | 4868/10740 [24:13:24<29:23:44, 18.02s/it]

 45%|████▌     | 4869/10740 [24:13:40<28:21:25, 17.39s/it]

 45%|████▌     | 4870/10740 [24:13:57<28:06:50, 17.24s/it]

 45%|████▌     | 4871/10740 [24:14:13<27:37:39, 16.95s/it]

 45%|████▌     | 4872/10740 [24:14:33<28:51:10, 17.70s/it]

 45%|████▌     | 4873/10740 [24:14:45<26:11:58, 16.08s/it]

 45%|████▌     | 4874/10740 [24:15:01<26:08:56, 16.05s/it]

 45%|████▌     | 4875/10740 [24:15:21<28:09:05, 17.28s/it]
[2024-04-02 19:29:04,782] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 4876/10740 [24:15:41<29:16:54, 17.98s/it]

 45%|████▌     | 4877/10740 [24:15:59<29:38:42, 18.20s/it]

 45%|████▌     | 4878/10740 [24:16:20<30:54:14, 18.98s/it]

 45%|████▌     | 4879/10740 [24:16:41<31:39:04, 19.44s/it]

 45%|████▌     | 4880/10740 [24:17:00<31:43:18, 19.49s/it]

 45%|████▌     | 4881/10740 [24:17:17<30:27:22, 18.71s/it]

 45%|████▌     | 4882/10740 [24:17:39<31:43:19, 19.49s/it]

 45%|████▌     | 4883/10740 [24:17:56<30:55:17, 19.01s/it]

 45%|████▌     | 4884/10740 [24:18:17<31:53:50, 19.61s/it]

 45%|████▌     | 4885/10740 [24:18:35<31:07:08, 19.13s/it]

 45%|████▌     | 4886/10740 [24:18:58<32:45:46, 20.15s/it]

 46%|████▌     | 4887/10740 [24:19:16<31:34:51, 19.42s/it]

 46%|████▌     | 4888/10740 [24:19:32<29:51:38, 18.37s/it]
[2024-04-02 19:33:15,312] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 4889/10740 [24:19:44<26:57:37, 16.59s/it]

 46%|████▌     | 4890/10740 [24:20:01<27:21:16, 16.83s/it]

 46%|████▌     | 4891/10740 [24:20:18<27:16:33, 16.79s/it]

 46%|████▌     | 4892/10740 [24:20:35<27:26:26, 16.89s/it]

 46%|████▌     | 4893/10740 [24:20:55<28:49:43, 17.75s/it]

 46%|████▌     | 4894/10740 [24:21:17<30:41:40, 18.90s/it]

 46%|████▌     | 4895/10740 [24:21:35<30:35:30, 18.84s/it]

 46%|████▌     | 4896/10740 [24:21:48<27:38:40, 17.03s/it]

 46%|████▌     | 4897/10740 [24:22:05<27:31:20, 16.96s/it]

 46%|████▌     | 4898/10740 [24:22:19<26:02:41, 16.05s/it]

 46%|████▌     | 4899/10740 [24:22:30<23:52:31, 14.72s/it]

 46%|████▌     | 4900/10740 [24:22:45<23:44:58, 14.64s/it]

 46%|████▌     | 4901/10740 [24:23:05<26:26:17, 16.30s/it]

 46%|████▌     | 4902/10740 [24:23:21<26:30:43, 16.35s/it]

 46%|████▌     | 4903/10740 [24:23:36<25:31:34, 15.74s/it]

 46%|████▌     | 4904/10740 [24:23:48<23:59:27, 14.80s/it]

 46%|████▌     | 4905/10740 [24:24:09<26:58:50, 16.65s/it]

 46%|████▌     | 4906/10740 [24:24:31<29:33:26, 18.24s/it]

 46%|████▌     | 4907/10740 [24:24:45<27:30:57, 16.98s/it]

 46%|████▌     | 4908/10740 [24:25:05<28:39:11, 17.69s/it]
[2024-04-02 19:38:48,453] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 4909/10740 [24:25:18<26:28:04, 16.34s/it]

 46%|████▌     | 4910/10740 [24:25:30<24:29:49, 15.13s/it]

 46%|████▌     | 4911/10740 [24:25:43<23:08:09, 14.29s/it]

 46%|████▌     | 4912/10740 [24:26:03<25:56:13, 16.02s/it]

 46%|████▌     | 4913/10740 [24:26:21<26:52:27, 16.60s/it]

 46%|████▌     | 4914/10740 [24:26:32<24:10:48, 14.94s/it]

 46%|████▌     | 4915/10740 [24:26:51<26:31:42, 16.40s/it]

 46%|████▌     | 4916/10740 [24:27:13<28:51:44, 17.84s/it]

 46%|████▌     | 4917/10740 [24:27:26<26:27:17, 16.36s/it]

 46%|████▌     | 4918/10740 [24:27:45<27:57:37, 17.29s/it]

 46%|████▌     | 4919/10740 [24:28:07<30:06:14, 18.62s/it]

 46%|████▌     | 4920/10740 [24:28:23<29:03:05, 17.97s/it]

 46%|████▌     | 4921/10740 [24:28:42<29:20:38, 18.15s/it]

 46%|████▌     | 4922/10740 [24:28:58<28:17:29, 17.51s/it]

 46%|████▌     | 4923/10740 [24:29:17<29:22:14, 18.18s/it]

 46%|████▌     | 4924/10740 [24:29:39<31:11:24, 19.31s/it]

 46%|████▌     | 4925/10740 [24:30:01<32:31:06, 20.13s/it]
[2024-04-02 19:43:45,223] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 4926/10740 [24:30:18<30:58:15, 19.18s/it]

 46%|████▌     | 4927/10740 [24:30:34<29:12:21, 18.09s/it]

 46%|████▌     | 4928/10740 [24:30:55<30:37:26, 18.97s/it]

 46%|████▌     | 4929/10740 [24:31:10<28:49:01, 17.85s/it]

 46%|████▌     | 4930/10740 [24:31:29<29:08:46, 18.06s/it]

 46%|████▌     | 4931/10740 [24:31:40<25:40:07, 15.91s/it]

 46%|████▌     | 4932/10740 [24:31:56<25:58:49, 16.10s/it]

 46%|████▌     | 4933/10740 [24:32:16<27:58:17, 17.34s/it]

 46%|████▌     | 4934/10740 [24:32:30<25:55:57, 16.08s/it]

 46%|████▌     | 4935/10740 [24:32:42<24:10:00, 14.99s/it]

 46%|████▌     | 4936/10740 [24:33:00<25:35:25, 15.87s/it]

 46%|████▌     | 4937/10740 [24:33:20<27:34:41, 17.11s/it]
{'loss': 0.407, 'learning_rate': 1.1783322180899687e-06, 'rewards/chosen': -1.439884901046753, 'rewards/rejected': -3.6342501640319824, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1943652629852295, 'policy_logps/rejected': -419.47357177734375, 'policy_logps/chosen': -425.20166015625, 'referece_logps/rejected': -383.13104248046875, 'referece_logps/chosen': -410.8028259277344, 'logits/rejected': 0.44373074173927307, 'logits/chosen': 0.33826297521591187, 'epoch': 2.76}


 46%|████▌     | 4939/10740 [24:33:48<25:29:51, 15.82s/it]

 46%|████▌     | 4940/10740 [24:34:09<27:55:24, 17.33s/it]

 46%|████▌     | 4941/10740 [24:34:28<28:53:11, 17.93s/it]
{'loss': 0.3948, 'learning_rate': 1.177145092757126e-06, 'rewards/chosen': -0.05169110745191574, 'rewards/rejected': -2.1037659645080566, 'rewards/accuracies': 0.875, 'rewards/margins': 2.052074670791626, 'policy_logps/rejected': -489.9152526855469, 'policy_logps/chosen': -440.78387451171875, 'referece_logps/rejected': -468.87762451171875, 'referece_logps/chosen': -440.2669677734375, 'logits/rejected': -0.2717987298965454, 'logits/chosen': -0.3538787364959717, 'epoch': 2.76}

 46%|████▌     | 4942/10740 [24:34:48<29:45:45, 18.48s/it]
[2024-04-02 19:48:53,797] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 46%|████▌     | 4944/10740 [24:35:32<32:45:39, 20.35s/it]

 46%|████▌     | 4945/10740 [24:35:50<31:40:59, 19.68s/it]
[2024-04-02 19:49:34,234] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 4946/10740 [24:36:03<28:18:01, 17.58s/it]
{'loss': 0.3768, 'learning_rate': 1.1756608237785327e-06, 'rewards/chosen': -1.8531379699707031, 'rewards/rejected': -3.7367563247680664, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8836179971694946, 'policy_logps/rejected': -446.7268371582031, 'policy_logps/chosen': -311.205322265625, 'referece_logps/rejected': -409.3592834472656, 'referece_logps/chosen': -292.6739501953125, 'logits/rejected': -1.7679953575134277, 'logits/chosen': -1.6701000928878784, 'epoch': 2.76}

 46%|████▌     | 4947/10740 [24:36:16<25:53:47, 16.09s/it]


 46%|████▌     | 4949/10740 [24:36:51<26:54:09, 16.72s/it]
{'loss': 0.3552, 'learning_rate': 1.1747700704539845e-06, 'rewards/chosen': -2.541137456893921, 'rewards/rejected': -4.900108337402344, 'rewards/accuracies': 0.875, 'rewards/margins': 2.358971357345581, 'policy_logps/rejected': -370.3155822753906, 'policy_logps/chosen': -339.27606201171875, 'referece_logps/rejected': -321.314453125, 'referece_logps/chosen': -313.8646545410156, 'logits/rejected': -0.7743771076202393, 'logits/chosen': -0.8468942046165466, 'epoch': 2.76}

 46%|████▌     | 4950/10740 [24:37:06<26:18:22, 16.36s/it]

 46%|████▌     | 4951/10740 [24:37:28<28:52:03, 17.95s/it]
[2024-04-02 19:51:35,409] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 46%|████▌     | 4953/10740 [24:38:13<32:18:24, 20.10s/it]
{'loss': 0.3054, 'learning_rate': 1.1735821769399112e-06, 'rewards/chosen': -1.3649954795837402, 'rewards/rejected': -2.9123384952545166, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5473428964614868, 'policy_logps/rejected': -363.761474609375, 'policy_logps/chosen': -362.9772644042969, 'referece_logps/rejected': -334.63812255859375, 'referece_logps/chosen': -349.3272705078125, 'logits/rejected': -1.196967601776123, 'logits/chosen': -1.19796884059906, 'epoch': 2.77}


 46%|████▌     | 4955/10740 [24:38:43<28:34:07, 17.78s/it]
{'loss': 0.4075, 'learning_rate': 1.172988135348481e-06, 'rewards/chosen': -1.5775749683380127, 'rewards/rejected': -3.506316661834717, 'rewards/accuracies': 0.875, 'rewards/margins': 1.928741693496704, 'policy_logps/rejected': -430.0033874511719, 'policy_logps/chosen': -361.5544128417969, 'referece_logps/rejected': -394.94024658203125, 'referece_logps/chosen': -345.7786560058594, 'logits/rejected': -0.5212893486022949, 'logits/chosen': -0.5558228492736816, 'epoch': 2.77}


 46%|████▌     | 4957/10740 [24:39:17<27:43:01, 17.25s/it]

 46%|████▌     | 4958/10740 [24:39:31<26:13:54, 16.33s/it]
{'loss': 0.4349, 'learning_rate': 1.172096955026168e-06, 'rewards/chosen': -1.5275726318359375, 'rewards/rejected': -2.8494460582733154, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3218733072280884, 'policy_logps/rejected': -248.74896240234375, 'policy_logps/chosen': -330.8045349121094, 'referece_logps/rejected': -220.25448608398438, 'referece_logps/chosen': -315.52880859375, 'logits/rejected': -1.2176803350448608, 'logits/chosen': -1.2344048023223877, 'epoch': 2.77}

 46%|████▌     | 4959/10740 [24:39:48<26:41:31, 16.62s/it]
[2024-04-02 19:53:55,461] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 46%|████▌     | 4961/10740 [24:40:29<29:22:30, 18.30s/it]
{'loss': 0.368, 'learning_rate': 1.1712056338301028e-06, 'rewards/chosen': -1.9743995666503906, 'rewards/rejected': -3.7023990154266357, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7279995679855347, 'policy_logps/rejected': -458.0209045410156, 'policy_logps/chosen': -472.8867492675781, 'referece_logps/rejected': -420.9968566894531, 'referece_logps/chosen': -453.1427001953125, 'logits/rejected': 0.31931373476982117, 'logits/chosen': 0.23492929339408875, 'epoch': 2.77}


 46%|████▌     | 4963/10740 [24:41:13<32:24:25, 20.19s/it]
{'loss': 0.343, 'learning_rate': 1.170611341796493e-06, 'rewards/chosen': -1.1921924352645874, 'rewards/rejected': -3.624382972717285, 'rewards/accuracies': 0.875, 'rewards/margins': 2.432190418243408, 'policy_logps/rejected': -378.0198059082031, 'policy_logps/chosen': -392.35009765625, 'referece_logps/rejected': -341.77593994140625, 'referece_logps/chosen': -380.42816162109375, 'logits/rejected': -1.0636357069015503, 'logits/chosen': -1.0972485542297363, 'epoch': 2.77}
[2024-04-02 19:55:20,073] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 46%|████▌     | 4965/10740 [24:41:59<34:31:44, 21.52s/it]

 46%|████▌     | 4966/10740 [24:42:19<33:49:20, 21.09s/it]
{'loss': 0.4241, 'learning_rate': 1.169719787432254e-06, 'rewards/chosen': -1.6843537092208862, 'rewards/rejected': -3.8768415451049805, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1924874782562256, 'policy_logps/rejected': -392.84405517578125, 'policy_logps/chosen': -402.7057189941406, 'referece_logps/rejected': -354.07562255859375, 'referece_logps/chosen': -385.8621520996094, 'logits/rejected': -0.19208559393882751, 'logits/chosen': -0.17684438824653625, 'epoch': 2.77}

 46%|████▌     | 4967/10740 [24:42:37<32:08:48, 20.05s/it]
[2024-04-02 19:56:41,944] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 4968/10740 [24:42:58<32:50:24, 20.48s/it]
[2024-04-02 19:57:01,748] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 4969/10740 [24:43:18<32:30:29, 20.28s/it]


 46%|████▋     | 4971/10740 [24:43:55<31:05:13, 19.40s/it]
[2024-04-02 19:57:38,792] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.351, 'learning_rate': 1.168233555123703e-06, 'rewards/chosen': -1.485794186592102, 'rewards/rejected': -3.4792420864105225, 'rewards/accuracies': 0.875, 'rewards/margins': 1.99344801902771, 'policy_logps/rejected': -434.94775390625, 'policy_logps/chosen': -423.41455078125, 'referece_logps/rejected': -400.1553955078125, 'referece_logps/chosen': -408.55657958984375, 'logits/rejected': -0.4246264696121216, 'logits/chosen': -0.2309977412223816, 'epoch': 2.78}
[2024-04-02 19:57:57,695] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 46%|████▋     | 4973/10740 [24:44:29<29:03:07, 18.14s/it]
[2024-04-02 19:58:13,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 4974/10740 [24:44:49<29:37:55, 18.50s/it]
{'loss': 0.442, 'learning_rate': 1.1673416319072345e-06, 'rewards/chosen': -1.7250434160232544, 'rewards/rejected': -3.073547124862671, 'rewards/accuracies': 0.75, 'rewards/margins': 1.348503589630127, 'policy_logps/rejected': -405.60247802734375, 'policy_logps/chosen': -284.58154296875, 'referece_logps/rejected': -374.8670654296875, 'referece_logps/chosen': -267.33111572265625, 'logits/rejected': -1.0622013807296753, 'logits/chosen': -0.9680153131484985, 'epoch': 2.78}

 46%|████▋     | 4975/10740 [24:45:09<30:13:29, 18.87s/it]
[2024-04-02 19:59:13,785] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 4976/10740 [24:45:30<31:27:39, 19.65s/it]

 46%|████▋     | 4977/10740 [24:45:45<29:09:02, 18.21s/it]
[2024-04-02 19:59:48,567] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 4978/10740 [24:46:05<29:58:19, 18.73s/it]


 46%|████▋     | 4980/10740 [24:46:35<26:39:08, 16.66s/it]
{'loss': 0.558, 'learning_rate': 1.1655573752609787e-06, 'rewards/chosen': -1.7475351095199585, 'rewards/rejected': -2.075833559036255, 'rewards/accuracies': 0.75, 'rewards/margins': 0.32829874753952026, 'policy_logps/rejected': -422.6689453125, 'policy_logps/chosen': -363.77166748046875, 'referece_logps/rejected': -401.9106140136719, 'referece_logps/chosen': -346.29632568359375, 'logits/rejected': -1.3244531154632568, 'logits/chosen': -1.3376076221466064, 'epoch': 2.78}

 46%|████▋     | 4981/10740 [24:46:56<28:44:00, 17.96s/it]

 46%|████▋     | 4982/10740 [24:47:17<29:54:54, 18.70s/it]

 46%|████▋     | 4983/10740 [24:47:36<30:29:46, 19.07s/it]


 46%|████▋     | 4985/10740 [24:48:09<28:03:55, 17.56s/it]

 46%|████▋     | 4986/10740 [24:48:29<29:12:26, 18.27s/it]

 46%|████▋     | 4987/10740 [24:48:45<28:05:02, 17.57s/it]

 46%|████▋     | 4988/10740 [24:49:03<28:19:33, 17.73s/it]
[2024-04-02 20:02:47,122] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4288, 'learning_rate': 1.1631775241699806e-06, 'rewards/chosen': -1.1988445520401, 'rewards/rejected': -3.4804115295410156, 'rewards/accuracies': 0.875, 'rewards/margins': 2.281567096710205, 'policy_logps/rejected': -419.14642333984375, 'policy_logps/chosen': -320.6821594238281, 'referece_logps/rejected': -384.34228515625, 'referece_logps/chosen': -308.6937561035156, 'logits/rejected': -0.7642630338668823, 'logits/chosen': -0.6703945398330688, 'epoch': 2.79}


 46%|████▋     | 4990/10740 [24:49:41<29:18:41, 18.35s/it]
{'loss': 0.4807, 'learning_rate': 1.162582412442087e-06, 'rewards/chosen': -1.6915597915649414, 'rewards/rejected': -2.642645835876465, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9510862231254578, 'policy_logps/rejected': -329.6105041503906, 'policy_logps/chosen': -314.7934265136719, 'referece_logps/rejected': -303.18402099609375, 'referece_logps/chosen': -297.8778076171875, 'logits/rejected': -0.15073531866073608, 'logits/chosen': -0.21975889801979065, 'epoch': 2.79}

 46%|████▋     | 4991/10740 [24:49:56<27:43:34, 17.36s/it]

 46%|████▋     | 4992/10740 [24:50:19<30:15:58, 18.96s/it]

 46%|████▋     | 4993/10740 [24:50:36<29:23:15, 18.41s/it]

 46%|████▋     | 4994/10740 [24:50:56<30:07:00, 18.87s/it]

 47%|████▋     | 4995/10740 [24:51:13<28:55:04, 18.12s/it]

 47%|████▋     | 4996/10740 [24:51:25<26:11:05, 16.41s/it]

 47%|████▋     | 4997/10740 [24:51:39<24:59:45, 15.67s/it]

 47%|████▋     | 4998/10740 [24:51:59<26:56:45, 16.89s/it]


 47%|████▋     | 5000/10740 [24:52:28<25:14:41, 15.83s/it]
{'loss': 0.3694, 'learning_rate': 1.159605970897392e-06, 'rewards/chosen': -1.811205506324768, 'rewards/rejected': -2.8850057125091553, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0738002061843872, 'policy_logps/rejected': -519.4644775390625, 'policy_logps/chosen': -334.8312072753906, 'referece_logps/rejected': -490.61444091796875, 'referece_logps/chosen': -316.71917724609375, 'logits/rejected': -0.9391423463821411, 'logits/chosen': -0.9014948606491089, 'epoch': 2.79}

 47%|████▋     | 5001/10740 [24:52:57<31:38:09, 19.84s/it]


 47%|████▋     | 5003/10740 [24:53:28<28:17:59, 17.76s/it]

 47%|████▋     | 5004/10740 [24:53:44<27:38:27, 17.35s/it]

 47%|████▋     | 5005/10740 [24:54:04<28:49:12, 18.09s/it]
[2024-04-02 20:07:47,835] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4825, 'learning_rate': 1.1581172040621757e-06, 'rewards/chosen': -1.0121487379074097, 'rewards/rejected': -2.019611120223999, 'rewards/accuracies': 0.75, 'rewards/margins': 1.007462501525879, 'policy_logps/rejected': -458.9060363769531, 'policy_logps/chosen': -407.02691650390625, 'referece_logps/rejected': -438.70989990234375, 'referece_logps/chosen': -396.9053955078125, 'logits/rejected': -0.40294957160949707, 'logits/chosen': -0.46463000774383545, 'epoch': 2.8}

 47%|████▋     | 5006/10740 [24:54:21<28:27:14, 17.86s/it]


 47%|████▋     | 5008/10740 [24:54:48<24:46:02, 15.56s/it]
{'loss': 0.4525, 'learning_rate': 1.157223771170623e-06, 'rewards/chosen': -0.8086608648300171, 'rewards/rejected': -2.3514161109924316, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5427554845809937, 'policy_logps/rejected': -316.460693359375, 'policy_logps/chosen': -258.4817199707031, 'referece_logps/rejected': -292.9465026855469, 'referece_logps/chosen': -250.39508056640625, 'logits/rejected': -0.5060684084892273, 'logits/chosen': -0.4488658010959625, 'epoch': 2.8}

 47%|████▋     | 5009/10740 [24:55:01<23:47:16, 14.94s/it]

 47%|████▋     | 5010/10740 [24:55:21<26:02:46, 16.36s/it]

 47%|████▋     | 5011/10740 [24:55:38<26:16:44, 16.51s/it]

 47%|████▋     | 5012/10740 [24:55:58<27:56:46, 17.56s/it]

 47%|████▋     | 5013/10740 [24:56:19<29:37:24, 18.62s/it]

 47%|████▋     | 5014/10740 [24:56:35<28:31:31, 17.93s/it]


 47%|████▋     | 5016/10740 [24:57:08<27:00:48, 16.99s/it]
{'loss': 0.3767, 'learning_rate': 1.1548406562514864e-06, 'rewards/chosen': -0.9304921627044678, 'rewards/rejected': -2.253037691116333, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3225455284118652, 'policy_logps/rejected': -225.9192657470703, 'policy_logps/chosen': -276.4908447265625, 'referece_logps/rejected': -203.38890075683594, 'referece_logps/chosen': -267.1858825683594, 'logits/rejected': -1.5580490827560425, 'logits/chosen': -1.6627042293548584, 'epoch': 2.8}


 47%|████▋     | 5018/10740 [24:57:40<26:41:12, 16.79s/it]

 47%|████▋     | 5019/10740 [24:58:00<28:02:27, 17.65s/it]
{'loss': 0.3165, 'learning_rate': 1.1539467550399075e-06, 'rewards/chosen': -1.3764030933380127, 'rewards/rejected': -2.264211416244507, 'rewards/accuracies': 1.0, 'rewards/margins': 0.8878082633018494, 'policy_logps/rejected': -370.57977294921875, 'policy_logps/chosen': -329.64300537109375, 'referece_logps/rejected': -347.93768310546875, 'referece_logps/chosen': -315.87896728515625, 'logits/rejected': -1.211958885192871, 'logits/chosen': -1.3069303035736084, 'epoch': 2.8}


 47%|████▋     | 5021/10740 [24:58:40<30:04:32, 18.93s/it]
[2024-04-02 20:12:23,729] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 5022/10740 [24:58:58<29:33:01, 18.60s/it]
{'loss': 0.4507, 'learning_rate': 1.1530527278118217e-06, 'rewards/chosen': -1.8249268531799316, 'rewards/rejected': -3.016814947128296, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1918880939483643, 'policy_logps/rejected': -336.8581237792969, 'policy_logps/chosen': -436.5848693847656, 'referece_logps/rejected': -306.6900329589844, 'referece_logps/chosen': -418.3355712890625, 'logits/rejected': -1.0396685600280762, 'logits/chosen': -1.2558649778366089, 'epoch': 2.81}


 47%|████▋     | 5024/10740 [24:59:40<32:03:53, 20.19s/it]

 47%|████▋     | 5025/10740 [25:00:02<32:37:17, 20.55s/it]
{'loss': 0.3447, 'learning_rate': 1.1521585752990555e-06, 'rewards/chosen': -1.3410096168518066, 'rewards/rejected': -2.7901666164398193, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4491569995880127, 'policy_logps/rejected': -357.1485900878906, 'policy_logps/chosen': -307.5941162109375, 'referece_logps/rejected': -329.2469482421875, 'referece_logps/chosen': -294.18402099609375, 'logits/rejected': -0.7840889692306519, 'logits/chosen': -0.8612858057022095, 'epoch': 2.81}


 47%|████▋     | 5027/10740 [25:00:34<29:19:25, 18.48s/it]

 47%|████▋     | 5028/10740 [25:00:46<26:17:38, 16.57s/it]

 47%|████▋     | 5029/10740 [25:01:07<28:22:57, 17.89s/it]

 47%|████▋     | 5030/10740 [25:01:27<29:20:20, 18.50s/it]

 47%|████▋     | 5031/10740 [25:01:47<29:58:45, 18.90s/it]

 47%|████▋     | 5032/10740 [25:02:07<30:27:16, 19.21s/it]
{'loss': 0.4059, 'learning_rate': 1.1500717363293696e-06, 'rewards/chosen': -1.984096646308899, 'rewards/rejected': -2.500263214111328, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5161666870117188, 'policy_logps/rejected': -401.05792236328125, 'policy_logps/chosen': -388.728271484375, 'referece_logps/rejected': -376.0552673339844, 'referece_logps/chosen': -368.8873291015625, 'logits/rejected': -0.5375560522079468, 'logits/chosen': -0.6798743009567261, 'epoch': 2.81}


 47%|████▋     | 5034/10740 [25:02:45<30:21:50, 19.16s/it]
{'loss': 0.3956, 'learning_rate': 1.149475373372462e-06, 'rewards/chosen': -1.9294732809066772, 'rewards/rejected': -2.869755983352661, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9402826428413391, 'policy_logps/rejected': -441.194580078125, 'policy_logps/chosen': -446.2126770019531, 'referece_logps/rejected': -412.49700927734375, 'referece_logps/chosen': -426.91790771484375, 'logits/rejected': -0.006600916385650635, 'logits/chosen': -0.02987578511238098, 'epoch': 2.81}

 47%|████▋     | 5035/10740 [25:03:02<29:35:56, 18.68s/it]

 47%|████▋     | 5036/10740 [25:03:16<27:02:49, 17.07s/it]

 47%|████▋     | 5037/10740 [25:03:36<28:45:58, 18.16s/it]


 47%|████▋     | 5039/10740 [25:04:15<29:36:00, 18.69s/it]

 47%|████▋     | 5040/10740 [25:04:35<30:08:56, 19.04s/it]
{'loss': 0.3224, 'learning_rate': 1.1476859590860533e-06, 'rewards/chosen': -1.2663342952728271, 'rewards/rejected': -3.085466146469116, 'rewards/accuracies': 0.75, 'rewards/margins': 1.819131851196289, 'policy_logps/rejected': -454.2485046386719, 'policy_logps/chosen': -452.2265930175781, 'referece_logps/rejected': -423.39385986328125, 'referece_logps/chosen': -439.563232421875, 'logits/rejected': -1.0385627746582031, 'logits/chosen': -1.0845428705215454, 'epoch': 2.82}


 47%|████▋     | 5042/10740 [25:05:10<28:49:02, 18.21s/it]
{'loss': 0.3989, 'learning_rate': 1.1470893799086958e-06, 'rewards/chosen': -1.492868185043335, 'rewards/rejected': -2.904587745666504, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4117194414138794, 'policy_logps/rejected': -430.4559326171875, 'policy_logps/chosen': -473.47381591796875, 'referece_logps/rejected': -401.4100646972656, 'referece_logps/chosen': -458.545166015625, 'logits/rejected': -0.6067530512809753, 'logits/chosen': -0.5963013768196106, 'epoch': 2.82}

 47%|████▋     | 5043/10740 [25:05:23<26:22:17, 16.66s/it]

 47%|████▋     | 5044/10740 [25:05:40<26:42:58, 16.89s/it]


 47%|████▋     | 5046/10740 [25:06:13<26:04:52, 16.49s/it]
{'loss': 0.4604, 'learning_rate': 1.1458960612333668e-06, 'rewards/chosen': -1.5960801839828491, 'rewards/rejected': -3.662283420562744, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0662028789520264, 'policy_logps/rejected': -536.18798828125, 'policy_logps/chosen': -449.6623840332031, 'referece_logps/rejected': -499.565185546875, 'referece_logps/chosen': -433.7015380859375, 'logits/rejected': -0.9781120419502258, 'logits/chosen': -1.1095026731491089, 'epoch': 2.82}

 47%|████▋     | 5047/10740 [25:06:26<24:36:05, 15.56s/it]

 47%|████▋     | 5048/10740 [25:06:42<24:55:41, 15.77s/it]


 47%|████▋     | 5050/10740 [25:07:11<23:27:53, 14.85s/it]

 47%|████▋     | 5051/10740 [25:07:33<26:40:22, 16.88s/it]

 47%|████▋     | 5052/10740 [25:07:54<28:28:33, 18.02s/it]
{'loss': 0.3955, 'learning_rate': 1.1441056856750427e-06, 'rewards/chosen': -2.333313226699829, 'rewards/rejected': -3.3513264656066895, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0180134773254395, 'policy_logps/rejected': -402.9117736816406, 'policy_logps/chosen': -431.5918273925781, 'referece_logps/rejected': -369.3984680175781, 'referece_logps/chosen': -408.2587585449219, 'logits/rejected': -0.2791121006011963, 'logits/chosen': -0.5234472751617432, 'epoch': 2.82}

 47%|████▋     | 5053/10740 [25:08:10<27:34:13, 17.45s/it]

 47%|████▋     | 5054/10740 [25:08:26<27:03:06, 17.13s/it]

 47%|████▋     | 5055/10740 [25:08:45<27:42:32, 17.55s/it]

 47%|████▋     | 5056/10740 [25:09:04<28:30:17, 18.05s/it]

 47%|████▋     | 5057/10740 [25:09:16<25:44:16, 16.30s/it]

 47%|████▋     | 5058/10740 [25:09:32<25:41:02, 16.27s/it]

 47%|████▋     | 5059/10740 [25:09:48<25:26:49, 16.13s/it]

 47%|████▋     | 5060/10740 [25:10:05<25:39:06, 16.26s/it]

 47%|████▋     | 5061/10740 [25:10:24<27:14:10, 17.27s/it]

 47%|████▋     | 5062/10740 [25:10:44<28:29:53, 18.07s/it]


 47%|████▋     | 5064/10740 [25:11:20<28:27:56, 18.05s/it]

 47%|████▋     | 5065/10740 [25:11:39<29:09:37, 18.50s/it]
{'loss': 0.4241, 'learning_rate': 1.1402249277698365e-06, 'rewards/chosen': -1.963093638420105, 'rewards/rejected': -3.7021636962890625, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7390702962875366, 'policy_logps/rejected': -440.22735595703125, 'policy_logps/chosen': -385.4062805175781, 'referece_logps/rejected': -403.20574951171875, 'referece_logps/chosen': -365.7752990722656, 'logits/rejected': -0.3764180541038513, 'logits/chosen': -0.31329676508903503, 'epoch': 2.83}

 47%|████▋     | 5066/10740 [25:11:52<26:28:13, 16.79s/it]


 47%|████▋     | 5068/10740 [25:12:29<27:36:43, 17.53s/it]
{'loss': 0.4184, 'learning_rate': 1.139329059989649e-06, 'rewards/chosen': -1.4350714683532715, 'rewards/rejected': -2.757115364074707, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3220438957214355, 'policy_logps/rejected': -275.1147155761719, 'policy_logps/chosen': -256.4413146972656, 'referece_logps/rejected': -247.54356384277344, 'referece_logps/chosen': -242.09060668945312, 'logits/rejected': -1.0319528579711914, 'logits/chosen': -0.8879451751708984, 'epoch': 2.83}


 47%|████▋     | 5070/10740 [25:12:58<25:01:02, 15.88s/it]
{'loss': 0.4791, 'learning_rate': 1.1387317513959914e-06, 'rewards/chosen': -2.807248592376709, 'rewards/rejected': -3.9766948223114014, 'rewards/accuracies': 0.875, 'rewards/margins': 1.169446587562561, 'policy_logps/rejected': -362.9063415527344, 'policy_logps/chosen': -327.2309875488281, 'referece_logps/rejected': -323.139404296875, 'referece_logps/chosen': -299.15850830078125, 'logits/rejected': -0.6724845767021179, 'logits/chosen': -0.6311272382736206, 'epoch': 2.83}

 47%|████▋     | 5071/10740 [25:13:08<22:32:33, 14.32s/it]
[2024-04-02 20:27:15,934] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 5072/10740 [25:13:32<27:03:22, 17.18s/it]
[2024-04-02 20:27:36,640] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 5073/10740 [25:13:53<28:42:51, 18.24s/it]

 47%|████▋     | 5074/10740 [25:14:12<29:20:22, 18.64s/it]


 47%|████▋     | 5076/10740 [25:14:45<27:22:15, 17.40s/it]

 47%|████▋     | 5077/10740 [25:15:06<28:45:16, 18.28s/it]

 47%|████▋     | 5078/10740 [25:15:26<29:31:54, 18.78s/it]

 47%|████▋     | 5079/10740 [25:15:45<29:57:41, 19.05s/it]

 47%|████▋     | 5080/10740 [25:16:08<31:32:10, 20.06s/it]
[2024-04-02 20:29:51,563] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3512, 'learning_rate': 1.135744455694915e-06, 'rewards/chosen': -1.312697410583496, 'rewards/rejected': -2.5664920806884766, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2537943124771118, 'policy_logps/rejected': -267.05780029296875, 'policy_logps/chosen': -290.008056640625, 'referece_logps/rejected': -241.39288330078125, 'referece_logps/chosen': -276.881103515625, 'logits/rejected': -0.3880632519721985, 'logits/chosen': -0.24047300219535828, 'epoch': 2.84}
[2024-04-02 20:30:08,767] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 5081/10740 [25:16:25<30:11:03, 19.20s/it]
[2024-04-02 20:30:29,097] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 47%|████▋     | 5083/10740 [25:17:08<32:07:43, 20.45s/it]
[2024-04-02 20:30:51,657] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 5084/10740 [25:17:22<28:56:20, 18.42s/it]
[2024-04-02 20:31:05,348] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.365, 'learning_rate': 1.1345491901966821e-06, 'rewards/chosen': -1.4840751886367798, 'rewards/rejected': -2.4936742782592773, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0095990896224976, 'policy_logps/rejected': -433.4052734375, 'policy_logps/chosen': -427.57794189453125, 'referece_logps/rejected': -408.468505859375, 'referece_logps/chosen': -412.7371826171875, 'logits/rejected': 0.5740011930465698, 'logits/chosen': 0.5030297636985779, 'epoch': 2.84}

 47%|████▋     | 5085/10740 [25:17:41<29:26:25, 18.74s/it]


 47%|████▋     | 5087/10740 [25:18:18<29:28:28, 18.77s/it]
[2024-04-02 20:32:01,513] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4236, 'learning_rate': 1.1336526124833105e-06, 'rewards/chosen': -1.2439829111099243, 'rewards/rejected': -3.146347999572754, 'rewards/accuracies': 0.75, 'rewards/margins': 1.90236496925354, 'policy_logps/rejected': -352.9939270019531, 'policy_logps/chosen': -324.7383117675781, 'referece_logps/rejected': -321.5304870605469, 'referece_logps/chosen': -312.2984619140625, 'logits/rejected': -0.11747315526008606, 'logits/chosen': -0.11380250751972198, 'epoch': 2.84}


 47%|████▋     | 5089/10740 [25:18:58<30:37:39, 19.51s/it]
{'loss': 0.3091, 'learning_rate': 1.1330548331822618e-06, 'rewards/chosen': -1.1182756423950195, 'rewards/rejected': -2.899552822113037, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7812769412994385, 'policy_logps/rejected': -450.21343994140625, 'policy_logps/chosen': -415.8870849609375, 'referece_logps/rejected': -421.2179260253906, 'referece_logps/chosen': -404.704345703125, 'logits/rejected': -0.7304899096488953, 'logits/chosen': -0.8168494701385498, 'epoch': 2.84}
[2024-04-02 20:33:05,167] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 47%|████▋     | 5091/10740 [25:19:36<29:31:28, 18.82s/it]
[2024-04-02 20:33:19,883] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3292, 'learning_rate': 1.1324570054745607e-06, 'rewards/chosen': -1.739652156829834, 'rewards/rejected': -2.662611246109009, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9229592084884644, 'policy_logps/rejected': -451.73822021484375, 'policy_logps/chosen': -365.6611022949219, 'referece_logps/rejected': -425.11212158203125, 'referece_logps/chosen': -348.26458740234375, 'logits/rejected': -0.7087900638580322, 'logits/chosen': -0.6579529047012329, 'epoch': 2.84}

 47%|████▋     | 5092/10740 [25:19:53<28:48:12, 18.36s/it]


 47%|████▋     | 5094/10740 [25:20:36<31:17:36, 19.95s/it]
{'loss': 0.4444, 'learning_rate': 1.1315601736263122e-06, 'rewards/chosen': -1.3993196487426758, 'rewards/rejected': -2.405618190765381, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0062986612319946, 'policy_logps/rejected': -443.93609619140625, 'policy_logps/chosen': -362.41607666015625, 'referece_logps/rejected': -419.87994384765625, 'referece_logps/chosen': -348.42291259765625, 'logits/rejected': -0.5398449897766113, 'logits/chosen': -0.5319997072219849, 'epoch': 2.85}


 47%|████▋     | 5096/10740 [25:21:06<27:50:56, 17.76s/it]
{'loss': 0.379, 'learning_rate': 1.1309622258535614e-06, 'rewards/chosen': -1.6334781646728516, 'rewards/rejected': -3.7662651538848877, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1327872276306152, 'policy_logps/rejected': -295.1915588378906, 'policy_logps/chosen': -278.3644714355469, 'referece_logps/rejected': -257.5289001464844, 'referece_logps/chosen': -262.0296630859375, 'logits/rejected': -0.5369483232498169, 'logits/chosen': -0.6337530016899109, 'epoch': 2.85}
[2024-04-02 20:35:09,474] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 47%|████▋     | 5098/10740 [25:21:46<29:43:02, 18.96s/it]
{'loss': 0.5229, 'learning_rate': 1.1303642304354694e-06, 'rewards/chosen': -2.3119449615478516, 'rewards/rejected': -3.41146183013916, 'rewards/accuracies': 0.875, 'rewards/margins': 1.099516749382019, 'policy_logps/rejected': -475.46746826171875, 'policy_logps/chosen': -578.251220703125, 'referece_logps/rejected': -441.35284423828125, 'referece_logps/chosen': -555.1317138671875, 'logits/rejected': -1.0651692152023315, 'logits/chosen': -1.1359291076660156, 'epoch': 2.85}


 47%|████▋     | 5100/10740 [25:22:26<30:38:18, 19.56s/it]
{'loss': 0.4278, 'learning_rate': 1.1297661875895926e-06, 'rewards/chosen': -1.513749361038208, 'rewards/rejected': -2.8553051948547363, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3415557146072388, 'policy_logps/rejected': -502.6872253417969, 'policy_logps/chosen': -324.995361328125, 'referece_logps/rejected': -474.1341552734375, 'referece_logps/chosen': -309.85784912109375, 'logits/rejected': -0.7420063018798828, 'logits/chosen': -0.7791725993156433, 'epoch': 2.85}

 47%|████▋     | 5101/10740 [25:22:45<30:26:49, 19.44s/it]


 48%|████▊     | 5103/10740 [25:23:22<29:21:53, 18.75s/it]

 48%|████▊     | 5104/10740 [25:23:36<27:06:14, 17.31s/it]
{'loss': 0.5115, 'learning_rate': 1.1285699604847967e-06, 'rewards/chosen': -0.5689060688018799, 'rewards/rejected': -2.8800947666168213, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3111884593963623, 'policy_logps/rejected': -389.3179016113281, 'policy_logps/chosen': -292.80133056640625, 'referece_logps/rejected': -360.5169677734375, 'referece_logps/chosen': -287.11224365234375, 'logits/rejected': -0.743537962436676, 'logits/chosen': -0.9541492462158203, 'epoch': 2.85}

 48%|████▊     | 5105/10740 [25:23:51<26:15:35, 16.78s/it]


 48%|████▊     | 5107/10740 [25:24:33<29:27:55, 18.83s/it]
{'loss': 0.3247, 'learning_rate': 1.1276726672765953e-06, 'rewards/chosen': -1.3472644090652466, 'rewards/rejected': -1.7594677209854126, 'rewards/accuracies': 0.75, 'rewards/margins': 0.41220322251319885, 'policy_logps/rejected': -415.91925048828125, 'policy_logps/chosen': -329.52899169921875, 'referece_logps/rejected': -398.3245544433594, 'referece_logps/chosen': -316.0563659667969, 'logits/rejected': 0.060627833008766174, 'logits/chosen': 0.014612987637519836, 'epoch': 2.85}


 48%|████▊     | 5109/10740 [25:25:08<28:06:58, 17.98s/it]
{'loss': 0.3569, 'learning_rate': 1.1270744136984104e-06, 'rewards/chosen': -1.7225271463394165, 'rewards/rejected': -3.3430583477020264, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6205309629440308, 'policy_logps/rejected': -356.50384521484375, 'policy_logps/chosen': -319.7636413574219, 'referece_logps/rejected': -323.0732421875, 'referece_logps/chosen': -302.53839111328125, 'logits/rejected': -0.4662134647369385, 'logits/chosen': -0.5069940090179443, 'epoch': 2.85}


 48%|████▊     | 5111/10740 [25:25:42<26:58:42, 17.25s/it]

 48%|████▊     | 5112/10740 [25:26:04<29:10:43, 18.66s/it]
{'loss': 0.4385, 'learning_rate': 1.1261769467161839e-06, 'rewards/chosen': -2.2088301181793213, 'rewards/rejected': -3.635207414627075, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4263774156570435, 'policy_logps/rejected': -442.9529724121094, 'policy_logps/chosen': -378.1270751953125, 'referece_logps/rejected': -406.6009216308594, 'referece_logps/chosen': -356.03875732421875, 'logits/rejected': 0.8040319085121155, 'logits/chosen': 0.8383191823959351, 'epoch': 2.86}
[2024-04-02 20:40:04,801] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 5113/10740 [25:26:21<28:19:43, 18.12s/it]

 48%|████▊     | 5114/10740 [25:26:35<26:24:11, 16.89s/it]

 48%|████▊     | 5115/10740 [25:26:50<25:30:44, 16.33s/it]

 48%|████▊     | 5116/10740 [25:27:03<24:00:06, 15.36s/it]

 48%|████▊     | 5117/10740 [25:27:22<25:24:27, 16.27s/it]

 48%|████▊     | 5118/10740 [25:27:39<26:01:02, 16.66s/it]

 48%|████▊     | 5119/10740 [25:27:53<24:42:00, 15.82s/it]


 48%|████▊     | 5121/10740 [25:28:25<24:27:47, 15.67s/it]
{'loss': 0.4053, 'learning_rate': 1.123483928998981e-06, 'rewards/chosen': -1.4440069198608398, 'rewards/rejected': -2.8191566467285156, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3751497268676758, 'policy_logps/rejected': -372.00341796875, 'policy_logps/chosen': -402.6059875488281, 'referece_logps/rejected': -343.81182861328125, 'referece_logps/chosen': -388.1659240722656, 'logits/rejected': -1.0214924812316895, 'logits/chosen': -1.1031858921051025, 'epoch': 2.86}

 48%|████▊     | 5122/10740 [25:28:42<25:08:16, 16.11s/it]


 48%|████▊     | 5124/10740 [25:29:08<23:06:13, 14.81s/it]
{'loss': 0.3891, 'learning_rate': 1.1225860532857715e-06, 'rewards/chosen': -1.5371814966201782, 'rewards/rejected': -4.375058174133301, 'rewards/accuracies': 1.0, 'rewards/margins': 2.837876796722412, 'policy_logps/rejected': -487.3666687011719, 'policy_logps/chosen': -437.68768310546875, 'referece_logps/rejected': -443.6160888671875, 'referece_logps/chosen': -422.31585693359375, 'logits/rejected': -0.35341426730155945, 'logits/chosen': -0.20139890909194946, 'epoch': 2.86}
[2024-04-02 20:43:12,365] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 5125/10740 [25:29:29<25:39:56, 16.46s/it]

 48%|████▊     | 5126/10740 [25:29:48<27:06:18, 17.38s/it]


 48%|████▊     | 5128/10740 [25:30:31<30:11:49, 19.37s/it]
{'loss': 0.3963, 'learning_rate': 1.1213887297021825e-06, 'rewards/chosen': -1.7791016101837158, 'rewards/rejected': -2.686455011367798, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9073535203933716, 'policy_logps/rejected': -440.7855224609375, 'policy_logps/chosen': -401.095458984375, 'referece_logps/rejected': -413.92095947265625, 'referece_logps/chosen': -383.304443359375, 'logits/rejected': 0.37105679512023926, 'logits/chosen': 0.4076860547065735, 'epoch': 2.86}

 48%|████▊     | 5129/10740 [25:30:48<28:55:24, 18.56s/it]

 48%|████▊     | 5130/10740 [25:31:01<26:39:06, 17.10s/it]

 48%|████▊     | 5131/10740 [25:31:23<28:37:03, 18.37s/it]

 48%|████▊     | 5132/10740 [25:31:36<26:15:24, 16.86s/it]

 48%|████▊     | 5133/10740 [25:31:52<25:49:55, 16.59s/it]

 48%|████▊     | 5134/10740 [25:32:09<25:58:54, 16.68s/it]


 48%|████▊     | 5136/10740 [25:32:45<27:20:32, 17.56s/it]
{'loss': 0.4312, 'learning_rate': 1.1189935543287857e-06, 'rewards/chosen': -0.8618579506874084, 'rewards/rejected': -2.7150301933288574, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8531723022460938, 'policy_logps/rejected': -321.4060974121094, 'policy_logps/chosen': -187.9833984375, 'referece_logps/rejected': -294.25579833984375, 'referece_logps/chosen': -179.36480712890625, 'logits/rejected': -0.3648402690887451, 'logits/chosen': -0.2610008418560028, 'epoch': 2.87}

 48%|████▊     | 5137/10740 [25:33:05<28:18:04, 18.18s/it]

 48%|████▊     | 5138/10740 [25:33:26<29:57:38, 19.25s/it]

 48%|████▊     | 5139/10740 [25:33:46<29:57:13, 19.25s/it]

 48%|████▊     | 5140/10740 [25:33:57<26:02:44, 16.74s/it]

 48%|████▊     | 5141/10740 [25:34:16<27:08:13, 17.45s/it]

 48%|████▊     | 5142/10740 [25:34:30<25:37:35, 16.48s/it]

 48%|████▊     | 5143/10740 [25:34:46<25:17:11, 16.26s/it]


 48%|████▊     | 5145/10740 [25:35:19<26:05:31, 16.79s/it]
{'loss': 0.3173, 'learning_rate': 1.1162981547878517e-06, 'rewards/chosen': -1.0365394353866577, 'rewards/rejected': -2.524489164352417, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4879498481750488, 'policy_logps/rejected': -260.4952087402344, 'policy_logps/chosen': -277.3765869140625, 'referece_logps/rejected': -235.25030517578125, 'referece_logps/chosen': -267.01116943359375, 'logits/rejected': -0.7935726642608643, 'logits/chosen': -0.8639415502548218, 'epoch': 2.87}

 48%|████▊     | 5146/10740 [25:35:36<26:16:17, 16.91s/it]

 48%|████▊     | 5147/10740 [25:35:55<27:06:32, 17.45s/it]

 48%|████▊     | 5148/10740 [25:36:17<29:03:11, 18.70s/it]

 48%|████▊     | 5149/10740 [25:36:37<29:47:10, 19.18s/it]

 48%|████▊     | 5150/10740 [25:36:50<27:03:09, 17.42s/it]

 48%|████▊     | 5151/10740 [25:37:11<28:34:18, 18.40s/it]

 48%|████▊     | 5152/10740 [25:37:30<28:44:36, 18.52s/it]


 48%|████▊     | 5154/10740 [25:38:13<31:10:13, 20.09s/it]
{'loss': 0.4724, 'learning_rate': 1.1136018984616435e-06, 'rewards/chosen': -1.8451627492904663, 'rewards/rejected': -2.8533713817596436, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0082083940505981, 'policy_logps/rejected': -379.0, 'policy_logps/chosen': -442.903564453125, 'referece_logps/rejected': -350.4662780761719, 'referece_logps/chosen': -424.4520263671875, 'logits/rejected': -0.016755033284425735, 'logits/chosen': -0.022518299520015717, 'epoch': 2.88}

 48%|████▊     | 5155/10740 [25:38:30<29:28:23, 19.00s/it]


 48%|████▊     | 5157/10740 [25:39:02<27:27:01, 17.70s/it]
{'loss': 0.3471, 'learning_rate': 1.1127029593892347e-06, 'rewards/chosen': -1.8025922775268555, 'rewards/rejected': -2.8615646362304688, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0589724779129028, 'policy_logps/rejected': -414.51507568359375, 'policy_logps/chosen': -449.0885925292969, 'referece_logps/rejected': -385.8994140625, 'referece_logps/chosen': -431.0626525878906, 'logits/rejected': -0.45510566234588623, 'logits/chosen': -0.5165374279022217, 'epoch': 2.88}

 48%|████▊     | 5158/10740 [25:39:21<28:25:59, 18.34s/it]

 48%|████▊     | 5159/10740 [25:39:43<29:51:54, 19.26s/it]

 48%|████▊     | 5160/10740 [25:39:55<26:38:15, 17.19s/it]


 48%|████▊     | 5162/10740 [25:40:32<26:58:57, 17.41s/it]
{'loss': 0.3421, 'learning_rate': 1.1112045229531028e-06, 'rewards/chosen': -2.0364866256713867, 'rewards/rejected': -2.5562660694122314, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5197793841362, 'policy_logps/rejected': -415.615478515625, 'policy_logps/chosen': -332.8961486816406, 'referece_logps/rejected': -390.0528259277344, 'referece_logps/chosen': -312.5312805175781, 'logits/rejected': -1.4774473905563354, 'logits/chosen': -1.4975242614746094, 'epoch': 2.88}

 48%|████▊     | 5163/10740 [25:40:47<26:12:37, 16.92s/it]
[2024-04-02 20:54:51,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 5164/10740 [25:41:07<27:35:20, 17.81s/it]

 48%|████▊     | 5165/10740 [25:41:27<28:27:41, 18.38s/it]


 48%|████▊     | 5167/10740 [25:42:14<32:39:49, 21.10s/it]
{'loss': 0.3326, 'learning_rate': 1.1097058336589154e-06, 'rewards/chosen': -1.6620086431503296, 'rewards/rejected': -3.0481531620025635, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3861445188522339, 'policy_logps/rejected': -281.23052978515625, 'policy_logps/chosen': -259.779541015625, 'referece_logps/rejected': -250.7490234375, 'referece_logps/chosen': -243.1594696044922, 'logits/rejected': -0.6504762768745422, 'logits/chosen': -0.4983828663825989, 'epoch': 2.89}

 48%|████▊     | 5168/10740 [25:42:27<29:04:26, 18.78s/it]

 48%|████▊     | 5169/10740 [25:42:49<30:20:07, 19.60s/it]


 48%|████▊     | 5171/10740 [25:43:28<30:39:00, 19.81s/it]
{'loss': 0.395, 'learning_rate': 1.108506702455744e-06, 'rewards/chosen': -1.5178906917572021, 'rewards/rejected': -2.5030417442321777, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9851510524749756, 'policy_logps/rejected': -350.45941162109375, 'policy_logps/chosen': -322.7457275390625, 'referece_logps/rejected': -325.4289855957031, 'referece_logps/chosen': -307.56683349609375, 'logits/rejected': -0.025141850113868713, 'logits/chosen': -0.11941307783126831, 'epoch': 2.89}

 48%|████▊     | 5172/10740 [25:43:47<30:23:34, 19.65s/it]

 48%|████▊     | 5173/10740 [25:44:08<31:06:37, 20.12s/it]

 48%|████▊     | 5174/10740 [25:44:23<28:26:40, 18.40s/it]
[2024-04-02 20:58:28,967] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 5175/10740 [25:44:45<30:21:39, 19.64s/it]
[2024-04-02 20:58:45,301] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 5176/10740 [25:45:02<28:49:20, 18.65s/it]


 48%|████▊     | 5178/10740 [25:45:37<27:52:09, 18.04s/it]

 48%|████▊     | 5179/10740 [25:45:57<28:28:44, 18.44s/it]

 48%|████▊     | 5180/10740 [25:46:15<28:30:00, 18.45s/it]

 48%|████▊     | 5181/10740 [25:46:31<27:09:29, 17.59s/it]

 48%|████▊     | 5182/10740 [25:46:47<26:45:16, 17.33s/it]

 48%|████▊     | 5183/10740 [25:47:06<27:05:52, 17.55s/it]

 48%|████▊     | 5184/10740 [25:47:24<27:35:07, 17.87s/it]

 48%|████▊     | 5185/10740 [25:47:43<28:07:39, 18.23s/it]

 48%|████▊     | 5186/10740 [25:48:03<28:41:48, 18.60s/it]

 48%|████▊     | 5187/10740 [25:48:21<28:29:46, 18.47s/it]

 48%|████▊     | 5188/10740 [25:48:42<29:55:37, 19.41s/it]
[2024-04-02 21:02:26,180] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 5189/10740 [25:49:01<29:45:34, 19.30s/it]

 48%|████▊     | 5190/10740 [25:49:17<27:47:00, 18.02s/it]

 48%|████▊     | 5191/10740 [25:49:33<26:53:23, 17.45s/it]

 48%|████▊     | 5192/10740 [25:49:53<28:24:14, 18.43s/it]
{'loss': 0.3764, 'learning_rate': 1.1022087136075864e-06, 'rewards/chosen': -1.9092658758163452, 'rewards/rejected': -3.695206880569458, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7859411239624023, 'policy_logps/rejected': -503.5513916015625, 'policy_logps/chosen': -392.04150390625, 'referece_logps/rejected': -466.5993347167969, 'referece_logps/chosen': -372.9488220214844, 'logits/rejected': -0.09985613822937012, 'logits/chosen': -0.2331654131412506, 'epoch': 2.9}


 48%|████▊     | 5194/10740 [25:50:32<28:43:19, 18.64s/it]

 48%|████▊     | 5195/10740 [25:50:49<27:55:14, 18.13s/it]

 48%|████▊     | 5196/10740 [25:51:08<28:32:28, 18.53s/it]

 48%|████▊     | 5197/10740 [25:51:27<28:58:56, 18.82s/it]

 48%|████▊     | 5198/10740 [25:51:49<30:10:30, 19.60s/it]

 48%|████▊     | 5199/10740 [25:52:05<28:37:08, 18.59s/it]

 48%|████▊     | 5200/10740 [25:52:27<30:18:51, 19.70s/it]

 48%|████▊     | 5201/10740 [25:52:47<30:18:23, 19.70s/it]

 48%|████▊     | 5202/10740 [25:53:10<31:34:31, 20.53s/it]

 48%|████▊     | 5203/10740 [25:53:31<32:11:49, 20.93s/it]

 48%|████▊     | 5204/10740 [25:53:50<31:04:55, 20.21s/it]

 48%|████▊     | 5205/10740 [25:54:05<28:53:38, 18.79s/it]

 48%|████▊     | 5206/10740 [25:54:22<27:50:55, 18.12s/it]

 48%|████▊     | 5207/10740 [25:54:44<29:36:13, 19.26s/it]

 48%|████▊     | 5208/10740 [25:54:57<26:54:50, 17.51s/it]

 49%|████▊     | 5209/10740 [25:55:10<24:35:05, 16.00s/it]

 49%|████▊     | 5210/10740 [25:55:25<24:08:37, 15.72s/it]

 49%|████▊     | 5211/10740 [25:55:43<25:09:26, 16.38s/it]

 49%|████▊     | 5212/10740 [25:56:02<26:27:55, 17.24s/it]

 49%|████▊     | 5213/10740 [25:56:18<26:00:03, 16.94s/it]

 49%|████▊     | 5214/10740 [25:56:34<25:34:53, 16.67s/it]

 49%|████▊     | 5215/10740 [25:56:51<25:32:03, 16.64s/it]

 49%|████▊     | 5216/10740 [25:57:12<27:40:16, 18.03s/it]

 49%|████▊     | 5217/10740 [25:57:29<26:57:18, 17.57s/it]

 49%|████▊     | 5218/10740 [25:57:49<28:26:18, 18.54s/it]

 49%|████▊     | 5219/10740 [25:58:08<28:20:56, 18.49s/it]

 49%|████▊     | 5220/10740 [25:58:28<28:56:03, 18.87s/it]

 49%|████▊     | 5221/10740 [25:58:45<28:18:31, 18.47s/it]

 49%|████▊     | 5222/10740 [25:58:58<25:30:36, 16.64s/it]

 49%|████▊     | 5223/10740 [25:59:17<26:59:00, 17.61s/it]

 49%|████▊     | 5224/10740 [25:59:30<24:44:47, 16.15s/it]
{'loss': 0.4783, 'learning_rate': 1.0926039799872881e-06, 'rewards/chosen': -1.2022879123687744, 'rewards/rejected': -3.950347423553467, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7480595111846924, 'policy_logps/rejected': -447.7341003417969, 'policy_logps/chosen': -437.7425842285156, 'referece_logps/rejected': -408.23065185546875, 'referece_logps/chosen': -425.71966552734375, 'logits/rejected': -0.5084058046340942, 'logits/chosen': -0.6234418749809265, 'epoch': 2.92}

 49%|████▊     | 5225/10740 [25:59:49<25:49:59, 16.86s/it]


 49%|████▊     | 5227/10740 [26:00:28<28:06:04, 18.35s/it]

 49%|████▊     | 5228/10740 [26:00:50<29:59:08, 19.58s/it]
[2024-04-02 21:14:34,014] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▊     | 5229/10740 [26:01:06<28:14:54, 18.45s/it]

 49%|████▊     | 5230/10740 [26:01:26<28:48:42, 18.82s/it]

 49%|████▊     | 5231/10740 [26:01:46<29:14:36, 19.11s/it]
{'loss': 0.2843, 'learning_rate': 1.0905017637688208e-06, 'rewards/chosen': -1.3533977270126343, 'rewards/rejected': -2.892024278640747, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5386263132095337, 'policy_logps/rejected': -408.76348876953125, 'policy_logps/chosen': -409.0581970214844, 'referece_logps/rejected': -379.8432312011719, 'referece_logps/chosen': -395.52423095703125, 'logits/rejected': -0.6337600350379944, 'logits/chosen': -0.6400861144065857, 'epoch': 2.92}

 49%|████▊     | 5232/10740 [26:02:05<29:29:35, 19.28s/it]


 49%|████▊     | 5234/10740 [26:02:49<31:22:21, 20.51s/it]
[2024-04-02 21:16:32,378] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▊     | 5235/10740 [26:03:09<31:08:41, 20.37s/it]
[2024-04-02 21:16:52,406] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5236/10740 [26:03:26<29:46:07, 19.47s/it]

 49%|████▉     | 5237/10740 [26:03:43<28:26:46, 18.61s/it]
{'loss': 0.4244, 'learning_rate': 1.0886995427750335e-06, 'rewards/chosen': -1.539811611175537, 'rewards/rejected': -2.535515069961548, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9957034587860107, 'policy_logps/rejected': -513.4149169921875, 'policy_logps/chosen': -510.7168884277344, 'referece_logps/rejected': -488.0597839355469, 'referece_logps/chosen': -495.31878662109375, 'logits/rejected': -1.1551190614700317, 'logits/chosen': -1.206829309463501, 'epoch': 2.93}


 49%|████▉     | 5239/10740 [26:04:20<28:43:55, 18.80s/it]

 49%|████▉     | 5240/10740 [26:04:40<28:56:15, 18.94s/it]
[2024-04-02 21:18:23,432] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5241/10740 [26:04:54<26:57:34, 17.65s/it]

 49%|████▉     | 5242/10740 [26:05:06<24:10:09, 15.83s/it]
{'loss': 0.4891, 'learning_rate': 1.0871974698418542e-06, 'rewards/chosen': -0.8959037661552429, 'rewards/rejected': -2.2696499824523926, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3737459182739258, 'policy_logps/rejected': -358.4478759765625, 'policy_logps/chosen': -412.1162109375, 'referece_logps/rejected': -335.75140380859375, 'referece_logps/chosen': -403.15716552734375, 'logits/rejected': 0.5861396789550781, 'logits/chosen': 0.5662586688995361, 'epoch': 2.93}


 49%|████▉     | 5244/10740 [26:05:41<26:06:12, 17.10s/it]

 49%|████▉     | 5245/10740 [26:05:56<25:19:03, 16.59s/it]

 49%|████▉     | 5246/10740 [26:06:17<27:04:26, 17.74s/it]

 49%|████▉     | 5247/10740 [26:06:32<26:01:02, 17.05s/it]

 49%|████▉     | 5248/10740 [26:06:50<26:33:54, 17.41s/it]
{'loss': 0.3689, 'learning_rate': 1.085394720905493e-06, 'rewards/chosen': -1.3959532976150513, 'rewards/rejected': -3.191196918487549, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7952436208724976, 'policy_logps/rejected': -299.3428039550781, 'policy_logps/chosen': -276.79864501953125, 'referece_logps/rejected': -267.43084716796875, 'referece_logps/chosen': -262.839111328125, 'logits/rejected': -1.8190429210662842, 'logits/chosen': -1.7718623876571655, 'epoch': 2.93}


 49%|████▉     | 5250/10740 [26:07:21<24:25:16, 16.01s/it]

 49%|████▉     | 5251/10740 [26:07:33<22:37:10, 14.84s/it]

 49%|████▉     | 5252/10740 [26:07:51<24:05:24, 15.80s/it]
{'loss': 0.4094, 'learning_rate': 1.0841927325797043e-06, 'rewards/chosen': -1.6256943941116333, 'rewards/rejected': -2.4586732387542725, 'rewards/accuracies': 0.75, 'rewards/margins': 0.832979142665863, 'policy_logps/rejected': -460.9219055175781, 'policy_logps/chosen': -459.35089111328125, 'referece_logps/rejected': -436.3351745605469, 'referece_logps/chosen': -443.09393310546875, 'logits/rejected': -0.9657481908798218, 'logits/chosen': -0.972918689250946, 'epoch': 2.93}


 49%|████▉     | 5254/10740 [26:08:23<24:00:01, 15.75s/it]
{'loss': 0.4978, 'learning_rate': 1.0835916923622711e-06, 'rewards/chosen': -1.7343003749847412, 'rewards/rejected': -1.7435764074325562, 'rewards/accuracies': 0.5, 'rewards/margins': 0.009275928139686584, 'policy_logps/rejected': -463.2281188964844, 'policy_logps/chosen': -499.1622009277344, 'referece_logps/rejected': -445.7923278808594, 'referece_logps/chosen': -481.8192138671875, 'logits/rejected': -1.0829339027404785, 'logits/chosen': -0.9971150159835815, 'epoch': 2.94}


 49%|████▉     | 5256/10740 [26:08:56<25:23:40, 16.67s/it]

 49%|████▉     | 5257/10740 [26:09:08<23:12:57, 15.24s/it]
{'loss': 0.4213, 'learning_rate': 1.0826900750829382e-06, 'rewards/chosen': -1.3978288173675537, 'rewards/rejected': -3.7357144355773926, 'rewards/accuracies': 1.0, 'rewards/margins': 2.337885618209839, 'policy_logps/rejected': -443.65533447265625, 'policy_logps/chosen': -394.6318359375, 'referece_logps/rejected': -406.2981872558594, 'referece_logps/chosen': -380.6535339355469, 'logits/rejected': -0.503159761428833, 'logits/chosen': -0.5406829714775085, 'epoch': 2.94}

 49%|████▉     | 5258/10740 [26:09:32<26:57:37, 17.70s/it]

 49%|████▉     | 5259/10740 [26:09:50<26:59:01, 17.72s/it]


 49%|████▉     | 5261/10740 [26:10:25<27:31:39, 18.09s/it]
[2024-04-02 21:24:08,868] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5262/10740 [26:10:45<28:22:03, 18.64s/it]

 49%|████▉     | 5263/10740 [26:11:05<28:47:16, 18.92s/it]

 49%|████▉     | 5264/10740 [26:11:19<26:50:52, 17.65s/it]

 49%|████▉     | 5265/10740 [26:11:36<26:27:33, 17.40s/it]
{'loss': 0.4004, 'learning_rate': 1.0802854334247295e-06, 'rewards/chosen': -1.6061511039733887, 'rewards/rejected': -1.8673312664031982, 'rewards/accuracies': 0.375, 'rewards/margins': 0.26118016242980957, 'policy_logps/rejected': -405.7041015625, 'policy_logps/chosen': -464.25299072265625, 'referece_logps/rejected': -387.03082275390625, 'referece_logps/chosen': -448.19146728515625, 'logits/rejected': -0.725556492805481, 'logits/chosen': -0.5421918630599976, 'epoch': 2.94}

 49%|████▉     | 5266/10740 [26:11:52<25:44:41, 16.93s/it]


 49%|████▉     | 5268/10740 [26:12:31<27:47:56, 18.29s/it]
{'loss': 0.3645, 'learning_rate': 1.0793835715654712e-06, 'rewards/chosen': -2.0940582752227783, 'rewards/rejected': -3.0413661003112793, 'rewards/accuracies': 0.625, 'rewards/margins': 0.947307825088501, 'policy_logps/rejected': -411.7853698730469, 'policy_logps/chosen': -318.1690368652344, 'referece_logps/rejected': -381.3717041015625, 'referece_logps/chosen': -297.22845458984375, 'logits/rejected': -0.492107629776001, 'logits/chosen': -0.5891621112823486, 'epoch': 2.94}

 49%|████▉     | 5269/10740 [26:12:50<27:56:54, 18.39s/it]


 49%|████▉     | 5271/10740 [26:13:29<28:59:17, 19.08s/it]

 49%|████▉     | 5272/10740 [26:13:49<29:14:33, 19.25s/it]
{'loss': 0.4856, 'learning_rate': 1.0781809881322432e-06, 'rewards/chosen': -2.0896830558776855, 'rewards/rejected': -3.627603769302368, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5379209518432617, 'policy_logps/rejected': -347.56671142578125, 'policy_logps/chosen': -382.34649658203125, 'referece_logps/rejected': -311.2906494140625, 'referece_logps/chosen': -361.4496765136719, 'logits/rejected': -1.0499331951141357, 'logits/chosen': -1.118403434753418, 'epoch': 2.95}


 49%|████▉     | 5274/10740 [26:14:30<30:11:28, 19.88s/it]
[2024-04-02 21:28:13,263] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5275/10740 [26:14:47<29:06:41, 19.18s/it]
[2024-04-02 21:28:30,789] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5276/10740 [26:15:08<30:07:37, 19.85s/it]

 49%|████▉     | 5277/10740 [26:15:27<29:39:46, 19.55s/it]

 49%|████▉     | 5278/10740 [26:15:47<29:48:23, 19.65s/it]
{'loss': 0.3939, 'learning_rate': 1.0763769002067826e-06, 'rewards/chosen': -1.7436732053756714, 'rewards/rejected': -4.031122207641602, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2874488830566406, 'policy_logps/rejected': -316.62933349609375, 'policy_logps/chosen': -301.287109375, 'referece_logps/rejected': -276.318115234375, 'referece_logps/chosen': -283.85040283203125, 'logits/rejected': -0.6023759841918945, 'logits/chosen': -0.602303683757782, 'epoch': 2.95}

 49%|████▉     | 5279/10740 [26:16:08<30:19:16, 19.99s/it]


 49%|████▉     | 5281/10740 [26:16:49<30:55:46, 20.40s/it]

 49%|████▉     | 5282/10740 [26:17:07<29:29:06, 19.45s/it]
{'loss': 0.49, 'learning_rate': 1.0751740356251815e-06, 'rewards/chosen': -1.5355243682861328, 'rewards/rejected': -2.6272106170654297, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0916860103607178, 'policy_logps/rejected': -474.028564453125, 'policy_logps/chosen': -486.0947570800781, 'referece_logps/rejected': -447.7564392089844, 'referece_logps/chosen': -470.739501953125, 'logits/rejected': -1.1059874296188354, 'logits/chosen': -1.005725622177124, 'epoch': 2.95}


 49%|████▉     | 5284/10740 [26:17:45<29:34:49, 19.52s/it]
[2024-04-02 21:31:28,371] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5285/10740 [26:18:00<27:28:11, 18.13s/it]

 49%|████▉     | 5286/10740 [26:18:12<24:47:24, 16.36s/it]
{'loss': 0.4534, 'learning_rate': 1.0739710616473862e-06, 'rewards/chosen': -1.4535597562789917, 'rewards/rejected': -2.501844882965088, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0482851266860962, 'policy_logps/rejected': -301.7754211425781, 'policy_logps/chosen': -371.5852355957031, 'referece_logps/rejected': -276.7569580078125, 'referece_logps/chosen': -357.0496520996094, 'logits/rejected': -0.7734835147857666, 'logits/chosen': -0.6992802023887634, 'epoch': 2.95}

 49%|████▉     | 5287/10740 [26:18:24<22:58:55, 15.17s/it]


 49%|████▉     | 5289/10740 [26:19:05<27:09:23, 17.93s/it]
{'loss': 0.4236, 'learning_rate': 1.073068760425886e-06, 'rewards/chosen': -1.377108097076416, 'rewards/rejected': -3.0502312183380127, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6731232404708862, 'policy_logps/rejected': -344.10443115234375, 'policy_logps/chosen': -422.7939453125, 'referece_logps/rejected': -313.60211181640625, 'referece_logps/chosen': -409.02288818359375, 'logits/rejected': -0.8221162557601929, 'logits/chosen': -0.7510135769844055, 'epoch': 2.95}


 49%|████▉     | 5291/10740 [26:19:39<27:08:54, 17.94s/it]
[2024-04-02 21:33:22,938] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 5292/10740 [26:19:59<28:09:04, 18.60s/it]

 49%|████▉     | 5293/10740 [26:20:16<27:06:55, 17.92s/it]

 49%|████▉     | 5294/10740 [26:20:37<28:47:04, 19.03s/it]

 49%|████▉     | 5295/10740 [26:20:52<26:43:28, 17.67s/it]
{'loss': 0.4692, 'learning_rate': 1.0712639792854038e-06, 'rewards/chosen': -1.8745076656341553, 'rewards/rejected': -2.6065778732299805, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7320703864097595, 'policy_logps/rejected': -358.9225158691406, 'policy_logps/chosen': -429.10614013671875, 'referece_logps/rejected': -332.8567199707031, 'referece_logps/chosen': -410.36102294921875, 'logits/rejected': -0.7122824788093567, 'logits/chosen': -0.7229040861129761, 'epoch': 2.96}

 49%|████▉     | 5296/10740 [26:21:06<25:19:05, 16.74s/it]

 49%|████▉     | 5297/10740 [26:21:27<26:57:28, 17.83s/it]


 49%|████▉     | 5299/10740 [26:22:01<25:56:24, 17.16s/it]
{'loss': 0.3804, 'learning_rate': 1.070060661860969e-06, 'rewards/chosen': -1.8668100833892822, 'rewards/rejected': -3.8217897415161133, 'rewards/accuracies': 0.875, 'rewards/margins': 1.954979658126831, 'policy_logps/rejected': -282.97650146484375, 'policy_logps/chosen': -269.5179443359375, 'referece_logps/rejected': -244.75860595703125, 'referece_logps/chosen': -250.849853515625, 'logits/rejected': -0.7832719087600708, 'logits/chosen': -0.7352619767189026, 'epoch': 2.96}


 49%|████▉     | 5301/10740 [26:22:39<27:10:15, 17.98s/it]
{'loss': 0.3258, 'learning_rate': 1.0694589648061711e-06, 'rewards/chosen': -0.8097350597381592, 'rewards/rejected': -2.9133996963500977, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1036643981933594, 'policy_logps/rejected': -374.3583679199219, 'policy_logps/chosen': -327.3695068359375, 'referece_logps/rejected': -345.224365234375, 'referece_logps/chosen': -319.2721862792969, 'logits/rejected': -0.643726110458374, 'logits/chosen': -0.6446237564086914, 'epoch': 2.96}

 49%|████▉     | 5302/10740 [26:22:59<27:51:00, 18.44s/it]


 49%|████▉     | 5304/10740 [26:23:37<28:32:26, 18.90s/it]

 49%|████▉     | 5305/10740 [26:23:49<25:20:31, 16.79s/it]

 49%|████▉     | 5306/10740 [26:24:06<25:10:11, 16.67s/it]

 49%|████▉     | 5307/10740 [26:24:24<25:49:31, 17.11s/it]

 49%|████▉     | 5308/10740 [26:24:35<23:11:33, 15.37s/it]
{'loss': 0.3803, 'learning_rate': 1.0673528275509285e-06, 'rewards/chosen': -1.7977681159973145, 'rewards/rejected': -3.4745326042175293, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6767646074295044, 'policy_logps/rejected': -270.8389587402344, 'policy_logps/chosen': -230.7392120361328, 'referece_logps/rejected': -236.0936279296875, 'referece_logps/chosen': -212.7615509033203, 'logits/rejected': -0.8912042379379272, 'logits/chosen': -0.9435356855392456, 'epoch': 2.97}

 49%|████▉     | 5309/10740 [26:24:54<24:51:06, 16.47s/it]

 49%|████▉     | 5310/10740 [26:25:06<22:50:52, 15.15s/it]

 49%|████▉     | 5311/10740 [26:25:27<25:09:20, 16.68s/it]

 49%|████▉     | 5312/10740 [26:25:45<25:58:35, 17.23s/it]


 49%|████▉     | 5314/10740 [26:26:26<28:37:37, 18.99s/it]

 49%|████▉     | 5315/10740 [26:26:43<27:47:52, 18.45s/it]
{'loss': 0.4631, 'learning_rate': 1.0652463901264577e-06, 'rewards/chosen': -1.3666901588439941, 'rewards/rejected': -3.593644618988037, 'rewards/accuracies': 1.0, 'rewards/margins': 2.226954221725464, 'policy_logps/rejected': -448.97515869140625, 'policy_logps/chosen': -416.2945861816406, 'referece_logps/rejected': -413.0386657714844, 'referece_logps/chosen': -402.6277160644531, 'logits/rejected': -0.3035293221473694, 'logits/chosen': -0.2803020477294922, 'epoch': 2.97}


 50%|████▉     | 5317/10740 [26:27:22<28:30:13, 18.92s/it]

 50%|████▉     | 5318/10740 [26:27:42<28:57:24, 19.23s/it]
{'loss': 0.3876, 'learning_rate': 1.0643435416680665e-06, 'rewards/chosen': -1.4448472261428833, 'rewards/rejected': -2.725797653198242, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2809505462646484, 'policy_logps/rejected': -315.2491455078125, 'policy_logps/chosen': -423.7723083496094, 'referece_logps/rejected': -287.9911804199219, 'referece_logps/chosen': -409.3238525390625, 'logits/rejected': -0.2789664566516876, 'logits/chosen': -0.1525401771068573, 'epoch': 2.97}

 50%|████▉     | 5319/10740 [26:28:01<28:44:00, 19.08s/it]


 50%|████▉     | 5321/10740 [26:28:36<27:19:30, 18.15s/it]

 50%|████▉     | 5322/10740 [26:28:48<24:33:19, 16.32s/it]

 50%|████▉     | 5323/10740 [26:29:00<22:41:56, 15.09s/it]

 50%|████▉     | 5324/10740 [26:29:12<21:18:29, 14.16s/it]
{'loss': 0.421, 'learning_rate': 1.0625376874809055e-06, 'rewards/chosen': -1.4442194700241089, 'rewards/rejected': -2.402435064315796, 'rewards/accuracies': 0.625, 'rewards/margins': 0.958215594291687, 'policy_logps/rejected': -409.51422119140625, 'policy_logps/chosen': -323.1130065917969, 'referece_logps/rejected': -385.4898681640625, 'referece_logps/chosen': -308.6708068847656, 'logits/rejected': -0.6106377243995667, 'logits/chosen': -0.6163794994354248, 'epoch': 2.97}

 50%|████▉     | 5325/10740 [26:29:29<22:15:55, 14.80s/it]

 50%|████▉     | 5326/10740 [26:29:41<21:08:36, 14.06s/it]

 50%|████▉     | 5327/10740 [26:30:03<24:44:25, 16.45s/it]

 50%|████▉     | 5328/10740 [26:30:14<22:07:41, 14.72s/it]

 50%|████▉     | 5329/10740 [26:30:33<24:25:21, 16.25s/it]

 50%|████▉     | 5330/10740 [26:30:51<25:04:02, 16.68s/it]


 50%|████▉     | 5332/10740 [26:31:26<25:27:27, 16.95s/it]
{'loss': 0.4387, 'learning_rate': 1.0601295643946465e-06, 'rewards/chosen': -1.7465076446533203, 'rewards/rejected': -3.3674685955047607, 'rewards/accuracies': 0.625, 'rewards/margins': 1.62096107006073, 'policy_logps/rejected': -282.2547302246094, 'policy_logps/chosen': -265.1142883300781, 'referece_logps/rejected': -248.58001708984375, 'referece_logps/chosen': -247.6492156982422, 'logits/rejected': -1.2270880937576294, 'logits/chosen': -1.127916932106018, 'epoch': 2.98}


 50%|████▉     | 5334/10740 [26:32:00<25:08:56, 16.75s/it]
{'loss': 0.4656, 'learning_rate': 1.0595274783861876e-06, 'rewards/chosen': -1.716796875, 'rewards/rejected': -2.6215407848358154, 'rewards/accuracies': 0.5, 'rewards/margins': 0.9047437906265259, 'policy_logps/rejected': -436.077880859375, 'policy_logps/chosen': -346.04345703125, 'referece_logps/rejected': -409.86248779296875, 'referece_logps/chosen': -328.87548828125, 'logits/rejected': -0.5749391317367554, 'logits/chosen': -0.6110982298851013, 'epoch': 2.98}


 50%|████▉     | 5336/10740 [26:32:38<26:38:44, 17.75s/it]
{'loss': 0.4545, 'learning_rate': 1.0589253707210485e-06, 'rewards/chosen': -1.762951135635376, 'rewards/rejected': -2.5953407287597656, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8323898315429688, 'policy_logps/rejected': -424.70013427734375, 'policy_logps/chosen': -342.2142333984375, 'referece_logps/rejected': -398.7467346191406, 'referece_logps/chosen': -324.584716796875, 'logits/rejected': -0.6949087381362915, 'logits/chosen': -0.7549130320549011, 'epoch': 2.98}


 50%|████▉     | 5338/10740 [26:33:10<24:46:22, 16.51s/it]

 50%|████▉     | 5339/10740 [26:33:23<22:53:55, 15.26s/it]

 50%|████▉     | 5340/10740 [26:33:42<24:54:00, 16.60s/it]
{'loss': 0.463, 'learning_rate': 1.057721091296949e-06, 'rewards/chosen': -1.106212854385376, 'rewards/rejected': -2.3844401836395264, 'rewards/accuracies': 0.625, 'rewards/margins': 1.27822744846344, 'policy_logps/rejected': -458.8122253417969, 'policy_logps/chosen': -426.0137023925781, 'referece_logps/rejected': -434.9678649902344, 'referece_logps/chosen': -414.95159912109375, 'logits/rejected': -1.2193489074707031, 'logits/chosen': -1.1536295413970947, 'epoch': 2.98}

 50%|████▉     | 5341/10740 [26:34:02<26:12:49, 17.48s/it]

 50%|████▉     | 5342/10740 [26:34:21<27:06:00, 18.07s/it]

 50%|████▉     | 5343/10740 [26:34:37<26:07:41, 17.43s/it]

 50%|████▉     | 5344/10740 [26:34:53<25:16:01, 16.86s/it]


 50%|████▉     | 5346/10740 [26:35:22<23:42:05, 15.82s/it]
{'loss': 0.3491, 'learning_rate': 1.0559145152122673e-06, 'rewards/chosen': -2.3919341564178467, 'rewards/rejected': -3.6418240070343018, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2498899698257446, 'policy_logps/rejected': -325.35528564453125, 'policy_logps/chosen': -340.1502990722656, 'referece_logps/rejected': -288.93707275390625, 'referece_logps/chosen': -316.2309265136719, 'logits/rejected': -0.6492882370948792, 'logits/chosen': -0.6332831978797913, 'epoch': 2.99}


 50%|████▉     | 5348/10740 [26:36:03<27:14:32, 18.19s/it]
{'loss': 0.3255, 'learning_rate': 1.0553122822074237e-06, 'rewards/chosen': -1.264853835105896, 'rewards/rejected': -2.4083313941955566, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1434773206710815, 'policy_logps/rejected': -367.3060302734375, 'policy_logps/chosen': -388.01409912109375, 'referece_logps/rejected': -343.22271728515625, 'referece_logps/chosen': -375.36553955078125, 'logits/rejected': -0.7631574869155884, 'logits/chosen': -0.8128544092178345, 'epoch': 2.99}

 50%|████▉     | 5349/10740 [26:36:19<26:26:59, 17.66s/it]


 50%|████▉     | 5351/10740 [26:36:50<25:25:41, 16.99s/it]

 50%|████▉     | 5352/10740 [26:37:03<23:16:31, 15.55s/it]

 50%|████▉     | 5353/10740 [26:37:23<25:21:33, 16.95s/it]
{'loss': 0.3337, 'learning_rate': 1.053806612135828e-06, 'rewards/chosen': -1.2065974473953247, 'rewards/rejected': -4.083756446838379, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8771586418151855, 'policy_logps/rejected': -326.48516845703125, 'policy_logps/chosen': -380.892578125, 'referece_logps/rejected': -285.6475830078125, 'referece_logps/chosen': -368.8265686035156, 'logits/rejected': -0.23209457099437714, 'logits/chosen': -0.314165860414505, 'epoch': 2.99}


 50%|████▉     | 5355/10740 [26:37:57<25:18:03, 16.91s/it]
{'loss': 0.3843, 'learning_rate': 1.0532043096585611e-06, 'rewards/chosen': -1.6862473487854004, 'rewards/rejected': -3.050384998321533, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3641375303268433, 'policy_logps/rejected': -459.30804443359375, 'policy_logps/chosen': -389.73370361328125, 'referece_logps/rejected': -428.80419921875, 'referece_logps/chosen': -372.8712158203125, 'logits/rejected': -1.205350399017334, 'logits/chosen': -1.072753667831421, 'epoch': 2.99}


 50%|████▉     | 5357/10740 [26:38:33<26:22:35, 17.64s/it]
{'loss': 0.345, 'learning_rate': 1.052601987825045e-06, 'rewards/chosen': -1.7048840522766113, 'rewards/rejected': -4.3547563552856445, 'rewards/accuracies': 0.75, 'rewards/margins': 2.649872303009033, 'policy_logps/rejected': -413.7789611816406, 'policy_logps/chosen': -487.9837341308594, 'referece_logps/rejected': -370.2313537597656, 'referece_logps/chosen': -470.9349060058594, 'logits/rejected': -0.16898459196090698, 'logits/chosen': -0.1684136986732483, 'epoch': 2.99}

 50%|████▉     | 5358/10740 [26:38:51<26:53:10, 17.98s/it]

 50%|████▉     | 5359/10740 [26:39:11<27:44:28, 18.56s/it]
[2024-04-02 21:53:16,130] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|████▉     | 5360/10740 [26:39:32<28:52:38, 19.32s/it]

 50%|████▉     | 5361/10740 [26:39:54<30:00:46, 20.09s/it]

 50%|████▉     | 5362/10740 [26:40:13<29:30:00, 19.75s/it]

 50%|████▉     | 5363/10740 [26:40:30<28:00:09, 18.75s/it]
[2024-04-02 21:54:31,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|████▉     | 5364/10740 [26:40:47<27:32:15, 18.44s/it]

 50%|████▉     | 5365/10740 [26:41:08<28:23:05, 19.01s/it]

 50%|████▉     | 5366/10740 [26:41:30<29:45:35, 19.94s/it]

 50%|████▉     | 5367/10740 [26:41:42<26:08:35, 17.52s/it]

 50%|████▉     | 5368/10740 [26:41:58<25:34:55, 17.14s/it]

 50%|████▉     | 5369/10740 [26:42:09<22:39:15, 15.18s/it]


 50%|█████     | 5371/10740 [26:42:41<23:19:53, 15.64s/it]

 50%|█████     | 5372/10740 [26:43:01<25:10:28, 16.88s/it]
{'loss': 0.4282, 'learning_rate': 1.0480839792128469e-06, 'rewards/chosen': -1.2990503311157227, 'rewards/rejected': -2.3147075176239014, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0156571865081787, 'policy_logps/rejected': -502.9768371582031, 'policy_logps/chosen': -432.1416931152344, 'referece_logps/rejected': -479.82977294921875, 'referece_logps/chosen': -419.1511535644531, 'logits/rejected': -1.0823856592178345, 'logits/chosen': -1.0212929248809814, 'epoch': 3.0}

 50%|█████     | 5373/10740 [26:43:22<27:09:47, 18.22s/it]

 50%|█████     | 5374/10740 [26:43:40<27:02:11, 18.14s/it]
[2024-04-02 21:57:43,350] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 5375/10740 [26:44:00<27:33:33, 18.49s/it]
[2024-04-02 21:58:02,154] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 5376/10740 [26:44:18<27:41:36, 18.59s/it]

 50%|█████     | 5377/10740 [26:44:31<24:52:02, 16.69s/it]

 50%|█████     | 5378/10740 [26:44:46<24:07:23, 16.20s/it]

 50%|█████     | 5379/10740 [26:45:04<25:07:47, 16.88s/it]


 50%|█████     | 5381/10740 [26:45:45<27:59:55, 18.81s/it]
{'loss': 0.3746, 'learning_rate': 1.045372695805745e-06, 'rewards/chosen': -1.3933923244476318, 'rewards/rejected': -2.947404384613037, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5540118217468262, 'policy_logps/rejected': -313.06732177734375, 'policy_logps/chosen': -331.7645263671875, 'referece_logps/rejected': -283.5932922363281, 'referece_logps/chosen': -317.83062744140625, 'logits/rejected': -1.1025762557983398, 'logits/chosen': -1.0180788040161133, 'epoch': 3.01}

 50%|█████     | 5382/10740 [26:46:05<28:21:48, 19.06s/it]


 50%|█████     | 5384/10740 [26:46:31<23:41:29, 15.92s/it]
{'loss': 0.4603, 'learning_rate': 1.0444688594019833e-06, 'rewards/chosen': -2.233898878097534, 'rewards/rejected': -3.116255521774292, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8823566436767578, 'policy_logps/rejected': -401.7462463378906, 'policy_logps/chosen': -359.49407958984375, 'referece_logps/rejected': -370.58367919921875, 'referece_logps/chosen': -337.15509033203125, 'logits/rejected': -0.06352315843105316, 'logits/chosen': -0.059205859899520874, 'epoch': 3.01}


 50%|█████     | 5386/10740 [26:46:57<21:40:54, 14.58s/it]
{'loss': 0.4008, 'learning_rate': 1.043866281531046e-06, 'rewards/chosen': -1.0138601064682007, 'rewards/rejected': -3.06655216217041, 'rewards/accuracies': 1.0, 'rewards/margins': 2.052692413330078, 'policy_logps/rejected': -302.5465393066406, 'policy_logps/chosen': -428.7601623535156, 'referece_logps/rejected': -271.88104248046875, 'referece_logps/chosen': -418.6215515136719, 'logits/rejected': 0.6337289214134216, 'logits/chosen': 0.5902111530303955, 'epoch': 3.01}

 50%|█████     | 5387/10740 [26:47:19<24:45:24, 16.65s/it]


 50%|█████     | 5389/10740 [26:47:53<25:12:28, 16.96s/it]
{'loss': 0.3356, 'learning_rate': 1.0429623848700559e-06, 'rewards/chosen': -1.1225745677947998, 'rewards/rejected': -2.9124815464019775, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7899069786071777, 'policy_logps/rejected': -541.281005859375, 'policy_logps/chosen': -453.9681091308594, 'referece_logps/rejected': -512.1561889648438, 'referece_logps/chosen': -442.74237060546875, 'logits/rejected': 0.3775246739387512, 'logits/chosen': 0.2196119725704193, 'epoch': 3.01}

 50%|█████     | 5390/10740 [26:48:15<27:14:19, 18.33s/it]
[2024-04-02 22:02:18,405] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 5391/10740 [26:48:35<27:53:34, 18.77s/it]

 50%|█████     | 5392/10740 [26:48:52<27:25:50, 18.46s/it]
[2024-04-02 22:02:48,639] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 50%|█████     | 5394/10740 [26:49:16<22:03:56, 14.86s/it]
{'loss': 0.4651, 'learning_rate': 1.0414558126497898e-06, 'rewards/chosen': -1.7225435972213745, 'rewards/rejected': -3.1182069778442383, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3956631422042847, 'policy_logps/rejected': -351.2420349121094, 'policy_logps/chosen': -346.2059020996094, 'referece_logps/rejected': -320.05999755859375, 'referece_logps/chosen': -328.9804382324219, 'logits/rejected': -0.6860061883926392, 'logits/chosen': -0.6640329957008362, 'epoch': 3.01}

 50%|█████     | 5395/10740 [26:49:36<24:34:14, 16.55s/it]


 50%|█████     | 5397/10740 [26:50:12<25:37:59, 17.27s/it]
{'loss': 0.407, 'learning_rate': 1.0405518238522933e-06, 'rewards/chosen': -1.3334360122680664, 'rewards/rejected': -2.0575780868530273, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7241418957710266, 'policy_logps/rejected': -530.623046875, 'policy_logps/chosen': -547.64404296875, 'referece_logps/rejected': -510.04730224609375, 'referece_logps/chosen': -534.3096923828125, 'logits/rejected': -0.408603310585022, 'logits/chosen': -0.48825204372406006, 'epoch': 3.02}

 50%|█████     | 5398/10740 [26:50:32<27:04:56, 18.25s/it]

 50%|█████     | 5399/10740 [26:50:50<26:54:21, 18.14s/it]

 50%|█████     | 5400/10740 [26:51:06<26:05:08, 17.59s/it]

 50%|█████     | 5401/10740 [26:51:23<25:31:10, 17.21s/it]
[2024-04-02 22:05:26,840] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 5402/10740 [26:51:43<26:57:47, 18.18s/it]

 50%|█████     | 5403/10740 [26:52:04<28:20:50, 19.12s/it]

 50%|█████     | 5404/10740 [26:52:25<28:47:26, 19.42s/it]

 50%|█████     | 5405/10740 [26:52:44<28:58:48, 19.56s/it]


 50%|█████     | 5407/10740 [26:53:16<25:35:12, 17.27s/it]
{'loss': 0.2937, 'learning_rate': 1.037538292278823e-06, 'rewards/chosen': -1.4653319120407104, 'rewards/rejected': -3.4394407272338867, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9741086959838867, 'policy_logps/rejected': -358.6553955078125, 'policy_logps/chosen': -285.31903076171875, 'referece_logps/rejected': -324.260986328125, 'referece_logps/chosen': -270.6656799316406, 'logits/rejected': -0.6901901960372925, 'logits/chosen': -0.7366292476654053, 'epoch': 3.02}

 50%|█████     | 5408/10740 [26:53:37<27:15:35, 18.41s/it]
[2024-04-02 22:07:40,186] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 50%|█████     | 5410/10740 [26:54:16<28:00:36, 18.92s/it]

 50%|█████     | 5411/10740 [26:54:36<28:28:26, 19.24s/it]

 50%|█████     | 5412/10740 [26:54:56<28:53:56, 19.53s/it]
[2024-04-02 22:08:39,758] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4555, 'learning_rate': 1.0360313967465199e-06, 'rewards/chosen': -0.8897961974143982, 'rewards/rejected': -2.336984157562256, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4471880197525024, 'policy_logps/rejected': -453.7456359863281, 'policy_logps/chosen': -252.10116577148438, 'referece_logps/rejected': -430.375732421875, 'referece_logps/chosen': -243.20321655273438, 'logits/rejected': -0.36282020807266235, 'logits/chosen': -0.49079811573028564, 'epoch': 3.02}

 50%|█████     | 5413/10740 [26:55:14<28:18:10, 19.13s/it]

 50%|█████     | 5414/10740 [26:55:31<27:28:12, 18.57s/it]

 50%|█████     | 5415/10740 [26:55:54<29:02:12, 19.63s/it]
[2024-04-02 22:09:56,668] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 50%|█████     | 5417/10740 [26:56:34<29:33:37, 19.99s/it]
[2024-04-02 22:10:17,705] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.442, 'learning_rate': 1.034524419285635e-06, 'rewards/chosen': -1.8742835521697998, 'rewards/rejected': -2.994088649749756, 'rewards/accuracies': 0.875, 'rewards/margins': 1.119805097579956, 'policy_logps/rejected': -357.5328674316406, 'policy_logps/chosen': -458.46002197265625, 'referece_logps/rejected': -327.59197998046875, 'referece_logps/chosen': -439.7171936035156, 'logits/rejected': -0.3570931553840637, 'logits/chosen': -0.34952014684677124, 'epoch': 3.03}
[2024-04-02 22:10:30,975] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 5418/10740 [26:56:47<26:34:26, 17.98s/it]


 50%|█████     | 5420/10740 [26:57:16<24:10:55, 16.36s/it]
{'loss': 0.4872, 'learning_rate': 1.0336201949088343e-06, 'rewards/chosen': -1.6493825912475586, 'rewards/rejected': -2.6154065132141113, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9660240411758423, 'policy_logps/rejected': -325.5543212890625, 'policy_logps/chosen': -322.8620910644531, 'referece_logps/rejected': -299.4002685546875, 'referece_logps/chosen': -306.3682556152344, 'logits/rejected': -0.361819863319397, 'logits/chosen': -0.4713524878025055, 'epoch': 3.03}

 50%|█████     | 5421/10740 [26:57:27<21:41:10, 14.68s/it]

 50%|█████     | 5422/10740 [26:57:37<19:56:08, 13.50s/it]

 50%|█████     | 5423/10740 [26:57:57<22:27:31, 15.21s/it]
[2024-04-02 22:12:00,215] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5424/10740 [26:58:16<24:31:17, 16.61s/it]

 51%|█████     | 5425/10740 [26:58:32<23:51:31, 16.16s/it]


 51%|█████     | 5427/10740 [26:59:06<24:30:26, 16.61s/it]

 51%|█████     | 5428/10740 [26:59:22<24:11:44, 16.40s/it]
{'loss': 0.3522, 'learning_rate': 1.0312087973695347e-06, 'rewards/chosen': -1.5799715518951416, 'rewards/rejected': -3.4609577655792236, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8809863328933716, 'policy_logps/rejected': -335.552978515625, 'policy_logps/chosen': -304.904052734375, 'referece_logps/rejected': -300.9433898925781, 'referece_logps/chosen': -289.1043395996094, 'logits/rejected': -0.15490029752254486, 'logits/chosen': -0.27045848965644836, 'epoch': 3.03}


 51%|█████     | 5430/10740 [27:00:00<26:29:18, 17.96s/it]
{'loss': 0.3826, 'learning_rate': 1.0306059190512268e-06, 'rewards/chosen': -2.147552251815796, 'rewards/rejected': -2.454525947570801, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3069738745689392, 'policy_logps/rejected': -370.4070129394531, 'policy_logps/chosen': -403.13525390625, 'referece_logps/rejected': -345.86175537109375, 'referece_logps/chosen': -381.6596984863281, 'logits/rejected': -0.24487179517745972, 'logits/chosen': -0.2244904488325119, 'epoch': 3.03}

 51%|█████     | 5431/10740 [27:00:21<27:48:19, 18.85s/it]

 51%|█████     | 5432/10740 [27:00:39<27:13:59, 18.47s/it]
[2024-04-02 22:14:43,482] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5433/10740 [27:01:00<28:19:59, 19.22s/it]


 51%|█████     | 5435/10740 [27:01:40<29:10:47, 19.80s/it]
{'loss': 0.3593, 'learning_rate': 1.0290986750208e-06, 'rewards/chosen': -1.4041696786880493, 'rewards/rejected': -3.320240020751953, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9160703420639038, 'policy_logps/rejected': -375.4040832519531, 'policy_logps/chosen': -314.7733459472656, 'referece_logps/rejected': -342.2016906738281, 'referece_logps/chosen': -300.7316589355469, 'logits/rejected': -0.16968421638011932, 'logits/chosen': -0.2685546875, 'epoch': 3.04}

 51%|█████     | 5436/10740 [27:02:00<29:05:34, 19.75s/it]

 51%|█████     | 5437/10740 [27:02:17<28:02:25, 19.04s/it]

 51%|█████     | 5438/10740 [27:02:35<27:29:06, 18.66s/it]

 51%|█████     | 5439/10740 [27:02:56<28:29:57, 19.35s/it]

 51%|█████     | 5440/10740 [27:03:13<27:21:33, 18.58s/it]

 51%|█████     | 5441/10740 [27:03:30<26:57:05, 18.31s/it]

 51%|█████     | 5442/10740 [27:03:52<28:21:39, 19.27s/it]

 51%|█████     | 5443/10740 [27:04:12<28:53:14, 19.63s/it]

 51%|█████     | 5444/10740 [27:04:32<28:51:38, 19.62s/it]

 51%|█████     | 5445/10740 [27:04:52<28:50:50, 19.61s/it]

 51%|█████     | 5446/10740 [27:05:13<29:37:14, 20.14s/it]

 51%|█████     | 5447/10740 [27:05:33<29:24:08, 20.00s/it]

 51%|█████     | 5448/10740 [27:05:46<26:39:40, 18.14s/it]

 51%|█████     | 5449/10740 [27:06:08<28:12:00, 19.19s/it]

 51%|█████     | 5450/10740 [27:06:29<28:53:07, 19.66s/it]

 51%|█████     | 5451/10740 [27:06:43<26:15:48, 17.88s/it]

 51%|█████     | 5452/10740 [27:07:01<26:41:19, 18.17s/it]

 51%|█████     | 5453/10740 [27:07:21<27:13:57, 18.54s/it]
[2024-04-02 22:21:25,899] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5454/10740 [27:07:42<28:26:56, 19.38s/it]
[2024-04-02 22:21:47,279] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5455/10740 [27:08:04<29:19:35, 19.98s/it]

 51%|█████     | 5456/10740 [27:08:22<28:51:20, 19.66s/it]

 51%|█████     | 5457/10740 [27:08:40<27:53:41, 19.01s/it]

 51%|█████     | 5458/10740 [27:08:55<26:12:57, 17.87s/it]

 51%|█████     | 5459/10740 [27:09:10<25:00:32, 17.05s/it]

 51%|█████     | 5460/10740 [27:09:27<24:46:05, 16.89s/it]

 51%|█████     | 5461/10740 [27:09:44<24:48:10, 16.91s/it]

 51%|█████     | 5462/10740 [27:10:05<26:44:48, 18.24s/it]

 51%|█████     | 5463/10740 [27:10:24<27:03:49, 18.46s/it]
[2024-04-02 22:24:29,179] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5464/10740 [27:10:45<28:19:22, 19.33s/it]
[2024-04-02 22:24:49,105] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5465/10740 [27:11:05<28:34:52, 19.51s/it]

 51%|█████     | 5466/10740 [27:11:17<25:13:00, 17.21s/it]

 51%|█████     | 5467/10740 [27:11:32<23:57:43, 16.36s/it]

 51%|█████     | 5468/10740 [27:11:55<26:53:24, 18.36s/it]

 51%|█████     | 5469/10740 [27:12:07<24:05:26, 16.45s/it]

 51%|█████     | 5470/10740 [27:12:19<22:28:52, 15.36s/it]

 51%|█████     | 5471/10740 [27:12:42<25:40:06, 17.54s/it]

 51%|█████     | 5472/10740 [27:12:57<24:31:07, 16.76s/it]

 51%|█████     | 5473/10740 [27:13:11<23:11:02, 15.85s/it]

 51%|█████     | 5474/10740 [27:13:25<22:38:22, 15.48s/it]
[2024-04-02 22:27:27,341] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5475/10740 [27:13:44<23:51:45, 16.32s/it]

 51%|█████     | 5476/10740 [27:13:59<23:39:47, 16.18s/it]

 51%|█████     | 5477/10740 [27:14:14<22:44:39, 15.56s/it]

 51%|█████     | 5478/10740 [27:14:33<24:26:35, 16.72s/it]

 51%|█████     | 5479/10740 [27:14:53<26:00:42, 17.80s/it]

 51%|█████     | 5480/10740 [27:15:16<27:55:43, 19.11s/it]

 51%|█████     | 5481/10740 [27:15:37<28:52:47, 19.77s/it]

 51%|█████     | 5482/10740 [27:15:58<29:39:37, 20.31s/it]

 51%|█████     | 5483/10740 [27:16:15<28:01:32, 19.19s/it]

 51%|█████     | 5484/10740 [27:16:36<28:45:09, 19.69s/it]

 51%|█████     | 5485/10740 [27:16:48<25:28:53, 17.46s/it]

 51%|█████     | 5486/10740 [27:17:02<24:03:41, 16.49s/it]
[2024-04-02 22:31:09,141] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5487/10740 [27:17:25<26:57:36, 18.48s/it]

 51%|█████     | 5488/10740 [27:17:46<27:44:05, 19.01s/it]
[2024-04-02 22:31:50,866] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5489/10740 [27:18:07<28:48:16, 19.75s/it]

 51%|█████     | 5490/10740 [27:18:28<29:12:06, 20.02s/it]

 51%|█████     | 5491/10740 [27:18:43<27:14:12, 18.68s/it]

 51%|█████     | 5492/10740 [27:19:02<27:05:09, 18.58s/it]

 51%|█████     | 5493/10740 [27:19:16<25:21:28, 17.40s/it]

 51%|█████     | 5494/10740 [27:19:31<24:21:34, 16.72s/it]

 51%|█████     | 5495/10740 [27:19:48<24:07:50, 16.56s/it]

 51%|█████     | 5496/10740 [27:20:09<26:14:35, 18.02s/it]
[2024-04-02 22:34:16,278] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5497/10740 [27:20:33<28:37:25, 19.65s/it]
[2024-04-02 22:34:36,826] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5498/10740 [27:20:53<29:00:31, 19.92s/it]

 51%|█████     | 5499/10740 [27:21:10<27:44:37, 19.06s/it]

 51%|█████     | 5500/10740 [27:21:30<27:56:01, 19.19s/it]
[2024-04-02 22:35:45,890] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5501/10740 [27:22:02<33:44:53, 23.19s/it]

 51%|█████     | 5502/10740 [27:22:20<31:36:47, 21.73s/it]
[2024-04-02 22:36:23,125] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 5503/10740 [27:22:39<30:22:56, 20.89s/it]

 51%|█████     | 5504/10740 [27:23:01<30:41:40, 21.10s/it]

 51%|█████▏    | 5505/10740 [27:23:19<29:19:20, 20.16s/it]

 51%|█████▏    | 5506/10740 [27:23:38<28:59:18, 19.94s/it]

 51%|█████▏    | 5507/10740 [27:24:01<30:00:13, 20.64s/it]

 51%|█████▏    | 5508/10740 [27:24:22<30:28:32, 20.97s/it]

 51%|█████▏    | 5509/10740 [27:24:42<29:59:14, 20.64s/it]

 51%|█████▏    | 5510/10740 [27:25:04<30:19:52, 20.88s/it]


 51%|█████▏    | 5512/10740 [27:25:35<26:01:17, 17.92s/it]

 51%|█████▏    | 5513/10740 [27:25:47<23:32:30, 16.21s/it]

 51%|█████▏    | 5514/10740 [27:26:04<23:54:56, 16.47s/it]
[2024-04-02 22:39:47,579] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████▏    | 5515/10740 [27:26:22<24:27:21, 16.85s/it]

 51%|█████▏    | 5516/10740 [27:26:45<27:11:44, 18.74s/it]

 51%|█████▏    | 5517/10740 [27:27:02<26:38:37, 18.36s/it]

 51%|█████▏    | 5518/10740 [27:27:22<27:04:17, 18.66s/it]

 51%|█████▏    | 5519/10740 [27:27:40<26:57:01, 18.58s/it]

 51%|█████▏    | 5520/10740 [27:28:02<28:16:31, 19.50s/it]
[2024-04-02 22:41:45,341] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4685, 'learning_rate': 1.0034682003540668e-06, 'rewards/chosen': -2.2875659465789795, 'rewards/rejected': -2.5882434844970703, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3006773293018341, 'policy_logps/rejected': -342.89404296875, 'policy_logps/chosen': -345.8766174316406, 'referece_logps/rejected': -317.0115966796875, 'referece_logps/chosen': -323.0009765625, 'logits/rejected': -0.6194716095924377, 'logits/chosen': -0.5371066927909851, 'epoch': 3.08}


 51%|█████▏    | 5522/10740 [27:28:46<30:09:05, 20.80s/it]
[2024-04-02 22:42:29,626] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████▏    | 5523/10740 [27:29:07<30:25:52, 21.00s/it]
[2024-04-02 22:42:51,086] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████▏    | 5524/10740 [27:29:21<27:12:08, 18.77s/it]
{'loss': 0.4291, 'learning_rate': 1.0022618724019534e-06, 'rewards/chosen': -1.6933951377868652, 'rewards/rejected': -3.0345089435577393, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3411136865615845, 'policy_logps/rejected': -391.543701171875, 'policy_logps/chosen': -370.89111328125, 'referece_logps/rejected': -361.1985778808594, 'referece_logps/chosen': -353.95721435546875, 'logits/rejected': -0.396091490983963, 'logits/chosen': -0.330478698015213, 'epoch': 3.09}


 51%|█████▏    | 5526/10740 [27:29:56<26:07:42, 18.04s/it]

 51%|█████▏    | 5527/10740 [27:30:12<25:36:02, 17.68s/it]

 51%|█████▏    | 5528/10740 [27:30:31<26:06:06, 18.03s/it]

 51%|█████▏    | 5529/10740 [27:30:43<23:26:10, 16.19s/it]

 51%|█████▏    | 5530/10740 [27:31:03<25:06:16, 17.35s/it]

 51%|█████▏    | 5531/10740 [27:31:22<25:34:50, 17.68s/it]

 52%|█████▏    | 5532/10740 [27:31:39<25:26:30, 17.59s/it]

 52%|█████▏    | 5533/10740 [27:31:52<23:24:31, 16.18s/it]

 52%|█████▏    | 5534/10740 [27:32:12<24:55:31, 17.24s/it]

 52%|█████▏    | 5535/10740 [27:32:25<23:22:22, 16.17s/it]

 52%|█████▏    | 5536/10740 [27:32:41<23:11:55, 16.05s/it]

 52%|█████▏    | 5537/10740 [27:32:53<21:18:24, 14.74s/it]

 52%|█████▏    | 5538/10740 [27:33:10<22:09:51, 15.34s/it]

 52%|█████▏    | 5539/10740 [27:33:21<20:33:45, 14.23s/it]

 52%|█████▏    | 5540/10740 [27:33:36<20:36:36, 14.27s/it]

 52%|█████▏    | 5541/10740 [27:33:55<23:03:28, 15.97s/it]
{'loss': 0.3255, 'learning_rate': 9.971349631008258e-07, 'rewards/chosen': -1.599057674407959, 'rewards/rejected': -2.5931732654571533, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9941156506538391, 'policy_logps/rejected': -474.67401123046875, 'policy_logps/chosen': -547.0108032226562, 'referece_logps/rejected': -448.7422790527344, 'referece_logps/chosen': -531.0203247070312, 'logits/rejected': -1.6112955808639526, 'logits/chosen': -1.4250471591949463, 'epoch': 3.1}


 52%|█████▏    | 5543/10740 [27:34:24<22:16:45, 15.43s/it]

 52%|█████▏    | 5544/10740 [27:34:46<25:00:12, 17.32s/it]
[2024-04-02 22:48:29,776] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 52%|█████▏    | 5545/10740 [27:35:06<26:07:08, 18.10s/it]

 52%|█████▏    | 5546/10740 [27:35:26<26:49:31, 18.59s/it]

 52%|█████▏    | 5547/10740 [27:35:46<27:24:22, 19.00s/it]

 52%|█████▏    | 5548/10740 [27:36:04<26:59:16, 18.71s/it]
{'loss': 0.4009, 'learning_rate': 9.950238970089417e-07, 'rewards/chosen': -1.1443241834640503, 'rewards/rejected': -2.228236436843872, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0839118957519531, 'policy_logps/rejected': -365.0880126953125, 'policy_logps/chosen': -417.0645751953125, 'referece_logps/rejected': -342.8056640625, 'referece_logps/chosen': -405.6213073730469, 'logits/rejected': -0.027174588292837143, 'logits/chosen': 0.05447070300579071, 'epoch': 3.1}

 52%|█████▏    | 5549/10740 [27:36:21<26:15:34, 18.21s/it]


 52%|█████▏    | 5551/10740 [27:37:01<27:49:12, 19.30s/it]

 52%|█████▏    | 5552/10740 [27:37:20<27:57:32, 19.40s/it]
{'loss': 0.3346, 'learning_rate': 9.938175828813687e-07, 'rewards/chosen': -1.0530651807785034, 'rewards/rejected': -2.24049973487854, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1874346733093262, 'policy_logps/rejected': -200.37962341308594, 'policy_logps/chosen': -261.7314453125, 'referece_logps/rejected': -177.97462463378906, 'referece_logps/chosen': -251.20082092285156, 'logits/rejected': -0.9829397797584534, 'logits/chosen': -1.0260645151138306, 'epoch': 3.1}


 52%|█████▏    | 5554/10740 [27:37:58<27:22:22, 19.00s/it]
{'loss': 0.3679, 'learning_rate': 9.932144290817012e-07, 'rewards/chosen': -0.2345246523618698, 'rewards/rejected': -3.2408878803253174, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0063633918762207, 'policy_logps/rejected': -233.6827850341797, 'policy_logps/chosen': -278.4776916503906, 'referece_logps/rejected': -201.27389526367188, 'referece_logps/chosen': -276.1324157714844, 'logits/rejected': -0.6564357280731201, 'logits/chosen': -0.7429574728012085, 'epoch': 3.1}

 52%|█████▏    | 5555/10740 [27:38:19<28:15:17, 19.62s/it]


 52%|█████▏    | 5557/10740 [27:39:03<29:56:02, 20.79s/it]
{'loss': 0.2694, 'learning_rate': 9.923097030795048e-07, 'rewards/chosen': -1.6414135694503784, 'rewards/rejected': -3.9187610149383545, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2773473262786865, 'policy_logps/rejected': -493.07861328125, 'policy_logps/chosen': -408.2242431640625, 'referece_logps/rejected': -453.8910217285156, 'referece_logps/chosen': -391.8100891113281, 'logits/rejected': 0.2720981240272522, 'logits/chosen': 0.24662552773952484, 'epoch': 3.1}


 52%|█████▏    | 5559/10740 [27:39:28<23:43:30, 16.49s/it]

 52%|█████▏    | 5560/10740 [27:39:39<21:15:23, 14.77s/it]

 52%|█████▏    | 5561/10740 [27:39:50<19:35:39, 13.62s/it]

 52%|█████▏    | 5562/10740 [27:40:00<18:24:22, 12.80s/it]

 52%|█████▏    | 5563/10740 [27:40:16<19:41:37, 13.69s/it]

 52%|█████▏    | 5564/10740 [27:40:36<22:30:11, 15.65s/it]
[2024-04-02 22:54:20,166] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4879, 'learning_rate': 9.901987015018536e-07, 'rewards/chosen': -3.1543831825256348, 'rewards/rejected': -3.8458521366119385, 'rewards/accuracies': 0.25, 'rewards/margins': 0.6914691925048828, 'policy_logps/rejected': -544.6519775390625, 'policy_logps/chosen': -461.5882568359375, 'referece_logps/rejected': -506.1934509277344, 'referece_logps/chosen': -430.0444030761719, 'logits/rejected': -0.304868221282959, 'logits/chosen': -0.4230848550796509, 'epoch': 3.11}


 52%|█████▏    | 5566/10740 [27:41:11<23:54:38, 16.64s/it]

 52%|█████▏    | 5567/10740 [27:41:32<25:58:29, 18.08s/it]
{'loss': 0.4376, 'learning_rate': 9.89293999363213e-07, 'rewards/chosen': -1.9622656106948853, 'rewards/rejected': -2.91512131690979, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9528557062149048, 'policy_logps/rejected': -338.7681884765625, 'policy_logps/chosen': -426.2355041503906, 'referece_logps/rejected': -309.6169738769531, 'referece_logps/chosen': -406.6128845214844, 'logits/rejected': -1.2332301139831543, 'logits/chosen': -1.216546654701233, 'epoch': 3.11}


 52%|█████▏    | 5569/10740 [27:42:02<23:18:03, 16.22s/it]

 52%|█████▏    | 5570/10740 [27:42:16<22:17:12, 15.52s/it]
{'loss': 0.442, 'learning_rate': 9.883893059882052e-07, 'rewards/chosen': -2.127800464630127, 'rewards/rejected': -3.731175422668457, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6033753156661987, 'policy_logps/rejected': -598.6571044921875, 'policy_logps/chosen': -563.834716796875, 'referece_logps/rejected': -561.3453979492188, 'referece_logps/chosen': -542.5567626953125, 'logits/rejected': -1.0828007459640503, 'logits/chosen': -0.7959523797035217, 'epoch': 3.11}

 52%|█████▏    | 5571/10740 [27:42:27<20:32:25, 14.31s/it]

 52%|█████▏    | 5572/10740 [27:42:48<23:08:12, 16.12s/it]

 52%|█████▏    | 5573/10740 [27:43:02<22:12:51, 15.48s/it]


 52%|█████▏    | 5575/10740 [27:43:44<26:15:36, 18.30s/it]

 52%|█████▏    | 5576/10740 [27:43:59<24:53:46, 17.36s/it]
{'loss': 0.4276, 'learning_rate': 9.865799484913065e-07, 'rewards/chosen': -2.585437297821045, 'rewards/rejected': -3.949820041656494, 'rewards/accuracies': 1.0, 'rewards/margins': 1.364382266998291, 'policy_logps/rejected': -424.4894104003906, 'policy_logps/chosen': -337.22564697265625, 'referece_logps/rejected': -384.9912414550781, 'referece_logps/chosen': -311.37127685546875, 'logits/rejected': 0.3200925886631012, 'logits/chosen': 0.3446522653102875, 'epoch': 3.12}


 52%|█████▏    | 5578/10740 [27:44:33<24:29:10, 17.08s/it]

 52%|█████▏    | 5579/10740 [27:44:51<24:43:34, 17.25s/it]

 52%|█████▏    | 5580/10740 [27:45:03<22:25:11, 15.64s/it]
{'loss': 0.3869, 'learning_rate': 9.85373734206071e-07, 'rewards/chosen': -2.331960916519165, 'rewards/rejected': -3.1791162490844727, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8471551537513733, 'policy_logps/rejected': -396.75238037109375, 'policy_logps/chosen': -389.82281494140625, 'referece_logps/rejected': -364.9612121582031, 'referece_logps/chosen': -366.50323486328125, 'logits/rejected': -0.8970420360565186, 'logits/chosen': -0.8427821397781372, 'epoch': 3.12}


 52%|█████▏    | 5582/10740 [27:45:36<22:47:44, 15.91s/it]

 52%|█████▏    | 5583/10740 [27:45:55<23:51:47, 16.66s/it]
{'loss': 0.4324, 'learning_rate': 9.844690873642437e-07, 'rewards/chosen': -2.838548421859741, 'rewards/rejected': -3.4496490955352783, 'rewards/accuracies': 0.625, 'rewards/margins': 0.611100971698761, 'policy_logps/rejected': -544.7977294921875, 'policy_logps/chosen': -409.6795349121094, 'referece_logps/rejected': -510.30126953125, 'referece_logps/chosen': -381.2940673828125, 'logits/rejected': -0.8609369397163391, 'logits/chosen': -0.7229006290435791, 'epoch': 3.12}

 52%|█████▏    | 5584/10740 [27:46:14<24:42:43, 17.25s/it]


 52%|█████▏    | 5586/10740 [27:46:55<26:55:53, 18.81s/it]

 52%|█████▏    | 5587/10740 [27:47:13<26:36:04, 18.58s/it]
{'loss': 0.3671, 'learning_rate': 9.832629114791737e-07, 'rewards/chosen': -1.268500804901123, 'rewards/rejected': -4.5273284912109375, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2588274478912354, 'policy_logps/rejected': -537.3240966796875, 'policy_logps/chosen': -409.4263916015625, 'referece_logps/rejected': -492.0508728027344, 'referece_logps/chosen': -396.74139404296875, 'logits/rejected': -0.43881145119667053, 'logits/chosen': -0.5239553451538086, 'epoch': 3.12}

 52%|█████▏    | 5588/10740 [27:47:30<25:52:46, 18.08s/it]

 52%|█████▏    | 5589/10740 [27:47:46<25:01:53, 17.49s/it]


 52%|█████▏    | 5591/10740 [27:48:13<22:24:39, 15.67s/it]
{'loss': 0.3283, 'learning_rate': 9.820567599505688e-07, 'rewards/chosen': -1.918717861175537, 'rewards/rejected': -3.7566959857940674, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8379778861999512, 'policy_logps/rejected': -437.8645324707031, 'policy_logps/chosen': -521.4827880859375, 'referece_logps/rejected': -400.297607421875, 'referece_logps/chosen': -502.29559326171875, 'logits/rejected': 0.13095858693122864, 'logits/chosen': 0.2285248041152954, 'epoch': 3.12}

 52%|█████▏    | 5592/10740 [27:48:26<21:03:39, 14.73s/it]

 52%|█████▏    | 5593/10740 [27:48:46<23:20:50, 16.33s/it]


 52%|█████▏    | 5595/10740 [27:49:12<20:48:06, 14.56s/it]

 52%|█████▏    | 5596/10740 [27:49:33<23:26:37, 16.41s/it]
{'loss': 0.4598, 'learning_rate': 9.805491074650847e-07, 'rewards/chosen': -2.003023147583008, 'rewards/rejected': -3.237302780151367, 'rewards/accuracies': 0.875, 'rewards/margins': 1.234279990196228, 'policy_logps/rejected': -469.4205322265625, 'policy_logps/chosen': -430.801025390625, 'referece_logps/rejected': -437.0475158691406, 'referece_logps/chosen': -410.77081298828125, 'logits/rejected': 0.2249828726053238, 'logits/chosen': 0.21643832325935364, 'epoch': 3.13}


 52%|█████▏    | 5598/10740 [27:50:06<23:22:46, 16.37s/it]
{'loss': 0.4101, 'learning_rate': 9.799460586626622e-07, 'rewards/chosen': -1.695070743560791, 'rewards/rejected': -3.1362366676330566, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4411659240722656, 'policy_logps/rejected': -369.1333923339844, 'policy_logps/chosen': -362.9053955078125, 'referece_logps/rejected': -337.77099609375, 'referece_logps/chosen': -345.9546813964844, 'logits/rejected': -1.0411150455474854, 'logits/chosen': -0.8345895409584045, 'epoch': 3.13}


 52%|█████▏    | 5600/10740 [27:50:43<24:33:40, 17.20s/it]

 52%|█████▏    | 5601/10740 [27:51:01<25:03:58, 17.56s/it]
{'loss': 0.3833, 'learning_rate': 9.790414992072518e-07, 'rewards/chosen': -1.4762483835220337, 'rewards/rejected': -4.7083353996276855, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2320868968963623, 'policy_logps/rejected': -354.474609375, 'policy_logps/chosen': -347.43841552734375, 'referece_logps/rejected': -307.3912658691406, 'referece_logps/chosen': -332.67596435546875, 'logits/rejected': -0.3430134654045105, 'logits/chosen': -0.44643718004226685, 'epoch': 3.13}


 52%|█████▏    | 5603/10740 [27:51:43<27:30:54, 19.28s/it]

 52%|█████▏    | 5604/10740 [27:51:59<25:52:59, 18.14s/it]

 52%|█████▏    | 5605/10740 [27:52:17<26:10:18, 18.35s/it]

 52%|█████▏    | 5606/10740 [27:52:33<25:01:27, 17.55s/it]
{'loss': 0.3836, 'learning_rate': 9.775339386050865e-07, 'rewards/chosen': -1.4700238704681396, 'rewards/rejected': -2.7747750282287598, 'rewards/accuracies': 0.875, 'rewards/margins': 1.304750919342041, 'policy_logps/rejected': -422.76312255859375, 'policy_logps/chosen': -470.6379699707031, 'referece_logps/rejected': -395.0153503417969, 'referece_logps/chosen': -455.9377136230469, 'logits/rejected': -0.6467254161834717, 'logits/chosen': -0.7163658142089844, 'epoch': 3.13}

 52%|█████▏    | 5607/10740 [27:52:56<27:29:31, 19.28s/it]


 52%|█████▏    | 5609/10740 [27:53:38<28:34:43, 20.05s/it]

 52%|█████▏    | 5610/10740 [27:53:55<27:39:28, 19.41s/it]

 52%|█████▏    | 5611/10740 [27:54:14<27:09:05, 19.06s/it]

 52%|█████▏    | 5612/10740 [27:54:29<25:20:34, 17.79s/it]

 52%|█████▏    | 5613/10740 [27:54:49<26:37:05, 18.69s/it]

 52%|█████▏    | 5614/10740 [27:55:09<27:12:49, 19.11s/it]

 52%|█████▏    | 5615/10740 [27:55:25<25:38:32, 18.01s/it]
{'loss': 0.4245, 'learning_rate': 9.748204605552789e-07, 'rewards/chosen': -1.4359753131866455, 'rewards/rejected': -2.9837231636047363, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5477478504180908, 'policy_logps/rejected': -477.7901916503906, 'policy_logps/chosen': -409.396728515625, 'referece_logps/rejected': -447.95294189453125, 'referece_logps/chosen': -395.0369873046875, 'logits/rejected': -0.0471235066652298, 'logits/chosen': -0.0067778825759887695, 'epoch': 3.14}

 52%|█████▏    | 5616/10740 [27:55:42<25:26:49, 17.88s/it]

 52%|█████▏    | 5617/10740 [27:55:59<24:42:14, 17.36s/it]


 52%|█████▏    | 5619/10740 [27:56:33<24:50:22, 17.46s/it]

 52%|█████▏    | 5620/10740 [27:56:55<26:45:15, 18.81s/it]

 52%|█████▏    | 5621/10740 [27:57:12<25:45:19, 18.11s/it]

 52%|█████▏    | 5622/10740 [27:57:32<26:51:32, 18.89s/it]

 52%|█████▏    | 5623/10740 [27:57:53<27:26:44, 19.31s/it]

 52%|█████▏    | 5624/10740 [27:58:13<27:45:10, 19.53s/it]

 52%|█████▏    | 5625/10740 [27:58:32<27:47:48, 19.56s/it]

 52%|█████▏    | 5626/10740 [27:58:52<27:48:06, 19.57s/it]

 52%|█████▏    | 5627/10740 [27:59:12<27:57:14, 19.68s/it]

 52%|█████▏    | 5628/10740 [27:59:34<28:56:50, 20.39s/it]

 52%|█████▏    | 5629/10740 [27:59:49<26:57:14, 18.99s/it]

 52%|█████▏    | 5630/10740 [28:00:12<28:31:16, 20.09s/it]

 52%|█████▏    | 5631/10740 [28:00:28<26:37:56, 18.77s/it]

 52%|█████▏    | 5632/10740 [28:00:47<26:59:07, 19.02s/it]

 52%|█████▏    | 5633/10740 [28:01:01<24:30:36, 17.28s/it]

 52%|█████▏    | 5634/10740 [28:01:14<22:53:38, 16.14s/it]

 52%|█████▏    | 5635/10740 [28:01:35<24:46:08, 17.47s/it]
{'loss': 0.3807, 'learning_rate': 9.687912026304653e-07, 'rewards/chosen': -1.6073659658432007, 'rewards/rejected': -2.679492712020874, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0721269845962524, 'policy_logps/rejected': -396.6769104003906, 'policy_logps/chosen': -409.739501953125, 'referece_logps/rejected': -369.8819580078125, 'referece_logps/chosen': -393.66583251953125, 'logits/rejected': -0.8882139921188354, 'logits/chosen': -0.8160268068313599, 'epoch': 3.15}


 52%|█████▏    | 5637/10740 [28:02:14<26:35:16, 18.76s/it]

 52%|█████▏    | 5638/10740 [28:02:36<27:56:30, 19.72s/it]

 53%|█████▎    | 5639/10740 [28:02:48<24:34:41, 17.35s/it]

 53%|█████▎    | 5640/10740 [28:03:05<24:40:12, 17.41s/it]

 53%|█████▎    | 5641/10740 [28:03:23<24:47:39, 17.51s/it]
{'loss': 0.3993, 'learning_rate': 9.669826366772515e-07, 'rewards/chosen': -1.5182414054870605, 'rewards/rejected': -3.298706531524658, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7804651260375977, 'policy_logps/rejected': -382.9906311035156, 'policy_logps/chosen': -377.6590576171875, 'referece_logps/rejected': -350.0035705566406, 'referece_logps/chosen': -362.47662353515625, 'logits/rejected': -0.24487031996250153, 'logits/chosen': -0.1847154200077057, 'epoch': 3.15}


 53%|█████▎    | 5643/10740 [28:03:55<24:04:41, 17.01s/it]

 53%|█████▎    | 5644/10740 [28:04:11<23:37:10, 16.69s/it]

 53%|█████▎    | 5645/10740 [28:04:31<25:14:30, 17.84s/it]

 53%|█████▎    | 5646/10740 [28:04:46<23:48:36, 16.83s/it]

 53%|█████▎    | 5647/10740 [28:04:58<21:44:50, 15.37s/it]

 53%|█████▎    | 5648/10740 [28:05:17<23:35:26, 16.68s/it]

 53%|█████▎    | 5649/10740 [28:05:30<21:49:04, 15.43s/it]

 53%|█████▎    | 5650/10740 [28:05:46<22:15:05, 15.74s/it]

 53%|█████▎    | 5651/10740 [28:05:58<20:19:49, 14.38s/it]

 53%|█████▎    | 5652/10740 [28:06:17<22:33:53, 15.97s/it]

 53%|█████▎    | 5653/10740 [28:06:36<23:45:44, 16.82s/it]

 53%|█████▎    | 5654/10740 [28:06:57<25:28:08, 18.03s/it]

 53%|█████▎    | 5655/10740 [28:07:13<24:47:40, 17.55s/it]

 53%|█████▎    | 5656/10740 [28:07:33<25:38:27, 18.16s/it]

 53%|█████▎    | 5657/10740 [28:07:51<25:22:36, 17.97s/it]

 53%|█████▎    | 5658/10740 [28:08:13<27:04:44, 19.18s/it]

 53%|█████▎    | 5659/10740 [28:08:33<27:28:22, 19.47s/it]
{'loss': 0.2859, 'learning_rate': 9.615576111529033e-07, 'rewards/chosen': -1.231879472732544, 'rewards/rejected': -3.054717779159546, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8228381872177124, 'policy_logps/rejected': -322.6719055175781, 'policy_logps/chosen': -292.02679443359375, 'referece_logps/rejected': -292.124755859375, 'referece_logps/chosen': -279.7080078125, 'logits/rejected': -0.1867963969707489, 'logits/chosen': -0.23655827343463898, 'epoch': 3.16}


 53%|█████▎    | 5661/10740 [28:09:08<26:18:44, 18.65s/it]
{'loss': 0.4219, 'learning_rate': 9.60954897543852e-07, 'rewards/chosen': -1.1265367269515991, 'rewards/rejected': -2.6538076400756836, 'rewards/accuracies': 0.875, 'rewards/margins': 1.527270793914795, 'policy_logps/rejected': -233.74417114257812, 'policy_logps/chosen': -215.50576782226562, 'referece_logps/rejected': -207.2061004638672, 'referece_logps/chosen': -204.2404022216797, 'logits/rejected': -0.6343799829483032, 'logits/chosen': -0.7008905410766602, 'epoch': 3.16}


 53%|█████▎    | 5663/10740 [28:09:41<24:07:52, 17.11s/it]
{'loss': 0.3961, 'learning_rate': 9.603521981397917e-07, 'rewards/chosen': -1.7284362316131592, 'rewards/rejected': -3.108644723892212, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3802082538604736, 'policy_logps/rejected': -604.7996826171875, 'policy_logps/chosen': -531.9011840820312, 'referece_logps/rejected': -573.7132568359375, 'referece_logps/chosen': -514.6168212890625, 'logits/rejected': -0.7840552926063538, 'logits/chosen': -0.7788227796554565, 'epoch': 3.16}

 53%|█████▎    | 5664/10740 [28:09:56<23:14:13, 16.48s/it]


 53%|█████▎    | 5666/10740 [28:10:28<22:52:00, 16.22s/it]
{'loss': 0.526, 'learning_rate': 9.594481761477068e-07, 'rewards/chosen': -1.4050098657608032, 'rewards/rejected': -2.081435441970825, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6764253377914429, 'policy_logps/rejected': -341.04278564453125, 'policy_logps/chosen': -344.3223876953125, 'referece_logps/rejected': -320.2284240722656, 'referece_logps/chosen': -330.27227783203125, 'logits/rejected': -0.5470582246780396, 'logits/chosen': -0.494304358959198, 'epoch': 3.17}

 53%|█████▎    | 5667/10740 [28:10:49<24:59:10, 17.73s/it]

 53%|█████▎    | 5668/10740 [28:11:06<24:29:20, 17.38s/it]

 53%|█████▎    | 5669/10740 [28:11:22<23:56:47, 17.00s/it]


 53%|█████▎    | 5671/10740 [28:11:48<21:25:27, 15.22s/it]

 53%|█████▎    | 5672/10740 [28:12:11<24:34:20, 17.45s/it]
{'loss': 0.4188, 'learning_rate': 9.576402324872817e-07, 'rewards/chosen': -1.262128472328186, 'rewards/rejected': -3.268500328063965, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0063722133636475, 'policy_logps/rejected': -254.87692260742188, 'policy_logps/chosen': -287.7492370605469, 'referece_logps/rejected': -222.19190979003906, 'referece_logps/chosen': -275.1279602050781, 'logits/rejected': -0.5742635726928711, 'logits/chosen': -0.65565025806427, 'epoch': 3.17}


 53%|█████▎    | 5674/10740 [28:12:52<26:33:46, 18.88s/it]

 53%|█████▎    | 5675/10740 [28:13:11<26:40:06, 18.95s/it]

 53%|█████▎    | 5676/10740 [28:13:31<26:56:18, 19.15s/it]

 53%|█████▎    | 5677/10740 [28:13:51<27:10:32, 19.32s/it]
{'loss': 0.4004, 'learning_rate': 9.561337184689538e-07, 'rewards/chosen': -1.2341325283050537, 'rewards/rejected': -2.589428424835205, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3552956581115723, 'policy_logps/rejected': -391.2482604980469, 'policy_logps/chosen': -371.02508544921875, 'referece_logps/rejected': -365.35400390625, 'referece_logps/chosen': -358.68377685546875, 'logits/rejected': -1.8106415271759033, 'logits/chosen': -1.7837870121002197, 'epoch': 3.17}

 53%|█████▎    | 5678/10740 [28:14:08<26:16:53, 18.69s/it]


 53%|█████▎    | 5680/10740 [28:14:39<23:55:51, 17.03s/it]
{'loss': 0.3841, 'learning_rate': 9.552298577156746e-07, 'rewards/chosen': -1.474463939666748, 'rewards/rejected': -3.0861763954162598, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6117124557495117, 'policy_logps/rejected': -390.22119140625, 'policy_logps/chosen': -466.2204284667969, 'referece_logps/rejected': -359.35943603515625, 'referece_logps/chosen': -451.47576904296875, 'logits/rejected': -0.0012069977819919586, 'logits/chosen': -0.1584082692861557, 'epoch': 3.17}

 53%|█████▎    | 5681/10740 [28:14:58<24:42:02, 17.58s/it]

 53%|█████▎    | 5682/10740 [28:15:18<25:35:07, 18.21s/it]


 53%|█████▎    | 5684/10740 [28:15:53<25:32:01, 18.18s/it]
{'loss': 0.389, 'learning_rate': 9.540247671798664e-07, 'rewards/chosen': -2.0074872970581055, 'rewards/rejected': -4.144189357757568, 'rewards/accuracies': 0.75, 'rewards/margins': 2.136702299118042, 'policy_logps/rejected': -314.05474853515625, 'policy_logps/chosen': -251.64169311523438, 'referece_logps/rejected': -272.61285400390625, 'referece_logps/chosen': -231.56680297851562, 'logits/rejected': -0.7889339923858643, 'logits/chosen': -0.915529727935791, 'epoch': 3.18}

 53%|█████▎    | 5685/10740 [28:16:10<25:09:01, 17.91s/it]


 53%|█████▎    | 5687/10740 [28:16:53<27:41:19, 19.73s/it]

 53%|█████▎    | 5688/10740 [28:17:05<24:34:27, 17.51s/it]
{'loss': 0.4153, 'learning_rate': 9.528197435490137e-07, 'rewards/chosen': -1.6190675497055054, 'rewards/rejected': -3.4903106689453125, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8712430000305176, 'policy_logps/rejected': -408.05047607421875, 'policy_logps/chosen': -422.3997802734375, 'referece_logps/rejected': -373.1473693847656, 'referece_logps/chosen': -406.2091064453125, 'logits/rejected': 0.2137078493833542, 'logits/chosen': 0.25174787640571594, 'epoch': 3.18}


 53%|█████▎    | 5690/10740 [28:17:39<23:55:47, 17.06s/it]

 53%|█████▎    | 5691/10740 [28:17:57<24:02:34, 17.14s/it]

 53%|█████▎    | 5692/10740 [28:18:17<25:30:18, 18.19s/it]

 53%|█████▎    | 5693/10740 [28:18:37<26:11:09, 18.68s/it]

 53%|█████▎    | 5694/10740 [28:18:54<25:20:07, 18.08s/it]

 53%|█████▎    | 5695/10740 [28:19:17<27:23:35, 19.55s/it]
{'loss': 0.3658, 'learning_rate': 9.50711118459506e-07, 'rewards/chosen': -1.5908544063568115, 'rewards/rejected': -2.659773111343384, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0689189434051514, 'policy_logps/rejected': -280.6269226074219, 'policy_logps/chosen': -235.85775756835938, 'referece_logps/rejected': -254.0291748046875, 'referece_logps/chosen': -219.94920349121094, 'logits/rejected': -0.368049293756485, 'logits/chosen': -0.31870371103286743, 'epoch': 3.18}


 53%|█████▎    | 5697/10740 [28:19:51<25:47:01, 18.41s/it]
{'loss': 0.3525, 'learning_rate': 9.501086940837622e-07, 'rewards/chosen': -1.7951600551605225, 'rewards/rejected': -3.0181076526641846, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2229474782943726, 'policy_logps/rejected': -380.87591552734375, 'policy_logps/chosen': -408.2886047363281, 'referece_logps/rejected': -350.6948547363281, 'referece_logps/chosen': -390.3370056152344, 'logits/rejected': -1.549206256866455, 'logits/chosen': -1.589523434638977, 'epoch': 3.18}

 53%|█████▎    | 5698/10740 [28:20:03<22:43:41, 16.23s/it]


 53%|█████▎    | 5700/10740 [28:20:39<24:08:15, 17.24s/it]

 53%|█████▎    | 5701/10740 [28:20:52<22:08:59, 15.82s/it]
{'loss': 0.5083, 'learning_rate': 9.489039000042749e-07, 'rewards/chosen': -2.755643844604492, 'rewards/rejected': -3.102335214614868, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3466911315917969, 'policy_logps/rejected': -297.239990234375, 'policy_logps/chosen': -340.93212890625, 'referece_logps/rejected': -266.21661376953125, 'referece_logps/chosen': -313.37567138671875, 'logits/rejected': -1.298799753189087, 'logits/chosen': -1.3012652397155762, 'epoch': 3.18}


 53%|█████▎    | 5703/10740 [28:21:20<20:28:05, 14.63s/it]

 53%|█████▎    | 5704/10740 [28:21:39<22:33:52, 16.13s/it]

 53%|█████▎    | 5705/10740 [28:22:00<24:24:06, 17.45s/it]

 53%|█████▎    | 5706/10740 [28:22:20<25:24:55, 18.18s/it]
{'loss': 0.4254, 'learning_rate': 9.473980121749551e-07, 'rewards/chosen': -2.2959766387939453, 'rewards/rejected': -3.2977066040039062, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0017297267913818, 'policy_logps/rejected': -376.3297119140625, 'policy_logps/chosen': -351.0309753417969, 'referece_logps/rejected': -343.3526611328125, 'referece_logps/chosen': -328.07122802734375, 'logits/rejected': -0.2025439441204071, 'logits/chosen': -0.2670924663543701, 'epoch': 3.19}


 53%|█████▎    | 5708/10740 [28:22:56<25:13:15, 18.04s/it]
{'loss': 0.3521, 'learning_rate': 9.467956903414387e-07, 'rewards/chosen': -2.1506450176239014, 'rewards/rejected': -2.973558187484741, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8229133486747742, 'policy_logps/rejected': -259.30487060546875, 'policy_logps/chosen': -333.5130920410156, 'referece_logps/rejected': -229.56932067871094, 'referece_logps/chosen': -312.0066223144531, 'logits/rejected': -0.7728277444839478, 'logits/chosen': -1.003934621810913, 'epoch': 3.19}


 53%|█████▎    | 5710/10740 [28:23:36<26:37:47, 19.06s/it]
{'loss': 0.3741, 'learning_rate': 9.461933878641717e-07, 'rewards/chosen': -1.9730650186538696, 'rewards/rejected': -2.7817211151123047, 'rewards/accuracies': 0.875, 'rewards/margins': 0.808655858039856, 'policy_logps/rejected': -486.0715637207031, 'policy_logps/chosen': -475.0880432128906, 'referece_logps/rejected': -458.2543640136719, 'referece_logps/chosen': -455.3573913574219, 'logits/rejected': -0.1691443771123886, 'logits/chosen': -0.2295481264591217, 'epoch': 3.19}


 53%|█████▎    | 5712/10740 [28:24:16<27:24:35, 19.63s/it]

 53%|█████▎    | 5713/10740 [28:24:36<27:35:09, 19.76s/it]
{'loss': 0.3858, 'learning_rate': 9.452899709205698e-07, 'rewards/chosen': -1.1704603433609009, 'rewards/rejected': -2.0759177207946777, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9054574966430664, 'policy_logps/rejected': -383.38720703125, 'policy_logps/chosen': -394.61846923828125, 'referece_logps/rejected': -362.6280517578125, 'referece_logps/chosen': -382.9139099121094, 'logits/rejected': -1.9099066257476807, 'logits/chosen': -1.7506844997406006, 'epoch': 3.19}


 53%|█████▎    | 5715/10740 [28:25:03<23:00:53, 16.49s/it]
{'loss': 0.4072, 'learning_rate': 9.446877177925762e-07, 'rewards/chosen': -1.6543112993240356, 'rewards/rejected': -3.40539813041687, 'rewards/accuracies': 1.0, 'rewards/margins': 1.751086950302124, 'policy_logps/rejected': -406.9205017089844, 'policy_logps/chosen': -334.55462646484375, 'referece_logps/rejected': -372.86651611328125, 'referece_logps/chosen': -318.0115051269531, 'logits/rejected': -0.2250240594148636, 'logits/chosen': -0.18433955311775208, 'epoch': 3.19}

 53%|█████▎    | 5716/10740 [28:25:14<20:39:00, 14.80s/it]


 53%|█████▎    | 5718/10740 [28:25:44<20:14:32, 14.51s/it]

 53%|█████▎    | 5719/10740 [28:25:58<19:57:40, 14.31s/it]

 53%|█████▎    | 5720/10740 [28:26:14<20:38:34, 14.80s/it]

 53%|█████▎    | 5721/10740 [28:26:34<22:47:23, 16.35s/it]
{'loss': 0.3128, 'learning_rate': 9.428810800238835e-07, 'rewards/chosen': -1.862147569656372, 'rewards/rejected': -3.1436047554016113, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2814571857452393, 'policy_logps/rejected': -399.5820617675781, 'policy_logps/chosen': -346.0818176269531, 'referece_logps/rejected': -368.14599609375, 'referece_logps/chosen': -327.4603576660156, 'logits/rejected': -0.1743902713060379, 'logits/chosen': -0.16002482175827026, 'epoch': 3.2}


 53%|█████▎    | 5723/10740 [28:27:15<25:48:01, 18.51s/it]
{'loss': 0.3175, 'learning_rate': 9.42278908703051e-07, 'rewards/chosen': -1.1374645233154297, 'rewards/rejected': -3.005851984024048, 'rewards/accuracies': 0.875, 'rewards/margins': 1.868387222290039, 'policy_logps/rejected': -469.92156982421875, 'policy_logps/chosen': -483.39080810546875, 'referece_logps/rejected': -439.863037109375, 'referece_logps/chosen': -472.0161437988281, 'logits/rejected': 0.32529717683792114, 'logits/chosen': 0.2425554096698761, 'epoch': 3.2}


 53%|█████▎    | 5725/10740 [28:27:50<24:30:30, 17.59s/it]

 53%|█████▎    | 5726/10740 [28:28:06<23:44:56, 17.05s/it]
{'loss': 0.4508, 'learning_rate': 9.413756911643222e-07, 'rewards/chosen': -2.065953493118286, 'rewards/rejected': -2.3716237545013428, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3056701719760895, 'policy_logps/rejected': -407.60321044921875, 'policy_logps/chosen': -462.4071044921875, 'referece_logps/rejected': -383.8869323730469, 'referece_logps/chosen': -441.7475891113281, 'logits/rejected': -1.1675629615783691, 'logits/chosen': -1.1146395206451416, 'epoch': 3.2}


 53%|█████▎    | 5728/10740 [28:28:41<24:25:45, 17.55s/it]
{'loss': 0.3577, 'learning_rate': 9.407735727529875e-07, 'rewards/chosen': -2.2396140098571777, 'rewards/rejected': -3.8845295906066895, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6449155807495117, 'policy_logps/rejected': -294.65673828125, 'policy_logps/chosen': -289.16748046875, 'referece_logps/rejected': -255.8114776611328, 'referece_logps/chosen': -266.7713623046875, 'logits/rejected': -0.8172706961631775, 'logits/chosen': -0.897692084312439, 'epoch': 3.2}


 53%|█████▎    | 5730/10740 [28:29:25<27:50:13, 20.00s/it]
{'loss': 0.3422, 'learning_rate': 9.401714758888075e-07, 'rewards/chosen': -1.7019284963607788, 'rewards/rejected': -2.984711170196533, 'rewards/accuracies': 0.875, 'rewards/margins': 1.282782793045044, 'policy_logps/rejected': -481.6507873535156, 'policy_logps/chosen': -426.30731201171875, 'referece_logps/rejected': -451.80364990234375, 'referece_logps/chosen': -409.2879943847656, 'logits/rejected': -0.8490362167358398, 'logits/chosen': -0.830061674118042, 'epoch': 3.2}

 53%|█████▎    | 5731/10740 [28:29:45<27:31:28, 19.78s/it]

 53%|█████▎    | 5732/10740 [28:30:03<27:03:28, 19.45s/it]

 53%|█████▎    | 5733/10740 [28:30:23<27:13:36, 19.58s/it]

 53%|█████▎    | 5734/10740 [28:30:45<27:58:39, 20.12s/it]

 53%|█████▎    | 5735/10740 [28:30:57<24:35:54, 17.69s/it]


 53%|█████▎    | 5737/10740 [28:31:28<22:50:04, 16.43s/it]
{'loss': 0.3513, 'learning_rate': 9.380643097104654e-07, 'rewards/chosen': -1.9132356643676758, 'rewards/rejected': -3.5858724117279053, 'rewards/accuracies': 0.75, 'rewards/margins': 1.67263662815094, 'policy_logps/rejected': -395.865478515625, 'policy_logps/chosen': -338.437744140625, 'referece_logps/rejected': -360.0067138671875, 'referece_logps/chosen': -319.305419921875, 'logits/rejected': -0.3786661922931671, 'logits/chosen': -0.2818758487701416, 'epoch': 3.21}

 53%|█████▎    | 5738/10740 [28:31:49<24:36:44, 17.71s/it]


 53%|█████▎    | 5740/10740 [28:32:22<24:13:29, 17.44s/it]
{'loss': 0.3825, 'learning_rate': 9.371613224416552e-07, 'rewards/chosen': -1.1379823684692383, 'rewards/rejected': -3.4723517894744873, 'rewards/accuracies': 0.875, 'rewards/margins': 2.334369421005249, 'policy_logps/rejected': -462.095947265625, 'policy_logps/chosen': -426.06378173828125, 'referece_logps/rejected': -427.37249755859375, 'referece_logps/chosen': -414.6839294433594, 'logits/rejected': -0.5075823664665222, 'logits/chosen': -0.48690783977508545, 'epoch': 3.21}

 53%|█████▎    | 5741/10740 [28:32:43<25:46:35, 18.56s/it]

 53%|█████▎    | 5742/10740 [28:33:01<25:28:28, 18.35s/it]

 53%|█████▎    | 5743/10740 [28:33:14<23:22:28, 16.84s/it]

 53%|█████▎    | 5744/10740 [28:33:37<25:37:25, 18.46s/it]


 54%|█████▎    | 5746/10740 [28:34:10<24:04:53, 17.36s/it]

 54%|█████▎    | 5747/10740 [28:34:24<22:54:55, 16.52s/it]
{'loss': 0.3895, 'learning_rate': 9.350545534618489e-07, 'rewards/chosen': -2.6943860054016113, 'rewards/rejected': -4.479398727416992, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7850124835968018, 'policy_logps/rejected': -455.01416015625, 'policy_logps/chosen': -386.93902587890625, 'referece_logps/rejected': -410.2201232910156, 'referece_logps/chosen': -359.9951171875, 'logits/rejected': 0.15618184208869934, 'logits/chosen': 0.1825857013463974, 'epoch': 3.21}


 54%|█████▎    | 5749/10740 [28:35:02<24:47:23, 17.88s/it]
{'loss': 0.4036, 'learning_rate': 9.344526722195547e-07, 'rewards/chosen': -1.4373723268508911, 'rewards/rejected': -2.6175498962402344, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1801775693893433, 'policy_logps/rejected': -368.75689697265625, 'policy_logps/chosen': -282.65020751953125, 'referece_logps/rejected': -342.58135986328125, 'referece_logps/chosen': -268.2764587402344, 'logits/rejected': -1.3685686588287354, 'logits/chosen': -1.3177406787872314, 'epoch': 3.21}

 54%|█████▎    | 5750/10740 [28:35:21<25:27:52, 18.37s/it]

 54%|█████▎    | 5751/10740 [28:35:37<24:15:05, 17.50s/it]

 54%|█████▎    | 5752/10740 [28:35:57<25:29:56, 18.40s/it]


 54%|█████▎    | 5754/10740 [28:36:34<25:18:36, 18.27s/it]

 54%|█████▎    | 5755/10740 [28:36:48<23:38:32, 17.07s/it]
{'loss': 0.379, 'learning_rate': 9.326471724490717e-07, 'rewards/chosen': -2.0972466468811035, 'rewards/rejected': -3.166538953781128, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0692920684814453, 'policy_logps/rejected': -378.332763671875, 'policy_logps/chosen': -374.53338623046875, 'referece_logps/rejected': -346.6673583984375, 'referece_logps/chosen': -353.5609130859375, 'logits/rejected': -0.9442480802536011, 'logits/chosen': -0.808554470539093, 'epoch': 3.22}

 54%|█████▎    | 5756/10740 [28:37:03<22:31:41, 16.27s/it]

 54%|█████▎    | 5757/10740 [28:37:17<21:55:11, 15.84s/it]

 54%|█████▎    | 5758/10740 [28:37:39<24:26:57, 17.67s/it]


 54%|█████▎    | 5760/10740 [28:38:16<25:00:48, 18.08s/it]
{'loss': 0.5198, 'learning_rate': 9.311427575184787e-07, 'rewards/chosen': -2.0113210678100586, 'rewards/rejected': -2.5288164615631104, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5174956321716309, 'policy_logps/rejected': -289.6874084472656, 'policy_logps/chosen': -260.7434997558594, 'referece_logps/rejected': -264.39923095703125, 'referece_logps/chosen': -240.6302947998047, 'logits/rejected': -0.3029124438762665, 'logits/chosen': -0.33475399017333984, 'epoch': 3.22}

 54%|█████▎    | 5761/10740 [28:38:37<26:03:16, 18.84s/it]

 54%|█████▎    | 5762/10740 [28:38:49<23:28:39, 16.98s/it]


 54%|█████▎    | 5764/10740 [28:39:28<24:45:52, 17.92s/it]
{'loss': 0.4181, 'learning_rate': 9.299393381390312e-07, 'rewards/chosen': -2.411280870437622, 'rewards/rejected': -3.5640218257904053, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1527410745620728, 'policy_logps/rejected': -404.9063720703125, 'policy_logps/chosen': -329.4765625, 'referece_logps/rejected': -369.2660827636719, 'referece_logps/chosen': -305.36376953125, 'logits/rejected': -0.2559143900871277, 'logits/chosen': -0.2262302041053772, 'epoch': 3.22}

 54%|█████▎    | 5765/10740 [28:39:44<24:01:05, 17.38s/it]

 54%|█████▎    | 5766/10740 [28:40:01<23:47:41, 17.22s/it]

 54%|█████▎    | 5767/10740 [28:40:13<21:43:48, 15.73s/it]

 54%|█████▎    | 5768/10740 [28:40:36<24:33:37, 17.78s/it]

 54%|█████▎    | 5769/10740 [28:40:58<26:11:33, 18.97s/it]

 54%|█████▎    | 5770/10740 [28:41:18<26:44:41, 19.37s/it]

 54%|█████▎    | 5771/10740 [28:41:40<28:03:59, 20.33s/it]

 54%|█████▎    | 5772/10740 [28:41:59<27:29:28, 19.92s/it]

 54%|█████▍    | 5773/10740 [28:42:18<27:01:32, 19.59s/it]

 54%|█████▍    | 5774/10740 [28:42:35<25:41:21, 18.62s/it]


 54%|█████▍    | 5776/10740 [28:43:13<25:56:19, 18.81s/it]
{'loss': 0.3485, 'learning_rate': 9.263296987350709e-07, 'rewards/chosen': -1.3384793996810913, 'rewards/rejected': -3.2809576988220215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.942477822303772, 'policy_logps/rejected': -372.3800048828125, 'policy_logps/chosen': -327.51727294921875, 'referece_logps/rejected': -339.5704040527344, 'referece_logps/chosen': -314.1324768066406, 'logits/rejected': 0.0448552668094635, 'logits/chosen': 0.027440190315246582, 'epoch': 3.23}

 54%|█████▍    | 5777/10740 [28:43:34<26:59:32, 19.58s/it]

 54%|█████▍    | 5778/10740 [28:43:53<26:40:59, 19.36s/it]

 54%|█████▍    | 5779/10740 [28:44:12<26:41:42, 19.37s/it]

 54%|█████▍    | 5780/10740 [28:44:33<27:21:41, 19.86s/it]


 54%|█████▍    | 5782/10740 [28:45:11<26:40:38, 19.37s/it]
{'loss': 0.3851, 'learning_rate': 9.245252379051454e-07, 'rewards/chosen': -2.4538042545318604, 'rewards/rejected': -3.723482608795166, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2696783542633057, 'policy_logps/rejected': -393.8563537597656, 'policy_logps/chosen': -348.3739318847656, 'referece_logps/rejected': -356.62152099609375, 'referece_logps/chosen': -323.83587646484375, 'logits/rejected': -0.5716707110404968, 'logits/chosen': -0.6741121411323547, 'epoch': 3.23}

 54%|█████▍    | 5783/10740 [28:45:27<25:30:56, 18.53s/it]


 54%|█████▍    | 5785/10740 [28:46:09<27:03:07, 19.65s/it]
{'loss': 0.3242, 'learning_rate': 9.236230997932173e-07, 'rewards/chosen': -1.169327974319458, 'rewards/rejected': -2.3310446739196777, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1617168188095093, 'policy_logps/rejected': -341.1971130371094, 'policy_logps/chosen': -324.96051025390625, 'referece_logps/rejected': -317.88665771484375, 'referece_logps/chosen': -313.26727294921875, 'logits/rejected': -1.002529263496399, 'logits/chosen': -1.084688425064087, 'epoch': 3.23}

 54%|█████▍    | 5786/10740 [28:46:30<27:33:19, 20.02s/it]

 54%|█████▍    | 5787/10740 [28:46:46<25:53:23, 18.82s/it]

 54%|█████▍    | 5788/10740 [28:47:00<23:51:57, 17.35s/it]

 54%|█████▍    | 5789/10740 [28:47:15<23:03:48, 16.77s/it]

 54%|█████▍    | 5790/10740 [28:47:32<23:10:19, 16.85s/it]

 54%|█████▍    | 5791/10740 [28:47:48<22:31:26, 16.38s/it]

 54%|█████▍    | 5792/10740 [28:48:04<22:32:07, 16.40s/it]

 54%|█████▍    | 5793/10740 [28:48:23<23:35:53, 17.17s/it]

 54%|█████▍    | 5794/10740 [28:48:44<25:23:03, 18.48s/it]

 54%|█████▍    | 5795/10740 [28:49:02<25:06:00, 18.27s/it]

 54%|█████▍    | 5796/10740 [28:49:16<23:14:55, 16.93s/it]

 54%|█████▍    | 5797/10740 [28:49:37<24:43:41, 18.01s/it]

 54%|█████▍    | 5798/10740 [28:49:52<23:43:36, 17.28s/it]

 54%|█████▍    | 5799/10740 [28:50:12<24:34:37, 17.91s/it]

 54%|█████▍    | 5800/10740 [28:50:32<25:25:41, 18.53s/it]

 54%|█████▍    | 5801/10740 [28:50:54<27:08:25, 19.78s/it]

 54%|█████▍    | 5802/10740 [28:51:14<27:06:47, 19.77s/it]

 54%|█████▍    | 5803/10740 [28:51:37<28:19:18, 20.65s/it]

 54%|█████▍    | 5804/10740 [28:51:55<27:14:18, 19.87s/it]

 54%|█████▍    | 5805/10740 [28:52:11<25:35:31, 18.67s/it]

 54%|█████▍    | 5806/10740 [28:52:30<25:51:21, 18.87s/it]


 54%|█████▍    | 5808/10740 [28:53:07<25:40:00, 18.73s/it]
{'loss': 0.276, 'learning_rate': 9.167088391644159e-07, 'rewards/chosen': -1.117107629776001, 'rewards/rejected': -2.6674461364746094, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5503382682800293, 'policy_logps/rejected': -535.9375, 'policy_logps/chosen': -454.38800048828125, 'referece_logps/rejected': -509.2630310058594, 'referece_logps/chosen': -443.2169189453125, 'logits/rejected': 0.02577391266822815, 'logits/chosen': 0.09800437092781067, 'epoch': 3.24}

 54%|█████▍    | 5809/10740 [28:53:23<24:24:54, 17.82s/it]

 54%|█████▍    | 5810/10740 [28:53:38<23:03:24, 16.84s/it]

 54%|█████▍    | 5811/10740 [28:53:58<24:21:37, 17.79s/it]


 54%|█████▍    | 5813/10740 [28:54:30<23:40:19, 17.30s/it]
{'loss': 0.5053, 'learning_rate': 9.152062578330042e-07, 'rewards/chosen': -1.912197470664978, 'rewards/rejected': -3.5300703048706055, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6178730726242065, 'policy_logps/rejected': -538.5473022460938, 'policy_logps/chosen': -376.84991455078125, 'referece_logps/rejected': -503.2466125488281, 'referece_logps/chosen': -357.7279357910156, 'logits/rejected': -1.0459872484207153, 'logits/chosen': -1.0429428815841675, 'epoch': 3.25}

 54%|█████▍    | 5814/10740 [28:54:42<21:35:16, 15.78s/it]


 54%|█████▍    | 5816/10740 [28:55:05<18:55:32, 13.84s/it]
{'loss': 0.5324, 'learning_rate': 9.143048013618827e-07, 'rewards/chosen': -1.70432710647583, 'rewards/rejected': -2.527233362197876, 'rewards/accuracies': 0.75, 'rewards/margins': 0.822905957698822, 'policy_logps/rejected': -557.4104614257812, 'policy_logps/chosen': -457.0990295410156, 'referece_logps/rejected': -532.1381225585938, 'referece_logps/chosen': -440.0557861328125, 'logits/rejected': -1.3503838777542114, 'logits/chosen': -1.351969838142395, 'epoch': 3.25}

 54%|█████▍    | 5817/10740 [28:55:16<17:38:47, 12.90s/it]

 54%|█████▍    | 5818/10740 [28:55:38<21:13:27, 15.52s/it]

 54%|█████▍    | 5819/10740 [28:55:57<22:38:05, 16.56s/it]

 54%|█████▍    | 5820/10740 [28:56:16<23:54:47, 17.50s/it]

 54%|█████▍    | 5821/10740 [28:56:37<25:20:12, 18.54s/it]

 54%|█████▍    | 5822/10740 [28:56:57<25:46:59, 18.87s/it]

 54%|█████▍    | 5823/10740 [28:57:16<25:48:54, 18.90s/it]

 54%|█████▍    | 5824/10740 [28:57:32<24:44:55, 18.12s/it]

 54%|█████▍    | 5825/10740 [28:57:50<24:38:11, 18.05s/it]

 54%|█████▍    | 5826/10740 [28:58:02<22:03:20, 16.16s/it]

 54%|█████▍    | 5827/10740 [28:58:21<23:08:42, 16.96s/it]

 54%|█████▍    | 5828/10740 [28:58:34<21:47:24, 15.97s/it]


 54%|█████▍    | 5830/10740 [28:59:08<22:19:52, 16.37s/it]
{'loss': 0.3754, 'learning_rate': 9.10098943928408e-07, 'rewards/chosen': -1.1384423971176147, 'rewards/rejected': -2.5991809368133545, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4607384204864502, 'policy_logps/rejected': -322.0135803222656, 'policy_logps/chosen': -241.99850463867188, 'referece_logps/rejected': -296.0217590332031, 'referece_logps/chosen': -230.6140899658203, 'logits/rejected': -1.2649282217025757, 'logits/chosen': -1.2657917737960815, 'epoch': 3.26}

 54%|█████▍    | 5831/10740 [28:59:23<21:47:07, 15.98s/it]

 54%|█████▍    | 5832/10740 [28:59:43<23:17:52, 17.09s/it]

 54%|█████▍    | 5833/10740 [29:00:01<23:39:18, 17.35s/it]

 54%|█████▍    | 5834/10740 [29:00:22<25:30:32, 18.72s/it]

 54%|█████▍    | 5835/10740 [29:00:37<23:57:44, 17.59s/it]

 54%|█████▍    | 5836/10740 [29:01:00<25:50:32, 18.97s/it]

 54%|█████▍    | 5837/10740 [29:01:18<25:38:41, 18.83s/it]

 54%|█████▍    | 5838/10740 [29:01:39<26:38:26, 19.56s/it]

 54%|█████▍    | 5839/10740 [29:01:59<26:33:16, 19.51s/it]

 54%|█████▍    | 5840/10740 [29:02:18<26:24:08, 19.40s/it]

 54%|█████▍    | 5841/10740 [29:02:37<26:26:00, 19.42s/it]

 54%|█████▍    | 5842/10740 [29:02:58<26:53:41, 19.77s/it]

 54%|█████▍    | 5843/10740 [29:03:18<26:49:31, 19.72s/it]

 54%|█████▍    | 5844/10740 [29:03:38<27:00:46, 19.86s/it]

 54%|█████▍    | 5845/10740 [29:03:57<26:46:37, 19.69s/it]

 54%|█████▍    | 5846/10740 [29:04:17<26:44:46, 19.67s/it]

 54%|█████▍    | 5847/10740 [29:04:36<26:47:40, 19.71s/it]

 54%|█████▍    | 5848/10740 [29:04:54<26:04:24, 19.19s/it]

 54%|█████▍    | 5849/10740 [29:05:11<25:08:58, 18.51s/it]

 54%|█████▍    | 5850/10740 [29:05:33<26:32:05, 19.53s/it]

 54%|█████▍    | 5851/10740 [29:05:53<26:33:34, 19.56s/it]


 54%|█████▍    | 5853/10740 [29:06:28<25:20:16, 18.67s/it]
{'loss': 0.4208, 'learning_rate': 9.031928350585537e-07, 'rewards/chosen': -1.2825692892074585, 'rewards/rejected': -3.0729026794433594, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7903335094451904, 'policy_logps/rejected': -365.69915771484375, 'policy_logps/chosen': -384.4159851074219, 'referece_logps/rejected': -334.9701232910156, 'referece_logps/chosen': -371.5903015136719, 'logits/rejected': -0.587543249130249, 'logits/chosen': -0.49430200457572937, 'epoch': 3.27}

 55%|█████▍    | 5854/10740 [29:06:50<26:38:54, 19.63s/it]

 55%|█████▍    | 5855/10740 [29:07:06<25:09:16, 18.54s/it]

 55%|█████▍    | 5856/10740 [29:07:23<24:24:05, 17.99s/it]

 55%|█████▍    | 5857/10740 [29:07:40<24:04:56, 17.75s/it]

 55%|█████▍    | 5858/10740 [29:08:01<25:32:43, 18.84s/it]

 55%|█████▍    | 5859/10740 [29:08:14<22:51:21, 16.86s/it]


 55%|█████▍    | 5861/10740 [29:08:50<24:02:58, 17.75s/it]
{'loss': 0.4373, 'learning_rate': 9.0079178510565e-07, 'rewards/chosen': -2.1047775745391846, 'rewards/rejected': -3.3408734798431396, 'rewards/accuracies': 0.875, 'rewards/margins': 1.236095905303955, 'policy_logps/rejected': -503.4251708984375, 'policy_logps/chosen': -511.4072570800781, 'referece_logps/rejected': -470.0164489746094, 'referece_logps/chosen': -490.3594665527344, 'logits/rejected': -0.702836275100708, 'logits/chosen': -0.599378764629364, 'epoch': 3.27}

 55%|█████▍    | 5862/10740 [29:09:07<23:38:10, 17.44s/it]

 55%|█████▍    | 5863/10740 [29:09:26<24:03:15, 17.76s/it]

 55%|█████▍    | 5864/10740 [29:09:43<24:00:52, 17.73s/it]

 55%|█████▍    | 5865/10740 [29:10:03<24:44:00, 18.26s/it]

 55%|█████▍    | 5866/10740 [29:10:24<26:00:59, 19.22s/it]

 55%|█████▍    | 5867/10740 [29:10:42<25:21:37, 18.74s/it]

 55%|█████▍    | 5868/10740 [29:10:58<24:15:23, 17.92s/it]

 55%|█████▍    | 5869/10740 [29:11:18<25:00:11, 18.48s/it]

 55%|█████▍    | 5870/10740 [29:11:32<23:15:07, 17.19s/it]

 55%|█████▍    | 5871/10740 [29:11:55<25:44:15, 19.03s/it]

 55%|█████▍    | 5872/10740 [29:12:11<24:33:56, 18.17s/it]

 55%|█████▍    | 5873/10740 [29:12:29<24:19:51, 18.00s/it]

 55%|█████▍    | 5874/10740 [29:12:49<25:13:07, 18.66s/it]

 55%|█████▍    | 5875/10740 [29:13:03<23:12:45, 17.18s/it]

 55%|█████▍    | 5876/10740 [29:13:19<22:39:04, 16.76s/it]

 55%|█████▍    | 5877/10740 [29:13:34<21:57:36, 16.26s/it]

 55%|█████▍    | 5878/10740 [29:13:54<23:23:22, 17.32s/it]

 55%|█████▍    | 5879/10740 [29:14:05<21:02:03, 15.58s/it]

 55%|█████▍    | 5880/10740 [29:14:26<23:11:21, 17.18s/it]

 55%|█████▍    | 5881/10740 [29:14:46<24:12:35, 17.94s/it]

 55%|█████▍    | 5882/10740 [29:14:59<22:26:54, 16.64s/it]

 55%|█████▍    | 5883/10740 [29:15:20<24:14:00, 17.96s/it]


 55%|█████▍    | 5885/10740 [29:15:56<24:15:07, 17.98s/it]

 55%|█████▍    | 5886/10740 [29:16:08<21:39:32, 16.06s/it]

 55%|█████▍    | 5887/10740 [29:16:23<21:21:02, 15.84s/it]

 55%|█████▍    | 5888/10740 [29:16:43<22:51:11, 16.96s/it]

 55%|█████▍    | 5889/10740 [29:17:03<23:55:54, 17.76s/it]

 55%|█████▍    | 5890/10740 [29:17:14<21:31:04, 15.97s/it]

 55%|█████▍    | 5891/10740 [29:17:31<21:52:15, 16.24s/it]

 55%|█████▍    | 5892/10740 [29:17:45<20:44:27, 15.40s/it]

 55%|█████▍    | 5893/10740 [29:18:04<22:28:21, 16.69s/it]

 55%|█████▍    | 5894/10740 [29:18:18<21:13:17, 15.76s/it]

 55%|█████▍    | 5895/10740 [29:18:33<21:03:17, 15.64s/it]

 55%|█████▍    | 5896/10740 [29:18:53<22:38:42, 16.83s/it]

 55%|█████▍    | 5897/10740 [29:19:10<22:32:07, 16.75s/it]

 55%|█████▍    | 5898/10740 [29:19:32<24:42:28, 18.37s/it]

 55%|█████▍    | 5899/10740 [29:19:52<25:20:30, 18.85s/it]

 55%|█████▍    | 5900/10740 [29:20:08<24:08:52, 17.96s/it]

 55%|█████▍    | 5901/10740 [29:20:20<21:51:28, 16.26s/it]

 55%|█████▍    | 5902/10740 [29:20:40<23:15:59, 17.31s/it]

 55%|█████▍    | 5903/10740 [29:20:58<23:35:35, 17.56s/it]

 55%|█████▍    | 5904/10740 [29:21:17<24:26:20, 18.19s/it]

 55%|█████▍    | 5905/10740 [29:21:35<24:16:39, 18.08s/it]

 55%|█████▍    | 5906/10740 [29:21:51<23:24:54, 17.44s/it]

 55%|█████▌    | 5907/10740 [29:22:11<24:17:40, 18.10s/it]

 55%|█████▌    | 5908/10740 [29:22:31<24:58:14, 18.60s/it]

 55%|█████▌    | 5909/10740 [29:22:52<26:15:01, 19.56s/it]

 55%|█████▌    | 5910/10740 [29:23:07<24:27:00, 18.22s/it]

 55%|█████▌    | 5911/10740 [29:23:23<23:17:06, 17.36s/it]

 55%|█████▌    | 5912/10740 [29:23:44<24:48:28, 18.50s/it]

 55%|█████▌    | 5913/10740 [29:24:04<25:15:17, 18.84s/it]

 55%|█████▌    | 5914/10740 [29:24:18<23:38:52, 17.64s/it]

 55%|█████▌    | 5915/10740 [29:24:36<23:34:45, 17.59s/it]

 55%|█████▌    | 5916/10740 [29:24:56<24:30:44, 18.29s/it]

 55%|█████▌    | 5917/10740 [29:25:17<25:29:11, 19.02s/it]

 55%|█████▌    | 5918/10740 [29:25:36<25:44:14, 19.21s/it]

 55%|█████▌    | 5919/10740 [29:25:53<24:41:38, 18.44s/it]

 55%|█████▌    | 5920/10740 [29:26:13<25:12:00, 18.82s/it]

 55%|█████▌    | 5921/10740 [29:26:31<24:52:11, 18.58s/it]

 55%|█████▌    | 5922/10740 [29:26:47<24:02:36, 17.97s/it]

 55%|█████▌    | 5923/10740 [29:26:59<21:47:11, 16.28s/it]

 55%|█████▌    | 5924/10740 [29:27:20<23:26:54, 17.53s/it]

 55%|█████▌    | 5925/10740 [29:27:39<24:03:20, 17.99s/it]

 55%|█████▌    | 5926/10740 [29:27:53<22:31:36, 16.85s/it]

 55%|█████▌    | 5927/10740 [29:28:09<22:15:58, 16.65s/it]

 55%|█████▌    | 5928/10740 [29:28:29<23:37:07, 17.67s/it]

 55%|█████▌    | 5929/10740 [29:28:44<22:12:29, 16.62s/it]

 55%|█████▌    | 5930/10740 [29:28:59<21:43:38, 16.26s/it]

 55%|█████▌    | 5931/10740 [29:29:17<22:34:45, 16.90s/it]

 55%|█████▌    | 5932/10740 [29:29:40<24:40:29, 18.48s/it]

 55%|█████▌    | 5933/10740 [29:29:59<25:12:41, 18.88s/it]

 55%|█████▌    | 5934/10740 [29:30:14<23:38:43, 17.71s/it]

 55%|█████▌    | 5935/10740 [29:30:31<23:06:43, 17.32s/it]

 55%|█████▌    | 5936/10740 [29:30:49<23:28:51, 17.60s/it]

 55%|█████▌    | 5937/10740 [29:31:09<24:23:22, 18.28s/it]

 55%|█████▌    | 5938/10740 [29:31:30<25:30:32, 19.12s/it]

 55%|█████▌    | 5939/10740 [29:31:51<26:23:03, 19.78s/it]

 55%|█████▌    | 5940/10740 [29:32:05<24:03:10, 18.04s/it]

 55%|█████▌    | 5941/10740 [29:32:22<23:25:25, 17.57s/it]

 55%|█████▌    | 5942/10740 [29:32:38<22:48:34, 17.11s/it]

 55%|█████▌    | 5943/10740 [29:32:52<21:34:41, 16.19s/it]

 55%|█████▌    | 5944/10740 [29:33:13<23:44:46, 17.82s/it]

 55%|█████▌    | 5945/10740 [29:33:35<25:22:58, 19.06s/it]

 55%|█████▌    | 5946/10740 [29:33:55<25:39:20, 19.27s/it]

 55%|█████▌    | 5947/10740 [29:34:15<25:50:01, 19.40s/it]

 55%|█████▌    | 5948/10740 [29:34:34<25:54:13, 19.46s/it]

 55%|█████▌    | 5949/10740 [29:34:54<26:00:39, 19.54s/it]

 55%|█████▌    | 5950/10740 [29:35:11<24:53:59, 18.71s/it]

 55%|█████▌    | 5951/10740 [29:35:27<23:44:53, 17.85s/it]

 55%|█████▌    | 5952/10740 [29:35:48<25:08:43, 18.91s/it]

 55%|█████▌    | 5953/10740 [29:36:08<25:26:47, 19.14s/it]

 55%|█████▌    | 5954/10740 [29:36:29<26:17:06, 19.77s/it]

 55%|█████▌    | 5955/10740 [29:36:42<23:22:53, 17.59s/it]

 55%|█████▌    | 5956/10740 [29:37:02<24:17:51, 18.28s/it]

 55%|█████▌    | 5957/10740 [29:37:21<24:54:53, 18.75s/it]

 55%|█████▌    | 5958/10740 [29:37:37<23:51:25, 17.96s/it]

 55%|█████▌    | 5959/10740 [29:37:59<25:15:50, 19.02s/it]

 55%|█████▌    | 5960/10740 [29:38:19<25:40:00, 19.33s/it]

 56%|█████▌    | 5961/10740 [29:38:39<26:01:19, 19.60s/it]

 56%|█████▌    | 5962/10740 [29:38:59<26:00:10, 19.59s/it]

 56%|█████▌    | 5963/10740 [29:39:19<26:13:22, 19.76s/it]

 56%|█████▌    | 5964/10740 [29:39:40<26:45:38, 20.17s/it]

 56%|█████▌    | 5965/10740 [29:39:54<24:05:27, 18.16s/it]

 56%|█████▌    | 5966/10740 [29:40:13<24:37:43, 18.57s/it]

 56%|█████▌    | 5967/10740 [29:40:34<25:21:22, 19.12s/it]

 56%|█████▌    | 5968/10740 [29:40:49<23:54:16, 18.03s/it]

 56%|█████▌    | 5969/10740 [29:41:00<21:07:46, 15.94s/it]

 56%|█████▌    | 5970/10740 [29:41:12<19:20:19, 14.60s/it]

 56%|█████▌    | 5971/10740 [29:41:25<18:59:01, 14.33s/it]

 56%|█████▌    | 5972/10740 [29:41:36<17:44:50, 13.40s/it]

 56%|█████▌    | 5973/10740 [29:41:55<19:38:05, 14.83s/it]

 56%|█████▌    | 5974/10740 [29:42:14<21:35:48, 16.31s/it]

 56%|█████▌    | 5975/10740 [29:42:28<20:31:33, 15.51s/it]

 56%|█████▌    | 5976/10740 [29:42:50<23:09:13, 17.50s/it]

 56%|█████▌    | 5977/10740 [29:43:11<24:30:03, 18.52s/it]

 56%|█████▌    | 5978/10740 [29:43:29<24:26:17, 18.47s/it]

 56%|█████▌    | 5979/10740 [29:43:41<21:51:38, 16.53s/it]

 56%|█████▌    | 5980/10740 [29:43:57<21:19:19, 16.13s/it]

 56%|█████▌    | 5981/10740 [29:44:09<19:40:01, 14.88s/it]

 56%|█████▌    | 5982/10740 [29:44:30<22:20:26, 16.90s/it]

 56%|█████▌    | 5983/10740 [29:44:51<23:47:26, 18.00s/it]
{'loss': 0.3061, 'learning_rate': 8.642555443050852e-07, 'rewards/chosen': -1.3802739381790161, 'rewards/rejected': -2.6690850257873535, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2888109683990479, 'policy_logps/rejected': -459.2095947265625, 'policy_logps/chosen': -517.7947387695312, 'referece_logps/rejected': -432.5187072753906, 'referece_logps/chosen': -503.9920349121094, 'logits/rejected': -0.6666232347488403, 'logits/chosen': -0.7190632820129395, 'epoch': 3.34}


 56%|█████▌    | 5985/10740 [29:45:31<25:12:54, 19.09s/it]

 56%|█████▌    | 5986/10740 [29:45:45<22:57:08, 17.38s/it]

 56%|█████▌    | 5987/10740 [29:46:04<23:52:29, 18.08s/it]

 56%|█████▌    | 5988/10740 [29:46:24<24:30:11, 18.56s/it]

 56%|█████▌    | 5989/10740 [29:46:38<22:40:29, 17.18s/it]
{'loss': 0.326, 'learning_rate': 8.624630169899116e-07, 'rewards/chosen': -1.3632630109786987, 'rewards/rejected': -2.2783074378967285, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9150443077087402, 'policy_logps/rejected': -309.6070861816406, 'policy_logps/chosen': -270.2190246582031, 'referece_logps/rejected': -286.8240051269531, 'referece_logps/chosen': -256.5863952636719, 'logits/rejected': -0.14842835068702698, 'logits/chosen': -0.20327353477478027, 'epoch': 3.35}


 56%|█████▌    | 5991/10740 [29:47:09<21:40:58, 16.44s/it]

 56%|█████▌    | 5992/10740 [29:47:29<23:02:50, 17.47s/it]

 56%|█████▌    | 5993/10740 [29:47:48<23:52:58, 18.11s/it]
{'loss': 0.4567, 'learning_rate': 8.612682486040086e-07, 'rewards/chosen': -2.790940046310425, 'rewards/rejected': -3.1610212326049805, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3700811564922333, 'policy_logps/rejected': -454.00360107421875, 'policy_logps/chosen': -392.546875, 'referece_logps/rejected': -422.3934020996094, 'referece_logps/chosen': -364.637451171875, 'logits/rejected': -0.3772912323474884, 'logits/chosen': -0.37043753266334534, 'epoch': 3.35}


 56%|█████▌    | 5995/10740 [29:48:25<23:56:35, 18.17s/it]

 56%|█████▌    | 5996/10740 [29:48:40<22:50:25, 17.33s/it]

 56%|█████▌    | 5997/10740 [29:48:52<20:43:01, 15.72s/it]
{'loss': 0.4598, 'learning_rate': 8.600736821059711e-07, 'rewards/chosen': -0.9943357110023499, 'rewards/rejected': -2.768845796585083, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7745099067687988, 'policy_logps/rejected': -473.8417663574219, 'policy_logps/chosen': -349.1091003417969, 'referece_logps/rejected': -446.1533203125, 'referece_logps/chosen': -339.16571044921875, 'logits/rejected': -0.928106427192688, 'logits/chosen': -0.8874695301055908, 'epoch': 3.35}


 56%|█████▌    | 5999/10740 [29:49:26<21:28:09, 16.30s/it]
{'loss': 0.5457, 'learning_rate': 8.594764751081568e-07, 'rewards/chosen': -2.1849465370178223, 'rewards/rejected': -2.950737714767456, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7657914161682129, 'policy_logps/rejected': -459.3108825683594, 'policy_logps/chosen': -402.83978271484375, 'referece_logps/rejected': -429.803466796875, 'referece_logps/chosen': -380.9903564453125, 'logits/rejected': -0.8872352838516235, 'logits/chosen': -0.7025235891342163, 'epoch': 3.35}


 56%|█████▌    | 6001/10740 [29:50:07<25:02:57, 19.03s/it]
[2024-04-03 01:03:50,927] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▌    | 6002/10740 [29:50:23<23:44:14, 18.04s/it]

 56%|█████▌    | 6003/10740 [29:50:37<22:06:28, 16.80s/it]
{'loss': 0.4224, 'learning_rate': 8.582822147012871e-07, 'rewards/chosen': -1.1149790287017822, 'rewards/rejected': -2.9749040603637695, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8599251508712769, 'policy_logps/rejected': -331.5630187988281, 'policy_logps/chosen': -314.8748779296875, 'referece_logps/rejected': -301.81396484375, 'referece_logps/chosen': -303.72509765625, 'logits/rejected': -0.5762218832969666, 'logits/chosen': -0.7865573167800903, 'epoch': 3.35}


 56%|█████▌    | 6005/10740 [29:51:13<22:42:36, 17.27s/it]

 56%|█████▌    | 6006/10740 [29:51:31<23:02:34, 17.52s/it]
[2024-04-03 01:05:15,178] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▌    | 6007/10740 [29:51:52<24:22:46, 18.54s/it]

 56%|█████▌    | 6008/10740 [29:52:13<25:19:47, 19.27s/it]

 56%|█████▌    | 6009/10740 [29:52:33<25:31:43, 19.43s/it]
{'loss': 0.2794, 'learning_rate': 8.564912113213646e-07, 'rewards/chosen': -1.0447680950164795, 'rewards/rejected': -5.352479457855225, 'rewards/accuracies': 1.0, 'rewards/margins': 4.307711124420166, 'policy_logps/rejected': -357.5690002441406, 'policy_logps/chosen': -302.2576904296875, 'referece_logps/rejected': -304.0442199707031, 'referece_logps/chosen': -291.80999755859375, 'logits/rejected': -0.3498682975769043, 'logits/chosen': -0.43818920850753784, 'epoch': 3.36}


 56%|█████▌    | 6011/10740 [29:53:08<24:38:47, 18.76s/it]

 56%|█████▌    | 6012/10740 [29:53:28<25:10:53, 19.17s/it]

 56%|█████▌    | 6013/10740 [29:53:48<25:09:10, 19.16s/it]

 56%|█████▌    | 6014/10740 [29:54:09<26:10:45, 19.94s/it]

 56%|█████▌    | 6015/10740 [29:54:27<25:22:55, 19.34s/it]

 56%|█████▌    | 6016/10740 [29:54:47<25:35:55, 19.51s/it]

 56%|█████▌    | 6017/10740 [29:55:07<25:36:47, 19.52s/it]

 56%|█████▌    | 6018/10740 [29:55:23<24:19:48, 18.55s/it]

 56%|█████▌    | 6019/10740 [29:55:42<24:17:38, 18.53s/it]

 56%|█████▌    | 6020/10740 [29:56:03<25:21:51, 19.35s/it]

 56%|█████▌    | 6021/10740 [29:56:17<23:14:15, 17.73s/it]

 56%|█████▌    | 6022/10740 [29:56:37<24:20:45, 18.58s/it]

 56%|█████▌    | 6023/10740 [29:56:54<23:30:34, 17.94s/it]

 56%|█████▌    | 6024/10740 [29:57:10<22:43:32, 17.35s/it]

 56%|█████▌    | 6025/10740 [29:57:22<20:33:58, 15.70s/it]
{'loss': 0.4086, 'learning_rate': 8.517175154661887e-07, 'rewards/chosen': -1.5499829053878784, 'rewards/rejected': -3.322751045227051, 'rewards/accuracies': 0.875, 'rewards/margins': 1.772768259048462, 'policy_logps/rejected': -288.2959289550781, 'policy_logps/chosen': -239.314208984375, 'referece_logps/rejected': -255.06842041015625, 'referece_logps/chosen': -223.8143768310547, 'logits/rejected': -0.3157356083393097, 'logits/chosen': -0.33622777462005615, 'epoch': 3.37}
[2024-04-03 01:11:25,926] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▌    | 6026/10740 [29:57:42<22:29:24, 17.18s/it]


 56%|█████▌    | 6028/10740 [29:58:27<25:52:26, 19.77s/it]
[2024-04-03 01:12:10,453] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▌    | 6029/10740 [29:58:39<23:06:42, 17.66s/it]
[2024-04-03 01:12:23,200] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▌    | 6030/10740 [29:58:57<23:12:13, 17.74s/it]

 56%|█████▌    | 6031/10740 [29:59:10<21:12:29, 16.21s/it]

 56%|█████▌    | 6032/10740 [29:59:29<22:24:19, 17.13s/it]

 56%|█████▌    | 6033/10740 [29:59:44<21:20:21, 16.32s/it]

 56%|█████▌    | 6034/10740 [30:00:03<22:29:00, 17.20s/it]
[2024-04-03 01:13:46,723] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2585, 'learning_rate': 8.490338216735057e-07, 'rewards/chosen': -1.5861570835113525, 'rewards/rejected': -3.3457882404327393, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7596311569213867, 'policy_logps/rejected': -431.5392761230469, 'policy_logps/chosen': -340.576171875, 'referece_logps/rejected': -398.0814208984375, 'referece_logps/chosen': -324.714599609375, 'logits/rejected': -0.3349124789237976, 'logits/chosen': -0.24820254743099213, 'epoch': 3.37}


 56%|█████▌    | 6036/10740 [30:00:39<23:02:01, 17.63s/it]
{'loss': 0.3853, 'learning_rate': 8.484375956172811e-07, 'rewards/chosen': -1.4296642541885376, 'rewards/rejected': -2.6386613845825195, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2089970111846924, 'policy_logps/rejected': -563.8516235351562, 'policy_logps/chosen': -383.6192932128906, 'referece_logps/rejected': -537.4650268554688, 'referece_logps/chosen': -369.3226623535156, 'logits/rejected': -0.2936096787452698, 'logits/chosen': -0.13591475784778595, 'epoch': 3.37}


 56%|█████▌    | 6038/10740 [30:01:13<22:13:53, 17.02s/it]
{'loss': 0.3931, 'learning_rate': 8.478414247009445e-07, 'rewards/chosen': -1.483314037322998, 'rewards/rejected': -2.8980872631073, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4147732257843018, 'policy_logps/rejected': -488.2953186035156, 'policy_logps/chosen': -492.3512878417969, 'referece_logps/rejected': -459.3144836425781, 'referece_logps/chosen': -477.5181579589844, 'logits/rejected': -0.6589275002479553, 'logits/chosen': -0.6714993715286255, 'epoch': 3.37}


 56%|█████▌    | 6040/10740 [30:01:52<23:41:38, 18.15s/it]

 56%|█████▌    | 6041/10740 [30:02:06<21:59:00, 16.84s/it]
{'loss': 0.4209, 'learning_rate': 8.469472721881782e-07, 'rewards/chosen': -1.4361443519592285, 'rewards/rejected': -2.226895570755005, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7907512784004211, 'policy_logps/rejected': -312.44085693359375, 'policy_logps/chosen': -246.2602081298828, 'referece_logps/rejected': -290.171875, 'referece_logps/chosen': -231.8987579345703, 'logits/rejected': -1.0909230709075928, 'logits/chosen': -1.0591861009597778, 'epoch': 3.37}


 56%|█████▋    | 6043/10740 [30:02:39<22:07:39, 16.96s/it]

 56%|█████▋    | 6044/10740 [30:02:58<22:52:36, 17.54s/it]
{'loss': 0.4676, 'learning_rate': 8.460532449600924e-07, 'rewards/chosen': -1.2444257736206055, 'rewards/rejected': -3.325385332107544, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0809595584869385, 'policy_logps/rejected': -270.86431884765625, 'policy_logps/chosen': -393.60174560546875, 'referece_logps/rejected': -237.6104736328125, 'referece_logps/chosen': -381.1575012207031, 'logits/rejected': -0.4471815228462219, 'logits/chosen': -0.5172621011734009, 'epoch': 3.38}


 56%|█████▋    | 6046/10740 [30:03:32<22:25:49, 17.20s/it]
{'loss': 0.4776, 'learning_rate': 8.454572967720358e-07, 'rewards/chosen': -1.6813278198242188, 'rewards/rejected': -2.3814685344696045, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7001407146453857, 'policy_logps/rejected': -319.08563232421875, 'policy_logps/chosen': -324.5227355957031, 'referece_logps/rejected': -295.27093505859375, 'referece_logps/chosen': -307.70947265625, 'logits/rejected': -0.2894434332847595, 'logits/chosen': -0.2031140923500061, 'epoch': 3.38}
[2024-04-03 01:17:36,375] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 56%|█████▋    | 6048/10740 [30:04:12<24:15:40, 18.61s/it]

 56%|█████▋    | 6049/10740 [30:04:31<24:38:30, 18.91s/it]

 56%|█████▋    | 6050/10740 [30:04:46<22:55:18, 17.59s/it]
{'loss': 0.3808, 'learning_rate': 8.442655692851637e-07, 'rewards/chosen': -1.758315920829773, 'rewards/rejected': -2.727952003479004, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9696360230445862, 'policy_logps/rejected': -388.624267578125, 'policy_logps/chosen': -399.421142578125, 'referece_logps/rejected': -361.34478759765625, 'referece_logps/chosen': -381.8380126953125, 'logits/rejected': -1.0231760740280151, 'logits/chosen': -1.1134979724884033, 'epoch': 3.38}


 56%|█████▋    | 6052/10740 [30:05:20<22:19:14, 17.14s/it]

 56%|█████▋    | 6053/10740 [30:05:37<22:18:35, 17.14s/it]

 56%|█████▋    | 6054/10740 [30:05:57<23:09:57, 17.80s/it]

 56%|█████▋    | 6055/10740 [30:06:16<23:40:34, 18.19s/it]

 56%|█████▋    | 6056/10740 [30:06:27<21:07:58, 16.24s/it]

 56%|█████▋    | 6057/10740 [30:06:46<21:53:21, 16.83s/it]

 56%|█████▋    | 6058/10740 [30:07:02<21:43:43, 16.71s/it]
{'loss': 0.3535, 'learning_rate': 8.418827959378241e-07, 'rewards/chosen': -1.516777753829956, 'rewards/rejected': -2.6850011348724365, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1682231426239014, 'policy_logps/rejected': -403.85009765625, 'policy_logps/chosen': -408.10784912109375, 'referece_logps/rejected': -377.0001220703125, 'referece_logps/chosen': -392.9400939941406, 'logits/rejected': -0.0650453269481659, 'logits/chosen': -0.0421886146068573, 'epoch': 3.38}

 56%|█████▋    | 6059/10740 [30:07:17<21:08:39, 16.26s/it]

 56%|█████▋    | 6060/10740 [30:07:37<22:25:33, 17.25s/it]


 56%|█████▋    | 6062/10740 [30:08:22<25:51:13, 19.90s/it]

 56%|█████▋    | 6063/10740 [30:08:37<24:18:30, 18.71s/it]

 56%|█████▋    | 6064/10740 [30:08:54<23:16:29, 17.92s/it]
{'loss': 0.3773, 'learning_rate': 8.400963191769097e-07, 'rewards/chosen': -1.9759347438812256, 'rewards/rejected': -3.1683828830718994, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1924480199813843, 'policy_logps/rejected': -394.4122009277344, 'policy_logps/chosen': -370.32904052734375, 'referece_logps/rejected': -362.72833251953125, 'referece_logps/chosen': -350.5697021484375, 'logits/rejected': -0.7280552387237549, 'logits/chosen': -0.6977350115776062, 'epoch': 3.39}


 56%|█████▋    | 6066/10740 [30:09:28<22:43:09, 17.50s/it]

 56%|█████▋    | 6067/10740 [30:09:46<22:59:53, 17.72s/it]

 56%|█████▋    | 6068/10740 [30:10:03<22:38:28, 17.45s/it]

 57%|█████▋    | 6069/10740 [30:10:24<24:05:47, 18.57s/it]

 57%|█████▋    | 6070/10740 [30:10:46<25:19:50, 19.53s/it]

 57%|█████▋    | 6071/10740 [30:11:07<25:50:38, 19.93s/it]

 57%|█████▋    | 6072/10740 [30:11:24<24:48:42, 19.14s/it]

 57%|█████▋    | 6073/10740 [30:11:36<22:02:31, 17.00s/it]

 57%|█████▋    | 6074/10740 [30:11:58<23:55:30, 18.46s/it]

 57%|█████▋    | 6075/10740 [30:12:12<22:25:51, 17.31s/it]
{'loss': 0.3691, 'learning_rate': 8.368224758300195e-07, 'rewards/chosen': -1.634884238243103, 'rewards/rejected': -4.281006813049316, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6461222171783447, 'policy_logps/rejected': -454.9906311035156, 'policy_logps/chosen': -421.8468322753906, 'referece_logps/rejected': -412.18060302734375, 'referece_logps/chosen': -405.49798583984375, 'logits/rejected': -0.23584839701652527, 'logits/chosen': -0.1963643580675125, 'epoch': 3.39}


 57%|█████▋    | 6077/10740 [30:12:47<22:28:16, 17.35s/it]

 57%|█████▋    | 6078/10740 [30:13:10<24:44:04, 19.10s/it]

 57%|█████▋    | 6079/10740 [30:13:30<25:04:43, 19.37s/it]

 57%|█████▋    | 6080/10740 [30:13:52<26:10:42, 20.22s/it]
{'loss': 0.3928, 'learning_rate': 8.353349567082662e-07, 'rewards/chosen': -1.8533157110214233, 'rewards/rejected': -2.8129405975341797, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9596248865127563, 'policy_logps/rejected': -386.60308837890625, 'policy_logps/chosen': -451.7789306640625, 'referece_logps/rejected': -358.47369384765625, 'referece_logps/chosen': -433.24578857421875, 'logits/rejected': -1.0424573421478271, 'logits/chosen': -1.2155817747116089, 'epoch': 3.4}


 57%|█████▋    | 6082/10740 [30:14:28<24:53:32, 19.24s/it]

 57%|█████▋    | 6083/10740 [30:14:49<25:17:41, 19.55s/it]
{'loss': 0.3478, 'learning_rate': 8.344426247390215e-07, 'rewards/chosen': -1.9540252685546875, 'rewards/rejected': -4.14161491394043, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1875901222229004, 'policy_logps/rejected': -471.0562744140625, 'policy_logps/chosen': -484.3797607421875, 'referece_logps/rejected': -429.6401062011719, 'referece_logps/chosen': -464.83953857421875, 'logits/rejected': -0.27298182249069214, 'logits/chosen': -0.26202160120010376, 'epoch': 3.4}


 57%|█████▋    | 6085/10740 [30:15:22<23:16:34, 18.00s/it]
[2024-04-03 01:29:06,046] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 6086/10740 [30:15:40<23:09:55, 17.92s/it]
{'loss': 0.3139, 'learning_rate': 8.335504282904125e-07, 'rewards/chosen': -0.19768944382667542, 'rewards/rejected': -4.311591148376465, 'rewards/accuracies': 1.0, 'rewards/margins': 4.113901615142822, 'policy_logps/rejected': -330.90972900390625, 'policy_logps/chosen': -286.6978454589844, 'referece_logps/rejected': -287.7937927246094, 'referece_logps/chosen': -284.7209167480469, 'logits/rejected': -0.6628986597061157, 'logits/chosen': -0.6799632906913757, 'epoch': 3.4}

 57%|█████▋    | 6087/10740 [30:15:53<21:24:51, 16.57s/it]


 57%|█████▋    | 6089/10740 [30:16:34<23:49:16, 18.44s/it]

 57%|█████▋    | 6090/10740 [30:16:46<21:30:44, 16.65s/it]
{'loss': 0.4465, 'learning_rate': 8.323610450977548e-07, 'rewards/chosen': -2.2056424617767334, 'rewards/rejected': -2.8925278186798096, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6868852972984314, 'policy_logps/rejected': -365.7537841796875, 'policy_logps/chosen': -478.2235107421875, 'referece_logps/rejected': -336.8284912109375, 'referece_logps/chosen': -456.16705322265625, 'logits/rejected': -1.1931387186050415, 'logits/chosen': -1.1129251718521118, 'epoch': 3.4}


 57%|█████▋    | 6092/10740 [30:17:23<22:15:17, 17.24s/it]

 57%|█████▋    | 6093/10740 [30:17:43<23:23:12, 18.12s/it]

 57%|█████▋    | 6094/10740 [30:18:02<23:49:32, 18.46s/it]

 57%|█████▋    | 6095/10740 [30:18:21<23:55:29, 18.54s/it]

 57%|█████▋    | 6096/10740 [30:18:39<23:41:05, 18.36s/it]

 57%|█████▋    | 6097/10740 [30:18:58<24:12:13, 18.77s/it]

 57%|█████▋    | 6098/10740 [30:19:14<23:10:32, 17.97s/it]

 57%|█████▋    | 6099/10740 [30:19:34<23:51:57, 18.51s/it]

 57%|█████▋    | 6100/10740 [30:19:51<23:22:05, 18.13s/it]

 57%|█████▋    | 6101/10740 [30:20:07<22:23:14, 17.37s/it]
{'loss': 0.3735, 'learning_rate': 8.290915044144285e-07, 'rewards/chosen': -1.7135536670684814, 'rewards/rejected': -3.0154614448547363, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3019077777862549, 'policy_logps/rejected': -275.325927734375, 'policy_logps/chosen': -331.9989013671875, 'referece_logps/rejected': -245.1713104248047, 'referece_logps/chosen': -314.8634033203125, 'logits/rejected': -0.5654826760292053, 'logits/chosen': -0.5398068428039551, 'epoch': 3.41}

 57%|█████▋    | 6102/10740 [30:20:24<22:11:36, 17.23s/it]


 57%|█████▋    | 6104/10740 [30:20:56<21:04:32, 16.37s/it]
{'loss': 0.3365, 'learning_rate': 8.282001364225756e-07, 'rewards/chosen': -0.8454681634902954, 'rewards/rejected': -3.546562671661377, 'rewards/accuracies': 0.875, 'rewards/margins': 2.701094388961792, 'policy_logps/rejected': -414.8254699707031, 'policy_logps/chosen': -382.81939697265625, 'referece_logps/rejected': -379.3598327636719, 'referece_logps/chosen': -374.36474609375, 'logits/rejected': -1.1564275026321411, 'logits/chosen': -1.0996899604797363, 'epoch': 3.41}


 57%|█████▋    | 6106/10740 [30:21:33<22:25:53, 17.43s/it]

 57%|█████▋    | 6107/10740 [30:21:52<22:39:05, 17.60s/it]
{'loss': 0.3729, 'learning_rate': 8.273089090612844e-07, 'rewards/chosen': -1.7757771015167236, 'rewards/rejected': -2.7451748847961426, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9693979024887085, 'policy_logps/rejected': -303.83782958984375, 'policy_logps/chosen': -274.81463623046875, 'referece_logps/rejected': -276.3861083984375, 'referece_logps/chosen': -257.05682373046875, 'logits/rejected': -1.061315655708313, 'logits/chosen': -1.102677583694458, 'epoch': 3.41}


 57%|█████▋    | 6109/10740 [30:22:29<23:23:52, 18.19s/it]

 57%|█████▋    | 6110/10740 [30:22:47<23:19:28, 18.14s/it]
{'loss': 0.3752, 'learning_rate': 8.264178230600884e-07, 'rewards/chosen': -1.4945874214172363, 'rewards/rejected': -2.746521234512329, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2519338130950928, 'policy_logps/rejected': -249.36270141601562, 'policy_logps/chosen': -278.46600341796875, 'referece_logps/rejected': -221.89752197265625, 'referece_logps/chosen': -263.5201416015625, 'logits/rejected': -0.9781964421272278, 'logits/chosen': -0.9456615447998047, 'epoch': 3.41}

 57%|█████▋    | 6111/10740 [30:22:58<20:34:06, 16.00s/it]

 57%|█████▋    | 6112/10740 [30:23:16<21:26:08, 16.67s/it]


 57%|█████▋    | 6114/10740 [30:23:55<23:07:12, 17.99s/it]
{'loss': 0.3037, 'learning_rate': 8.252299295460157e-07, 'rewards/chosen': -1.364652156829834, 'rewards/rejected': -2.673978090286255, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3093258142471313, 'policy_logps/rejected': -256.3130187988281, 'policy_logps/chosen': -255.4164276123047, 'referece_logps/rejected': -229.57322692871094, 'referece_logps/chosen': -241.7698974609375, 'logits/rejected': -0.7731236815452576, 'logits/chosen': -0.7733896970748901, 'epoch': 3.42}


 57%|█████▋    | 6116/10740 [30:24:29<22:20:16, 17.39s/it]
{'loss': 0.2982, 'learning_rate': 8.246360780555398e-07, 'rewards/chosen': -1.318226933479309, 'rewards/rejected': -2.695361852645874, 'rewards/accuracies': 0.875, 'rewards/margins': 1.377134919166565, 'policy_logps/rejected': -426.2821044921875, 'policy_logps/chosen': -563.5595092773438, 'referece_logps/rejected': -399.3284912109375, 'referece_logps/chosen': -550.3772583007812, 'logits/rejected': 0.5168533325195312, 'logits/chosen': 0.36601686477661133, 'epoch': 3.42}

 57%|█████▋    | 6117/10740 [30:24:43<20:45:13, 16.16s/it]
[2024-04-03 01:38:50,353] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 57%|█████▋    | 6119/10740 [30:25:23<22:55:14, 17.86s/it]
[2024-04-03 01:39:06,787] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3825, 'learning_rate': 8.237454205106732e-07, 'rewards/chosen': -0.27114611864089966, 'rewards/rejected': -3.006765365600586, 'rewards/accuracies': 1.0, 'rewards/margins': 2.735619306564331, 'policy_logps/rejected': -318.00372314453125, 'policy_logps/chosen': -291.0924987792969, 'referece_logps/rejected': -287.9360656738281, 'referece_logps/chosen': -288.38104248046875, 'logits/rejected': 0.16970255970954895, 'logits/chosen': 0.22680436074733734, 'epoch': 3.42}

 57%|█████▋    | 6120/10740 [30:25:37<21:22:51, 16.66s/it]
[2024-04-03 01:39:43,901] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 6121/10740 [30:26:00<23:54:33, 18.63s/it]
[2024-04-03 01:40:00,085] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 6122/10740 [30:26:16<22:57:39, 17.90s/it]

 57%|█████▋    | 6123/10740 [30:26:28<20:39:32, 16.11s/it]

 57%|█████▋    | 6124/10740 [30:26:40<18:58:23, 14.80s/it]


 57%|█████▋    | 6126/10740 [30:27:19<22:03:50, 17.22s/it]

 57%|█████▋    | 6127/10740 [30:27:34<20:59:36, 16.38s/it]
{'loss': 0.3568, 'learning_rate': 8.213710410587233e-07, 'rewards/chosen': -1.67265784740448, 'rewards/rejected': -2.842094898223877, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1694369316101074, 'policy_logps/rejected': -364.7767333984375, 'policy_logps/chosen': -515.8358764648438, 'referece_logps/rejected': -336.35577392578125, 'referece_logps/chosen': -499.1092834472656, 'logits/rejected': -0.979261040687561, 'logits/chosen': -1.1222727298736572, 'epoch': 3.42}

 57%|█████▋    | 6128/10740 [30:27:51<21:07:25, 16.49s/it]


 57%|█████▋    | 6130/10740 [30:28:20<20:25:58, 15.96s/it]
[2024-04-03 01:42:03,693] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 6131/10740 [30:28:42<22:46:37, 17.79s/it]
{'loss': 0.4043, 'learning_rate': 8.201842403907808e-07, 'rewards/chosen': -0.8626784086227417, 'rewards/rejected': -2.6234612464904785, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7607828378677368, 'policy_logps/rejected': -323.6075744628906, 'policy_logps/chosen': -280.21142578125, 'referece_logps/rejected': -297.3729553222656, 'referece_logps/chosen': -271.58465576171875, 'logits/rejected': -1.0818147659301758, 'logits/chosen': -1.2618507146835327, 'epoch': 3.43}

 57%|█████▋    | 6132/10740 [30:29:01<23:08:37, 18.08s/it]


 57%|█████▋    | 6134/10740 [30:29:26<19:30:14, 15.24s/it]

 57%|█████▋    | 6135/10740 [30:29:45<21:10:23, 16.55s/it]
{'loss': 0.4555, 'learning_rate': 8.189977013977575e-07, 'rewards/chosen': -1.4994547367095947, 'rewards/rejected': -4.283581256866455, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7841267585754395, 'policy_logps/rejected': -429.44464111328125, 'policy_logps/chosen': -428.51275634765625, 'referece_logps/rejected': -386.60882568359375, 'referece_logps/chosen': -413.51824951171875, 'logits/rejected': -0.10514834523200989, 'logits/chosen': -0.2224542200565338, 'epoch': 3.43}


 57%|█████▋    | 6137/10740 [30:30:26<23:27:10, 18.34s/it]

 57%|█████▋    | 6138/10740 [30:30:44<23:14:55, 18.19s/it]
{'loss': 0.4384, 'learning_rate': 8.181079699158873e-07, 'rewards/chosen': -2.0670952796936035, 'rewards/rejected': -3.105107545852661, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0380120277404785, 'policy_logps/rejected': -538.8157958984375, 'policy_logps/chosen': -532.723388671875, 'referece_logps/rejected': -507.7646789550781, 'referece_logps/chosen': -512.0524291992188, 'logits/rejected': -0.42978155612945557, 'logits/chosen': -0.3290884494781494, 'epoch': 3.43}


 57%|█████▋    | 6140/10740 [30:31:24<24:13:29, 18.96s/it]
{'loss': 0.3712, 'learning_rate': 8.17514898267314e-07, 'rewards/chosen': -1.1458669900894165, 'rewards/rejected': -2.4956836700439453, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3498166799545288, 'policy_logps/rejected': -362.0316162109375, 'policy_logps/chosen': -388.059814453125, 'referece_logps/rejected': -337.0748291015625, 'referece_logps/chosen': -376.60113525390625, 'logits/rejected': -0.8941689133644104, 'logits/chosen': -0.9543836712837219, 'epoch': 3.43}


 57%|█████▋    | 6142/10740 [30:32:02<24:27:15, 19.15s/it]

 57%|█████▋    | 6143/10740 [30:32:22<24:37:06, 19.28s/it]

 57%|█████▋    | 6144/10740 [30:32:42<24:47:46, 19.42s/it]

 57%|█████▋    | 6145/10740 [30:33:04<25:51:49, 20.26s/it]

 57%|█████▋    | 6146/10740 [30:33:24<25:49:40, 20.24s/it]

 57%|█████▋    | 6147/10740 [30:33:46<26:26:01, 20.72s/it]
{'loss': 0.2706, 'learning_rate': 8.154396717332646e-07, 'rewards/chosen': -1.5262342691421509, 'rewards/rejected': -2.9760992527008057, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4498651027679443, 'policy_logps/rejected': -551.55029296875, 'policy_logps/chosen': -519.6804809570312, 'referece_logps/rejected': -521.7893676757812, 'referece_logps/chosen': -504.41815185546875, 'logits/rejected': -0.6568893194198608, 'logits/chosen': -0.6590746641159058, 'epoch': 3.43}


 57%|█████▋    | 6149/10740 [30:34:24<25:36:33, 20.08s/it]

 57%|█████▋    | 6150/10740 [30:34:42<24:46:43, 19.43s/it]
{'loss': 0.3544, 'learning_rate': 8.145505401869359e-07, 'rewards/chosen': -1.1607176065444946, 'rewards/rejected': -3.4064035415649414, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2456862926483154, 'policy_logps/rejected': -238.1678924560547, 'policy_logps/chosen': -232.06991577148438, 'referece_logps/rejected': -204.10385131835938, 'referece_logps/chosen': -220.46273803710938, 'logits/rejected': -1.166750192642212, 'logits/chosen': -1.2403440475463867, 'epoch': 3.44}


 57%|█████▋    | 6152/10740 [30:35:22<25:07:34, 19.72s/it]
{'loss': 0.3156, 'learning_rate': 8.139578701132052e-07, 'rewards/chosen': -1.7465721368789673, 'rewards/rejected': -3.8821966648101807, 'rewards/accuracies': 1.0, 'rewards/margins': 2.135624647140503, 'policy_logps/rejected': -446.63592529296875, 'policy_logps/chosen': -471.39886474609375, 'referece_logps/rejected': -407.8139953613281, 'referece_logps/chosen': -453.93310546875, 'logits/rejected': -1.3619215488433838, 'logits/chosen': -1.453261137008667, 'epoch': 3.44}
[2024-04-03 01:49:26,356] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 57%|█████▋    | 6154/10740 [30:36:00<24:32:52, 19.27s/it]
{'loss': 0.385, 'learning_rate': 8.133652677234252e-07, 'rewards/chosen': -1.2669916152954102, 'rewards/rejected': -2.8196616172790527, 'rewards/accuracies': 1.0, 'rewards/margins': 1.552669882774353, 'policy_logps/rejected': -530.2939453125, 'policy_logps/chosen': -446.58966064453125, 'referece_logps/rejected': -502.09735107421875, 'referece_logps/chosen': -433.9197082519531, 'logits/rejected': 0.27885642647743225, 'logits/chosen': 0.12195482850074768, 'epoch': 3.44}


 57%|█████▋    | 6156/10740 [30:36:28<21:23:26, 16.80s/it]

 57%|█████▋    | 6157/10740 [30:36:40<19:33:20, 15.36s/it]

 57%|█████▋    | 6158/10740 [30:36:54<19:10:04, 15.06s/it]

 57%|█████▋    | 6159/10740 [30:37:17<21:56:37, 17.24s/it]

 57%|█████▋    | 6160/10740 [30:37:38<23:31:13, 18.49s/it]
[2024-04-03 01:51:21,649] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3598, 'learning_rate': 8.115878688136086e-07, 'rewards/chosen': -2.6128971576690674, 'rewards/rejected': -4.460307598114014, 'rewards/accuracies': 0.875, 'rewards/margins': 1.847410798072815, 'policy_logps/rejected': -414.85235595703125, 'policy_logps/chosen': -388.32293701171875, 'referece_logps/rejected': -370.249267578125, 'referece_logps/chosen': -362.1939697265625, 'logits/rejected': 0.23361018300056458, 'logits/chosen': 0.13250018656253815, 'epoch': 3.44}

 57%|█████▋    | 6161/10740 [30:37:57<23:39:32, 18.60s/it]


 57%|█████▋    | 6163/10740 [30:38:33<23:19:03, 18.34s/it]
[2024-04-03 01:52:16,341] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 6164/10740 [30:38:52<23:49:14, 18.74s/it]

 57%|█████▋    | 6165/10740 [30:39:13<24:26:32, 19.23s/it]
{'loss': 0.4275, 'learning_rate': 8.10107174063384e-07, 'rewards/chosen': -1.8577083349227905, 'rewards/rejected': -3.1687889099121094, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3110805749893188, 'policy_logps/rejected': -422.76007080078125, 'policy_logps/chosen': -389.74493408203125, 'referece_logps/rejected': -391.0721740722656, 'referece_logps/chosen': -371.1678771972656, 'logits/rejected': -0.3918158710002899, 'logits/chosen': -0.40933558344841003, 'epoch': 3.44}

 57%|█████▋    | 6166/10740 [30:39:31<23:59:31, 18.88s/it]

 57%|█████▋    | 6167/10740 [30:39:52<24:44:50, 19.48s/it]

 57%|█████▋    | 6168/10740 [30:40:10<24:17:50, 19.13s/it]


 57%|█████▋    | 6170/10740 [30:40:48<24:31:57, 19.33s/it]

 57%|█████▋    | 6171/10740 [30:41:02<22:25:02, 17.66s/it]

 57%|█████▋    | 6172/10740 [30:41:23<23:33:11, 18.56s/it]

 57%|█████▋    | 6173/10740 [30:41:44<24:45:13, 19.51s/it]
{'loss': 0.3785, 'learning_rate': 8.077389619664378e-07, 'rewards/chosen': -2.540601968765259, 'rewards/rejected': -3.7126975059509277, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1720951795578003, 'policy_logps/rejected': -599.8124389648438, 'policy_logps/chosen': -513.9217529296875, 'referece_logps/rejected': -562.6854248046875, 'referece_logps/chosen': -488.5157775878906, 'logits/rejected': 0.1076202541589737, 'logits/chosen': 0.014692164957523346, 'epoch': 3.45}

 57%|█████▋    | 6174/10740 [30:42:06<25:23:41, 20.02s/it]

 57%|█████▋    | 6175/10740 [30:42:24<24:41:43, 19.48s/it]

 58%|█████▊    | 6176/10740 [30:42:40<23:19:34, 18.40s/it]

 58%|█████▊    | 6177/10740 [30:42:59<23:45:57, 18.75s/it]

 58%|█████▊    | 6178/10740 [30:43:11<21:12:24, 16.73s/it]

 58%|█████▊    | 6179/10740 [30:43:27<20:48:44, 16.43s/it]

 58%|█████▊    | 6180/10740 [30:43:43<20:43:35, 16.36s/it]
[2024-04-03 01:57:48,777] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 58%|█████▊    | 6182/10740 [30:44:17<20:22:59, 16.10s/it]
[2024-04-03 01:58:00,476] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 58%|█████▊    | 6183/10740 [30:44:35<21:04:54, 16.65s/it]

 58%|█████▊    | 6184/10740 [30:44:55<22:18:23, 17.63s/it]

 58%|█████▊    | 6185/10740 [30:45:14<23:06:29, 18.26s/it]
{'loss': 0.3662, 'learning_rate': 8.041887465196786e-07, 'rewards/chosen': -1.3650094270706177, 'rewards/rejected': -3.153334379196167, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7883245944976807, 'policy_logps/rejected': -266.6563415527344, 'policy_logps/chosen': -350.4683837890625, 'referece_logps/rejected': -235.1230010986328, 'referece_logps/chosen': -336.8182678222656, 'logits/rejected': -0.5475874543190002, 'logits/chosen': -0.6150343418121338, 'epoch': 3.46}

 58%|█████▊    | 6186/10740 [30:45:32<22:46:43, 18.01s/it]

 58%|█████▊    | 6187/10740 [30:45:52<23:39:49, 18.71s/it]

 58%|█████▊    | 6188/10740 [30:46:10<23:16:18, 18.40s/it]

 58%|█████▊    | 6189/10740 [30:46:30<23:50:13, 18.86s/it]

 58%|█████▊    | 6190/10740 [30:46:50<24:14:19, 19.18s/it]

 58%|█████▊    | 6191/10740 [30:47:09<24:23:40, 19.31s/it]


 58%|█████▊    | 6193/10740 [30:47:49<25:07:54, 19.90s/it]
[2024-04-03 02:01:32,825] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3662, 'learning_rate': 8.018233581125418e-07, 'rewards/chosen': -1.5422874689102173, 'rewards/rejected': -3.2929396629333496, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7506523132324219, 'policy_logps/rejected': -498.4007568359375, 'policy_logps/chosen': -432.6812744140625, 'referece_logps/rejected': -465.4713439941406, 'referece_logps/chosen': -417.2583923339844, 'logits/rejected': -0.46579039096832275, 'logits/chosen': -0.47476035356521606, 'epoch': 3.46}


 58%|█████▊    | 6195/10740 [30:48:17<21:13:43, 16.81s/it]
{'loss': 0.4053, 'learning_rate': 8.012321907194898e-07, 'rewards/chosen': -1.570029854774475, 'rewards/rejected': -3.3474185466766357, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7773890495300293, 'policy_logps/rejected': -328.4784240722656, 'policy_logps/chosen': -411.74005126953125, 'referece_logps/rejected': -295.0042419433594, 'referece_logps/chosen': -396.0397033691406, 'logits/rejected': -0.5073074102401733, 'logits/chosen': -0.5854769945144653, 'epoch': 3.46}

 58%|█████▊    | 6196/10740 [30:48:40<23:33:31, 18.66s/it]

 58%|█████▊    | 6197/10740 [30:48:55<22:09:13, 17.56s/it]

 58%|█████▊    | 6198/10740 [30:49:10<21:14:33, 16.84s/it]

 58%|█████▊    | 6199/10740 [30:49:22<19:39:47, 15.59s/it]


 58%|█████▊    | 6201/10740 [30:49:57<20:27:44, 16.23s/it]

 58%|█████▊    | 6202/10740 [30:50:17<21:46:01, 17.27s/it]

 58%|█████▊    | 6203/10740 [30:50:31<20:31:13, 16.28s/it]
{'loss': 0.4021, 'learning_rate': 7.988682464343845e-07, 'rewards/chosen': -1.9226192235946655, 'rewards/rejected': -3.5083277225494385, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5857086181640625, 'policy_logps/rejected': -524.7020874023438, 'policy_logps/chosen': -487.7298278808594, 'referece_logps/rejected': -489.61883544921875, 'referece_logps/chosen': -468.50360107421875, 'logits/rejected': -0.2689304053783417, 'logits/chosen': -0.3633512258529663, 'epoch': 3.47}


 58%|█████▊    | 6205/10740 [30:51:09<22:20:22, 17.73s/it]

 58%|█████▊    | 6206/10740 [30:51:21<20:07:39, 15.98s/it]
{'loss': 0.4378, 'learning_rate': 7.979820684299905e-07, 'rewards/chosen': -1.69217050075531, 'rewards/rejected': -5.04799747467041, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3558268547058105, 'policy_logps/rejected': -567.4144287109375, 'policy_logps/chosen': -353.4141845703125, 'referece_logps/rejected': -516.9344482421875, 'referece_logps/chosen': -336.49249267578125, 'logits/rejected': 0.16025805473327637, 'logits/chosen': 0.23974654078483582, 'epoch': 3.47}

 58%|█████▊    | 6207/10740 [30:51:44<22:37:14, 17.96s/it]

 58%|█████▊    | 6208/10740 [30:52:04<23:39:38, 18.79s/it]

 58%|█████▊    | 6209/10740 [30:52:26<24:50:34, 19.74s/it]

 58%|█████▊    | 6210/10740 [30:52:40<22:41:54, 18.04s/it]

 58%|█████▊    | 6211/10740 [30:52:58<22:44:14, 18.07s/it]


 58%|█████▊    | 6213/10740 [30:53:25<19:55:10, 15.84s/it]

 58%|█████▊    | 6214/10740 [30:53:44<20:54:17, 16.63s/it]

 58%|█████▊    | 6215/10740 [30:54:02<21:26:01, 17.05s/it]

 58%|█████▊    | 6216/10740 [30:54:22<22:31:48, 17.93s/it]
{'loss': 0.3286, 'learning_rate': 7.950293401340815e-07, 'rewards/chosen': -1.136062741279602, 'rewards/rejected': -2.773441791534424, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6373789310455322, 'policy_logps/rejected': -324.4607849121094, 'policy_logps/chosen': -317.96484375, 'referece_logps/rejected': -296.72637939453125, 'referece_logps/chosen': -306.604248046875, 'logits/rejected': -0.28871023654937744, 'logits/chosen': -0.24492576718330383, 'epoch': 3.47}

 58%|█████▊    | 6217/10740 [30:54:41<23:07:29, 18.41s/it]

 58%|█████▊    | 6218/10740 [30:55:00<23:26:30, 18.66s/it]

 58%|█████▊    | 6219/10740 [30:55:17<22:42:42, 18.08s/it]

 58%|█████▊    | 6220/10740 [30:55:31<20:57:56, 16.70s/it]
[2024-04-03 02:09:35,506] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 58%|█████▊    | 6221/10740 [30:55:52<22:39:24, 18.05s/it]

 58%|█████▊    | 6222/10740 [30:56:10<22:38:02, 18.04s/it]

 58%|█████▊    | 6223/10740 [30:56:29<23:14:30, 18.52s/it]

 58%|█████▊    | 6224/10740 [30:56:51<24:20:53, 19.41s/it]

 58%|█████▊    | 6225/10740 [30:57:09<23:44:07, 18.93s/it]

 58%|█████▊    | 6226/10740 [30:57:26<23:16:09, 18.56s/it]

 58%|█████▊    | 6227/10740 [30:57:38<20:45:06, 16.55s/it]

 58%|█████▊    | 6228/10740 [30:57:58<21:58:14, 17.53s/it]

 58%|█████▊    | 6229/10740 [30:58:15<21:45:49, 17.37s/it]


 58%|█████▊    | 6231/10740 [30:58:50<22:02:33, 17.60s/it]
{'loss': 0.5184, 'learning_rate': 7.906037515599282e-07, 'rewards/chosen': -1.5226426124572754, 'rewards/rejected': -3.225796699523926, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7031540870666504, 'policy_logps/rejected': -386.1665344238281, 'policy_logps/chosen': -397.7829284667969, 'referece_logps/rejected': -353.9085693359375, 'referece_logps/chosen': -382.5565185546875, 'logits/rejected': -0.5882721543312073, 'logits/chosen': -0.6043173670768738, 'epoch': 3.48}

 58%|█████▊    | 6232/10740 [30:59:06<21:40:21, 17.31s/it]

 58%|█████▊    | 6233/10740 [30:59:20<20:27:58, 16.35s/it]

 58%|█████▊    | 6234/10740 [30:59:39<21:08:24, 16.89s/it]

 58%|█████▊    | 6235/10740 [30:59:58<22:12:17, 17.74s/it]

 58%|█████▊    | 6236/10740 [31:00:17<22:24:15, 17.91s/it]

 58%|█████▊    | 6237/10740 [31:00:38<23:30:50, 18.80s/it]

 58%|█████▊    | 6238/10740 [31:00:58<24:07:01, 19.29s/it]

 58%|█████▊    | 6239/10740 [31:01:18<24:16:13, 19.41s/it]

 58%|█████▊    | 6240/10740 [31:01:37<24:20:08, 19.47s/it]

 58%|█████▊    | 6241/10740 [31:01:57<24:25:48, 19.55s/it]

 58%|█████▊    | 6242/10740 [31:02:16<24:17:57, 19.45s/it]

 58%|█████▊    | 6243/10740 [31:02:36<24:19:24, 19.47s/it]

 58%|█████▊    | 6244/10740 [31:02:56<24:37:28, 19.72s/it]

 58%|█████▊    | 6245/10740 [31:03:14<24:02:52, 19.26s/it]

 58%|█████▊    | 6246/10740 [31:03:34<24:11:11, 19.38s/it]

 58%|█████▊    | 6247/10740 [31:03:46<21:24:51, 17.16s/it]

 58%|█████▊    | 6248/10740 [31:04:06<22:30:57, 18.04s/it]

 58%|█████▊    | 6249/10740 [31:04:26<23:12:08, 18.60s/it]

 58%|█████▊    | 6250/10740 [31:04:43<22:47:25, 18.27s/it]

 58%|█████▊    | 6251/10740 [31:04:55<20:15:21, 16.24s/it]

 58%|█████▊    | 6252/10740 [31:05:13<20:48:34, 16.69s/it]

 58%|█████▊    | 6253/10740 [31:05:25<19:15:32, 15.45s/it]

 58%|█████▊    | 6254/10740 [31:05:38<18:15:15, 14.65s/it]

 58%|█████▊    | 6255/10740 [31:05:55<19:00:01, 15.25s/it]

 58%|█████▊    | 6256/10740 [31:06:12<19:55:02, 15.99s/it]

 58%|█████▊    | 6257/10740 [31:06:30<20:36:08, 16.54s/it]

 58%|█████▊    | 6258/10740 [31:06:47<20:36:24, 16.55s/it]

 58%|█████▊    | 6259/10740 [31:07:06<21:47:10, 17.50s/it]


 58%|█████▊    | 6261/10740 [31:07:42<21:44:57, 17.48s/it]

 58%|█████▊    | 6262/10740 [31:07:57<20:39:45, 16.61s/it]

 58%|█████▊    | 6263/10740 [31:08:20<23:04:06, 18.55s/it]

 58%|█████▊    | 6264/10740 [31:08:42<24:25:35, 19.65s/it]

 58%|█████▊    | 6265/10740 [31:09:00<23:45:43, 19.12s/it]

 58%|█████▊    | 6266/10740 [31:09:20<24:11:30, 19.47s/it]
{'loss': 0.3517, 'learning_rate': 7.802941990266135e-07, 'rewards/chosen': -2.5430986881256104, 'rewards/rejected': -4.2295823097229, 'rewards/accuracies': 0.75, 'rewards/margins': 1.686483383178711, 'policy_logps/rejected': -345.6971130371094, 'policy_logps/chosen': -364.8443298339844, 'referece_logps/rejected': -303.40130615234375, 'referece_logps/chosen': -339.413330078125, 'logits/rejected': -0.9369393587112427, 'logits/chosen': -0.817132830619812, 'epoch': 3.5}


 58%|█████▊    | 6268/10740 [31:09:45<19:34:43, 15.76s/it]

 58%|█████▊    | 6269/10740 [31:10:03<20:22:53, 16.41s/it]

 58%|█████▊    | 6270/10740 [31:10:15<18:46:25, 15.12s/it]

 58%|█████▊    | 6271/10740 [31:10:31<19:14:17, 15.50s/it]

 58%|█████▊    | 6272/10740 [31:10:53<21:40:11, 17.46s/it]

 58%|█████▊    | 6273/10740 [31:11:10<21:09:56, 17.06s/it]

 58%|█████▊    | 6274/10740 [31:11:26<20:57:32, 16.89s/it]

 58%|█████▊    | 6275/10740 [31:11:47<22:34:10, 18.20s/it]

 58%|█████▊    | 6276/10740 [31:12:05<22:28:24, 18.12s/it]
{'loss': 0.3568, 'learning_rate': 7.773530585073464e-07, 'rewards/chosen': -1.9181199073791504, 'rewards/rejected': -4.360311508178711, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4421913623809814, 'policy_logps/rejected': -531.998046875, 'policy_logps/chosen': -575.14697265625, 'referece_logps/rejected': -488.39495849609375, 'referece_logps/chosen': -555.9658203125, 'logits/rejected': -0.48251062631607056, 'logits/chosen': -0.4284436106681824, 'epoch': 3.51}


 58%|█████▊    | 6278/10740 [31:12:40<21:35:04, 17.41s/it]

 58%|█████▊    | 6279/10740 [31:12:56<21:05:50, 17.03s/it]
{'loss': 0.3781, 'learning_rate': 7.764711100151722e-07, 'rewards/chosen': -1.8341102600097656, 'rewards/rejected': -2.808608293533325, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9744980931282043, 'policy_logps/rejected': -445.09124755859375, 'policy_logps/chosen': -352.05010986328125, 'referece_logps/rejected': -417.0051574707031, 'referece_logps/chosen': -333.7090148925781, 'logits/rejected': -1.0722378492355347, 'logits/chosen': -1.0829991102218628, 'epoch': 3.51}


 58%|█████▊    | 6281/10740 [31:13:31<20:43:47, 16.74s/it]

 58%|█████▊    | 6282/10740 [31:13:49<21:24:42, 17.29s/it]

 59%|█████▊    | 6283/10740 [31:14:07<21:38:48, 17.48s/it]

 59%|█████▊    | 6284/10740 [31:14:24<21:13:19, 17.15s/it]

 59%|█████▊    | 6285/10740 [31:14:45<22:43:43, 18.37s/it]

 59%|█████▊    | 6286/10740 [31:15:05<23:24:08, 18.92s/it]

 59%|█████▊    | 6287/10740 [31:15:18<21:16:40, 17.20s/it]

 59%|█████▊    | 6288/10740 [31:15:38<22:15:40, 18.00s/it]

 59%|█████▊    | 6289/10740 [31:15:58<22:57:45, 18.57s/it]

 59%|█████▊    | 6290/10740 [31:16:18<23:18:56, 18.86s/it]

 59%|█████▊    | 6291/10740 [31:16:37<23:35:50, 19.09s/it]

 59%|█████▊    | 6292/10740 [31:16:58<24:11:02, 19.57s/it]

 59%|█████▊    | 6293/10740 [31:17:20<25:10:25, 20.38s/it]

 59%|█████▊    | 6294/10740 [31:17:40<24:55:54, 20.19s/it]

 59%|█████▊    | 6295/10740 [31:18:00<24:42:25, 20.01s/it]

 59%|█████▊    | 6296/10740 [31:18:19<24:37:14, 19.94s/it]

 59%|█████▊    | 6297/10740 [31:18:39<24:30:33, 19.86s/it]

 59%|█████▊    | 6298/10740 [31:18:58<24:14:57, 19.65s/it]
{'loss': 0.4291, 'learning_rate': 7.708897150973386e-07, 'rewards/chosen': -1.6513984203338623, 'rewards/rejected': -3.5745015144348145, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9231032133102417, 'policy_logps/rejected': -386.1053771972656, 'policy_logps/chosen': -403.8248596191406, 'referece_logps/rejected': -350.36041259765625, 'referece_logps/chosen': -387.31085205078125, 'logits/rejected': -0.4664868414402008, 'logits/chosen': -0.5901473760604858, 'epoch': 3.52}


 59%|█████▊    | 6300/10740 [31:19:31<22:00:11, 17.84s/it]

 59%|█████▊    | 6301/10740 [31:19:42<19:31:08, 15.83s/it]

 59%|█████▊    | 6302/10740 [31:19:58<19:36:51, 15.91s/it]
{'loss': 0.3319, 'learning_rate': 7.6971563699967e-07, 'rewards/chosen': -1.4244251251220703, 'rewards/rejected': -3.7251806259155273, 'rewards/accuracies': 1.0, 'rewards/margins': 2.300755500793457, 'policy_logps/rejected': -422.78326416015625, 'policy_logps/chosen': -446.7891845703125, 'referece_logps/rejected': -385.53143310546875, 'referece_logps/chosen': -432.544921875, 'logits/rejected': -0.9553381204605103, 'logits/chosen': -0.9826977252960205, 'epoch': 3.52}


 59%|█████▊    | 6304/10740 [31:20:30<20:00:22, 16.24s/it]

 59%|█████▊    | 6305/10740 [31:20:45<19:29:51, 15.83s/it]

 59%|█████▊    | 6306/10740 [31:21:02<19:45:33, 16.04s/it]

 59%|█████▊    | 6307/10740 [31:21:24<22:01:44, 17.89s/it]

 59%|█████▊    | 6308/10740 [31:21:46<23:25:05, 19.02s/it]
{'loss': 0.5376, 'learning_rate': 7.679551487347156e-07, 'rewards/chosen': -2.3733692169189453, 'rewards/rejected': -3.3836517333984375, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0102826356887817, 'policy_logps/rejected': -444.6474914550781, 'policy_logps/chosen': -380.36834716796875, 'referece_logps/rejected': -410.81097412109375, 'referece_logps/chosen': -356.6346740722656, 'logits/rejected': 0.33179372549057007, 'logits/chosen': 0.3166888356208801, 'epoch': 3.52}

 59%|█████▊    | 6309/10740 [31:21:59<21:21:54, 17.36s/it]


 59%|█████▉    | 6311/10740 [31:22:37<22:39:52, 18.42s/it]

 59%|█████▉    | 6312/10740 [31:23:00<24:03:22, 19.56s/it]

 59%|█████▉    | 6313/10740 [31:23:19<23:47:26, 19.35s/it]
{'loss': 0.3342, 'learning_rate': 7.664886553247071e-07, 'rewards/chosen': -1.4096837043762207, 'rewards/rejected': -2.4204957485198975, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0108121633529663, 'policy_logps/rejected': -324.1578369140625, 'policy_logps/chosen': -233.9508056640625, 'referece_logps/rejected': -299.9529113769531, 'referece_logps/chosen': -219.85398864746094, 'logits/rejected': -0.6283789873123169, 'logits/chosen': -0.6940493583679199, 'epoch': 3.53}

 59%|█████▉    | 6314/10740 [31:23:39<24:19:02, 19.78s/it]


 59%|█████▉    | 6316/10740 [31:24:14<22:53:48, 18.63s/it]

 59%|█████▉    | 6317/10740 [31:24:28<21:11:12, 17.24s/it]

 59%|█████▉    | 6318/10740 [31:24:47<21:51:37, 17.80s/it]

 59%|█████▉    | 6319/10740 [31:25:07<22:36:11, 18.41s/it]

 59%|█████▉    | 6320/10740 [31:25:21<21:00:34, 17.11s/it]

 59%|█████▉    | 6321/10740 [31:25:44<23:12:31, 18.91s/it]

 59%|█████▉    | 6322/10740 [31:26:04<23:31:23, 19.17s/it]

 59%|█████▉    | 6323/10740 [31:26:24<23:50:19, 19.43s/it]

 59%|█████▉    | 6324/10740 [31:26:43<23:41:20, 19.31s/it]

 59%|█████▉    | 6325/10740 [31:27:00<22:48:19, 18.60s/it]

 59%|█████▉    | 6326/10740 [31:27:18<22:49:26, 18.61s/it]

 59%|█████▉    | 6327/10740 [31:27:35<22:05:24, 18.02s/it]

 59%|█████▉    | 6328/10740 [31:27:57<23:20:55, 19.05s/it]

 59%|█████▉    | 6329/10740 [31:28:17<23:51:27, 19.47s/it]

 59%|█████▉    | 6330/10740 [31:28:39<24:54:06, 20.33s/it]

 59%|█████▉    | 6331/10740 [31:29:01<25:29:09, 20.81s/it]

 59%|█████▉    | 6332/10740 [31:29:19<24:24:11, 19.93s/it]

 59%|█████▉    | 6333/10740 [31:29:37<23:33:39, 19.25s/it]
{'loss': 0.3247, 'learning_rate': 7.606280246182055e-07, 'rewards/chosen': -2.5754072666168213, 'rewards/rejected': -4.546180725097656, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9707731008529663, 'policy_logps/rejected': -400.03070068359375, 'policy_logps/chosen': -485.6092834472656, 'referece_logps/rejected': -354.5688781738281, 'referece_logps/chosen': -459.855224609375, 'logits/rejected': -1.147611141204834, 'logits/chosen': -1.134934663772583, 'epoch': 3.54}


 59%|█████▉    | 6335/10740 [31:30:14<23:14:56, 19.00s/it]

 59%|█████▉    | 6336/10740 [31:30:29<21:48:24, 17.83s/it]
{'loss': 0.3494, 'learning_rate': 7.597496759192979e-07, 'rewards/chosen': -1.6588435173034668, 'rewards/rejected': -2.918325662612915, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2594821453094482, 'policy_logps/rejected': -317.40631103515625, 'policy_logps/chosen': -302.2303466796875, 'referece_logps/rejected': -288.22308349609375, 'referece_logps/chosen': -285.64190673828125, 'logits/rejected': -0.2386307269334793, 'logits/chosen': -0.3183111548423767, 'epoch': 3.54}


 59%|█████▉    | 6338/10740 [31:30:57<19:34:56, 16.01s/it]

 59%|█████▉    | 6339/10740 [31:31:18<21:28:13, 17.56s/it]

 59%|█████▉    | 6340/10740 [31:31:39<22:40:10, 18.55s/it]
{'loss': 0.3549, 'learning_rate': 7.585788503639688e-07, 'rewards/chosen': -1.5415434837341309, 'rewards/rejected': -3.5223965644836426, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9808531999588013, 'policy_logps/rejected': -280.20562744140625, 'policy_logps/chosen': -366.5873718261719, 'referece_logps/rejected': -244.98165893554688, 'referece_logps/chosen': -351.1719665527344, 'logits/rejected': -0.5484183430671692, 'logits/chosen': -0.7696006894111633, 'epoch': 3.54}


 59%|█████▉    | 6342/10740 [31:32:18<23:18:03, 19.07s/it]

 59%|█████▉    | 6343/10740 [31:32:39<24:11:18, 19.80s/it]
{'loss': 0.4565, 'learning_rate': 7.577009616616661e-07, 'rewards/chosen': -1.1003409624099731, 'rewards/rejected': -2.869875431060791, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7695344686508179, 'policy_logps/rejected': -387.67291259765625, 'policy_logps/chosen': -419.8692321777344, 'referece_logps/rejected': -358.97418212890625, 'referece_logps/chosen': -408.8658447265625, 'logits/rejected': -0.9657269716262817, 'logits/chosen': -1.0494701862335205, 'epoch': 3.54}


 59%|█████▉    | 6345/10740 [31:33:05<19:40:57, 16.12s/it]
{'loss': 0.3799, 'learning_rate': 7.571158126708973e-07, 'rewards/chosen': -0.964826762676239, 'rewards/rejected': -2.8798954486846924, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9150687456130981, 'policy_logps/rejected': -398.2633056640625, 'policy_logps/chosen': -384.3047180175781, 'referece_logps/rejected': -369.46435546875, 'referece_logps/chosen': -374.6564636230469, 'logits/rejected': 0.004382380284368992, 'logits/chosen': -0.03809012845158577, 'epoch': 3.54}


 59%|█████▉    | 6347/10740 [31:33:43<21:43:23, 17.80s/it]

 59%|█████▉    | 6348/10740 [31:34:02<22:09:31, 18.16s/it]
{'loss': 0.3924, 'learning_rate': 7.562382549331014e-07, 'rewards/chosen': -1.5845777988433838, 'rewards/rejected': -3.2570693492889404, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6724917888641357, 'policy_logps/rejected': -348.53460693359375, 'policy_logps/chosen': -356.1892395019531, 'referece_logps/rejected': -315.9638977050781, 'referece_logps/chosen': -340.3434143066406, 'logits/rejected': -0.868154764175415, 'logits/chosen': -0.8822635412216187, 'epoch': 3.55}


 59%|█████▉    | 6350/10740 [31:34:44<23:46:29, 19.50s/it]
{'loss': 0.3607, 'learning_rate': 7.556533272505376e-07, 'rewards/chosen': -1.4291293621063232, 'rewards/rejected': -2.377911329269409, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9487817287445068, 'policy_logps/rejected': -384.81170654296875, 'policy_logps/chosen': -362.49951171875, 'referece_logps/rejected': -361.0326232910156, 'referece_logps/chosen': -348.208251953125, 'logits/rejected': -1.5532182455062866, 'logits/chosen': -1.6175843477249146, 'epoch': 3.55}

 59%|█████▉    | 6351/10740 [31:35:02<23:23:07, 19.18s/it]


 59%|█████▉    | 6353/10740 [31:35:40<22:54:19, 18.80s/it]
{'loss': 0.5017, 'learning_rate': 7.547761024726483e-07, 'rewards/chosen': -1.8061715364456177, 'rewards/rejected': -3.1519124507904053, 'rewards/accuracies': 0.875, 'rewards/margins': 1.345740556716919, 'policy_logps/rejected': -250.41827392578125, 'policy_logps/chosen': -231.13131713867188, 'referece_logps/rejected': -218.89915466308594, 'referece_logps/chosen': -213.06961059570312, 'logits/rejected': -0.22255703806877136, 'logits/chosen': -0.1277613341808319, 'epoch': 3.55}


 59%|█████▉    | 6355/10740 [31:36:15<21:50:02, 17.93s/it]
{'loss': 0.561, 'learning_rate': 7.541913974283048e-07, 'rewards/chosen': -1.9303027391433716, 'rewards/rejected': -3.1789493560791016, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2486463785171509, 'policy_logps/rejected': -432.89129638671875, 'policy_logps/chosen': -433.97894287109375, 'referece_logps/rejected': -401.101806640625, 'referece_logps/chosen': -414.6759033203125, 'logits/rejected': -0.6970164775848389, 'logits/chosen': -0.7775014638900757, 'epoch': 3.55}


 59%|█████▉    | 6357/10740 [31:36:53<22:28:30, 18.46s/it]
{'loss': 0.404, 'learning_rate': 7.5360678181154e-07, 'rewards/chosen': -1.8644146919250488, 'rewards/rejected': -3.048792600631714, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1843780279159546, 'policy_logps/rejected': -397.5816345214844, 'policy_logps/chosen': -351.3201904296875, 'referece_logps/rejected': -367.09368896484375, 'referece_logps/chosen': -332.6760559082031, 'logits/rejected': -0.2531520128250122, 'logits/chosen': -0.2663559317588806, 'epoch': 3.55}


 59%|█████▉    | 6359/10740 [31:37:23<19:56:18, 16.38s/it]
{'loss': 0.5051, 'learning_rate': 7.53022255835043e-07, 'rewards/chosen': -1.2081035375595093, 'rewards/rejected': -3.0718014240264893, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8636976480484009, 'policy_logps/rejected': -345.07940673828125, 'policy_logps/chosen': -339.2979431152344, 'referece_logps/rejected': -314.36138916015625, 'referece_logps/chosen': -327.2169189453125, 'logits/rejected': -0.9358192086219788, 'logits/chosen': -1.0139751434326172, 'epoch': 3.55}

 59%|█████▉    | 6360/10740 [31:37:34<17:53:06, 14.70s/it]

 59%|█████▉    | 6361/10740 [31:37:50<18:19:42, 15.07s/it]

 59%|█████▉    | 6362/10740 [31:38:01<16:42:07, 13.73s/it]


 59%|█████▉    | 6364/10740 [31:38:35<18:37:19, 15.32s/it]

 59%|█████▉    | 6365/10740 [31:38:54<19:47:42, 16.29s/it]

 59%|█████▉    | 6366/10740 [31:39:07<18:43:45, 15.42s/it]
{'loss': 0.3975, 'learning_rate': 7.50977123904342e-07, 'rewards/chosen': -1.5655461549758911, 'rewards/rejected': -2.9449265003204346, 'rewards/accuracies': 0.875, 'rewards/margins': 1.379380464553833, 'policy_logps/rejected': -323.55035400390625, 'policy_logps/chosen': -240.24595642089844, 'referece_logps/rejected': -294.1011047363281, 'referece_logps/chosen': -224.5904998779297, 'logits/rejected': -1.2147102355957031, 'logits/chosen': -1.1266553401947021, 'epoch': 3.56}


 59%|█████▉    | 6368/10740 [31:39:44<20:45:02, 17.09s/it]

 59%|█████▉    | 6369/10740 [31:40:04<21:41:43, 17.87s/it]

 59%|█████▉    | 6370/10740 [31:40:19<20:49:23, 17.15s/it]
{'loss': 0.2976, 'learning_rate': 7.498089747855272e-07, 'rewards/chosen': -1.1611193418502808, 'rewards/rejected': -3.4023613929748535, 'rewards/accuracies': 1.0, 'rewards/margins': 2.241241693496704, 'policy_logps/rejected': -304.8616638183594, 'policy_logps/chosen': -275.39495849609375, 'referece_logps/rejected': -270.83807373046875, 'referece_logps/chosen': -263.78375244140625, 'logits/rejected': -0.7287511825561523, 'logits/chosen': -0.6583080887794495, 'epoch': 3.56}


 59%|█████▉    | 6372/10740 [31:40:54<20:56:10, 17.26s/it]

 59%|█████▉    | 6373/10740 [31:41:11<21:06:57, 17.41s/it]

 59%|█████▉    | 6374/10740 [31:41:29<21:09:30, 17.45s/it]

 59%|█████▉    | 6375/10740 [31:41:42<19:31:24, 16.10s/it]
{'loss': 0.3769, 'learning_rate': 7.483493005845761e-07, 'rewards/chosen': -2.262186050415039, 'rewards/rejected': -4.512243747711182, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2500576972961426, 'policy_logps/rejected': -322.98626708984375, 'policy_logps/chosen': -380.2821044921875, 'referece_logps/rejected': -277.86383056640625, 'referece_logps/chosen': -357.66021728515625, 'logits/rejected': -0.7290389537811279, 'logits/chosen': -0.5906455516815186, 'epoch': 3.56}

 59%|█████▉    | 6376/10740 [31:41:54<18:07:41, 14.95s/it]


 59%|█████▉    | 6378/10740 [31:42:25<18:10:42, 15.00s/it]
{'loss': 0.4254, 'learning_rate': 7.474737705105709e-07, 'rewards/chosen': -1.2694454193115234, 'rewards/rejected': -2.3233258724212646, 'rewards/accuracies': 1.0, 'rewards/margins': 1.053880214691162, 'policy_logps/rejected': -291.4044189453125, 'policy_logps/chosen': -321.468994140625, 'referece_logps/rejected': -268.1711730957031, 'referece_logps/chosen': -308.7745361328125, 'logits/rejected': -1.193148136138916, 'logits/chosen': -1.2752708196640015, 'epoch': 3.56}

 59%|█████▉    | 6379/10740 [31:42:42<18:52:36, 15.58s/it]

 59%|█████▉    | 6380/10740 [31:42:57<18:29:30, 15.27s/it]

 59%|█████▉    | 6381/10740 [31:43:17<20:08:08, 16.63s/it]


 59%|█████▉    | 6383/10740 [31:43:50<20:29:47, 16.94s/it]

 59%|█████▉    | 6384/10740 [31:44:10<21:23:48, 17.68s/it]
{'loss': 0.4258, 'learning_rate': 7.457233312117954e-07, 'rewards/chosen': -1.5864604711532593, 'rewards/rejected': -3.5415256023406982, 'rewards/accuracies': 0.875, 'rewards/margins': 1.955065369606018, 'policy_logps/rejected': -408.3002624511719, 'policy_logps/chosen': -358.1863708496094, 'referece_logps/rejected': -372.8849792480469, 'referece_logps/chosen': -342.3217468261719, 'logits/rejected': 0.3228840231895447, 'logits/chosen': 0.18840110301971436, 'epoch': 3.57}


 59%|█████▉    | 6386/10740 [31:44:49<22:38:13, 18.72s/it]
{'loss': 0.3559, 'learning_rate': 7.451400361792249e-07, 'rewards/chosen': -1.949973702430725, 'rewards/rejected': -4.45404577255249, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5040721893310547, 'policy_logps/rejected': -350.0740966796875, 'policy_logps/chosen': -348.7857971191406, 'referece_logps/rejected': -305.53363037109375, 'referece_logps/chosen': -329.2860412597656, 'logits/rejected': -0.3177473545074463, 'logits/chosen': -0.3707917630672455, 'epoch': 3.57}


 59%|█████▉    | 6388/10740 [31:45:19<20:01:33, 16.57s/it]

 59%|█████▉    | 6389/10740 [31:45:33<19:06:09, 15.81s/it]

 59%|█████▉    | 6390/10740 [31:45:44<17:16:09, 14.29s/it]
{'loss': 0.3985, 'learning_rate': 7.43973724487917e-07, 'rewards/chosen': -1.5007160902023315, 'rewards/rejected': -2.014575958251953, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5138600468635559, 'policy_logps/rejected': -433.2304992675781, 'policy_logps/chosen': -351.1485290527344, 'referece_logps/rejected': -413.084716796875, 'referece_logps/chosen': -336.14141845703125, 'logits/rejected': -0.6570891737937927, 'logits/chosen': -0.6661405563354492, 'epoch': 3.57}

 60%|█████▉    | 6391/10740 [31:45:59<17:18:58, 14.33s/it]

 60%|█████▉    | 6392/10740 [31:46:14<17:54:15, 14.82s/it]

 60%|█████▉    | 6393/10740 [31:46:33<19:12:55, 15.91s/it]

 60%|█████▉    | 6394/10740 [31:46:49<19:18:06, 15.99s/it]


 60%|█████▉    | 6396/10740 [31:47:28<21:15:16, 17.61s/it]

 60%|█████▉    | 6397/10740 [31:47:46<21:33:03, 17.86s/it]
{'loss': 0.3919, 'learning_rate': 7.419335765681095e-07, 'rewards/chosen': -1.4431090354919434, 'rewards/rejected': -3.2445950508117676, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8014863729476929, 'policy_logps/rejected': -470.8546142578125, 'policy_logps/chosen': -382.9335021972656, 'referece_logps/rejected': -438.4087219238281, 'referece_logps/chosen': -368.50244140625, 'logits/rejected': -0.5366779565811157, 'logits/chosen': -0.4466973841190338, 'epoch': 3.57}


 60%|█████▉    | 6399/10740 [31:48:24<21:59:50, 18.24s/it]
{'loss': 0.4085, 'learning_rate': 7.413508880108472e-07, 'rewards/chosen': -2.2382681369781494, 'rewards/rejected': -2.6712594032287598, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4329911172389984, 'policy_logps/rejected': -356.8116760253906, 'policy_logps/chosen': -411.4037170410156, 'referece_logps/rejected': -330.09912109375, 'referece_logps/chosen': -389.02105712890625, 'logits/rejected': -0.4573572278022766, 'logits/chosen': -0.4270665645599365, 'epoch': 3.57}

 60%|█████▉    | 6400/10740 [31:48:47<23:42:10, 19.66s/it]

 60%|█████▉    | 6401/10740 [31:49:03<22:26:48, 18.62s/it]


 60%|█████▉    | 6403/10740 [31:49:40<22:19:23, 18.53s/it]

 60%|█████▉    | 6404/10740 [31:49:59<22:19:02, 18.53s/it]

 60%|█████▉    | 6405/10740 [31:50:18<22:47:30, 18.93s/it]
{'loss': 0.3876, 'learning_rate': 7.396033877813326e-07, 'rewards/chosen': -2.2386999130249023, 'rewards/rejected': -4.5252180099487305, 'rewards/accuracies': 1.0, 'rewards/margins': 2.286517858505249, 'policy_logps/rejected': -567.5491943359375, 'policy_logps/chosen': -299.0156555175781, 'referece_logps/rejected': -522.2969970703125, 'referece_logps/chosen': -276.6286926269531, 'logits/rejected': -0.842092752456665, 'logits/chosen': -0.9166796207427979, 'epoch': 3.58}


 60%|█████▉    | 6407/10740 [31:50:56<23:01:31, 19.13s/it]

 60%|█████▉    | 6408/10740 [31:51:16<23:12:17, 19.28s/it]

 60%|█████▉    | 6409/10740 [31:51:36<23:31:23, 19.55s/it]
{'loss': 0.4843, 'learning_rate': 7.384388609493226e-07, 'rewards/chosen': -3.2543022632598877, 'rewards/rejected': -4.178981304168701, 'rewards/accuracies': 0.5, 'rewards/margins': 0.9246791005134583, 'policy_logps/rejected': -361.7315979003906, 'policy_logps/chosen': -386.5263671875, 'referece_logps/rejected': -319.9417724609375, 'referece_logps/chosen': -353.98333740234375, 'logits/rejected': -1.111474633216858, 'logits/chosen': -1.1757898330688477, 'epoch': 3.58}


 60%|█████▉    | 6411/10740 [31:52:12<22:32:43, 18.75s/it]

 60%|█████▉    | 6412/10740 [31:52:26<20:47:02, 17.29s/it]

 60%|█████▉    | 6413/10740 [31:52:42<20:07:04, 16.74s/it]
{'loss': 0.3916, 'learning_rate': 7.372747147513025e-07, 'rewards/chosen': -1.5274732112884521, 'rewards/rejected': -4.091670513153076, 'rewards/accuracies': 1.0, 'rewards/margins': 2.564197540283203, 'policy_logps/rejected': -340.555419921875, 'policy_logps/chosen': -305.464111328125, 'referece_logps/rejected': -299.6387023925781, 'referece_logps/chosen': -290.18939208984375, 'logits/rejected': -0.3907703757286072, 'logits/chosen': -0.5628207325935364, 'epoch': 3.58}

 60%|█████▉    | 6414/10740 [31:52:59<20:21:40, 16.94s/it]

 60%|█████▉    | 6415/10740 [31:53:17<20:51:01, 17.36s/it]

 60%|█████▉    | 6416/10740 [31:53:37<21:36:30, 17.99s/it]

 60%|█████▉    | 6417/10740 [31:53:54<21:08:37, 17.61s/it]

 60%|█████▉    | 6418/10740 [31:54:13<21:53:46, 18.24s/it]


 60%|█████▉    | 6420/10740 [31:54:44<19:38:00, 16.36s/it]
{'loss': 0.3239, 'learning_rate': 7.352383799005592e-07, 'rewards/chosen': -0.906498908996582, 'rewards/rejected': -3.18939471244812, 'rewards/accuracies': 0.875, 'rewards/margins': 2.282896041870117, 'policy_logps/rejected': -487.95977783203125, 'policy_logps/chosen': -410.0547790527344, 'referece_logps/rejected': -456.0658264160156, 'referece_logps/chosen': -400.9898376464844, 'logits/rejected': -0.6720123291015625, 'logits/chosen': -0.7320161461830139, 'epoch': 3.59}

 60%|█████▉    | 6421/10740 [31:55:06<21:32:12, 17.95s/it]


 60%|█████▉    | 6423/10740 [31:55:47<23:13:38, 19.37s/it]

 60%|█████▉    | 6424/10740 [31:56:02<21:47:29, 18.18s/it]

 60%|█████▉    | 6425/10740 [31:56:23<22:34:39, 18.84s/it]
{'loss': 0.3797, 'learning_rate': 7.337845768995056e-07, 'rewards/chosen': -2.177459716796875, 'rewards/rejected': -2.874858856201172, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6973993182182312, 'policy_logps/rejected': -532.9586791992188, 'policy_logps/chosen': -491.69940185546875, 'referece_logps/rejected': -504.21014404296875, 'referece_logps/chosen': -469.9248046875, 'logits/rejected': 0.3638467490673065, 'logits/chosen': 0.06592437624931335, 'epoch': 3.59}


 60%|█████▉    | 6427/10740 [31:57:03<23:22:57, 19.52s/it]
{'loss': 0.319, 'learning_rate': 7.33203225004601e-07, 'rewards/chosen': -1.3143430948257446, 'rewards/rejected': -3.1409878730773926, 'rewards/accuracies': 1.0, 'rewards/margins': 1.826644778251648, 'policy_logps/rejected': -362.90673828125, 'policy_logps/chosen': -318.2581787109375, 'referece_logps/rejected': -331.49688720703125, 'referece_logps/chosen': -305.11474609375, 'logits/rejected': -0.593662440776825, 'logits/chosen': -0.625816822052002, 'epoch': 3.59}

 60%|█████▉    | 6428/10740 [31:57:18<21:51:15, 18.25s/it]

 60%|█████▉    | 6429/10740 [31:57:40<23:08:31, 19.33s/it]


 60%|█████▉    | 6431/10740 [31:58:19<23:16:43, 19.45s/it]

 60%|█████▉    | 6432/10740 [31:58:38<23:11:11, 19.38s/it]

 60%|█████▉    | 6433/10740 [31:58:52<21:20:27, 17.84s/it]
{'loss': 0.3703, 'learning_rate': 7.314597525454068e-07, 'rewards/chosen': -1.7123360633850098, 'rewards/rejected': -3.29771089553833, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5853748321533203, 'policy_logps/rejected': -272.9749450683594, 'policy_logps/chosen': -252.13487243652344, 'referece_logps/rejected': -239.99783325195312, 'referece_logps/chosen': -235.01150512695312, 'logits/rejected': -1.3208895921707153, 'logits/chosen': -1.2058494091033936, 'epoch': 3.59}

 60%|█████▉    | 6434/10740 [31:59:12<22:00:40, 18.40s/it]


 60%|█████▉    | 6436/10740 [31:59:51<22:19:48, 18.68s/it]

 60%|█████▉    | 6437/10740 [32:00:11<22:43:33, 19.01s/it]
{'loss': 0.4101, 'learning_rate': 7.302979257080966e-07, 'rewards/chosen': -2.1841788291931152, 'rewards/rejected': -3.808701992034912, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6245230436325073, 'policy_logps/rejected': -499.7958068847656, 'policy_logps/chosen': -436.0679931640625, 'referece_logps/rejected': -461.70880126953125, 'referece_logps/chosen': -414.2261657714844, 'logits/rejected': -0.41837260127067566, 'logits/chosen': -0.4743279218673706, 'epoch': 3.6}


 60%|█████▉    | 6439/10740 [32:00:49<22:41:26, 18.99s/it]
{'loss': 0.4279, 'learning_rate': 7.297171593641664e-07, 'rewards/chosen': -2.1360340118408203, 'rewards/rejected': -3.257046937942505, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1210129261016846, 'policy_logps/rejected': -490.16400146484375, 'policy_logps/chosen': -481.2857666015625, 'referece_logps/rejected': -457.5935363769531, 'referece_logps/chosen': -459.9254150390625, 'logits/rejected': -0.32487207651138306, 'logits/chosen': -0.27737534046173096, 'epoch': 3.6}

 60%|█████▉    | 6440/10740 [32:01:06<21:45:56, 18.22s/it]

 60%|█████▉    | 6441/10740 [32:01:21<20:32:08, 17.20s/it]

 60%|█████▉    | 6442/10740 [32:01:38<20:47:41, 17.42s/it]

 60%|█████▉    | 6443/10740 [32:01:56<20:54:38, 17.52s/it]

 60%|██████    | 6444/10740 [32:02:12<20:23:37, 17.09s/it]


 60%|██████    | 6446/10740 [32:02:47<20:55:39, 17.55s/it]

 60%|██████    | 6447/10740 [32:03:05<21:05:22, 17.69s/it]
{'loss': 0.4712, 'learning_rate': 7.273950794162593e-07, 'rewards/chosen': -1.8155590295791626, 'rewards/rejected': -2.6532034873962402, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8376441597938538, 'policy_logps/rejected': -654.9095458984375, 'policy_logps/chosen': -390.654541015625, 'referece_logps/rejected': -628.3775634765625, 'referece_logps/chosen': -372.49896240234375, 'logits/rejected': -0.657051682472229, 'logits/chosen': -0.6003760099411011, 'epoch': 3.6}

 60%|██████    | 6448/10740 [32:03:21<20:16:21, 17.00s/it]

 60%|██████    | 6449/10740 [32:03:39<20:38:36, 17.32s/it]

 60%|██████    | 6450/10740 [32:03:59<21:32:35, 18.08s/it]

 60%|██████    | 6451/10740 [32:04:19<22:11:49, 18.63s/it]

 60%|██████    | 6452/10740 [32:04:39<22:42:48, 19.07s/it]

 60%|██████    | 6453/10740 [32:04:54<21:14:13, 17.83s/it]

 60%|██████    | 6454/10740 [32:05:16<22:43:42, 19.09s/it]

 60%|██████    | 6455/10740 [32:05:30<21:03:28, 17.69s/it]

 60%|██████    | 6456/10740 [32:05:46<20:22:31, 17.12s/it]

 60%|██████    | 6457/10740 [32:06:08<22:11:00, 18.65s/it]

 60%|██████    | 6458/10740 [32:06:26<21:58:02, 18.47s/it]

 60%|██████    | 6459/10740 [32:06:46<22:34:56, 18.99s/it]

 60%|██████    | 6460/10740 [32:07:01<20:58:21, 17.64s/it]

 60%|██████    | 6461/10740 [32:07:20<21:28:11, 18.06s/it]

 60%|██████    | 6462/10740 [32:07:40<22:12:48, 18.69s/it]

 60%|██████    | 6463/10740 [32:07:58<21:53:34, 18.43s/it]

 60%|██████    | 6464/10740 [32:08:18<22:28:34, 18.92s/it]

 60%|██████    | 6465/10740 [32:08:40<23:35:11, 19.86s/it]

 60%|██████    | 6466/10740 [32:08:59<23:06:38, 19.47s/it]


 60%|██████    | 6468/10740 [32:09:37<23:00:14, 19.39s/it]
{'loss': 0.4632, 'learning_rate': 7.213072041556494e-07, 'rewards/chosen': -1.7721127271652222, 'rewards/rejected': -5.098343849182129, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3262314796447754, 'policy_logps/rejected': -367.8338623046875, 'policy_logps/chosen': -385.7735595703125, 'referece_logps/rejected': -316.8504638671875, 'referece_logps/chosen': -368.05242919921875, 'logits/rejected': -0.390518456697464, 'logits/chosen': -0.4771924614906311, 'epoch': 3.61}


 60%|██████    | 6470/10740 [32:10:16<22:55:35, 19.33s/it]
{'loss': 0.4286, 'learning_rate': 7.207279856719642e-07, 'rewards/chosen': -1.348302960395813, 'rewards/rejected': -1.9872781038284302, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6389750838279724, 'policy_logps/rejected': -378.8950500488281, 'policy_logps/chosen': -328.73663330078125, 'referece_logps/rejected': -359.02227783203125, 'referece_logps/chosen': -315.2535705566406, 'logits/rejected': -0.5446733832359314, 'logits/chosen': -0.5727651715278625, 'epoch': 3.61}

 60%|██████    | 6471/10740 [32:10:38<23:55:40, 20.18s/it]

 60%|██████    | 6472/10740 [32:10:59<24:16:17, 20.47s/it]

 60%|██████    | 6473/10740 [32:11:19<23:56:09, 20.19s/it]

 60%|██████    | 6474/10740 [32:11:35<22:32:55, 19.03s/it]

 60%|██████    | 6475/10740 [32:11:55<22:47:34, 19.24s/it]

 60%|██████    | 6476/10740 [32:12:07<20:08:28, 17.00s/it]

 60%|██████    | 6477/10740 [32:12:26<21:07:04, 17.83s/it]

 60%|██████    | 6478/10740 [32:12:39<19:14:04, 16.25s/it]


 60%|██████    | 6480/10740 [32:13:16<20:31:13, 17.34s/it]

 60%|██████    | 6481/10740 [32:13:28<18:35:11, 15.71s/it]
{'loss': 0.4599, 'learning_rate': 7.17544105793801e-07, 'rewards/chosen': -1.7918118238449097, 'rewards/rejected': -2.6473729610443115, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8555616140365601, 'policy_logps/rejected': -238.3353271484375, 'policy_logps/chosen': -222.43557739257812, 'referece_logps/rejected': -211.86158752441406, 'referece_logps/chosen': -204.5174560546875, 'logits/rejected': -1.3464603424072266, 'logits/chosen': -1.488438367843628, 'epoch': 3.62}


 60%|██████    | 6483/10740 [32:14:02<19:01:20, 16.09s/it]
{'loss': 0.4158, 'learning_rate': 7.169655514875003e-07, 'rewards/chosen': -1.4634560346603394, 'rewards/rejected': -3.0592856407165527, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5958298444747925, 'policy_logps/rejected': -342.9723815917969, 'policy_logps/chosen': -296.32720947265625, 'referece_logps/rejected': -312.3795166015625, 'referece_logps/chosen': -281.692626953125, 'logits/rejected': -0.6801317930221558, 'logits/chosen': -0.7022086381912231, 'epoch': 3.62}

 60%|██████    | 6484/10740 [32:14:21<20:03:18, 16.96s/it]


 60%|██████    | 6486/10740 [32:14:54<19:32:27, 16.54s/it]
{'loss': 0.4654, 'learning_rate': 7.160979131638905e-07, 'rewards/chosen': -1.9530088901519775, 'rewards/rejected': -3.688018321990967, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7350094318389893, 'policy_logps/rejected': -365.73260498046875, 'policy_logps/chosen': -364.2115783691406, 'referece_logps/rejected': -328.8523864746094, 'referece_logps/chosen': -344.6815185546875, 'logits/rejected': 0.008749246597290039, 'logits/chosen': 0.03212171792984009, 'epoch': 3.62}

 60%|██████    | 6487/10740 [32:15:14<20:39:21, 17.48s/it]

 60%|██████    | 6488/10740 [32:15:30<20:28:09, 17.33s/it]

 60%|██████    | 6489/10740 [32:15:41<18:05:01, 15.31s/it]

 60%|██████    | 6490/10740 [32:15:57<18:09:39, 15.38s/it]

 60%|██████    | 6491/10740 [32:16:14<18:42:56, 15.86s/it]

 60%|██████    | 6492/10740 [32:16:27<17:40:01, 14.97s/it]


 60%|██████    | 6494/10740 [32:17:04<19:56:08, 16.90s/it]
{'loss': 0.3293, 'learning_rate': 7.137853490459354e-07, 'rewards/chosen': -1.9646111726760864, 'rewards/rejected': -4.125474452972412, 'rewards/accuracies': 1.0, 'rewards/margins': 2.160863161087036, 'policy_logps/rejected': -466.5465393066406, 'policy_logps/chosen': -416.61553955078125, 'referece_logps/rejected': -425.29180908203125, 'referece_logps/chosen': -396.96942138671875, 'logits/rejected': -0.2960800230503082, 'logits/chosen': -0.2157992720603943, 'epoch': 3.63}

 60%|██████    | 6495/10740 [32:17:24<20:55:53, 17.75s/it]

 60%|██████    | 6496/10740 [32:17:45<22:15:17, 18.88s/it]

 60%|██████    | 6497/10740 [32:18:05<22:32:49, 19.13s/it]

 61%|██████    | 6498/10740 [32:18:23<22:11:31, 18.83s/it]

 61%|██████    | 6499/10740 [32:18:41<21:51:24, 18.55s/it]

 61%|██████    | 6500/10740 [32:19:01<22:10:45, 18.83s/it]

 61%|██████    | 6501/10740 [32:19:36<27:56:07, 23.72s/it]


 61%|██████    | 6503/10740 [32:20:15<25:20:43, 21.53s/it]
{'loss': 0.4184, 'learning_rate': 7.111857065245639e-07, 'rewards/chosen': -2.2753190994262695, 'rewards/rejected': -3.2662816047668457, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9909623861312866, 'policy_logps/rejected': -583.308837890625, 'policy_logps/chosen': -504.6064758300781, 'referece_logps/rejected': -550.64599609375, 'referece_logps/chosen': -481.8532409667969, 'logits/rejected': -0.8763682842254639, 'logits/chosen': -0.8802570104598999, 'epoch': 3.63}

 61%|██████    | 6504/10740 [32:20:33<24:23:57, 20.74s/it]

 61%|██████    | 6505/10740 [32:20:49<22:31:58, 19.15s/it]

 61%|██████    | 6506/10740 [32:21:10<23:11:27, 19.72s/it]

 61%|██████    | 6507/10740 [32:21:29<23:08:53, 19.69s/it]


 61%|██████    | 6509/10740 [32:22:06<22:38:42, 19.27s/it]
{'loss': 0.3923, 'learning_rate': 7.094537924045977e-07, 'rewards/chosen': -1.1435941457748413, 'rewards/rejected': -2.895103693008423, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7515093088150024, 'policy_logps/rejected': -335.66131591796875, 'policy_logps/chosen': -471.1298828125, 'referece_logps/rejected': -306.71026611328125, 'referece_logps/chosen': -459.69390869140625, 'logits/rejected': -0.2399328649044037, 'logits/chosen': -0.2591065764427185, 'epoch': 3.64}

 61%|██████    | 6510/10740 [32:22:24<22:06:16, 18.81s/it]


 61%|██████    | 6512/10740 [32:23:02<22:01:01, 18.75s/it]

 61%|██████    | 6513/10740 [32:23:21<21:53:17, 18.64s/it]
{'loss': 0.277, 'learning_rate': 7.082997111590285e-07, 'rewards/chosen': -1.7556320428848267, 'rewards/rejected': -4.599665641784668, 'rewards/accuracies': 1.0, 'rewards/margins': 2.844033718109131, 'policy_logps/rejected': -636.057373046875, 'policy_logps/chosen': -629.448486328125, 'referece_logps/rejected': -590.060791015625, 'referece_logps/chosen': -611.8921508789062, 'logits/rejected': -0.1988282948732376, 'logits/chosen': -0.12667272984981537, 'epoch': 3.64}

 61%|██████    | 6514/10740 [32:23:41<22:28:39, 19.15s/it]

 61%|██████    | 6515/10740 [32:24:00<22:32:13, 19.20s/it]

 61%|██████    | 6516/10740 [32:24:16<21:14:19, 18.10s/it]

 61%|██████    | 6517/10740 [32:24:36<21:57:06, 18.71s/it]

 61%|██████    | 6518/10740 [32:24:50<20:17:41, 17.31s/it]

 61%|██████    | 6519/10740 [32:25:13<22:23:52, 19.10s/it]

 61%|██████    | 6520/10740 [32:25:32<22:09:07, 18.90s/it]

 61%|██████    | 6521/10740 [32:25:47<20:59:42, 17.91s/it]

 61%|██████    | 6522/10740 [32:26:07<21:46:42, 18.59s/it]

 61%|██████    | 6523/10740 [32:26:27<22:13:17, 18.97s/it]

 61%|██████    | 6524/10740 [32:26:48<22:39:40, 19.35s/it]

 61%|██████    | 6525/10740 [32:27:07<22:43:05, 19.40s/it]

 61%|██████    | 6526/10740 [32:27:24<21:46:30, 18.60s/it]

 61%|██████    | 6527/10740 [32:27:45<22:47:49, 19.48s/it]


 61%|██████    | 6529/10740 [32:28:19<20:49:21, 17.80s/it]
{'loss': 0.3045, 'learning_rate': 7.03687647898692e-07, 'rewards/chosen': -1.1002955436706543, 'rewards/rejected': -3.2757856845855713, 'rewards/accuracies': 0.875, 'rewards/margins': 2.175490140914917, 'policy_logps/rejected': -359.3698425292969, 'policy_logps/chosen': -448.04925537109375, 'referece_logps/rejected': -326.61199951171875, 'referece_logps/chosen': -437.0462951660156, 'logits/rejected': -0.422465056180954, 'logits/chosen': -0.49684959650039673, 'epoch': 3.65}

 61%|██████    | 6530/10740 [32:28:38<21:10:46, 18.11s/it]

 61%|██████    | 6531/10740 [32:28:51<19:33:26, 16.73s/it]


 61%|██████    | 6533/10740 [32:29:17<17:10:53, 14.70s/it]
{'loss': 0.5348, 'learning_rate': 7.025357059036627e-07, 'rewards/chosen': -0.9834365248680115, 'rewards/rejected': -2.242605686187744, 'rewards/accuracies': 0.75, 'rewards/margins': 1.259169101715088, 'policy_logps/rejected': -299.1647644042969, 'policy_logps/chosen': -297.4638366699219, 'referece_logps/rejected': -276.73870849609375, 'referece_logps/chosen': -287.62945556640625, 'logits/rejected': 0.4539695978164673, 'logits/chosen': 0.458684504032135, 'epoch': 3.65}

 61%|██████    | 6534/10740 [32:29:31<17:07:45, 14.66s/it]

 61%|██████    | 6535/10740 [32:29:47<17:30:48, 14.99s/it]

 61%|██████    | 6536/10740 [32:30:01<16:54:53, 14.48s/it]

 61%|██████    | 6537/10740 [32:30:22<19:15:19, 16.49s/it]

 61%|██████    | 6538/10740 [32:30:40<19:43:15, 16.90s/it]

 61%|██████    | 6539/10740 [32:30:56<19:43:43, 16.91s/it]


 61%|██████    | 6541/10740 [32:31:33<20:29:53, 17.57s/it]
{'loss': 0.4177, 'learning_rate': 7.002331222343163e-07, 'rewards/chosen': -1.5847618579864502, 'rewards/rejected': -3.8891384601593018, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3043768405914307, 'policy_logps/rejected': -454.60186767578125, 'policy_logps/chosen': -468.03106689453125, 'referece_logps/rejected': -415.7104797363281, 'referece_logps/chosen': -452.1834716796875, 'logits/rejected': -0.018437441438436508, 'logits/chosen': 0.06586508452892303, 'epoch': 3.65}

 61%|██████    | 6542/10740 [32:31:47<19:02:37, 16.33s/it]

 61%|██████    | 6543/10740 [32:32:02<18:33:39, 15.92s/it]

 61%|██████    | 6544/10740 [32:32:21<19:36:14, 16.82s/it]

 61%|██████    | 6545/10740 [32:32:34<18:27:39, 15.84s/it]

 61%|██████    | 6546/10740 [32:32:52<19:01:39, 16.33s/it]

 61%|██████    | 6547/10740 [32:33:09<19:29:12, 16.73s/it]

 61%|██████    | 6548/10740 [32:33:29<20:33:16, 17.65s/it]

 61%|██████    | 6549/10740 [32:33:40<18:04:30, 15.53s/it]

 61%|██████    | 6550/10740 [32:33:55<17:56:38, 15.42s/it]

 61%|██████    | 6551/10740 [32:34:12<18:26:16, 15.85s/it]

 61%|██████    | 6552/10740 [32:34:33<20:15:16, 17.41s/it]

 61%|██████    | 6553/10740 [32:34:47<19:16:58, 16.58s/it]

 61%|██████    | 6554/10740 [32:35:01<18:21:54, 15.79s/it]

 61%|██████    | 6555/10740 [32:35:21<19:38:18, 16.89s/it]

 61%|██████    | 6556/10740 [32:35:33<18:05:25, 15.57s/it]

 61%|██████    | 6557/10740 [32:35:53<19:33:16, 16.83s/it]

 61%|██████    | 6558/10740 [32:36:13<20:46:26, 17.88s/it]

 61%|██████    | 6559/10740 [32:36:33<21:21:45, 18.39s/it]

 61%|██████    | 6560/10740 [32:36:49<20:32:03, 17.69s/it]

 61%|██████    | 6561/10740 [32:37:02<18:47:11, 16.18s/it]

 61%|██████    | 6562/10740 [32:37:20<19:41:43, 16.97s/it]

 61%|██████    | 6563/10740 [32:37:40<20:36:58, 17.77s/it]

 61%|██████    | 6564/10740 [32:38:00<21:17:23, 18.35s/it]

 61%|██████    | 6565/10740 [32:38:18<21:11:06, 18.27s/it]

 61%|██████    | 6566/10740 [32:38:33<20:02:28, 17.29s/it]

 61%|██████    | 6567/10740 [32:38:53<20:55:59, 18.06s/it]

 61%|██████    | 6568/10740 [32:39:12<21:30:23, 18.56s/it]

 61%|██████    | 6569/10740 [32:39:32<21:51:06, 18.86s/it]

 61%|██████    | 6570/10740 [32:39:52<22:16:10, 19.23s/it]

 61%|██████    | 6571/10740 [32:40:05<19:53:25, 17.18s/it]

 61%|██████    | 6572/10740 [32:40:22<20:05:43, 17.36s/it]

 61%|██████    | 6573/10740 [32:40:41<20:39:01, 17.84s/it]

 61%|██████    | 6574/10740 [32:41:01<21:12:26, 18.33s/it]

 61%|██████    | 6575/10740 [32:41:16<20:05:19, 17.36s/it]

 61%|██████    | 6576/10740 [32:41:35<20:49:57, 18.01s/it]

 61%|██████    | 6577/10740 [32:41:55<21:23:23, 18.50s/it]

 61%|██████    | 6578/10740 [32:42:11<20:28:37, 17.71s/it]

 61%|██████▏   | 6579/10740 [32:42:30<21:05:58, 18.25s/it]

 61%|██████▏   | 6580/10740 [32:42:45<19:45:27, 17.10s/it]

 61%|██████▏   | 6581/10740 [32:43:04<20:27:49, 17.71s/it]

 61%|██████▏   | 6582/10740 [32:43:27<22:28:59, 19.47s/it]

 61%|██████▏   | 6583/10740 [32:43:46<22:04:59, 19.12s/it]

 61%|██████▏   | 6584/10740 [32:44:00<20:17:40, 17.58s/it]

 61%|██████▏   | 6585/10740 [32:44:17<20:14:44, 17.54s/it]

 61%|██████▏   | 6586/10740 [32:44:31<18:47:24, 16.28s/it]

 61%|██████▏   | 6587/10740 [32:44:45<18:11:53, 15.78s/it]

 61%|██████▏   | 6588/10740 [32:45:03<18:46:23, 16.28s/it]

 61%|██████▏   | 6589/10740 [32:45:18<18:19:30, 15.89s/it]

 61%|██████▏   | 6590/10740 [32:45:36<19:04:40, 16.55s/it]

 61%|██████▏   | 6591/10740 [32:45:54<19:37:25, 17.03s/it]

 61%|██████▏   | 6592/10740 [32:46:16<21:33:12, 18.71s/it]

 61%|██████▏   | 6593/10740 [32:46:35<21:27:32, 18.63s/it]

 61%|██████▏   | 6594/10740 [32:46:55<21:47:06, 18.92s/it]

 61%|██████▏   | 6595/10740 [32:47:13<21:31:18, 18.69s/it]

 61%|██████▏   | 6596/10740 [32:47:33<22:05:39, 19.19s/it]

 61%|██████▏   | 6597/10740 [32:47:53<22:11:20, 19.28s/it]

 61%|██████▏   | 6598/10740 [32:48:03<19:15:20, 16.74s/it]

 61%|██████▏   | 6599/10740 [32:48:23<20:25:08, 17.75s/it]

 61%|██████▏   | 6600/10740 [32:48:40<20:06:04, 17.48s/it]

 61%|██████▏   | 6601/10740 [32:48:55<19:13:19, 16.72s/it]

 61%|██████▏   | 6602/10740 [32:49:14<19:58:03, 17.37s/it]

 61%|██████▏   | 6603/10740 [32:49:36<21:37:07, 18.81s/it]

 61%|██████▏   | 6604/10740 [32:49:57<22:07:27, 19.26s/it]

 61%|██████▏   | 6605/10740 [32:50:16<22:04:11, 19.21s/it]

 62%|██████▏   | 6606/10740 [32:50:34<21:45:28, 18.95s/it]

 62%|██████▏   | 6607/10740 [32:50:54<21:59:05, 19.15s/it]

 62%|██████▏   | 6608/10740 [32:51:09<20:38:52, 17.99s/it]

 62%|██████▏   | 6609/10740 [32:51:29<21:21:02, 18.61s/it]

 62%|██████▏   | 6610/10740 [32:51:50<22:12:03, 19.35s/it]

 62%|██████▏   | 6611/10740 [32:52:08<21:51:05, 19.05s/it]

 62%|██████▏   | 6612/10740 [32:52:28<22:07:52, 19.30s/it]

 62%|██████▏   | 6613/10740 [32:52:48<22:13:07, 19.38s/it]

 62%|██████▏   | 6614/10740 [32:53:00<19:46:03, 17.25s/it]

 62%|██████▏   | 6615/10740 [32:53:13<18:16:06, 15.94s/it]

 62%|██████▏   | 6616/10740 [32:53:34<19:50:19, 17.32s/it]

 62%|██████▏   | 6617/10740 [32:53:48<18:46:21, 16.39s/it]

 62%|██████▏   | 6618/10740 [32:54:08<20:04:59, 17.54s/it]

 62%|██████▏   | 6619/10740 [32:54:25<19:54:45, 17.40s/it]

 62%|██████▏   | 6620/10740 [32:54:42<19:51:01, 17.34s/it]

 62%|██████▏   | 6621/10740 [32:55:03<20:52:44, 18.25s/it]

 62%|██████▏   | 6622/10740 [32:55:23<21:26:26, 18.74s/it]

 62%|██████▏   | 6623/10740 [32:55:44<22:12:59, 19.43s/it]

 62%|██████▏   | 6624/10740 [32:56:06<23:12:27, 20.30s/it]

 62%|██████▏   | 6625/10740 [32:56:26<23:03:10, 20.17s/it]

 62%|██████▏   | 6626/10740 [32:56:44<22:15:55, 19.48s/it]

 62%|██████▏   | 6627/10740 [32:57:03<22:22:15, 19.58s/it]

 62%|██████▏   | 6628/10740 [32:57:22<21:51:32, 19.14s/it]


 62%|██████▏   | 6630/10740 [32:58:01<22:04:03, 19.33s/it]

 62%|██████▏   | 6631/10740 [32:58:21<22:10:21, 19.43s/it]

 62%|██████▏   | 6632/10740 [32:58:43<23:05:44, 20.24s/it]

 62%|██████▏   | 6633/10740 [32:58:54<19:52:04, 17.42s/it]

 62%|██████▏   | 6634/10740 [32:59:11<19:45:15, 17.32s/it]

 62%|██████▏   | 6635/10740 [32:59:27<19:22:19, 16.99s/it]

 62%|██████▏   | 6636/10740 [32:59:49<20:59:50, 18.42s/it]

 62%|██████▏   | 6637/10740 [33:00:10<21:45:48, 19.10s/it]

 62%|██████▏   | 6638/10740 [33:00:31<22:27:19, 19.71s/it]

 62%|██████▏   | 6639/10740 [33:00:51<22:29:48, 19.75s/it]

 62%|██████▏   | 6640/10740 [33:01:08<21:50:08, 19.17s/it]

 62%|██████▏   | 6641/10740 [33:01:29<22:10:44, 19.48s/it]

 62%|██████▏   | 6642/10740 [33:01:45<21:15:49, 18.68s/it]

 62%|██████▏   | 6643/10740 [33:02:03<20:49:31, 18.30s/it]

 62%|██████▏   | 6644/10740 [33:02:24<21:40:11, 19.05s/it]

 62%|██████▏   | 6645/10740 [33:02:43<21:52:51, 19.24s/it]

 62%|██████▏   | 6646/10740 [33:02:59<20:38:57, 18.16s/it]

 62%|██████▏   | 6647/10740 [33:03:19<21:15:12, 18.69s/it]

 62%|██████▏   | 6648/10740 [33:03:34<19:59:35, 17.59s/it]
{'loss': 0.3775, 'learning_rate': 6.696091098858989e-07, 'rewards/chosen': -1.963371753692627, 'rewards/rejected': -3.951284646987915, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9879127740859985, 'policy_logps/rejected': -386.45379638671875, 'policy_logps/chosen': -353.69122314453125, 'referece_logps/rejected': -346.9409484863281, 'referece_logps/chosen': -334.0574951171875, 'logits/rejected': -0.2575953006744385, 'logits/chosen': -0.2812753915786743, 'epoch': 3.71}


 62%|██████▏   | 6650/10740 [33:04:12<20:46:57, 18.29s/it]

 62%|██████▏   | 6651/10740 [33:04:28<20:07:12, 17.71s/it]
{'loss': 0.3934, 'learning_rate': 6.687553024463211e-07, 'rewards/chosen': -1.9087207317352295, 'rewards/rejected': -3.5747764110565186, 'rewards/accuracies': 0.875, 'rewards/margins': 1.666055679321289, 'policy_logps/rejected': -384.1016540527344, 'policy_logps/chosen': -263.0660705566406, 'referece_logps/rejected': -348.3539123535156, 'referece_logps/chosen': -243.97885131835938, 'logits/rejected': -0.36953452229499817, 'logits/chosen': -0.34078094363212585, 'epoch': 3.72}


 62%|██████▏   | 6653/10740 [33:04:53<16:57:35, 14.94s/it]
{'loss': 0.4866, 'learning_rate': 6.681862480810482e-07, 'rewards/chosen': -1.652319312095642, 'rewards/rejected': -2.403583288192749, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7512639760971069, 'policy_logps/rejected': -505.12677001953125, 'policy_logps/chosen': -441.10821533203125, 'referece_logps/rejected': -481.0909423828125, 'referece_logps/chosen': -424.58502197265625, 'logits/rejected': -0.9991836547851562, 'logits/chosen': -1.018633484840393, 'epoch': 3.72}


 62%|██████▏   | 6655/10740 [33:05:27<18:16:43, 16.11s/it]

 62%|██████▏   | 6656/10740 [33:05:45<18:58:56, 16.73s/it]
{'loss': 0.4089, 'learning_rate': 6.673328929423769e-07, 'rewards/chosen': -1.8344752788543701, 'rewards/rejected': -3.7752578258514404, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9407827854156494, 'policy_logps/rejected': -315.3922119140625, 'policy_logps/chosen': -324.66400146484375, 'referece_logps/rejected': -277.6396484375, 'referece_logps/chosen': -306.31927490234375, 'logits/rejected': -1.5235472917556763, 'logits/chosen': -1.5709973573684692, 'epoch': 3.72}

 62%|██████▏   | 6657/10740 [33:05:58<17:40:30, 15.58s/it]


 62%|██████▏   | 6659/10740 [33:06:26<16:33:15, 14.60s/it]

 62%|██████▏   | 6660/10740 [33:06:46<18:18:00, 16.15s/it]

 62%|██████▏   | 6661/10740 [33:07:02<18:17:43, 16.15s/it]

 62%|██████▏   | 6662/10740 [33:07:14<16:36:23, 14.66s/it]

 62%|██████▏   | 6663/10740 [33:07:31<17:38:51, 15.58s/it]

 62%|██████▏   | 6664/10740 [33:07:51<19:07:38, 16.89s/it]

 62%|██████▏   | 6665/10740 [33:08:11<20:01:14, 17.69s/it]

 62%|██████▏   | 6666/10740 [33:08:32<21:18:40, 18.83s/it]

 62%|██████▏   | 6667/10740 [33:08:49<20:29:13, 18.11s/it]

 62%|██████▏   | 6668/10740 [33:09:05<19:48:33, 17.51s/it]

 62%|██████▏   | 6669/10740 [33:09:23<19:53:52, 17.60s/it]

 62%|██████▏   | 6670/10740 [33:09:39<19:25:44, 17.19s/it]

 62%|██████▏   | 6671/10740 [33:09:59<20:31:57, 18.17s/it]

 62%|██████▏   | 6672/10740 [33:10:16<19:52:46, 17.59s/it]
{'loss': 0.3858, 'learning_rate': 6.627862816137737e-07, 'rewards/chosen': -1.5473753213882446, 'rewards/rejected': -2.7665462493896484, 'rewards/accuracies': 0.875, 'rewards/margins': 1.219171166419983, 'policy_logps/rejected': -489.73651123046875, 'policy_logps/chosen': -402.225830078125, 'referece_logps/rejected': -462.071044921875, 'referece_logps/chosen': -386.7520751953125, 'logits/rejected': -0.37312716245651245, 'logits/chosen': -0.3752785325050354, 'epoch': 3.73}


 62%|██████▏   | 6674/10740 [33:10:45<18:27:06, 16.34s/it]

 62%|██████▏   | 6675/10740 [33:11:05<19:38:55, 17.40s/it]
{'loss': 0.4787, 'learning_rate': 6.619346629058959e-07, 'rewards/chosen': -2.004319190979004, 'rewards/rejected': -3.943514347076416, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9391953945159912, 'policy_logps/rejected': -375.2550048828125, 'policy_logps/chosen': -379.0440979003906, 'referece_logps/rejected': -335.81982421875, 'referece_logps/chosen': -359.0008544921875, 'logits/rejected': -0.8564215302467346, 'logits/chosen': -0.9262951016426086, 'epoch': 3.73}


 62%|██████▏   | 6677/10740 [33:11:41<20:03:36, 17.77s/it]

 62%|██████▏   | 6678/10740 [33:12:01<20:51:20, 18.48s/it]

 62%|██████▏   | 6679/10740 [33:12:21<21:28:18, 19.03s/it]
{'loss': 0.4472, 'learning_rate': 6.607996018860225e-07, 'rewards/chosen': -1.5511637926101685, 'rewards/rejected': -3.4837253093719482, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9325616359710693, 'policy_logps/rejected': -456.6292419433594, 'policy_logps/chosen': -366.9395446777344, 'referece_logps/rejected': -421.7919921875, 'referece_logps/chosen': -351.42791748046875, 'logits/rejected': -0.6302556395530701, 'logits/chosen': -0.7732855677604675, 'epoch': 3.73}


 62%|██████▏   | 6681/10740 [33:12:52<19:16:41, 17.10s/it]

 62%|██████▏   | 6682/10740 [33:13:11<20:06:35, 17.84s/it]

 62%|██████▏   | 6683/10740 [33:13:27<19:16:59, 17.11s/it]
{'loss': 0.3907, 'learning_rate': 6.59665034483822e-07, 'rewards/chosen': -2.026287794113159, 'rewards/rejected': -3.508991003036499, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4827033281326294, 'policy_logps/rejected': -297.1679382324219, 'policy_logps/chosen': -303.3023681640625, 'referece_logps/rejected': -262.0780029296875, 'referece_logps/chosen': -283.03948974609375, 'logits/rejected': -0.9378588795661926, 'logits/chosen': -0.9977384805679321, 'epoch': 3.73}


 62%|██████▏   | 6685/10740 [33:14:02<19:28:42, 17.29s/it]

 62%|██████▏   | 6686/10740 [33:14:15<17:57:30, 15.95s/it]
{'loss': 0.3384, 'learning_rate': 6.588144338620167e-07, 'rewards/chosen': -2.1942741870880127, 'rewards/rejected': -3.799882650375366, 'rewards/accuracies': 1.0, 'rewards/margins': 1.605608344078064, 'policy_logps/rejected': -390.1615295410156, 'policy_logps/chosen': -455.792236328125, 'referece_logps/rejected': -352.1627197265625, 'referece_logps/chosen': -433.8494873046875, 'logits/rejected': -1.1064821481704712, 'logits/chosen': -1.1453540325164795, 'epoch': 3.74}

 62%|██████▏   | 6687/10740 [33:14:34<19:15:18, 17.10s/it]

 62%|██████▏   | 6688/10740 [33:14:46<17:27:05, 15.50s/it]


 62%|██████▏   | 6690/10740 [33:15:24<19:13:59, 17.10s/it]

 62%|██████▏   | 6691/10740 [33:15:45<20:43:14, 18.42s/it]
{'loss': 0.3412, 'learning_rate': 6.57397387135987e-07, 'rewards/chosen': -1.7040867805480957, 'rewards/rejected': -3.439042568206787, 'rewards/accuracies': 0.875, 'rewards/margins': 1.73495614528656, 'policy_logps/rejected': -374.0423278808594, 'policy_logps/chosen': -325.0238342285156, 'referece_logps/rejected': -339.65191650390625, 'referece_logps/chosen': -307.98297119140625, 'logits/rejected': -0.6191450357437134, 'logits/chosen': -0.6425367593765259, 'epoch': 3.74}


 62%|██████▏   | 6693/10740 [33:16:20<20:08:05, 17.91s/it]

 62%|██████▏   | 6694/10740 [33:16:35<19:25:26, 17.28s/it]

 62%|██████▏   | 6695/10740 [33:16:50<18:31:48, 16.49s/it]

 62%|██████▏   | 6696/10740 [33:17:09<19:28:31, 17.34s/it]

 62%|██████▏   | 6697/10740 [33:17:24<18:38:56, 16.61s/it]
{'loss': 0.3521, 'learning_rate': 6.556979596460628e-07, 'rewards/chosen': -1.7626341581344604, 'rewards/rejected': -3.1661808490753174, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4035464525222778, 'policy_logps/rejected': -388.162841796875, 'policy_logps/chosen': -480.04534912109375, 'referece_logps/rejected': -356.50103759765625, 'referece_logps/chosen': -462.41900634765625, 'logits/rejected': -1.0846785306930542, 'logits/chosen': -1.0306119918823242, 'epoch': 3.74}


 62%|██████▏   | 6699/10740 [33:18:02<20:15:05, 18.04s/it]

 62%|██████▏   | 6700/10740 [33:18:18<19:30:09, 17.38s/it]
{'loss': 0.4754, 'learning_rate': 6.548486683074085e-07, 'rewards/chosen': -1.4718700647354126, 'rewards/rejected': -2.970889091491699, 'rewards/accuracies': 0.875, 'rewards/margins': 1.499018907546997, 'policy_logps/rejected': -658.8243408203125, 'policy_logps/chosen': -463.638671875, 'referece_logps/rejected': -629.1154174804688, 'referece_logps/chosen': -448.91998291015625, 'logits/rejected': -0.6968685984611511, 'logits/chosen': -0.8014419078826904, 'epoch': 3.74}


 62%|██████▏   | 6702/10740 [33:18:52<19:29:10, 17.37s/it]

 62%|██████▏   | 6703/10740 [33:19:08<19:08:11, 17.06s/it]

 62%|██████▏   | 6704/10740 [33:19:26<19:27:23, 17.35s/it]

 62%|██████▏   | 6705/10740 [33:19:49<21:20:54, 19.05s/it]

 62%|██████▏   | 6706/10740 [33:20:05<20:20:38, 18.16s/it]

 62%|██████▏   | 6707/10740 [33:20:24<20:29:45, 18.30s/it]

 62%|██████▏   | 6708/10740 [33:20:44<20:58:40, 18.73s/it]

 62%|██████▏   | 6709/10740 [33:21:04<21:31:40, 19.23s/it]

 62%|██████▏   | 6710/10740 [33:21:20<20:30:42, 18.32s/it]

 62%|██████▏   | 6711/10740 [33:21:40<20:55:42, 18.70s/it]

 62%|██████▏   | 6712/10740 [33:21:58<20:34:03, 18.38s/it]

 63%|██████▎   | 6713/10740 [33:22:16<20:42:10, 18.51s/it]

 63%|██████▎   | 6714/10740 [33:22:37<21:28:33, 19.20s/it]
{'loss': 0.3068, 'learning_rate': 6.508890556462809e-07, 'rewards/chosen': -1.2277323007583618, 'rewards/rejected': -3.7186102867126465, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4908783435821533, 'policy_logps/rejected': -497.5454406738281, 'policy_logps/chosen': -396.77801513671875, 'referece_logps/rejected': -460.3593444824219, 'referece_logps/chosen': -384.5007629394531, 'logits/rejected': -0.04666289687156677, 'logits/chosen': -0.0127716064453125, 'epoch': 3.75}


 63%|██████▎   | 6716/10740 [33:23:10<19:21:04, 17.31s/it]

 63%|██████▎   | 6717/10740 [33:23:24<18:11:13, 16.27s/it]
{'loss': 0.2771, 'learning_rate': 6.500413745032521e-07, 'rewards/chosen': -1.2629295587539673, 'rewards/rejected': -3.2872672080993652, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0243377685546875, 'policy_logps/rejected': -462.3536682128906, 'policy_logps/chosen': -496.2120361328125, 'referece_logps/rejected': -429.48101806640625, 'referece_logps/chosen': -483.5827331542969, 'logits/rejected': -0.6346290111541748, 'logits/chosen': -0.6895838975906372, 'epoch': 3.75}


 63%|██████▎   | 6719/10740 [33:24:08<21:31:44, 19.27s/it]
{'loss': 0.415, 'learning_rate': 6.494764128463768e-07, 'rewards/chosen': -1.765188455581665, 'rewards/rejected': -3.9036459922790527, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1384572982788086, 'policy_logps/rejected': -684.6578979492188, 'policy_logps/chosen': -512.3656616210938, 'referece_logps/rejected': -645.6213989257812, 'referece_logps/chosen': -494.71380615234375, 'logits/rejected': -0.8430357575416565, 'logits/chosen': -0.9334734082221985, 'epoch': 3.75}


 63%|██████▎   | 6721/10740 [33:24:40<19:36:50, 17.57s/it]

 63%|██████▎   | 6722/10740 [33:24:56<19:08:04, 17.14s/it]

 63%|██████▎   | 6723/10740 [33:25:09<17:50:28, 15.99s/it]
{'loss': 0.5075, 'learning_rate': 6.483468723098683e-07, 'rewards/chosen': -1.101427674293518, 'rewards/rejected': -2.2078590393066406, 'rewards/accuracies': 0.75, 'rewards/margins': 1.106431245803833, 'policy_logps/rejected': -629.9883422851562, 'policy_logps/chosen': -584.633544921875, 'referece_logps/rejected': -607.9097900390625, 'referece_logps/chosen': -573.619384765625, 'logits/rejected': -0.11037459969520569, 'logits/chosen': -0.015776798129081726, 'epoch': 3.76}


 63%|██████▎   | 6725/10740 [33:25:45<18:31:15, 16.61s/it]

 63%|██████▎   | 6726/10740 [33:26:03<19:00:14, 17.04s/it]

 63%|██████▎   | 6727/10740 [33:26:24<20:28:58, 18.37s/it]

 63%|██████▎   | 6728/10740 [33:26:46<21:37:53, 19.41s/it]

 63%|██████▎   | 6729/10740 [33:26:58<19:06:50, 17.16s/it]

 63%|██████▎   | 6730/10740 [33:27:17<19:41:56, 17.68s/it]

 63%|██████▎   | 6731/10740 [33:27:37<20:26:32, 18.36s/it]

 63%|██████▎   | 6732/10740 [33:27:53<19:43:14, 17.71s/it]

 63%|██████▎   | 6733/10740 [33:28:10<19:22:38, 17.41s/it]
{'loss': 0.3167, 'learning_rate': 6.455252634222206e-07, 'rewards/chosen': -1.2368534803390503, 'rewards/rejected': -3.6222591400146484, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3854055404663086, 'policy_logps/rejected': -350.3056945800781, 'policy_logps/chosen': -295.86895751953125, 'referece_logps/rejected': -314.0830993652344, 'referece_logps/chosen': -283.5004577636719, 'logits/rejected': -1.395640254020691, 'logits/chosen': -1.4840236902236938, 'epoch': 3.76}

 63%|██████▎   | 6734/10740 [33:28:29<20:09:08, 18.11s/it]

 63%|██████▎   | 6735/10740 [33:28:41<18:00:45, 16.19s/it]


 63%|██████▎   | 6737/10740 [33:29:15<18:19:54, 16.49s/it]

 63%|██████▎   | 6738/10740 [33:29:33<18:41:12, 16.81s/it]
{'loss': 0.3475, 'learning_rate': 6.4411566638768e-07, 'rewards/chosen': -1.422926425933838, 'rewards/rejected': -3.1271109580993652, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7041845321655273, 'policy_logps/rejected': -486.15576171875, 'policy_logps/chosen': -458.4946594238281, 'referece_logps/rejected': -454.8846435546875, 'referece_logps/chosen': -444.2653503417969, 'logits/rejected': -0.2752920687198639, 'logits/chosen': -0.4494752287864685, 'epoch': 3.76}

 63%|██████▎   | 6739/10740 [33:29:53<20:03:13, 18.04s/it]


 63%|██████▎   | 6741/10740 [33:30:33<20:55:23, 18.84s/it]
{'loss': 0.3062, 'learning_rate': 6.432702963844665e-07, 'rewards/chosen': -1.7954890727996826, 'rewards/rejected': -4.205173492431641, 'rewards/accuracies': 0.875, 'rewards/margins': 2.409684181213379, 'policy_logps/rejected': -471.54156494140625, 'policy_logps/chosen': -400.61737060546875, 'referece_logps/rejected': -429.4898376464844, 'referece_logps/chosen': -382.6624450683594, 'logits/rejected': 0.644016444683075, 'logits/chosen': 0.5281073451042175, 'epoch': 3.77}


 63%|██████▎   | 6743/10740 [33:31:06<19:59:37, 18.01s/it]

 63%|██████▎   | 6744/10740 [33:31:22<19:18:04, 17.39s/it]

 63%|██████▎   | 6745/10740 [33:31:42<19:58:53, 18.01s/it]

 63%|██████▎   | 6746/10740 [33:32:02<20:38:50, 18.61s/it]

 63%|██████▎   | 6747/10740 [33:32:25<22:00:16, 19.84s/it]

 63%|██████▎   | 6748/10740 [33:32:47<22:43:40, 20.50s/it]

 63%|██████▎   | 6749/10740 [33:33:06<22:30:28, 20.30s/it]

 63%|██████▎   | 6750/10740 [33:33:26<22:22:06, 20.18s/it]

 63%|██████▎   | 6751/10740 [33:33:44<21:35:59, 19.49s/it]

 63%|██████▎   | 6752/10740 [33:34:04<21:39:26, 19.55s/it]
{'loss': 0.3007, 'learning_rate': 6.401731099315306e-07, 'rewards/chosen': -2.090128183364868, 'rewards/rejected': -4.423749923706055, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3336212635040283, 'policy_logps/rejected': -545.6300048828125, 'policy_logps/chosen': -466.46282958984375, 'referece_logps/rejected': -501.39251708984375, 'referece_logps/chosen': -445.5615539550781, 'logits/rejected': -0.7869052886962891, 'logits/chosen': -0.759797215461731, 'epoch': 3.77}


 63%|██████▎   | 6754/10740 [33:34:45<22:00:23, 19.88s/it]
{'loss': 0.3621, 'learning_rate': 6.396104095762666e-07, 'rewards/chosen': -0.9581727981567383, 'rewards/rejected': -2.9092564582824707, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9510834217071533, 'policy_logps/rejected': -389.8522033691406, 'policy_logps/chosen': -329.34686279296875, 'referece_logps/rejected': -360.7596435546875, 'referece_logps/chosen': -319.76513671875, 'logits/rejected': -0.5823284387588501, 'logits/chosen': -0.5469611883163452, 'epoch': 3.77}

 63%|██████▎   | 6755/10740 [33:35:02<21:08:14, 19.10s/it]


 63%|██████▎   | 6757/10740 [33:35:37<20:24:59, 18.45s/it]

 63%|██████▎   | 6758/10740 [33:35:59<21:40:27, 19.59s/it]
{'loss': 0.3957, 'learning_rate': 6.384854024102027e-07, 'rewards/chosen': -1.5673582553863525, 'rewards/rejected': -3.0594959259033203, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4921376705169678, 'policy_logps/rejected': -363.78033447265625, 'policy_logps/chosen': -399.64715576171875, 'referece_logps/rejected': -333.1853332519531, 'referece_logps/chosen': -383.9735412597656, 'logits/rejected': -0.6341615319252014, 'logits/chosen': -0.6616091132164001, 'epoch': 3.78}

 63%|██████▎   | 6759/10740 [33:36:12<19:23:22, 17.53s/it]

 63%|██████▎   | 6760/10740 [33:36:34<20:46:32, 18.79s/it]


 63%|██████▎   | 6762/10740 [33:37:13<21:17:57, 19.28s/it]

 63%|██████▎   | 6763/10740 [33:37:29<20:06:05, 18.20s/it]

 63%|██████▎   | 6764/10740 [33:37:47<20:06:08, 18.20s/it]

 63%|██████▎   | 6765/10740 [33:38:07<20:35:31, 18.65s/it]
{'loss': 0.443, 'learning_rate': 6.365179067584769e-07, 'rewards/chosen': -2.2551803588867188, 'rewards/rejected': -4.041571140289307, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7863906621932983, 'policy_logps/rejected': -520.8473510742188, 'policy_logps/chosen': -481.4521484375, 'referece_logps/rejected': -480.431640625, 'referece_logps/chosen': -458.9003601074219, 'logits/rejected': -0.8572720289230347, 'logits/chosen': -0.8504980802536011, 'epoch': 3.78}

 63%|██████▎   | 6766/10740 [33:38:28<21:32:31, 19.51s/it]

 63%|██████▎   | 6767/10740 [33:38:48<21:27:01, 19.44s/it]

 63%|██████▎   | 6768/10740 [33:39:08<21:50:22, 19.79s/it]

 63%|██████▎   | 6769/10740 [33:39:31<22:42:15, 20.58s/it]

 63%|██████▎   | 6770/10740 [33:39:50<22:20:51, 20.26s/it]

 63%|██████▎   | 6771/10740 [33:40:10<22:12:06, 20.14s/it]

 63%|██████▎   | 6772/10740 [33:40:30<22:01:43, 19.99s/it]

 63%|██████▎   | 6773/10740 [33:40:48<21:26:57, 19.47s/it]


 63%|██████▎   | 6775/10740 [33:41:19<19:30:34, 17.71s/it]
{'loss': 0.4507, 'learning_rate': 6.337100109199362e-07, 'rewards/chosen': -1.3622777462005615, 'rewards/rejected': -2.462801456451416, 'rewards/accuracies': 0.75, 'rewards/margins': 1.100523591041565, 'policy_logps/rejected': -394.1561279296875, 'policy_logps/chosen': -286.32269287109375, 'referece_logps/rejected': -369.528076171875, 'referece_logps/chosen': -272.69989013671875, 'logits/rejected': -1.0901520252227783, 'logits/chosen': -1.0971190929412842, 'epoch': 3.78}

 63%|██████▎   | 6776/10740 [33:41:38<19:47:49, 17.98s/it]

 63%|██████▎   | 6777/10740 [33:41:55<19:22:58, 17.61s/it]


 63%|██████▎   | 6779/10740 [33:42:35<20:41:33, 18.81s/it]

 63%|██████▎   | 6780/10740 [33:42:55<20:58:14, 19.06s/it]
{'loss': 0.506, 'learning_rate': 6.323073107162343e-07, 'rewards/chosen': -2.692518949508667, 'rewards/rejected': -3.661752462387085, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9692337512969971, 'policy_logps/rejected': -480.476318359375, 'policy_logps/chosen': -508.7142333984375, 'referece_logps/rejected': -443.8587951660156, 'referece_logps/chosen': -481.7889709472656, 'logits/rejected': -0.9948766827583313, 'logits/chosen': -1.0833767652511597, 'epoch': 3.79}

 63%|██████▎   | 6781/10740 [33:43:15<21:06:37, 19.20s/it]

 63%|██████▎   | 6782/10740 [33:43:33<20:44:19, 18.86s/it]


 63%|██████▎   | 6784/10740 [33:44:07<19:49:57, 18.05s/it]
{'loss': 0.4176, 'learning_rate': 6.311857523660651e-07, 'rewards/chosen': -2.3365132808685303, 'rewards/rejected': -4.0805230140686035, 'rewards/accuracies': 0.625, 'rewards/margins': 1.744010090827942, 'policy_logps/rejected': -391.66705322265625, 'policy_logps/chosen': -454.9324951171875, 'referece_logps/rejected': -350.86181640625, 'referece_logps/chosen': -431.5673522949219, 'logits/rejected': -0.6635931730270386, 'logits/chosen': -0.6906086206436157, 'epoch': 3.79}

 63%|██████▎   | 6785/10740 [33:44:27<20:11:07, 18.37s/it]

 63%|██████▎   | 6786/10740 [33:44:48<21:17:43, 19.39s/it]

 63%|██████▎   | 6787/10740 [33:45:02<19:29:04, 17.74s/it]


 63%|██████▎   | 6789/10740 [33:45:35<18:20:37, 16.71s/it]

 63%|██████▎   | 6790/10740 [33:45:53<18:49:05, 17.15s/it]

 63%|██████▎   | 6791/10740 [33:46:09<18:24:37, 16.78s/it]
{'loss': 0.4573, 'learning_rate': 6.292243177000853e-07, 'rewards/chosen': -0.9424863457679749, 'rewards/rejected': -3.8411829471588135, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8986964225769043, 'policy_logps/rejected': -417.1734619140625, 'policy_logps/chosen': -605.7225341796875, 'referece_logps/rejected': -378.7615966796875, 'referece_logps/chosen': -596.297607421875, 'logits/rejected': -0.5551115274429321, 'logits/chosen': -0.4400031864643097, 'epoch': 3.79}


 63%|██████▎   | 6793/10740 [33:46:38<16:46:53, 15.31s/it]
{'loss': 0.5088, 'learning_rate': 6.286642109199316e-07, 'rewards/chosen': -1.175917625427246, 'rewards/rejected': -3.89674711227417, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7208292484283447, 'policy_logps/rejected': -390.69610595703125, 'policy_logps/chosen': -369.750732421875, 'referece_logps/rejected': -351.7286376953125, 'referece_logps/chosen': -357.9915771484375, 'logits/rejected': -0.8711686730384827, 'logits/chosen': -1.0249755382537842, 'epoch': 3.79}

 63%|██████▎   | 6794/10740 [33:46:49<15:25:01, 14.07s/it]

 63%|██████▎   | 6795/10740 [33:47:05<15:51:40, 14.47s/it]

 63%|██████▎   | 6796/10740 [33:47:16<14:59:43, 13.69s/it]

 63%|██████▎   | 6797/10740 [33:47:31<15:08:58, 13.83s/it]


 63%|██████▎   | 6799/10740 [33:48:00<15:20:11, 14.01s/it]

 63%|██████▎   | 6800/10740 [33:48:16<16:05:26, 14.70s/it]

 63%|██████▎   | 6801/10740 [33:48:38<18:29:18, 16.90s/it]

 63%|██████▎   | 6802/10740 [33:48:54<18:16:48, 16.71s/it]

 63%|██████▎   | 6803/10740 [33:49:12<18:30:57, 16.93s/it]

 63%|██████▎   | 6804/10740 [33:49:32<19:33:02, 17.88s/it]
{'loss': 0.3171, 'learning_rate': 6.255860439241028e-07, 'rewards/chosen': -2.589646816253662, 'rewards/rejected': -5.416898250579834, 'rewards/accuracies': 0.75, 'rewards/margins': 2.827251434326172, 'policy_logps/rejected': -390.9886779785156, 'policy_logps/chosen': -485.9656982421875, 'referece_logps/rejected': -336.8197021484375, 'referece_logps/chosen': -460.06927490234375, 'logits/rejected': -1.0883619785308838, 'logits/chosen': -1.1830438375473022, 'epoch': 3.8}


 63%|██████▎   | 6806/10740 [33:50:14<21:25:17, 19.60s/it]
{'loss': 0.3999, 'learning_rate': 6.250268189055772e-07, 'rewards/chosen': -1.522262692451477, 'rewards/rejected': -4.847077369689941, 'rewards/accuracies': 1.0, 'rewards/margins': 3.324814558029175, 'policy_logps/rejected': -437.37139892578125, 'policy_logps/chosen': -551.1943969726562, 'referece_logps/rejected': -388.9006042480469, 'referece_logps/chosen': -535.9717407226562, 'logits/rejected': 0.12149341404438019, 'logits/chosen': 0.15993182361125946, 'epoch': 3.8}


 63%|██████▎   | 6808/10740 [33:50:42<18:19:04, 16.77s/it]
{'loss': 0.3645, 'learning_rate': 6.244677303059705e-07, 'rewards/chosen': -1.1972451210021973, 'rewards/rejected': -2.1280806064605713, 'rewards/accuracies': 0.75, 'rewards/margins': 0.930835485458374, 'policy_logps/rejected': -337.6683349609375, 'policy_logps/chosen': -294.27752685546875, 'referece_logps/rejected': -316.3875427246094, 'referece_logps/chosen': -282.3050537109375, 'logits/rejected': -0.7767344117164612, 'logits/chosen': -0.6406071782112122, 'epoch': 3.8}

 63%|██████▎   | 6809/10740 [33:51:01<19:15:52, 17.64s/it]


 63%|██████▎   | 6811/10740 [33:51:38<19:18:40, 17.69s/it]
{'loss': 0.4582, 'learning_rate': 6.236293536369607e-07, 'rewards/chosen': -2.31062912940979, 'rewards/rejected': -2.8935811519622803, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5829517245292664, 'policy_logps/rejected': -305.7434387207031, 'policy_logps/chosen': -346.7173767089844, 'referece_logps/rejected': -276.8076477050781, 'referece_logps/chosen': -323.6110534667969, 'logits/rejected': -0.6207789182662964, 'logits/chosen': -0.6729350090026855, 'epoch': 3.81}


 63%|██████▎   | 6813/10740 [33:52:12<19:23:37, 17.78s/it]
{'loss': 0.2562, 'learning_rate': 6.23070606974425e-07, 'rewards/chosen': -0.9214898347854614, 'rewards/rejected': -3.5702366828918457, 'rewards/accuracies': 0.75, 'rewards/margins': 2.648746967315674, 'policy_logps/rejected': -566.5078735351562, 'policy_logps/chosen': -505.79144287109375, 'referece_logps/rejected': -530.8054809570312, 'referece_logps/chosen': -496.57659912109375, 'logits/rejected': -0.3795939087867737, 'logits/chosen': -0.6887835264205933, 'epoch': 3.81}


 63%|██████▎   | 6815/10740 [33:52:44<18:30:13, 16.97s/it]

 63%|██████▎   | 6816/10740 [33:53:04<19:24:42, 17.81s/it]
{'loss': 0.4196, 'learning_rate': 6.222327441640214e-07, 'rewards/chosen': -2.367560386657715, 'rewards/rejected': -3.479526996612549, 'rewards/accuracies': 0.75, 'rewards/margins': 1.111966609954834, 'policy_logps/rejected': -456.3929443359375, 'policy_logps/chosen': -494.9355163574219, 'referece_logps/rejected': -421.5976257324219, 'referece_logps/chosen': -471.2599182128906, 'logits/rejected': -0.007339153438806534, 'logits/chosen': -0.056909527629613876, 'epoch': 3.81}

 63%|██████▎   | 6817/10740 [33:53:15<17:07:37, 15.72s/it]

 63%|██████▎   | 6818/10740 [33:53:29<16:43:15, 15.35s/it]


 64%|██████▎   | 6820/10740 [33:54:08<18:52:27, 17.33s/it]
{'loss': 0.2962, 'learning_rate': 6.21116074892643e-07, 'rewards/chosen': -1.777101993560791, 'rewards/rejected': -3.350494623184204, 'rewards/accuracies': 0.875, 'rewards/margins': 1.573392391204834, 'policy_logps/rejected': -249.1531219482422, 'policy_logps/chosen': -304.21063232421875, 'referece_logps/rejected': -215.64816284179688, 'referece_logps/chosen': -286.43963623046875, 'logits/rejected': -0.8865634202957153, 'logits/chosen': -0.8710752129554749, 'epoch': 3.81}


 64%|██████▎   | 6822/10740 [33:54:44<19:28:29, 17.89s/it]
{'loss': 0.3318, 'learning_rate': 6.205579469179309e-07, 'rewards/chosen': -1.3101489543914795, 'rewards/rejected': -3.838402271270752, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5282535552978516, 'policy_logps/rejected': -381.67755126953125, 'policy_logps/chosen': -449.8819580078125, 'referece_logps/rejected': -343.2935791015625, 'referece_logps/chosen': -436.78045654296875, 'logits/rejected': -0.15742097795009613, 'logits/chosen': 0.025621265172958374, 'epoch': 3.81}


 64%|██████▎   | 6824/10740 [33:55:16<18:42:41, 17.20s/it]

 64%|██████▎   | 6825/10740 [33:55:38<20:13:00, 18.59s/it]
{'loss': 0.2908, 'learning_rate': 6.197210138531869e-07, 'rewards/chosen': -2.1041712760925293, 'rewards/rejected': -3.7064316272735596, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6022605895996094, 'policy_logps/rejected': -364.2430114746094, 'policy_logps/chosen': -324.3670654296875, 'referece_logps/rejected': -327.1786804199219, 'referece_logps/chosen': -303.3253173828125, 'logits/rejected': -0.6503228545188904, 'logits/chosen': -0.7902306318283081, 'epoch': 3.81}


 64%|██████▎   | 6827/10740 [33:56:05<17:10:39, 15.80s/it]

 64%|██████▎   | 6828/10740 [33:56:24<18:24:08, 16.93s/it]

 64%|██████▎   | 6829/10740 [33:56:46<20:02:52, 18.45s/it]
{'loss': 0.2384, 'learning_rate': 6.186055874408287e-07, 'rewards/chosen': -1.0999537706375122, 'rewards/rejected': -3.22904372215271, 'rewards/accuracies': 1.0, 'rewards/margins': 2.129089832305908, 'policy_logps/rejected': -342.3469543457031, 'policy_logps/chosen': -330.1589050292969, 'referece_logps/rejected': -310.0565185546875, 'referece_logps/chosen': -319.1593933105469, 'logits/rejected': -0.18710990250110626, 'logits/chosen': -0.24800173938274384, 'epoch': 3.82}

 64%|██████▎   | 6830/10740 [33:57:01<18:52:20, 17.38s/it]

 64%|██████▎   | 6831/10740 [33:57:24<20:32:25, 18.92s/it]


 64%|██████▎   | 6833/10740 [33:57:54<18:43:26, 17.25s/it]

 64%|██████▎   | 6834/10740 [33:58:12<18:55:11, 17.44s/it]
{'loss': 0.4603, 'learning_rate': 6.172120851124716e-07, 'rewards/chosen': -1.9197214841842651, 'rewards/rejected': -3.2609097957611084, 'rewards/accuracies': 0.5, 'rewards/margins': 1.3411880731582642, 'policy_logps/rejected': -419.0613098144531, 'policy_logps/chosen': -398.541259765625, 'referece_logps/rejected': -386.45220947265625, 'referece_logps/chosen': -379.3440246582031, 'logits/rejected': 0.567259669303894, 'logits/chosen': 0.46065056324005127, 'epoch': 3.82}

 64%|██████▎   | 6835/10740 [33:58:31<19:29:08, 17.96s/it]

 64%|██████▎   | 6836/10740 [33:58:43<17:30:22, 16.14s/it]


 64%|██████▎   | 6838/10740 [33:59:13<17:01:16, 15.70s/it]
{'loss': 0.4873, 'learning_rate': 6.160979097766406e-07, 'rewards/chosen': -2.280205249786377, 'rewards/rejected': -3.933638334274292, 'rewards/accuracies': 0.75, 'rewards/margins': 1.653432846069336, 'policy_logps/rejected': -518.9945068359375, 'policy_logps/chosen': -409.71112060546875, 'referece_logps/rejected': -479.65814208984375, 'referece_logps/chosen': -386.9090576171875, 'logits/rejected': -1.1610904932022095, 'logits/chosen': -1.0771138668060303, 'epoch': 3.82}

 64%|██████▎   | 6839/10740 [33:59:28<16:47:34, 15.50s/it]


 64%|██████▎   | 6841/10740 [34:00:00<17:25:50, 16.09s/it]

 64%|██████▎   | 6842/10740 [34:00:22<19:21:24, 17.88s/it]

 64%|██████▎   | 6843/10740 [34:00:43<20:10:04, 18.63s/it]

 64%|██████▎   | 6844/10740 [34:00:54<17:51:32, 16.50s/it]

 64%|██████▎   | 6845/10740 [34:01:11<17:52:20, 16.52s/it]

 64%|██████▎   | 6846/10740 [34:01:31<18:55:27, 17.50s/it]
{'loss': 0.3411, 'learning_rate': 6.138712367335235e-07, 'rewards/chosen': -1.5797711610794067, 'rewards/rejected': -3.0770466327667236, 'rewards/accuracies': 1.0, 'rewards/margins': 1.497275471687317, 'policy_logps/rejected': -542.9993896484375, 'policy_logps/chosen': -474.1081237792969, 'referece_logps/rejected': -512.2289428710938, 'referece_logps/chosen': -458.3104553222656, 'logits/rejected': 0.15516220033168793, 'logits/chosen': 0.1707758605480194, 'epoch': 3.82}


 64%|██████▍   | 6848/10740 [34:02:03<18:18:08, 16.93s/it]
{'loss': 0.3948, 'learning_rate': 6.133149191601397e-07, 'rewards/chosen': -2.7437291145324707, 'rewards/rejected': -2.7850217819213867, 'rewards/accuracies': 0.625, 'rewards/margins': 0.04129272699356079, 'policy_logps/rejected': -371.7100524902344, 'policy_logps/chosen': -387.5320739746094, 'referece_logps/rejected': -343.8598327636719, 'referece_logps/chosen': -360.09478759765625, 'logits/rejected': -0.5550642013549805, 'logits/chosen': -0.6936095356941223, 'epoch': 3.83}

 64%|██████▍   | 6849/10740 [34:02:14<16:27:07, 15.22s/it]

 64%|██████▍   | 6850/10740 [34:02:34<17:53:22, 16.56s/it]


 64%|██████▍   | 6852/10740 [34:03:03<16:56:55, 15.69s/it]
{'loss': 0.3324, 'learning_rate': 6.122027062551823e-07, 'rewards/chosen': -2.048496723175049, 'rewards/rejected': -3.2692646980285645, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2207682132720947, 'policy_logps/rejected': -239.32550048828125, 'policy_logps/chosen': -361.0989685058594, 'referece_logps/rejected': -206.63284301757812, 'referece_logps/chosen': -340.614013671875, 'logits/rejected': -0.8259018659591675, 'logits/chosen': -0.8197919130325317, 'epoch': 3.83}

 64%|██████▍   | 6853/10740 [34:03:18<16:41:19, 15.46s/it]


 64%|██████▍   | 6855/10740 [34:03:49<16:32:31, 15.33s/it]

 64%|██████▍   | 6856/10740 [34:04:11<18:45:44, 17.39s/it]

 64%|██████▍   | 6857/10740 [34:04:27<18:18:19, 16.97s/it]
{'loss': 0.4273, 'learning_rate': 6.10813233913575e-07, 'rewards/chosen': -1.6857504844665527, 'rewards/rejected': -2.8639395236968994, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1781890392303467, 'policy_logps/rejected': -456.35321044921875, 'policy_logps/chosen': -480.30963134765625, 'referece_logps/rejected': -427.7138366699219, 'referece_logps/chosen': -463.4521484375, 'logits/rejected': 0.17875733971595764, 'logits/chosen': 0.20143558084964752, 'epoch': 3.83}

 64%|██████▍   | 6858/10740 [34:04:43<18:06:59, 16.80s/it]

 64%|██████▍   | 6859/10740 [34:05:03<19:07:36, 17.74s/it]

 64%|██████▍   | 6860/10740 [34:05:16<17:38:16, 16.37s/it]

 64%|██████▍   | 6861/10740 [34:05:31<16:56:08, 15.72s/it]


 64%|██████▍   | 6863/10740 [34:05:57<15:48:27, 14.68s/it]
{'loss': 0.4153, 'learning_rate': 6.09147035498516e-07, 'rewards/chosen': -1.4747529029846191, 'rewards/rejected': -2.849790334701538, 'rewards/accuracies': 0.75, 'rewards/margins': 1.375037431716919, 'policy_logps/rejected': -442.82916259765625, 'policy_logps/chosen': -476.4793701171875, 'referece_logps/rejected': -414.3312683105469, 'referece_logps/chosen': -461.7318115234375, 'logits/rejected': -0.49395349621772766, 'logits/chosen': -0.6527624130249023, 'epoch': 3.83}

 64%|██████▍   | 6864/10740 [34:06:17<17:20:06, 16.10s/it]

 64%|██████▍   | 6865/10740 [34:06:35<18:04:25, 16.79s/it]

 64%|██████▍   | 6866/10740 [34:06:55<19:02:04, 17.69s/it]

 64%|██████▍   | 6867/10740 [34:07:10<18:11:30, 16.91s/it]

 64%|██████▍   | 6868/10740 [34:07:26<17:56:48, 16.69s/it]

 64%|██████▍   | 6869/10740 [34:07:45<18:30:59, 17.22s/it]


 64%|██████▍   | 6871/10740 [34:08:25<20:10:50, 18.78s/it]

 64%|██████▍   | 6872/10740 [34:08:41<19:15:24, 17.92s/it]
{'loss': 0.4349, 'learning_rate': 6.066501391389317e-07, 'rewards/chosen': -1.8272984027862549, 'rewards/rejected': -3.1268374919891357, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2995390892028809, 'policy_logps/rejected': -278.36395263671875, 'policy_logps/chosen': -302.314697265625, 'referece_logps/rejected': -247.0955810546875, 'referece_logps/chosen': -284.041748046875, 'logits/rejected': -1.7180304527282715, 'logits/chosen': -1.8112454414367676, 'epoch': 3.84}


 64%|██████▍   | 6874/10740 [34:09:17<19:26:59, 18.11s/it]
{'loss': 0.4257, 'learning_rate': 6.060956661712519e-07, 'rewards/chosen': -1.2461172342300415, 'rewards/rejected': -2.046938896179199, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8008215427398682, 'policy_logps/rejected': -266.83074951171875, 'policy_logps/chosen': -358.09454345703125, 'referece_logps/rejected': -246.36135864257812, 'referece_logps/chosen': -345.6333923339844, 'logits/rejected': -0.5258592367172241, 'logits/chosen': -0.4114842414855957, 'epoch': 3.84}

 64%|██████▍   | 6875/10740 [34:09:38<20:20:29, 18.95s/it]

 64%|██████▍   | 6876/10740 [34:09:58<20:41:55, 19.28s/it]

 64%|██████▍   | 6877/10740 [34:10:18<20:56:15, 19.51s/it]

 64%|██████▍   | 6878/10740 [34:10:38<20:57:22, 19.53s/it]

 64%|██████▍   | 6879/10740 [34:10:50<18:32:08, 17.28s/it]


 64%|██████▍   | 6881/10740 [34:11:34<20:50:04, 19.44s/it]
{'loss': 0.3215, 'learning_rate': 6.041561406444854e-07, 'rewards/chosen': -1.6640154123306274, 'rewards/rejected': -2.8510451316833496, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1870297193527222, 'policy_logps/rejected': -361.5644226074219, 'policy_logps/chosen': -269.1375732421875, 'referece_logps/rejected': -333.053955078125, 'referece_logps/chosen': -252.49742126464844, 'logits/rejected': -0.35207420587539673, 'logits/chosen': -0.47809094190597534, 'epoch': 3.84}

 64%|██████▍   | 6882/10740 [34:11:48<19:06:08, 17.82s/it]

 64%|██████▍   | 6883/10740 [34:12:09<20:16:09, 18.92s/it]


 64%|██████▍   | 6885/10740 [34:12:38<17:31:23, 16.36s/it]
{'loss': 0.4723, 'learning_rate': 6.030486318544321e-07, 'rewards/chosen': -1.9386391639709473, 'rewards/rejected': -2.4049739837646484, 'rewards/accuracies': 0.75, 'rewards/margins': 0.466334730386734, 'policy_logps/rejected': -557.345458984375, 'policy_logps/chosen': -493.6750183105469, 'referece_logps/rejected': -533.2957153320312, 'referece_logps/chosen': -474.28863525390625, 'logits/rejected': -0.9801530241966248, 'logits/chosen': -0.9921120405197144, 'epoch': 3.85}

 64%|██████▍   | 6886/10740 [34:12:57<18:35:08, 17.36s/it]

 64%|██████▍   | 6887/10740 [34:13:12<17:39:31, 16.50s/it]

 64%|██████▍   | 6888/10740 [34:13:29<17:45:05, 16.59s/it]

 64%|██████▍   | 6889/10740 [34:13:43<17:11:16, 16.07s/it]

 64%|██████▍   | 6890/10740 [34:14:03<18:09:13, 16.97s/it]

 64%|██████▍   | 6891/10740 [34:14:16<17:07:02, 16.01s/it]

 64%|██████▍   | 6892/10740 [34:14:35<17:56:32, 16.79s/it]

 64%|██████▍   | 6893/10740 [34:14:54<18:42:31, 17.51s/it]

 64%|██████▍   | 6894/10740 [34:15:08<17:34:52, 16.46s/it]

 64%|██████▍   | 6895/10740 [34:15:26<18:09:39, 17.00s/it]

 64%|██████▍   | 6896/10740 [34:15:46<19:03:38, 17.85s/it]

 64%|██████▍   | 6897/10740 [34:16:00<17:44:39, 16.62s/it]

 64%|██████▍   | 6898/10740 [34:16:20<18:55:35, 17.73s/it]

 64%|██████▍   | 6899/10740 [34:16:39<19:22:28, 18.16s/it]

 64%|██████▍   | 6900/10740 [34:16:54<18:13:29, 17.09s/it]

 64%|██████▍   | 6901/10740 [34:17:11<18:06:22, 16.98s/it]


 64%|██████▍   | 6903/10740 [34:17:46<18:07:25, 17.00s/it]

 64%|██████▍   | 6904/10740 [34:18:06<19:05:19, 17.91s/it]
{'loss': 0.3601, 'learning_rate': 5.977958812471268e-07, 'rewards/chosen': -1.3652691841125488, 'rewards/rejected': -3.276092052459717, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9108229875564575, 'policy_logps/rejected': -490.5023498535156, 'policy_logps/chosen': -519.064453125, 'referece_logps/rejected': -457.741455078125, 'referece_logps/chosen': -505.4117431640625, 'logits/rejected': -0.499894917011261, 'logits/chosen': -0.6125259399414062, 'epoch': 3.86}

 64%|██████▍   | 6905/10740 [34:18:18<17:19:56, 16.27s/it]


 64%|██████▍   | 6907/10740 [34:18:56<18:46:20, 17.63s/it]
{'loss': 0.4052, 'learning_rate': 5.969677023373841e-07, 'rewards/chosen': -1.562410831451416, 'rewards/rejected': -3.246304988861084, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6838945150375366, 'policy_logps/rejected': -500.2066345214844, 'policy_logps/chosen': -263.8412780761719, 'referece_logps/rejected': -467.7435607910156, 'referece_logps/chosen': -248.2171630859375, 'logits/rejected': -0.9597301483154297, 'logits/chosen': -1.0517659187316895, 'epoch': 3.86}

 64%|██████▍   | 6908/10740 [34:19:15<19:17:58, 18.13s/it]

 64%|██████▍   | 6909/10740 [34:19:31<18:34:38, 17.46s/it]

 64%|██████▍   | 6910/10740 [34:19:52<19:42:44, 18.53s/it]


 64%|██████▍   | 6912/10740 [34:20:24<18:19:48, 17.24s/it]
{'loss': 0.4366, 'learning_rate': 5.955881376246076e-07, 'rewards/chosen': -1.272874355316162, 'rewards/rejected': -2.903557062149048, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6306827068328857, 'policy_logps/rejected': -411.00921630859375, 'policy_logps/chosen': -313.9928894042969, 'referece_logps/rejected': -381.9736328125, 'referece_logps/chosen': -301.26416015625, 'logits/rejected': -1.1078771352767944, 'logits/chosen': -1.1979557275772095, 'epoch': 3.86}

 64%|██████▍   | 6913/10740 [34:20:46<19:55:58, 18.75s/it]

 64%|██████▍   | 6914/10740 [34:20:57<17:23:48, 16.37s/it]

 64%|██████▍   | 6915/10740 [34:21:09<15:59:03, 15.04s/it]

 64%|██████▍   | 6916/10740 [34:21:30<17:45:44, 16.72s/it]

 64%|██████▍   | 6917/10740 [34:21:50<18:43:03, 17.63s/it]

 64%|██████▍   | 6918/10740 [34:22:11<19:52:54, 18.73s/it]

 64%|██████▍   | 6919/10740 [34:22:31<20:28:54, 19.30s/it]

 64%|██████▍   | 6920/10740 [34:22:49<19:58:37, 18.83s/it]

 64%|██████▍   | 6921/10740 [34:23:07<19:47:17, 18.65s/it]

 64%|██████▍   | 6922/10740 [34:23:26<19:36:41, 18.49s/it]


 64%|██████▍   | 6924/10740 [34:23:58<18:09:56, 17.14s/it]
{'loss': 0.4224, 'learning_rate': 5.922809400710485e-07, 'rewards/chosen': -2.135982036590576, 'rewards/rejected': -3.4371964931488037, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3012144565582275, 'policy_logps/rejected': -379.57952880859375, 'policy_logps/chosen': -493.01763916015625, 'referece_logps/rejected': -345.20758056640625, 'referece_logps/chosen': -471.6578369140625, 'logits/rejected': -0.557540774345398, 'logits/chosen': -0.4977555572986603, 'epoch': 3.87}

 64%|██████▍   | 6925/10740 [34:24:17<18:34:48, 17.53s/it]

 64%|██████▍   | 6926/10740 [34:24:38<19:43:42, 18.62s/it]

 64%|██████▍   | 6927/10740 [34:24:52<18:10:17, 17.16s/it]

 65%|██████▍   | 6928/10740 [34:25:06<17:21:21, 16.39s/it]

 65%|██████▍   | 6929/10740 [34:25:26<18:25:18, 17.40s/it]


 65%|██████▍   | 6931/10740 [34:26:00<18:48:35, 17.78s/it]

 65%|██████▍   | 6932/10740 [34:26:20<19:30:09, 18.44s/it]

 65%|██████▍   | 6933/10740 [34:26:40<20:00:09, 18.92s/it]
{'loss': 0.3249, 'learning_rate': 5.898040438839995e-07, 'rewards/chosen': -1.2466694116592407, 'rewards/rejected': -3.4353952407836914, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1887259483337402, 'policy_logps/rejected': -344.02276611328125, 'policy_logps/chosen': -270.4801330566406, 'referece_logps/rejected': -309.6687927246094, 'referece_logps/chosen': -258.0134582519531, 'logits/rejected': -0.7457810640335083, 'logits/chosen': -0.7426065802574158, 'epoch': 3.87}

 65%|██████▍   | 6934/10740 [34:26:53<18:07:33, 17.14s/it]

 65%|██████▍   | 6935/10740 [34:27:16<19:42:14, 18.64s/it]

 65%|██████▍   | 6936/10740 [34:27:32<19:04:48, 18.06s/it]

 65%|██████▍   | 6937/10740 [34:27:52<19:34:12, 18.53s/it]

 65%|██████▍   | 6938/10740 [34:28:12<20:11:25, 19.12s/it]

 65%|██████▍   | 6939/10740 [34:28:28<18:57:21, 17.95s/it]


 65%|██████▍   | 6941/10740 [34:28:59<17:11:15, 16.29s/it]
{'loss': 0.4486, 'learning_rate': 5.876048947823071e-07, 'rewards/chosen': -1.2568118572235107, 'rewards/rejected': -2.8705039024353027, 'rewards/accuracies': 1.0, 'rewards/margins': 1.613692045211792, 'policy_logps/rejected': -344.83929443359375, 'policy_logps/chosen': -354.4220275878906, 'referece_logps/rejected': -316.1342468261719, 'referece_logps/chosen': -341.85394287109375, 'logits/rejected': -1.3384523391723633, 'logits/chosen': -1.523708462715149, 'epoch': 3.88}

 65%|██████▍   | 6942/10740 [34:29:13<16:37:24, 15.76s/it]

 65%|██████▍   | 6943/10740 [34:29:27<16:06:08, 15.27s/it]

 65%|██████▍   | 6944/10740 [34:29:38<14:38:02, 13.88s/it]

 65%|██████▍   | 6945/10740 [34:29:57<16:26:29, 15.60s/it]

 65%|██████▍   | 6946/10740 [34:30:10<15:25:59, 14.64s/it]

 65%|██████▍   | 6947/10740 [34:30:29<16:54:16, 16.04s/it]

 65%|██████▍   | 6948/10740 [34:30:51<18:34:21, 17.63s/it]

 65%|██████▍   | 6949/10740 [34:31:08<18:23:48, 17.47s/it]

 65%|██████▍   | 6950/10740 [34:31:29<19:39:48, 18.68s/it]

 65%|██████▍   | 6951/10740 [34:31:50<20:12:38, 19.20s/it]

 65%|██████▍   | 6952/10740 [34:32:09<20:24:46, 19.40s/it]

 65%|██████▍   | 6953/10740 [34:32:29<20:27:43, 19.45s/it]

 65%|██████▍   | 6954/10740 [34:32:46<19:37:28, 18.66s/it]

 65%|██████▍   | 6955/10740 [34:33:01<18:37:47, 17.72s/it]

 65%|██████▍   | 6956/10740 [34:33:23<19:53:58, 18.93s/it]

 65%|██████▍   | 6957/10740 [34:33:40<19:06:37, 18.19s/it]

 65%|██████▍   | 6958/10740 [34:33:53<17:44:07, 16.88s/it]

 65%|██████▍   | 6959/10740 [34:34:09<17:25:09, 16.59s/it]

 65%|██████▍   | 6960/10740 [34:34:30<18:41:03, 17.79s/it]

 65%|██████▍   | 6961/10740 [34:34:47<18:19:57, 17.46s/it]

 65%|██████▍   | 6962/10740 [34:35:03<18:07:34, 17.27s/it]

 65%|██████▍   | 6963/10740 [34:35:21<18:18:50, 17.46s/it]

 65%|██████▍   | 6964/10740 [34:35:37<17:42:54, 16.89s/it]

 65%|██████▍   | 6965/10740 [34:35:50<16:34:41, 15.81s/it]

 65%|██████▍   | 6966/10740 [34:36:05<16:23:44, 15.64s/it]

 65%|██████▍   | 6967/10740 [34:36:22<16:38:33, 15.88s/it]

 65%|██████▍   | 6968/10740 [34:36:35<15:39:38, 14.95s/it]


 65%|██████▍   | 6970/10740 [34:37:05<16:12:37, 15.48s/it]
{'loss': 0.4833, 'learning_rate': 5.79653196258316e-07, 'rewards/chosen': -1.5945245027542114, 'rewards/rejected': -2.5508391857147217, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9563148021697998, 'policy_logps/rejected': -305.23040771484375, 'policy_logps/chosen': -347.1863708496094, 'referece_logps/rejected': -279.7220153808594, 'referece_logps/chosen': -331.24114990234375, 'logits/rejected': -0.68049556016922, 'logits/chosen': -0.7981785535812378, 'epoch': 3.89}

 65%|██████▍   | 6971/10740 [34:37:22<16:44:27, 15.99s/it]


 65%|██████▍   | 6973/10740 [34:37:55<17:04:10, 16.31s/it]
{'loss': 0.4365, 'learning_rate': 5.78832431507047e-07, 'rewards/chosen': -1.4306784868240356, 'rewards/rejected': -2.7565135955810547, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3258352279663086, 'policy_logps/rejected': -388.4571228027344, 'policy_logps/chosen': -442.9801940917969, 'referece_logps/rejected': -360.8919982910156, 'referece_logps/chosen': -428.67340087890625, 'logits/rejected': -0.5453309416770935, 'logits/chosen': -0.3686262369155884, 'epoch': 3.9}

 65%|██████▍   | 6974/10740 [34:38:14<17:57:07, 17.16s/it]

 65%|██████▍   | 6975/10740 [34:38:34<18:42:38, 17.89s/it]

 65%|██████▍   | 6976/10740 [34:38:54<19:22:08, 18.53s/it]

 65%|██████▍   | 6977/10740 [34:39:15<20:16:56, 19.40s/it]

 65%|██████▍   | 6978/10740 [34:39:30<18:54:41, 18.10s/it]

 65%|██████▍   | 6979/10740 [34:39:48<18:38:34, 17.84s/it]

 65%|██████▍   | 6980/10740 [34:40:09<19:43:19, 18.88s/it]

 65%|██████▌   | 6981/10740 [34:40:29<19:57:02, 19.11s/it]

 65%|██████▌   | 6982/10740 [34:40:50<20:34:46, 19.71s/it]

 65%|██████▌   | 6983/10740 [34:41:10<20:37:30, 19.76s/it]

 65%|██████▌   | 6984/10740 [34:41:29<20:30:06, 19.65s/it]

 65%|██████▌   | 6985/10740 [34:41:49<20:28:47, 19.63s/it]

 65%|██████▌   | 6986/10740 [34:42:01<18:06:18, 17.36s/it]

 65%|██████▌   | 6987/10740 [34:42:13<16:39:35, 15.98s/it]

 65%|██████▌   | 6988/10740 [34:42:34<18:09:34, 17.42s/it]

 65%|██████▌   | 6989/10740 [34:42:47<16:39:52, 15.99s/it]

 65%|██████▌   | 6990/10740 [34:43:08<18:20:51, 17.61s/it]

 65%|██████▌   | 6991/10740 [34:43:27<18:35:38, 17.85s/it]

 65%|██████▌   | 6992/10740 [34:43:42<17:50:40, 17.14s/it]

 65%|██████▌   | 6993/10740 [34:44:02<18:39:59, 17.93s/it]


 65%|██████▌   | 6995/10740 [34:44:30<16:31:48, 15.89s/it]

 65%|██████▌   | 6996/10740 [34:44:46<16:32:40, 15.91s/it]

 65%|██████▌   | 6997/10740 [34:45:03<16:40:20, 16.04s/it]

 65%|██████▌   | 6998/10740 [34:45:19<16:50:25, 16.20s/it]

 65%|██████▌   | 6999/10740 [34:45:39<17:58:23, 17.30s/it]

 65%|██████▌   | 7000/10740 [34:45:50<16:04:36, 15.48s/it]
[2024-04-03 05:59:34,110] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 7001/10740 [34:46:22<21:15:03, 20.46s/it]
{'loss': 0.4316, 'learning_rate': 5.711886752830674e-07, 'rewards/chosen': -1.4784796237945557, 'rewards/rejected': -2.6528563499450684, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1743769645690918, 'policy_logps/rejected': -327.80718994140625, 'policy_logps/chosen': -333.437744140625, 'referece_logps/rejected': -301.2785949707031, 'referece_logps/chosen': -318.6529541015625, 'logits/rejected': 0.012325599789619446, 'logits/chosen': 0.05887189507484436, 'epoch': 3.91}


 65%|██████▌   | 7003/10740 [34:46:56<19:09:58, 18.46s/it]

 65%|██████▌   | 7004/10740 [34:47:15<19:06:26, 18.41s/it]

 65%|██████▌   | 7005/10740 [34:47:34<19:27:29, 18.75s/it]

 65%|██████▌   | 7006/10740 [34:47:55<20:05:52, 19.38s/it]

 65%|██████▌   | 7007/10740 [34:48:14<20:03:28, 19.34s/it]

 65%|██████▌   | 7008/10740 [34:48:33<19:50:52, 19.15s/it]

 65%|██████▌   | 7009/10740 [34:48:53<19:57:23, 19.26s/it]
{'loss': 0.3612, 'learning_rate': 5.69010337325857e-07, 'rewards/chosen': -1.4516525268554688, 'rewards/rejected': -2.8336338996887207, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3819811344146729, 'policy_logps/rejected': -260.05780029296875, 'policy_logps/chosen': -294.9558410644531, 'referece_logps/rejected': -231.721435546875, 'referece_logps/chosen': -280.4393005371094, 'logits/rejected': -1.237918496131897, 'logits/chosen': -1.2269067764282227, 'epoch': 3.92}

 65%|██████▌   | 7010/10740 [34:49:12<20:01:31, 19.33s/it]


 65%|██████▌   | 7012/10740 [34:49:46<19:09:25, 18.50s/it]

 65%|██████▌   | 7013/10740 [34:50:08<20:16:19, 19.58s/it]
[2024-04-03 06:03:51,714] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 7014/10740 [34:50:21<18:19:20, 17.70s/it]

 65%|██████▌   | 7015/10740 [34:50:41<18:57:12, 18.32s/it]
{'loss': 0.4606, 'learning_rate': 5.673782295460783e-07, 'rewards/chosen': -0.9230303168296814, 'rewards/rejected': -3.2629942893981934, 'rewards/accuracies': 1.0, 'rewards/margins': 2.339963912963867, 'policy_logps/rejected': -338.61456298828125, 'policy_logps/chosen': -254.43263244628906, 'referece_logps/rejected': -305.984619140625, 'referece_logps/chosen': -245.2023162841797, 'logits/rejected': -1.8027280569076538, 'logits/chosen': -1.8760015964508057, 'epoch': 3.92}


 65%|██████▌   | 7017/10740 [34:51:17<18:44:56, 18.13s/it]

 65%|██████▌   | 7018/10740 [34:51:39<19:53:36, 19.24s/it]

 65%|██████▌   | 7019/10740 [34:51:59<20:01:18, 19.37s/it]
{'loss': 0.25, 'learning_rate': 5.662909443232184e-07, 'rewards/chosen': -0.4188578128814697, 'rewards/rejected': -3.135773181915283, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7169156074523926, 'policy_logps/rejected': -266.09588623046875, 'policy_logps/chosen': -296.9922180175781, 'referece_logps/rejected': -234.73814392089844, 'referece_logps/chosen': -292.8037109375, 'logits/rejected': -0.7902208566665649, 'logits/chosen': -0.9048683643341064, 'epoch': 3.92}


 65%|██████▌   | 7021/10740 [34:52:34<18:48:02, 18.20s/it]
{'loss': 0.3239, 'learning_rate': 5.6574753829432e-07, 'rewards/chosen': -0.7034793496131897, 'rewards/rejected': -3.0095622539520264, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3060824871063232, 'policy_logps/rejected': -449.9728088378906, 'policy_logps/chosen': -491.36181640625, 'referece_logps/rejected': -419.8772277832031, 'referece_logps/chosen': -484.3269958496094, 'logits/rejected': 0.04425475001335144, 'logits/chosen': -0.13671834766864777, 'epoch': 3.92}

 65%|██████▌   | 7022/10740 [34:52:52<18:59:07, 18.38s/it]

 65%|██████▌   | 7023/10740 [34:53:12<19:26:08, 18.82s/it]


 65%|██████▌   | 7025/10740 [34:53:48<18:49:20, 18.24s/it]

 65%|██████▌   | 7026/10740 [34:54:09<19:38:23, 19.04s/it]
{'loss': 0.4497, 'learning_rate': 5.643897148400883e-07, 'rewards/chosen': -2.231034755706787, 'rewards/rejected': -3.115586280822754, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8845516443252563, 'policy_logps/rejected': -400.2059020996094, 'policy_logps/chosen': -348.6357116699219, 'referece_logps/rejected': -369.050048828125, 'referece_logps/chosen': -326.3253479003906, 'logits/rejected': -1.2214043140411377, 'logits/chosen': -1.337902545928955, 'epoch': 3.93}

 65%|██████▌   | 7027/10740 [34:54:25<18:41:16, 18.12s/it]


 65%|██████▌   | 7029/10740 [34:54:58<18:26:06, 17.88s/it]
[2024-04-03 06:08:42,078] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 7030/10740 [34:55:13<17:34:23, 17.05s/it]
[2024-04-03 06:08:57,188] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 7031/10740 [34:55:26<16:07:56, 15.66s/it]

 65%|██████▌   | 7032/10740 [34:55:45<17:19:44, 16.82s/it]
{'loss': 0.406, 'learning_rate': 5.627616344204917e-07, 'rewards/chosen': -0.7265733480453491, 'rewards/rejected': -3.596510171890259, 'rewards/accuracies': 0.875, 'rewards/margins': 2.869936943054199, 'policy_logps/rejected': -393.36181640625, 'policy_logps/chosen': -526.99169921875, 'referece_logps/rejected': -357.3966979980469, 'referece_logps/chosen': -519.7259521484375, 'logits/rejected': -0.789084792137146, 'logits/chosen': -0.8921477198600769, 'epoch': 3.93}


 65%|██████▌   | 7034/10740 [34:56:27<19:21:01, 18.80s/it]

 66%|██████▌   | 7035/10740 [34:56:44<18:36:55, 18.09s/it]

 66%|██████▌   | 7036/10740 [34:57:02<18:42:38, 18.19s/it]

 66%|██████▌   | 7037/10740 [34:57:14<16:52:58, 16.41s/it]

 66%|██████▌   | 7038/10740 [34:57:26<15:32:00, 15.11s/it]

 66%|██████▌   | 7039/10740 [34:57:40<15:12:49, 14.80s/it]

 66%|██████▌   | 7040/10740 [34:57:56<15:30:11, 15.08s/it]

 66%|██████▌   | 7041/10740 [34:58:12<15:50:09, 15.41s/it]

 66%|██████▌   | 7042/10740 [34:58:34<17:43:00, 17.25s/it]

 66%|██████▌   | 7043/10740 [34:58:48<16:44:04, 16.30s/it]

 66%|██████▌   | 7044/10740 [34:58:58<14:59:54, 14.61s/it]

 66%|██████▌   | 7045/10740 [34:59:11<14:30:19, 14.13s/it]

 66%|██████▌   | 7046/10740 [34:59:30<15:51:18, 15.45s/it]

 66%|██████▌   | 7047/10740 [34:59:46<15:53:26, 15.49s/it]
{'loss': 0.4471, 'learning_rate': 5.586977084639384e-07, 'rewards/chosen': -0.5261095762252808, 'rewards/rejected': -3.235084056854248, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7089743614196777, 'policy_logps/rejected': -462.0810852050781, 'policy_logps/chosen': -461.974853515625, 'referece_logps/rejected': -429.73028564453125, 'referece_logps/chosen': -456.7137451171875, 'logits/rejected': 0.9133224487304688, 'logits/chosen': 1.0473135709762573, 'epoch': 3.94}


 66%|██████▌   | 7049/10740 [35:00:16<15:53:52, 15.51s/it]

 66%|██████▌   | 7050/10740 [35:00:27<14:26:44, 14.09s/it]
{'loss': 0.437, 'learning_rate': 5.578860043259346e-07, 'rewards/chosen': -1.113296389579773, 'rewards/rejected': -3.352288007736206, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2389917373657227, 'policy_logps/rejected': -460.23504638671875, 'policy_logps/chosen': -409.8682556152344, 'referece_logps/rejected': -426.71209716796875, 'referece_logps/chosen': -398.73529052734375, 'logits/rejected': -1.329526424407959, 'logits/chosen': -1.4394433498382568, 'epoch': 3.94}


 66%|██████▌   | 7052/10740 [35:01:09<17:45:49, 17.34s/it]
{'loss': 0.3811, 'learning_rate': 5.573450692496783e-07, 'rewards/chosen': -1.3411865234375, 'rewards/rejected': -3.0037553310394287, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6625686883926392, 'policy_logps/rejected': -231.48194885253906, 'policy_logps/chosen': -242.65359497070312, 'referece_logps/rejected': -201.44439697265625, 'referece_logps/chosen': -229.2417449951172, 'logits/rejected': -0.9675612449645996, 'logits/chosen': -0.9248505234718323, 'epoch': 3.94}


 66%|██████▌   | 7054/10740 [35:01:49<19:23:20, 18.94s/it]
[2024-04-03 06:15:33,198] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 7055/10740 [35:02:05<18:11:54, 17.78s/it]

 66%|██████▌   | 7056/10740 [35:02:20<17:26:08, 17.04s/it]

 66%|██████▌   | 7057/10740 [35:02:32<15:57:55, 15.61s/it]

 66%|██████▌   | 7058/10740 [35:02:48<16:08:09, 15.78s/it]

 66%|██████▌   | 7059/10740 [35:03:09<17:30:12, 17.12s/it]
{'loss': 0.4079, 'learning_rate': 5.55453065981079e-07, 'rewards/chosen': -2.1649272441864014, 'rewards/rejected': -3.2297534942626953, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0648263692855835, 'policy_logps/rejected': -507.59417724609375, 'policy_logps/chosen': -448.0300598144531, 'referece_logps/rejected': -475.296630859375, 'referece_logps/chosen': -426.38079833984375, 'logits/rejected': -1.1446009874343872, 'logits/chosen': -1.0898922681808472, 'epoch': 3.94}


 66%|██████▌   | 7061/10740 [35:03:40<16:55:07, 16.56s/it]

 66%|██████▌   | 7062/10740 [35:03:59<17:41:06, 17.31s/it]
[2024-04-03 06:17:42,345] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 7063/10740 [35:04:12<16:29:25, 16.15s/it]

 66%|██████▌   | 7064/10740 [35:04:32<17:31:26, 17.16s/it]

 66%|██████▌   | 7065/10740 [35:04:49<17:27:35, 17.10s/it]

 66%|██████▌   | 7066/10740 [35:05:06<17:41:18, 17.33s/it]

 66%|██████▌   | 7067/10740 [35:05:21<16:45:42, 16.43s/it]

 66%|██████▌   | 7068/10740 [35:05:32<15:08:34, 14.85s/it]
{'loss': 0.3678, 'learning_rate': 5.530234026775806e-07, 'rewards/chosen': -2.3435254096984863, 'rewards/rejected': -3.6077752113342285, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2642499208450317, 'policy_logps/rejected': -353.8568420410156, 'policy_logps/chosen': -342.78021240234375, 'referece_logps/rejected': -317.77911376953125, 'referece_logps/chosen': -319.344970703125, 'logits/rejected': 0.20962625741958618, 'logits/chosen': 0.19035695493221283, 'epoch': 3.95}


 66%|██████▌   | 7070/10740 [35:06:08<16:41:33, 16.37s/it]

 66%|██████▌   | 7071/10740 [35:06:22<15:52:27, 15.58s/it]

 66%|██████▌   | 7072/10740 [35:06:36<15:21:51, 15.08s/it]
{'loss': 0.3151, 'learning_rate': 5.519446082507737e-07, 'rewards/chosen': -0.5470909476280212, 'rewards/rejected': -3.3866703510284424, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8395791053771973, 'policy_logps/rejected': -488.85101318359375, 'policy_logps/chosen': -411.57928466796875, 'referece_logps/rejected': -454.9843444824219, 'referece_logps/chosen': -406.1083984375, 'logits/rejected': 0.50504469871521, 'logits/chosen': 0.3730076849460602, 'epoch': 3.95}
[2024-04-03 06:20:41,056] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 66%|██████▌   | 7074/10740 [35:07:12<16:32:21, 16.24s/it]
[2024-04-03 06:20:55,436] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 7075/10740 [35:07:30<17:17:36, 16.99s/it]
{'loss': 0.3757, 'learning_rate': 5.511359402380368e-07, 'rewards/chosen': -1.5771485567092896, 'rewards/rejected': -2.8143527507781982, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2372041940689087, 'policy_logps/rejected': -463.2143859863281, 'policy_logps/chosen': -355.0836181640625, 'referece_logps/rejected': -435.0708312988281, 'referece_logps/chosen': -339.3121337890625, 'logits/rejected': -0.19085383415222168, 'logits/chosen': -0.2798691987991333, 'epoch': 3.95}


 66%|██████▌   | 7077/10740 [35:08:14<19:51:18, 19.51s/it]

 66%|██████▌   | 7078/10740 [35:08:27<17:43:32, 17.43s/it]

 66%|██████▌   | 7079/10740 [35:08:46<18:23:56, 18.09s/it]

 66%|██████▌   | 7080/10740 [35:09:04<18:22:25, 18.07s/it]

 66%|██████▌   | 7081/10740 [35:09:25<19:06:44, 18.80s/it]
[2024-04-03 06:23:08,477] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 7082/10740 [35:09:43<18:59:39, 18.69s/it]

 66%|██████▌   | 7083/10740 [35:10:03<19:22:10, 19.07s/it]

 66%|██████▌   | 7084/10740 [35:10:23<19:40:53, 19.38s/it]
{'loss': 0.3869, 'learning_rate': 5.487121434115003e-07, 'rewards/chosen': -1.1257679462432861, 'rewards/rejected': -2.8317861557006836, 'rewards/accuracies': 0.75, 'rewards/margins': 1.706018090248108, 'policy_logps/rejected': -401.90106201171875, 'policy_logps/chosen': -283.5002136230469, 'referece_logps/rejected': -373.5832214355469, 'referece_logps/chosen': -272.2425231933594, 'logits/rejected': -1.1091551780700684, 'logits/chosen': -1.0448466539382935, 'epoch': 3.96}


 66%|██████▌   | 7086/10740 [35:10:54<17:26:33, 17.18s/it]
{'loss': 0.4634, 'learning_rate': 5.481739727691159e-07, 'rewards/chosen': -0.886625349521637, 'rewards/rejected': -3.0970966815948486, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2104711532592773, 'policy_logps/rejected': -257.81707763671875, 'policy_logps/chosen': -330.49591064453125, 'referece_logps/rejected': -226.84608459472656, 'referece_logps/chosen': -321.6296691894531, 'logits/rejected': -0.9987563490867615, 'logits/chosen': -1.168837547302246, 'epoch': 3.96}


 66%|██████▌   | 7088/10740 [35:11:19<14:54:48, 14.70s/it]

 66%|██████▌   | 7089/10740 [35:11:37<16:08:55, 15.92s/it]

 66%|██████▌   | 7090/10740 [35:11:54<16:29:18, 16.26s/it]

 66%|██████▌   | 7091/10740 [35:12:09<16:02:45, 15.83s/it]

 66%|██████▌   | 7092/10740 [35:12:29<17:07:35, 16.90s/it]
{'loss': 0.3615, 'learning_rate': 5.465604478972536e-07, 'rewards/chosen': -1.7182343006134033, 'rewards/rejected': -2.160935640335083, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4427013099193573, 'policy_logps/rejected': -420.24859619140625, 'policy_logps/chosen': -342.2016906738281, 'referece_logps/rejected': -398.6392517089844, 'referece_logps/chosen': -325.01934814453125, 'logits/rejected': -0.9984825849533081, 'logits/chosen': -1.0873173475265503, 'epoch': 3.96}

 66%|██████▌   | 7093/10740 [35:12:48<17:56:00, 17.70s/it]

 66%|██████▌   | 7094/10740 [35:13:06<17:52:12, 17.64s/it]


 66%|██████▌   | 7096/10740 [35:13:41<17:57:24, 17.74s/it]

 66%|██████▌   | 7097/10740 [35:13:55<16:50:33, 16.64s/it]

 66%|██████▌   | 7098/10740 [35:14:07<15:25:18, 15.24s/it]

 66%|██████▌   | 7099/10740 [35:14:29<17:21:07, 17.16s/it]
{'loss': 0.3609, 'learning_rate': 5.446798790690959e-07, 'rewards/chosen': -1.5091991424560547, 'rewards/rejected': -2.522134304046631, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0129352807998657, 'policy_logps/rejected': -350.5885009765625, 'policy_logps/chosen': -364.91943359375, 'referece_logps/rejected': -325.36712646484375, 'referece_logps/chosen': -349.8274230957031, 'logits/rejected': -0.878704845905304, 'logits/chosen': -1.0490394830703735, 'epoch': 3.97}

 66%|██████▌   | 7100/10740 [35:14:50<18:37:22, 18.42s/it]


 66%|██████▌   | 7102/10740 [35:15:15<15:38:04, 15.47s/it]
{'loss': 0.3412, 'learning_rate': 5.438745416985115e-07, 'rewards/chosen': -2.2713935375213623, 'rewards/rejected': -3.430530309677124, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1591368913650513, 'policy_logps/rejected': -490.2117919921875, 'policy_logps/chosen': -537.0031127929688, 'referece_logps/rejected': -455.9064636230469, 'referece_logps/chosen': -514.2891845703125, 'logits/rejected': -0.920859158039093, 'logits/chosen': -0.9374995231628418, 'epoch': 3.97}

 66%|██████▌   | 7103/10740 [35:15:26<14:13:20, 14.08s/it]


 66%|██████▌   | 7105/10740 [35:15:55<14:41:52, 14.56s/it]

 66%|██████▌   | 7106/10740 [35:16:11<15:03:39, 14.92s/it]

 66%|██████▌   | 7107/10740 [35:16:31<16:34:16, 16.42s/it]

 66%|██████▌   | 7108/10740 [35:16:49<17:09:51, 17.01s/it]
{'loss': 0.2627, 'learning_rate': 5.422649877308796e-07, 'rewards/chosen': -1.0732234716415405, 'rewards/rejected': -3.680419921875, 'rewards/accuracies': 1.0, 'rewards/margins': 2.60719633102417, 'policy_logps/rejected': -365.9504699707031, 'policy_logps/chosen': -311.08709716796875, 'referece_logps/rejected': -329.14630126953125, 'referece_logps/chosen': -300.35491943359375, 'logits/rejected': -0.4674571454524994, 'logits/chosen': -0.2837170362472534, 'epoch': 3.97}

 66%|██████▌   | 7109/10740 [35:17:07<17:17:26, 17.14s/it]

 66%|██████▌   | 7110/10740 [35:17:26<18:00:50, 17.87s/it]

 66%|██████▌   | 7111/10740 [35:17:42<17:27:42, 17.32s/it]

 66%|██████▌   | 7112/10740 [35:18:02<18:14:11, 18.10s/it]

 66%|██████▌   | 7113/10740 [35:18:16<16:55:27, 16.80s/it]

 66%|██████▌   | 7114/10740 [35:18:27<15:03:38, 14.95s/it]

 66%|██████▌   | 7115/10740 [35:18:40<14:33:44, 14.46s/it]

 66%|██████▋   | 7116/10740 [35:18:57<15:16:53, 15.18s/it]


 66%|██████▋   | 7118/10740 [35:19:28<15:38:49, 15.55s/it]

 66%|██████▋   | 7119/10740 [35:19:46<16:23:56, 16.30s/it]
{'loss': 0.3924, 'learning_rate': 5.393180351913435e-07, 'rewards/chosen': -1.910102367401123, 'rewards/rejected': -3.0607736110687256, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1506712436676025, 'policy_logps/rejected': -251.0327911376953, 'policy_logps/chosen': -257.1054992675781, 'referece_logps/rejected': -220.42506408691406, 'referece_logps/chosen': -238.00445556640625, 'logits/rejected': -0.41621774435043335, 'logits/chosen': -0.29872262477874756, 'epoch': 3.98}

 66%|██████▋   | 7120/10740 [35:20:07<17:43:39, 17.63s/it]


 66%|██████▋   | 7122/10740 [35:20:38<16:51:09, 16.77s/it]

 66%|██████▋   | 7123/10740 [35:21:00<18:26:44, 18.36s/it]

 66%|██████▋   | 7124/10740 [35:21:20<18:51:15, 18.77s/it]

 66%|██████▋   | 7125/10740 [35:21:33<17:18:31, 17.24s/it]

 66%|██████▋   | 7126/10740 [35:21:53<18:11:37, 18.12s/it]
[2024-04-03 06:35:37,196] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 7127/10740 [35:22:06<16:31:06, 16.46s/it]

 66%|██████▋   | 7128/10740 [35:22:18<15:09:28, 15.11s/it]

 66%|██████▋   | 7129/10740 [35:22:34<15:17:11, 15.24s/it]

 66%|██████▋   | 7130/10740 [35:22:51<16:04:57, 16.04s/it]
{'loss': 0.4348, 'learning_rate': 5.363761525658899e-07, 'rewards/chosen': -1.113189458847046, 'rewards/rejected': -2.3624627590179443, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2492731809616089, 'policy_logps/rejected': -504.9029235839844, 'policy_logps/chosen': -282.79833984375, 'referece_logps/rejected': -481.2782897949219, 'referece_logps/chosen': -271.66644287109375, 'logits/rejected': -0.6199901103973389, 'logits/chosen': -0.4827958047389984, 'epoch': 3.98}


 66%|██████▋   | 7132/10740 [35:23:27<17:19:33, 17.29s/it]

 66%|██████▋   | 7133/10740 [35:23:48<18:14:40, 18.21s/it]

 66%|██████▋   | 7134/10740 [35:24:09<19:15:16, 19.22s/it]
{'loss': 0.4429, 'learning_rate': 5.35307640395367e-07, 'rewards/chosen': -1.7407548427581787, 'rewards/rejected': -2.0542452335357666, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3134903907775879, 'policy_logps/rejected': -326.8312683105469, 'policy_logps/chosen': -427.44671630859375, 'referece_logps/rejected': -306.2888488769531, 'referece_logps/chosen': -410.0391540527344, 'logits/rejected': -0.12216299772262573, 'logits/chosen': -0.019576765596866608, 'epoch': 3.99}


 66%|██████▋   | 7136/10740 [35:24:52<20:20:31, 20.32s/it]

 66%|██████▋   | 7137/10740 [35:25:06<18:26:34, 18.43s/it]

 66%|██████▋   | 7138/10740 [35:25:26<18:49:12, 18.81s/it]

 66%|██████▋   | 7139/10740 [35:25:44<18:30:05, 18.50s/it]
{'loss': 0.3377, 'learning_rate': 5.339729513247011e-07, 'rewards/chosen': -1.806361198425293, 'rewards/rejected': -3.728119134902954, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9217579364776611, 'policy_logps/rejected': -317.7051696777344, 'policy_logps/chosen': -310.18487548828125, 'referece_logps/rejected': -280.4239807128906, 'referece_logps/chosen': -292.1213073730469, 'logits/rejected': -0.2696109116077423, 'logits/chosen': -0.4188173711299896, 'epoch': 3.99}


 66%|██████▋   | 7141/10740 [35:26:15<16:51:47, 16.87s/it]
{'loss': 0.5357, 'learning_rate': 5.334393722306315e-07, 'rewards/chosen': -1.3269680738449097, 'rewards/rejected': -2.4122865200042725, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0853180885314941, 'policy_logps/rejected': -334.4024963378906, 'policy_logps/chosen': -439.72802734375, 'referece_logps/rejected': -310.27960205078125, 'referece_logps/chosen': -426.4583740234375, 'logits/rejected': -0.3992689847946167, 'logits/chosen': -0.27308183908462524, 'epoch': 3.99}


 67%|██████▋   | 7143/10740 [35:26:50<17:06:40, 17.13s/it]
{'loss': 0.4395, 'learning_rate': 5.329059628758914e-07, 'rewards/chosen': -1.7296375036239624, 'rewards/rejected': -2.76521372795105, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0355761051177979, 'policy_logps/rejected': -504.2986145019531, 'policy_logps/chosen': -397.9112854003906, 'referece_logps/rejected': -476.64654541015625, 'referece_logps/chosen': -380.6148681640625, 'logits/rejected': -0.5620970129966736, 'logits/chosen': -0.44474121928215027, 'epoch': 3.99}

 67%|██████▋   | 7144/10740 [35:27:05<16:29:22, 16.51s/it]


 67%|██████▋   | 7146/10740 [35:27:44<18:08:24, 18.17s/it]

 67%|██████▋   | 7147/10740 [35:28:06<19:03:13, 19.09s/it]

 67%|██████▋   | 7148/10740 [35:28:24<18:47:33, 18.83s/it]

 67%|██████▋   | 7149/10740 [35:28:42<18:42:39, 18.76s/it]
{'loss': 0.3683, 'learning_rate': 5.31306755187934e-07, 'rewards/chosen': -1.4360289573669434, 'rewards/rejected': -3.8609185218811035, 'rewards/accuracies': 0.875, 'rewards/margins': 2.42488956451416, 'policy_logps/rejected': -530.9295043945312, 'policy_logps/chosen': -353.1093444824219, 'referece_logps/rejected': -492.3203125, 'referece_logps/chosen': -338.7490539550781, 'logits/rejected': -1.0023781061172485, 'logits/chosen': -0.9922969341278076, 'epoch': 3.99}

 67%|██████▋   | 7150/10740 [35:28:59<18:05:47, 18.15s/it]

 67%|██████▋   | 7151/10740 [35:29:15<17:30:18, 17.56s/it]

 67%|██████▋   | 7152/10740 [35:29:35<18:09:22, 18.22s/it]


 67%|██████▋   | 7154/10740 [35:30:14<18:38:56, 18.72s/it]
{'loss': 0.3763, 'learning_rate': 5.29975254184259e-07, 'rewards/chosen': -0.5179885029792786, 'rewards/rejected': -3.359706163406372, 'rewards/accuracies': 1.0, 'rewards/margins': 2.841717481613159, 'policy_logps/rejected': -371.2586975097656, 'policy_logps/chosen': -372.2486877441406, 'referece_logps/rejected': -337.6615905761719, 'referece_logps/chosen': -367.0688171386719, 'logits/rejected': -0.9962353706359863, 'logits/chosen': -0.8915910720825195, 'epoch': 4.0}
[2024-04-03 06:44:14,612] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 7156/10740 [35:30:50<18:34:20, 18.66s/it]
{'loss': 0.4923, 'learning_rate': 5.294429528625949e-07, 'rewards/chosen': -2.1874961853027344, 'rewards/rejected': -3.2936062812805176, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1061103343963623, 'policy_logps/rejected': -508.37200927734375, 'policy_logps/chosen': -488.4638671875, 'referece_logps/rejected': -475.4360046386719, 'referece_logps/chosen': -466.58892822265625, 'logits/rejected': -0.4051143229007721, 'logits/chosen': -0.34287089109420776, 'epoch': 4.0}


 67%|██████▋   | 7158/10740 [35:31:22<16:58:14, 17.06s/it]

 67%|██████▋   | 7159/10740 [35:31:44<18:19:19, 18.42s/it]
[2024-04-03 06:45:27,728] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4363, 'learning_rate': 5.286448219279732e-07, 'rewards/chosen': -1.8411582708358765, 'rewards/rejected': -3.370811700820923, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5296530723571777, 'policy_logps/rejected': -572.5603637695312, 'policy_logps/chosen': -611.25830078125, 'referece_logps/rejected': -538.8522338867188, 'referece_logps/chosen': -592.8467407226562, 'logits/rejected': -0.014569245278835297, 'logits/chosen': -0.14382126927375793, 'epoch': 4.0}
[2024-04-03 06:45:43,076] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 7160/10740 [35:31:59<17:24:02, 17.50s/it]

 67%|██████▋   | 7161/10740 [35:32:16<17:03:46, 17.16s/it]


 67%|██████▋   | 7163/10740 [35:32:50<16:32:01, 16.64s/it]

 67%|██████▋   | 7164/10740 [35:33:04<15:44:29, 15.85s/it]

 67%|██████▋   | 7165/10740 [35:33:21<15:53:19, 16.00s/it]
{'loss': 0.3295, 'learning_rate': 5.270497182262251e-07, 'rewards/chosen': -1.8805102109909058, 'rewards/rejected': -3.706876039505005, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8263660669326782, 'policy_logps/rejected': -390.9402770996094, 'policy_logps/chosen': -329.370849609375, 'referece_logps/rejected': -353.87152099609375, 'referece_logps/chosen': -310.5657043457031, 'logits/rejected': 0.11429613828659058, 'logits/chosen': 0.05335932970046997, 'epoch': 4.0}


 67%|██████▋   | 7167/10740 [35:33:52<15:49:34, 15.95s/it]
{'loss': 0.3852, 'learning_rate': 5.2651836086235e-07, 'rewards/chosen': -1.7558830976486206, 'rewards/rejected': -3.935513734817505, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1796305179595947, 'policy_logps/rejected': -348.5086364746094, 'policy_logps/chosen': -403.08306884765625, 'referece_logps/rejected': -309.1535339355469, 'referece_logps/chosen': -385.52423095703125, 'logits/rejected': -0.26762938499450684, 'logits/chosen': -0.2464539110660553, 'epoch': 4.0}

 67%|██████▋   | 7168/10740 [35:34:09<16:12:48, 16.34s/it]


 67%|██████▋   | 7170/10740 [35:34:49<17:40:33, 17.82s/it]

 67%|██████▋   | 7171/10740 [35:35:03<16:37:27, 16.77s/it]
{'loss': 0.3679, 'learning_rate': 5.254561630996339e-07, 'rewards/chosen': -1.6538653373718262, 'rewards/rejected': -3.1325135231018066, 'rewards/accuracies': 1.0, 'rewards/margins': 1.478648066520691, 'policy_logps/rejected': -319.13134765625, 'policy_logps/chosen': -243.53836059570312, 'referece_logps/rejected': -287.80621337890625, 'referece_logps/chosen': -226.99969482421875, 'logits/rejected': -0.623753011226654, 'logits/chosen': -0.7785996794700623, 'epoch': 4.01}

 67%|██████▋   | 7172/10740 [35:35:19<16:28:18, 16.62s/it]

 67%|██████▋   | 7173/10740 [35:35:38<17:07:17, 17.28s/it]

 67%|██████▋   | 7174/10740 [35:35:50<15:28:07, 15.62s/it]

 67%|██████▋   | 7175/10740 [35:36:08<16:14:19, 16.40s/it]


 67%|██████▋   | 7177/10740 [35:36:40<16:27:24, 16.63s/it]
[2024-04-03 06:50:23,857] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2858, 'learning_rate': 5.238641617659578e-07, 'rewards/chosen': -1.1781587600708008, 'rewards/rejected': -4.069252014160156, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8910927772521973, 'policy_logps/rejected': -466.8876953125, 'policy_logps/chosen': -341.3506164550781, 'referece_logps/rejected': -426.1951904296875, 'referece_logps/chosen': -329.56903076171875, 'logits/rejected': -0.6425435543060303, 'logits/chosen': -0.8156008720397949, 'epoch': 4.01}

 67%|██████▋   | 7178/10740 [35:36:59<17:12:42, 17.40s/it]


 67%|██████▋   | 7180/10740 [35:37:41<18:55:10, 19.13s/it]
[2024-04-03 06:51:24,447] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 7181/10740 [35:37:57<18:04:49, 18.29s/it]
[2024-04-03 06:51:40,768] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3595, 'learning_rate': 5.228036933361603e-07, 'rewards/chosen': -2.0466389656066895, 'rewards/rejected': -3.1509714126586914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.104332447052002, 'policy_logps/rejected': -445.516357421875, 'policy_logps/chosen': -486.9827880859375, 'referece_logps/rejected': -414.00665283203125, 'referece_logps/chosen': -466.516357421875, 'logits/rejected': -0.14745648205280304, 'logits/chosen': -0.1594618856906891, 'epoch': 4.01}

 67%|██████▋   | 7182/10740 [35:38:15<18:07:10, 18.33s/it]


 67%|██████▋   | 7184/10740 [35:38:38<14:43:33, 14.91s/it]

 67%|██████▋   | 7185/10740 [35:38:52<14:25:01, 14.60s/it]

 67%|██████▋   | 7186/10740 [35:39:09<14:53:43, 15.09s/it]

 67%|██████▋   | 7187/10740 [35:39:28<16:18:42, 16.53s/it]

 67%|██████▋   | 7188/10740 [35:39:41<15:06:06, 15.31s/it]

 67%|██████▋   | 7189/10740 [35:40:00<16:22:27, 16.60s/it]

 67%|██████▋   | 7190/10740 [35:40:19<16:52:51, 17.12s/it]
{'loss': 0.3358, 'learning_rate': 5.204201807454161e-07, 'rewards/chosen': -1.3722432851791382, 'rewards/rejected': -3.9519455432891846, 'rewards/accuracies': 1.0, 'rewards/margins': 2.579702615737915, 'policy_logps/rejected': -531.1322021484375, 'policy_logps/chosen': -443.0531005859375, 'referece_logps/rejected': -491.6127014160156, 'referece_logps/chosen': -429.3306579589844, 'logits/rejected': -1.1313868761062622, 'logits/chosen': -0.8929240703582764, 'epoch': 4.02}

 67%|██████▋   | 7191/10740 [35:40:35<16:43:10, 16.96s/it]

 67%|██████▋   | 7192/10740 [35:40:54<17:13:47, 17.48s/it]


 67%|██████▋   | 7194/10740 [35:41:33<18:05:56, 18.37s/it]
{'loss': 0.3051, 'learning_rate': 5.19361974865645e-07, 'rewards/chosen': -1.8790770769119263, 'rewards/rejected': -3.0274243354797363, 'rewards/accuracies': 0.875, 'rewards/margins': 1.14834725856781, 'policy_logps/rejected': -320.4184265136719, 'policy_logps/chosen': -286.7638244628906, 'referece_logps/rejected': -290.1441955566406, 'referece_logps/chosen': -267.9730529785156, 'logits/rejected': -0.6838513016700745, 'logits/chosen': -0.7254741787910461, 'epoch': 4.02}
[2024-04-03 06:55:38,145] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 7196/10740 [35:42:14<19:16:40, 19.58s/it]
{'loss': 0.2655, 'learning_rate': 5.188331341207659e-07, 'rewards/chosen': -0.3989129066467285, 'rewards/rejected': -3.900066375732422, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5011534690856934, 'policy_logps/rejected': -367.6234436035156, 'policy_logps/chosen': -394.837890625, 'referece_logps/rejected': -328.6227722167969, 'referece_logps/chosen': -390.8487243652344, 'logits/rejected': -0.8494824171066284, 'logits/chosen': -0.8695781230926514, 'epoch': 4.02}

 67%|██████▋   | 7197/10740 [35:42:34<19:18:05, 19.61s/it]


 67%|██████▋   | 7199/10740 [35:43:05<16:55:09, 17.20s/it]
{'loss': 0.5324, 'learning_rate': 5.180402012883451e-07, 'rewards/chosen': -2.0761990547180176, 'rewards/rejected': -4.013080596923828, 'rewards/accuracies': 0.5, 'rewards/margins': 1.936881184577942, 'policy_logps/rejected': -408.8231506347656, 'policy_logps/chosen': -362.0177307128906, 'referece_logps/rejected': -368.6923522949219, 'referece_logps/chosen': -341.2557373046875, 'logits/rejected': -0.43360498547554016, 'logits/chosen': -0.4645104706287384, 'epoch': 4.02}

 67%|██████▋   | 7200/10740 [35:43:24<17:38:35, 17.94s/it]

 67%|██████▋   | 7201/10740 [35:43:44<18:04:17, 18.38s/it]


 67%|██████▋   | 7203/10740 [35:44:19<17:27:16, 17.77s/it]
{'loss': 0.3611, 'learning_rate': 5.169835713198445e-07, 'rewards/chosen': -2.4517111778259277, 'rewards/rejected': -3.5174872875213623, 'rewards/accuracies': 0.5, 'rewards/margins': 1.0657758712768555, 'policy_logps/rejected': -362.18719482421875, 'policy_logps/chosen': -322.14312744140625, 'referece_logps/rejected': -327.0122985839844, 'referece_logps/chosen': -297.62603759765625, 'logits/rejected': -0.7130599617958069, 'logits/chosen': -0.750953197479248, 'epoch': 4.02}


 67%|██████▋   | 7205/10740 [35:44:51<17:00:08, 17.31s/it]
{'loss': 0.3644, 'learning_rate': 5.164555198286744e-07, 'rewards/chosen': -1.3358463048934937, 'rewards/rejected': -3.7881171703338623, 'rewards/accuracies': 0.75, 'rewards/margins': 2.452270984649658, 'policy_logps/rejected': -301.9153747558594, 'policy_logps/chosen': -428.87957763671875, 'referece_logps/rejected': -264.0342102050781, 'referece_logps/chosen': -415.5211181640625, 'logits/rejected': -0.2969060242176056, 'logits/chosen': -0.564667284488678, 'epoch': 4.03}

 67%|██████▋   | 7206/10740 [35:45:13<18:06:16, 18.44s/it]

 67%|██████▋   | 7207/10740 [35:45:27<16:47:54, 17.12s/it]

 67%|██████▋   | 7208/10740 [35:45:48<18:03:52, 18.41s/it]

 67%|██████▋   | 7209/10740 [35:45:59<15:48:50, 16.12s/it]

 67%|██████▋   | 7210/10740 [35:46:11<14:34:59, 14.87s/it]

 67%|██████▋   | 7211/10740 [35:46:26<14:43:39, 15.02s/it]

 67%|██████▋   | 7212/10740 [35:46:38<13:54:56, 14.20s/it]

 67%|██████▋   | 7213/10740 [35:46:51<13:28:50, 13.76s/it]

 67%|██████▋   | 7214/10740 [35:47:08<14:28:24, 14.78s/it]

 67%|██████▋   | 7215/10740 [35:47:26<15:19:28, 15.65s/it]

 67%|██████▋   | 7216/10740 [35:47:41<15:07:51, 15.46s/it]
[2024-04-03 07:01:47,756] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 7217/10740 [35:48:04<17:20:53, 17.73s/it]
[2024-04-03 07:02:06,802] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 7219/10740 [35:48:44<18:27:18, 18.87s/it]
{'loss': 0.3671, 'learning_rate': 5.127640958507796e-07, 'rewards/chosen': -2.826638698577881, 'rewards/rejected': -3.7967236042022705, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9700847864151001, 'policy_logps/rejected': -341.7688293457031, 'policy_logps/chosen': -296.3357849121094, 'referece_logps/rejected': -303.8016357421875, 'referece_logps/chosen': -268.0693664550781, 'logits/rejected': -0.6881941556930542, 'logits/chosen': -0.6424238681793213, 'epoch': 4.03}

 67%|██████▋   | 7220/10740 [35:49:05<19:08:37, 19.58s/it]

 67%|██████▋   | 7221/10740 [35:49:25<19:22:03, 19.81s/it]

 67%|██████▋   | 7222/10740 [35:49:45<19:19:18, 19.77s/it]

 67%|██████▋   | 7223/10740 [35:50:00<17:57:13, 18.38s/it]

 67%|██████▋   | 7224/10740 [35:50:18<17:53:06, 18.31s/it]
[2024-04-03 07:04:21,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 7225/10740 [35:50:38<18:17:08, 18.73s/it]

 67%|██████▋   | 7226/10740 [35:50:49<16:08:11, 16.53s/it]

 67%|██████▋   | 7227/10740 [35:51:09<17:02:07, 17.46s/it]

 67%|██████▋   | 7228/10740 [35:51:23<15:58:20, 16.37s/it]

 67%|██████▋   | 7229/10740 [35:51:37<15:11:40, 15.58s/it]

 67%|██████▋   | 7230/10740 [35:51:57<16:44:42, 17.17s/it]

 67%|██████▋   | 7231/10740 [35:52:17<17:29:27, 17.94s/it]
[2024-04-03 07:06:23,963] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 7233/10740 [35:53:00<19:02:49, 19.55s/it]
[2024-04-03 07:06:43,682] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2573, 'learning_rate': 5.090813576589243e-07, 'rewards/chosen': -1.4658095836639404, 'rewards/rejected': -3.1361632347106934, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6703534126281738, 'policy_logps/rejected': -440.5108947753906, 'policy_logps/chosen': -406.9398193359375, 'referece_logps/rejected': -409.1492614746094, 'referece_logps/chosen': -392.2817077636719, 'logits/rejected': -0.7899628281593323, 'logits/chosen': -0.7055986523628235, 'epoch': 4.04}
[2024-04-03 07:07:06,143] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 7234/10740 [35:53:22<19:53:30, 20.43s/it]
[2024-04-03 07:07:26,330] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 7236/10740 [35:53:56<17:47:10, 18.27s/it]

 67%|██████▋   | 7237/10740 [35:54:16<18:13:13, 18.72s/it]
{'loss': 0.301, 'learning_rate': 5.080307512866084e-07, 'rewards/chosen': -1.490400791168213, 'rewards/rejected': -3.0244295597076416, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5340287685394287, 'policy_logps/rejected': -239.92567443847656, 'policy_logps/chosen': -263.7884216308594, 'referece_logps/rejected': -209.68138122558594, 'referece_logps/chosen': -248.88442993164062, 'logits/rejected': -1.4664216041564941, 'logits/chosen': -1.4363402128219604, 'epoch': 4.04}

 67%|██████▋   | 7238/10740 [35:54:32<17:36:04, 18.09s/it]

 67%|██████▋   | 7239/10740 [35:54:53<18:15:12, 18.77s/it]
[2024-04-03 07:08:52,631] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 7240/10740 [35:55:09<17:28:47, 17.98s/it]


 67%|██████▋   | 7242/10740 [35:55:48<18:15:19, 18.79s/it]
{'loss': 0.3534, 'learning_rate': 5.067185002808838e-07, 'rewards/chosen': -1.279400110244751, 'rewards/rejected': -6.197153568267822, 'rewards/accuracies': 1.0, 'rewards/margins': 4.91775369644165, 'policy_logps/rejected': -437.41925048828125, 'policy_logps/chosen': -369.0265197753906, 'referece_logps/rejected': -375.44769287109375, 'referece_logps/chosen': -356.2325134277344, 'logits/rejected': -0.9727784395217896, 'logits/chosen': -1.0408849716186523, 'epoch': 4.05}

 67%|██████▋   | 7243/10740 [35:56:00<16:20:10, 16.82s/it]

 67%|██████▋   | 7244/10740 [35:56:19<16:49:52, 17.33s/it]

 67%|██████▋   | 7245/10740 [35:56:39<17:30:43, 18.04s/it]

 67%|██████▋   | 7246/10740 [35:56:57<17:44:03, 18.27s/it]

 67%|██████▋   | 7247/10740 [35:57:15<17:35:52, 18.14s/it]

 67%|██████▋   | 7248/10740 [35:57:33<17:32:14, 18.08s/it]

 67%|██████▋   | 7249/10740 [35:57:51<17:35:14, 18.14s/it]

 68%|██████▊   | 7250/10740 [35:58:10<17:40:39, 18.23s/it]
[2024-04-03 07:12:15,726] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 7251/10740 [35:58:32<18:47:30, 19.39s/it]


 68%|██████▊   | 7253/10740 [35:59:06<17:38:14, 18.21s/it]
{'loss': 0.3768, 'learning_rate': 5.038355003992737e-07, 'rewards/chosen': -1.2971715927124023, 'rewards/rejected': -2.9905169010162354, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6933454275131226, 'policy_logps/rejected': -265.3795471191406, 'policy_logps/chosen': -210.34068298339844, 'referece_logps/rejected': -235.474365234375, 'referece_logps/chosen': -197.3689422607422, 'logits/rejected': -0.01237083226442337, 'logits/chosen': -0.10041830688714981, 'epoch': 4.05}

 68%|██████▊   | 7254/10740 [35:59:23<17:16:04, 17.83s/it]

 68%|██████▊   | 7255/10740 [35:59:43<17:41:23, 18.27s/it]


 68%|██████▊   | 7257/10740 [36:00:10<15:21:18, 15.87s/it]
{'loss': 0.4283, 'learning_rate': 5.027884889592902e-07, 'rewards/chosen': -2.2645761966705322, 'rewards/rejected': -4.480774402618408, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2161977291107178, 'policy_logps/rejected': -511.1172790527344, 'policy_logps/chosen': -507.68304443359375, 'referece_logps/rejected': -466.3095703125, 'referece_logps/chosen': -485.0372619628906, 'logits/rejected': -0.12127432227134705, 'logits/chosen': -0.16707509756088257, 'epoch': 4.05}


 68%|██████▊   | 7259/10740 [36:00:42<15:32:04, 16.07s/it]
{'loss': 0.5173, 'learning_rate': 5.022652544797177e-07, 'rewards/chosen': -1.9125049114227295, 'rewards/rejected': -3.2307112216949463, 'rewards/accuracies': 0.875, 'rewards/margins': 1.318206548690796, 'policy_logps/rejected': -452.00311279296875, 'policy_logps/chosen': -402.7882080078125, 'referece_logps/rejected': -419.69598388671875, 'referece_logps/chosen': -383.6632080078125, 'logits/rejected': -0.47251030802726746, 'logits/chosen': -0.3073634207248688, 'epoch': 4.06}

 68%|██████▊   | 7260/10740 [36:00:57<15:04:20, 15.59s/it]

 68%|██████▊   | 7261/10740 [36:01:08<13:45:20, 14.23s/it]


 68%|██████▊   | 7263/10740 [36:01:42<15:17:01, 15.82s/it]

 68%|██████▊   | 7264/10740 [36:02:05<17:06:18, 17.72s/it]

 68%|██████▊   | 7265/10740 [36:02:20<16:31:38, 17.12s/it]
{'loss': 0.4754, 'learning_rate': 5.00696638286777e-07, 'rewards/chosen': -1.6878130435943604, 'rewards/rejected': -3.1055445671081543, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4177320003509521, 'policy_logps/rejected': -469.7897033691406, 'policy_logps/chosen': -435.46710205078125, 'referece_logps/rejected': -438.73431396484375, 'referece_logps/chosen': -418.5889587402344, 'logits/rejected': -0.6013840436935425, 'logits/chosen': -0.6486988663673401, 'epoch': 4.06}

 68%|██████▊   | 7266/10740 [36:02:34<15:34:18, 16.14s/it]

 68%|██████▊   | 7267/10740 [36:02:50<15:24:12, 15.97s/it]
[2024-04-03 07:16:57,000] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 7269/10740 [36:03:32<17:51:13, 18.52s/it]
{'loss': 0.3658, 'learning_rate': 4.996518020983901e-07, 'rewards/chosen': -1.548365592956543, 'rewards/rejected': -4.14414644241333, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5957813262939453, 'policy_logps/rejected': -506.4424133300781, 'policy_logps/chosen': -422.95330810546875, 'referece_logps/rejected': -465.00091552734375, 'referece_logps/chosen': -407.46966552734375, 'logits/rejected': -0.5773440003395081, 'logits/chosen': -0.6393399238586426, 'epoch': 4.06}

 68%|██████▊   | 7270/10740 [36:03:50<17:29:58, 18.16s/it]

 68%|██████▊   | 7271/10740 [36:04:03<16:05:40, 16.70s/it]

 68%|██████▊   | 7272/10740 [36:04:19<15:58:52, 16.59s/it]

 68%|██████▊   | 7273/10740 [36:04:32<14:55:38, 15.50s/it]

 68%|██████▊   | 7274/10740 [36:04:47<14:45:55, 15.34s/it]

 68%|██████▊   | 7275/10740 [36:05:06<15:39:01, 16.26s/it]

 68%|██████▊   | 7276/10740 [36:05:23<16:00:39, 16.64s/it]

 68%|██████▊   | 7277/10740 [36:05:40<15:57:58, 16.60s/it]

 68%|██████▊   | 7278/10740 [36:05:59<16:50:07, 17.51s/it]

 68%|██████▊   | 7279/10740 [36:06:14<16:07:03, 16.77s/it]

 68%|██████▊   | 7280/10740 [36:06:34<17:02:35, 17.73s/it]

 68%|██████▊   | 7281/10740 [36:06:54<17:35:28, 18.31s/it]

 68%|██████▊   | 7282/10740 [36:07:09<16:38:39, 17.33s/it]

 68%|██████▊   | 7283/10740 [36:07:30<17:45:51, 18.50s/it]

 68%|██████▊   | 7284/10740 [36:07:47<17:07:06, 17.83s/it]

 68%|██████▊   | 7285/10740 [36:08:04<17:03:07, 17.77s/it]

 68%|██████▊   | 7286/10740 [36:08:24<17:46:02, 18.52s/it]

 68%|██████▊   | 7287/10740 [36:08:38<16:25:09, 17.12s/it]

 68%|██████▊   | 7288/10740 [36:08:54<15:53:06, 16.57s/it]

 68%|██████▊   | 7289/10740 [36:09:06<14:43:19, 15.36s/it]

 68%|██████▊   | 7290/10740 [36:09:27<16:26:19, 17.15s/it]

 68%|██████▊   | 7291/10740 [36:09:46<16:45:51, 17.50s/it]

 68%|██████▊   | 7292/10740 [36:10:05<17:09:23, 17.91s/it]

 68%|██████▊   | 7293/10740 [36:10:25<17:46:08, 18.56s/it]


 68%|██████▊   | 7295/10740 [36:11:01<17:25:41, 18.21s/it]

 68%|██████▊   | 7296/10740 [36:11:13<15:38:53, 16.36s/it]
{'loss': 0.3967, 'learning_rate': 4.926182789887825e-07, 'rewards/chosen': -1.7456485033035278, 'rewards/rejected': -3.3522167205810547, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6065680980682373, 'policy_logps/rejected': -369.98309326171875, 'policy_logps/chosen': -361.52484130859375, 'referece_logps/rejected': -336.4609375, 'referece_logps/chosen': -344.0683288574219, 'logits/rejected': -0.5739333629608154, 'logits/chosen': -0.5841295719146729, 'epoch': 4.08}

 68%|██████▊   | 7297/10740 [36:11:31<16:14:50, 16.99s/it]

 68%|██████▊   | 7298/10740 [36:11:46<15:23:27, 16.10s/it]

 68%|██████▊   | 7299/10740 [36:12:02<15:34:30, 16.29s/it]

 68%|██████▊   | 7300/10740 [36:12:23<16:43:39, 17.51s/it]

 68%|██████▊   | 7301/10740 [36:12:34<15:04:47, 15.79s/it]


 68%|██████▊   | 7303/10740 [36:13:09<15:41:37, 16.44s/it]

 68%|██████▊   | 7304/10740 [36:13:31<17:17:33, 18.12s/it]
{'loss': 0.5677, 'learning_rate': 4.905407118210641e-07, 'rewards/chosen': -1.8356447219848633, 'rewards/rejected': -1.8446755409240723, 'rewards/accuracies': 0.5, 'rewards/margins': 0.009030744433403015, 'policy_logps/rejected': -406.9902038574219, 'policy_logps/chosen': -371.250732421875, 'referece_logps/rejected': -388.5434265136719, 'referece_logps/chosen': -352.894287109375, 'logits/rejected': -1.5816471576690674, 'logits/chosen': -1.6970195770263672, 'epoch': 4.08}

 68%|██████▊   | 7305/10740 [36:13:50<17:22:40, 18.21s/it]

 68%|██████▊   | 7306/10740 [36:14:02<15:48:35, 16.57s/it]


 68%|██████▊   | 7308/10740 [36:14:39<16:41:53, 17.52s/it]
{'loss': 0.319, 'learning_rate': 4.895030395592287e-07, 'rewards/chosen': -1.3771780729293823, 'rewards/rejected': -3.0485758781433105, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6713979244232178, 'policy_logps/rejected': -360.39520263671875, 'policy_logps/chosen': -424.9027404785156, 'referece_logps/rejected': -329.9094543457031, 'referece_logps/chosen': -411.1309814453125, 'logits/rejected': -0.5940800905227661, 'logits/chosen': -0.3775084912776947, 'epoch': 4.08}

 68%|██████▊   | 7309/10740 [36:15:00<17:39:15, 18.52s/it]

 68%|██████▊   | 7310/10740 [36:15:20<17:59:57, 18.89s/it]

 68%|██████▊   | 7311/10740 [36:15:34<16:40:14, 17.50s/it]

 68%|██████▊   | 7312/10740 [36:15:55<17:37:21, 18.51s/it]

 68%|██████▊   | 7313/10740 [36:16:08<15:55:27, 16.73s/it]

 68%|██████▊   | 7314/10740 [36:16:25<16:02:55, 16.86s/it]

 68%|██████▊   | 7315/10740 [36:16:38<14:55:52, 15.69s/it]

 68%|██████▊   | 7316/10740 [36:16:59<16:24:44, 17.26s/it]

 68%|██████▊   | 7317/10740 [36:17:17<16:51:27, 17.73s/it]

 68%|██████▊   | 7318/10740 [36:17:38<17:31:33, 18.44s/it]

 68%|██████▊   | 7319/10740 [36:17:59<18:31:09, 19.49s/it]

 68%|██████▊   | 7320/10740 [36:18:17<18:00:27, 18.96s/it]

 68%|██████▊   | 7321/10740 [36:18:33<17:02:03, 17.94s/it]

 68%|██████▊   | 7322/10740 [36:18:46<15:48:26, 16.65s/it]

 68%|██████▊   | 7323/10740 [36:19:03<15:47:03, 16.63s/it]

 68%|██████▊   | 7324/10740 [36:19:20<15:48:28, 16.66s/it]

 68%|██████▊   | 7325/10740 [36:19:38<16:13:57, 17.11s/it]

 68%|██████▊   | 7326/10740 [36:19:58<17:03:28, 17.99s/it]

 68%|██████▊   | 7327/10740 [36:20:16<17:04:25, 18.01s/it]

 68%|██████▊   | 7328/10740 [36:20:35<17:20:33, 18.30s/it]

 68%|██████▊   | 7329/10740 [36:20:50<16:24:07, 17.31s/it]

 68%|██████▊   | 7330/10740 [36:21:04<15:24:26, 16.27s/it]

 68%|██████▊   | 7331/10740 [36:21:23<16:21:22, 17.27s/it]

 68%|██████▊   | 7332/10740 [36:21:45<17:30:21, 18.49s/it]

 68%|██████▊   | 7333/10740 [36:22:05<18:02:45, 19.07s/it]

 68%|██████▊   | 7334/10740 [36:22:23<17:41:37, 18.70s/it]

 68%|██████▊   | 7335/10740 [36:22:45<18:39:15, 19.72s/it]

 68%|██████▊   | 7336/10740 [36:23:03<18:04:42, 19.12s/it]

 68%|██████▊   | 7337/10740 [36:23:24<18:41:37, 19.78s/it]

 68%|██████▊   | 7338/10740 [36:23:43<18:29:26, 19.57s/it]

 68%|██████▊   | 7339/10740 [36:24:03<18:30:54, 19.60s/it]

 68%|██████▊   | 7340/10740 [36:24:24<18:56:00, 20.05s/it]

 68%|██████▊   | 7341/10740 [36:24:36<16:47:21, 17.78s/it]

 68%|██████▊   | 7342/10740 [36:24:56<17:23:02, 18.42s/it]

 68%|██████▊   | 7343/10740 [36:25:16<17:43:06, 18.78s/it]

 68%|██████▊   | 7344/10740 [36:25:33<17:18:46, 18.35s/it]

 68%|██████▊   | 7345/10740 [36:25:53<17:40:51, 18.75s/it]
[2024-04-03 07:39:58,095] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 7346/10740 [36:26:14<18:24:23, 19.52s/it]

 68%|██████▊   | 7347/10740 [36:26:34<18:25:49, 19.55s/it]

 68%|██████▊   | 7348/10740 [36:26:52<17:54:42, 19.01s/it]

 68%|██████▊   | 7349/10740 [36:27:04<15:54:03, 16.88s/it]

 68%|██████▊   | 7350/10740 [36:27:19<15:34:28, 16.54s/it]

 68%|██████▊   | 7351/10740 [36:27:32<14:21:10, 15.25s/it]

 68%|██████▊   | 7352/10740 [36:27:42<13:01:09, 13.83s/it]

 68%|██████▊   | 7353/10740 [36:28:01<14:31:43, 15.44s/it]

 68%|██████▊   | 7354/10740 [36:28:13<13:27:10, 14.30s/it]


 68%|██████▊   | 7356/10740 [36:28:45<14:44:11, 15.68s/it]

 69%|██████▊   | 7357/10740 [36:28:59<14:08:09, 15.04s/it]

 69%|██████▊   | 7358/10740 [36:29:15<14:26:20, 15.37s/it]

 69%|██████▊   | 7359/10740 [36:29:28<13:53:18, 14.79s/it]

 69%|██████▊   | 7360/10740 [36:29:43<13:48:43, 14.71s/it]
{'loss': 0.4109, 'learning_rate': 4.760814517916278e-07, 'rewards/chosen': -2.051647901535034, 'rewards/rejected': -2.964454174041748, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9128060340881348, 'policy_logps/rejected': -332.6947937011719, 'policy_logps/chosen': -301.74395751953125, 'referece_logps/rejected': -303.05023193359375, 'referece_logps/chosen': -281.22747802734375, 'logits/rejected': -1.1486777067184448, 'logits/chosen': -1.0032589435577393, 'epoch': 4.11}

 69%|██████▊   | 7361/10740 [36:30:03<15:13:00, 16.21s/it]


 69%|██████▊   | 7363/10740 [36:30:38<16:04:10, 17.13s/it]

 69%|██████▊   | 7364/10740 [36:30:56<16:16:15, 17.35s/it]

 69%|██████▊   | 7365/10740 [36:31:20<18:07:31, 19.33s/it]

 69%|██████▊   | 7366/10740 [36:31:38<17:45:03, 18.94s/it]
{'loss': 0.3899, 'learning_rate': 4.7454103595466877e-07, 'rewards/chosen': -1.4213197231292725, 'rewards/rejected': -3.2336597442626953, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8123401403427124, 'policy_logps/rejected': -472.63330078125, 'policy_logps/chosen': -426.9168395996094, 'referece_logps/rejected': -440.2967224121094, 'referece_logps/chosen': -412.70367431640625, 'logits/rejected': -0.13517999649047852, 'logits/chosen': 0.002286359667778015, 'epoch': 4.12}


 69%|██████▊   | 7368/10740 [36:32:06<15:15:57, 16.30s/it]
{'loss': 0.3979, 'learning_rate': 4.740279460942531e-07, 'rewards/chosen': -1.764318585395813, 'rewards/rejected': -2.677785634994507, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9134671092033386, 'policy_logps/rejected': -301.243408203125, 'policy_logps/chosen': -326.3162841796875, 'referece_logps/rejected': -274.4655456542969, 'referece_logps/chosen': -308.6731262207031, 'logits/rejected': -0.8455938100814819, 'logits/chosen': -0.8914759755134583, 'epoch': 4.12}


 69%|██████▊   | 7370/10740 [36:32:31<13:21:26, 14.27s/it]

 69%|██████▊   | 7371/10740 [36:32:54<15:48:27, 16.89s/it]

 69%|██████▊   | 7372/10740 [36:33:09<15:26:25, 16.50s/it]

 69%|██████▊   | 7373/10740 [36:33:26<15:29:11, 16.56s/it]

 69%|██████▊   | 7374/10740 [36:33:40<14:36:06, 15.62s/it]

 69%|██████▊   | 7375/10740 [36:33:59<15:31:53, 16.62s/it]
{'loss': 0.3682, 'learning_rate': 4.7223363971825647e-07, 'rewards/chosen': -2.197467565536499, 'rewards/rejected': -3.928131103515625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7306632995605469, 'policy_logps/rejected': -449.2447509765625, 'policy_logps/chosen': -478.7474365234375, 'referece_logps/rejected': -409.9634704589844, 'referece_logps/chosen': -456.772705078125, 'logits/rejected': -0.15648287534713745, 'logits/chosen': -0.27945056557655334, 'epoch': 4.12}


 69%|██████▊   | 7377/10740 [36:34:29<14:58:37, 16.03s/it]

 69%|██████▊   | 7378/10740 [36:34:51<16:40:41, 17.86s/it]

 69%|██████▊   | 7379/10740 [36:35:03<15:03:54, 16.14s/it]

 69%|██████▊   | 7380/10740 [36:35:22<15:38:59, 16.77s/it]
{'loss': 0.3572, 'learning_rate': 4.7095343188978876e-07, 'rewards/chosen': -1.9923816919326782, 'rewards/rejected': -4.062218189239502, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0698366165161133, 'policy_logps/rejected': -392.89776611328125, 'policy_logps/chosen': -370.5787048339844, 'referece_logps/rejected': -352.2756042480469, 'referece_logps/chosen': -350.6549072265625, 'logits/rejected': -0.574838399887085, 'logits/chosen': -0.56118243932724, 'epoch': 4.12}


 69%|██████▊   | 7382/10740 [36:35:58<16:09:12, 17.32s/it]

 69%|██████▊   | 7383/10740 [36:36:20<17:15:07, 18.50s/it]

 69%|██████▉   | 7384/10740 [36:36:39<17:31:23, 18.80s/it]

 69%|██████▉   | 7385/10740 [36:36:58<17:31:16, 18.80s/it]

 69%|██████▉   | 7386/10740 [36:37:16<17:22:21, 18.65s/it]

 69%|██████▉   | 7387/10740 [36:37:37<18:05:09, 19.42s/it]

 69%|██████▉   | 7388/10740 [36:37:57<18:13:08, 19.57s/it]

 69%|██████▉   | 7389/10740 [36:38:19<18:42:40, 20.10s/it]

 69%|██████▉   | 7390/10740 [36:38:36<17:51:34, 19.19s/it]

 69%|██████▉   | 7391/10740 [36:38:55<17:49:08, 19.15s/it]

 69%|██████▉   | 7392/10740 [36:39:16<18:16:55, 19.66s/it]

 69%|██████▉   | 7393/10740 [36:39:28<16:16:14, 17.50s/it]

 69%|██████▉   | 7394/10740 [36:39:44<15:42:47, 16.91s/it]

 69%|██████▉   | 7395/10740 [36:40:04<16:33:56, 17.83s/it]

 69%|██████▉   | 7396/10740 [36:40:23<16:54:12, 18.20s/it]

 69%|██████▉   | 7397/10740 [36:40:44<17:45:47, 19.13s/it]

 69%|██████▉   | 7398/10740 [36:41:04<18:00:03, 19.39s/it]

 69%|██████▉   | 7399/10740 [36:41:23<17:48:29, 19.19s/it]
{'loss': 0.356, 'learning_rate': 4.6609963780671744e-07, 'rewards/chosen': -2.032665967941284, 'rewards/rejected': -5.425964832305908, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3932993412017822, 'policy_logps/rejected': -417.7232666015625, 'policy_logps/chosen': -366.94537353515625, 'referece_logps/rejected': -363.46356201171875, 'referece_logps/chosen': -346.61871337890625, 'logits/rejected': -0.2242862582206726, 'logits/chosen': -0.17477795481681824, 'epoch': 4.13}

 69%|██████▉   | 7400/10740 [36:41:39<17:03:20, 18.38s/it]


 69%|██████▉   | 7402/10740 [36:42:19<17:45:37, 19.15s/it]

 69%|██████▉   | 7403/10740 [36:42:41<18:34:43, 20.04s/it]
{'loss': 0.3639, 'learning_rate': 4.650800148204906e-07, 'rewards/chosen': -1.2560700178146362, 'rewards/rejected': -4.159739017486572, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9036691188812256, 'policy_logps/rejected': -434.4761657714844, 'policy_logps/chosen': -369.77264404296875, 'referece_logps/rejected': -392.8787536621094, 'referece_logps/chosen': -357.2119445800781, 'logits/rejected': -1.0626988410949707, 'logits/chosen': -1.1276994943618774, 'epoch': 4.14}


 69%|██████▉   | 7405/10740 [36:43:13<16:50:47, 18.19s/it]

 69%|██████▉   | 7406/10740 [36:43:33<17:20:11, 18.72s/it]

 69%|██████▉   | 7407/10740 [36:43:51<17:15:34, 18.64s/it]

 69%|██████▉   | 7408/10740 [36:44:04<15:32:16, 16.79s/it]

 69%|██████▉   | 7409/10740 [36:44:23<16:20:25, 17.66s/it]

 69%|██████▉   | 7410/10740 [36:44:44<17:06:46, 18.50s/it]

 69%|██████▉   | 7411/10740 [36:45:01<16:42:13, 18.06s/it]

 69%|██████▉   | 7412/10740 [36:45:14<15:26:34, 16.70s/it]

 69%|██████▉   | 7413/10740 [36:45:32<15:41:26, 16.98s/it]

 69%|██████▉   | 7414/10740 [36:45:49<15:46:28, 17.07s/it]

 69%|██████▉   | 7415/10740 [36:46:06<15:46:05, 17.07s/it]

 69%|██████▉   | 7416/10740 [36:46:26<16:26:54, 17.81s/it]
{'loss': 0.303, 'learning_rate': 4.617716238699698e-07, 'rewards/chosen': -1.732969880104065, 'rewards/rejected': -4.330324649810791, 'rewards/accuracies': 0.75, 'rewards/margins': 2.597355365753174, 'policy_logps/rejected': -407.390625, 'policy_logps/chosen': -369.4964599609375, 'referece_logps/rejected': -364.08740234375, 'referece_logps/chosen': -352.166748046875, 'logits/rejected': -0.39749932289123535, 'logits/chosen': -0.46072518825531006, 'epoch': 4.14}


 69%|██████▉   | 7418/10740 [36:47:07<17:48:49, 19.30s/it]

 69%|██████▉   | 7419/10740 [36:47:25<17:31:51, 19.00s/it]

 69%|██████▉   | 7420/10740 [36:47:47<18:10:41, 19.71s/it]

 69%|██████▉   | 7421/10740 [36:48:05<17:45:08, 19.26s/it]

 69%|██████▉   | 7422/10740 [36:48:27<18:33:28, 20.14s/it]
{'loss': 0.3702, 'learning_rate': 4.6024746145504924e-07, 'rewards/chosen': -1.8887476921081543, 'rewards/rejected': -3.0468862056732178, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1581382751464844, 'policy_logps/rejected': -383.8915710449219, 'policy_logps/chosen': -472.01605224609375, 'referece_logps/rejected': -353.4227294921875, 'referece_logps/chosen': -453.1285400390625, 'logits/rejected': -0.8720802068710327, 'logits/chosen': -0.8997405767440796, 'epoch': 4.15}


 69%|██████▉   | 7424/10740 [36:49:05<17:48:20, 19.33s/it]
{'loss': 0.479, 'learning_rate': 4.597397998048949e-07, 'rewards/chosen': -2.133268356323242, 'rewards/rejected': -3.2624266147613525, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1291580200195312, 'policy_logps/rejected': -376.146240234375, 'policy_logps/chosen': -400.861328125, 'referece_logps/rejected': -343.52197265625, 'referece_logps/chosen': -379.52862548828125, 'logits/rejected': -0.4547087848186493, 'logits/chosen': -0.534042239189148, 'epoch': 4.15}


 69%|██████▉   | 7426/10740 [36:49:45<18:05:49, 19.66s/it]

 69%|██████▉   | 7427/10740 [36:49:57<15:51:29, 17.23s/it]

 69%|██████▉   | 7428/10740 [36:50:13<15:36:13, 16.96s/it]

 69%|██████▉   | 7429/10740 [36:50:35<16:51:38, 18.33s/it]
{'loss': 0.3578, 'learning_rate': 4.5847150599815496e-07, 'rewards/chosen': -1.8458245992660522, 'rewards/rejected': -3.0514800548553467, 'rewards/accuracies': 0.75, 'rewards/margins': 1.205655574798584, 'policy_logps/rejected': -326.9078063964844, 'policy_logps/chosen': -325.3287353515625, 'referece_logps/rejected': -296.39300537109375, 'referece_logps/chosen': -306.8704833984375, 'logits/rejected': -0.5575864911079407, 'logits/chosen': -0.4972706139087677, 'epoch': 4.15}

 69%|██████▉   | 7430/10740 [36:50:56<17:40:37, 19.23s/it]


 69%|██████▉   | 7432/10740 [36:51:33<17:14:32, 18.76s/it]
{'loss': 0.4084, 'learning_rate': 4.577111205696881e-07, 'rewards/chosen': -1.3904656171798706, 'rewards/rejected': -2.9733452796936035, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5828795433044434, 'policy_logps/rejected': -330.8411560058594, 'policy_logps/chosen': -359.15069580078125, 'referece_logps/rejected': -301.10772705078125, 'referece_logps/chosen': -345.24609375, 'logits/rejected': -0.12948423624038696, 'logits/chosen': -0.38697513937950134, 'epoch': 4.15}


 69%|██████▉   | 7434/10740 [36:52:07<16:22:14, 17.83s/it]
{'loss': 0.3457, 'learning_rate': 4.572044435248057e-07, 'rewards/chosen': -2.311490058898926, 'rewards/rejected': -4.670698165893555, 'rewards/accuracies': 0.75, 'rewards/margins': 2.35920786857605, 'policy_logps/rejected': -582.3107299804688, 'policy_logps/chosen': -559.0593872070312, 'referece_logps/rejected': -535.603759765625, 'referece_logps/chosen': -535.9444580078125, 'logits/rejected': 0.059339553117752075, 'logits/chosen': 0.12339484691619873, 'epoch': 4.15}

 69%|██████▉   | 7435/10740 [36:52:24<16:05:54, 17.54s/it]


 69%|██████▉   | 7437/10740 [36:53:04<17:11:12, 18.73s/it]

 69%|██████▉   | 7438/10740 [36:53:21<16:43:30, 18.23s/it]
{'loss': 0.4145, 'learning_rate': 4.561916820423346e-07, 'rewards/chosen': -2.262918710708618, 'rewards/rejected': -4.281280994415283, 'rewards/accuracies': 0.625, 'rewards/margins': 2.018362522125244, 'policy_logps/rejected': -409.851806640625, 'policy_logps/chosen': -355.6480407714844, 'referece_logps/rejected': -367.03900146484375, 'referece_logps/chosen': -333.01885986328125, 'logits/rejected': -0.7525485754013062, 'logits/chosen': -0.7040272355079651, 'epoch': 4.16}

 69%|██████▉   | 7439/10740 [36:53:40<16:59:45, 18.54s/it]


 69%|██████▉   | 7441/10740 [36:54:20<17:33:04, 19.15s/it]

 69%|██████▉   | 7442/10740 [36:54:37<17:04:12, 18.63s/it]

 69%|██████▉   | 7443/10740 [36:54:54<16:28:29, 17.99s/it]

 69%|██████▉   | 7444/10740 [36:55:08<15:24:36, 16.83s/it]
{'loss': 0.3162, 'learning_rate': 4.5467402409971464e-07, 'rewards/chosen': -1.5058212280273438, 'rewards/rejected': -2.911358594894409, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4055372476577759, 'policy_logps/rejected': -478.9883728027344, 'policy_logps/chosen': -514.9059448242188, 'referece_logps/rejected': -449.8747863769531, 'referece_logps/chosen': -499.8477783203125, 'logits/rejected': -1.4048441648483276, 'logits/chosen': -1.388724684715271, 'epoch': 4.16}


 69%|██████▉   | 7446/10740 [36:55:47<16:49:37, 18.39s/it]

 69%|██████▉   | 7447/10740 [36:56:06<16:54:30, 18.48s/it]

 69%|██████▉   | 7448/10740 [36:56:25<17:05:24, 18.69s/it]

 69%|██████▉   | 7449/10740 [36:56:45<17:34:38, 19.23s/it]

 69%|██████▉   | 7450/10740 [36:57:06<17:53:56, 19.59s/it]
[2024-04-03 08:10:49,579] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 7451/10740 [36:57:27<18:22:32, 20.11s/it]
[2024-04-03 08:11:10,924] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2387, 'learning_rate': 4.5290568025709287e-07, 'rewards/chosen': -1.7838072776794434, 'rewards/rejected': -5.07609748840332, 'rewards/accuracies': 1.0, 'rewards/margins': 3.292289972305298, 'policy_logps/rejected': -370.5678405761719, 'policy_logps/chosen': -335.8844909667969, 'referece_logps/rejected': -319.8068542480469, 'referece_logps/chosen': -318.0464172363281, 'logits/rejected': -0.3212140202522278, 'logits/chosen': -0.34919363260269165, 'epoch': 4.16}

 69%|██████▉   | 7452/10740 [36:57:49<18:43:48, 20.51s/it]

 69%|██████▉   | 7453/10740 [36:58:01<16:23:36, 17.95s/it]


 69%|██████▉   | 7455/10740 [36:58:34<16:02:02, 17.57s/it]

 69%|██████▉   | 7456/10740 [36:58:53<16:24:23, 17.99s/it]

 69%|██████▉   | 7457/10740 [36:59:11<16:26:06, 18.02s/it]

 69%|██████▉   | 7458/10740 [36:59:29<16:24:53, 18.01s/it]

 69%|██████▉   | 7459/10740 [36:59:46<16:10:15, 17.74s/it]
{'loss': 0.3937, 'learning_rate': 4.5088770189767547e-07, 'rewards/chosen': -1.44820237159729, 'rewards/rejected': -2.744936227798462, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2967338562011719, 'policy_logps/rejected': -409.68927001953125, 'policy_logps/chosen': -340.1529541015625, 'referece_logps/rejected': -382.2398986816406, 'referece_logps/chosen': -325.6708984375, 'logits/rejected': -0.28126615285873413, 'logits/chosen': -0.25466597080230713, 'epoch': 4.17}


 69%|██████▉   | 7461/10740 [37:00:22<16:31:16, 18.14s/it]

 69%|██████▉   | 7462/10740 [37:00:38<15:53:49, 17.46s/it]

 69%|██████▉   | 7463/10740 [37:01:00<17:06:11, 18.79s/it]

 69%|██████▉   | 7464/10740 [37:01:19<17:19:17, 19.03s/it]
{'loss': 0.4007, 'learning_rate': 4.4962808782848815e-07, 'rewards/chosen': -1.4242186546325684, 'rewards/rejected': -2.6363906860351562, 'rewards/accuracies': 0.625, 'rewards/margins': 1.212172031402588, 'policy_logps/rejected': -323.4058837890625, 'policy_logps/chosen': -342.67523193359375, 'referece_logps/rejected': -297.0419616699219, 'referece_logps/chosen': -328.43304443359375, 'logits/rejected': -0.6202088594436646, 'logits/chosen': -0.6124591827392578, 'epoch': 4.17}


 70%|██████▉   | 7466/10740 [37:01:47<15:00:45, 16.51s/it]

 70%|██████▉   | 7467/10740 [37:02:10<16:35:35, 18.25s/it]

 70%|██████▉   | 7468/10740 [37:02:26<16:06:09, 17.72s/it]

 70%|██████▉   | 7469/10740 [37:02:38<14:29:20, 15.95s/it]
{'loss': 0.393, 'learning_rate': 4.483697252009515e-07, 'rewards/chosen': -1.0205988883972168, 'rewards/rejected': -2.10664439201355, 'rewards/accuracies': 0.875, 'rewards/margins': 1.086045265197754, 'policy_logps/rejected': -396.8841552734375, 'policy_logps/chosen': -479.3181457519531, 'referece_logps/rejected': -375.8177185058594, 'referece_logps/chosen': -469.1121520996094, 'logits/rejected': -1.1228452920913696, 'logits/chosen': -1.1935276985168457, 'epoch': 4.17}


 70%|██████▉   | 7471/10740 [37:03:18<16:33:55, 18.24s/it]

 70%|██████▉   | 7472/10740 [37:03:33<15:40:51, 17.27s/it]

 70%|██████▉   | 7473/10740 [37:03:50<15:36:13, 17.19s/it]

 70%|██████▉   | 7474/10740 [37:04:12<16:49:40, 18.55s/it]

 70%|██████▉   | 7475/10740 [37:04:30<16:47:06, 18.51s/it]
{'loss': 0.3409, 'learning_rate': 4.468613459793674e-07, 'rewards/chosen': -0.9643522500991821, 'rewards/rejected': -3.146186113357544, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1818339824676514, 'policy_logps/rejected': -397.5499267578125, 'policy_logps/chosen': -394.416748046875, 'referece_logps/rejected': -366.0880432128906, 'referece_logps/chosen': -384.7732238769531, 'logits/rejected': 0.5353323817253113, 'logits/chosen': 0.2892914414405823, 'epoch': 4.18}

 70%|██████▉   | 7476/10740 [37:04:51<17:15:33, 19.04s/it]


 70%|██████▉   | 7478/10740 [37:05:28<16:41:12, 18.42s/it]

 70%|██████▉   | 7479/10740 [37:05:48<17:15:25, 19.05s/it]
{'loss': 0.3797, 'learning_rate': 4.458567657130955e-07, 'rewards/chosen': -2.991187810897827, 'rewards/rejected': -3.2704339027404785, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2792460322380066, 'policy_logps/rejected': -333.7148742675781, 'policy_logps/chosen': -324.3620910644531, 'referece_logps/rejected': -301.010498046875, 'referece_logps/chosen': -294.4501953125, 'logits/rejected': -0.26811137795448303, 'logits/chosen': -0.19550728797912598, 'epoch': 4.18}


 70%|██████▉   | 7481/10740 [37:06:22<15:54:00, 17.56s/it]

 70%|██████▉   | 7482/10740 [37:06:42<16:39:25, 18.41s/it]
{'loss': 0.3143, 'learning_rate': 4.4510385964063215e-07, 'rewards/chosen': -2.2089977264404297, 'rewards/rejected': -4.073125839233398, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8641282320022583, 'policy_logps/rejected': -287.383544921875, 'policy_logps/chosen': -308.8865051269531, 'referece_logps/rejected': -246.65228271484375, 'referece_logps/chosen': -286.7965087890625, 'logits/rejected': -0.7011578679084778, 'logits/chosen': -0.658120334148407, 'epoch': 4.18}


 70%|██████▉   | 7484/10740 [37:07:16<15:48:10, 17.47s/it]

 70%|██████▉   | 7485/10740 [37:07:36<16:23:39, 18.13s/it]

 70%|██████▉   | 7486/10740 [37:07:52<15:45:28, 17.43s/it]

 70%|██████▉   | 7487/10740 [37:08:14<17:01:46, 18.85s/it]

 70%|██████▉   | 7488/10740 [37:08:35<17:34:26, 19.45s/it]

 70%|██████▉   | 7489/10740 [37:08:54<17:30:46, 19.39s/it]

 70%|██████▉   | 7490/10740 [37:09:16<18:19:13, 20.29s/it]
{'loss': 0.3189, 'learning_rate': 4.430983324299241e-07, 'rewards/chosen': -1.4652607440948486, 'rewards/rejected': -4.1814703941345215, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7162094116210938, 'policy_logps/rejected': -416.012939453125, 'policy_logps/chosen': -494.5960998535156, 'referece_logps/rejected': -374.1982421875, 'referece_logps/chosen': -479.9435119628906, 'logits/rejected': 0.3797030448913574, 'logits/chosen': 0.2992037534713745, 'epoch': 4.18}


 70%|██████▉   | 7492/10740 [37:09:59<18:34:54, 20.60s/it]
{'loss': 0.4119, 'learning_rate': 4.4259745668712455e-07, 'rewards/chosen': -0.9936691522598267, 'rewards/rejected': -3.232483386993408, 'rewards/accuracies': 0.75, 'rewards/margins': 2.238814353942871, 'policy_logps/rejected': -271.1868896484375, 'policy_logps/chosen': -316.371826171875, 'referece_logps/rejected': -238.862060546875, 'referece_logps/chosen': -306.4350891113281, 'logits/rejected': -0.18173959851264954, 'logits/chosen': -0.2382144033908844, 'epoch': 4.19}


 70%|██████▉   | 7494/10740 [37:10:20<14:04:52, 15.62s/it]
{'loss': 0.3816, 'learning_rate': 4.4209678373283353e-07, 'rewards/chosen': -1.557639241218567, 'rewards/rejected': -3.0158753395080566, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4582358598709106, 'policy_logps/rejected': -362.69525146484375, 'policy_logps/chosen': -326.3089599609375, 'referece_logps/rejected': -332.5364990234375, 'referece_logps/chosen': -310.73260498046875, 'logits/rejected': -0.8955703973770142, 'logits/chosen': -0.9649868011474609, 'epoch': 4.19}


 70%|██████▉   | 7496/10740 [37:10:46<12:40:43, 14.07s/it]

 70%|██████▉   | 7497/10740 [37:11:06<14:13:31, 15.79s/it]

 70%|██████▉   | 7498/10740 [37:11:20<13:47:32, 15.32s/it]

 70%|██████▉   | 7499/10740 [37:11:40<14:59:51, 16.66s/it]

 70%|██████▉   | 7500/10740 [37:11:51<13:26:45, 14.94s/it]

 70%|██████▉   | 7501/10740 [37:12:20<17:13:19, 19.14s/it]

 70%|██████▉   | 7502/10740 [37:12:42<18:01:33, 20.04s/it]

 70%|██████▉   | 7503/10740 [37:13:05<18:44:43, 20.85s/it]
{'loss': 0.3194, 'learning_rate': 4.398462698284755e-07, 'rewards/chosen': -1.4763190746307373, 'rewards/rejected': -2.815906286239624, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3395872116088867, 'policy_logps/rejected': -361.72900390625, 'policy_logps/chosen': -371.80865478515625, 'referece_logps/rejected': -333.5699157714844, 'referece_logps/chosen': -357.0455017089844, 'logits/rejected': -0.3946053683757782, 'logits/chosen': -0.511231541633606, 'epoch': 4.19}


 70%|██████▉   | 7505/10740 [37:13:33<15:27:48, 17.21s/it]
{'loss': 0.5042, 'learning_rate': 4.393467154649271e-07, 'rewards/chosen': -2.3004698753356934, 'rewards/rejected': -3.3521270751953125, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0516573190689087, 'policy_logps/rejected': -344.0389099121094, 'policy_logps/chosen': -240.6845703125, 'referece_logps/rejected': -310.51763916015625, 'referece_logps/chosen': -217.6798858642578, 'logits/rejected': -1.253639817237854, 'logits/chosen': -1.1567660570144653, 'epoch': 4.19}

 70%|██████▉   | 7506/10740 [37:13:50<15:20:20, 17.08s/it]


 70%|██████▉   | 7508/10740 [37:14:17<13:29:16, 15.02s/it]

 70%|██████▉   | 7509/10740 [37:14:31<13:20:42, 14.87s/it]
{'loss': 0.4093, 'learning_rate': 4.3834821883297866e-07, 'rewards/chosen': -1.9965229034423828, 'rewards/rejected': -4.129494667053223, 'rewards/accuracies': 0.875, 'rewards/margins': 2.132972002029419, 'policy_logps/rejected': -307.9461975097656, 'policy_logps/chosen': -304.3011169433594, 'referece_logps/rejected': -266.6512145996094, 'referece_logps/chosen': -284.33587646484375, 'logits/rejected': -0.4076065719127655, 'logits/chosen': -0.5259811282157898, 'epoch': 4.19}


 70%|██████▉   | 7511/10740 [37:15:12<15:48:15, 17.62s/it]
{'loss': 0.2789, 'learning_rate': 4.378492769278417e-07, 'rewards/chosen': -2.526737689971924, 'rewards/rejected': -5.477540969848633, 'rewards/accuracies': 1.0, 'rewards/margins': 2.95080304145813, 'policy_logps/rejected': -454.34832763671875, 'policy_logps/chosen': -386.7862548828125, 'referece_logps/rejected': -399.57293701171875, 'referece_logps/chosen': -361.51885986328125, 'logits/rejected': 0.721010684967041, 'logits/chosen': 0.6951207518577576, 'epoch': 4.2}
[2024-04-03 08:29:18,922] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|██████▉   | 7513/10740 [37:15:57<17:54:10, 19.97s/it]
[2024-04-03 08:29:40,650] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 7514/10740 [37:16:12<16:40:55, 18.62s/it]
[2024-04-03 08:29:56,101] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3715, 'learning_rate': 4.3710124759423405e-07, 'rewards/chosen': -1.1307220458984375, 'rewards/rejected': -2.5852408409118652, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4545185565948486, 'policy_logps/rejected': -256.7457275390625, 'policy_logps/chosen': -254.39053344726562, 'referece_logps/rejected': -230.89329528808594, 'referece_logps/chosen': -243.08328247070312, 'logits/rejected': -1.080319881439209, 'logits/chosen': -1.1000581979751587, 'epoch': 4.2}

 70%|██████▉   | 7515/10740 [37:16:30<16:19:34, 18.22s/it]


 70%|██████▉   | 7517/10740 [37:17:10<17:20:46, 19.38s/it]
{'loss': 0.3278, 'learning_rate': 4.3635367903379616e-07, 'rewards/chosen': -1.523586392402649, 'rewards/rejected': -2.9174060821533203, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3938195705413818, 'policy_logps/rejected': -250.75074768066406, 'policy_logps/chosen': -292.1304931640625, 'referece_logps/rejected': -221.57669067382812, 'referece_logps/chosen': -276.8946533203125, 'logits/rejected': -0.9414651989936829, 'logits/chosen': -0.8904561996459961, 'epoch': 4.2}


 70%|███████   | 7519/10740 [37:17:47<16:37:16, 18.58s/it]

 70%|███████   | 7520/10740 [37:18:07<17:07:24, 19.14s/it]
[2024-04-03 08:31:50,772] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|███████   | 7521/10740 [37:18:29<17:55:16, 20.04s/it]

 70%|███████   | 7522/10740 [37:18:51<18:24:52, 20.60s/it]
{'loss': 0.3643, 'learning_rate': 4.3510875703532924e-07, 'rewards/chosen': -2.6812448501586914, 'rewards/rejected': -4.204914093017578, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5236692428588867, 'policy_logps/rejected': -416.51239013671875, 'policy_logps/chosen': -454.87493896484375, 'referece_logps/rejected': -374.4632568359375, 'referece_logps/chosen': -428.0625, 'logits/rejected': -0.06384351849555969, 'logits/chosen': -0.02310912311077118, 'epoch': 4.2}

 70%|███████   | 7523/10740 [37:19:08<17:21:01, 19.42s/it]


 70%|███████   | 7525/10740 [37:19:39<15:25:46, 17.28s/it]

 70%|███████   | 7526/10740 [37:19:59<16:03:20, 17.98s/it]

 70%|███████   | 7527/10740 [37:20:19<16:47:28, 18.81s/it]
[2024-04-03 08:34:03,187] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|███████   | 7528/10740 [37:20:39<17:01:07, 19.07s/it]
{'loss': 0.4122, 'learning_rate': 4.3361654636780707e-07, 'rewards/chosen': -2.1934547424316406, 'rewards/rejected': -2.939026117324829, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7455713152885437, 'policy_logps/rejected': -428.50885009765625, 'policy_logps/chosen': -429.2166748046875, 'referece_logps/rejected': -399.11859130859375, 'referece_logps/chosen': -407.2821044921875, 'logits/rejected': -0.20889854431152344, 'logits/chosen': -0.22882692515850067, 'epoch': 4.21}


 70%|███████   | 7530/10740 [37:21:13<16:28:32, 18.48s/it]
[2024-04-03 08:34:57,233] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4263, 'learning_rate': 4.3311955468246453e-07, 'rewards/chosen': -2.4910709857940674, 'rewards/rejected': -3.194720506668091, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7036497592926025, 'policy_logps/rejected': -782.9255981445312, 'policy_logps/chosen': -715.6258544921875, 'referece_logps/rejected': -750.9783935546875, 'referece_logps/chosen': -690.7151489257812, 'logits/rejected': -0.1058921068906784, 'logits/chosen': -0.08513130247592926, 'epoch': 4.21}

 70%|███████   | 7531/10740 [37:21:31<16:06:46, 18.08s/it]

 70%|███████   | 7532/10740 [37:21:51<16:42:09, 18.74s/it]


 70%|███████   | 7534/10740 [37:22:19<14:42:47, 16.52s/it]

 70%|███████   | 7535/10740 [37:22:39<15:40:43, 17.61s/it]
{'loss': 0.4468, 'learning_rate': 4.318779781498428e-07, 'rewards/chosen': -1.633253574371338, 'rewards/rejected': -2.763434648513794, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1301811933517456, 'policy_logps/rejected': -531.8264770507812, 'policy_logps/chosen': -464.51953125, 'referece_logps/rejected': -504.192138671875, 'referece_logps/chosen': -448.1869812011719, 'logits/rejected': -0.2736974358558655, 'logits/chosen': -0.2949211597442627, 'epoch': 4.21}


 70%|███████   | 7537/10740 [37:23:17<16:11:39, 18.20s/it]
{'loss': 0.428, 'learning_rate': 4.313817090833712e-07, 'rewards/chosen': -2.2667124271392822, 'rewards/rejected': -2.598456621170044, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3317440152168274, 'policy_logps/rejected': -549.2291259765625, 'policy_logps/chosen': -385.43023681640625, 'referece_logps/rejected': -523.2445678710938, 'referece_logps/chosen': -362.76312255859375, 'logits/rejected': -1.1384615898132324, 'logits/chosen': -1.1469955444335938, 'epoch': 4.21}

 70%|███████   | 7538/10740 [37:23:37<16:32:57, 18.61s/it]

 70%|███████   | 7539/10740 [37:23:53<15:55:15, 17.91s/it]

 70%|███████   | 7540/10740 [37:24:09<15:18:17, 17.22s/it]

 70%|███████   | 7541/10740 [37:24:29<16:03:02, 18.06s/it]

 70%|███████   | 7542/10740 [37:24:48<16:24:07, 18.46s/it]

 70%|███████   | 7543/10740 [37:25:07<16:27:21, 18.53s/it]

 70%|███████   | 7544/10740 [37:25:27<16:49:10, 18.95s/it]

 70%|███████   | 7545/10740 [37:25:42<15:48:45, 17.82s/it]


 70%|███████   | 7547/10740 [37:26:10<13:46:36, 15.53s/it]

 70%|███████   | 7548/10740 [37:26:26<13:54:56, 15.69s/it]
{'loss': 0.3408, 'learning_rate': 4.286559318366435e-07, 'rewards/chosen': -1.7819600105285645, 'rewards/rejected': -3.800808906555176, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0188491344451904, 'policy_logps/rejected': -279.68963623046875, 'policy_logps/chosen': -312.503662109375, 'referece_logps/rejected': -241.6815643310547, 'referece_logps/chosen': -294.68408203125, 'logits/rejected': -1.1139085292816162, 'logits/chosen': -1.30814790725708, 'epoch': 4.22}

 70%|███████   | 7549/10740 [37:26:39<13:11:08, 14.88s/it]

 70%|███████   | 7550/10740 [37:26:58<14:25:50, 16.29s/it]


 70%|███████   | 7552/10740 [37:27:34<15:32:22, 17.55s/it]
[2024-04-03 08:41:17,604] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3764, 'learning_rate': 4.2766629748796857e-07, 'rewards/chosen': -1.1431541442871094, 'rewards/rejected': -2.47973370552063, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3365795612335205, 'policy_logps/rejected': -285.7662353515625, 'policy_logps/chosen': -421.5052185058594, 'referece_logps/rejected': -260.96893310546875, 'referece_logps/chosen': -410.07366943359375, 'logits/rejected': -0.5449287295341492, 'logits/chosen': -0.4913993179798126, 'epoch': 4.22}

 70%|███████   | 7553/10740 [37:27:53<15:49:39, 17.88s/it]


 70%|███████   | 7555/10740 [37:28:30<16:14:11, 18.35s/it]

 70%|███████   | 7556/10740 [37:28:52<17:11:25, 19.44s/it]
{'loss': 0.2263, 'learning_rate': 4.2667749602166046e-07, 'rewards/chosen': -1.0606138706207275, 'rewards/rejected': -5.010704517364502, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9500906467437744, 'policy_logps/rejected': -489.37744140625, 'policy_logps/chosen': -440.3724670410156, 'referece_logps/rejected': -439.2703552246094, 'referece_logps/chosen': -429.76629638671875, 'logits/rejected': 0.5329226851463318, 'logits/chosen': 0.5160042643547058, 'epoch': 4.22}

 70%|███████   | 7557/10740 [37:29:13<17:39:02, 19.96s/it]


 70%|███████   | 7559/10740 [37:29:48<16:10:42, 18.31s/it]
{'loss': 0.3634, 'learning_rate': 4.2593644236664215e-07, 'rewards/chosen': -1.1989456415176392, 'rewards/rejected': -3.243136405944824, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0441904067993164, 'policy_logps/rejected': -330.4573669433594, 'policy_logps/chosen': -265.161376953125, 'referece_logps/rejected': -298.0260009765625, 'referece_logps/chosen': -253.1719207763672, 'logits/rejected': -1.0972882509231567, 'logits/chosen': -1.082357406616211, 'epoch': 4.22}


 70%|███████   | 7561/10740 [37:30:18<14:59:27, 16.98s/it]

 70%|███████   | 7562/10740 [37:30:38<15:45:39, 17.85s/it]
{'loss': 0.3443, 'learning_rate': 4.251958586239908e-07, 'rewards/chosen': -3.4794039726257324, 'rewards/rejected': -4.643969535827637, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1645653247833252, 'policy_logps/rejected': -457.51483154296875, 'policy_logps/chosen': -554.3956298828125, 'referece_logps/rejected': -411.07513427734375, 'referece_logps/chosen': -519.6015625, 'logits/rejected': -0.18659614026546478, 'logits/chosen': -0.19453075528144836, 'epoch': 4.22}

 70%|███████   | 7563/10740 [37:30:57<15:59:48, 18.13s/it]

 70%|███████   | 7564/10740 [37:31:12<15:17:02, 17.32s/it]


 70%|███████   | 7566/10740 [37:31:52<16:26:05, 18.64s/it]
{'loss': 0.3423, 'learning_rate': 4.2420914565631684e-07, 'rewards/chosen': -1.4453482627868652, 'rewards/rejected': -3.4687998294830322, 'rewards/accuracies': 0.875, 'rewards/margins': 2.023451566696167, 'policy_logps/rejected': -416.9500732421875, 'policy_logps/chosen': -383.25360107421875, 'referece_logps/rejected': -382.2620849609375, 'referece_logps/chosen': -368.8001403808594, 'logits/rejected': -0.5133401155471802, 'logits/chosen': -0.6192682385444641, 'epoch': 4.23}

 70%|███████   | 7567/10740 [37:32:10<16:19:15, 18.52s/it]

 70%|███████   | 7568/10740 [37:32:29<16:14:43, 18.44s/it]

 70%|███████   | 7569/10740 [37:32:49<16:39:57, 18.92s/it]

 70%|███████   | 7570/10740 [37:33:08<16:42:27, 18.97s/it]

 70%|███████   | 7571/10740 [37:33:29<17:16:06, 19.62s/it]

 71%|███████   | 7572/10740 [37:33:50<17:33:58, 19.96s/it]

 71%|███████   | 7573/10740 [37:34:08<17:03:52, 19.40s/it]

 71%|███████   | 7574/10740 [37:34:27<17:05:53, 19.44s/it]

 71%|███████   | 7575/10740 [37:34:43<16:05:20, 18.30s/it]

 71%|███████   | 7576/10740 [37:35:03<16:39:43, 18.96s/it]

 71%|███████   | 7577/10740 [37:35:19<15:40:04, 17.83s/it]

 71%|███████   | 7578/10740 [37:35:39<16:13:13, 18.47s/it]

 71%|███████   | 7579/10740 [37:35:50<14:17:05, 16.27s/it]

 71%|███████   | 7580/10740 [37:36:05<13:59:04, 15.93s/it]


 71%|███████   | 7582/10740 [37:36:42<15:22:25, 17.53s/it]
{'loss': 0.3983, 'learning_rate': 4.202706872598437e-07, 'rewards/chosen': -1.3297760486602783, 'rewards/rejected': -3.4347846508026123, 'rewards/accuracies': 0.625, 'rewards/margins': 2.105008602142334, 'policy_logps/rejected': -461.6978759765625, 'policy_logps/chosen': -465.4906005859375, 'referece_logps/rejected': -427.3499755859375, 'referece_logps/chosen': -452.19281005859375, 'logits/rejected': 0.38201940059661865, 'logits/chosen': 0.4264269769191742, 'epoch': 4.24}


 71%|███████   | 7584/10740 [37:37:20<15:59:41, 18.25s/it]
{'loss': 0.3398, 'learning_rate': 4.197793271817033e-07, 'rewards/chosen': -1.0315377712249756, 'rewards/rejected': -3.3763906955718994, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3448526859283447, 'policy_logps/rejected': -388.27008056640625, 'policy_logps/chosen': -329.6337585449219, 'referece_logps/rejected': -354.5062255859375, 'referece_logps/chosen': -319.3183898925781, 'logits/rejected': -0.6276944875717163, 'logits/chosen': -0.5812012553215027, 'epoch': 4.24}

 71%|███████   | 7585/10740 [37:37:35<15:08:37, 17.28s/it]


 71%|███████   | 7587/10740 [37:38:13<15:33:14, 17.76s/it]

 71%|███████   | 7588/10740 [37:38:32<15:59:59, 18.27s/it]
{'loss': 0.2916, 'learning_rate': 4.1879724047400976e-07, 'rewards/chosen': -1.5833699703216553, 'rewards/rejected': -3.367128849029541, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7837588787078857, 'policy_logps/rejected': -435.50311279296875, 'policy_logps/chosen': -431.6345520019531, 'referece_logps/rejected': -401.8318176269531, 'referece_logps/chosen': -415.8008117675781, 'logits/rejected': 0.39736172556877136, 'logits/chosen': 0.39433038234710693, 'epoch': 4.24}

 71%|███████   | 7589/10740 [37:38:52<16:25:01, 18.76s/it]

 71%|███████   | 7590/10740 [37:39:11<16:28:04, 18.82s/it]

 71%|███████   | 7591/10740 [37:39:26<15:23:47, 17.60s/it]

 71%|███████   | 7592/10740 [37:39:45<15:57:56, 18.26s/it]


 71%|███████   | 7594/10740 [37:40:28<17:30:47, 20.04s/it]
{'loss': 0.3544, 'learning_rate': 4.1732569671305907e-07, 'rewards/chosen': -2.159900665283203, 'rewards/rejected': -2.780813455581665, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6209126710891724, 'policy_logps/rejected': -345.22222900390625, 'policy_logps/chosen': -332.8067321777344, 'referece_logps/rejected': -317.4140625, 'referece_logps/chosen': -311.2077331542969, 'logits/rejected': -0.4522164762020111, 'logits/chosen': -0.4456440210342407, 'epoch': 4.24}

 71%|███████   | 7595/10740 [37:40:50<17:50:23, 20.42s/it]

 71%|███████   | 7596/10740 [37:41:09<17:33:23, 20.10s/it]

 71%|███████   | 7597/10740 [37:41:28<17:06:18, 19.59s/it]


 71%|███████   | 7599/10740 [37:42:01<16:02:33, 18.39s/it]

 71%|███████   | 7600/10740 [37:42:17<15:29:30, 17.76s/it]
{'loss': 0.3538, 'learning_rate': 4.15856060795249e-07, 'rewards/chosen': -1.3479115962982178, 'rewards/rejected': -2.8685264587402344, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5206146240234375, 'policy_logps/rejected': -343.8013610839844, 'policy_logps/chosen': -253.38082885742188, 'referece_logps/rejected': -315.1160888671875, 'referece_logps/chosen': -239.90170288085938, 'logits/rejected': -1.1433521509170532, 'logits/chosen': -1.211958408355713, 'epoch': 4.25}


 71%|███████   | 7602/10740 [37:42:45<13:47:27, 15.82s/it]
{'loss': 0.4795, 'learning_rate': 4.1536660695300716e-07, 'rewards/chosen': -1.6162595748901367, 'rewards/rejected': -2.2173898220062256, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6011303663253784, 'policy_logps/rejected': -448.51519775390625, 'policy_logps/chosen': -398.5133361816406, 'referece_logps/rejected': -426.34130859375, 'referece_logps/chosen': -382.35076904296875, 'logits/rejected': -1.3897011280059814, 'logits/chosen': -1.4588342905044556, 'epoch': 4.25}

 71%|███████   | 7603/10740 [37:42:58<13:02:29, 14.97s/it]

 71%|███████   | 7604/10740 [37:43:13<13:09:33, 15.11s/it]


 71%|███████   | 7606/10740 [37:43:41<12:21:22, 14.19s/it]

 71%|███████   | 7607/10740 [37:43:53<11:45:31, 13.51s/it]
{'loss': 0.4157, 'learning_rate': 4.141439032789296e-07, 'rewards/chosen': -1.7216054201126099, 'rewards/rejected': -2.5880866050720215, 'rewards/accuracies': 1.0, 'rewards/margins': 0.8664811849594116, 'policy_logps/rejected': -398.03240966796875, 'policy_logps/chosen': -495.0279541015625, 'referece_logps/rejected': -372.1515197753906, 'referece_logps/chosen': -477.8118896484375, 'logits/rejected': -1.3086044788360596, 'logits/chosen': -1.2377742528915405, 'epoch': 4.25}


 71%|███████   | 7609/10740 [37:44:21<12:20:02, 14.18s/it]

 71%|███████   | 7610/10740 [37:44:43<14:20:29, 16.49s/it]
{'loss': 0.3736, 'learning_rate': 4.1341092031714496e-07, 'rewards/chosen': -1.4352657794952393, 'rewards/rejected': -3.7803773880004883, 'rewards/accuracies': 1.0, 'rewards/margins': 2.34511137008667, 'policy_logps/rejected': -418.5033874511719, 'policy_logps/chosen': -418.25494384765625, 'referece_logps/rejected': -380.6995849609375, 'referece_logps/chosen': -403.9023132324219, 'logits/rejected': -0.24990183115005493, 'logits/chosen': -0.2846103012561798, 'epoch': 4.25}


 71%|███████   | 7612/10740 [37:45:15<14:06:12, 16.23s/it]
{'loss': 0.4652, 'learning_rate': 4.129225317308287e-07, 'rewards/chosen': -0.7198808193206787, 'rewards/rejected': -2.0364880561828613, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3166072368621826, 'policy_logps/rejected': -383.3687438964844, 'policy_logps/chosen': -411.5442810058594, 'referece_logps/rejected': -363.00384521484375, 'referece_logps/chosen': -404.34552001953125, 'logits/rejected': -0.9218313097953796, 'logits/chosen': -0.9075963497161865, 'epoch': 4.25}

 71%|███████   | 7613/10740 [37:45:34<14:57:22, 17.22s/it]

 71%|███████   | 7614/10740 [37:45:53<15:14:33, 17.55s/it]

 71%|███████   | 7615/10740 [37:46:05<13:47:29, 15.89s/it]

 71%|███████   | 7616/10740 [37:46:24<14:46:01, 17.02s/it]

 71%|███████   | 7617/10740 [37:46:44<15:34:15, 17.95s/it]

 71%|███████   | 7618/10740 [37:47:04<16:05:29, 18.56s/it]

 71%|███████   | 7619/10740 [37:47:24<16:20:09, 18.84s/it]

 71%|███████   | 7620/10740 [37:47:40<15:40:15, 18.08s/it]

 71%|███████   | 7621/10740 [37:47:59<15:56:25, 18.40s/it]


 71%|███████   | 7623/10740 [37:48:35<15:41:36, 18.13s/it]

 71%|███████   | 7624/10740 [37:48:51<15:07:52, 17.48s/it]

 71%|███████   | 7625/10740 [37:49:07<14:42:00, 16.99s/it]
{'loss': 0.4585, 'learning_rate': 4.097532199755801e-07, 'rewards/chosen': -1.769571304321289, 'rewards/rejected': -3.2816243171691895, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5120530128479004, 'policy_logps/rejected': -330.1610107421875, 'policy_logps/chosen': -333.7212829589844, 'referece_logps/rejected': -297.34478759765625, 'referece_logps/chosen': -316.0256042480469, 'logits/rejected': -0.8391289710998535, 'logits/chosen': -0.711655855178833, 'epoch': 4.26}

 71%|███████   | 7626/10740 [37:49:20<13:44:46, 15.89s/it]

 71%|███████   | 7627/10740 [37:49:40<14:42:28, 17.01s/it]

 71%|███████   | 7628/10740 [37:50:00<15:30:25, 17.94s/it]

 71%|███████   | 7629/10740 [37:50:22<16:27:05, 19.04s/it]

 71%|███████   | 7630/10740 [37:50:41<16:31:33, 19.13s/it]

 71%|███████   | 7631/10740 [37:51:00<16:39:00, 19.28s/it]

 71%|███████   | 7632/10740 [37:51:20<16:42:41, 19.36s/it]

 71%|███████   | 7633/10740 [37:51:40<16:47:58, 19.47s/it]


 71%|███████   | 7635/10740 [37:52:15<15:51:34, 18.39s/it]
{'loss': 0.402, 'learning_rate': 4.073214590084845e-07, 'rewards/chosen': -2.383432149887085, 'rewards/rejected': -3.5774519443511963, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1940199136734009, 'policy_logps/rejected': -443.86602783203125, 'policy_logps/chosen': -436.756591796875, 'referece_logps/rejected': -408.0915222167969, 'referece_logps/chosen': -412.9222412109375, 'logits/rejected': -0.3699432909488678, 'logits/chosen': -0.3649662435054779, 'epoch': 4.27}

 71%|███████   | 7636/10740 [37:52:32<15:24:18, 17.87s/it]

 71%|███████   | 7637/10740 [37:52:45<14:03:43, 16.31s/it]

 71%|███████   | 7638/10740 [37:53:04<14:54:50, 17.31s/it]

 71%|███████   | 7639/10740 [37:53:19<14:14:47, 16.54s/it]
[2024-04-03 09:07:27,050] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 7640/10740 [37:53:43<16:14:12, 18.86s/it]

 71%|███████   | 7641/10740 [37:54:05<16:53:49, 19.63s/it]

 71%|███████   | 7642/10740 [37:54:18<15:14:41, 17.72s/it]
[2024-04-03 09:08:22,958] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 7643/10740 [37:54:39<16:08:44, 18.77s/it]

 71%|███████   | 7644/10740 [37:54:58<16:12:02, 18.84s/it]

 71%|███████   | 7645/10740 [37:55:12<14:46:58, 17.20s/it]

 71%|███████   | 7646/10740 [37:55:31<15:14:11, 17.73s/it]

 71%|███████   | 7647/10740 [37:55:44<14:01:22, 16.32s/it]

 71%|███████   | 7648/10740 [37:56:01<14:19:02, 16.67s/it]

 71%|███████   | 7649/10740 [37:56:23<15:36:53, 18.19s/it]

 71%|███████   | 7650/10740 [37:56:42<15:45:09, 18.35s/it]

 71%|███████   | 7651/10740 [37:57:01<16:03:38, 18.72s/it]

 71%|███████   | 7652/10740 [37:57:15<14:49:28, 17.28s/it]
[2024-04-03 09:11:20,498] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 7653/10740 [37:57:37<15:57:36, 18.61s/it]

 71%|███████▏  | 7654/10740 [37:57:56<16:11:26, 18.89s/it]

 71%|███████▏  | 7655/10740 [37:58:15<16:08:19, 18.83s/it]

 71%|███████▏  | 7656/10740 [37:58:31<15:19:04, 17.88s/it]

 71%|███████▏  | 7657/10740 [37:58:49<15:24:29, 17.99s/it]

 71%|███████▏  | 7658/10740 [37:59:07<15:19:09, 17.89s/it]
[2024-04-03 09:13:13,229] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 7659/10740 [37:59:29<16:36:16, 19.40s/it]

 71%|███████▏  | 7660/10740 [37:59:51<17:13:20, 20.13s/it]

 71%|███████▏  | 7661/10740 [38:00:07<16:02:00, 18.75s/it]
[2024-04-03 09:14:14,808] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 7662/10740 [38:00:31<17:26:07, 20.39s/it]
[2024-04-03 09:14:31,661] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 7663/10740 [38:00:48<16:31:18, 19.33s/it]

 71%|███████▏  | 7664/10740 [38:01:01<14:54:02, 17.44s/it]
[2024-04-03 09:15:02,291] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 7665/10740 [38:01:19<14:56:17, 17.49s/it]
[2024-04-03 09:15:24,115] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 7666/10740 [38:01:40<16:02:37, 18.79s/it]

 71%|███████▏  | 7667/10740 [38:02:01<16:25:48, 19.25s/it]

 71%|███████▏  | 7668/10740 [38:02:20<16:24:43, 19.23s/it]

 71%|███████▏  | 7669/10740 [38:02:39<16:24:52, 19.24s/it]

 71%|███████▏  | 7670/10740 [38:02:56<15:55:00, 18.66s/it]

 71%|███████▏  | 7671/10740 [38:03:15<15:55:20, 18.68s/it]

 71%|███████▏  | 7672/10740 [38:03:35<16:17:18, 19.11s/it]

 71%|███████▏  | 7673/10740 [38:03:54<16:07:07, 18.92s/it]

 71%|███████▏  | 7674/10740 [38:04:13<16:16:34, 19.11s/it]

 71%|███████▏  | 7675/10740 [38:04:26<14:34:04, 17.11s/it]

 71%|███████▏  | 7676/10740 [38:04:37<13:07:35, 15.42s/it]

 71%|███████▏  | 7677/10740 [38:04:59<14:44:32, 17.33s/it]

 71%|███████▏  | 7678/10740 [38:05:15<14:26:20, 16.98s/it]

 71%|███████▏  | 7679/10740 [38:05:29<13:42:55, 16.13s/it]

 72%|███████▏  | 7680/10740 [38:05:48<14:20:44, 16.88s/it]

 72%|███████▏  | 7681/10740 [38:06:11<15:50:56, 18.65s/it]

 72%|███████▏  | 7682/10740 [38:06:24<14:22:29, 16.92s/it]

 72%|███████▏  | 7683/10740 [38:06:39<13:56:33, 16.42s/it]

 72%|███████▏  | 7684/10740 [38:06:52<13:07:33, 15.46s/it]

 72%|███████▏  | 7685/10740 [38:07:09<13:27:05, 15.85s/it]

 72%|███████▏  | 7686/10740 [38:07:22<12:46:50, 15.07s/it]

 72%|███████▏  | 7687/10740 [38:07:40<13:25:34, 15.83s/it]

 72%|███████▏  | 7688/10740 [38:07:59<14:23:48, 16.98s/it]

 72%|███████▏  | 7689/10740 [38:08:22<15:49:52, 18.68s/it]

 72%|███████▏  | 7690/10740 [38:08:38<15:08:53, 17.88s/it]

 72%|███████▏  | 7691/10740 [38:08:51<13:55:51, 16.45s/it]

 72%|███████▏  | 7692/10740 [38:09:05<13:19:25, 15.74s/it]

 72%|███████▏  | 7693/10740 [38:09:21<13:26:05, 15.87s/it]

 72%|███████▏  | 7694/10740 [38:09:42<14:30:09, 17.14s/it]
[2024-04-03 09:23:47,928] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7695/10740 [38:10:04<15:53:55, 18.80s/it]
[2024-04-03 09:24:06,629] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7696/10740 [38:10:23<15:52:08, 18.77s/it]

 72%|███████▏  | 7697/10740 [38:10:43<16:06:25, 19.06s/it]

 72%|███████▏  | 7698/10740 [38:11:04<16:35:41, 19.64s/it]
[2024-04-03 09:25:06,814] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7699/10740 [38:11:23<16:32:37, 19.58s/it]

 72%|███████▏  | 7700/10740 [38:11:44<16:54:33, 20.02s/it]

 72%|███████▏  | 7701/10740 [38:12:01<16:07:43, 19.11s/it]

 72%|███████▏  | 7702/10740 [38:12:21<16:15:03, 19.26s/it]

 72%|███████▏  | 7703/10740 [38:12:39<16:06:23, 19.09s/it]

 72%|███████▏  | 7704/10740 [38:12:58<16:03:23, 19.04s/it]

 72%|███████▏  | 7705/10740 [38:13:17<15:51:08, 18.80s/it]

 72%|███████▏  | 7706/10740 [38:13:35<15:43:19, 18.66s/it]

 72%|███████▏  | 7707/10740 [38:13:50<14:56:13, 17.73s/it]

 72%|███████▏  | 7708/10740 [38:14:07<14:38:15, 17.38s/it]

 72%|███████▏  | 7709/10740 [38:14:25<14:47:18, 17.56s/it]

 72%|███████▏  | 7710/10740 [38:14:42<14:36:19, 17.35s/it]

 72%|███████▏  | 7711/10740 [38:15:02<15:11:20, 18.05s/it]


 72%|███████▏  | 7713/10740 [38:15:34<14:28:09, 17.21s/it]

 72%|███████▏  | 7714/10740 [38:15:49<13:56:59, 16.60s/it]

 72%|███████▏  | 7715/10740 [38:16:07<14:14:43, 16.95s/it]

 72%|███████▏  | 7716/10740 [38:16:28<15:16:59, 18.19s/it]

 72%|███████▏  | 7717/10740 [38:16:43<14:31:13, 17.29s/it]

 72%|███████▏  | 7718/10740 [38:17:03<15:04:53, 17.97s/it]

 72%|███████▏  | 7719/10740 [38:17:15<13:43:48, 16.36s/it]

 72%|███████▏  | 7720/10740 [38:17:30<13:19:24, 15.88s/it]

 72%|███████▏  | 7721/10740 [38:17:48<13:56:40, 16.63s/it]

 72%|███████▏  | 7722/10740 [38:18:09<14:56:51, 17.83s/it]

 72%|███████▏  | 7723/10740 [38:18:29<15:22:57, 18.36s/it]

 72%|███████▏  | 7724/10740 [38:18:50<16:07:47, 19.25s/it]

 72%|███████▏  | 7725/10740 [38:19:11<16:35:46, 19.82s/it]

 72%|███████▏  | 7726/10740 [38:19:26<15:23:52, 18.39s/it]

 72%|███████▏  | 7727/10740 [38:19:44<15:13:33, 18.19s/it]

 72%|███████▏  | 7728/10740 [38:20:01<14:51:10, 17.75s/it]

 72%|███████▏  | 7729/10740 [38:20:14<13:44:32, 16.43s/it]

 72%|███████▏  | 7730/10740 [38:20:32<14:14:12, 17.03s/it]

 72%|███████▏  | 7731/10740 [38:20:45<13:01:46, 15.59s/it]

 72%|███████▏  | 7732/10740 [38:21:08<14:53:45, 17.83s/it]
{'loss': 0.3424, 'learning_rate': 3.8401642721474157e-07, 'rewards/chosen': -2.446481466293335, 'rewards/rejected': -3.4541308879852295, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0076494216918945, 'policy_logps/rejected': -526.32763671875, 'policy_logps/chosen': -423.1546936035156, 'referece_logps/rejected': -491.786376953125, 'referece_logps/chosen': -398.6898498535156, 'logits/rejected': 0.011968068778514862, 'logits/chosen': -0.4433770775794983, 'epoch': 4.32}


 72%|███████▏  | 7734/10740 [38:21:47<15:42:03, 18.80s/it]

 72%|███████▏  | 7735/10740 [38:22:07<15:53:12, 19.03s/it]

 72%|███████▏  | 7736/10740 [38:22:24<15:30:07, 18.58s/it]

 72%|███████▏  | 7737/10740 [38:22:43<15:27:10, 18.52s/it]

 72%|███████▏  | 7738/10740 [38:23:04<16:05:05, 19.29s/it]
[2024-04-03 09:36:47,363] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7739/10740 [38:23:24<16:15:22, 19.50s/it]

 72%|███████▏  | 7740/10740 [38:23:43<16:12:49, 19.46s/it]

 72%|███████▏  | 7741/10740 [38:24:04<16:34:55, 19.91s/it]
[2024-04-03 09:37:47,663] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7742/10740 [38:24:23<16:20:34, 19.62s/it]

 72%|███████▏  | 7743/10740 [38:24:37<15:01:51, 18.06s/it]

 72%|███████▏  | 7744/10740 [38:24:58<15:36:22, 18.75s/it]
{'loss': 0.3487, 'learning_rate': 3.8116956828102365e-07, 'rewards/chosen': -1.0892674922943115, 'rewards/rejected': -2.686124086380005, 'rewards/accuracies': 0.875, 'rewards/margins': 1.596856951713562, 'policy_logps/rejected': -286.85565185546875, 'policy_logps/chosen': -468.4984436035156, 'referece_logps/rejected': -259.994384765625, 'referece_logps/chosen': -457.6058349609375, 'logits/rejected': -0.6586620807647705, 'logits/chosen': -0.706758975982666, 'epoch': 4.33}
[2024-04-03 09:39:03,606] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 72%|███████▏  | 7746/10740 [38:25:42<16:56:05, 20.36s/it]

 72%|███████▏  | 7747/10740 [38:26:01<16:46:39, 20.18s/it]
[2024-04-03 09:39:45,067] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7748/10740 [38:26:21<16:42:15, 20.10s/it]

 72%|███████▏  | 7749/10740 [38:26:37<15:41:11, 18.88s/it]
{'loss': 0.3498, 'learning_rate': 3.7998576699614637e-07, 'rewards/chosen': -3.1997859477996826, 'rewards/rejected': -5.835234642028809, 'rewards/accuracies': 1.0, 'rewards/margins': 2.635448694229126, 'policy_logps/rejected': -555.7811279296875, 'policy_logps/chosen': -433.6673583984375, 'referece_logps/rejected': -497.42877197265625, 'referece_logps/chosen': -401.6695251464844, 'logits/rejected': 0.8198658227920532, 'logits/chosen': 0.8123273849487305, 'epoch': 4.33}


 72%|███████▏  | 7751/10740 [38:27:12<14:59:25, 18.05s/it]

 72%|███████▏  | 7752/10740 [38:27:32<15:37:33, 18.83s/it]

 72%|███████▏  | 7753/10740 [38:27:53<16:09:37, 19.48s/it]

 72%|███████▏  | 7754/10740 [38:28:11<15:47:10, 19.03s/it]
{'loss': 0.438, 'learning_rate': 3.788033755063883e-07, 'rewards/chosen': -2.600379228591919, 'rewards/rejected': -3.0770840644836426, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4767047166824341, 'policy_logps/rejected': -342.4233703613281, 'policy_logps/chosen': -350.7281494140625, 'referece_logps/rejected': -311.65252685546875, 'referece_logps/chosen': -324.724365234375, 'logits/rejected': 0.04111504554748535, 'logits/chosen': 0.06910586357116699, 'epoch': 4.33}


 72%|███████▏  | 7756/10740 [38:28:49<15:45:42, 19.02s/it]

 72%|███████▏  | 7757/10740 [38:29:09<16:12:06, 19.55s/it]

 72%|███████▏  | 7758/10740 [38:29:31<16:44:13, 20.21s/it]

 72%|███████▏  | 7759/10740 [38:29:47<15:33:06, 18.78s/it]
{'loss': 0.4094, 'learning_rate': 3.7762239650028447e-07, 'rewards/chosen': -1.712217926979065, 'rewards/rejected': -3.021450996398926, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3092331886291504, 'policy_logps/rejected': -329.71319580078125, 'policy_logps/chosen': -295.64068603515625, 'referece_logps/rejected': -299.49871826171875, 'referece_logps/chosen': -278.51849365234375, 'logits/rejected': -0.4582938253879547, 'logits/chosen': -0.505807638168335, 'epoch': 4.33}


 72%|███████▏  | 7761/10740 [38:30:22<15:04:29, 18.22s/it]
{'loss': 0.4407, 'learning_rate': 3.7715040099488613e-07, 'rewards/chosen': -1.7803049087524414, 'rewards/rejected': -3.6186089515686035, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8383039236068726, 'policy_logps/rejected': -537.167724609375, 'policy_logps/chosen': -487.01898193359375, 'referece_logps/rejected': -500.9816589355469, 'referece_logps/chosen': -469.21588134765625, 'logits/rejected': -0.8002724647521973, 'logits/chosen': -0.701505184173584, 'epoch': 4.34}

 72%|███████▏  | 7762/10740 [38:30:36<13:53:32, 16.79s/it]


 72%|███████▏  | 7764/10740 [38:31:12<14:26:42, 17.47s/it]
{'loss': 0.4307, 'learning_rate': 3.7644283266315724e-07, 'rewards/chosen': -2.0838475227355957, 'rewards/rejected': -3.8154571056365967, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7316094636917114, 'policy_logps/rejected': -452.4172668457031, 'policy_logps/chosen': -589.60302734375, 'referece_logps/rejected': -414.2626953125, 'referece_logps/chosen': -568.7645263671875, 'logits/rejected': -0.2471252828836441, 'logits/chosen': -0.3829702138900757, 'epoch': 4.34}


 72%|███████▏  | 7766/10740 [38:31:46<14:21:52, 17.39s/it]
{'loss': 0.3902, 'learning_rate': 3.759714039765195e-07, 'rewards/chosen': -2.6829404830932617, 'rewards/rejected': -4.377141952514648, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6942009925842285, 'policy_logps/rejected': -345.56573486328125, 'policy_logps/chosen': -351.05682373046875, 'referece_logps/rejected': -301.7943115234375, 'referece_logps/chosen': -324.2274169921875, 'logits/rejected': -0.963921070098877, 'logits/chosen': -0.8997863531112671, 'epoch': 4.34}


 72%|███████▏  | 7768/10740 [38:32:19<14:03:15, 17.02s/it]

 72%|███████▏  | 7769/10740 [38:32:38<14:29:35, 17.56s/it]

 72%|███████▏  | 7770/10740 [38:32:53<13:54:25, 16.86s/it]

 72%|███████▏  | 7771/10740 [38:33:13<14:38:34, 17.75s/it]

 72%|███████▏  | 7772/10740 [38:33:32<14:54:54, 18.09s/it]

 72%|███████▏  | 7773/10740 [38:33:51<15:13:43, 18.48s/it]

 72%|███████▏  | 7774/10740 [38:34:07<14:32:18, 17.65s/it]

 72%|███████▏  | 7775/10740 [38:34:21<13:38:16, 16.56s/it]

 72%|███████▏  | 7776/10740 [38:34:37<13:26:25, 16.32s/it]
{'loss': 0.3686, 'learning_rate': 3.736176693864408e-07, 'rewards/chosen': -1.3358970880508423, 'rewards/rejected': -3.872483491897583, 'rewards/accuracies': 0.875, 'rewards/margins': 2.536586284637451, 'policy_logps/rejected': -461.18017578125, 'policy_logps/chosen': -376.4904479980469, 'referece_logps/rejected': -422.4552917480469, 'referece_logps/chosen': -363.1314392089844, 'logits/rejected': -0.4144502580165863, 'logits/chosen': -0.34506213665008545, 'epoch': 4.34}

 72%|███████▏  | 7777/10740 [38:34:56<14:14:21, 17.30s/it]
[2024-04-03 09:49:02,246] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 72%|███████▏  | 7779/10740 [38:35:37<15:26:17, 18.77s/it]
[2024-04-03 09:49:21,103] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7780/10740 [38:35:55<15:07:40, 18.40s/it]

 72%|███████▏  | 7781/10740 [38:36:17<16:03:20, 19.53s/it]
[2024-04-03 09:50:00,818] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 7782/10740 [38:36:32<14:50:45, 18.07s/it]

 72%|███████▏  | 7783/10740 [38:36:46<13:55:08, 16.95s/it]
{'loss': 0.3825, 'learning_rate': 3.7197344367420134e-07, 'rewards/chosen': -2.3497722148895264, 'rewards/rejected': -3.9103081226348877, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5605360269546509, 'policy_logps/rejected': -374.6271057128906, 'policy_logps/chosen': -339.6601867675781, 'referece_logps/rejected': -335.5240478515625, 'referece_logps/chosen': -316.1624450683594, 'logits/rejected': -0.6433281898498535, 'logits/chosen': -0.4903240203857422, 'epoch': 4.35}


 72%|███████▏  | 7785/10740 [38:37:19<13:24:15, 16.33s/it]

 72%|███████▏  | 7786/10740 [38:37:30<12:00:21, 14.63s/it]

 73%|███████▎  | 7787/10740 [38:37:52<13:46:00, 16.78s/it]

 73%|███████▎  | 7788/10740 [38:38:06<13:10:46, 16.07s/it]

 73%|███████▎  | 7789/10740 [38:38:26<14:07:19, 17.23s/it]
{'loss': 0.3766, 'learning_rate': 3.705663347728869e-07, 'rewards/chosen': -1.3099291324615479, 'rewards/rejected': -2.6478023529052734, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3378732204437256, 'policy_logps/rejected': -543.1036987304688, 'policy_logps/chosen': -543.1350708007812, 'referece_logps/rejected': -516.6256103515625, 'referece_logps/chosen': -530.0357055664062, 'logits/rejected': 0.5269371867179871, 'logits/chosen': 0.5706431865692139, 'epoch': 4.35}


 73%|███████▎  | 7791/10740 [38:38:48<11:34:39, 14.13s/it]

 73%|███████▎  | 7792/10740 [38:39:12<13:59:32, 17.09s/it]

 73%|███████▎  | 7793/10740 [38:39:27<13:33:31, 16.56s/it]
{'loss': 0.4554, 'learning_rate': 3.696294068582941e-07, 'rewards/chosen': -1.3254238367080688, 'rewards/rejected': -2.6093521118164062, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2839282751083374, 'policy_logps/rejected': -400.201171875, 'policy_logps/chosen': -457.78472900390625, 'referece_logps/rejected': -374.107666015625, 'referece_logps/chosen': -444.530517578125, 'logits/rejected': -0.19950330257415771, 'logits/chosen': -0.26165714859962463, 'epoch': 4.35}


 73%|███████▎  | 7795/10740 [38:39:54<12:17:56, 15.03s/it]

 73%|███████▎  | 7796/10740 [38:40:15<13:45:06, 16.82s/it]
{'loss': 0.441, 'learning_rate': 3.6892731285215697e-07, 'rewards/chosen': -1.3266645669937134, 'rewards/rejected': -2.8520030975341797, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5253384113311768, 'policy_logps/rejected': -401.07110595703125, 'policy_logps/chosen': -425.63134765625, 'referece_logps/rejected': -372.5510559082031, 'referece_logps/chosen': -412.3647155761719, 'logits/rejected': -0.5256409645080566, 'logits/chosen': -0.47704190015792847, 'epoch': 4.36}


 73%|███████▎  | 7798/10740 [38:40:50<13:43:47, 16.80s/it]
{'loss': 0.3482, 'learning_rate': 3.6845953713396516e-07, 'rewards/chosen': -2.089411735534668, 'rewards/rejected': -4.071127891540527, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9817159175872803, 'policy_logps/rejected': -408.2826232910156, 'policy_logps/chosen': -360.1942138671875, 'referece_logps/rejected': -367.57135009765625, 'referece_logps/chosen': -339.3000793457031, 'logits/rejected': -0.9342197775840759, 'logits/chosen': -0.9828612804412842, 'epoch': 4.36}

 73%|███████▎  | 7799/10740 [38:41:07<13:40:07, 16.73s/it]

 73%|███████▎  | 7800/10740 [38:41:27<14:23:47, 17.63s/it]


 73%|███████▎  | 7802/10740 [38:42:01<14:06:57, 17.30s/it]

 73%|███████▎  | 7803/10740 [38:42:20<14:42:15, 18.02s/it]

 73%|███████▎  | 7804/10740 [38:42:40<15:08:03, 18.56s/it]
{'loss': 0.3108, 'learning_rate': 3.6705758922335994e-07, 'rewards/chosen': -2.594086170196533, 'rewards/rejected': -4.397642612457275, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8035564422607422, 'policy_logps/rejected': -577.0350952148438, 'policy_logps/chosen': -441.57183837890625, 'referece_logps/rejected': -533.0587158203125, 'referece_logps/chosen': -415.6309509277344, 'logits/rejected': 0.7675600051879883, 'logits/chosen': 0.6690876483917236, 'epoch': 4.36}


 73%|███████▎  | 7806/10740 [38:43:18<15:26:05, 18.94s/it]

 73%|███████▎  | 7807/10740 [38:43:38<15:35:55, 19.15s/it]
{'loss': 0.3763, 'learning_rate': 3.663573921446882e-07, 'rewards/chosen': -1.274875521659851, 'rewards/rejected': -2.0023486614227295, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7274731397628784, 'policy_logps/rejected': -366.1828918457031, 'policy_logps/chosen': -372.1790466308594, 'referece_logps/rejected': -346.1593933105469, 'referece_logps/chosen': -359.4302978515625, 'logits/rejected': -0.7579302787780762, 'logits/chosen': -0.9618169069290161, 'epoch': 4.36}

 73%|███████▎  | 7808/10740 [38:43:57<15:42:00, 19.28s/it]


 73%|███████▎  | 7810/10740 [38:44:36<15:47:01, 19.39s/it]
{'loss': 0.4027, 'learning_rate': 3.6565771374812705e-07, 'rewards/chosen': -2.334737777709961, 'rewards/rejected': -4.957231521606445, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6224942207336426, 'policy_logps/rejected': -558.3157958984375, 'policy_logps/chosen': -529.613037109375, 'referece_logps/rejected': -508.74346923828125, 'referece_logps/chosen': -506.26568603515625, 'logits/rejected': 1.1007864475250244, 'logits/chosen': 1.0858830213546753, 'epoch': 4.36}


 73%|███████▎  | 7812/10740 [38:45:10<14:44:34, 18.13s/it]

 73%|███████▎  | 7813/10740 [38:45:24<13:30:19, 16.61s/it]

 73%|███████▎  | 7814/10740 [38:45:41<13:38:58, 16.79s/it]

 73%|███████▎  | 7815/10740 [38:45:57<13:26:14, 16.54s/it]
{'loss': 0.3831, 'learning_rate': 3.6449273726949537e-07, 'rewards/chosen': -1.9612635374069214, 'rewards/rejected': -4.67606258392334, 'rewards/accuracies': 0.875, 'rewards/margins': 2.714799165725708, 'policy_logps/rejected': -380.16217041015625, 'policy_logps/chosen': -313.8690490722656, 'referece_logps/rejected': -333.4015808105469, 'referece_logps/chosen': -294.2563781738281, 'logits/rejected': -1.0651578903198242, 'logits/chosen': -1.0411274433135986, 'epoch': 4.37}

 73%|███████▎  | 7816/10740 [38:46:13<13:23:04, 16.48s/it]

 73%|███████▎  | 7817/10740 [38:46:33<14:15:05, 17.55s/it]


 73%|███████▎  | 7819/10740 [38:47:12<15:04:26, 18.58s/it]

 73%|███████▎  | 7820/10740 [38:47:29<14:30:39, 17.89s/it]

 73%|███████▎  | 7821/10740 [38:47:43<13:36:48, 16.79s/it]

 73%|███████▎  | 7822/10740 [38:48:03<14:18:59, 17.66s/it]

 73%|███████▎  | 7823/10740 [38:48:18<13:41:31, 16.90s/it]
{'loss': 0.3235, 'learning_rate': 3.6263178165295696e-07, 'rewards/chosen': -2.004481792449951, 'rewards/rejected': -5.587379455566406, 'rewards/accuracies': 1.0, 'rewards/margins': 3.582897663116455, 'policy_logps/rejected': -372.7246398925781, 'policy_logps/chosen': -357.75439453125, 'referece_logps/rejected': -316.85089111328125, 'referece_logps/chosen': -337.7095947265625, 'logits/rejected': -0.33369916677474976, 'logits/chosen': -0.34498757123947144, 'epoch': 4.37}


 73%|███████▎  | 7825/10740 [38:48:57<14:45:08, 18.22s/it]
{'loss': 0.3445, 'learning_rate': 3.621671220278992e-07, 'rewards/chosen': -1.7274874448776245, 'rewards/rejected': -3.4501802921295166, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7226930856704712, 'policy_logps/rejected': -414.66888427734375, 'policy_logps/chosen': -329.5941162109375, 'referece_logps/rejected': -380.1670837402344, 'referece_logps/chosen': -312.3192138671875, 'logits/rejected': -0.38177067041397095, 'logits/chosen': -0.3947640061378479, 'epoch': 4.37}


 73%|███████▎  | 7827/10740 [38:49:19<11:43:35, 14.49s/it]
{'loss': 0.4984, 'learning_rate': 3.6170269445269395e-07, 'rewards/chosen': -2.3221797943115234, 'rewards/rejected': -3.031200885772705, 'rewards/accuracies': 0.375, 'rewards/margins': 0.7090210318565369, 'policy_logps/rejected': -357.46563720703125, 'policy_logps/chosen': -271.708251953125, 'referece_logps/rejected': -327.15362548828125, 'referece_logps/chosen': -248.4864501953125, 'logits/rejected': -1.099134087562561, 'logits/chosen': -1.0691200494766235, 'epoch': 4.37}


 73%|███████▎  | 7829/10740 [38:49:54<13:22:43, 16.55s/it]
{'loss': 0.3819, 'learning_rate': 3.6123849909630455e-07, 'rewards/chosen': -1.3647345304489136, 'rewards/rejected': -2.363309860229492, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9985755085945129, 'policy_logps/rejected': -636.2242431640625, 'policy_logps/chosen': -495.7547607421875, 'referece_logps/rejected': -612.5911254882812, 'referece_logps/chosen': -482.10736083984375, 'logits/rejected': -0.9879493117332458, 'logits/chosen': -0.7860407829284668, 'epoch': 4.37}


 73%|███████▎  | 7831/10740 [38:50:34<14:45:40, 18.27s/it]

 73%|███████▎  | 7832/10740 [38:50:53<14:54:38, 18.46s/it]
{'loss': 0.3753, 'learning_rate': 3.6054264184140015e-07, 'rewards/chosen': -2.3455166816711426, 'rewards/rejected': -3.465975522994995, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1204586029052734, 'policy_logps/rejected': -386.25225830078125, 'policy_logps/chosen': -393.0006408691406, 'referece_logps/rejected': -351.592529296875, 'referece_logps/chosen': -369.54547119140625, 'logits/rejected': -0.94499272108078, 'logits/chosen': -1.1014256477355957, 'epoch': 4.38}


 73%|███████▎  | 7834/10740 [38:51:30<14:57:48, 18.54s/it]

 73%|███████▎  | 7835/10740 [38:51:46<14:21:02, 17.78s/it]
{'loss': 0.335, 'learning_rate': 3.598473080283981e-07, 'rewards/chosen': -2.0833139419555664, 'rewards/rejected': -3.063302755355835, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9799888730049133, 'policy_logps/rejected': -344.8599853515625, 'policy_logps/chosen': -518.036865234375, 'referece_logps/rejected': -314.2269592285156, 'referece_logps/chosen': -497.20367431640625, 'logits/rejected': -0.6067784428596497, 'logits/chosen': -0.631507933139801, 'epoch': 4.38}


 73%|███████▎  | 7837/10740 [38:52:17<13:07:37, 16.28s/it]

 73%|███████▎  | 7838/10740 [38:52:29<12:07:01, 15.03s/it]
{'loss': 0.3733, 'learning_rate': 3.5915249822648e-07, 'rewards/chosen': -1.1784857511520386, 'rewards/rejected': -2.5041611194610596, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3256752490997314, 'policy_logps/rejected': -447.4720764160156, 'policy_logps/chosen': -457.95782470703125, 'referece_logps/rejected': -422.4305419921875, 'referece_logps/chosen': -446.1729736328125, 'logits/rejected': -0.753480851650238, 'logits/chosen': -0.8108900785446167, 'epoch': 4.38}


 73%|███████▎  | 7840/10740 [38:53:01<12:27:24, 15.46s/it]

 73%|███████▎  | 7841/10740 [38:53:16<12:21:13, 15.34s/it]
{'loss': 0.4084, 'learning_rate': 3.584582130043973e-07, 'rewards/chosen': -1.756993293762207, 'rewards/rejected': -4.086970329284668, 'rewards/accuracies': 1.0, 'rewards/margins': 2.329977035522461, 'policy_logps/rejected': -438.3662109375, 'policy_logps/chosen': -516.1654052734375, 'referece_logps/rejected': -397.4964904785156, 'referece_logps/chosen': -498.59552001953125, 'logits/rejected': -1.0718190670013428, 'logits/chosen': -1.0521575212478638, 'epoch': 4.38}

 73%|███████▎  | 7842/10740 [38:53:30<11:54:51, 14.80s/it]

 73%|███████▎  | 7843/10740 [38:53:42<11:12:46, 13.93s/it]


 73%|███████▎  | 7845/10740 [38:54:22<13:52:50, 17.26s/it]
{'loss': 0.3412, 'learning_rate': 3.5753331637025463e-07, 'rewards/chosen': -1.0141521692276, 'rewards/rejected': -2.5342068672180176, 'rewards/accuracies': 0.875, 'rewards/margins': 1.520054817199707, 'policy_logps/rejected': -289.7593078613281, 'policy_logps/chosen': -240.03741455078125, 'referece_logps/rejected': -264.417236328125, 'referece_logps/chosen': -229.89588928222656, 'logits/rejected': -0.10712838172912598, 'logits/chosen': 0.0004138350486755371, 'epoch': 4.38}

 73%|███████▎  | 7846/10740 [38:54:42<14:26:41, 17.97s/it]

 73%|███████▎  | 7847/10740 [38:55:02<14:55:45, 18.58s/it]


 73%|███████▎  | 7849/10740 [38:55:39<15:01:55, 18.72s/it]

 73%|███████▎  | 7850/10740 [38:55:57<14:50:57, 18.50s/it]

 73%|███████▎  | 7851/10740 [38:56:17<15:05:04, 18.80s/it]
{'loss': 0.3699, 'learning_rate': 3.5614772485669164e-07, 'rewards/chosen': -1.567417860031128, 'rewards/rejected': -3.729616403579712, 'rewards/accuracies': 0.875, 'rewards/margins': 2.162198305130005, 'policy_logps/rejected': -284.3758850097656, 'policy_logps/chosen': -398.563720703125, 'referece_logps/rejected': -247.07974243164062, 'referece_logps/chosen': -382.8895263671875, 'logits/rejected': -0.5985170006752014, 'logits/chosen': -0.8283344507217407, 'epoch': 4.39}

 73%|███████▎  | 7852/10740 [38:56:38<15:35:30, 19.44s/it]

 73%|███████▎  | 7853/10740 [38:56:56<15:19:34, 19.11s/it]


 73%|███████▎  | 7855/10740 [38:57:37<15:58:18, 19.93s/it]

 73%|███████▎  | 7856/10740 [38:57:57<15:52:17, 19.81s/it]
{'loss': 0.3612, 'learning_rate': 3.549946754677454e-07, 'rewards/chosen': -1.8595002889633179, 'rewards/rejected': -3.51228928565979, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6527889966964722, 'policy_logps/rejected': -358.7198181152344, 'policy_logps/chosen': -306.04400634765625, 'referece_logps/rejected': -323.596923828125, 'referece_logps/chosen': -287.448974609375, 'logits/rejected': -1.1038603782653809, 'logits/chosen': -1.0649070739746094, 'epoch': 4.39}


 73%|███████▎  | 7858/10740 [38:58:33<15:06:14, 18.87s/it]

 73%|███████▎  | 7859/10740 [38:58:51<14:53:40, 18.61s/it]

 73%|███████▎  | 7860/10740 [38:59:10<15:00:58, 18.77s/it]

 73%|███████▎  | 7861/10740 [38:59:34<16:14:57, 20.32s/it]

 73%|███████▎  | 7862/10740 [38:59:51<15:26:14, 19.31s/it]
{'loss': 0.4253, 'learning_rate': 3.536129523700735e-07, 'rewards/chosen': -2.6027040481567383, 'rewards/rejected': -3.3583226203918457, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7556184530258179, 'policy_logps/rejected': -337.93560791015625, 'policy_logps/chosen': -274.2181396484375, 'referece_logps/rejected': -304.3523864746094, 'referece_logps/chosen': -248.1910858154297, 'logits/rejected': -0.2597126364707947, 'logits/chosen': -0.272972971200943, 'epoch': 4.39}


 73%|███████▎  | 7864/10740 [39:00:29<15:37:33, 19.56s/it]

 73%|███████▎  | 7865/10740 [39:00:49<15:39:53, 19.62s/it]
{'loss': 0.2489, 'learning_rate': 3.5292288421011483e-07, 'rewards/chosen': -1.5306956768035889, 'rewards/rejected': -3.9128339290618896, 'rewards/accuracies': 0.875, 'rewards/margins': 2.382138729095459, 'policy_logps/rejected': -359.1518249511719, 'policy_logps/chosen': -417.8509521484375, 'referece_logps/rejected': -320.0234680175781, 'referece_logps/chosen': -402.5439453125, 'logits/rejected': -1.0911298990249634, 'logits/chosen': -1.2028915882110596, 'epoch': 4.39}


 73%|███████▎  | 7867/10740 [39:01:29<15:49:07, 19.82s/it]

 73%|███████▎  | 7868/10740 [39:01:49<15:55:25, 19.96s/it]
{'loss': 0.3991, 'learning_rate': 3.5223334572937947e-07, 'rewards/chosen': -1.6252198219299316, 'rewards/rejected': -3.8038806915283203, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1786608695983887, 'policy_logps/rejected': -421.37896728515625, 'policy_logps/chosen': -323.9262390136719, 'referece_logps/rejected': -383.34014892578125, 'referece_logps/chosen': -307.6740417480469, 'logits/rejected': -1.179215908050537, 'logits/chosen': -1.3046295642852783, 'epoch': 4.4}

 73%|███████▎  | 7869/10740 [39:02:08<15:36:04, 19.56s/it]

 73%|███████▎  | 7870/10740 [39:02:21<14:01:50, 17.60s/it]

 73%|███████▎  | 7871/10740 [39:02:37<13:37:55, 17.11s/it]


 73%|███████▎  | 7873/10740 [39:03:20<15:23:53, 19.33s/it]
{'loss': 0.2234, 'learning_rate': 3.5108529352593586e-07, 'rewards/chosen': -1.1462312936782837, 'rewards/rejected': -2.8918228149414062, 'rewards/accuracies': 1.0, 'rewards/margins': 1.745591402053833, 'policy_logps/rejected': -339.8736877441406, 'policy_logps/chosen': -315.9398193359375, 'referece_logps/rejected': -310.9554443359375, 'referece_logps/chosen': -304.47747802734375, 'logits/rejected': -0.3768725097179413, 'logits/chosen': -0.32546862959861755, 'epoch': 4.4}


 73%|███████▎  | 7875/10740 [39:04:04<16:21:36, 20.56s/it]

 73%|███████▎  | 7876/10740 [39:04:18<14:51:55, 18.69s/it]

 73%|███████▎  | 7877/10740 [39:04:32<13:40:44, 17.20s/it]
{'loss': 0.4211, 'learning_rate': 3.501679140047101e-07, 'rewards/chosen': -1.5075738430023193, 'rewards/rejected': -2.1186459064483643, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6110719442367554, 'policy_logps/rejected': -396.0462341308594, 'policy_logps/chosen': -422.41986083984375, 'referece_logps/rejected': -374.8597412109375, 'referece_logps/chosen': -407.3440856933594, 'logits/rejected': -0.4605533182621002, 'logits/chosen': -0.4855036437511444, 'epoch': 4.4}


 73%|███████▎  | 7879/10740 [39:05:08<13:51:15, 17.43s/it]

 73%|███████▎  | 7880/10740 [39:05:20<12:32:50, 15.79s/it]
{'loss': 0.385, 'learning_rate': 3.4948049988089366e-07, 'rewards/chosen': -2.699352264404297, 'rewards/rejected': -3.5451180934906006, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8457657694816589, 'policy_logps/rejected': -461.94342041015625, 'policy_logps/chosen': -379.7295837402344, 'referece_logps/rejected': -426.49224853515625, 'referece_logps/chosen': -352.7360534667969, 'logits/rejected': 0.0961654782295227, 'logits/chosen': 0.2010515332221985, 'epoch': 4.4}


 73%|███████▎  | 7882/10740 [39:05:56<13:25:08, 16.90s/it]
{'loss': 0.3245, 'learning_rate': 3.490225195953341e-07, 'rewards/chosen': -1.5528264045715332, 'rewards/rejected': -2.5213143825531006, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9684880375862122, 'policy_logps/rejected': -295.2998046875, 'policy_logps/chosen': -290.5334777832031, 'referece_logps/rejected': -270.086669921875, 'referece_logps/chosen': -275.0052185058594, 'logits/rejected': -0.40231359004974365, 'logits/chosen': -0.36127135157585144, 'epoch': 4.4}


 73%|███████▎  | 7884/10740 [39:06:34<14:22:27, 18.12s/it]
{'loss': 0.3616, 'learning_rate': 3.4856477614176283e-07, 'rewards/chosen': -0.9551888704299927, 'rewards/rejected': -4.711117744445801, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7559289932250977, 'policy_logps/rejected': -371.4826965332031, 'policy_logps/chosen': -297.603515625, 'referece_logps/rejected': -324.37152099609375, 'referece_logps/chosen': -288.0516357421875, 'logits/rejected': -1.029098391532898, 'logits/chosen': -1.0190287828445435, 'epoch': 4.4}


 73%|███████▎  | 7886/10740 [39:07:12<14:27:31, 18.24s/it]
{'loss': 0.3217, 'learning_rate': 3.481072696867106e-07, 'rewards/chosen': -1.2003921270370483, 'rewards/rejected': -3.9539918899536133, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7535998821258545, 'policy_logps/rejected': -315.4789123535156, 'policy_logps/chosen': -310.9129638671875, 'referece_logps/rejected': -275.9389953613281, 'referece_logps/chosen': -298.9090576171875, 'logits/rejected': -0.16543982923030853, 'logits/chosen': -0.22495222091674805, 'epoch': 4.41}

 73%|███████▎  | 7887/10740 [39:07:28<14:03:00, 17.73s/it]


 73%|███████▎  | 7889/10740 [39:08:04<13:54:53, 17.57s/it]

 73%|███████▎  | 7890/10740 [39:08:24<14:26:42, 18.25s/it]
{'loss': 0.4919, 'learning_rate': 3.471929684378597e-07, 'rewards/chosen': -1.531042456626892, 'rewards/rejected': -2.8984620571136475, 'rewards/accuracies': 0.75, 'rewards/margins': 1.367419719696045, 'policy_logps/rejected': -369.1158752441406, 'policy_logps/chosen': -411.51690673828125, 'referece_logps/rejected': -340.13128662109375, 'referece_logps/chosen': -396.2064514160156, 'logits/rejected': -0.5100279450416565, 'logits/chosen': -0.46627870202064514, 'epoch': 4.41}

 73%|███████▎  | 7891/10740 [39:08:39<13:47:47, 17.43s/it]


 73%|███████▎  | 7893/10740 [39:09:12<13:18:42, 16.83s/it]
{'loss': 0.324, 'learning_rate': 3.465078658596445e-07, 'rewards/chosen': -1.0217591524124146, 'rewards/rejected': -2.650167465209961, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6284080743789673, 'policy_logps/rejected': -451.4842529296875, 'policy_logps/chosen': -498.51641845703125, 'referece_logps/rejected': -424.98260498046875, 'referece_logps/chosen': -488.298828125, 'logits/rejected': -0.7796425819396973, 'logits/chosen': -0.8994683623313904, 'epoch': 4.41}


 74%|███████▎  | 7895/10740 [39:09:46<13:21:28, 16.90s/it]

 74%|███████▎  | 7896/10740 [39:10:06<13:59:39, 17.71s/it]

 74%|███████▎  | 7897/10740 [39:10:22<13:45:58, 17.43s/it]

 74%|███████▎  | 7898/10740 [39:10:42<14:21:12, 18.18s/it]
{'loss': 0.282, 'learning_rate': 3.4536721724020133e-07, 'rewards/chosen': -1.9453952312469482, 'rewards/rejected': -3.380093574523926, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4346981048583984, 'policy_logps/rejected': -361.2186279296875, 'policy_logps/chosen': -249.27769470214844, 'referece_logps/rejected': -327.4176940917969, 'referece_logps/chosen': -229.82373046875, 'logits/rejected': -1.1524126529693604, 'logits/chosen': -1.0961179733276367, 'epoch': 4.41}

 74%|███████▎  | 7899/10740 [39:10:59<14:03:17, 17.81s/it]

 74%|███████▎  | 7900/10740 [39:11:21<14:53:28, 18.88s/it]

 74%|███████▎  | 7901/10740 [39:11:35<13:53:18, 17.61s/it]


 74%|███████▎  | 7903/10740 [39:12:12<14:21:13, 18.21s/it]
{'loss': 0.2605, 'learning_rate': 3.4422805713191305e-07, 'rewards/chosen': -1.8022713661193848, 'rewards/rejected': -3.4244167804718018, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6221455335617065, 'policy_logps/rejected': -345.9598693847656, 'policy_logps/chosen': -280.0365295410156, 'referece_logps/rejected': -311.7156982421875, 'referece_logps/chosen': -262.0138244628906, 'logits/rejected': -0.5405218601226807, 'logits/chosen': -0.6335485577583313, 'epoch': 4.42}

 74%|███████▎  | 7904/10740 [39:12:25<13:11:42, 16.75s/it]


 74%|███████▎  | 7906/10740 [39:13:00<13:29:10, 17.13s/it]

 74%|███████▎  | 7907/10740 [39:13:18<13:46:52, 17.51s/it]
{'loss': 0.2993, 'learning_rate': 3.433178025140656e-07, 'rewards/chosen': -1.5847482681274414, 'rewards/rejected': -3.079340934753418, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4945926666259766, 'policy_logps/rejected': -310.4212646484375, 'policy_logps/chosen': -350.34417724609375, 'referece_logps/rejected': -279.62786865234375, 'referece_logps/chosen': -334.4967041015625, 'logits/rejected': -0.13602390885353088, 'logits/chosen': -0.054179705679416656, 'epoch': 4.42}

 74%|███████▎  | 7908/10740 [39:13:37<14:06:18, 17.93s/it]

 74%|███████▎  | 7909/10740 [39:13:54<13:44:40, 17.48s/it]

 74%|███████▎  | 7910/10740 [39:14:12<13:52:20, 17.65s/it]

 74%|███████▎  | 7911/10740 [39:14:24<12:29:20, 15.89s/it]


 74%|███████▎  | 7913/10740 [39:14:58<13:03:16, 16.62s/it]
{'loss': 0.3477, 'learning_rate': 3.419542128063494e-07, 'rewards/chosen': -1.7887060642242432, 'rewards/rejected': -3.552657127380371, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7639508247375488, 'policy_logps/rejected': -285.8507385253906, 'policy_logps/chosen': -299.0004577636719, 'referece_logps/rejected': -250.32418823242188, 'referece_logps/chosen': -281.1133728027344, 'logits/rejected': -1.0257964134216309, 'logits/chosen': -1.0412784814834595, 'epoch': 4.42}

 74%|███████▎  | 7914/10740 [39:15:18<13:39:39, 17.40s/it]


 74%|███████▎  | 7916/10740 [39:15:54<13:58:17, 17.81s/it]

 74%|███████▎  | 7917/10740 [39:16:14<14:24:10, 18.37s/it]

 74%|███████▎  | 7918/10740 [39:16:36<15:15:30, 19.47s/it]
{'loss': 0.3657, 'learning_rate': 3.4081953375936647e-07, 'rewards/chosen': -1.712355136871338, 'rewards/rejected': -2.867394208908081, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1550391912460327, 'policy_logps/rejected': -385.27093505859375, 'policy_logps/chosen': -429.61053466796875, 'referece_logps/rejected': -356.5969543457031, 'referece_logps/chosen': -412.4869689941406, 'logits/rejected': -0.417603462934494, 'logits/chosen': -0.3725830316543579, 'epoch': 4.42}

 74%|███████▎  | 7919/10740 [39:16:53<14:43:44, 18.80s/it]

 74%|███████▎  | 7920/10740 [39:17:14<15:02:53, 19.21s/it]

 74%|███████▍  | 7921/10740 [39:17:32<14:51:06, 18.97s/it]

 74%|███████▍  | 7922/10740 [39:17:52<14:59:14, 19.15s/it]

 74%|███████▍  | 7923/10740 [39:18:11<15:07:12, 19.32s/it]

 74%|███████▍  | 7924/10740 [39:18:26<13:59:11, 17.88s/it]

 74%|███████▍  | 7925/10740 [39:18:48<14:57:01, 19.12s/it]

 74%|███████▍  | 7926/10740 [39:19:04<14:11:22, 18.15s/it]


 74%|███████▍  | 7928/10740 [39:19:38<13:55:08, 17.82s/it]

 74%|███████▍  | 7929/10740 [39:19:54<13:29:33, 17.28s/it]
{'loss': 0.4069, 'learning_rate': 3.383285194417479e-07, 'rewards/chosen': -1.6806126832962036, 'rewards/rejected': -3.777926206588745, 'rewards/accuracies': 0.875, 'rewards/margins': 2.097313404083252, 'policy_logps/rejected': -563.0574340820312, 'policy_logps/chosen': -432.52679443359375, 'referece_logps/rejected': -525.2781982421875, 'referece_logps/chosen': -415.7206726074219, 'logits/rejected': -0.9064861536026001, 'logits/chosen': -0.9421250820159912, 'epoch': 4.43}

 74%|███████▍  | 7930/10740 [39:20:08<12:41:36, 16.26s/it]


 74%|███████▍  | 7932/10740 [39:20:49<14:09:30, 18.15s/it]
{'loss': 0.258, 'learning_rate': 3.3765041454144916e-07, 'rewards/chosen': -1.8801467418670654, 'rewards/rejected': -3.9863569736480713, 'rewards/accuracies': 0.75, 'rewards/margins': 2.106210231781006, 'policy_logps/rejected': -428.4485778808594, 'policy_logps/chosen': -517.4222412109375, 'referece_logps/rejected': -388.5850524902344, 'referece_logps/chosen': -498.62078857421875, 'logits/rejected': -0.56618332862854, 'logits/chosen': -0.540269672870636, 'epoch': 4.43}

 74%|███████▍  | 7933/10740 [39:21:05<13:46:44, 17.67s/it]

 74%|███████▍  | 7934/10740 [39:21:25<14:20:05, 18.39s/it]

 74%|███████▍  | 7935/10740 [39:21:45<14:44:16, 18.91s/it]

 74%|███████▍  | 7936/10740 [39:22:05<14:57:47, 19.21s/it]

 74%|███████▍  | 7937/10740 [39:22:23<14:43:13, 18.91s/it]


 74%|███████▍  | 7939/10740 [39:22:59<14:17:52, 18.38s/it]
{'loss': 0.3489, 'learning_rate': 3.3607027921369034e-07, 'rewards/chosen': -1.7192384004592896, 'rewards/rejected': -2.9636435508728027, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2444047927856445, 'policy_logps/rejected': -298.4734191894531, 'policy_logps/chosen': -292.5785217285156, 'referece_logps/rejected': -268.83697509765625, 'referece_logps/chosen': -275.3861083984375, 'logits/rejected': -1.215291142463684, 'logits/chosen': -1.26055109500885, 'epoch': 4.44}


 74%|███████▍  | 7941/10740 [39:23:25<12:21:02, 15.88s/it]

 74%|███████▍  | 7942/10740 [39:23:37<11:26:28, 14.72s/it]
{'loss': 0.4352, 'learning_rate': 3.3539398373899806e-07, 'rewards/chosen': -2.0892155170440674, 'rewards/rejected': -2.70701003074646, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6177942752838135, 'policy_logps/rejected': -481.1259460449219, 'policy_logps/chosen': -456.5887451171875, 'referece_logps/rejected': -454.05584716796875, 'referece_logps/chosen': -435.69659423828125, 'logits/rejected': -1.1189477443695068, 'logits/chosen': -1.3864171504974365, 'epoch': 4.44}


 74%|███████▍  | 7944/10740 [39:24:13<12:38:59, 16.29s/it]

 74%|███████▍  | 7945/10740 [39:24:27<12:05:24, 15.57s/it]

 74%|███████▍  | 7946/10740 [39:24:45<12:32:25, 16.16s/it]
{'loss': 0.3848, 'learning_rate': 3.3449310280063545e-07, 'rewards/chosen': -2.0719587802886963, 'rewards/rejected': -3.042128562927246, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9701699018478394, 'policy_logps/rejected': -547.9696655273438, 'policy_logps/chosen': -433.5860595703125, 'referece_logps/rejected': -517.54833984375, 'referece_logps/chosen': -412.8664855957031, 'logits/rejected': 0.07756221294403076, 'logits/chosen': 0.22985993325710297, 'epoch': 4.44}

 74%|███████▍  | 7947/10740 [39:25:00<12:16:11, 15.81s/it]


 74%|███████▍  | 7949/10740 [39:25:37<13:28:18, 17.38s/it]
{'loss': 0.267, 'learning_rate': 3.3381807758473545e-07, 'rewards/chosen': -2.227851152420044, 'rewards/rejected': -5.648631572723389, 'rewards/accuracies': 0.875, 'rewards/margins': 3.420780658721924, 'policy_logps/rejected': -426.06890869140625, 'policy_logps/chosen': -370.057373046875, 'referece_logps/rejected': -369.58258056640625, 'referece_logps/chosen': -347.7789001464844, 'logits/rejected': -0.11396843194961548, 'logits/chosen': -0.2061692476272583, 'epoch': 4.44}

 74%|███████▍  | 7950/10740 [39:26:00<14:36:28, 18.85s/it]


 74%|███████▍  | 7952/10740 [39:26:43<15:43:34, 20.31s/it]
{'loss': 0.3035, 'learning_rate': 3.3314359768671884e-07, 'rewards/chosen': -1.0074478387832642, 'rewards/rejected': -3.9966869354248047, 'rewards/accuracies': 1.0, 'rewards/margins': 2.98923921585083, 'policy_logps/rejected': -393.2682800292969, 'policy_logps/chosen': -367.9267883300781, 'referece_logps/rejected': -353.3014221191406, 'referece_logps/chosen': -357.852294921875, 'logits/rejected': 0.17678949236869812, 'logits/chosen': 0.005911916494369507, 'epoch': 4.44}

 74%|███████▍  | 7953/10740 [39:27:04<15:59:18, 20.65s/it]

 74%|███████▍  | 7954/10740 [39:27:24<15:50:08, 20.46s/it]

 74%|███████▍  | 7955/10740 [39:27:37<13:53:08, 17.95s/it]


 74%|███████▍  | 7957/10740 [39:28:09<13:12:12, 17.08s/it]
{'loss': 0.3727, 'learning_rate': 3.320206778402328e-07, 'rewards/chosen': -1.9560970067977905, 'rewards/rejected': -3.9074995517730713, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9514025449752808, 'policy_logps/rejected': -377.76055908203125, 'policy_logps/chosen': -473.76397705078125, 'referece_logps/rejected': -338.68560791015625, 'referece_logps/chosen': -454.2030029296875, 'logits/rejected': -0.6301603317260742, 'logits/chosen': -0.6850063800811768, 'epoch': 4.45}


 74%|███████▍  | 7959/10740 [39:28:39<12:19:08, 15.95s/it]
{'loss': 0.3932, 'learning_rate': 3.315719350391947e-07, 'rewards/chosen': -1.761811375617981, 'rewards/rejected': -2.7224931716918945, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9606817364692688, 'policy_logps/rejected': -346.751953125, 'policy_logps/chosen': -358.3208312988281, 'referece_logps/rejected': -319.52703857421875, 'referece_logps/chosen': -340.70269775390625, 'logits/rejected': -1.093113660812378, 'logits/chosen': -1.0477745532989502, 'epoch': 4.45}

 74%|███████▍  | 7960/10740 [39:29:01<13:40:27, 17.71s/it]

 74%|███████▍  | 7961/10740 [39:29:22<14:32:16, 18.83s/it]

 74%|███████▍  | 7962/10740 [39:29:37<13:30:57, 17.52s/it]

 74%|███████▍  | 7963/10740 [39:29:51<12:41:53, 16.46s/it]


 74%|███████▍  | 7965/10740 [39:30:13<10:37:28, 13.78s/it]
{'loss': 0.3746, 'learning_rate': 3.302271663727546e-07, 'rewards/chosen': -2.1158366203308105, 'rewards/rejected': -3.0591788291931152, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9433422684669495, 'policy_logps/rejected': -531.5733032226562, 'policy_logps/chosen': -516.9967651367188, 'referece_logps/rejected': -500.9815368652344, 'referece_logps/chosen': -495.83843994140625, 'logits/rejected': 0.26340293884277344, 'logits/chosen': 0.32694777846336365, 'epoch': 4.45}


 74%|███████▍  | 7967/10740 [39:30:35<9:36:14, 12.47s/it]

 74%|███████▍  | 7968/10740 [39:30:55<11:19:29, 14.71s/it]
{'loss': 0.3504, 'learning_rate': 3.295556041504035e-07, 'rewards/chosen': -1.8594120740890503, 'rewards/rejected': -2.7209107875823975, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8614985346794128, 'policy_logps/rejected': -528.1620483398438, 'policy_logps/chosen': -456.85552978515625, 'referece_logps/rejected': -500.9530029296875, 'referece_logps/chosen': -438.2614440917969, 'logits/rejected': -0.7596227526664734, 'logits/chosen': -0.8033146858215332, 'epoch': 4.45}

 74%|███████▍  | 7969/10740 [39:31:06<10:22:50, 13.49s/it]

 74%|███████▍  | 7970/10740 [39:31:26<11:50:29, 15.39s/it]


 74%|███████▍  | 7972/10740 [39:31:57<11:37:28, 15.12s/it]
{'loss': 0.4215, 'learning_rate': 3.2866104164870076e-07, 'rewards/chosen': -2.116964817047119, 'rewards/rejected': -3.355555534362793, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2385908365249634, 'policy_logps/rejected': -404.83612060546875, 'policy_logps/chosen': -248.6216278076172, 'referece_logps/rejected': -371.2805480957031, 'referece_logps/chosen': -227.4519805908203, 'logits/rejected': -1.1210706233978271, 'logits/chosen': -1.1333189010620117, 'epoch': 4.45}

 74%|███████▍  | 7973/10740 [39:32:17<12:38:03, 16.44s/it]


 74%|███████▍  | 7975/10740 [39:32:58<14:04:18, 18.32s/it]
{'loss': 0.355, 'learning_rate': 3.27990760830421e-07, 'rewards/chosen': -1.7059706449508667, 'rewards/rejected': -3.9576334953308105, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2516627311706543, 'policy_logps/rejected': -338.41693115234375, 'policy_logps/chosen': -322.81011962890625, 'referece_logps/rejected': -298.84063720703125, 'referece_logps/chosen': -305.75042724609375, 'logits/rejected': -0.9187736511230469, 'logits/chosen': -0.947190523147583, 'epoch': 4.46}

 74%|███████▍  | 7976/10740 [39:33:12<13:07:53, 17.10s/it]

 74%|███████▍  | 7977/10740 [39:33:31<13:37:58, 17.76s/it]

 74%|███████▍  | 7978/10740 [39:33:51<14:03:53, 18.33s/it]

 74%|███████▍  | 7979/10740 [39:34:13<14:50:04, 19.34s/it]

 74%|███████▍  | 7980/10740 [39:34:31<14:38:24, 19.10s/it]

 74%|███████▍  | 7981/10740 [39:34:43<13:05:09, 17.07s/it]

 74%|███████▍  | 7982/10740 [39:35:06<14:14:55, 18.60s/it]

 74%|███████▍  | 7983/10740 [39:35:18<12:50:04, 16.76s/it]

 74%|███████▍  | 7984/10740 [39:35:32<12:13:11, 15.96s/it]

 74%|███████▍  | 7985/10740 [39:35:52<13:02:33, 17.04s/it]


 74%|███████▍  | 7987/10740 [39:36:26<13:09:07, 17.20s/it]
{'loss': 0.3717, 'learning_rate': 3.2531514391689364e-07, 'rewards/chosen': -1.557227611541748, 'rewards/rejected': -2.721993923187256, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1647664308547974, 'policy_logps/rejected': -442.5102844238281, 'policy_logps/chosen': -426.3195495605469, 'referece_logps/rejected': -415.2903137207031, 'referece_logps/chosen': -410.7472839355469, 'logits/rejected': 0.41481614112854004, 'logits/chosen': 0.2918161451816559, 'epoch': 4.46}

 74%|███████▍  | 7988/10740 [39:36:47<13:57:29, 18.26s/it]

 74%|███████▍  | 7989/10740 [39:36:57<12:12:51, 15.98s/it]

 74%|███████▍  | 7990/10740 [39:37:10<11:30:20, 15.06s/it]

 74%|███████▍  | 7991/10740 [39:37:30<12:31:44, 16.41s/it]

 74%|███████▍  | 7992/10740 [39:37:48<12:57:57, 16.99s/it]

 74%|███████▍  | 7993/10740 [39:38:08<13:35:21, 17.81s/it]

 74%|███████▍  | 7994/10740 [39:38:30<14:37:48, 19.18s/it]


 74%|███████▍  | 7996/10740 [39:39:04<13:43:06, 18.00s/it]
{'loss': 0.4671, 'learning_rate': 3.2331422823429475e-07, 'rewards/chosen': -1.8128139972686768, 'rewards/rejected': -2.836188793182373, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0233746767044067, 'policy_logps/rejected': -260.7943115234375, 'policy_logps/chosen': -233.9069366455078, 'referece_logps/rejected': -232.43240356445312, 'referece_logps/chosen': -215.77882385253906, 'logits/rejected': -0.7138161659240723, 'logits/chosen': -0.7615426182746887, 'epoch': 4.47}


 74%|███████▍  | 7998/10740 [39:39:44<14:30:38, 19.05s/it]
{'loss': 0.3767, 'learning_rate': 3.2287025679466774e-07, 'rewards/chosen': -1.907459020614624, 'rewards/rejected': -2.8334200382232666, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9259610176086426, 'policy_logps/rejected': -460.4075012207031, 'policy_logps/chosen': -456.3682556152344, 'referece_logps/rejected': -432.0732421875, 'referece_logps/chosen': -437.2936706542969, 'logits/rejected': -0.29015272855758667, 'logits/chosen': -0.17208701372146606, 'epoch': 4.47}

 74%|███████▍  | 7999/10740 [39:39:59<13:35:48, 17.86s/it]

 74%|███████▍  | 8000/10740 [39:40:17<13:39:25, 17.94s/it]

 74%|███████▍  | 8001/10740 [39:40:44<15:37:53, 20.55s/it]

 75%|███████▍  | 8002/10740 [39:41:04<15:27:20, 20.32s/it]

 75%|███████▍  | 8003/10740 [39:41:17<13:47:24, 18.14s/it]

 75%|███████▍  | 8004/10740 [39:41:31<12:54:06, 16.98s/it]

 75%|███████▍  | 8005/10740 [39:41:47<12:35:33, 16.58s/it]

 75%|███████▍  | 8006/10740 [39:42:05<13:04:33, 17.22s/it]

 75%|███████▍  | 8007/10740 [39:42:26<13:49:44, 18.22s/it]

 75%|███████▍  | 8008/10740 [39:42:41<13:07:26, 17.29s/it]

 75%|███████▍  | 8009/10740 [39:43:01<13:47:38, 18.18s/it]

 75%|███████▍  | 8010/10740 [39:43:18<13:25:06, 17.69s/it]

 75%|███████▍  | 8011/10740 [39:43:38<14:00:03, 18.47s/it]

 75%|███████▍  | 8012/10740 [39:43:58<14:16:42, 18.84s/it]

 75%|███████▍  | 8013/10740 [39:44:14<13:37:27, 17.99s/it]

 75%|███████▍  | 8014/10740 [39:44:31<13:25:52, 17.74s/it]

 75%|███████▍  | 8015/10740 [39:44:51<13:51:11, 18.30s/it]

 75%|███████▍  | 8016/10740 [39:45:07<13:22:56, 17.69s/it]

 75%|███████▍  | 8017/10740 [39:45:28<14:09:52, 18.73s/it]

 75%|███████▍  | 8018/10740 [39:45:46<13:52:16, 18.35s/it]

 75%|███████▍  | 8019/10740 [39:45:57<12:17:42, 16.27s/it]


 75%|███████▍  | 8021/10740 [39:46:29<12:02:14, 15.94s/it]
{'loss': 0.3534, 'learning_rate': 3.17782331945623e-07, 'rewards/chosen': -0.8697108030319214, 'rewards/rejected': -3.0158424377441406, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1461315155029297, 'policy_logps/rejected': -615.2068481445312, 'policy_logps/chosen': -517.9512939453125, 'referece_logps/rejected': -585.0484619140625, 'referece_logps/chosen': -509.2542419433594, 'logits/rejected': -0.14265300333499908, 'logits/chosen': -0.04690348729491234, 'epoch': 4.48}

 75%|███████▍  | 8022/10740 [39:46:47<12:42:14, 16.83s/it]

 75%|███████▍  | 8023/10740 [39:47:04<12:38:19, 16.75s/it]

 75%|███████▍  | 8024/10740 [39:47:22<12:52:32, 17.07s/it]

 75%|███████▍  | 8025/10740 [39:47:38<12:36:28, 16.72s/it]

 75%|███████▍  | 8026/10740 [39:47:52<11:56:40, 15.84s/it]

 75%|███████▍  | 8027/10740 [39:48:03<10:57:45, 14.55s/it]

 75%|███████▍  | 8028/10740 [39:48:20<11:24:13, 15.14s/it]


 75%|███████▍  | 8030/10740 [39:48:51<11:22:24, 15.11s/it]
{'loss': 0.3647, 'learning_rate': 3.1580032643575893e-07, 'rewards/chosen': -2.337846279144287, 'rewards/rejected': -3.2331392765045166, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8952925801277161, 'policy_logps/rejected': -497.784423828125, 'policy_logps/chosen': -527.549560546875, 'referece_logps/rejected': -465.4530029296875, 'referece_logps/chosen': -504.1710510253906, 'logits/rejected': -0.9055489897727966, 'logits/chosen': -0.9046379923820496, 'epoch': 4.49}

 75%|███████▍  | 8031/10740 [39:49:09<12:04:12, 16.04s/it]

 75%|███████▍  | 8032/10740 [39:49:32<13:42:47, 18.23s/it]

 75%|███████▍  | 8033/10740 [39:49:47<12:57:46, 17.24s/it]

 75%|███████▍  | 8034/10740 [39:50:05<13:04:26, 17.39s/it]

 75%|███████▍  | 8035/10740 [39:50:25<13:33:10, 18.04s/it]

 75%|███████▍  | 8036/10740 [39:50:44<13:53:14, 18.49s/it]

 75%|███████▍  | 8037/10740 [39:51:04<14:07:45, 18.82s/it]

 75%|███████▍  | 8038/10740 [39:51:22<14:03:21, 18.73s/it]

 75%|███████▍  | 8039/10740 [39:51:42<14:15:04, 18.99s/it]


 75%|███████▍  | 8041/10740 [39:52:17<13:31:02, 18.03s/it]
{'loss': 0.4023, 'learning_rate': 3.1338472197893693e-07, 'rewards/chosen': -1.7587217092514038, 'rewards/rejected': -3.9292407035827637, 'rewards/accuracies': 0.875, 'rewards/margins': 2.170518636703491, 'policy_logps/rejected': -611.2037353515625, 'policy_logps/chosen': -404.487060546875, 'referece_logps/rejected': -571.911376953125, 'referece_logps/chosen': -386.8998718261719, 'logits/rejected': -0.5814921855926514, 'logits/chosen': -0.5475361943244934, 'epoch': 4.49}

 75%|███████▍  | 8042/10740 [39:52:34<13:12:58, 17.63s/it]

 75%|███████▍  | 8043/10740 [39:52:50<12:48:17, 17.09s/it]

 75%|███████▍  | 8044/10740 [39:53:08<13:02:06, 17.41s/it]

 75%|███████▍  | 8045/10740 [39:53:27<13:24:50, 17.92s/it]

 75%|███████▍  | 8046/10740 [39:53:44<13:18:56, 17.79s/it]

 75%|███████▍  | 8047/10740 [39:54:05<14:03:20, 18.79s/it]

 75%|███████▍  | 8048/10740 [39:54:25<14:13:37, 19.03s/it]

 75%|███████▍  | 8049/10740 [39:54:38<12:49:01, 17.15s/it]

 75%|███████▍  | 8050/10740 [39:54:54<12:38:36, 16.92s/it]

 75%|███████▍  | 8051/10740 [39:55:08<11:53:33, 15.92s/it]

 75%|███████▍  | 8052/10740 [39:55:22<11:30:59, 15.42s/it]

 75%|███████▍  | 8053/10740 [39:55:42<12:36:28, 16.89s/it]

 75%|███████▍  | 8054/10740 [39:55:58<12:22:04, 16.58s/it]

 75%|███████▌  | 8055/10740 [39:56:18<13:12:15, 17.70s/it]

 75%|███████▌  | 8056/10740 [39:56:38<13:39:56, 18.33s/it]


 75%|███████▌  | 8058/10740 [39:57:13<13:09:57, 17.67s/it]
{'loss': 0.3827, 'learning_rate': 3.096663875022143e-07, 'rewards/chosen': -1.7490897178649902, 'rewards/rejected': -3.1982622146606445, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4491723775863647, 'policy_logps/rejected': -432.86767578125, 'policy_logps/chosen': -482.97222900390625, 'referece_logps/rejected': -400.8850402832031, 'referece_logps/chosen': -465.4813232421875, 'logits/rejected': -1.1039294004440308, 'logits/chosen': -1.20724618434906, 'epoch': 4.5}

 75%|███████▌  | 8059/10740 [39:57:30<12:57:59, 17.41s/it]

 75%|███████▌  | 8060/10740 [39:57:42<11:49:17, 15.88s/it]

 75%|███████▌  | 8061/10740 [39:58:02<12:41:20, 17.05s/it]

 75%|███████▌  | 8062/10740 [39:58:19<12:45:00, 17.14s/it]

 75%|███████▌  | 8063/10740 [39:58:38<13:02:28, 17.54s/it]

 75%|███████▌  | 8064/10740 [39:59:00<14:07:41, 19.01s/it]


 75%|███████▌  | 8066/10740 [39:59:39<14:19:28, 19.29s/it]
{'loss': 0.3689, 'learning_rate': 3.079228558322633e-07, 'rewards/chosen': -1.6882516145706177, 'rewards/rejected': -4.462863922119141, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7746119499206543, 'policy_logps/rejected': -406.91143798828125, 'policy_logps/chosen': -433.5359191894531, 'referece_logps/rejected': -362.28277587890625, 'referece_logps/chosen': -416.6534118652344, 'logits/rejected': -0.4188136160373688, 'logits/chosen': -0.4132183790206909, 'epoch': 4.51}

 75%|███████▌  | 8067/10740 [39:59:56<13:40:37, 18.42s/it]

 75%|███████▌  | 8068/10740 [40:00:16<13:59:36, 18.85s/it]

 75%|███████▌  | 8069/10740 [40:00:37<14:27:14, 19.48s/it]

 75%|███████▌  | 8070/10740 [40:00:56<14:29:06, 19.53s/it]

 75%|███████▌  | 8071/10740 [40:01:07<12:31:12, 16.89s/it]

 75%|███████▌  | 8072/10740 [40:01:23<12:14:28, 16.52s/it]

 75%|███████▌  | 8073/10740 [40:01:35<11:16:31, 15.22s/it]

 75%|███████▌  | 8074/10740 [40:01:54<12:14:40, 16.53s/it]

 75%|███████▌  | 8075/10740 [40:02:12<12:26:53, 16.82s/it]

 75%|███████▌  | 8076/10740 [40:02:29<12:24:34, 16.77s/it]

 75%|███████▌  | 8077/10740 [40:02:48<13:03:45, 17.66s/it]

 75%|███████▌  | 8078/10740 [40:03:07<13:20:10, 18.04s/it]

 75%|███████▌  | 8079/10740 [40:03:27<13:40:25, 18.50s/it]

 75%|███████▌  | 8080/10740 [40:03:48<14:11:31, 19.21s/it]

 75%|███████▌  | 8081/10740 [40:04:09<14:34:27, 19.73s/it]

 75%|███████▌  | 8082/10740 [40:04:23<13:29:11, 18.27s/it]

 75%|███████▌  | 8083/10740 [40:04:43<13:50:34, 18.76s/it]

 75%|███████▌  | 8084/10740 [40:05:01<13:40:05, 18.53s/it]

 75%|███████▌  | 8085/10740 [40:05:19<13:25:23, 18.20s/it]

 75%|███████▌  | 8086/10740 [40:05:38<13:44:51, 18.65s/it]


 75%|███████▌  | 8088/10740 [40:06:11<12:45:04, 17.31s/it]

 75%|███████▌  | 8089/10740 [40:06:24<11:39:35, 15.83s/it]

 75%|███████▌  | 8090/10740 [40:06:45<12:46:56, 17.36s/it]

 75%|███████▌  | 8091/10740 [40:07:05<13:26:42, 18.27s/it]

 75%|███████▌  | 8092/10740 [40:07:20<12:45:08, 17.34s/it]

 75%|███████▌  | 8093/10740 [40:07:39<12:57:21, 17.62s/it]

 75%|███████▌  | 8094/10740 [40:07:52<12:01:10, 16.35s/it]
{'loss': 0.4157, 'learning_rate': 3.018522862046747e-07, 'rewards/chosen': -1.0940446853637695, 'rewards/rejected': -3.189131736755371, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0950870513916016, 'policy_logps/rejected': -411.2599792480469, 'policy_logps/chosen': -448.5345764160156, 'referece_logps/rejected': -379.3686828613281, 'referece_logps/chosen': -437.5941162109375, 'logits/rejected': 0.08416703343391418, 'logits/chosen': 0.27037477493286133, 'epoch': 4.52}


 75%|███████▌  | 8096/10740 [40:08:33<13:37:51, 18.56s/it]

 75%|███████▌  | 8097/10740 [40:08:54<14:08:04, 19.25s/it]

 75%|███████▌  | 8098/10740 [40:09:09<13:17:10, 18.10s/it]

 75%|███████▌  | 8099/10740 [40:09:29<13:40:00, 18.63s/it]

 75%|███████▌  | 8100/10740 [40:09:51<14:24:47, 19.65s/it]
{'loss': 0.3942, 'learning_rate': 3.0055791195562885e-07, 'rewards/chosen': -1.9047316312789917, 'rewards/rejected': -3.1502041816711426, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2454724311828613, 'policy_logps/rejected': -379.8398132324219, 'policy_logps/chosen': -332.59735107421875, 'referece_logps/rejected': -348.3377990722656, 'referece_logps/chosen': -313.5500183105469, 'logits/rejected': -0.5370909571647644, 'logits/chosen': -0.5690099000930786, 'epoch': 4.53}


 75%|███████▌  | 8102/10740 [40:10:28<14:02:39, 19.17s/it]
{'loss': 0.2485, 'learning_rate': 3.001269625911885e-07, 'rewards/chosen': -2.0916099548339844, 'rewards/rejected': -4.069399833679199, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9777902364730835, 'policy_logps/rejected': -374.8103332519531, 'policy_logps/chosen': -360.1409912109375, 'referece_logps/rejected': -334.1163635253906, 'referece_logps/chosen': -339.2248840332031, 'logits/rejected': -0.8814138770103455, 'logits/chosen': -0.8870210647583008, 'epoch': 4.53}

 75%|███████▌  | 8103/10740 [40:10:42<13:00:26, 17.76s/it]


 75%|███████▌  | 8105/10740 [40:11:10<11:23:34, 15.57s/it]

 75%|███████▌  | 8106/10740 [40:11:30<12:17:52, 16.81s/it]

 75%|███████▌  | 8107/10740 [40:11:49<12:52:48, 17.61s/it]

 75%|███████▌  | 8108/10740 [40:12:09<13:23:53, 18.33s/it]

 76%|███████▌  | 8109/10740 [40:12:29<13:47:40, 18.88s/it]

 76%|███████▌  | 8110/10740 [40:12:41<12:11:43, 16.69s/it]

 76%|███████▌  | 8111/10740 [40:12:54<11:25:13, 15.64s/it]
{'loss': 0.3071, 'learning_rate': 2.9819084364339774e-07, 'rewards/chosen': -2.196979284286499, 'rewards/rejected': -4.8910722732543945, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6940927505493164, 'policy_logps/rejected': -371.2269287109375, 'policy_logps/chosen': -387.28607177734375, 'referece_logps/rejected': -322.31622314453125, 'referece_logps/chosen': -365.3162536621094, 'logits/rejected': -0.48408234119415283, 'logits/chosen': -0.5312327146530151, 'epoch': 4.53}


 76%|███████▌  | 8113/10740 [40:13:26<11:45:16, 16.11s/it]
{'loss': 0.313, 'learning_rate': 2.97761296630359e-07, 'rewards/chosen': -1.515885829925537, 'rewards/rejected': -3.9361774921417236, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4202916622161865, 'policy_logps/rejected': -426.21282958984375, 'policy_logps/chosen': -397.5211181640625, 'referece_logps/rejected': -386.8510437011719, 'referece_logps/chosen': -382.3622741699219, 'logits/rejected': -1.0055856704711914, 'logits/chosen': -1.0270463228225708, 'epoch': 4.53}


 76%|███████▌  | 8115/10740 [40:13:55<11:00:44, 15.10s/it]
{'loss': 0.4031, 'learning_rate': 2.9733200509864307e-07, 'rewards/chosen': -2.5319740772247314, 'rewards/rejected': -3.841379404067993, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3094053268432617, 'policy_logps/rejected': -341.42535400390625, 'policy_logps/chosen': -398.9861755371094, 'referece_logps/rejected': -303.01153564453125, 'referece_logps/chosen': -373.6664123535156, 'logits/rejected': -0.8995952606201172, 'logits/chosen': -0.9791441559791565, 'epoch': 4.53}


 76%|███████▌  | 8117/10740 [40:14:35<12:53:17, 17.69s/it]

 76%|███████▌  | 8118/10740 [40:14:48<11:49:51, 16.24s/it]
{'loss': 0.2961, 'learning_rate': 2.966885471701699e-07, 'rewards/chosen': -2.0696587562561035, 'rewards/rejected': -4.248921871185303, 'rewards/accuracies': 0.875, 'rewards/margins': 2.179263114929199, 'policy_logps/rejected': -437.3674011230469, 'policy_logps/chosen': -376.72576904296875, 'referece_logps/rejected': -394.8781433105469, 'referece_logps/chosen': -356.0291748046875, 'logits/rejected': -1.0046178102493286, 'logits/chosen': -1.0433123111724854, 'epoch': 4.54}


 76%|███████▌  | 8120/10740 [40:15:15<10:48:02, 14.84s/it]

 76%|███████▌  | 8121/10740 [40:15:36<12:07:45, 16.67s/it]
{'loss': 0.3119, 'learning_rate': 2.960456649527745e-07, 'rewards/chosen': -1.9539461135864258, 'rewards/rejected': -3.3543899059295654, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4004437923431396, 'policy_logps/rejected': -465.20867919921875, 'policy_logps/chosen': -343.4566955566406, 'referece_logps/rejected': -431.66473388671875, 'referece_logps/chosen': -323.9172668457031, 'logits/rejected': -0.5336557626724243, 'logits/chosen': -0.5336111783981323, 'epoch': 4.54}


 76%|███████▌  | 8123/10740 [40:16:11<12:16:59, 16.90s/it]

 76%|███████▌  | 8124/10740 [40:16:33<13:23:16, 18.42s/it]

 76%|███████▌  | 8125/10740 [40:16:50<13:08:56, 18.10s/it]
{'loss': 0.457, 'learning_rate': 2.9518938512297264e-07, 'rewards/chosen': -2.002967596054077, 'rewards/rejected': -2.7654173374176025, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7624496817588806, 'policy_logps/rejected': -436.8569030761719, 'policy_logps/chosen': -278.06475830078125, 'referece_logps/rejected': -409.20269775390625, 'referece_logps/chosen': -258.03509521484375, 'logits/rejected': -0.6653773188591003, 'logits/chosen': -0.770179808139801, 'epoch': 4.54}


 76%|███████▌  | 8127/10740 [40:17:24<12:45:21, 17.57s/it]

 76%|███████▌  | 8128/10740 [40:17:41<12:28:13, 17.19s/it]

 76%|███████▌  | 8129/10740 [40:17:54<11:45:51, 16.22s/it]

 76%|███████▌  | 8130/10740 [40:18:06<10:38:29, 14.68s/it]

 76%|███████▌  | 8131/10740 [40:18:25<11:44:14, 16.20s/it]

 76%|███████▌  | 8132/10740 [40:18:45<12:23:41, 17.11s/it]

 76%|███████▌  | 8133/10740 [40:19:04<12:52:17, 17.77s/it]

 76%|███████▌  | 8134/10740 [40:19:15<11:19:44, 15.65s/it]

 76%|███████▌  | 8135/10740 [40:19:31<11:24:09, 15.76s/it]

 76%|███████▌  | 8136/10740 [40:19:43<10:44:14, 14.84s/it]

 76%|███████▌  | 8137/10740 [40:20:03<11:43:38, 16.22s/it]

 76%|███████▌  | 8138/10740 [40:20:25<12:55:53, 17.89s/it]

 76%|███████▌  | 8139/10740 [40:20:37<11:39:40, 16.14s/it]

 76%|███████▌  | 8140/10740 [40:20:58<12:41:57, 17.58s/it]

 76%|███████▌  | 8141/10740 [40:21:16<12:58:03, 17.96s/it]

 76%|███████▌  | 8142/10740 [40:21:36<13:21:50, 18.52s/it]

 76%|███████▌  | 8143/10740 [40:21:56<13:36:10, 18.86s/it]
{'loss': 0.3753, 'learning_rate': 2.9134883648510756e-07, 'rewards/chosen': -1.7421419620513916, 'rewards/rejected': -3.387707233428955, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6455655097961426, 'policy_logps/rejected': -326.4246826171875, 'policy_logps/chosen': -287.9468078613281, 'referece_logps/rejected': -292.547607421875, 'referece_logps/chosen': -270.525390625, 'logits/rejected': -0.6950646042823792, 'logits/chosen': -0.7780630588531494, 'epoch': 4.55}


 76%|███████▌  | 8145/10740 [40:22:35<13:42:56, 19.03s/it]

 76%|███████▌  | 8146/10740 [40:22:54<13:39:24, 18.95s/it]

 76%|███████▌  | 8147/10740 [40:23:05<11:57:50, 16.61s/it]

 76%|███████▌  | 8148/10740 [40:23:24<12:37:12, 17.53s/it]

 76%|███████▌  | 8149/10740 [40:23:43<12:48:17, 17.79s/it]

 76%|███████▌  | 8150/10740 [40:23:54<11:26:04, 15.89s/it]

 76%|███████▌  | 8151/10740 [40:24:08<10:54:05, 15.16s/it]

 76%|███████▌  | 8152/10740 [40:24:26<11:30:40, 16.01s/it]

 76%|███████▌  | 8153/10740 [40:24:46<12:19:40, 17.16s/it]

 76%|███████▌  | 8154/10740 [40:25:02<12:15:40, 17.07s/it]
{'loss': 0.4564, 'learning_rate': 2.890121076609564e-07, 'rewards/chosen': -1.8944107294082642, 'rewards/rejected': -2.6924960613250732, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7980852127075195, 'policy_logps/rejected': -591.3565673828125, 'policy_logps/chosen': -630.5720825195312, 'referece_logps/rejected': -564.4315795898438, 'referece_logps/chosen': -611.6279907226562, 'logits/rejected': -0.9924513697624207, 'logits/chosen': -0.9202150106430054, 'epoch': 4.56}


 76%|███████▌  | 8156/10740 [40:25:40<12:57:23, 18.05s/it]

 76%|███████▌  | 8157/10740 [40:26:01<13:26:36, 18.74s/it]

 76%|███████▌  | 8158/10740 [40:26:21<13:46:21, 19.20s/it]
{'loss': 0.301, 'learning_rate': 2.8816432671974997e-07, 'rewards/chosen': -2.3847415447235107, 'rewards/rejected': -4.953963756561279, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5692219734191895, 'policy_logps/rejected': -472.4921569824219, 'policy_logps/chosen': -291.64495849609375, 'referece_logps/rejected': -422.9525146484375, 'referece_logps/chosen': -267.79754638671875, 'logits/rejected': -0.6959455609321594, 'logits/chosen': -0.7776216268539429, 'epoch': 4.56}

 76%|███████▌  | 8159/10740 [40:26:38<13:10:59, 18.39s/it]

 76%|███████▌  | 8160/10740 [40:26:57<13:29:28, 18.83s/it]

 76%|███████▌  | 8161/10740 [40:27:11<12:26:42, 17.37s/it]

 76%|███████▌  | 8162/10740 [40:27:33<13:26:58, 18.78s/it]


 76%|███████▌  | 8164/10740 [40:28:08<12:58:57, 18.14s/it]

 76%|███████▌  | 8165/10740 [40:28:29<13:35:28, 19.00s/it]
{'loss': 0.399, 'learning_rate': 2.866832034268754e-07, 'rewards/chosen': -1.4762886762619019, 'rewards/rejected': -2.200275182723999, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7239863872528076, 'policy_logps/rejected': -417.4145812988281, 'policy_logps/chosen': -319.3433837890625, 'referece_logps/rejected': -395.411865234375, 'referece_logps/chosen': -304.5805358886719, 'logits/rejected': 0.3714914917945862, 'logits/chosen': 0.4130743741989136, 'epoch': 4.56}

 76%|███████▌  | 8166/10740 [40:28:45<13:01:39, 18.22s/it]


 76%|███████▌  | 8168/10740 [40:29:28<14:08:51, 19.80s/it]

 76%|███████▌  | 8169/10740 [40:29:45<13:31:27, 18.94s/it]
{'loss': 0.3609, 'learning_rate': 2.858382741502672e-07, 'rewards/chosen': -1.482981562614441, 'rewards/rejected': -3.3930141925811768, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9100325107574463, 'policy_logps/rejected': -345.8407287597656, 'policy_logps/chosen': -343.9477233886719, 'referece_logps/rejected': -311.91064453125, 'referece_logps/chosen': -329.1178894042969, 'logits/rejected': -0.9076583385467529, 'logits/chosen': -0.8167937994003296, 'epoch': 4.56}


 76%|███████▌  | 8171/10740 [40:30:23<13:35:57, 19.06s/it]

 76%|███████▌  | 8172/10740 [40:30:44<13:55:59, 19.53s/it]

 76%|███████▌  | 8173/10740 [40:31:03<13:53:48, 19.49s/it]
{'loss': 0.4267, 'learning_rate': 2.849943841497071e-07, 'rewards/chosen': -1.3945372104644775, 'rewards/rejected': -4.322193622589111, 'rewards/accuracies': 1.0, 'rewards/margins': 2.927656650543213, 'policy_logps/rejected': -509.10009765625, 'policy_logps/chosen': -423.8218994140625, 'referece_logps/rejected': -465.878173828125, 'referece_logps/chosen': -409.87652587890625, 'logits/rejected': -0.12227891385555267, 'logits/chosen': -0.13686612248420715, 'epoch': 4.57}


 76%|███████▌  | 8175/10740 [40:31:44<14:10:56, 19.91s/it]

 76%|███████▌  | 8176/10740 [40:32:03<13:55:27, 19.55s/it]
{'loss': 0.3032, 'learning_rate': 2.8436214941300617e-07, 'rewards/chosen': -2.4367854595184326, 'rewards/rejected': -4.082519054412842, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6457334756851196, 'policy_logps/rejected': -426.111572265625, 'policy_logps/chosen': -544.87548828125, 'referece_logps/rejected': -385.286376953125, 'referece_logps/chosen': -520.507568359375, 'logits/rejected': -0.6271851658821106, 'logits/chosen': -0.88896244764328, 'epoch': 4.57}


 76%|███████▌  | 8178/10740 [40:32:33<12:30:21, 17.57s/it]
{'loss': 0.3443, 'learning_rate': 2.8394098500168074e-07, 'rewards/chosen': -2.1297872066497803, 'rewards/rejected': -2.921292543411255, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7915051579475403, 'policy_logps/rejected': -474.2032470703125, 'policy_logps/chosen': -477.5771484375, 'referece_logps/rejected': -444.9903564453125, 'referece_logps/chosen': -456.27923583984375, 'logits/rejected': -0.5389645099639893, 'logits/chosen': -0.5408676266670227, 'epoch': 4.57}

 76%|███████▌  | 8179/10740 [40:32:50<12:24:57, 17.45s/it]


 76%|███████▌  | 8181/10740 [40:33:21<12:01:42, 16.92s/it]

 76%|███████▌  | 8182/10740 [40:33:41<12:37:57, 17.78s/it]

 76%|███████▌  | 8183/10740 [40:33:57<12:12:45, 17.19s/it]

 76%|███████▌  | 8184/10740 [40:34:17<12:50:21, 18.08s/it]

 76%|███████▌  | 8185/10740 [40:34:29<11:36:04, 16.35s/it]
{'loss': 0.4292, 'learning_rate': 2.8246896207735995e-07, 'rewards/chosen': -2.096553325653076, 'rewards/rejected': -2.6996219158172607, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6030687093734741, 'policy_logps/rejected': -284.9521484375, 'policy_logps/chosen': -234.6492919921875, 'referece_logps/rejected': -257.9559326171875, 'referece_logps/chosen': -213.68374633789062, 'logits/rejected': -0.7968775033950806, 'logits/chosen': -0.6114009618759155, 'epoch': 4.57}


 76%|███████▌  | 8187/10740 [40:35:13<13:28:27, 19.00s/it]

 76%|███████▌  | 8188/10740 [40:35:33<13:39:14, 19.26s/it]

 76%|███████▌  | 8189/10740 [40:35:53<13:49:49, 19.52s/it]

 76%|███████▋  | 8190/10740 [40:36:08<12:49:13, 18.10s/it]

 76%|███████▋  | 8191/10740 [40:36:28<13:12:09, 18.65s/it]
{'loss': 0.4959, 'learning_rate': 2.812097730801416e-07, 'rewards/chosen': -0.8603349924087524, 'rewards/rejected': -3.079888343811035, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2195534706115723, 'policy_logps/rejected': -463.9418640136719, 'policy_logps/chosen': -496.18463134765625, 'referece_logps/rejected': -433.1430358886719, 'referece_logps/chosen': -487.581298828125, 'logits/rejected': 0.41825053095817566, 'logits/chosen': 0.4326428174972534, 'epoch': 4.58}

 76%|███████▋  | 8192/10740 [40:36:44<12:46:12, 18.04s/it]


 76%|███████▋  | 8194/10740 [40:37:23<13:17:50, 18.80s/it]

 76%|███████▋  | 8195/10740 [40:37:43<13:33:56, 19.19s/it]

 76%|███████▋  | 8196/10740 [40:38:04<13:45:03, 19.46s/it]

 76%|███████▋  | 8197/10740 [40:38:22<13:26:19, 19.02s/it]

 76%|███████▋  | 8198/10740 [40:38:44<14:03:43, 19.91s/it]

 76%|███████▋  | 8199/10740 [40:39:04<14:04:17, 19.94s/it]

 76%|███████▋  | 8200/10740 [40:39:24<14:06:17, 19.99s/it]

 76%|███████▋  | 8201/10740 [40:39:37<12:41:43, 18.00s/it]

 76%|███████▋  | 8202/10740 [40:39:51<11:51:40, 16.82s/it]
{'loss': 0.3197, 'learning_rate': 2.789073755172485e-07, 'rewards/chosen': -1.9007983207702637, 'rewards/rejected': -4.933472633361816, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0326743125915527, 'policy_logps/rejected': -336.7263488769531, 'policy_logps/chosen': -305.2169494628906, 'referece_logps/rejected': -287.3915710449219, 'referece_logps/chosen': -286.2090148925781, 'logits/rejected': 0.020709291100502014, 'logits/chosen': -0.026859905570745468, 'epoch': 4.58}

 76%|███████▋  | 8203/10740 [40:40:10<12:22:58, 17.57s/it]

 76%|███████▋  | 8204/10740 [40:40:25<11:42:01, 16.61s/it]


 76%|███████▋  | 8206/10740 [40:40:56<11:35:39, 16.47s/it]

 76%|███████▋  | 8207/10740 [40:41:12<11:25:02, 16.23s/it]
{'loss': 0.3476, 'learning_rate': 2.778634530540447e-07, 'rewards/chosen': -1.61668860912323, 'rewards/rejected': -3.7815909385681152, 'rewards/accuracies': 0.5, 'rewards/margins': 2.1649022102355957, 'policy_logps/rejected': -338.66259765625, 'policy_logps/chosen': -417.81475830078125, 'referece_logps/rejected': -300.8466796875, 'referece_logps/chosen': -401.64788818359375, 'logits/rejected': -0.8082942962646484, 'logits/chosen': -0.8905876874923706, 'epoch': 4.58}


 76%|███████▋  | 8209/10740 [40:41:50<12:29:49, 17.78s/it]

 76%|███████▋  | 8210/10740 [40:42:10<12:54:05, 18.36s/it]

 76%|███████▋  | 8211/10740 [40:42:30<13:16:32, 18.90s/it]
{'loss': 0.4207, 'learning_rate': 2.7702949721107395e-07, 'rewards/chosen': -2.634404420852661, 'rewards/rejected': -3.517101287841797, 'rewards/accuracies': 0.375, 'rewards/margins': 0.8826965689659119, 'policy_logps/rejected': -377.7024230957031, 'policy_logps/chosen': -381.50408935546875, 'referece_logps/rejected': -342.5314025878906, 'referece_logps/chosen': -355.1600646972656, 'logits/rejected': -0.4094340205192566, 'logits/chosen': -0.4264375567436218, 'epoch': 4.59}

 76%|███████▋  | 8212/10740 [40:42:49<13:17:21, 18.92s/it]


 76%|███████▋  | 8214/10740 [40:43:22<12:09:33, 17.33s/it]

 76%|███████▋  | 8215/10740 [40:43:41<12:39:41, 18.05s/it]

 76%|███████▋  | 8216/10740 [40:43:59<12:36:27, 17.98s/it]

 77%|███████▋  | 8217/10740 [40:44:20<13:12:13, 18.84s/it]

 77%|███████▋  | 8218/10740 [40:44:36<12:37:21, 18.02s/it]

 77%|███████▋  | 8219/10740 [40:44:58<13:28:44, 19.25s/it]

 77%|███████▋  | 8220/10740 [40:45:18<13:32:15, 19.34s/it]

 77%|███████▋  | 8221/10740 [40:45:38<13:36:07, 19.44s/it]

 77%|███████▋  | 8222/10740 [40:45:58<13:43:05, 19.61s/it]
{'loss': 0.3402, 'learning_rate': 2.74741547152186e-07, 'rewards/chosen': -0.9028469920158386, 'rewards/rejected': -1.891835331916809, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9889883995056152, 'policy_logps/rejected': -320.946044921875, 'policy_logps/chosen': -433.3827209472656, 'referece_logps/rejected': -302.0276794433594, 'referece_logps/chosen': -424.3542175292969, 'logits/rejected': -0.7359597682952881, 'logits/chosen': -0.7943215370178223, 'epoch': 4.59}


 77%|███████▋  | 8224/10740 [40:46:36<13:35:55, 19.46s/it]

 77%|███████▋  | 8225/10740 [40:46:56<13:40:46, 19.58s/it]

 77%|███████▋  | 8226/10740 [40:47:13<13:11:49, 18.90s/it]
{'loss': 0.2978, 'learning_rate': 2.7391154291032736e-07, 'rewards/chosen': -1.5136955976486206, 'rewards/rejected': -3.5483498573303223, 'rewards/accuracies': 1.0, 'rewards/margins': 2.034654378890991, 'policy_logps/rejected': -340.5139465332031, 'policy_logps/chosen': -346.85980224609375, 'referece_logps/rejected': -305.03045654296875, 'referece_logps/chosen': -331.7228698730469, 'logits/rejected': -1.4030864238739014, 'logits/chosen': -1.4108878374099731, 'epoch': 4.6}


 77%|███████▋  | 8228/10740 [40:47:52<13:19:29, 19.10s/it]

 77%|███████▋  | 8229/10740 [40:48:14<13:47:47, 19.78s/it]
{'loss': 0.4479, 'learning_rate': 2.732897330778795e-07, 'rewards/chosen': -3.1242318153381348, 'rewards/rejected': -3.0231857299804688, 'rewards/accuracies': 0.5, 'rewards/margins': -0.10104618966579437, 'policy_logps/rejected': -444.0050354003906, 'policy_logps/chosen': -400.0062255859375, 'referece_logps/rejected': -413.7732238769531, 'referece_logps/chosen': -368.763916015625, 'logits/rejected': 0.008182629942893982, 'logits/chosen': -0.01006697490811348, 'epoch': 4.6}

 77%|███████▋  | 8230/10740 [40:48:31<13:21:07, 19.15s/it]

 77%|███████▋  | 8231/10740 [40:48:47<12:42:07, 18.23s/it]

 77%|███████▋  | 8232/10740 [40:49:07<13:02:34, 18.72s/it]

 77%|███████▋  | 8233/10740 [40:49:27<13:16:17, 19.06s/it]

 77%|███████▋  | 8234/10740 [40:49:43<12:40:16, 18.20s/it]


 77%|███████▋  | 8236/10740 [40:50:28<14:09:06, 20.35s/it]
{'loss': 0.3678, 'learning_rate': 2.7184115771036285e-07, 'rewards/chosen': -0.9691687226295471, 'rewards/rejected': -2.565795421600342, 'rewards/accuracies': 0.875, 'rewards/margins': 1.59662663936615, 'policy_logps/rejected': -621.60888671875, 'policy_logps/chosen': -497.551513671875, 'referece_logps/rejected': -595.9509887695312, 'referece_logps/chosen': -487.8598937988281, 'logits/rejected': 0.42035573720932007, 'logits/chosen': 0.45185595750808716, 'epoch': 4.6}

 77%|███████▋  | 8237/10740 [40:50:39<12:17:20, 17.68s/it]

 77%|███████▋  | 8238/10740 [40:50:59<12:46:59, 18.39s/it]

 77%|███████▋  | 8239/10740 [40:51:18<12:43:33, 18.32s/it]

 77%|███████▋  | 8240/10740 [40:51:37<12:58:16, 18.68s/it]

 77%|███████▋  | 8241/10740 [40:51:55<12:50:56, 18.51s/it]

 77%|███████▋  | 8242/10740 [40:52:15<13:12:09, 19.03s/it]

 77%|███████▋  | 8243/10740 [40:52:32<12:37:05, 18.19s/it]

 77%|███████▋  | 8244/10740 [40:52:49<12:29:59, 18.03s/it]


 77%|███████▋  | 8246/10740 [40:53:20<11:20:50, 16.38s/it]
{'loss': 0.4042, 'learning_rate': 2.697773952886745e-07, 'rewards/chosen': -2.470576286315918, 'rewards/rejected': -3.6031994819641113, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1326231956481934, 'policy_logps/rejected': -288.8509216308594, 'policy_logps/chosen': -331.8982238769531, 'referece_logps/rejected': -252.8189239501953, 'referece_logps/chosen': -307.19244384765625, 'logits/rejected': -1.3335334062576294, 'logits/chosen': -1.5230300426483154, 'epoch': 4.61}


 77%|███████▋  | 8248/10740 [40:53:56<11:50:06, 17.10s/it]

 77%|███████▋  | 8249/10740 [40:54:16<12:23:21, 17.91s/it]
{'loss': 0.3888, 'learning_rate': 2.691595608116873e-07, 'rewards/chosen': -0.928642213344574, 'rewards/rejected': -2.244133472442627, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3154911994934082, 'policy_logps/rejected': -224.30960083007812, 'policy_logps/chosen': -251.65206909179688, 'referece_logps/rejected': -201.8682403564453, 'referece_logps/chosen': -242.3656463623047, 'logits/rejected': -0.7109573483467102, 'logits/chosen': -0.914767861366272, 'epoch': 4.61}


 77%|███████▋  | 8251/10740 [40:54:50<11:51:51, 17.16s/it]

 77%|███████▋  | 8252/10740 [40:55:09<12:13:30, 17.69s/it]

 77%|███████▋  | 8253/10740 [40:55:28<12:31:25, 18.13s/it]

 77%|███████▋  | 8254/10740 [40:55:49<12:59:49, 18.82s/it]
{'loss': 0.437, 'learning_rate': 2.681311663674275e-07, 'rewards/chosen': -1.9022817611694336, 'rewards/rejected': -2.2767751216888428, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3744933009147644, 'policy_logps/rejected': -255.57864379882812, 'policy_logps/chosen': -274.2266845703125, 'referece_logps/rejected': -232.8109130859375, 'referece_logps/chosen': -255.2039031982422, 'logits/rejected': -1.0219297409057617, 'logits/chosen': -1.2329812049865723, 'epoch': 4.61}

 77%|███████▋  | 8255/10740 [40:56:12<13:52:57, 20.11s/it]

 77%|███████▋  | 8256/10740 [40:56:29<13:21:50, 19.37s/it]


 77%|███████▋  | 8258/10740 [40:57:06<12:54:53, 18.73s/it]
{'loss': 0.3562, 'learning_rate': 2.6730964887446175e-07, 'rewards/chosen': -1.1365123987197876, 'rewards/rejected': -2.6432945728302, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5067824125289917, 'policy_logps/rejected': -341.48419189453125, 'policy_logps/chosen': -314.43963623046875, 'referece_logps/rejected': -315.0512390136719, 'referece_logps/chosen': -303.07452392578125, 'logits/rejected': 0.14159050583839417, 'logits/chosen': 0.07953792065382004, 'epoch': 4.61}


 77%|███████▋  | 8260/10740 [40:57:47<13:29:04, 19.57s/it]

 77%|███████▋  | 8261/10740 [40:58:07<13:31:04, 19.63s/it]

 77%|███████▋  | 8262/10740 [40:58:24<13:07:05, 19.06s/it]
{'loss': 0.3561, 'learning_rate': 2.6648919762112354e-07, 'rewards/chosen': -1.604722023010254, 'rewards/rejected': -3.2051119804382324, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6003899574279785, 'policy_logps/rejected': -385.9852600097656, 'policy_logps/chosen': -339.0168762207031, 'referece_logps/rejected': -353.93414306640625, 'referece_logps/chosen': -322.96966552734375, 'logits/rejected': -0.469655305147171, 'logits/chosen': -0.5469247102737427, 'epoch': 4.62}

 77%|███████▋  | 8263/10740 [40:58:39<12:15:55, 17.83s/it]

 77%|███████▋  | 8264/10740 [40:59:02<13:16:35, 19.30s/it]

 77%|███████▋  | 8265/10740 [40:59:21<13:17:37, 19.34s/it]

 77%|███████▋  | 8266/10740 [40:59:42<13:28:15, 19.60s/it]


 77%|███████▋  | 8268/10740 [41:00:12<11:48:54, 17.21s/it]

 77%|███████▋  | 8269/10740 [41:00:35<12:56:44, 18.86s/it]

 77%|███████▋  | 8270/10740 [41:00:51<12:23:52, 18.07s/it]

 77%|███████▋  | 8271/10740 [41:01:11<12:40:51, 18.49s/it]
{'loss': 0.4006, 'learning_rate': 2.646470869215383e-07, 'rewards/chosen': -1.420949935913086, 'rewards/rejected': -2.3644723892211914, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9435223937034607, 'policy_logps/rejected': -335.51922607421875, 'policy_logps/chosen': -290.8968811035156, 'referece_logps/rejected': -311.8744812011719, 'referece_logps/chosen': -276.6873779296875, 'logits/rejected': -1.1561038494110107, 'logits/chosen': -1.1296656131744385, 'epoch': 4.62}

 77%|███████▋  | 8272/10740 [41:01:26<11:59:33, 17.49s/it]


 77%|███████▋  | 8274/10740 [41:02:03<12:17:57, 17.96s/it]

 77%|███████▋  | 8275/10740 [41:02:17<11:26:13, 16.70s/it]

 77%|███████▋  | 8276/10740 [41:02:35<11:48:38, 17.26s/it]

 77%|███████▋  | 8277/10740 [41:02:57<12:44:17, 18.62s/it]

 77%|███████▋  | 8278/10740 [41:03:17<12:58:48, 18.98s/it]

 77%|███████▋  | 8279/10740 [41:03:39<13:37:15, 19.92s/it]

 77%|███████▋  | 8280/10740 [41:04:01<14:00:28, 20.50s/it]
{'loss': 0.4134, 'learning_rate': 2.6281039367288647e-07, 'rewards/chosen': -2.620915412902832, 'rewards/rejected': -3.7432262897491455, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1223108768463135, 'policy_logps/rejected': -554.004638671875, 'policy_logps/chosen': -442.4107360839844, 'referece_logps/rejected': -516.5723876953125, 'referece_logps/chosen': -416.20159912109375, 'logits/rejected': -1.3661977052688599, 'logits/chosen': -1.4132205247879028, 'epoch': 4.63}

 77%|███████▋  | 8281/10740 [41:04:22<14:08:34, 20.71s/it]


 77%|███████▋  | 8283/10740 [41:04:55<12:46:48, 18.73s/it]

 77%|███████▋  | 8284/10740 [41:05:07<11:19:15, 16.59s/it]

 77%|███████▋  | 8285/10740 [41:05:27<12:01:20, 17.63s/it]
{'loss': 0.298, 'learning_rate': 2.6179235439085576e-07, 'rewards/chosen': -1.312218189239502, 'rewards/rejected': -3.8149213790893555, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5027034282684326, 'policy_logps/rejected': -397.694580078125, 'policy_logps/chosen': -435.3742980957031, 'referece_logps/rejected': -359.5453186035156, 'referece_logps/chosen': -422.25213623046875, 'logits/rejected': -0.4594649374485016, 'logits/chosen': -0.5184563398361206, 'epoch': 4.63}


 77%|███████▋  | 8287/10740 [41:06:07<12:51:34, 18.87s/it]

 77%|███████▋  | 8288/10740 [41:06:21<11:49:58, 17.37s/it]
{'loss': 0.3416, 'learning_rate': 2.6118233637504974e-07, 'rewards/chosen': -0.9861003756523132, 'rewards/rejected': -4.324780464172363, 'rewards/accuracies': 0.75, 'rewards/margins': 3.338679790496826, 'policy_logps/rejected': -461.16302490234375, 'policy_logps/chosen': -576.5029907226562, 'referece_logps/rejected': -417.9151916503906, 'referece_logps/chosen': -566.6419067382812, 'logits/rejected': -0.4076305329799652, 'logits/chosen': -0.5065714716911316, 'epoch': 4.63}

 77%|███████▋  | 8289/10740 [41:06:38<11:48:56, 17.35s/it]

 77%|███████▋  | 8290/10740 [41:06:53<11:14:37, 16.52s/it]

 77%|███████▋  | 8291/10740 [41:07:12<11:50:58, 17.42s/it]

 77%|███████▋  | 8292/10740 [41:07:33<12:26:24, 18.29s/it]


 77%|███████▋  | 8294/10740 [41:08:01<11:10:46, 16.45s/it]
{'loss': 0.4585, 'learning_rate': 2.5996411516869265e-07, 'rewards/chosen': -2.264289140701294, 'rewards/rejected': -2.585886001586914, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3215968608856201, 'policy_logps/rejected': -459.1726989746094, 'policy_logps/chosen': -373.3072509765625, 'referece_logps/rejected': -433.3138427734375, 'referece_logps/chosen': -350.66436767578125, 'logits/rejected': -1.0207359790802002, 'logits/chosen': -0.921873927116394, 'epoch': 4.63}


 77%|███████▋  | 8296/10740 [41:08:26<9:35:24, 14.13s/it]
{'loss': 0.5027, 'learning_rate': 2.595585797009765e-07, 'rewards/chosen': -2.4375481605529785, 'rewards/rejected': -3.1181528568267822, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6806049346923828, 'policy_logps/rejected': -361.05718994140625, 'policy_logps/chosen': -312.7183837890625, 'referece_logps/rejected': -329.87567138671875, 'referece_logps/chosen': -288.3428955078125, 'logits/rejected': -0.6180956363677979, 'logits/chosen': -0.626616358757019, 'epoch': 4.63}

 77%|███████▋  | 8297/10740 [41:08:39<9:24:41, 13.87s/it]

 77%|███████▋  | 8298/10740 [41:08:57<10:18:47, 15.20s/it]

 77%|███████▋  | 8299/10740 [41:09:17<11:11:46, 16.51s/it]

 77%|███████▋  | 8300/10740 [41:09:31<10:44:38, 15.85s/it]


 77%|███████▋  | 8302/10740 [41:10:00<10:23:48, 15.35s/it]
{'loss': 0.3045, 'learning_rate': 2.5834359016656625e-07, 'rewards/chosen': -2.2403883934020996, 'rewards/rejected': -5.065834045410156, 'rewards/accuracies': 0.75, 'rewards/margins': 2.825446128845215, 'policy_logps/rejected': -401.6583251953125, 'policy_logps/chosen': -336.32061767578125, 'referece_logps/rejected': -350.9999694824219, 'referece_logps/chosen': -313.916748046875, 'logits/rejected': -0.37331363558769226, 'logits/chosen': -0.3800504803657532, 'epoch': 4.64}


 77%|███████▋  | 8304/10740 [41:10:38<11:29:08, 16.97s/it]

 77%|███████▋  | 8305/10740 [41:10:50<10:32:32, 15.59s/it]
{'loss': 0.3501, 'learning_rate': 2.5773700579962676e-07, 'rewards/chosen': -1.037689208984375, 'rewards/rejected': -3.3462400436401367, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3085508346557617, 'policy_logps/rejected': -299.741943359375, 'policy_logps/chosen': -459.096923828125, 'referece_logps/rejected': -266.279541015625, 'referece_logps/chosen': -448.72003173828125, 'logits/rejected': 0.27235162258148193, 'logits/chosen': 0.15280351042747498, 'epoch': 4.64}

 77%|███████▋  | 8306/10740 [41:11:11<11:39:36, 17.25s/it]

 77%|███████▋  | 8307/10740 [41:11:23<10:31:19, 15.57s/it]

 77%|███████▋  | 8308/10740 [41:11:37<10:18:52, 15.27s/it]

 77%|███████▋  | 8309/10740 [41:11:50<9:50:34, 14.58s/it]

 77%|███████▋  | 8310/10740 [41:12:07<10:18:45, 15.28s/it]


 77%|███████▋  | 8312/10740 [41:12:36<9:48:36, 14.55s/it]

 77%|███████▋  | 8313/10740 [41:12:56<10:52:43, 16.14s/it]

 77%|███████▋  | 8314/10740 [41:13:14<11:18:14, 16.77s/it]
{'loss': 0.2065, 'learning_rate': 2.559209002567738e-07, 'rewards/chosen': -2.1708693504333496, 'rewards/rejected': -4.878968238830566, 'rewards/accuracies': 0.875, 'rewards/margins': 2.708098888397217, 'policy_logps/rejected': -531.0993041992188, 'policy_logps/chosen': -475.6031799316406, 'referece_logps/rejected': -482.30963134765625, 'referece_logps/chosen': -453.8944396972656, 'logits/rejected': 0.24983574450016022, 'logits/chosen': 0.2888930141925812, 'epoch': 4.64}

 77%|███████▋  | 8315/10740 [41:13:31<11:24:39, 16.94s/it]

 77%|███████▋  | 8316/10740 [41:13:53<12:18:15, 18.27s/it]

 77%|███████▋  | 8317/10740 [41:14:12<12:35:40, 18.71s/it]


 77%|███████▋  | 8319/10740 [41:14:52<12:52:14, 19.14s/it]
{'loss': 0.3072, 'learning_rate': 2.5491432053035856e-07, 'rewards/chosen': -1.7835421562194824, 'rewards/rejected': -3.7685112953186035, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9849693775177002, 'policy_logps/rejected': -412.4153747558594, 'policy_logps/chosen': -331.82855224609375, 'referece_logps/rejected': -374.730224609375, 'referece_logps/chosen': -313.9931640625, 'logits/rejected': -0.37480098009109497, 'logits/chosen': -0.36480313539505005, 'epoch': 4.65}


 77%|███████▋  | 8321/10740 [41:15:28<12:33:24, 18.69s/it]
{'loss': 0.4433, 'learning_rate': 2.5451216288325285e-07, 'rewards/chosen': -1.6317757368087769, 'rewards/rejected': -4.646228313446045, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0144526958465576, 'policy_logps/rejected': -432.79461669921875, 'policy_logps/chosen': -406.646484375, 'referece_logps/rejected': -386.33233642578125, 'referece_logps/chosen': -390.3287658691406, 'logits/rejected': -0.8788183927536011, 'logits/chosen': -0.8176305294036865, 'epoch': 4.65}

 77%|███████▋  | 8322/10740 [41:15:39<11:00:51, 16.40s/it]

 77%|███████▋  | 8323/10740 [41:15:59<11:37:42, 17.32s/it]


 78%|███████▊  | 8325/10740 [41:16:38<12:26:25, 18.54s/it]
{'loss': 0.3833, 'learning_rate': 2.5370866138259803e-07, 'rewards/chosen': -2.7530250549316406, 'rewards/rejected': -5.488195419311523, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7351698875427246, 'policy_logps/rejected': -441.2991638183594, 'policy_logps/chosen': -485.2185974121094, 'referece_logps/rejected': -386.417236328125, 'referece_logps/chosen': -457.6883239746094, 'logits/rejected': -0.2743191123008728, 'logits/chosen': -0.2973247766494751, 'epoch': 4.65}

 78%|███████▊  | 8326/10740 [41:16:55<12:04:27, 18.01s/it]


 78%|███████▊  | 8328/10740 [41:17:22<10:29:18, 15.65s/it]
{'loss': 0.4541, 'learning_rate': 2.5310674790192796e-07, 'rewards/chosen': -1.9416828155517578, 'rewards/rejected': -4.191322326660156, 'rewards/accuracies': 0.625, 'rewards/margins': 2.2496395111083984, 'policy_logps/rejected': -427.163330078125, 'policy_logps/chosen': -384.84326171875, 'referece_logps/rejected': -385.2500915527344, 'referece_logps/chosen': -365.4264221191406, 'logits/rejected': -0.42102378606796265, 'logits/chosen': -0.4276604652404785, 'epoch': 4.65}

 78%|███████▊  | 8329/10740 [41:17:36<9:58:38, 14.90s/it]

 78%|███████▊  | 8330/10740 [41:17:57<11:22:16, 16.99s/it]

 78%|███████▊  | 8331/10740 [41:18:18<12:00:34, 17.95s/it]

 78%|███████▊  | 8332/10740 [41:18:35<11:54:16, 17.80s/it]

 78%|███████▊  | 8333/10740 [41:18:51<11:35:30, 17.34s/it]

 78%|███████▊  | 8334/10740 [41:19:10<11:46:49, 17.63s/it]

 78%|███████▊  | 8335/10740 [41:19:30<12:19:04, 18.44s/it]

 78%|███████▊  | 8336/10740 [41:19:47<12:04:24, 18.08s/it]

 78%|███████▊  | 8337/10740 [41:20:07<12:26:30, 18.64s/it]
[2024-04-03 12:34:10,722] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8338/10740 [41:20:27<12:41:07, 19.01s/it]


 78%|███████▊  | 8340/10740 [41:21:08<13:14:19, 19.86s/it]
[2024-04-03 12:34:52,009] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3127, 'learning_rate': 2.507052127580317e-07, 'rewards/chosen': -2.393963098526001, 'rewards/rejected': -5.51499080657959, 'rewards/accuracies': 0.875, 'rewards/margins': 3.121027708053589, 'policy_logps/rejected': -566.693115234375, 'policy_logps/chosen': -438.6412658691406, 'referece_logps/rejected': -511.543212890625, 'referece_logps/chosen': -414.70166015625, 'logits/rejected': -0.3506602942943573, 'logits/chosen': -0.4263281226158142, 'epoch': 4.66}
[2024-04-03 12:35:08,901] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 78%|███████▊  | 8342/10740 [41:21:39<11:30:47, 17.28s/it]
{'loss': 0.4376, 'learning_rate': 2.5030591015490656e-07, 'rewards/chosen': -1.8428690433502197, 'rewards/rejected': -4.642752170562744, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7998833656311035, 'policy_logps/rejected': -404.49298095703125, 'policy_logps/chosen': -366.8367004394531, 'referece_logps/rejected': -358.06549072265625, 'referece_logps/chosen': -348.4079895019531, 'logits/rejected': -0.303836464881897, 'logits/chosen': -0.35519587993621826, 'epoch': 4.66}

 78%|███████▊  | 8343/10740 [41:21:58<11:59:57, 18.02s/it]


 78%|███████▊  | 8345/10740 [41:22:27<10:29:00, 15.76s/it]

 78%|███████▊  | 8346/10740 [41:22:45<10:55:43, 16.43s/it]
{'loss': 0.4031, 'learning_rate': 2.4950812333200143e-07, 'rewards/chosen': -1.8776767253875732, 'rewards/rejected': -2.9742980003356934, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0966209173202515, 'policy_logps/rejected': -618.3704223632812, 'policy_logps/chosen': -545.08251953125, 'referece_logps/rejected': -588.62744140625, 'referece_logps/chosen': -526.3057250976562, 'logits/rejected': -0.8049159646034241, 'logits/chosen': -0.8008913993835449, 'epoch': 4.66}


 78%|███████▊  | 8348/10740 [41:23:17<11:04:08, 16.66s/it]
{'loss': 0.5279, 'learning_rate': 2.49109639402464e-07, 'rewards/chosen': -1.03175950050354, 'rewards/rejected': -3.7716503143310547, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7398905754089355, 'policy_logps/rejected': -416.9039001464844, 'policy_logps/chosen': -515.2076416015625, 'referece_logps/rejected': -379.1874084472656, 'referece_logps/chosen': -504.89007568359375, 'logits/rejected': -0.2552182376384735, 'logits/chosen': -0.35067835450172424, 'epoch': 4.66}


 78%|███████▊  | 8350/10740 [41:23:49<11:00:57, 16.59s/it]
{'loss': 0.4067, 'learning_rate': 2.487114286541997e-07, 'rewards/chosen': -2.14542293548584, 'rewards/rejected': -4.950677871704102, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8052544593811035, 'policy_logps/rejected': -488.6036682128906, 'policy_logps/chosen': -450.78118896484375, 'referece_logps/rejected': -439.0968933105469, 'referece_logps/chosen': -429.32696533203125, 'logits/rejected': -0.4658937454223633, 'logits/chosen': -0.32420358061790466, 'epoch': 4.66}

 78%|███████▊  | 8351/10740 [41:24:08<11:37:02, 17.51s/it]

 78%|███████▊  | 8352/10740 [41:24:22<10:45:32, 16.22s/it]


 78%|███████▊  | 8354/10740 [41:24:59<11:19:41, 17.09s/it]
{'loss': 0.3235, 'learning_rate': 2.4791582728088165e-07, 'rewards/chosen': -1.640000581741333, 'rewards/rejected': -4.341631889343262, 'rewards/accuracies': 0.875, 'rewards/margins': 2.701631784439087, 'policy_logps/rejected': -374.5563049316406, 'policy_logps/chosen': -382.7312316894531, 'referece_logps/rejected': -331.1400146484375, 'referece_logps/chosen': -366.3312072753906, 'logits/rejected': -0.8949175477027893, 'logits/chosen': -0.8922756910324097, 'epoch': 4.67}

 78%|███████▊  | 8355/10740 [41:25:20<12:12:36, 18.43s/it]

 78%|███████▊  | 8356/10740 [41:25:34<11:11:55, 16.91s/it]

 78%|███████▊  | 8357/10740 [41:25:54<11:48:50, 17.85s/it]

 78%|███████▊  | 8358/10740 [41:26:13<12:09:30, 18.38s/it]


 78%|███████▊  | 8360/10740 [41:26:47<11:27:06, 17.32s/it]
{'loss': 0.3575, 'learning_rate': 2.467244776990439e-07, 'rewards/chosen': -2.198984146118164, 'rewards/rejected': -3.1943535804748535, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9953696131706238, 'policy_logps/rejected': -304.31402587890625, 'policy_logps/chosen': -274.9153137207031, 'referece_logps/rejected': -272.3705139160156, 'referece_logps/chosen': -252.92547607421875, 'logits/rejected': -0.784968376159668, 'logits/chosen': -0.6844500303268433, 'epoch': 4.67}


 78%|███████▊  | 8362/10740 [41:27:21<11:20:41, 17.17s/it]
{'loss': 0.3627, 'learning_rate': 2.463279090772673e-07, 'rewards/chosen': -1.1803597211837769, 'rewards/rejected': -2.367436170578003, 'rewards/accuracies': 0.75, 'rewards/margins': 1.187076449394226, 'policy_logps/rejected': -218.6706085205078, 'policy_logps/chosen': -203.78977966308594, 'referece_logps/rejected': -194.99624633789062, 'referece_logps/chosen': -191.9862060546875, 'logits/rejected': -1.1321978569030762, 'logits/chosen': -1.2442883253097534, 'epoch': 4.67}
[2024-04-03 12:41:27,266] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8363/10740 [41:27:44<12:24:57, 18.80s/it]
[2024-04-03 12:41:47,370] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8364/10740 [41:28:04<12:40:05, 19.19s/it]

 78%|███████▊  | 8365/10740 [41:28:22<12:31:30, 18.99s/it]

 78%|███████▊  | 8366/10740 [41:28:42<12:40:10, 19.21s/it]

 78%|███████▊  | 8367/10740 [41:28:58<12:08:37, 18.42s/it]

 78%|███████▊  | 8368/10740 [41:29:19<12:38:59, 19.20s/it]


 78%|███████▊  | 8370/10740 [41:29:47<10:39:56, 16.20s/it]
{'loss': 0.3837, 'learning_rate': 2.4474437796435755e-07, 'rewards/chosen': -1.2995198965072632, 'rewards/rejected': -3.0721421241760254, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7726221084594727, 'policy_logps/rejected': -473.4576721191406, 'policy_logps/chosen': -461.37872314453125, 'referece_logps/rejected': -442.73626708984375, 'referece_logps/chosen': -448.3835144042969, 'logits/rejected': 0.06379558145999908, 'logits/chosen': -0.04182162880897522, 'epoch': 4.68}

 78%|███████▊  | 8371/10740 [41:30:08<11:29:58, 17.47s/it]
[2024-04-03 12:44:13,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8372/10740 [41:30:30<12:27:54, 18.95s/it]
[2024-04-03 12:44:36,053] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8373/10740 [41:30:52<13:07:28, 19.96s/it]
[2024-04-03 12:44:55,872] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8374/10740 [41:31:12<13:05:26, 19.92s/it]

 78%|███████▊  | 8375/10740 [41:31:30<12:41:13, 19.31s/it]

 78%|███████▊  | 8376/10740 [41:31:51<12:56:46, 19.72s/it]

 78%|███████▊  | 8377/10740 [41:32:10<12:56:05, 19.71s/it]

 78%|███████▊  | 8378/10740 [41:32:34<13:37:58, 20.78s/it]

 78%|███████▊  | 8379/10740 [41:32:54<13:31:24, 20.62s/it]

 78%|███████▊  | 8380/10740 [41:33:14<13:19:24, 20.32s/it]

 78%|███████▊  | 8381/10740 [41:33:24<11:24:58, 17.42s/it]


 78%|███████▊  | 8383/10740 [41:34:01<11:40:46, 17.84s/it]
{'loss': 0.4763, 'learning_rate': 2.421805204927525e-07, 'rewards/chosen': -1.8124897480010986, 'rewards/rejected': -2.54025936126709, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7277697324752808, 'policy_logps/rejected': -348.1315612792969, 'policy_logps/chosen': -353.2786865234375, 'referece_logps/rejected': -322.7289733886719, 'referece_logps/chosen': -335.15380859375, 'logits/rejected': -0.7371575236320496, 'logits/chosen': -0.6958048939704895, 'epoch': 4.68}

 78%|███████▊  | 8384/10740 [41:34:22<12:12:27, 18.65s/it]

 78%|███████▊  | 8385/10740 [41:34:35<11:04:15, 16.92s/it]

 78%|███████▊  | 8386/10740 [41:34:52<11:03:54, 16.92s/it]


 78%|███████▊  | 8388/10740 [41:35:27<11:32:07, 17.66s/it]
{'loss': 0.3645, 'learning_rate': 2.4119752096388137e-07, 'rewards/chosen': -2.027811050415039, 'rewards/rejected': -3.4949746131896973, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4671640396118164, 'policy_logps/rejected': -379.453857421875, 'policy_logps/chosen': -440.0107421875, 'referece_logps/rejected': -344.50408935546875, 'referece_logps/chosen': -419.73260498046875, 'logits/rejected': -0.31968486309051514, 'logits/chosen': -0.5090240240097046, 'epoch': 4.69}

 78%|███████▊  | 8389/10740 [41:35:48<12:11:23, 18.67s/it]

 78%|███████▊  | 8390/10740 [41:36:08<12:27:55, 19.10s/it]


 78%|███████▊  | 8392/10740 [41:36:45<12:17:29, 18.85s/it]

 78%|███████▊  | 8393/10740 [41:37:04<12:11:12, 18.69s/it]
{'loss': 0.4064, 'learning_rate': 2.4021624680835526e-07, 'rewards/chosen': -1.5511162281036377, 'rewards/rejected': -3.557560920715332, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0064446926116943, 'policy_logps/rejected': -266.1298828125, 'policy_logps/chosen': -268.51165771484375, 'referece_logps/rejected': -230.55429077148438, 'referece_logps/chosen': -253.00048828125, 'logits/rejected': -0.9737294316291809, 'logits/chosen': -1.1138404607772827, 'epoch': 4.69}

 78%|███████▊  | 8394/10740 [41:37:21<11:58:17, 18.37s/it]

 78%|███████▊  | 8395/10740 [41:37:41<12:15:26, 18.82s/it]
[2024-04-03 12:51:46,987] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8396/10740 [41:38:03<12:54:29, 19.83s/it]
[2024-04-03 12:52:06,667] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8397/10740 [41:38:23<12:52:27, 19.78s/it]

 78%|███████▊  | 8398/10740 [41:38:45<13:22:25, 20.56s/it]

 78%|███████▊  | 8399/10740 [41:39:05<13:11:32, 20.29s/it]

 78%|███████▊  | 8400/10740 [41:39:24<12:53:17, 19.83s/it]

 78%|███████▊  | 8401/10740 [41:39:36<11:19:35, 17.43s/it]

 78%|███████▊  | 8402/10740 [41:39:55<11:46:21, 18.13s/it]

 78%|███████▊  | 8403/10740 [41:40:13<11:39:48, 17.97s/it]

 78%|███████▊  | 8404/10740 [41:40:30<11:31:30, 17.76s/it]
[2024-04-03 12:54:33,799] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8405/10740 [41:40:50<11:55:59, 18.40s/it]

 78%|███████▊  | 8406/10740 [41:41:08<11:53:26, 18.34s/it]

 78%|███████▊  | 8407/10740 [41:41:20<10:33:46, 16.30s/it]

 78%|███████▊  | 8408/10740 [41:41:37<10:45:26, 16.61s/it]

 78%|███████▊  | 8409/10740 [41:41:59<11:46:54, 18.20s/it]

 78%|███████▊  | 8410/10740 [41:42:18<11:57:40, 18.48s/it]

 78%|███████▊  | 8411/10740 [41:42:38<12:18:03, 19.01s/it]

 78%|███████▊  | 8412/10740 [41:42:53<11:28:14, 17.74s/it]

 78%|███████▊  | 8413/10740 [41:43:15<12:16:19, 18.99s/it]

 78%|███████▊  | 8414/10740 [41:43:34<12:14:46, 18.95s/it]

 78%|███████▊  | 8415/10740 [41:43:54<12:28:35, 19.32s/it]

 78%|███████▊  | 8416/10740 [41:44:09<11:36:24, 17.98s/it]
[2024-04-03 12:58:15,507] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8417/10740 [41:44:32<12:31:46, 19.42s/it]
[2024-04-03 12:58:32,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8418/10740 [41:44:48<11:58:35, 18.57s/it]

 78%|███████▊  | 8419/10740 [41:45:05<11:32:44, 17.91s/it]

 78%|███████▊  | 8420/10740 [41:45:24<11:46:07, 18.26s/it]

 78%|███████▊  | 8421/10740 [41:45:35<10:21:54, 16.09s/it]

 78%|███████▊  | 8422/10740 [41:45:48<9:46:19, 15.18s/it]

 78%|███████▊  | 8423/10740 [41:46:04<10:02:15, 15.60s/it]

 78%|███████▊  | 8424/10740 [41:46:21<10:17:47, 16.00s/it]

 78%|███████▊  | 8425/10740 [41:46:33<9:26:23, 14.68s/it]

 78%|███████▊  | 8426/10740 [41:46:50<9:47:39, 15.24s/it]

 78%|███████▊  | 8427/10740 [41:47:09<10:38:01, 16.55s/it]

 78%|███████▊  | 8428/10740 [41:47:28<10:58:40, 17.09s/it]
[2024-04-03 13:01:33,243] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 8429/10740 [41:47:50<11:54:58, 18.56s/it]

 78%|███████▊  | 8430/10740 [41:48:02<10:46:55, 16.80s/it]

 79%|███████▊  | 8431/10740 [41:48:17<10:22:29, 16.18s/it]

 79%|███████▊  | 8432/10740 [41:48:35<10:49:36, 16.89s/it]

 79%|███████▊  | 8433/10740 [41:48:56<11:36:45, 18.12s/it]

 79%|███████▊  | 8434/10740 [41:49:15<11:40:58, 18.24s/it]

 79%|███████▊  | 8435/10740 [41:49:35<12:02:19, 18.80s/it]

 79%|███████▊  | 8436/10740 [41:49:46<10:27:18, 16.34s/it]

 79%|███████▊  | 8437/10740 [41:50:03<10:34:15, 16.52s/it]

 79%|███████▊  | 8438/10740 [41:50:15<9:47:24, 15.31s/it]

 79%|███████▊  | 8439/10740 [41:50:34<10:23:55, 16.27s/it]

 79%|███████▊  | 8440/10740 [41:50:45<9:26:18, 14.77s/it]

 79%|███████▊  | 8441/10740 [41:50:57<8:56:35, 14.00s/it]

 79%|███████▊  | 8442/10740 [41:51:17<10:02:23, 15.73s/it]

 79%|███████▊  | 8443/10740 [41:51:28<9:06:07, 14.27s/it]

 79%|███████▊  | 8444/10740 [41:51:43<9:21:37, 14.68s/it]

 79%|███████▊  | 8445/10740 [41:52:04<10:30:56, 16.50s/it]

 79%|███████▊  | 8446/10740 [41:52:24<11:05:55, 17.42s/it]

 79%|███████▊  | 8447/10740 [41:52:37<10:24:06, 16.33s/it]

 79%|███████▊  | 8448/10740 [41:52:57<10:58:10, 17.23s/it]

 79%|███████▊  | 8449/10740 [41:53:14<10:59:57, 17.28s/it]

 79%|███████▊  | 8450/10740 [41:53:34<11:26:02, 17.98s/it]

 79%|███████▊  | 8451/10740 [41:53:54<11:48:11, 18.56s/it]

 79%|███████▊  | 8452/10740 [41:54:09<11:15:51, 17.72s/it]

 79%|███████▊  | 8453/10740 [41:54:28<11:19:48, 17.83s/it]


 79%|███████▊  | 8455/10740 [41:55:04<11:22:38, 17.93s/it]

 79%|███████▊  | 8456/10740 [41:55:22<11:18:14, 17.82s/it]

 79%|███████▊  | 8457/10740 [41:55:45<12:24:20, 19.56s/it]

 79%|███████▉  | 8458/10740 [41:56:05<12:28:52, 19.69s/it]

 79%|███████▉  | 8459/10740 [41:56:25<12:29:25, 19.71s/it]
{'loss': 0.2696, 'learning_rate': 2.274261837357303e-07, 'rewards/chosen': -2.944584608078003, 'rewards/rejected': -5.669631004333496, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7250466346740723, 'policy_logps/rejected': -502.9941711425781, 'policy_logps/chosen': -451.186767578125, 'referece_logps/rejected': -446.2978820800781, 'referece_logps/chosen': -421.74090576171875, 'logits/rejected': -0.7432960271835327, 'logits/chosen': -0.7115933895111084, 'epoch': 4.73}


 79%|███████▉  | 8461/10740 [41:57:05<12:31:10, 19.78s/it]

 79%|███████▉  | 8462/10740 [41:57:21<11:45:50, 18.59s/it]
{'loss': 0.3788, 'learning_rate': 2.2685205183476852e-07, 'rewards/chosen': -1.9134199619293213, 'rewards/rejected': -2.8689088821411133, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9554888010025024, 'policy_logps/rejected': -258.9329528808594, 'policy_logps/chosen': -287.31585693359375, 'referece_logps/rejected': -230.24386596679688, 'referece_logps/chosen': -268.1816711425781, 'logits/rejected': -1.1446014642715454, 'logits/chosen': -1.1884496212005615, 'epoch': 4.73}


 79%|███████▉  | 8464/10740 [41:57:53<11:03:02, 17.48s/it]

 79%|███████▉  | 8465/10740 [41:58:13<11:31:47, 18.25s/it]

 79%|███████▉  | 8466/10740 [41:58:25<10:12:48, 16.17s/it]

 79%|███████▉  | 8467/10740 [41:58:45<10:56:43, 17.34s/it]

 79%|███████▉  | 8468/10740 [41:59:04<11:22:31, 18.02s/it]

 79%|███████▉  | 8469/10740 [41:59:24<11:41:39, 18.54s/it]

 79%|███████▉  | 8470/10740 [41:59:37<10:35:38, 16.80s/it]

 79%|███████▉  | 8471/10740 [41:59:50<9:52:24, 15.67s/it]

 79%|███████▉  | 8472/10740 [42:00:07<10:05:57, 16.03s/it]

 79%|███████▉  | 8473/10740 [42:00:26<10:44:54, 17.07s/it]

 79%|███████▉  | 8474/10740 [42:00:45<10:58:02, 17.42s/it]

 79%|███████▉  | 8475/10740 [42:01:06<11:41:07, 18.57s/it]

 79%|███████▉  | 8476/10740 [42:01:24<11:37:42, 18.49s/it]

 79%|███████▉  | 8477/10740 [42:01:45<12:00:01, 19.09s/it]
{'loss': 0.3887, 'learning_rate': 2.2399089487037715e-07, 'rewards/chosen': -2.011162281036377, 'rewards/rejected': -3.9785141944885254, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9673517942428589, 'policy_logps/rejected': -490.206787109375, 'policy_logps/chosen': -452.4923095703125, 'referece_logps/rejected': -450.42169189453125, 'referece_logps/chosen': -432.3807067871094, 'logits/rejected': -0.6760437488555908, 'logits/chosen': -0.79909348487854, 'epoch': 4.74}


 79%|███████▉  | 8479/10740 [42:02:26<12:33:15, 19.99s/it]

 79%|███████▉  | 8480/10740 [42:02:45<12:23:05, 19.73s/it]

 79%|███████▉  | 8481/10740 [42:03:04<12:20:29, 19.67s/it]

 79%|███████▉  | 8482/10740 [42:03:21<11:49:14, 18.85s/it]

 79%|███████▉  | 8483/10740 [42:03:41<12:00:29, 19.15s/it]

 79%|███████▉  | 8484/10740 [42:03:58<11:31:53, 18.40s/it]

 79%|███████▉  | 8485/10740 [42:04:18<11:55:36, 19.04s/it]

 79%|███████▉  | 8486/10740 [42:04:36<11:37:03, 18.56s/it]

 79%|███████▉  | 8487/10740 [42:04:48<10:25:39, 16.66s/it]

 79%|███████▉  | 8488/10740 [42:05:10<11:30:21, 18.39s/it]

 79%|███████▉  | 8489/10740 [42:05:27<11:13:24, 17.95s/it]

 79%|███████▉  | 8490/10740 [42:05:44<10:56:13, 17.50s/it]

 79%|███████▉  | 8491/10740 [42:06:06<11:49:14, 18.92s/it]

 79%|███████▉  | 8492/10740 [42:06:17<10:19:00, 16.52s/it]

 79%|███████▉  | 8493/10740 [42:06:36<10:52:29, 17.42s/it]

 79%|███████▉  | 8494/10740 [42:06:53<10:46:24, 17.27s/it]

 79%|███████▉  | 8495/10740 [42:07:15<11:37:23, 18.64s/it]

 79%|███████▉  | 8496/10740 [42:07:27<10:23:13, 16.66s/it]

 79%|███████▉  | 8497/10740 [42:07:42<10:03:27, 16.14s/it]
{'loss': 0.2986, 'learning_rate': 2.2020073192506316e-07, 'rewards/chosen': -0.7272522449493408, 'rewards/rejected': -3.4450602531433105, 'rewards/accuracies': 1.0, 'rewards/margins': 2.717808246612549, 'policy_logps/rejected': -275.11212158203125, 'policy_logps/chosen': -210.5093536376953, 'referece_logps/rejected': -240.66151428222656, 'referece_logps/chosen': -203.23683166503906, 'logits/rejected': -1.1771364212036133, 'logits/chosen': -1.0669432878494263, 'epoch': 4.75}


 79%|███████▉  | 8499/10740 [42:08:27<12:03:41, 19.38s/it]
[2024-04-03 13:22:10,555] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 8500/10740 [42:08:49<12:33:25, 20.18s/it]
[2024-04-03 13:22:32,615] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 8501/10740 [42:09:18<14:08:34, 22.74s/it]

 79%|███████▉  | 8502/10740 [42:09:37<13:32:19, 21.78s/it]

 79%|███████▉  | 8503/10740 [42:09:55<12:49:33, 20.64s/it]

 79%|███████▉  | 8504/10740 [42:10:15<12:46:08, 20.56s/it]
{'loss': 0.4516, 'learning_rate': 2.1888087022668366e-07, 'rewards/chosen': -2.4827370643615723, 'rewards/rejected': -2.870971918106079, 'rewards/accuracies': 0.75, 'rewards/margins': 0.388234943151474, 'policy_logps/rejected': -464.7508850097656, 'policy_logps/chosen': -406.8710632324219, 'referece_logps/rejected': -436.04119873046875, 'referece_logps/chosen': -382.043701171875, 'logits/rejected': 0.548622727394104, 'logits/chosen': 0.5915458798408508, 'epoch': 4.75}


 79%|███████▉  | 8506/10740 [42:10:48<11:31:33, 18.57s/it]

 79%|███████▉  | 8507/10740 [42:11:06<11:26:21, 18.44s/it]

 79%|███████▉  | 8508/10740 [42:11:23<11:07:53, 17.95s/it]

 79%|███████▉  | 8509/10740 [42:11:45<12:00:43, 19.38s/it]
{'loss': 0.3457, 'learning_rate': 2.1794024286856639e-07, 'rewards/chosen': -1.837074875831604, 'rewards/rejected': -3.0029170513153076, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1658422946929932, 'policy_logps/rejected': -401.7794189453125, 'policy_logps/chosen': -359.1565246582031, 'referece_logps/rejected': -371.750244140625, 'referece_logps/chosen': -340.7857666015625, 'logits/rejected': -0.23991598188877106, 'logits/chosen': -0.32116007804870605, 'epoch': 4.75}


 79%|███████▉  | 8511/10740 [42:12:18<10:42:18, 17.29s/it]
{'loss': 0.3859, 'learning_rate': 2.175644897173613e-07, 'rewards/chosen': -2.1776282787323, 'rewards/rejected': -3.552769184112549, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3751407861709595, 'policy_logps/rejected': -294.40008544921875, 'policy_logps/chosen': -320.1499938964844, 'referece_logps/rejected': -258.8724060058594, 'referece_logps/chosen': -298.3736877441406, 'logits/rejected': 0.1509096622467041, 'logits/chosen': 0.0809672623872757, 'epoch': 4.75}

 79%|███████▉  | 8512/10740 [42:12:34<10:36:55, 17.15s/it]

 79%|███████▉  | 8513/10740 [42:12:48<9:59:31, 16.15s/it]


 79%|███████▉  | 8515/10740 [42:13:17<9:14:16, 14.95s/it]

 79%|███████▉  | 8516/10740 [42:13:33<9:25:33, 15.26s/it]
{'loss': 0.5079, 'learning_rate': 2.16626352515602e-07, 'rewards/chosen': -1.7735977172851562, 'rewards/rejected': -3.2052340507507324, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4316364526748657, 'policy_logps/rejected': -547.85693359375, 'policy_logps/chosen': -569.7276000976562, 'referece_logps/rejected': -515.8045654296875, 'referece_logps/chosen': -551.9915771484375, 'logits/rejected': -1.6193089485168457, 'logits/chosen': -1.6664241552352905, 'epoch': 4.76}

 79%|███████▉  | 8517/10740 [42:13:54<10:29:30, 16.99s/it]

 79%|███████▉  | 8518/10740 [42:14:07<9:39:17, 15.64s/it]


 79%|███████▉  | 8520/10740 [42:14:30<8:23:13, 13.60s/it]

 79%|███████▉  | 8521/10740 [42:14:47<9:05:47, 14.76s/it]
{'loss': 0.3345, 'learning_rate': 2.1568999655737828e-07, 'rewards/chosen': -2.0840864181518555, 'rewards/rejected': -5.1312103271484375, 'rewards/accuracies': 0.75, 'rewards/margins': 3.047124147415161, 'policy_logps/rejected': -532.9276123046875, 'policy_logps/chosen': -461.9430236816406, 'referece_logps/rejected': -481.6155090332031, 'referece_logps/chosen': -441.1021423339844, 'logits/rejected': -0.09958276152610779, 'logits/chosen': -0.05551952123641968, 'epoch': 4.76}


 79%|███████▉  | 8523/10740 [42:15:28<10:53:29, 17.69s/it]
{'loss': 0.2742, 'learning_rate': 2.1531595339933295e-07, 'rewards/chosen': -1.419821858406067, 'rewards/rejected': -4.130099296569824, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7102770805358887, 'policy_logps/rejected': -370.10791015625, 'policy_logps/chosen': -305.56591796875, 'referece_logps/rejected': -328.8069152832031, 'referece_logps/chosen': -291.3677062988281, 'logits/rejected': -0.6877487897872925, 'logits/chosen': -0.6530770063400269, 'epoch': 4.76}


 79%|███████▉  | 8525/10740 [42:16:05<11:08:09, 18.10s/it]

 79%|███████▉  | 8526/10740 [42:16:24<11:17:22, 18.36s/it]

 79%|███████▉  | 8527/10740 [42:16:40<10:51:28, 17.66s/it]

 79%|███████▉  | 8528/10740 [42:16:58<10:53:15, 17.72s/it]

 79%|███████▉  | 8529/10740 [42:17:18<11:18:57, 18.42s/it]

 79%|███████▉  | 8530/10740 [42:17:38<11:34:04, 18.84s/it]

 79%|███████▉  | 8531/10740 [42:17:59<12:03:25, 19.65s/it]
{'loss': 0.2664, 'learning_rate': 2.1382263688387004e-07, 'rewards/chosen': -1.450337290763855, 'rewards/rejected': -4.8310346603393555, 'rewards/accuracies': 1.0, 'rewards/margins': 3.380697250366211, 'policy_logps/rejected': -483.8119812011719, 'policy_logps/chosen': -356.3455505371094, 'referece_logps/rejected': -435.50164794921875, 'referece_logps/chosen': -341.8421630859375, 'logits/rejected': -0.29806968569755554, 'logits/chosen': -0.27539676427841187, 'epoch': 4.77}
[2024-04-03 13:32:04,036] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 8532/10740 [42:18:20<12:19:38, 20.10s/it]
[2024-04-03 13:32:18,300] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 8534/10740 [42:18:53<11:19:40, 18.49s/it]

 79%|███████▉  | 8535/10740 [42:19:14<11:44:47, 19.18s/it]
{'loss': 0.3317, 'learning_rate': 2.1307769419729737e-07, 'rewards/chosen': -3.2845382690429688, 'rewards/rejected': -4.942795753479004, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6582573652267456, 'policy_logps/rejected': -604.3987426757812, 'policy_logps/chosen': -569.78857421875, 'referece_logps/rejected': -554.9707641601562, 'referece_logps/chosen': -536.9431762695312, 'logits/rejected': 0.7127445340156555, 'logits/chosen': 0.6361708641052246, 'epoch': 4.77}


 79%|███████▉  | 8537/10740 [42:19:51<11:24:37, 18.65s/it]
[2024-04-03 13:33:34,854] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3713, 'learning_rate': 2.1270565222135417e-07, 'rewards/chosen': -1.8201396465301514, 'rewards/rejected': -3.261646270751953, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4415061473846436, 'policy_logps/rejected': -339.8198547363281, 'policy_logps/chosen': -368.3835754394531, 'referece_logps/rejected': -307.2033996582031, 'referece_logps/chosen': -350.1822204589844, 'logits/rejected': -0.2350481152534485, 'logits/chosen': -0.2678707540035248, 'epoch': 4.77}


 80%|███████▉  | 8539/10740 [42:20:30<11:37:47, 19.02s/it]
{'loss': 0.2989, 'learning_rate': 2.123338966708117e-07, 'rewards/chosen': -2.596992254257202, 'rewards/rejected': -4.663413047790527, 'rewards/accuracies': 1.0, 'rewards/margins': 2.066420793533325, 'policy_logps/rejected': -319.2421569824219, 'policy_logps/chosen': -369.3921813964844, 'referece_logps/rejected': -272.6080322265625, 'referece_logps/chosen': -343.4222717285156, 'logits/rejected': -1.7162858247756958, 'logits/chosen': -1.7343614101409912, 'epoch': 4.77}

 80%|███████▉  | 8540/10740 [42:20:49<11:34:17, 18.94s/it]
[2024-04-03 13:34:52,338] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 8541/10740 [42:21:09<11:45:35, 19.25s/it]

 80%|███████▉  | 8542/10740 [42:21:29<11:52:56, 19.46s/it]


 80%|███████▉  | 8544/10740 [42:22:04<11:27:58, 18.80s/it]

 80%|███████▉  | 8545/10740 [42:22:24<11:33:31, 18.96s/it]

 80%|███████▉  | 8546/10740 [42:22:40<11:02:18, 18.11s/it]

 80%|███████▉  | 8547/10740 [42:23:00<11:29:19, 18.86s/it]
[2024-04-03 13:36:44,052] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.282, 'learning_rate': 2.1084974142604627e-07, 'rewards/chosen': -1.052233099937439, 'rewards/rejected': -6.023338794708252, 'rewards/accuracies': 1.0, 'rewards/margins': 4.971105575561523, 'policy_logps/rejected': -495.3883056640625, 'policy_logps/chosen': -449.7732238769531, 'referece_logps/rejected': -435.15496826171875, 'referece_logps/chosen': -439.2508544921875, 'logits/rejected': -0.3426111936569214, 'logits/chosen': -0.37280476093292236, 'epoch': 4.77}

 80%|███████▉  | 8548/10740 [42:23:23<12:07:14, 19.91s/it]
[2024-04-03 13:37:28,540] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 8549/10740 [42:23:45<12:31:22, 20.58s/it]
[2024-04-03 13:37:46,603] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 8551/10740 [42:24:18<11:16:22, 18.54s/it]

 80%|███████▉  | 8552/10740 [42:24:34<10:46:31, 17.73s/it]

 80%|███████▉  | 8553/10740 [42:24:56<11:27:37, 18.86s/it]

 80%|███████▉  | 8554/10740 [42:25:14<11:22:17, 18.73s/it]
{'loss': 0.4697, 'learning_rate': 2.095548734858944e-07, 'rewards/chosen': -1.583440899848938, 'rewards/rejected': -3.623030662536621, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0395896434783936, 'policy_logps/rejected': -315.3209533691406, 'policy_logps/chosen': -369.026611328125, 'referece_logps/rejected': -279.0906677246094, 'referece_logps/chosen': -353.1921691894531, 'logits/rejected': -0.5311414003372192, 'logits/chosen': -0.6312665939331055, 'epoch': 4.78}


 80%|███████▉  | 8556/10740 [42:25:53<11:37:56, 19.17s/it]

 80%|███████▉  | 8557/10740 [42:26:04<10:08:03, 16.71s/it]

 80%|███████▉  | 8558/10740 [42:26:16<9:23:52, 15.51s/it]

 80%|███████▉  | 8559/10740 [42:26:37<10:15:10, 16.92s/it]

 80%|███████▉  | 8560/10740 [42:26:50<9:34:16, 15.81s/it]
{'loss': 0.4236, 'learning_rate': 2.0844779028472403e-07, 'rewards/chosen': -1.7962723970413208, 'rewards/rejected': -3.1579205989837646, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3616483211517334, 'policy_logps/rejected': -333.21380615234375, 'policy_logps/chosen': -307.8544616699219, 'referece_logps/rejected': -301.6346435546875, 'referece_logps/chosen': -289.8917541503906, 'logits/rejected': -0.7044980525970459, 'logits/chosen': -0.7292211055755615, 'epoch': 4.78}
[2024-04-03 13:40:53,251] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 8561/10740 [42:27:10<10:17:14, 17.00s/it]


 80%|███████▉  | 8563/10740 [42:27:42<10:02:19, 16.60s/it]

 80%|███████▉  | 8564/10740 [42:28:00<10:22:04, 17.15s/it]

 80%|███████▉  | 8565/10740 [42:28:18<10:31:27, 17.42s/it]
{'loss': 0.4455, 'learning_rate': 2.0752720062071439e-07, 'rewards/chosen': -1.9080592393875122, 'rewards/rejected': -4.356019973754883, 'rewards/accuracies': 1.0, 'rewards/margins': 2.447960376739502, 'policy_logps/rejected': -322.413330078125, 'policy_logps/chosen': -330.96533203125, 'referece_logps/rejected': -278.8531494140625, 'referece_logps/chosen': -311.8847961425781, 'logits/rejected': -0.3240819573402405, 'logits/chosen': -0.1976965218782425, 'epoch': 4.78}


 80%|███████▉  | 8567/10740 [42:28:55<10:49:35, 17.94s/it]
{'loss': 0.3335, 'learning_rate': 2.0715946917934656e-07, 'rewards/chosen': -3.21443247795105, 'rewards/rejected': -4.372805118560791, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1583726406097412, 'policy_logps/rejected': -399.13232421875, 'policy_logps/chosen': -326.0050964355469, 'referece_logps/rejected': -355.4042663574219, 'referece_logps/chosen': -293.86077880859375, 'logits/rejected': 0.15200206637382507, 'logits/chosen': 0.13509473204612732, 'epoch': 4.79}
[2024-04-03 13:43:00,784] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 8569/10740 [42:29:28<10:04:26, 16.71s/it]
{'loss': 0.3883, 'learning_rate': 2.06792026181135e-07, 'rewards/chosen': -1.3149055242538452, 'rewards/rejected': -2.4000418186187744, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0851362943649292, 'policy_logps/rejected': -439.17047119140625, 'policy_logps/chosen': -378.2572326660156, 'referece_logps/rejected': -415.1700744628906, 'referece_logps/chosen': -365.108154296875, 'logits/rejected': -0.267640620470047, 'logits/chosen': -0.28965890407562256, 'epoch': 4.79}

 80%|███████▉  | 8570/10740 [42:29:39<9:08:22, 15.16s/it]
[2024-04-03 13:43:38,881] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 8572/10740 [42:30:10<9:08:40, 15.18s/it]
{'loss': 0.3853, 'learning_rate': 2.062414028071301e-07, 'rewards/chosen': -1.192643404006958, 'rewards/rejected': -2.7517552375793457, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5591115951538086, 'policy_logps/rejected': -419.51031494140625, 'policy_logps/chosen': -432.61676025390625, 'referece_logps/rejected': -391.9927673339844, 'referece_logps/chosen': -420.6903381347656, 'logits/rejected': -0.6243860125541687, 'logits/chosen': -0.7441250085830688, 'epoch': 4.79}

 80%|███████▉  | 8573/10740 [42:30:29<9:56:02, 16.50s/it]

 80%|███████▉  | 8574/10740 [42:30:47<10:10:20, 16.91s/it]

 80%|███████▉  | 8575/10740 [42:31:04<10:03:52, 16.74s/it]


 80%|███████▉  | 8577/10740 [42:31:39<10:15:32, 17.07s/it]
[2024-04-03 13:45:22,366] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 8578/10740 [42:31:57<10:29:35, 17.47s/it]

 80%|███████▉  | 8579/10740 [42:32:17<10:52:01, 18.10s/it]

 80%|███████▉  | 8580/10740 [42:32:32<10:27:15, 17.42s/it]

 80%|███████▉  | 8581/10740 [42:32:54<11:13:00, 18.70s/it]

 80%|███████▉  | 8582/10740 [42:33:16<11:49:44, 19.73s/it]

 80%|███████▉  | 8583/10740 [42:33:33<11:17:58, 18.86s/it]
{'loss': 0.3232, 'learning_rate': 2.042280128178552e-07, 'rewards/chosen': -1.569600224494934, 'rewards/rejected': -3.372663736343384, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8030635118484497, 'policy_logps/rejected': -470.3861083984375, 'policy_logps/chosen': -360.94140625, 'referece_logps/rejected': -436.65948486328125, 'referece_logps/chosen': -345.24542236328125, 'logits/rejected': -0.6985402703285217, 'logits/chosen': -0.7034776210784912, 'epoch': 4.79}
[2024-04-03 13:47:37,399] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 8585/10740 [42:34:12<11:28:05, 19.16s/it]

 80%|███████▉  | 8586/10740 [42:34:34<11:56:23, 19.96s/it]

 80%|███████▉  | 8587/10740 [42:34:54<11:56:25, 19.97s/it]

 80%|███████▉  | 8588/10740 [42:35:09<11:02:38, 18.48s/it]

 80%|███████▉  | 8589/10740 [42:35:27<10:53:30, 18.23s/it]
{'loss': 0.3699, 'learning_rate': 2.031334899300551e-07, 'rewards/chosen': -1.5564390420913696, 'rewards/rejected': -3.461568593978882, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9051294326782227, 'policy_logps/rejected': -359.1387634277344, 'policy_logps/chosen': -359.3283996582031, 'referece_logps/rejected': -324.5230407714844, 'referece_logps/chosen': -343.76397705078125, 'logits/rejected': -0.640470564365387, 'logits/chosen': -0.5941552519798279, 'epoch': 4.8}

 80%|███████▉  | 8590/10740 [42:35:42<10:17:40, 17.24s/it]

 80%|███████▉  | 8591/10740 [42:36:02<10:46:33, 18.05s/it]


 80%|████████  | 8593/10740 [42:36:40<11:09:17, 18.70s/it]

 80%|████████  | 8594/10740 [42:36:57<10:42:37, 17.97s/it]

 80%|████████  | 8595/10740 [42:37:15<10:43:38, 18.00s/it]

 80%|████████  | 8596/10740 [42:37:31<10:25:13, 17.50s/it]

 80%|████████  | 8597/10740 [42:37:50<10:42:49, 18.00s/it]
{'loss': 0.4493, 'learning_rate': 2.0167818540671433e-07, 'rewards/chosen': -2.03905987739563, 'rewards/rejected': -2.562593460083008, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5235337615013123, 'policy_logps/rejected': -384.1521301269531, 'policy_logps/chosen': -416.83575439453125, 'referece_logps/rejected': -358.5262451171875, 'referece_logps/chosen': -396.4451904296875, 'logits/rejected': -1.3462287187576294, 'logits/chosen': -1.3758257627487183, 'epoch': 4.8}

 80%|████████  | 8598/10740 [42:38:10<10:59:01, 18.46s/it]

 80%|████████  | 8599/10740 [42:38:26<10:33:16, 17.75s/it]

 80%|████████  | 8600/10740 [42:38:38<9:31:21, 16.02s/it]
[2024-04-03 13:52:43,979] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 8601/10740 [42:39:00<10:38:59, 17.92s/it]
[2024-04-03 13:53:05,551] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 8602/10740 [42:39:22<11:17:41, 19.02s/it]

 80%|████████  | 8603/10740 [42:39:34<10:07:40, 17.06s/it]

 80%|████████  | 8604/10740 [42:39:56<10:55:48, 18.42s/it]

 80%|████████  | 8605/10740 [42:40:16<11:11:55, 18.88s/it]


 80%|████████  | 8607/10740 [42:40:53<11:00:06, 18.57s/it]

 80%|████████  | 8608/10740 [42:41:13<11:10:32, 18.87s/it]

 80%|████████  | 8609/10740 [42:41:34<11:32:02, 19.48s/it]
[2024-04-03 13:55:17,282] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2509, 'learning_rate': 1.9950394437663952e-07, 'rewards/chosen': -0.8793646097183228, 'rewards/rejected': -3.538935899734497, 'rewards/accuracies': 1.0, 'rewards/margins': 2.659571409225464, 'policy_logps/rejected': -421.64422607421875, 'policy_logps/chosen': -268.1237487792969, 'referece_logps/rejected': -386.2548828125, 'referece_logps/chosen': -259.3301086425781, 'logits/rejected': -0.4278079867362976, 'logits/chosen': -0.5964775681495667, 'epoch': 4.81}
[2024-04-03 13:55:39,979] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|████████  | 8611/10740 [42:42:17<12:04:16, 20.41s/it]

 80%|████████  | 8612/10740 [42:42:35<11:47:56, 19.96s/it]

 80%|████████  | 8613/10740 [42:42:58<12:10:05, 20.59s/it]
[2024-04-03 13:56:41,290] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 8614/10740 [42:43:18<12:05:27, 20.47s/it]
[2024-04-03 13:57:01,481] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.262, 'learning_rate': 1.9860110328218304e-07, 'rewards/chosen': -1.3861055374145508, 'rewards/rejected': -3.218350648880005, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8322452306747437, 'policy_logps/rejected': -609.9971923828125, 'policy_logps/chosen': -511.0977783203125, 'referece_logps/rejected': -577.813720703125, 'referece_logps/chosen': -497.2366943359375, 'logits/rejected': -0.29358112812042236, 'logits/chosen': -0.4305057227611542, 'epoch': 4.81}


 80%|████████  | 8616/10740 [42:44:02<12:31:48, 21.24s/it]
{'loss': 0.3274, 'learning_rate': 1.9824047695385027e-07, 'rewards/chosen': -1.4263378381729126, 'rewards/rejected': -3.493482828140259, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0671451091766357, 'policy_logps/rejected': -459.8538818359375, 'policy_logps/chosen': -445.6622619628906, 'referece_logps/rejected': -424.91900634765625, 'referece_logps/chosen': -431.39892578125, 'logits/rejected': -0.42425379157066345, 'logits/chosen': -0.5009592175483704, 'epoch': 4.81}

 80%|████████  | 8617/10740 [42:44:23<12:27:39, 21.13s/it]
[2024-04-03 13:58:26,594] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|████████  | 8619/10740 [42:45:02<11:57:53, 20.31s/it]

 80%|████████  | 8620/10740 [42:45:21<11:50:05, 20.10s/it]

 80%|████████  | 8621/10740 [42:45:39<11:27:39, 19.47s/it]
{'loss': 0.4267, 'learning_rate': 1.9734018755463644e-07, 'rewards/chosen': -1.3887847661972046, 'rewards/rejected': -3.4394094944000244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0506246089935303, 'policy_logps/rejected': -384.3692932128906, 'policy_logps/chosen': -462.47491455078125, 'referece_logps/rejected': -349.9752197265625, 'referece_logps/chosen': -448.58709716796875, 'logits/rejected': -0.43396905064582825, 'logits/chosen': -0.6110292673110962, 'epoch': 4.82}


 80%|████████  | 8623/10740 [42:46:17<11:15:43, 19.15s/it]

 80%|████████  | 8624/10740 [42:46:31<10:20:44, 17.60s/it]
{'loss': 0.3863, 'learning_rate': 1.968008898306489e-07, 'rewards/chosen': -2.2289042472839355, 'rewards/rejected': -3.095100164413452, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8661956787109375, 'policy_logps/rejected': -452.79632568359375, 'policy_logps/chosen': -341.7916564941406, 'referece_logps/rejected': -421.8453063964844, 'referece_logps/chosen': -319.5025634765625, 'logits/rejected': -0.895976722240448, 'logits/chosen': -0.8366501331329346, 'epoch': 4.82}

 80%|████████  | 8625/10740 [42:46:49<10:18:05, 17.53s/it]

 80%|████████  | 8626/10740 [42:47:03<9:39:42, 16.45s/it]

 80%|████████  | 8627/10740 [42:47:23<10:18:56, 17.58s/it]

 80%|████████  | 8628/10740 [42:47:43<10:47:56, 18.41s/it]


 80%|████████  | 8630/10740 [42:48:18<10:18:12, 17.58s/it]
{'loss': 0.3111, 'learning_rate': 1.95724267252549e-07, 'rewards/chosen': -1.1563977003097534, 'rewards/rejected': -3.5122413635253906, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3558433055877686, 'policy_logps/rejected': -414.3534851074219, 'policy_logps/chosen': -472.0718078613281, 'referece_logps/rejected': -379.2310791015625, 'referece_logps/chosen': -460.50787353515625, 'logits/rejected': -0.014270760118961334, 'logits/chosen': -0.10508205741643906, 'epoch': 4.82}

 80%|████████  | 8631/10740 [42:48:30<9:27:19, 16.14s/it]

 80%|████████  | 8632/10740 [42:48:48<9:45:01, 16.65s/it]


 80%|████████  | 8634/10740 [42:49:26<10:34:31, 18.08s/it]
{'loss': 0.4195, 'learning_rate': 1.9500798166651866e-07, 'rewards/chosen': -1.6735762357711792, 'rewards/rejected': -2.8853282928466797, 'rewards/accuracies': 0.875, 'rewards/margins': 1.21175217628479, 'policy_logps/rejected': -488.91925048828125, 'policy_logps/chosen': -540.4408569335938, 'referece_logps/rejected': -460.06591796875, 'referece_logps/chosen': -523.705078125, 'logits/rejected': 0.2587487995624542, 'logits/chosen': 0.3436305820941925, 'epoch': 4.82}

 80%|████████  | 8635/10740 [42:49:45<10:41:58, 18.30s/it]

 80%|████████  | 8636/10740 [42:50:05<10:57:16, 18.74s/it]

 80%|████████  | 8637/10740 [42:50:23<10:54:04, 18.66s/it]

 80%|████████  | 8638/10740 [42:50:46<11:31:33, 19.74s/it]

 80%|████████  | 8639/10740 [42:51:05<11:27:43, 19.64s/it]

 80%|████████  | 8640/10740 [42:51:17<10:09:51, 17.42s/it]

 80%|████████  | 8641/10740 [42:51:37<10:31:35, 18.05s/it]
[2024-04-03 14:05:42,245] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 8642/10740 [42:51:59<11:10:40, 19.18s/it]
[2024-04-03 14:06:02,151] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 8643/10740 [42:52:18<11:17:58, 19.40s/it]

 80%|████████  | 8644/10740 [42:52:39<11:33:33, 19.85s/it]

 80%|████████  | 8645/10740 [42:52:57<11:08:35, 19.15s/it]

 81%|████████  | 8646/10740 [42:53:17<11:16:57, 19.40s/it]

 81%|████████  | 8647/10740 [42:53:35<11:08:31, 19.16s/it]

 81%|████████  | 8648/10740 [42:53:53<10:52:17, 18.71s/it]

 81%|████████  | 8649/10740 [42:54:13<11:01:30, 18.98s/it]

 81%|████████  | 8650/10740 [42:54:29<10:33:31, 18.19s/it]


 81%|████████  | 8652/10740 [42:55:10<11:18:07, 19.49s/it]
{'loss': 0.4687, 'learning_rate': 1.9179920830454445e-07, 'rewards/chosen': -1.892357587814331, 'rewards/rejected': -2.6162421703338623, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7238845825195312, 'policy_logps/rejected': -285.54815673828125, 'policy_logps/chosen': -414.0744323730469, 'referece_logps/rejected': -259.3857727050781, 'referece_logps/chosen': -395.1508483886719, 'logits/rejected': -1.1453933715820312, 'logits/chosen': -1.2923626899719238, 'epoch': 4.83}

 81%|████████  | 8653/10740 [42:55:29<11:14:09, 19.38s/it]

 81%|████████  | 8654/10740 [42:55:45<10:39:16, 18.39s/it]

 81%|████████  | 8655/10740 [42:55:58<9:34:25, 16.53s/it]


 81%|████████  | 8657/10740 [42:56:34<10:13:11, 17.66s/it]

 81%|████████  | 8658/10740 [42:56:57<11:00:36, 19.04s/it]
{'loss': 0.3372, 'learning_rate': 1.907349050863707e-07, 'rewards/chosen': -1.2746144533157349, 'rewards/rejected': -3.2414073944091797, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9667930603027344, 'policy_logps/rejected': -552.3306274414062, 'policy_logps/chosen': -494.0594177246094, 'referece_logps/rejected': -519.9165649414062, 'referece_logps/chosen': -481.313232421875, 'logits/rejected': -0.8959901332855225, 'logits/chosen': -0.9607036709785461, 'epoch': 4.84}

 81%|████████  | 8659/10740 [42:57:08<9:40:42, 16.74s/it]


 81%|████████  | 8661/10740 [42:57:48<10:49:54, 18.76s/it]
[2024-04-03 14:11:32,078] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3328, 'learning_rate': 1.9020374692226494e-07, 'rewards/chosen': -2.2383246421813965, 'rewards/rejected': -4.713312149047852, 'rewards/accuracies': 1.0, 'rewards/margins': 2.474987745285034, 'policy_logps/rejected': -561.7191772460938, 'policy_logps/chosen': -569.8943481445312, 'referece_logps/rejected': -514.5860595703125, 'referece_logps/chosen': -547.5111083984375, 'logits/rejected': 0.10548613965511322, 'logits/chosen': 0.039244696497917175, 'epoch': 4.84}
[2024-04-03 14:11:47,481] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8662/10740 [42:58:04<10:14:46, 17.75s/it]


 81%|████████  | 8664/10740 [42:58:37<9:41:49, 16.82s/it]

 81%|████████  | 8665/10740 [42:58:56<10:11:06, 17.67s/it]

 81%|████████  | 8666/10740 [42:59:17<10:37:51, 18.45s/it]
{'loss': 0.303, 'learning_rate': 1.8931995658890244e-07, 'rewards/chosen': -0.8899948000907898, 'rewards/rejected': -3.5526304244995117, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6626358032226562, 'policy_logps/rejected': -285.25018310546875, 'policy_logps/chosen': -270.2478332519531, 'referece_logps/rejected': -249.723876953125, 'referece_logps/chosen': -261.347900390625, 'logits/rejected': -0.2897990345954895, 'logits/chosen': -0.23097416758537292, 'epoch': 4.84}

 81%|████████  | 8667/10740 [42:59:38<11:03:39, 19.21s/it]


 81%|████████  | 8669/10740 [43:00:16<11:04:38, 19.26s/it]
{'loss': 0.3245, 'learning_rate': 1.8879056706034237e-07, 'rewards/chosen': -1.0397059917449951, 'rewards/rejected': -3.4416985511779785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4019925594329834, 'policy_logps/rejected': -306.284912109375, 'policy_logps/chosen': -273.35711669921875, 'referece_logps/rejected': -271.867919921875, 'referece_logps/chosen': -262.9600830078125, 'logits/rejected': -0.5703240633010864, 'logits/chosen': -0.4306005835533142, 'epoch': 4.84}

 81%|████████  | 8670/10740 [43:00:37<11:22:54, 19.79s/it]

 81%|████████  | 8671/10740 [43:00:50<10:08:35, 17.65s/it]

 81%|████████  | 8672/10740 [43:01:12<10:52:53, 18.94s/it]

 81%|████████  | 8673/10740 [43:01:23<9:33:11, 16.64s/it]

 81%|████████  | 8674/10740 [43:01:40<9:34:17, 16.68s/it]


 81%|████████  | 8676/10740 [43:02:15<9:56:25, 17.34s/it]
[2024-04-03 14:15:58,248] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4162, 'learning_rate': 1.8755790792690762e-07, 'rewards/chosen': -2.4383952617645264, 'rewards/rejected': -3.5276613235473633, 'rewards/accuracies': 0.75, 'rewards/margins': 1.089266300201416, 'policy_logps/rejected': -409.1927185058594, 'policy_logps/chosen': -389.7403564453125, 'referece_logps/rejected': -373.91607666015625, 'referece_logps/chosen': -365.35638427734375, 'logits/rejected': -0.9146807789802551, 'logits/chosen': -0.8226139545440674, 'epoch': 4.85}

 81%|████████  | 8677/10740 [43:02:28<9:16:18, 16.18s/it]


 81%|████████  | 8679/10740 [43:02:54<8:17:51, 14.49s/it]
{'loss': 0.3471, 'learning_rate': 1.8703073352500475e-07, 'rewards/chosen': -1.7169933319091797, 'rewards/rejected': -3.8159339427948, 'rewards/accuracies': 1.0, 'rewards/margins': 2.09894061088562, 'policy_logps/rejected': -470.4111633300781, 'policy_logps/chosen': -279.95294189453125, 'referece_logps/rejected': -432.2518310546875, 'referece_logps/chosen': -262.7829895019531, 'logits/rejected': -0.3161751329898834, 'logits/chosen': -0.08071701228618622, 'epoch': 4.85}

 81%|████████  | 8680/10740 [43:03:14<9:11:13, 16.05s/it]

 81%|████████  | 8681/10740 [43:03:36<10:12:52, 17.86s/it]

 81%|████████  | 8682/10740 [43:03:57<10:45:22, 18.82s/it]


 81%|████████  | 8684/10740 [43:04:33<10:19:10, 18.07s/it]
{'loss': 0.344, 'learning_rate': 1.8615358856556872e-07, 'rewards/chosen': -1.4371730089187622, 'rewards/rejected': -4.269989967346191, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8328170776367188, 'policy_logps/rejected': -371.3148498535156, 'policy_logps/chosen': -303.17181396484375, 'referece_logps/rejected': -328.61492919921875, 'referece_logps/chosen': -288.8000793457031, 'logits/rejected': -0.7939196228981018, 'logits/chosen': -0.6536245942115784, 'epoch': 4.85}


 81%|████████  | 8686/10740 [43:05:09<10:22:10, 18.17s/it]
[2024-04-03 14:18:52,372] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.254, 'learning_rate': 1.8580324861944053e-07, 'rewards/chosen': -1.2943531274795532, 'rewards/rejected': -3.969939708709717, 'rewards/accuracies': 0.875, 'rewards/margins': 2.675586700439453, 'policy_logps/rejected': -464.3002014160156, 'policy_logps/chosen': -531.8580932617188, 'referece_logps/rejected': -424.6008605957031, 'referece_logps/chosen': -518.91455078125, 'logits/rejected': -0.12067413330078125, 'logits/chosen': -0.21295960247516632, 'epoch': 4.85}


 81%|████████  | 8688/10740 [43:05:49<11:02:00, 19.36s/it]
{'loss': 0.3435, 'learning_rate': 1.8545320488607107e-07, 'rewards/chosen': -1.7464996576309204, 'rewards/rejected': -3.651402473449707, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9049029350280762, 'policy_logps/rejected': -382.7863464355469, 'policy_logps/chosen': -433.864990234375, 'referece_logps/rejected': -346.2723083496094, 'referece_logps/chosen': -416.3999938964844, 'logits/rejected': -0.19975996017456055, 'logits/chosen': -0.3595677614212036, 'epoch': 4.85}

 81%|████████  | 8689/10740 [43:06:04<10:22:10, 18.20s/it]

 81%|████████  | 8690/10740 [43:06:22<10:15:41, 18.02s/it]
[2024-04-03 14:20:29,365] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8691/10740 [43:06:46<11:15:16, 19.77s/it]
[2024-04-03 14:20:43,481] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8692/10740 [43:07:00<10:17:00, 18.08s/it]

 81%|████████  | 8693/10740 [43:07:18<10:19:34, 18.16s/it]

 81%|████████  | 8694/10740 [43:07:35<10:11:08, 17.92s/it]

 81%|████████  | 8695/10740 [43:07:53<10:03:39, 17.71s/it]

 81%|████████  | 8696/10740 [43:08:12<10:21:36, 18.25s/it]

 81%|████████  | 8697/10740 [43:08:34<10:57:45, 19.32s/it]


 81%|████████  | 8699/10740 [43:09:11<10:36:14, 18.70s/it]

 81%|████████  | 8700/10740 [43:09:29<10:29:19, 18.51s/it]
{'loss': 0.3089, 'learning_rate': 1.83359170077794e-07, 'rewards/chosen': -1.8956552743911743, 'rewards/rejected': -5.256224155426025, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3605692386627197, 'policy_logps/rejected': -335.3096618652344, 'policy_logps/chosen': -308.59979248046875, 'referece_logps/rejected': -282.7474060058594, 'referece_logps/chosen': -289.64324951171875, 'logits/rejected': -0.4238698482513428, 'logits/chosen': -0.4650364816188812, 'epoch': 4.86}
[2024-04-03 14:23:33,731] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8701/10740 [43:09:50<10:54:11, 19.25s/it]
[2024-04-03 14:23:52,195] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8702/10740 [43:10:08<10:45:51, 19.01s/it]

 81%|████████  | 8703/10740 [43:10:28<10:53:16, 19.24s/it]

 81%|████████  | 8704/10740 [43:10:45<10:25:17, 18.43s/it]

 81%|████████  | 8705/10740 [43:10:56<9:15:46, 16.39s/it]

 81%|████████  | 8706/10740 [43:11:13<9:13:28, 16.33s/it]
[2024-04-03 14:25:15,986] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 81%|████████  | 8708/10740 [43:11:53<10:24:33, 18.44s/it]
[2024-04-03 14:25:37,014] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8709/10740 [43:12:10<10:02:33, 17.80s/it]
{'loss': 0.3432, 'learning_rate': 1.8179566150283852e-07, 'rewards/chosen': -1.0100774765014648, 'rewards/rejected': -3.396329164505005, 'rewards/accuracies': 1.0, 'rewards/margins': 2.386251449584961, 'policy_logps/rejected': -682.98583984375, 'policy_logps/chosen': -543.8998413085938, 'referece_logps/rejected': -649.0225219726562, 'referece_logps/chosen': -533.7991333007812, 'logits/rejected': -0.2026405930519104, 'logits/chosen': -0.47839871048927307, 'epoch': 4.87}

 81%|████████  | 8710/10740 [43:12:28<10:12:07, 18.09s/it]


 81%|████████  | 8712/10740 [43:13:01<9:31:36, 16.91s/it]
{'loss': 0.3004, 'learning_rate': 1.8127583092794106e-07, 'rewards/chosen': -1.975218415260315, 'rewards/rejected': -5.535687446594238, 'rewards/accuracies': 0.875, 'rewards/margins': 3.560469388961792, 'policy_logps/rejected': -325.91455078125, 'policy_logps/chosen': -356.28326416015625, 'referece_logps/rejected': -270.5576477050781, 'referece_logps/chosen': -336.5310974121094, 'logits/rejected': -0.9043796062469482, 'logits/chosen': -0.9271299242973328, 'epoch': 4.87}

 81%|████████  | 8713/10740 [43:13:22<10:13:23, 18.16s/it]

 81%|████████  | 8714/10740 [43:13:37<9:35:18, 17.04s/it]

 81%|████████  | 8715/10740 [43:13:56<9:55:59, 17.66s/it]

 81%|████████  | 8716/10740 [43:14:08<9:00:09, 16.01s/it]

 81%|████████  | 8717/10740 [43:14:28<9:44:17, 17.33s/it]

 81%|████████  | 8718/10740 [43:14:47<9:53:29, 17.61s/it]

 81%|████████  | 8719/10740 [43:15:08<10:30:10, 18.71s/it]


 81%|████████  | 8721/10740 [43:15:39<9:31:10, 16.97s/it]
{'loss': 0.3844, 'learning_rate': 1.797203620107728e-07, 'rewards/chosen': -2.295506000518799, 'rewards/rejected': -2.695714235305786, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4002082645893097, 'policy_logps/rejected': -319.4747314453125, 'policy_logps/chosen': -334.8191223144531, 'referece_logps/rejected': -292.517578125, 'referece_logps/chosen': -311.8641052246094, 'logits/rejected': -1.3140811920166016, 'logits/chosen': -1.345508098602295, 'epoch': 4.87}


 81%|████████  | 8723/10740 [43:16:22<10:35:45, 18.91s/it]
{'loss': 0.3623, 'learning_rate': 1.793755225192053e-07, 'rewards/chosen': -1.5754796266555786, 'rewards/rejected': -4.326488018035889, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7510085105895996, 'policy_logps/rejected': -419.8402099609375, 'policy_logps/chosen': -319.2214660644531, 'referece_logps/rejected': -376.5753173828125, 'referece_logps/chosen': -303.4666748046875, 'logits/rejected': -0.6383965611457825, 'logits/chosen': -0.6835429668426514, 'epoch': 4.87}

 81%|████████  | 8724/10740 [43:16:38<10:14:07, 18.28s/it]
[2024-04-03 14:30:44,814] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8725/10740 [43:17:01<10:58:49, 19.62s/it]
[2024-04-03 14:31:00,125] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 8726/10740 [43:17:16<10:15:09, 18.33s/it]

 81%|████████▏ | 8727/10740 [43:17:37<10:39:25, 19.06s/it]


 81%|████████▏ | 8729/10740 [43:18:20<11:15:31, 20.15s/it]
{'loss': 0.4585, 'learning_rate': 1.7834279585315225e-07, 'rewards/chosen': -0.8242889642715454, 'rewards/rejected': -2.786168098449707, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9618792533874512, 'policy_logps/rejected': -399.9319152832031, 'policy_logps/chosen': -480.9496154785156, 'referece_logps/rejected': -372.07025146484375, 'referece_logps/chosen': -472.7066955566406, 'logits/rejected': -0.2452230155467987, 'logits/chosen': 0.06420119106769562, 'epoch': 4.88}

 81%|████████▏ | 8730/10740 [43:18:40<11:12:56, 20.09s/it]

 81%|████████▏ | 8731/10740 [43:19:01<11:18:46, 20.27s/it]

 81%|████████▏ | 8732/10740 [43:19:13<9:57:03, 17.84s/it]

 81%|████████▏ | 8733/10740 [43:19:26<9:12:30, 16.52s/it]

 81%|████████▏ | 8734/10740 [43:19:41<8:54:02, 15.97s/it]

 81%|████████▏ | 8735/10740 [43:19:55<8:30:46, 15.29s/it]

 81%|████████▏ | 8736/10740 [43:20:07<8:01:13, 14.41s/it]

 81%|████████▏ | 8737/10740 [43:20:27<8:54:47, 16.02s/it]

 81%|████████▏ | 8738/10740 [43:20:47<9:33:58, 17.20s/it]
[2024-04-03 14:34:52,664] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████▏ | 8739/10740 [43:21:09<10:23:33, 18.70s/it]
[2024-04-03 14:35:12,976] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████▏ | 8740/10740 [43:21:29<10:39:23, 19.18s/it]

 81%|████████▏ | 8741/10740 [43:21:48<10:31:10, 18.94s/it]

 81%|████████▏ | 8742/10740 [43:22:06<10:26:00, 18.80s/it]

 81%|████████▏ | 8743/10740 [43:22:24<10:13:10, 18.42s/it]

 81%|████████▏ | 8744/10740 [43:22:40<9:55:53, 17.91s/it]

 81%|████████▏ | 8745/10740 [43:22:57<9:41:51, 17.50s/it]

 81%|████████▏ | 8746/10740 [43:23:13<9:31:09, 17.19s/it]

 81%|████████▏ | 8747/10740 [43:23:30<9:22:52, 16.95s/it]

 81%|████████▏ | 8748/10740 [43:23:50<9:52:36, 17.85s/it]

 81%|████████▏ | 8749/10740 [43:24:09<10:08:13, 18.33s/it]

 81%|████████▏ | 8750/10740 [43:24:26<9:49:29, 17.77s/it]
[2024-04-03 14:38:32,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████▏ | 8751/10740 [43:24:49<10:44:56, 19.46s/it]

 81%|████████▏ | 8752/10740 [43:25:07<10:26:16, 18.90s/it]

 81%|████████▏ | 8753/10740 [43:25:22<9:49:52, 17.81s/it]

 82%|████████▏ | 8754/10740 [43:25:39<9:45:25, 17.69s/it]

 82%|████████▏ | 8755/10740 [43:25:59<10:05:20, 18.30s/it]

 82%|████████▏ | 8756/10740 [43:26:15<9:38:57, 17.51s/it]

 82%|████████▏ | 8757/10740 [43:26:36<10:16:55, 18.67s/it]

 82%|████████▏ | 8758/10740 [43:26:56<10:27:31, 19.00s/it]
[2024-04-03 14:41:03,254] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8759/10740 [43:27:20<11:14:00, 20.41s/it]
[2024-04-03 14:41:20,218] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8760/10740 [43:27:36<10:39:30, 19.38s/it]

 82%|████████▏ | 8761/10740 [43:27:56<10:40:58, 19.43s/it]

 82%|████████▏ | 8762/10740 [43:28:08<9:30:33, 17.31s/it]

 82%|████████▏ | 8763/10740 [43:28:23<9:05:22, 16.55s/it]
[2024-04-03 14:42:27,348] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8764/10740 [43:28:44<9:43:28, 17.72s/it]

 82%|████████▏ | 8765/10740 [43:29:05<10:20:10, 18.84s/it]

 82%|████████▏ | 8766/10740 [43:29:18<9:21:00, 17.05s/it]

 82%|████████▏ | 8767/10740 [43:29:36<9:26:05, 17.22s/it]

 82%|████████▏ | 8768/10740 [43:29:52<9:21:40, 17.09s/it]

 82%|████████▏ | 8769/10740 [43:30:12<9:48:58, 17.93s/it]

 82%|████████▏ | 8770/10740 [43:30:33<10:18:58, 18.85s/it]

 82%|████████▏ | 8771/10740 [43:30:54<10:32:54, 19.29s/it]

 82%|████████▏ | 8772/10740 [43:31:07<9:37:28, 17.61s/it]

 82%|████████▏ | 8773/10740 [43:31:25<9:41:43, 17.74s/it]

 82%|████████▏ | 8774/10740 [43:31:44<9:48:01, 17.95s/it]

 82%|████████▏ | 8775/10740 [43:31:56<8:50:53, 16.21s/it]

 82%|████████▏ | 8776/10740 [43:32:16<9:25:59, 17.29s/it]

 82%|████████▏ | 8777/10740 [43:32:34<9:35:55, 17.60s/it]

 82%|████████▏ | 8778/10740 [43:32:51<9:26:43, 17.33s/it]

 82%|████████▏ | 8779/10740 [43:33:10<9:49:15, 18.03s/it]

 82%|████████▏ | 8780/10740 [43:33:29<9:56:49, 18.27s/it]

 82%|████████▏ | 8781/10740 [43:33:49<10:10:06, 18.69s/it]

 82%|████████▏ | 8782/10740 [43:34:05<9:47:29, 18.00s/it]

 82%|████████▏ | 8783/10740 [43:34:25<10:00:54, 18.42s/it]

 82%|████████▏ | 8784/10740 [43:34:45<10:21:54, 19.08s/it]

 82%|████████▏ | 8785/10740 [43:35:05<10:26:54, 19.24s/it]

 82%|████████▏ | 8786/10740 [43:35:18<9:27:20, 17.42s/it]

 82%|████████▏ | 8787/10740 [43:35:31<8:45:01, 16.13s/it]

 82%|████████▏ | 8788/10740 [43:35:49<9:03:18, 16.70s/it]

 82%|████████▏ | 8789/10740 [43:36:02<8:24:06, 15.50s/it]

 82%|████████▏ | 8790/10740 [43:36:13<7:39:27, 14.14s/it]

 82%|████████▏ | 8791/10740 [43:36:33<8:41:16, 16.05s/it]

 82%|████████▏ | 8792/10740 [43:36:54<9:21:10, 17.28s/it]

 82%|████████▏ | 8793/10740 [43:37:09<9:05:54, 16.82s/it]

 82%|████████▏ | 8794/10740 [43:37:29<9:32:20, 17.65s/it]


 82%|████████▏ | 8796/10740 [43:38:09<10:15:03, 18.98s/it]
{'loss': 0.4129, 'learning_rate': 1.6699420958050414e-07, 'rewards/chosen': -1.7454875707626343, 'rewards/rejected': -3.6640524864196777, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9185651540756226, 'policy_logps/rejected': -396.54705810546875, 'policy_logps/chosen': -454.0983581542969, 'referece_logps/rejected': -359.90655517578125, 'referece_logps/chosen': -436.6434631347656, 'logits/rejected': -0.5177892446517944, 'logits/chosen': -0.5663347840309143, 'epoch': 4.91}

 82%|████████▏ | 8797/10740 [43:38:30<10:29:47, 19.45s/it]

 82%|████████▏ | 8798/10740 [43:38:48<10:13:52, 18.97s/it]

 82%|████████▏ | 8799/10740 [43:39:05<9:58:53, 18.51s/it]

 82%|████████▏ | 8800/10740 [43:39:20<9:23:18, 17.42s/it]

 82%|████████▏ | 8801/10740 [43:39:38<9:26:22, 17.53s/it]

 82%|████████▏ | 8802/10740 [43:39:58<9:47:51, 18.20s/it]

 82%|████████▏ | 8803/10740 [43:40:09<8:45:43, 16.28s/it]

 82%|████████▏ | 8804/10740 [43:40:27<8:54:18, 16.56s/it]

 82%|████████▏ | 8805/10740 [43:40:48<9:40:51, 18.01s/it]

 82%|████████▏ | 8806/10740 [43:41:08<10:00:44, 18.64s/it]

 82%|████████▏ | 8807/10740 [43:41:25<9:40:33, 18.02s/it]

 82%|████████▏ | 8808/10740 [43:41:44<9:54:58, 18.48s/it]

 82%|████████▏ | 8809/10740 [43:42:05<10:14:21, 19.09s/it]

 82%|████████▏ | 8810/10740 [43:42:24<10:20:13, 19.28s/it]

 82%|████████▏ | 8811/10740 [43:42:41<9:50:15, 18.36s/it]

 82%|████████▏ | 8812/10740 [43:42:58<9:39:37, 18.04s/it]
[2024-04-03 14:57:03,116] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8813/10740 [43:43:19<10:12:31, 19.07s/it]
[2024-04-03 14:57:22,991] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8814/10740 [43:43:39<10:19:51, 19.31s/it]

 82%|████████▏ | 8815/10740 [43:44:00<10:30:41, 19.66s/it]

 82%|████████▏ | 8816/10740 [43:44:13<9:28:37, 17.73s/it]


 82%|████████▏ | 8818/10740 [43:44:45<8:53:40, 16.66s/it]

 82%|████████▏ | 8819/10740 [43:45:03<9:03:41, 16.98s/it]

 82%|████████▏ | 8820/10740 [43:45:25<9:52:17, 18.51s/it]

 82%|████████▏ | 8821/10740 [43:45:37<8:49:21, 16.55s/it]

 82%|████████▏ | 8822/10740 [43:45:54<8:56:00, 16.77s/it]
[2024-04-03 14:59:37,617] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8823/10740 [43:46:13<9:22:33, 17.61s/it]

 82%|████████▏ | 8824/10740 [43:46:26<8:33:03, 16.07s/it]

 82%|████████▏ | 8825/10740 [43:46:48<9:26:03, 17.74s/it]
[2024-04-03 15:00:31,286] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8826/10740 [43:47:10<10:06:19, 19.01s/it]
[2024-04-03 15:00:53,259] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8827/10740 [43:47:30<10:19:57, 19.44s/it]

 82%|████████▏ | 8828/10740 [43:47:51<10:38:50, 20.05s/it]

 82%|████████▏ | 8829/10740 [43:48:12<10:39:31, 20.08s/it]

 82%|████████▏ | 8830/10740 [43:48:33<10:51:48, 20.48s/it]

 82%|████████▏ | 8831/10740 [43:48:51<10:26:48, 19.70s/it]

 82%|████████▏ | 8832/10740 [43:49:12<10:44:27, 20.27s/it]
[2024-04-03 15:02:56,211] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8833/10740 [43:49:31<10:32:04, 19.89s/it]

 82%|████████▏ | 8834/10740 [43:49:51<10:29:01, 19.80s/it]

 82%|████████▏ | 8835/10740 [43:50:07<9:48:55, 18.55s/it]

 82%|████████▏ | 8836/10740 [43:50:19<8:50:38, 16.72s/it]

 82%|████████▏ | 8837/10740 [43:50:35<8:39:56, 16.39s/it]

 82%|████████▏ | 8838/10740 [43:50:56<9:21:53, 17.73s/it]

 82%|████████▏ | 8839/10740 [43:51:11<8:58:51, 17.01s/it]

 82%|████████▏ | 8840/10740 [43:51:28<8:57:46, 16.98s/it]

 82%|████████▏ | 8841/10740 [43:51:45<9:00:58, 17.09s/it]

 82%|████████▏ | 8842/10740 [43:52:04<9:15:23, 17.56s/it]

 82%|████████▏ | 8843/10740 [43:52:25<9:49:00, 18.63s/it]
{'loss': 0.3695, 'learning_rate': 1.5923595605022057e-07, 'rewards/chosen': -2.3763368129730225, 'rewards/rejected': -3.1529910564422607, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7766542434692383, 'policy_logps/rejected': -383.5370178222656, 'policy_logps/chosen': -392.39154052734375, 'referece_logps/rejected': -352.0071105957031, 'referece_logps/chosen': -368.6281433105469, 'logits/rejected': -0.45106610655784607, 'logits/chosen': -0.5076426267623901, 'epoch': 4.94}


 82%|████████▏ | 8845/10740 [43:52:50<8:05:52, 15.38s/it]

 82%|████████▏ | 8846/10740 [43:53:07<8:21:24, 15.88s/it]

 82%|████████▏ | 8847/10740 [43:53:24<8:27:39, 16.09s/it]

 82%|████████▏ | 8848/10740 [43:53:46<9:24:36, 17.91s/it]
[2024-04-03 15:07:29,459] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 8849/10740 [43:54:07<9:56:44, 18.93s/it]

 82%|████████▏ | 8850/10740 [43:54:19<8:51:54, 16.89s/it]
{'loss': 0.4015, 'learning_rate': 1.5809488659306446e-07, 'rewards/chosen': -2.1277916431427, 'rewards/rejected': -3.4965317249298096, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3687399625778198, 'policy_logps/rejected': -388.2941589355469, 'policy_logps/chosen': -396.056884765625, 'referece_logps/rejected': -353.3288269042969, 'referece_logps/chosen': -374.7790222167969, 'logits/rejected': -1.1471635103225708, 'logits/chosen': -1.1558860540390015, 'epoch': 4.94}

 82%|████████▏ | 8851/10740 [43:54:33<8:18:52, 15.85s/it]


 82%|████████▏ | 8853/10740 [43:55:07<8:44:09, 16.67s/it]

 82%|████████▏ | 8854/10740 [43:55:20<8:07:58, 15.52s/it]
{'loss': 0.3337, 'learning_rate': 1.5744453119052692e-07, 'rewards/chosen': -2.1408653259277344, 'rewards/rejected': -3.7159342765808105, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5750689506530762, 'policy_logps/rejected': -274.1735534667969, 'policy_logps/chosen': -378.27655029296875, 'referece_logps/rejected': -237.0142059326172, 'referece_logps/chosen': -356.867919921875, 'logits/rejected': -0.8617995381355286, 'logits/chosen': -0.9536606073379517, 'epoch': 4.95}


 82%|████████▏ | 8856/10740 [43:56:00<9:21:22, 17.88s/it]

 82%|████████▏ | 8857/10740 [43:56:16<9:08:43, 17.48s/it]
{'loss': 0.3572, 'learning_rate': 1.5695756922794701e-07, 'rewards/chosen': -0.8224452137947083, 'rewards/rejected': -3.8643932342529297, 'rewards/accuracies': 0.875, 'rewards/margins': 3.041947603225708, 'policy_logps/rejected': -476.82537841796875, 'policy_logps/chosen': -450.5761413574219, 'referece_logps/rejected': -438.18145751953125, 'referece_logps/chosen': -442.3516540527344, 'logits/rejected': 0.47247251868247986, 'logits/chosen': 0.4778677523136139, 'epoch': 4.95}


 82%|████████▏ | 8859/10740 [43:56:54<9:33:40, 18.30s/it]

 82%|████████▏ | 8860/10740 [43:57:08<8:48:10, 16.86s/it]

 83%|████████▎ | 8861/10740 [43:57:29<9:29:59, 18.20s/it]
{'loss': 0.2758, 'learning_rate': 1.5630936015479512e-07, 'rewards/chosen': -1.5501500368118286, 'rewards/rejected': -4.044980049133301, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4948298931121826, 'policy_logps/rejected': -450.4989318847656, 'policy_logps/chosen': -461.7799072265625, 'referece_logps/rejected': -410.04913330078125, 'referece_logps/chosen': -446.2784118652344, 'logits/rejected': 0.09847520291805267, 'logits/chosen': 0.20465226471424103, 'epoch': 4.95}


 83%|████████▎ | 8863/10740 [43:58:04<9:29:16, 18.20s/it]
{'loss': 0.2704, 'learning_rate': 1.5598571597367737e-07, 'rewards/chosen': -1.8302340507507324, 'rewards/rejected': -3.4629104137420654, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6326760053634644, 'policy_logps/rejected': -496.4321594238281, 'policy_logps/chosen': -463.2989196777344, 'referece_logps/rejected': -461.8031005859375, 'referece_logps/chosen': -444.99658203125, 'logits/rejected': 0.5318104028701782, 'logits/chosen': 0.46167421340942383, 'epoch': 4.95}


 83%|████████▎ | 8865/10740 [43:58:36<8:55:56, 17.15s/it]

 83%|████████▎ | 8866/10740 [43:58:56<9:25:44, 18.11s/it]

 83%|████████▎ | 8867/10740 [43:59:19<10:02:43, 19.31s/it]

 83%|████████▎ | 8868/10740 [43:59:32<9:08:14, 17.57s/it]

 83%|████████▎ | 8869/10740 [43:59:48<8:50:58, 17.03s/it]

 83%|████████▎ | 8870/10740 [44:00:08<9:15:48, 17.83s/it]

 83%|████████▎ | 8871/10740 [44:00:28<9:39:24, 18.60s/it]

 83%|████████▎ | 8872/10740 [44:00:48<9:49:40, 18.94s/it]

 83%|████████▎ | 8873/10740 [44:01:08<9:59:08, 19.25s/it]

 83%|████████▎ | 8874/10740 [44:01:27<10:03:56, 19.42s/it]

 83%|████████▎ | 8875/10740 [44:01:49<10:19:22, 19.93s/it]

 83%|████████▎ | 8876/10740 [44:02:08<10:15:21, 19.81s/it]
{'loss': 0.4512, 'learning_rate': 1.538895186513337e-07, 'rewards/chosen': -1.4100184440612793, 'rewards/rejected': -3.727083444595337, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3170647621154785, 'policy_logps/rejected': -505.7662658691406, 'policy_logps/chosen': -418.9062194824219, 'referece_logps/rejected': -468.49542236328125, 'referece_logps/chosen': -404.8060607910156, 'logits/rejected': -0.5323649048805237, 'logits/chosen': -0.44427159428596497, 'epoch': 4.96}


 83%|████████▎ | 8878/10740 [44:02:36<8:47:40, 17.00s/it]

 83%|████████▎ | 8879/10740 [44:02:51<8:28:19, 16.39s/it]

 83%|████████▎ | 8880/10740 [44:03:07<8:24:55, 16.29s/it]

 83%|████████▎ | 8881/10740 [44:03:30<9:26:23, 18.28s/it]

 83%|████████▎ | 8882/10740 [44:03:50<9:47:50, 18.98s/it]
{'loss': 0.3412, 'learning_rate': 1.5292642749734486e-07, 'rewards/chosen': -2.6428334712982178, 'rewards/rejected': -3.4789185523986816, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8360849022865295, 'policy_logps/rejected': -545.3931884765625, 'policy_logps/chosen': -469.3958740234375, 'referece_logps/rejected': -510.6039733886719, 'referece_logps/chosen': -442.967529296875, 'logits/rejected': -0.7177228331565857, 'logits/chosen': -0.6596583724021912, 'epoch': 4.96}


 83%|████████▎ | 8884/10740 [44:04:22<9:09:34, 17.77s/it]

 83%|████████▎ | 8885/10740 [44:04:37<8:36:07, 16.69s/it]

 83%|████████▎ | 8886/10740 [44:04:56<9:01:11, 17.51s/it]
{'loss': 0.4545, 'learning_rate': 1.5228590740170289e-07, 'rewards/chosen': -2.3299734592437744, 'rewards/rejected': -2.8290822505950928, 'rewards/accuracies': 0.625, 'rewards/margins': 0.49910861253738403, 'policy_logps/rejected': -333.7412109375, 'policy_logps/chosen': -340.309814453125, 'referece_logps/rejected': -305.45037841796875, 'referece_logps/chosen': -317.01007080078125, 'logits/rejected': -0.6327857375144958, 'logits/chosen': -0.6559864282608032, 'epoch': 4.96}

 83%|████████▎ | 8887/10740 [44:05:13<8:57:38, 17.41s/it]


 83%|████████▎ | 8889/10740 [44:05:49<8:59:24, 17.48s/it]

 83%|████████▎ | 8890/10740 [44:06:06<8:58:39, 17.47s/it]
{'loss': 0.4479, 'learning_rate': 1.5164662093273172e-07, 'rewards/chosen': -2.6274831295013428, 'rewards/rejected': -3.343538522720337, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7160555124282837, 'policy_logps/rejected': -370.1964416503906, 'policy_logps/chosen': -376.23248291015625, 'referece_logps/rejected': -336.7611083984375, 'referece_logps/chosen': -349.9576721191406, 'logits/rejected': -0.7196494340896606, 'logits/chosen': -0.7773014307022095, 'epoch': 4.97}

 83%|████████▎ | 8891/10740 [44:06:22<8:36:34, 16.76s/it]

 83%|████████▎ | 8892/10740 [44:06:42<9:06:43, 17.75s/it]


 83%|████████▎ | 8894/10740 [44:07:13<8:35:04, 16.74s/it]

 83%|████████▎ | 8895/10740 [44:07:32<8:57:36, 17.48s/it]
{'loss': 0.4375, 'learning_rate': 1.508492490511285e-07, 'rewards/chosen': -1.2696532011032104, 'rewards/rejected': -3.2337121963500977, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9640591144561768, 'policy_logps/rejected': -405.0541076660156, 'policy_logps/chosen': -377.1022033691406, 'referece_logps/rejected': -372.71697998046875, 'referece_logps/chosen': -364.40570068359375, 'logits/rejected': 0.2891645133495331, 'logits/chosen': 0.35063880681991577, 'epoch': 4.97}

 83%|████████▎ | 8896/10740 [44:07:52<9:17:55, 18.15s/it]


 83%|████████▎ | 8898/10740 [44:08:31<9:42:45, 18.98s/it]

 83%|████████▎ | 8899/10740 [44:08:49<9:30:12, 18.58s/it]

 83%|████████▎ | 8900/10740 [44:09:07<9:24:31, 18.41s/it]

 83%|████████▎ | 8901/10740 [44:09:26<9:30:38, 18.62s/it]

 83%|████████▎ | 8902/10740 [44:09:42<9:10:43, 17.98s/it]

 83%|████████▎ | 8903/10740 [44:09:55<8:21:00, 16.36s/it]

 83%|████████▎ | 8904/10740 [44:10:06<7:32:52, 14.80s/it]

 83%|████████▎ | 8905/10740 [44:10:26<8:18:52, 16.31s/it]

 83%|████████▎ | 8906/10740 [44:10:41<8:08:42, 15.99s/it]

 83%|████████▎ | 8907/10740 [44:11:01<8:43:03, 17.12s/it]

 83%|████████▎ | 8908/10740 [44:11:18<8:41:43, 17.09s/it]

 83%|████████▎ | 8909/10740 [44:11:35<8:41:24, 17.09s/it]

 83%|████████▎ | 8910/10740 [44:11:55<9:07:59, 17.97s/it]
{'loss': 0.4287, 'learning_rate': 1.4846872548603784e-07, 'rewards/chosen': -1.8955333232879639, 'rewards/rejected': -3.62493896484375, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7294059991836548, 'policy_logps/rejected': -407.6519775390625, 'policy_logps/chosen': -381.1578369140625, 'referece_logps/rejected': -371.4025573730469, 'referece_logps/chosen': -362.2024841308594, 'logits/rejected': 0.382511705160141, 'logits/chosen': 0.254336416721344, 'epoch': 4.98}


 83%|████████▎ | 8912/10740 [44:12:29<9:06:29, 17.94s/it]
{'loss': 0.2907, 'learning_rate': 1.48152637913194e-07, 'rewards/chosen': -1.6524724960327148, 'rewards/rejected': -3.9853837490081787, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3329110145568848, 'policy_logps/rejected': -505.7198791503906, 'policy_logps/chosen': -390.5357666015625, 'referece_logps/rejected': -465.86602783203125, 'referece_logps/chosen': -374.0110778808594, 'logits/rejected': 0.32229217886924744, 'logits/chosen': 0.3175857663154602, 'epoch': 4.98}

 83%|████████▎ | 8913/10740 [44:12:46<8:52:21, 17.48s/it]


 83%|████████▎ | 8915/10740 [44:13:25<9:33:15, 18.85s/it]

 83%|████████▎ | 8916/10740 [44:13:39<8:49:19, 17.41s/it]
{'loss': 0.3732, 'learning_rate': 1.4752139261364872e-07, 'rewards/chosen': -2.2674407958984375, 'rewards/rejected': -3.423253297805786, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1558126211166382, 'policy_logps/rejected': -301.1405029296875, 'policy_logps/chosen': -321.6924133300781, 'referece_logps/rejected': -266.9079895019531, 'referece_logps/chosen': -299.01800537109375, 'logits/rejected': -1.1316944360733032, 'logits/chosen': -1.1162868738174438, 'epoch': 4.98}


 83%|████████▎ | 8918/10740 [44:14:11<8:37:07, 17.03s/it]

 83%|████████▎ | 8919/10740 [44:14:31<8:59:51, 17.79s/it]

 83%|████████▎ | 8920/10740 [44:14:47<8:39:03, 17.11s/it]

 83%|████████▎ | 8921/10740 [44:15:03<8:33:04, 16.92s/it]

 83%|████████▎ | 8922/10740 [44:15:17<8:03:38, 15.96s/it]

 83%|████████▎ | 8923/10740 [44:15:35<8:28:02, 16.78s/it]
{'loss': 0.3046, 'learning_rate': 1.464196989889588e-07, 'rewards/chosen': -1.9017430543899536, 'rewards/rejected': -3.2762176990509033, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3744747638702393, 'policy_logps/rejected': -469.9247131347656, 'policy_logps/chosen': -541.8842163085938, 'referece_logps/rejected': -437.1625061035156, 'referece_logps/chosen': -522.8667602539062, 'logits/rejected': -0.09676751494407654, 'logits/chosen': -0.08502436429262161, 'epoch': 4.98}


 83%|████████▎ | 8925/10740 [44:16:10<8:35:14, 17.03s/it]

 83%|████████▎ | 8926/10740 [44:16:30<8:56:07, 17.73s/it]

 83%|████████▎ | 8927/10740 [44:16:51<9:29:15, 18.84s/it]

 83%|████████▎ | 8928/10740 [44:17:09<9:19:12, 18.52s/it]

 83%|████████▎ | 8929/10740 [44:17:25<8:56:54, 17.79s/it]
{'loss': 0.3355, 'learning_rate': 1.4547841775471115e-07, 'rewards/chosen': -1.8608299493789673, 'rewards/rejected': -2.899850368499756, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0390206575393677, 'policy_logps/rejected': -430.94775390625, 'policy_logps/chosen': -355.6590576171875, 'referece_logps/rejected': -401.94927978515625, 'referece_logps/chosen': -337.0507507324219, 'logits/rejected': -0.2559678256511688, 'logits/chosen': -0.30535489320755005, 'epoch': 4.99}

 83%|████████▎ | 8930/10740 [44:17:44<9:10:58, 18.26s/it]


 83%|████████▎ | 8932/10740 [44:18:19<8:59:21, 17.90s/it]
{'loss': 0.3402, 'learning_rate': 1.450088261762933e-07, 'rewards/chosen': -1.434714436531067, 'rewards/rejected': -3.24843168258667, 'rewards/accuracies': 0.875, 'rewards/margins': 1.813717007637024, 'policy_logps/rejected': -370.7869873046875, 'policy_logps/chosen': -374.28216552734375, 'referece_logps/rejected': -338.30267333984375, 'referece_logps/chosen': -359.93499755859375, 'logits/rejected': -1.2180256843566895, 'logits/chosen': -1.2561372518539429, 'epoch': 4.99}

 83%|████████▎ | 8933/10740 [44:18:34<8:35:48, 17.13s/it]


 83%|████████▎ | 8935/10740 [44:19:11<8:56:37, 17.84s/it]

 83%|████████▎ | 8936/10740 [44:19:23<8:00:59, 16.00s/it]

 83%|████████▎ | 8937/10740 [44:19:37<7:42:32, 15.39s/it]
{'loss': 0.4186, 'learning_rate': 1.4422772900594838e-07, 'rewards/chosen': -1.6490825414657593, 'rewards/rejected': -3.0315515995025635, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3824691772460938, 'policy_logps/rejected': -365.26153564453125, 'policy_logps/chosen': -334.7897644042969, 'referece_logps/rejected': -334.946044921875, 'referece_logps/chosen': -318.2989501953125, 'logits/rejected': -1.2359474897384644, 'logits/chosen': -1.304866075515747, 'epoch': 4.99}

 83%|████████▎ | 8938/10740 [44:19:54<8:03:05, 16.09s/it]


 83%|████████▎ | 8940/10740 [44:20:32<8:39:03, 17.30s/it]
{'loss': 0.2378, 'learning_rate': 1.437600046051285e-07, 'rewards/chosen': -1.1257948875427246, 'rewards/rejected': -3.4181876182556152, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2923927307128906, 'policy_logps/rejected': -404.61663818359375, 'policy_logps/chosen': -269.6935729980469, 'referece_logps/rejected': -370.43475341796875, 'referece_logps/chosen': -258.43560791015625, 'logits/rejected': 0.11358965188264847, 'logits/chosen': 0.11051252484321594, 'epoch': 4.99}


 83%|████████▎ | 8942/10740 [44:21:04<8:28:21, 16.96s/it]
{'loss': 0.4082, 'learning_rate': 1.4344857769991892e-07, 'rewards/chosen': -2.2630884647369385, 'rewards/rejected': -4.3963117599487305, 'rewards/accuracies': 0.75, 'rewards/margins': 2.133223056793213, 'policy_logps/rejected': -462.56201171875, 'policy_logps/chosen': -402.32568359375, 'referece_logps/rejected': -418.598876953125, 'referece_logps/chosen': -379.69476318359375, 'logits/rejected': -0.4887830317020416, 'logits/chosen': -0.5243926048278809, 'epoch': 5.0}

 83%|████████▎ | 8943/10740 [44:21:22<8:42:55, 17.46s/it]


 83%|████████▎ | 8945/10740 [44:22:05<9:42:58, 19.49s/it]

 83%|████████▎ | 8946/10740 [44:22:23<9:32:03, 19.13s/it]

 83%|████████▎ | 8947/10740 [44:22:35<8:27:07, 16.97s/it]
{'loss': 0.4487, 'learning_rate': 1.4267137402984709e-07, 'rewards/chosen': -1.866189956665039, 'rewards/rejected': -3.209768056869507, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3435779809951782, 'policy_logps/rejected': -336.53607177734375, 'policy_logps/chosen': -364.3255615234375, 'referece_logps/rejected': -304.4383850097656, 'referece_logps/chosen': -345.6637268066406, 'logits/rejected': -0.24776864051818848, 'logits/chosen': -0.17304891347885132, 'epoch': 5.0}


 83%|████████▎ | 8949/10740 [44:23:07<8:04:06, 16.22s/it]
{'loss': 0.429, 'learning_rate': 1.423610382958913e-07, 'rewards/chosen': -2.084601402282715, 'rewards/rejected': -3.059490203857422, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9748884439468384, 'policy_logps/rejected': -425.91363525390625, 'policy_logps/chosen': -449.0652770996094, 'referece_logps/rejected': -395.3187561035156, 'referece_logps/chosen': -428.21923828125, 'logits/rejected': -0.7056071758270264, 'logits/chosen': -0.8220059275627136, 'epoch': 5.0}


 83%|████████▎ | 8951/10740 [44:23:38<7:56:47, 15.99s/it]

 83%|████████▎ | 8952/10740 [44:23:51<7:34:22, 15.25s/it]

 83%|████████▎ | 8953/10740 [44:24:11<8:13:55, 16.58s/it]
{'loss': 0.3967, 'learning_rate': 1.4174130299315056e-07, 'rewards/chosen': -1.4499551057815552, 'rewards/rejected': -2.0994873046875, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6495320796966553, 'policy_logps/rejected': -384.20526123046875, 'policy_logps/chosen': -420.3713073730469, 'referece_logps/rejected': -363.2104187011719, 'referece_logps/chosen': -405.87176513671875, 'logits/rejected': -0.6082959771156311, 'logits/chosen': -0.5708765983581543, 'epoch': 5.0}

 83%|████████▎ | 8954/10740 [44:24:22<7:26:00, 14.98s/it]

 83%|████████▎ | 8955/10740 [44:24:42<8:10:54, 16.50s/it]


 83%|████████▎ | 8957/10740 [44:25:22<8:55:29, 18.02s/it]
{'loss': 0.3721, 'learning_rate': 1.411228166620011e-07, 'rewards/chosen': -2.058241605758667, 'rewards/rejected': -4.194951057434082, 'rewards/accuracies': 0.875, 'rewards/margins': 2.136709690093994, 'policy_logps/rejected': -411.96405029296875, 'policy_logps/chosen': -429.2992248535156, 'referece_logps/rejected': -370.0145263671875, 'referece_logps/chosen': -408.7168273925781, 'logits/rejected': 0.15710361301898956, 'logits/chosen': 0.25073519349098206, 'epoch': 5.0}

 83%|████████▎ | 8958/10740 [44:25:41<9:04:26, 18.33s/it]

 83%|████████▎ | 8959/10740 [44:25:59<9:03:14, 18.30s/it]


 83%|████████▎ | 8961/10740 [44:26:38<9:27:18, 19.13s/it]
{'loss': 0.3436, 'learning_rate': 1.4050558020248737e-07, 'rewards/chosen': -2.0562641620635986, 'rewards/rejected': -3.45139741897583, 'rewards/accuracies': 0.625, 'rewards/margins': 1.395133137702942, 'policy_logps/rejected': -285.2404479980469, 'policy_logps/chosen': -303.5907287597656, 'referece_logps/rejected': -250.7264404296875, 'referece_logps/chosen': -283.0281066894531, 'logits/rejected': -0.98701411485672, 'logits/chosen': -1.0239602327346802, 'epoch': 5.01}


 83%|████████▎ | 8963/10740 [44:27:10<8:31:33, 17.27s/it]
{'loss': 0.3961, 'learning_rate': 1.401974309553613e-07, 'rewards/chosen': -1.962952733039856, 'rewards/rejected': -2.8568811416625977, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8939282894134521, 'policy_logps/rejected': -377.8242492675781, 'policy_logps/chosen': -389.2288513183594, 'referece_logps/rejected': -349.25543212890625, 'referece_logps/chosen': -369.59930419921875, 'logits/rejected': -0.3333626389503479, 'logits/chosen': -0.409890353679657, 'epoch': 5.01}


 83%|████████▎ | 8965/10740 [44:27:32<6:58:37, 14.15s/it]

 83%|████████▎ | 8966/10740 [44:27:44<6:39:07, 13.50s/it]

 83%|████████▎ | 8967/10740 [44:28:04<7:34:10, 15.37s/it]

 84%|████████▎ | 8968/10740 [44:28:24<8:16:16, 16.80s/it]
{'loss': 0.3594, 'learning_rate': 1.3942842660263465e-07, 'rewards/chosen': -1.2249281406402588, 'rewards/rejected': -2.339123249053955, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1141948699951172, 'policy_logps/rejected': -426.9256286621094, 'policy_logps/chosen': -270.1321716308594, 'referece_logps/rejected': -403.534423828125, 'referece_logps/chosen': -257.88287353515625, 'logits/rejected': -1.189600944519043, 'logits/chosen': -1.3031662702560425, 'epoch': 5.01}


 84%|████████▎ | 8970/10740 [44:29:06<9:23:09, 19.09s/it]
{'loss': 0.3617, 'learning_rate': 1.391213726613396e-07, 'rewards/chosen': -1.370558261871338, 'rewards/rejected': -2.9568231105804443, 'rewards/accuracies': 1.0, 'rewards/margins': 1.586264729499817, 'policy_logps/rejected': -293.74298095703125, 'policy_logps/chosen': -348.7627258300781, 'referece_logps/rejected': -264.1747741699219, 'referece_logps/chosen': -335.0571594238281, 'logits/rejected': -0.5406216979026794, 'logits/chosen': -0.6829252243041992, 'epoch': 5.01}


 84%|████████▎ | 8972/10740 [44:29:40<8:48:57, 17.95s/it]

 84%|████████▎ | 8973/10740 [44:29:58<8:46:48, 17.89s/it]
{'loss': 0.4665, 'learning_rate': 1.3866137902692943e-07, 'rewards/chosen': -1.35073721408844, 'rewards/rejected': -3.152242660522461, 'rewards/accuracies': 0.875, 'rewards/margins': 1.801505208015442, 'policy_logps/rejected': -309.60107421875, 'policy_logps/chosen': -322.1333312988281, 'referece_logps/rejected': -278.07867431640625, 'referece_logps/chosen': -308.6259765625, 'logits/rejected': -0.6747621297836304, 'logits/chosen': -0.6916796565055847, 'epoch': 5.01}

 84%|████████▎ | 8974/10740 [44:30:15<8:36:00, 17.53s/it]


 84%|████████▎ | 8976/10740 [44:30:46<8:20:40, 17.03s/it]

 84%|████████▎ | 8977/10740 [44:30:58<7:34:10, 15.46s/it]
{'loss': 0.4287, 'learning_rate': 1.3804915101801905e-07, 'rewards/chosen': -1.2632074356079102, 'rewards/rejected': -2.913527011871338, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6503195762634277, 'policy_logps/rejected': -481.14862060546875, 'policy_logps/chosen': -369.50225830078125, 'referece_logps/rejected': -452.0133361816406, 'referece_logps/chosen': -356.87017822265625, 'logits/rejected': -1.0364165306091309, 'logits/chosen': -0.9357322454452515, 'epoch': 5.02}


 84%|████████▎ | 8979/10740 [44:31:36<8:32:45, 17.47s/it]
{'loss': 0.3548, 'learning_rate': 1.3774350733715977e-07, 'rewards/chosen': -2.0554962158203125, 'rewards/rejected': -4.111448287963867, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0559520721435547, 'policy_logps/rejected': -416.875244140625, 'policy_logps/chosen': -359.1025695800781, 'referece_logps/rejected': -375.7607727050781, 'referece_logps/chosen': -338.547607421875, 'logits/rejected': -1.1298161745071411, 'logits/chosen': -1.076920986175537, 'epoch': 5.02}

 84%|████████▎ | 8980/10740 [44:31:50<7:57:02, 16.26s/it]


 84%|████████▎ | 8982/10740 [44:32:28<8:39:35, 17.73s/it]
{'loss': 0.4485, 'learning_rate': 1.3728563003314465e-07, 'rewards/chosen': -1.8998510837554932, 'rewards/rejected': -4.0186991691589355, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1188478469848633, 'policy_logps/rejected': -373.4161376953125, 'policy_logps/chosen': -279.1667785644531, 'referece_logps/rejected': -333.22918701171875, 'referece_logps/chosen': -260.1682434082031, 'logits/rejected': -1.2288975715637207, 'logits/chosen': -1.316730260848999, 'epoch': 5.02}

 84%|████████▎ | 8983/10740 [44:32:49<9:10:09, 18.79s/it]

 84%|████████▎ | 8984/10740 [44:33:08<9:07:28, 18.71s/it]

 84%|████████▎ | 8985/10740 [44:33:25<8:52:07, 18.19s/it]

 84%|████████▎ | 8986/10740 [44:33:37<8:02:55, 16.52s/it]

 84%|████████▎ | 8987/10740 [44:33:57<8:29:45, 17.45s/it]

 84%|████████▎ | 8988/10740 [44:34:19<9:11:43, 18.89s/it]


 84%|████████▎ | 8990/10740 [44:34:48<8:06:37, 16.68s/it]
{'loss': 0.383, 'learning_rate': 1.3606807740851135e-07, 'rewards/chosen': -1.3357799053192139, 'rewards/rejected': -3.238685131072998, 'rewards/accuracies': 1.0, 'rewards/margins': 1.902904987335205, 'policy_logps/rejected': -338.3572692871094, 'policy_logps/chosen': -443.9957580566406, 'referece_logps/rejected': -305.9704284667969, 'referece_logps/chosen': -430.637939453125, 'logits/rejected': -1.5588016510009766, 'logits/chosen': -1.4233590364456177, 'epoch': 5.02}


 84%|████████▎ | 8992/10740 [44:35:25<8:30:19, 17.52s/it]
{'loss': 0.3948, 'learning_rate': 1.3576447474304332e-07, 'rewards/chosen': -1.732207179069519, 'rewards/rejected': -3.7098405361175537, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9776334762573242, 'policy_logps/rejected': -647.9115600585938, 'policy_logps/chosen': -541.786376953125, 'referece_logps/rejected': -610.8131103515625, 'referece_logps/chosen': -524.4642944335938, 'logits/rejected': 0.460121750831604, 'logits/chosen': 0.43467313051223755, 'epoch': 5.02}


 84%|████████▎ | 8994/10740 [44:35:59<8:33:52, 17.66s/it]

 84%|████████▍ | 8995/10740 [44:36:19<8:52:44, 18.32s/it]
{'loss': 0.3921, 'learning_rate': 1.3530966031186697e-07, 'rewards/chosen': -2.078864336013794, 'rewards/rejected': -3.104682445526123, 'rewards/accuracies': 0.875, 'rewards/margins': 1.025818109512329, 'policy_logps/rejected': -445.76397705078125, 'policy_logps/chosen': -308.14556884765625, 'referece_logps/rejected': -414.7171630859375, 'referece_logps/chosen': -287.3569641113281, 'logits/rejected': 0.14497998356819153, 'logits/chosen': -0.043511196970939636, 'epoch': 5.03}

 84%|████████▍ | 8996/10740 [44:36:36<8:42:13, 17.97s/it]

 84%|████████▍ | 8997/10740 [44:36:55<8:56:08, 18.46s/it]

 84%|████████▍ | 8998/10740 [44:37:17<9:24:44, 19.45s/it]

 84%|████████▍ | 8999/10740 [44:37:32<8:45:02, 18.09s/it]

 84%|████████▍ | 9000/10740 [44:37:45<8:01:49, 16.61s/it]

 84%|████████▍ | 9001/10740 [44:38:19<10:33:13, 21.85s/it]


 84%|████████▍ | 9003/10740 [44:38:51<9:08:38, 18.95s/it]

 84%|████████▍ | 9004/10740 [44:39:09<8:59:57, 18.66s/it]
{'loss': 0.3672, 'learning_rate': 1.3394946537255325e-07, 'rewards/chosen': -1.2217941284179688, 'rewards/rejected': -3.5211591720581055, 'rewards/accuracies': 0.625, 'rewards/margins': 2.2993650436401367, 'policy_logps/rejected': -395.43267822265625, 'policy_logps/chosen': -416.01837158203125, 'referece_logps/rejected': -360.2210998535156, 'referece_logps/chosen': -403.8003845214844, 'logits/rejected': -0.03586691617965698, 'logits/chosen': -0.16859401762485504, 'epoch': 5.03}


 84%|████████▍ | 9006/10740 [44:39:39<8:05:43, 16.81s/it]
{'loss': 0.3892, 'learning_rate': 1.3364806594168642e-07, 'rewards/chosen': -1.8147867918014526, 'rewards/rejected': -3.580232620239258, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7654459476470947, 'policy_logps/rejected': -384.11309814453125, 'policy_logps/chosen': -409.8566589355469, 'referece_logps/rejected': -348.3108215332031, 'referece_logps/chosen': -391.708740234375, 'logits/rejected': -0.02891281247138977, 'logits/chosen': -0.1120082437992096, 'epoch': 5.03}

 84%|████████▍ | 9007/10740 [44:39:52<7:30:46, 15.61s/it]

 84%|████████▍ | 9008/10740 [44:40:05<7:11:05, 14.93s/it]


 84%|████████▍ | 9010/10740 [44:40:41<7:57:16, 16.55s/it]
{'loss': 0.3433, 'learning_rate': 1.3304621275146544e-07, 'rewards/chosen': -2.0536060333251953, 'rewards/rejected': -4.864020824432373, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8104147911071777, 'policy_logps/rejected': -416.702880859375, 'policy_logps/chosen': -386.5178527832031, 'referece_logps/rejected': -368.0626525878906, 'referece_logps/chosen': -365.9818115234375, 'logits/rejected': -1.1920576095581055, 'logits/chosen': -1.1877473592758179, 'epoch': 5.03}

 84%|████████▍ | 9011/10740 [44:41:00<8:19:50, 17.35s/it]


 84%|████████▍ | 9012/10740 [44:41:20<8:37:35, 17.97s/it]
{'loss': 0.3326, 'learning_rate': 1.3259565075239454e-07, 'rewards/chosen': -1.9394440650939941, 'rewards/rejected': -4.591972351074219, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6525282859802246, 'policy_logps/rejected': -431.3495178222656, 'policy_logps/chosen': -424.4796447753906, 'referece_logps/rejected': -385.4298095703125, 'referece_logps/chosen': -405.0851745605469, 'logits/rejected': -0.28602781891822815, 'logits/chosen': -0.1933193951845169, 'epoch': 5.04}

 84%|████████▍ | 9014/10740 [44:41:53<8:13:23, 17.15s/it]
{'loss': 0.3138, 'learning_rate': 1.3244562118627045e-07, 'rewards/chosen': -2.007718801498413, 'rewards/rejected': -4.81129264831543, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8035738468170166, 'policy_logps/rejected': -313.07086181640625, 'policy_logps/chosen': -275.90338134765625, 'referece_logps/rejected': -264.95794677734375, 'referece_logps/chosen': -255.82618713378906, 'logits/rejected': -0.5822058916091919, 'logits/chosen': -0.5786322355270386, 'epoch': 5.04}
[2024-04-03 15:55:57,665] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▍ | 9016/10740 [44:42:35<9:11:05, 19.18s/it]
{'loss': 0.3843, 'learning_rate': 1.3214579878625588e-07, 'rewards/chosen': -0.8640438914299011, 'rewards/rejected': -1.8336764574050903, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9696325063705444, 'policy_logps/rejected': -491.1983337402344, 'policy_logps/chosen': -386.594482421875, 'referece_logps/rejected': -472.8615417480469, 'referece_logps/chosen': -377.95404052734375, 'logits/rejected': -1.3637735843658447, 'logits/chosen': -1.4877389669418335, 'epoch': 5.04}

 84%|████████▍ | 9017/10740 [44:42:54<9:07:37, 19.07s/it]

 84%|████████▍ | 9018/10740 [44:43:14<9:16:51, 19.40s/it]


 84%|████████▍ | 9020/10740 [44:43:49<8:40:30, 18.16s/it]

 84%|████████▍ | 9021/10740 [44:44:07<8:42:30, 18.24s/it]

 84%|████████▍ | 9022/10740 [44:44:27<8:58:21, 18.80s/it]
{'loss': 0.3246, 'learning_rate': 1.312482264251382e-07, 'rewards/chosen': -2.832890033721924, 'rewards/rejected': -4.200285911560059, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3673956394195557, 'policy_logps/rejected': -378.68798828125, 'policy_logps/chosen': -325.689453125, 'referece_logps/rejected': -336.6850891113281, 'referece_logps/chosen': -297.3605651855469, 'logits/rejected': 0.1587962657213211, 'logits/chosen': 0.12009064108133316, 'epoch': 5.04}

 84%|████████▍ | 9023/10740 [44:44:44<8:40:07, 18.18s/it]

 84%|████████▍ | 9024/10740 [44:45:01<8:25:50, 17.69s/it]


 84%|████████▍ | 9026/10740 [44:45:33<8:11:08, 17.19s/it]

 84%|████████▍ | 9027/10740 [44:45:53<8:34:37, 18.03s/it]
{'loss': 0.3432, 'learning_rate': 1.305024222478911e-07, 'rewards/chosen': -2.75559663772583, 'rewards/rejected': -5.312546730041504, 'rewards/accuracies': 0.75, 'rewards/margins': 2.556950569152832, 'policy_logps/rejected': -469.6751403808594, 'policy_logps/chosen': -503.0146179199219, 'referece_logps/rejected': -416.5497131347656, 'referece_logps/chosen': -475.45867919921875, 'logits/rejected': 0.25990891456604004, 'logits/chosen': 0.2701670825481415, 'epoch': 5.04}


 84%|████████▍ | 9029/10740 [44:46:29<8:34:29, 18.04s/it]
{'loss': 0.3161, 'learning_rate': 1.3020465406263648e-07, 'rewards/chosen': -1.9320720434188843, 'rewards/rejected': -4.767123699188232, 'rewards/accuracies': 1.0, 'rewards/margins': 2.835052728652954, 'policy_logps/rejected': -385.62847900390625, 'policy_logps/chosen': -440.0829162597656, 'referece_logps/rejected': -337.9572448730469, 'referece_logps/chosen': -420.76214599609375, 'logits/rejected': -0.8567526936531067, 'logits/chosen': -0.9560190439224243, 'epoch': 5.04}

 84%|████████▍ | 9030/10740 [44:46:45<8:12:14, 17.27s/it]

 84%|████████▍ | 9031/10740 [44:47:04<8:26:56, 17.80s/it]


 84%|████████▍ | 9033/10740 [44:47:35<7:48:53, 16.48s/it]

 84%|████████▍ | 9034/10740 [44:47:54<8:05:24, 17.07s/it]
{'loss': 0.3654, 'learning_rate': 1.2946161826150548e-07, 'rewards/chosen': -1.6647495031356812, 'rewards/rejected': -3.0358433723449707, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3710941076278687, 'policy_logps/rejected': -330.14544677734375, 'policy_logps/chosen': -278.19525146484375, 'referece_logps/rejected': -299.7869873046875, 'referece_logps/chosen': -261.5477294921875, 'logits/rejected': -0.7144291400909424, 'logits/chosen': -0.5797919034957886, 'epoch': 5.05}


 84%|████████▍ | 9036/10740 [44:48:19<7:04:39, 14.95s/it]
{'loss': 0.372, 'learning_rate': 1.291649580896954e-07, 'rewards/chosen': -2.006998062133789, 'rewards/rejected': -3.2087042331695557, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2017061710357666, 'policy_logps/rejected': -371.175048828125, 'policy_logps/chosen': -321.04180908203125, 'referece_logps/rejected': -339.08807373046875, 'referece_logps/chosen': -300.97186279296875, 'logits/rejected': -0.6762683391571045, 'logits/chosen': -0.44234704971313477, 'epoch': 5.05}


 84%|████████▍ | 9038/10740 [44:48:47<6:56:13, 14.67s/it]
{'loss': 0.4432, 'learning_rate': 1.2886861473620892e-07, 'rewards/chosen': -1.7003194093704224, 'rewards/rejected': -3.8168370723724365, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1165175437927246, 'policy_logps/rejected': -515.5887451171875, 'policy_logps/chosen': -417.8956298828125, 'referece_logps/rejected': -477.42041015625, 'referece_logps/chosen': -400.89239501953125, 'logits/rejected': -0.40325459837913513, 'logits/chosen': -0.24258866906166077, 'epoch': 5.05}

 84%|████████▍ | 9039/10740 [44:48:59<6:29:08, 13.73s/it]

 84%|████████▍ | 9040/10740 [44:49:18<7:16:49, 15.42s/it]

 84%|████████▍ | 9041/10740 [44:49:38<7:55:07, 16.78s/it]


 84%|████████▍ | 9043/10740 [44:50:13<8:08:14, 17.26s/it]
{'loss': 0.337, 'learning_rate': 1.281291431398953e-07, 'rewards/chosen': -1.926494836807251, 'rewards/rejected': -3.411928653717041, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4854340553283691, 'policy_logps/rejected': -309.40887451171875, 'policy_logps/chosen': -296.700439453125, 'referece_logps/rejected': -275.28955078125, 'referece_logps/chosen': -277.43548583984375, 'logits/rejected': -0.8277300596237183, 'logits/chosen': -0.9103469252586365, 'epoch': 5.05}

 84%|████████▍ | 9044/10740 [44:50:27<7:37:57, 16.20s/it]


 84%|████████▍ | 9046/10740 [44:51:09<8:50:41, 18.80s/it]
{'loss': 0.2699, 'learning_rate': 1.2768641166002226e-07, 'rewards/chosen': -1.0956987142562866, 'rewards/rejected': -4.586129188537598, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4904308319091797, 'policy_logps/rejected': -526.83203125, 'policy_logps/chosen': -413.4000549316406, 'referece_logps/rejected': -480.97076416015625, 'referece_logps/chosen': -402.44305419921875, 'logits/rejected': -0.08925434947013855, 'logits/chosen': -0.08648154139518738, 'epoch': 5.05}


 84%|████████▍ | 9048/10740 [44:51:37<7:36:53, 16.20s/it]

 84%|████████▍ | 9049/10740 [44:51:56<7:55:03, 16.86s/it]

 84%|████████▍ | 9050/10740 [44:52:17<8:34:39, 18.27s/it]
{'loss': 0.4435, 'learning_rate': 1.2709721382952054e-07, 'rewards/chosen': -1.7171252965927124, 'rewards/rejected': -2.140479803085327, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4233543574810028, 'policy_logps/rejected': -411.3612365722656, 'policy_logps/chosen': -449.89434814453125, 'referece_logps/rejected': -389.9564514160156, 'referece_logps/chosen': -432.7231140136719, 'logits/rejected': -0.2200268805027008, 'logits/chosen': -0.331717848777771, 'epoch': 5.06}


 84%|████████▍ | 9052/10740 [44:52:49<8:00:09, 17.07s/it]
{'loss': 0.4499, 'learning_rate': 1.2680309121659327e-07, 'rewards/chosen': -2.3018054962158203, 'rewards/rejected': -3.3080861568450928, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0062806606292725, 'policy_logps/rejected': -329.1344299316406, 'policy_logps/chosen': -295.68701171875, 'referece_logps/rejected': -296.0535583496094, 'referece_logps/chosen': -272.6689453125, 'logits/rejected': -0.42623111605644226, 'logits/chosen': -0.40949252247810364, 'epoch': 5.06}

 84%|████████▍ | 9053/10740 [44:53:06<7:56:32, 16.95s/it]

 84%|████████▍ | 9054/10740 [44:53:22<7:47:30, 16.64s/it]

 84%|████████▍ | 9055/10740 [44:53:37<7:33:10, 16.14s/it]

 84%|████████▍ | 9056/10740 [44:53:55<7:48:08, 16.68s/it]

 84%|████████▍ | 9057/10740 [44:54:15<8:12:19, 17.55s/it]


 84%|████████▍ | 9059/10740 [44:54:54<8:42:40, 18.66s/it]
{'loss': 0.296, 'learning_rate': 1.257761644835763e-07, 'rewards/chosen': -2.664330244064331, 'rewards/rejected': -3.8203492164611816, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1560189723968506, 'policy_logps/rejected': -270.5889892578125, 'policy_logps/chosen': -289.688232421875, 'referece_logps/rejected': -232.385498046875, 'referece_logps/chosen': -263.044921875, 'logits/rejected': -0.9735039472579956, 'logits/chosen': -0.9458765983581543, 'epoch': 5.06}


 84%|████████▍ | 9061/10740 [44:55:34<8:58:00, 19.23s/it]

 84%|████████▍ | 9062/10740 [44:55:54<9:04:13, 19.46s/it]
{'loss': 0.3014, 'learning_rate': 1.2533724545191505e-07, 'rewards/chosen': -2.77726411819458, 'rewards/rejected': -5.158266067504883, 'rewards/accuracies': 1.0, 'rewards/margins': 2.381002187728882, 'policy_logps/rejected': -396.54168701171875, 'policy_logps/chosen': -353.4119873046875, 'referece_logps/rejected': -344.958984375, 'referece_logps/chosen': -325.6393127441406, 'logits/rejected': -0.5692437887191772, 'logits/chosen': -0.6056047677993774, 'epoch': 5.06}

 84%|████████▍ | 9063/10740 [44:56:09<8:29:40, 18.24s/it]


 84%|████████▍ | 9065/10740 [44:56:40<7:45:02, 16.66s/it]
{'loss': 0.4208, 'learning_rate': 1.2489904239471384e-07, 'rewards/chosen': -1.5704885721206665, 'rewards/rejected': -2.1229751110076904, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5524864196777344, 'policy_logps/rejected': -466.1967468261719, 'policy_logps/chosen': -724.4414672851562, 'referece_logps/rejected': -444.9670104980469, 'referece_logps/chosen': -708.7366333007812, 'logits/rejected': -0.009584859013557434, 'logits/chosen': -0.22143793106079102, 'epoch': 5.06}

 84%|████████▍ | 9066/10740 [44:56:50<6:55:47, 14.90s/it]

 84%|████████▍ | 9067/10740 [44:57:07<7:11:18, 15.47s/it]

 84%|████████▍ | 9068/10740 [44:57:27<7:46:02, 16.72s/it]


 84%|████████▍ | 9070/10740 [44:58:06<8:27:16, 18.23s/it]
{'loss': 0.29, 'learning_rate': 1.2417029599438356e-07, 'rewards/chosen': -1.9465937614440918, 'rewards/rejected': -4.7060546875, 'rewards/accuracies': 1.0, 'rewards/margins': 2.759460926055908, 'policy_logps/rejected': -524.800048828125, 'policy_logps/chosen': -385.1498107910156, 'referece_logps/rejected': -477.739501953125, 'referece_logps/chosen': -365.6838684082031, 'logits/rejected': -0.46137332916259766, 'logits/chosen': -0.46640944480895996, 'epoch': 5.07}


 84%|████████▍ | 9072/10740 [44:58:36<7:41:01, 16.58s/it]
{'loss': 0.3391, 'learning_rate': 1.2387935495352453e-07, 'rewards/chosen': -1.351758360862732, 'rewards/rejected': -4.231889724731445, 'rewards/accuracies': 0.75, 'rewards/margins': 2.880131483078003, 'policy_logps/rejected': -394.7552795410156, 'policy_logps/chosen': -323.97979736328125, 'referece_logps/rejected': -352.4363708496094, 'referece_logps/chosen': -310.4622497558594, 'logits/rejected': -0.5127338767051697, 'logits/chosen': -0.4923210144042969, 'epoch': 5.07}


 84%|████████▍ | 9074/10740 [44:59:16<8:28:17, 18.31s/it]
{'loss': 0.2872, 'learning_rate': 1.2358873265394344e-07, 'rewards/chosen': -0.6651524305343628, 'rewards/rejected': -2.918323516845703, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2531707286834717, 'policy_logps/rejected': -643.938232421875, 'policy_logps/chosen': -399.79876708984375, 'referece_logps/rejected': -614.7548828125, 'referece_logps/chosen': -393.147216796875, 'logits/rejected': -0.5347617268562317, 'logits/chosen': -0.5051127076148987, 'epoch': 5.07}

 84%|████████▍ | 9075/10740 [44:59:35<8:31:04, 18.42s/it]

 85%|████████▍ | 9076/10740 [44:59:53<8:27:25, 18.30s/it]


 85%|████████▍ | 9078/10740 [45:00:26<8:11:38, 17.75s/it]
{'loss': 0.3319, 'learning_rate': 1.2300844470142402e-07, 'rewards/chosen': -1.6977753639221191, 'rewards/rejected': -3.4497222900390625, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7519466876983643, 'policy_logps/rejected': -234.53231811523438, 'policy_logps/chosen': -292.0130920410156, 'referece_logps/rejected': -200.03512573242188, 'referece_logps/chosen': -275.0353088378906, 'logits/rejected': -1.536852240562439, 'logits/chosen': -1.5188617706298828, 'epoch': 5.07}

 85%|████████▍ | 9079/10740 [45:00:46<8:25:52, 18.27s/it]

 85%|████████▍ | 9080/10740 [45:01:00<7:53:38, 17.12s/it]


 85%|████████▍ | 9082/10740 [45:01:32<7:48:55, 16.97s/it]

 85%|████████▍ | 9083/10740 [45:01:50<7:58:31, 17.33s/it]
{'loss': 0.3699, 'learning_rate': 1.2228487956133993e-07, 'rewards/chosen': -2.576385736465454, 'rewards/rejected': -3.739830732345581, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1634446382522583, 'policy_logps/rejected': -348.7935791015625, 'policy_logps/chosen': -473.69085693359375, 'referece_logps/rejected': -311.395263671875, 'referece_logps/chosen': -447.92706298828125, 'logits/rejected': -0.6945651769638062, 'logits/chosen': -0.9105836153030396, 'epoch': 5.07}

 85%|████████▍ | 9084/10740 [45:02:11<8:23:31, 18.24s/it]

 85%|████████▍ | 9085/10740 [45:02:32<8:46:54, 19.10s/it]


 85%|████████▍ | 9087/10740 [45:03:11<8:50:16, 19.25s/it]
{'loss': 0.3568, 'learning_rate': 1.217074643163605e-07, 'rewards/chosen': -1.6789408922195435, 'rewards/rejected': -3.6924846172332764, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0135436058044434, 'policy_logps/rejected': -389.5787353515625, 'policy_logps/chosen': -446.8250427246094, 'referece_logps/rejected': -352.6538391113281, 'referece_logps/chosen': -430.03558349609375, 'logits/rejected': -0.810124933719635, 'logits/chosen': -0.9235098361968994, 'epoch': 5.08}

 85%|████████▍ | 9088/10740 [45:03:30<8:47:23, 19.15s/it]

 85%|████████▍ | 9089/10740 [45:03:46<8:23:09, 18.29s/it]

 85%|████████▍ | 9090/10740 [45:04:06<8:34:55, 18.72s/it]

 85%|████████▍ | 9091/10740 [45:04:18<7:42:11, 16.82s/it]

 85%|████████▍ | 9092/10740 [45:04:29<6:55:23, 15.12s/it]

 85%|████████▍ | 9093/10740 [45:04:45<7:05:05, 15.49s/it]

 85%|████████▍ | 9094/10740 [45:05:07<7:56:51, 17.38s/it]

 85%|████████▍ | 9095/10740 [45:05:24<7:55:23, 17.34s/it]

 85%|████████▍ | 9096/10740 [45:05:44<8:09:27, 17.86s/it]

 85%|████████▍ | 9097/10740 [45:05:59<7:48:54, 17.12s/it]

 85%|████████▍ | 9098/10740 [45:06:17<7:59:03, 17.51s/it]

 85%|████████▍ | 9099/10740 [45:06:39<8:32:33, 18.74s/it]

 85%|████████▍ | 9100/10740 [45:06:58<8:37:36, 18.94s/it]

 85%|████████▍ | 9101/10740 [45:07:16<8:25:58, 18.52s/it]

 85%|████████▍ | 9102/10740 [45:07:34<8:20:08, 18.32s/it]

 85%|████████▍ | 9103/10740 [45:07:50<8:04:15, 17.75s/it]

 85%|████████▍ | 9104/10740 [45:08:05<7:40:23, 16.88s/it]


 85%|████████▍ | 9106/10740 [45:08:33<7:00:43, 15.45s/it]
{'loss': 0.3838, 'learning_rate': 1.1898221059584523e-07, 'rewards/chosen': -1.7148222923278809, 'rewards/rejected': -3.127788782119751, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4129664897918701, 'policy_logps/rejected': -315.0986328125, 'policy_logps/chosen': -315.135009765625, 'referece_logps/rejected': -283.8207702636719, 'referece_logps/chosen': -297.98681640625, 'logits/rejected': -0.45758765935897827, 'logits/chosen': -0.5428410768508911, 'epoch': 5.09}

 85%|████████▍ | 9107/10740 [45:08:47<6:52:20, 15.15s/it]


 85%|████████▍ | 9109/10740 [45:09:29<8:16:00, 18.25s/it]
{'loss': 0.3719, 'learning_rate': 1.1855454940073817e-07, 'rewards/chosen': -1.4596575498580933, 'rewards/rejected': -4.2505903244018555, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7909328937530518, 'policy_logps/rejected': -548.0048828125, 'policy_logps/chosen': -565.3106689453125, 'referece_logps/rejected': -505.4990234375, 'referece_logps/chosen': -550.714111328125, 'logits/rejected': -0.4120248556137085, 'logits/chosen': -0.5857237577438354, 'epoch': 5.09}


 85%|████████▍ | 9111/10740 [45:10:07<8:27:25, 18.69s/it]
{'loss': 0.4004, 'learning_rate': 1.1826984276385121e-07, 'rewards/chosen': -1.8069751262664795, 'rewards/rejected': -2.726668357849121, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9196931719779968, 'policy_logps/rejected': -354.6629638671875, 'policy_logps/chosen': -304.3280944824219, 'referece_logps/rejected': -327.39630126953125, 'referece_logps/chosen': -286.2583312988281, 'logits/rejected': -1.1207175254821777, 'logits/chosen': -0.9861587285995483, 'epoch': 5.09}

 85%|████████▍ | 9112/10740 [45:10:22<7:53:40, 17.46s/it]

 85%|████████▍ | 9113/10740 [45:10:32<6:58:39, 15.44s/it]

 85%|████████▍ | 9114/10740 [45:10:43<6:23:37, 14.16s/it]

 85%|████████▍ | 9115/10740 [45:11:03<7:11:29, 15.93s/it]

 85%|████████▍ | 9116/10740 [45:11:25<7:58:17, 17.67s/it]

 85%|████████▍ | 9117/10740 [45:11:40<7:37:34, 16.92s/it]

 85%|████████▍ | 9118/10740 [45:12:00<7:55:46, 17.60s/it]

 85%|████████▍ | 9119/10740 [45:12:12<7:16:44, 16.17s/it]

 85%|████████▍ | 9120/10740 [45:12:33<7:48:27, 17.35s/it]

 85%|████████▍ | 9121/10740 [45:12:52<8:08:45, 18.11s/it]

 85%|████████▍ | 9122/10740 [45:13:08<7:44:49, 17.24s/it]

 85%|████████▍ | 9123/10740 [45:13:21<7:11:47, 16.02s/it]

 85%|████████▍ | 9124/10740 [45:13:41<7:41:57, 17.15s/it]

 85%|████████▍ | 9125/10740 [45:13:59<7:49:32, 17.44s/it]

 85%|████████▍ | 9126/10740 [45:14:17<7:55:33, 17.68s/it]

 85%|████████▍ | 9127/10740 [45:14:40<8:36:11, 19.20s/it]


 85%|████████▌ | 9129/10740 [45:15:09<7:35:07, 16.95s/it]
{'loss': 0.3315, 'learning_rate': 1.1572193061612868e-07, 'rewards/chosen': -2.1754729747772217, 'rewards/rejected': -2.777534008026123, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6020610332489014, 'policy_logps/rejected': -548.6491088867188, 'policy_logps/chosen': -600.5104370117188, 'referece_logps/rejected': -520.873779296875, 'referece_logps/chosen': -578.7557373046875, 'logits/rejected': 0.5878506898880005, 'logits/chosen': 0.6337276697158813, 'epoch': 5.1}

 85%|████████▌ | 9130/10740 [45:15:29<7:56:57, 17.78s/it]

 85%|████████▌ | 9131/10740 [45:15:49<8:14:31, 18.44s/it]

 85%|████████▌ | 9132/10740 [45:16:07<8:11:36, 18.34s/it]

 85%|████████▌ | 9133/10740 [45:16:25<8:05:23, 18.12s/it]

 85%|████████▌ | 9134/10740 [45:16:44<8:17:07, 18.57s/it]


 85%|████████▌ | 9136/10740 [45:17:21<8:11:01, 18.37s/it]
{'loss': 0.2784, 'learning_rate': 1.1473810915463677e-07, 'rewards/chosen': -1.8090156316757202, 'rewards/rejected': -4.245340824127197, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4363253116607666, 'policy_logps/rejected': -395.98553466796875, 'policy_logps/chosen': -400.366455078125, 'referece_logps/rejected': -353.5321044921875, 'referece_logps/chosen': -382.27630615234375, 'logits/rejected': -0.7609186172485352, 'logits/chosen': -0.8916987180709839, 'epoch': 5.1}

 85%|████████▌ | 9137/10740 [45:17:43<8:37:15, 19.36s/it]

 85%|████████▌ | 9138/10740 [45:18:03<8:41:47, 19.54s/it]

 85%|████████▌ | 9139/10740 [45:18:14<7:30:55, 16.90s/it]

 85%|████████▌ | 9140/10740 [45:18:24<6:41:20, 15.05s/it]

 85%|████████▌ | 9141/10740 [45:18:35<6:06:07, 13.74s/it]

 85%|████████▌ | 9142/10740 [45:18:49<6:05:07, 13.71s/it]

 85%|████████▌ | 9143/10740 [45:19:09<6:56:18, 15.64s/it]

 85%|████████▌ | 9144/10740 [45:19:21<6:29:37, 14.65s/it]

 85%|████████▌ | 9145/10740 [45:19:33<6:08:39, 13.87s/it]

 85%|████████▌ | 9146/10740 [45:19:45<5:47:49, 13.09s/it]

 85%|████████▌ | 9147/10740 [45:20:04<6:39:16, 15.04s/it]

 85%|████████▌ | 9148/10740 [45:20:18<6:32:11, 14.78s/it]

 85%|████████▌ | 9149/10740 [45:20:32<6:27:05, 14.60s/it]

 85%|████████▌ | 9150/10740 [45:20:54<7:25:02, 16.79s/it]

 85%|████████▌ | 9151/10740 [45:21:11<7:27:00, 16.88s/it]

 85%|████████▌ | 9152/10740 [45:21:34<8:08:10, 18.45s/it]

 85%|████████▌ | 9153/10740 [45:21:47<7:25:04, 16.83s/it]

 85%|████████▌ | 9154/10740 [45:21:57<6:36:01, 14.98s/it]


 85%|████████▌ | 9156/10740 [45:22:32<7:01:03, 15.95s/it]
{'loss': 0.3499, 'learning_rate': 1.1194894508838126e-07, 'rewards/chosen': -2.037992238998413, 'rewards/rejected': -4.253947734832764, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2159554958343506, 'policy_logps/rejected': -517.9545288085938, 'policy_logps/chosen': -491.7942810058594, 'referece_logps/rejected': -475.41497802734375, 'referece_logps/chosen': -471.4143371582031, 'logits/rejected': -0.5905506610870361, 'logits/chosen': -0.5918616652488708, 'epoch': 5.12}

 85%|████████▌ | 9157/10740 [45:22:54<7:52:51, 17.92s/it]

 85%|████████▌ | 9158/10740 [45:23:09<7:27:28, 16.97s/it]


 85%|████████▌ | 9160/10740 [45:23:40<7:07:05, 16.22s/it]
{'loss': 0.3649, 'learning_rate': 1.1139498601718167e-07, 'rewards/chosen': -1.4818406105041504, 'rewards/rejected': -3.4530582427978516, 'rewards/accuracies': 0.875, 'rewards/margins': 1.971217155456543, 'policy_logps/rejected': -325.0054931640625, 'policy_logps/chosen': -332.16363525390625, 'referece_logps/rejected': -290.4749450683594, 'referece_logps/chosen': -317.34521484375, 'logits/rejected': -0.5933924913406372, 'logits/chosen': -0.7055045962333679, 'epoch': 5.12}

 85%|████████▌ | 9161/10740 [45:23:59<7:32:53, 17.21s/it]

 85%|████████▌ | 9162/10740 [45:24:18<7:43:53, 17.64s/it]

 85%|████████▌ | 9163/10740 [45:24:40<8:19:39, 19.01s/it]

 85%|████████▌ | 9164/10740 [45:25:00<8:23:29, 19.17s/it]

 85%|████████▌ | 9165/10740 [45:25:19<8:26:24, 19.29s/it]

 85%|████████▌ | 9166/10740 [45:25:39<8:25:10, 19.26s/it]

 85%|████████▌ | 9167/10740 [45:25:50<7:26:40, 17.04s/it]

 85%|████████▌ | 9168/10740 [45:26:09<7:35:27, 17.38s/it]

 85%|████████▌ | 9169/10740 [45:26:25<7:31:01, 17.23s/it]

 85%|████████▌ | 9170/10740 [45:26:36<6:40:52, 15.32s/it]

 85%|████████▌ | 9171/10740 [45:26:55<7:09:33, 16.43s/it]

 85%|████████▌ | 9172/10740 [45:27:15<7:34:05, 17.38s/it]


 85%|████████▌ | 9174/10740 [45:27:54<7:59:47, 18.38s/it]
{'loss': 0.3682, 'learning_rate': 1.0946631796161632e-07, 'rewards/chosen': -2.1175425052642822, 'rewards/rejected': -5.339621543884277, 'rewards/accuracies': 1.0, 'rewards/margins': 3.222079038619995, 'policy_logps/rejected': -494.6553955078125, 'policy_logps/chosen': -358.4162902832031, 'referece_logps/rejected': -441.2591857910156, 'referece_logps/chosen': -337.2408447265625, 'logits/rejected': 0.05380857735872269, 'logits/chosen': 0.09678709506988525, 'epoch': 5.13}

 85%|████████▌ | 9175/10740 [45:28:09<7:34:10, 17.41s/it]

 85%|████████▌ | 9176/10740 [45:28:20<6:43:16, 15.47s/it]

 85%|████████▌ | 9177/10740 [45:28:31<6:08:09, 14.13s/it]

 85%|████████▌ | 9178/10740 [45:28:52<6:56:50, 16.01s/it]

 85%|████████▌ | 9179/10740 [45:29:07<6:55:08, 15.96s/it]

 85%|████████▌ | 9180/10740 [45:29:27<7:23:40, 17.06s/it]

 85%|████████▌ | 9181/10740 [45:29:42<7:09:44, 16.54s/it]

 85%|████████▌ | 9182/10740 [45:30:03<7:38:20, 17.65s/it]

 86%|████████▌ | 9183/10740 [45:30:20<7:37:09, 17.62s/it]

 86%|████████▌ | 9184/10740 [45:30:35<7:14:28, 16.75s/it]

 86%|████████▌ | 9185/10740 [45:30:54<7:35:47, 17.59s/it]


 86%|████████▌ | 9187/10740 [45:31:31<7:42:18, 17.86s/it]
{'loss': 0.3092, 'learning_rate': 1.0768962599745579e-07, 'rewards/chosen': -1.4746320247650146, 'rewards/rejected': -4.539149284362793, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0645174980163574, 'policy_logps/rejected': -408.5135498046875, 'policy_logps/chosen': -507.5450439453125, 'referece_logps/rejected': -363.1220703125, 'referece_logps/chosen': -492.7987060546875, 'logits/rejected': -0.1306389570236206, 'logits/chosen': -0.018588975071907043, 'epoch': 5.13}


 86%|████████▌ | 9189/10740 [45:32:14<8:23:44, 19.49s/it]

 86%|████████▌ | 9190/10740 [45:32:32<8:15:21, 19.18s/it]

 86%|████████▌ | 9191/10740 [45:32:54<8:39:46, 20.13s/it]

 86%|████████▌ | 9192/10740 [45:33:08<7:45:25, 18.04s/it]

 86%|████████▌ | 9193/10740 [45:33:26<7:49:30, 18.21s/it]

 86%|████████▌ | 9194/10740 [45:33:46<8:02:20, 18.72s/it]

 86%|████████▌ | 9195/10740 [45:34:06<8:09:57, 19.03s/it]

 86%|████████▌ | 9196/10740 [45:34:25<8:12:04, 19.12s/it]

 86%|████████▌ | 9197/10740 [45:34:46<8:27:58, 19.75s/it]

 86%|████████▌ | 9198/10740 [45:35:05<8:16:13, 19.31s/it]

 86%|████████▌ | 9199/10740 [45:35:25<8:20:34, 19.49s/it]

 86%|████████▌ | 9200/10740 [45:35:44<8:22:16, 19.57s/it]

 86%|████████▌ | 9201/10740 [45:36:03<8:12:40, 19.21s/it]
{'loss': 0.2896, 'learning_rate': 1.0579160500698047e-07, 'rewards/chosen': -2.019782781600952, 'rewards/rejected': -2.972839593887329, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9530568718910217, 'policy_logps/rejected': -464.73602294921875, 'policy_logps/chosen': -413.0336608886719, 'referece_logps/rejected': -435.00762939453125, 'referece_logps/chosen': -392.8358459472656, 'logits/rejected': -0.32889699935913086, 'logits/chosen': -0.39137977361679077, 'epoch': 5.14}


 86%|████████▌ | 9203/10740 [45:36:46<8:42:14, 20.39s/it]

 86%|████████▌ | 9204/10740 [45:37:05<8:34:03, 20.08s/it]

 86%|████████▌ | 9205/10740 [45:37:25<8:34:04, 20.09s/it]

 86%|████████▌ | 9206/10740 [45:37:45<8:31:49, 20.02s/it]

 86%|████████▌ | 9207/10740 [45:38:02<8:08:19, 19.11s/it]

 86%|████████▌ | 9208/10740 [45:38:20<7:54:54, 18.60s/it]

 86%|████████▌ | 9209/10740 [45:38:33<7:18:00, 17.17s/it]

 86%|████████▌ | 9210/10740 [45:38:52<7:28:55, 17.60s/it]

 86%|████████▌ | 9211/10740 [45:39:09<7:22:00, 17.35s/it]
{'loss': 0.2491, 'learning_rate': 1.0444563340555513e-07, 'rewards/chosen': -2.3456578254699707, 'rewards/rejected': -4.771723747253418, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4260659217834473, 'policy_logps/rejected': -327.78564453125, 'policy_logps/chosen': -339.318359375, 'referece_logps/rejected': -280.0684509277344, 'referece_logps/chosen': -315.86181640625, 'logits/rejected': -0.8319731950759888, 'logits/chosen': -0.8558447360992432, 'epoch': 5.15}

 86%|████████▌ | 9212/10740 [45:39:29<7:42:55, 18.18s/it]


 86%|████████▌ | 9214/10740 [45:40:11<8:15:16, 19.47s/it]

 86%|████████▌ | 9215/10740 [45:40:27<7:53:22, 18.62s/it]

 86%|████████▌ | 9216/10740 [45:40:44<7:37:41, 18.02s/it]

 86%|████████▌ | 9217/10740 [45:41:02<7:36:55, 18.00s/it]

 86%|████████▌ | 9218/10740 [45:41:23<7:56:33, 18.79s/it]

 86%|████████▌ | 9219/10740 [45:41:46<8:27:50, 20.03s/it]
{'loss': 0.3966, 'learning_rate': 1.0337472014123561e-07, 'rewards/chosen': -1.499001383781433, 'rewards/rejected': -2.9831490516662598, 'rewards/accuracies': 0.5, 'rewards/margins': 1.4841476678848267, 'policy_logps/rejected': -327.2958068847656, 'policy_logps/chosen': -234.2356719970703, 'referece_logps/rejected': -297.4643249511719, 'referece_logps/chosen': -219.2456512451172, 'logits/rejected': 0.0683228150010109, 'logits/chosen': 0.026802316308021545, 'epoch': 5.15}


 86%|████████▌ | 9221/10740 [45:42:24<8:12:54, 19.47s/it]
{'loss': 0.378, 'learning_rate': 1.0310780708460942e-07, 'rewards/chosen': -1.822531819343567, 'rewards/rejected': -3.3268017768859863, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5042699575424194, 'policy_logps/rejected': -434.9582214355469, 'policy_logps/chosen': -428.623779296875, 'referece_logps/rejected': -401.690185546875, 'referece_logps/chosen': -410.3984375, 'logits/rejected': -0.10592730343341827, 'logits/chosen': -0.22753389179706573, 'epoch': 5.15}


 86%|████████▌ | 9223/10740 [45:43:01<8:04:00, 19.14s/it]

 86%|████████▌ | 9224/10740 [45:43:19<7:54:38, 18.79s/it]
{'loss': 0.4332, 'learning_rate': 1.0270804933905286e-07, 'rewards/chosen': -1.461001992225647, 'rewards/rejected': -2.534193515777588, 'rewards/accuracies': 0.625, 'rewards/margins': 1.073191523551941, 'policy_logps/rejected': -460.2847900390625, 'policy_logps/chosen': -353.9707946777344, 'referece_logps/rejected': -434.9429016113281, 'referece_logps/chosen': -339.36077880859375, 'logits/rejected': 0.03357495367527008, 'logits/chosen': 0.024197638034820557, 'epoch': 5.15}


 86%|████████▌ | 9226/10740 [45:43:51<7:16:30, 17.30s/it]

 86%|████████▌ | 9227/10740 [45:44:11<7:37:42, 18.15s/it]

 86%|████████▌ | 9228/10740 [45:44:28<7:27:02, 17.74s/it]

 86%|████████▌ | 9229/10740 [45:44:41<6:53:44, 16.43s/it]

 86%|████████▌ | 9230/10740 [45:44:59<7:04:50, 16.88s/it]
{'loss': 0.3617, 'learning_rate': 1.0191073766882873e-07, 'rewards/chosen': -2.3657472133636475, 'rewards/rejected': -3.3412795066833496, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9755322933197021, 'policy_logps/rejected': -531.819091796875, 'policy_logps/chosen': -468.8487243652344, 'referece_logps/rejected': -498.4062805175781, 'referece_logps/chosen': -445.1912536621094, 'logits/rejected': -1.1709071397781372, 'logits/chosen': -1.0801961421966553, 'epoch': 5.16}


 86%|████████▌ | 9232/10740 [45:45:28<6:25:22, 15.33s/it]

 86%|████████▌ | 9233/10740 [45:45:46<6:47:16, 16.22s/it]

 86%|████████▌ | 9234/10740 [45:46:02<6:43:27, 16.07s/it]

 86%|████████▌ | 9235/10740 [45:46:21<7:04:24, 16.92s/it]

 86%|████████▌ | 9236/10740 [45:46:37<6:57:41, 16.66s/it]

 86%|████████▌ | 9237/10740 [45:46:54<7:01:53, 16.84s/it]
{'loss': 0.2726, 'learning_rate': 1.009842574976717e-07, 'rewards/chosen': -0.602936863899231, 'rewards/rejected': -2.9096388816833496, 'rewards/accuracies': 0.875, 'rewards/margins': 2.306701898574829, 'policy_logps/rejected': -288.8026428222656, 'policy_logps/chosen': -209.9483184814453, 'referece_logps/rejected': -259.7062683105469, 'referece_logps/chosen': -203.91893005371094, 'logits/rejected': -0.6010298728942871, 'logits/chosen': -0.6447441577911377, 'epoch': 5.16}


 86%|████████▌ | 9239/10740 [45:47:37<8:01:45, 19.26s/it]

 86%|████████▌ | 9240/10740 [45:47:55<7:50:35, 18.82s/it]

 86%|████████▌ | 9241/10740 [45:48:15<8:01:15, 19.26s/it]

 86%|████████▌ | 9242/10740 [45:48:35<8:06:01, 19.47s/it]

 86%|████████▌ | 9243/10740 [45:48:47<7:05:03, 17.04s/it]

 86%|████████▌ | 9244/10740 [45:49:03<7:01:18, 16.90s/it]
{'loss': 0.4862, 'learning_rate': 1.0006178394169795e-07, 'rewards/chosen': -0.9724255204200745, 'rewards/rejected': -3.2633285522460938, 'rewards/accuracies': 1.0, 'rewards/margins': 2.290903091430664, 'policy_logps/rejected': -525.0125122070312, 'policy_logps/chosen': -371.7869873046875, 'referece_logps/rejected': -492.3792724609375, 'referece_logps/chosen': -362.0627136230469, 'logits/rejected': -0.3973384499549866, 'logits/chosen': -0.21857018768787384, 'epoch': 5.16}


 86%|████████▌ | 9246/10740 [45:49:29<6:09:11, 14.83s/it]

 86%|████████▌ | 9247/10740 [45:49:43<6:01:24, 14.52s/it]

 86%|████████▌ | 9248/10740 [45:50:02<6:40:19, 16.10s/it]

 86%|████████▌ | 9249/10740 [45:50:22<7:05:43, 17.13s/it]

 86%|████████▌ | 9250/10740 [45:50:43<7:37:21, 18.42s/it]

 86%|████████▌ | 9251/10740 [45:51:00<7:24:07, 17.90s/it]

 86%|████████▌ | 9252/10740 [45:51:18<7:24:06, 17.91s/it]

 86%|████████▌ | 9253/10740 [45:51:39<7:43:44, 18.71s/it]

 86%|████████▌ | 9254/10740 [45:51:59<7:52:39, 19.08s/it]

 86%|████████▌ | 9255/10740 [45:52:18<7:56:52, 19.27s/it]

 86%|████████▌ | 9256/10740 [45:52:34<7:32:52, 18.31s/it]
{'loss': 0.3049, 'learning_rate': 9.848973404862237e-08, 'rewards/chosen': -1.798980712890625, 'rewards/rejected': -3.985680341720581, 'rewards/accuracies': 1.0, 'rewards/margins': 2.186699390411377, 'policy_logps/rejected': -339.5947570800781, 'policy_logps/chosen': -276.4544372558594, 'referece_logps/rejected': -299.7379150390625, 'referece_logps/chosen': -258.4646301269531, 'logits/rejected': -0.7305695414543152, 'logits/chosen': -0.7373754978179932, 'epoch': 5.17}

 86%|████████▌ | 9257/10740 [45:52:54<7:42:07, 18.70s/it]


 86%|████████▌ | 9259/10740 [45:53:30<7:26:37, 18.09s/it]

 86%|████████▌ | 9260/10740 [45:53:53<8:02:11, 19.55s/it]

 86%|████████▌ | 9261/10740 [45:54:12<8:04:01, 19.64s/it]
{'loss': 0.3563, 'learning_rate': 9.783819684904516e-08, 'rewards/chosen': -3.0964555740356445, 'rewards/rejected': -5.895444393157959, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7989892959594727, 'policy_logps/rejected': -479.48162841796875, 'policy_logps/chosen': -555.8726196289062, 'referece_logps/rejected': -420.52716064453125, 'referece_logps/chosen': -524.9080810546875, 'logits/rejected': -0.502243161201477, 'logits/chosen': -0.7185824513435364, 'epoch': 5.17}


 86%|████████▌ | 9263/10740 [45:54:50<7:57:09, 19.38s/it]

 86%|████████▋ | 9264/10740 [45:55:09<7:57:56, 19.43s/it]

 86%|████████▋ | 9265/10740 [45:55:29<7:59:40, 19.51s/it]

 86%|████████▋ | 9266/10740 [45:55:45<7:35:34, 18.54s/it]

 86%|████████▋ | 9267/10740 [45:55:59<6:56:29, 16.96s/it]

 86%|████████▋ | 9268/10740 [45:56:15<6:51:11, 16.76s/it]

 86%|████████▋ | 9269/10740 [45:56:32<6:51:03, 16.77s/it]

 86%|████████▋ | 9270/10740 [45:56:46<6:31:02, 15.96s/it]
{'loss': 0.3666, 'learning_rate': 9.667060027263151e-08, 'rewards/chosen': -1.7633450031280518, 'rewards/rejected': -3.4741039276123047, 'rewards/accuracies': 0.75, 'rewards/margins': 1.710758924484253, 'policy_logps/rejected': -284.31085205078125, 'policy_logps/chosen': -307.7448425292969, 'referece_logps/rejected': -249.56982421875, 'referece_logps/chosen': -290.11138916015625, 'logits/rejected': -0.9551966190338135, 'logits/chosen': -0.8499506711959839, 'epoch': 5.18}

 86%|████████▋ | 9271/10740 [45:57:02<6:35:15, 16.14s/it]


 86%|████████▋ | 9273/10740 [45:57:37<6:50:15, 16.78s/it]

 86%|████████▋ | 9274/10740 [45:57:54<6:48:12, 16.71s/it]

 86%|████████▋ | 9275/10740 [45:58:12<6:58:35, 17.14s/it]
{'loss': 0.2881, 'learning_rate': 9.602481055842226e-08, 'rewards/chosen': -1.5324431657791138, 'rewards/rejected': -4.362911224365234, 'rewards/accuracies': 1.0, 'rewards/margins': 2.830467700958252, 'policy_logps/rejected': -468.5392761230469, 'policy_logps/chosen': -378.82958984375, 'referece_logps/rejected': -424.91015625, 'referece_logps/chosen': -363.5051574707031, 'logits/rejected': 0.027905605733394623, 'logits/chosen': 0.010885782539844513, 'epoch': 5.18}


 86%|████████▋ | 9277/10740 [45:58:55<7:54:03, 19.44s/it]

 86%|████████▋ | 9278/10740 [45:59:09<7:14:03, 17.81s/it]

 86%|████████▋ | 9279/10740 [45:59:25<7:02:53, 17.37s/it]

 86%|████████▋ | 9280/10740 [45:59:43<7:09:02, 17.63s/it]
{'loss': 0.4019, 'learning_rate': 9.538107631284942e-08, 'rewards/chosen': -1.8896523714065552, 'rewards/rejected': -2.681509494781494, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7918572425842285, 'policy_logps/rejected': -347.4640808105469, 'policy_logps/chosen': -400.2021789550781, 'referece_logps/rejected': -320.6489562988281, 'referece_logps/chosen': -381.3056335449219, 'logits/rejected': -1.0562883615493774, 'logits/chosen': -1.0675941705703735, 'epoch': 5.18}


 86%|████████▋ | 9282/10740 [46:00:23<7:30:59, 18.56s/it]

 86%|████████▋ | 9283/10740 [46:00:35<6:46:31, 16.74s/it]

 86%|████████▋ | 9284/10740 [46:00:47<6:09:15, 15.22s/it]

 86%|████████▋ | 9285/10740 [46:01:00<5:55:20, 14.65s/it]

 86%|████████▋ | 9286/10740 [46:01:17<6:12:22, 15.37s/it]

 86%|████████▋ | 9287/10740 [46:01:29<5:47:09, 14.34s/it]

 86%|████████▋ | 9288/10740 [46:01:48<6:19:34, 15.69s/it]

 86%|████████▋ | 9289/10740 [46:02:06<6:37:15, 16.43s/it]
{'loss': 0.2594, 'learning_rate': 9.42275391210221e-08, 'rewards/chosen': -1.6335722208023071, 'rewards/rejected': -3.4742722511291504, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8407000303268433, 'policy_logps/rejected': -410.33856201171875, 'policy_logps/chosen': -403.8855895996094, 'referece_logps/rejected': -375.5958251953125, 'referece_logps/chosen': -387.54986572265625, 'logits/rejected': -0.6948052048683167, 'logits/chosen': -0.6155086755752563, 'epoch': 5.19}

 86%|████████▋ | 9290/10740 [46:02:27<7:07:38, 17.70s/it]

 87%|████████▋ | 9291/10740 [46:02:47<7:26:47, 18.50s/it]

 87%|████████▋ | 9292/10740 [46:03:07<7:34:08, 18.82s/it]


 87%|████████▋ | 9294/10740 [46:03:38<6:55:44, 17.25s/it]

 87%|████████▋ | 9295/10740 [46:03:56<7:00:56, 17.48s/it]

 87%|████████▋ | 9296/10740 [46:04:08<6:22:36, 15.90s/it]

 87%|████████▋ | 9297/10740 [46:04:20<5:53:55, 14.72s/it]
{'loss': 0.4449, 'learning_rate': 9.320777445317018e-08, 'rewards/chosen': -2.1312150955200195, 'rewards/rejected': -2.96510910987854, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8338941335678101, 'policy_logps/rejected': -405.7190856933594, 'policy_logps/chosen': -380.5592041015625, 'referece_logps/rejected': -376.0679626464844, 'referece_logps/chosen': -359.2469787597656, 'logits/rejected': -0.36175018548965454, 'logits/chosen': -0.3174305856227875, 'epoch': 5.19}


 87%|████████▋ | 9299/10740 [46:04:54<6:29:10, 16.20s/it]

 87%|████████▋ | 9300/10740 [46:05:14<6:56:22, 17.35s/it]
{'loss': 0.2758, 'learning_rate': 9.282672322134366e-08, 'rewards/chosen': -1.267203688621521, 'rewards/rejected': -4.223102569580078, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9558990001678467, 'policy_logps/rejected': -502.8208923339844, 'policy_logps/chosen': -460.028564453125, 'referece_logps/rejected': -460.5898742675781, 'referece_logps/chosen': -447.35650634765625, 'logits/rejected': -0.41409456729888916, 'logits/chosen': -0.40940749645233154, 'epoch': 5.2}


 87%|████████▋ | 9302/10740 [46:05:54<7:29:31, 18.76s/it]
{'loss': 0.4251, 'learning_rate': 9.257310159570997e-08, 'rewards/chosen': -2.9440512657165527, 'rewards/rejected': -5.553757667541504, 'rewards/accuracies': 0.875, 'rewards/margins': 2.609705924987793, 'policy_logps/rejected': -449.0674133300781, 'policy_logps/chosen': -397.9792175292969, 'referece_logps/rejected': -393.52984619140625, 'referece_logps/chosen': -368.5386962890625, 'logits/rejected': -0.19650065898895264, 'logits/chosen': -0.13525009155273438, 'epoch': 5.2}

 87%|████████▋ | 9303/10740 [46:06:11<7:13:23, 18.10s/it]


 87%|████████▋ | 9305/10740 [46:06:46<7:06:33, 17.83s/it]

 87%|████████▋ | 9306/10740 [46:06:56<6:14:28, 15.67s/it]

 87%|████████▋ | 9307/10740 [46:07:10<6:02:58, 15.20s/it]

 87%|████████▋ | 9308/10740 [46:07:32<6:47:51, 17.09s/it]

 87%|████████▋ | 9309/10740 [46:07:53<7:13:09, 18.16s/it]

 87%|████████▋ | 9310/10740 [46:08:11<7:11:27, 18.10s/it]

 87%|████████▋ | 9311/10740 [46:08:32<7:34:46, 19.10s/it]

 87%|████████▋ | 9312/10740 [46:08:50<7:26:45, 18.77s/it]

 87%|████████▋ | 9313/10740 [46:09:05<6:56:19, 17.50s/it]

 87%|████████▋ | 9314/10740 [46:09:24<7:11:08, 18.14s/it]
{'loss': 0.3434, 'learning_rate': 9.105830780974289e-08, 'rewards/chosen': -2.3346784114837646, 'rewards/rejected': -3.847226142883301, 'rewards/accuracies': 0.875, 'rewards/margins': 1.512547492980957, 'policy_logps/rejected': -445.1639709472656, 'policy_logps/chosen': -430.2159423828125, 'referece_logps/rejected': -406.6916809082031, 'referece_logps/chosen': -406.8691711425781, 'logits/rejected': 0.08147955685853958, 'logits/chosen': -0.0011664032936096191, 'epoch': 5.2}


 87%|████████▋ | 9316/10740 [46:09:58<6:54:47, 17.48s/it]
{'loss': 0.335, 'learning_rate': 9.080699903064937e-08, 'rewards/chosen': -1.660019040107727, 'rewards/rejected': -3.4823057651519775, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8222867250442505, 'policy_logps/rejected': -341.282470703125, 'policy_logps/chosen': -415.0403747558594, 'referece_logps/rejected': -306.45941162109375, 'referece_logps/chosen': -398.440185546875, 'logits/rejected': -0.5694493651390076, 'logits/chosen': -0.7921948432922363, 'epoch': 5.2}

 87%|████████▋ | 9317/10740 [46:10:13<6:40:34, 16.89s/it]


 87%|████████▋ | 9319/10740 [46:10:43<6:18:01, 15.96s/it]

 87%|████████▋ | 9320/10740 [46:11:03<6:46:22, 17.17s/it]

 87%|████████▋ | 9321/10740 [46:11:16<6:19:24, 16.04s/it]

 87%|████████▋ | 9322/10740 [46:11:30<6:05:52, 15.48s/it]

 87%|████████▋ | 9323/10740 [46:11:52<6:50:58, 17.40s/it]
{'loss': 0.3113, 'learning_rate': 8.993002374267222e-08, 'rewards/chosen': -2.5073115825653076, 'rewards/rejected': -3.8699536323547363, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3626415729522705, 'policy_logps/rejected': -363.07830810546875, 'policy_logps/chosen': -286.291259765625, 'referece_logps/rejected': -324.3787536621094, 'referece_logps/chosen': -261.2181396484375, 'logits/rejected': -0.8982340097427368, 'logits/chosen': -0.8588853478431702, 'epoch': 5.21}


 87%|████████▋ | 9325/10740 [46:12:30<7:10:21, 18.25s/it]

 87%|████████▋ | 9326/10740 [46:12:46<6:54:41, 17.60s/it]
{'loss': 0.4456, 'learning_rate': 8.955541855941873e-08, 'rewards/chosen': -2.019979953765869, 'rewards/rejected': -2.5873470306396484, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5673670768737793, 'policy_logps/rejected': -301.79339599609375, 'policy_logps/chosen': -386.7125549316406, 'referece_logps/rejected': -275.919921875, 'referece_logps/chosen': -366.5127868652344, 'logits/rejected': -1.2652004957199097, 'logits/chosen': -1.3081943988800049, 'epoch': 5.21}

 87%|████████▋ | 9327/10740 [46:13:00<6:24:53, 16.34s/it]


 87%|████████▋ | 9329/10740 [46:13:35<6:46:18, 17.28s/it]

 87%|████████▋ | 9330/10740 [46:13:48<6:18:51, 16.12s/it]
{'loss': 0.4483, 'learning_rate': 8.905710433493374e-08, 'rewards/chosen': -0.9835048317909241, 'rewards/rejected': -3.355269193649292, 'rewards/accuracies': 1.0, 'rewards/margins': 2.371764659881592, 'policy_logps/rejected': -410.52276611328125, 'policy_logps/chosen': -355.595703125, 'referece_logps/rejected': -376.9701232910156, 'referece_logps/chosen': -345.76068115234375, 'logits/rejected': -0.46973639726638794, 'logits/chosen': -0.45377299189567566, 'epoch': 5.21}

 87%|████████▋ | 9331/10740 [46:14:10<6:56:34, 17.74s/it]


 87%|████████▋ | 9333/10740 [46:14:52<7:39:30, 19.59s/it]

 87%|████████▋ | 9334/10740 [46:15:12<7:44:13, 19.81s/it]

 87%|████████▋ | 9335/10740 [46:15:32<7:44:40, 19.84s/it]

 87%|████████▋ | 9336/10740 [46:15:52<7:45:53, 19.91s/it]

 87%|████████▋ | 9337/10740 [46:16:04<6:49:00, 17.49s/it]

 87%|████████▋ | 9338/10740 [46:16:21<6:43:38, 17.27s/it]

 87%|████████▋ | 9339/10740 [46:16:41<7:00:00, 17.99s/it]

 87%|████████▋ | 9340/10740 [46:17:03<7:26:51, 19.15s/it]
{'loss': 0.2748, 'learning_rate': 8.781712002894204e-08, 'rewards/chosen': -0.8571151494979858, 'rewards/rejected': -2.992313861846924, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1351988315582275, 'policy_logps/rejected': -459.94769287109375, 'policy_logps/chosen': -398.9318542480469, 'referece_logps/rejected': -430.0245666503906, 'referece_logps/chosen': -390.3607177734375, 'logits/rejected': -0.456594318151474, 'logits/chosen': -0.4505939483642578, 'epoch': 5.22}

 87%|████████▋ | 9341/10740 [46:17:20<7:11:13, 18.49s/it]

 87%|████████▋ | 9342/10740 [46:17:32<6:30:08, 16.74s/it]

 87%|████████▋ | 9343/10740 [46:17:54<7:03:00, 18.17s/it]


 87%|████████▋ | 9345/10740 [46:18:33<7:21:34, 18.99s/it]
{'loss': 0.3589, 'learning_rate': 8.720023836931079e-08, 'rewards/chosen': -1.6683869361877441, 'rewards/rejected': -3.9957046508789062, 'rewards/accuracies': 1.0, 'rewards/margins': 2.327317476272583, 'policy_logps/rejected': -371.85418701171875, 'policy_logps/chosen': -329.5121154785156, 'referece_logps/rejected': -331.8971252441406, 'referece_logps/chosen': -312.8282470703125, 'logits/rejected': -0.7858482599258423, 'logits/chosen': -0.7801932096481323, 'epoch': 5.22}

 87%|████████▋ | 9346/10740 [46:18:50<7:06:43, 18.37s/it]

 87%|████████▋ | 9347/10740 [46:19:08<7:08:40, 18.46s/it]

 87%|████████▋ | 9348/10740 [46:19:28<7:16:47, 18.83s/it]

 87%|████████▋ | 9349/10740 [46:19:48<7:21:58, 19.06s/it]

 87%|████████▋ | 9350/10740 [46:20:08<7:29:04, 19.38s/it]


 87%|████████▋ | 9352/10740 [46:20:51<7:54:42, 20.52s/it]
{'loss': 0.272, 'learning_rate': 8.634009125632202e-08, 'rewards/chosen': -1.255722999572754, 'rewards/rejected': -3.6082372665405273, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3525147438049316, 'policy_logps/rejected': -478.361328125, 'policy_logps/chosen': -487.70452880859375, 'referece_logps/rejected': -442.2789611816406, 'referece_logps/chosen': -475.14727783203125, 'logits/rejected': -0.32694873213768005, 'logits/chosen': -0.48296070098876953, 'epoch': 5.22}


 87%|████████▋ | 9354/10740 [46:21:35<8:09:36, 21.20s/it]

 87%|████████▋ | 9355/10740 [46:21:55<7:58:50, 20.74s/it]
{'loss': 0.3776, 'learning_rate': 8.597270305013481e-08, 'rewards/chosen': -1.2655036449432373, 'rewards/rejected': -4.451466083526611, 'rewards/accuracies': 0.875, 'rewards/margins': 3.185962200164795, 'policy_logps/rejected': -584.5421752929688, 'policy_logps/chosen': -435.73773193359375, 'referece_logps/rejected': -540.0274658203125, 'referece_logps/chosen': -423.0826721191406, 'logits/rejected': -0.057919055223464966, 'logits/chosen': -0.3247756063938141, 'epoch': 5.23}

 87%|████████▋ | 9356/10740 [46:22:12<7:35:50, 19.76s/it]

 87%|████████▋ | 9357/10740 [46:22:30<7:21:22, 19.15s/it]

 87%|████████▋ | 9358/10740 [46:22:50<7:26:07, 19.37s/it]


 87%|████████▋ | 9360/10740 [46:23:25<7:01:36, 18.33s/it]
{'loss': 0.331, 'learning_rate': 8.53620521817735e-08, 'rewards/chosen': -1.581276774406433, 'rewards/rejected': -3.2077438831329346, 'rewards/accuracies': 0.875, 'rewards/margins': 1.626466989517212, 'policy_logps/rejected': -233.10992431640625, 'policy_logps/chosen': -215.1198272705078, 'referece_logps/rejected': -201.032470703125, 'referece_logps/chosen': -199.30706787109375, 'logits/rejected': -0.23910948634147644, 'logits/chosen': -0.22455540299415588, 'epoch': 5.23}


 87%|████████▋ | 9362/10740 [46:24:06<7:22:22, 19.26s/it]
{'loss': 0.351, 'learning_rate': 8.511837407667665e-08, 'rewards/chosen': -1.7935137748718262, 'rewards/rejected': -3.0435945987701416, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2500814199447632, 'policy_logps/rejected': -295.5121154785156, 'policy_logps/chosen': -307.4912414550781, 'referece_logps/rejected': -265.0761413574219, 'referece_logps/chosen': -289.55609130859375, 'logits/rejected': -0.017558395862579346, 'logits/chosen': -0.03315330296754837, 'epoch': 5.23}


 87%|████████▋ | 9364/10740 [46:24:41<7:03:06, 18.45s/it]
{'loss': 0.3541, 'learning_rate': 8.487502881448272e-08, 'rewards/chosen': -1.7291967868804932, 'rewards/rejected': -5.217019557952881, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4878225326538086, 'policy_logps/rejected': -514.348388671875, 'policy_logps/chosen': -399.412109375, 'referece_logps/rejected': -462.1781921386719, 'referece_logps/chosen': -382.12017822265625, 'logits/rejected': -0.6035009622573853, 'logits/chosen': -0.5797046422958374, 'epoch': 5.23}

 87%|████████▋ | 9365/10740 [46:25:03<7:22:48, 19.32s/it]

 87%|████████▋ | 9366/10740 [46:25:23<7:29:10, 19.61s/it]

 87%|████████▋ | 9367/10740 [46:25:38<7:01:22, 18.41s/it]


 87%|████████▋ | 9369/10740 [46:26:18<7:17:18, 19.14s/it]
{'loss': 0.3638, 'learning_rate': 8.426812242738523e-08, 'rewards/chosen': -1.2487331628799438, 'rewards/rejected': -3.4185502529144287, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1698169708251953, 'policy_logps/rejected': -592.5997314453125, 'policy_logps/chosen': -411.71795654296875, 'referece_logps/rejected': -558.4142456054688, 'referece_logps/chosen': -399.2306213378906, 'logits/rejected': -1.5590989589691162, 'logits/chosen': -1.571500301361084, 'epoch': 5.23}

 87%|████████▋ | 9370/10740 [46:26:33<6:48:46, 17.90s/it]

 87%|████████▋ | 9371/10740 [46:26:55<7:17:40, 19.18s/it]

 87%|████████▋ | 9372/10740 [46:27:15<7:23:00, 19.43s/it]

 87%|████████▋ | 9373/10740 [46:27:35<7:27:27, 19.64s/it]


 87%|████████▋ | 9375/10740 [46:28:15<7:26:48, 19.64s/it]

 87%|████████▋ | 9376/10740 [46:28:35<7:30:33, 19.82s/it]

 87%|████████▋ | 9377/10740 [46:28:53<7:16:39, 19.22s/it]
{'loss': 0.3975, 'learning_rate': 8.33014037586669e-08, 'rewards/chosen': -2.0531153678894043, 'rewards/rejected': -3.281442403793335, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2283270359039307, 'policy_logps/rejected': -448.48284912109375, 'policy_logps/chosen': -391.4778747558594, 'referece_logps/rejected': -415.66839599609375, 'referece_logps/chosen': -370.94671630859375, 'logits/rejected': -0.3194010555744171, 'logits/chosen': -0.3894723057746887, 'epoch': 5.24}


 87%|████████▋ | 9379/10740 [46:29:30<7:00:39, 18.54s/it]

 87%|████████▋ | 9380/10740 [46:29:49<7:08:05, 18.89s/it]

 87%|████████▋ | 9381/10740 [46:30:08<7:03:39, 18.70s/it]

 87%|████████▋ | 9382/10740 [46:30:27<7:10:38, 19.03s/it]
{'loss': 0.4438, 'learning_rate': 8.269991395304788e-08, 'rewards/chosen': -2.201856851577759, 'rewards/rejected': -3.339290142059326, 'rewards/accuracies': 0.5, 'rewards/margins': 1.1374330520629883, 'policy_logps/rejected': -556.741455078125, 'policy_logps/chosen': -425.5726623535156, 'referece_logps/rejected': -523.3485717773438, 'referece_logps/chosen': -403.5540466308594, 'logits/rejected': -0.6719140410423279, 'logits/chosen': -0.5739296078681946, 'epoch': 5.24}

 87%|████████▋ | 9383/10740 [46:30:42<6:42:29, 17.80s/it]

 87%|████████▋ | 9384/10740 [46:30:53<5:54:16, 15.68s/it]


 87%|████████▋ | 9386/10740 [46:31:22<5:38:19, 14.99s/it]
{'loss': 0.4023, 'learning_rate': 8.222022379525428e-08, 'rewards/chosen': -1.0535410642623901, 'rewards/rejected': -3.7721493244171143, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7186079025268555, 'policy_logps/rejected': -538.4153442382812, 'policy_logps/chosen': -437.6458435058594, 'referece_logps/rejected': -500.69384765625, 'referece_logps/chosen': -427.11041259765625, 'logits/rejected': -0.5242573618888855, 'logits/chosen': -0.5741873383522034, 'epoch': 5.24}

 87%|████████▋ | 9387/10740 [46:31:40<6:00:10, 15.97s/it]

 87%|████████▋ | 9388/10740 [46:32:02<6:40:51, 17.79s/it]


 87%|████████▋ | 9390/10740 [46:32:34<6:20:13, 16.90s/it]

 87%|████████▋ | 9391/10740 [46:32:48<5:58:40, 15.95s/it]
{'loss': 0.4293, 'learning_rate': 8.162248935165018e-08, 'rewards/chosen': -1.804129958152771, 'rewards/rejected': -3.035282850265503, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2311527729034424, 'policy_logps/rejected': -409.314208984375, 'policy_logps/chosen': -337.7803955078125, 'referece_logps/rejected': -378.96136474609375, 'referece_logps/chosen': -319.73907470703125, 'logits/rejected': -1.2836501598358154, 'logits/chosen': -1.3025063276290894, 'epoch': 5.25}


 87%|████████▋ | 9393/10740 [46:33:18<5:40:10, 15.15s/it]
{'loss': 0.3807, 'learning_rate': 8.138398019895786e-08, 'rewards/chosen': -1.8174856901168823, 'rewards/rejected': -2.739382028579712, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9218965172767639, 'policy_logps/rejected': -383.06591796875, 'policy_logps/chosen': -337.91058349609375, 'referece_logps/rejected': -355.672119140625, 'referece_logps/chosen': -319.7357482910156, 'logits/rejected': -0.5494441986083984, 'logits/chosen': -0.5462572574615479, 'epoch': 5.25}


 87%|████████▋ | 9395/10740 [46:33:42<4:59:22, 13.35s/it]
{'loss': 0.4451, 'learning_rate': 8.114580524777758e-08, 'rewards/chosen': -2.1793501377105713, 'rewards/rejected': -2.918252944946289, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7389028072357178, 'policy_logps/rejected': -315.50860595703125, 'policy_logps/chosen': -420.8865966796875, 'referece_logps/rejected': -286.3260803222656, 'referece_logps/chosen': -399.09307861328125, 'logits/rejected': 1.0467946529388428, 'logits/chosen': 1.0342012643814087, 'epoch': 5.25}

 87%|████████▋ | 9396/10740 [46:33:53<4:41:28, 12.57s/it]

 87%|████████▋ | 9397/10740 [46:34:07<4:50:28, 12.98s/it]

 88%|████████▊ | 9398/10740 [46:34:17<4:35:18, 12.31s/it]


 88%|████████▊ | 9400/10740 [46:34:48<5:14:37, 14.09s/it]
{'loss': 0.2793, 'learning_rate': 8.055183056978421e-08, 'rewards/chosen': -0.3129875361919403, 'rewards/rejected': -3.8975534439086914, 'rewards/accuracies': 1.0, 'rewards/margins': 3.584566116333008, 'policy_logps/rejected': -550.505859375, 'policy_logps/chosen': -677.6640625, 'referece_logps/rejected': -511.5302734375, 'referece_logps/chosen': -674.5341796875, 'logits/rejected': 0.10910521447658539, 'logits/chosen': -0.16162525117397308, 'epoch': 5.25}

 88%|████████▊ | 9401/10740 [46:35:04<5:29:37, 14.77s/it]

 88%|████████▊ | 9402/10740 [46:35:23<5:54:09, 15.88s/it]


 88%|████████▊ | 9404/10740 [46:35:50<5:28:44, 14.76s/it]

 88%|████████▊ | 9405/10740 [46:36:10<6:01:30, 16.25s/it]

 88%|████████▊ | 9406/10740 [46:36:32<6:38:56, 17.94s/it]
{'loss': 0.2651, 'learning_rate': 7.984182073438117e-08, 'rewards/chosen': -1.8193684816360474, 'rewards/rejected': -4.389511585235596, 'rewards/accuracies': 0.875, 'rewards/margins': 2.570143222808838, 'policy_logps/rejected': -402.29510498046875, 'policy_logps/chosen': -459.3736572265625, 'referece_logps/rejected': -358.39996337890625, 'referece_logps/chosen': -441.1800231933594, 'logits/rejected': -0.4604915976524353, 'logits/chosen': -0.5577136278152466, 'epoch': 5.25}


 88%|████████▊ | 9408/10740 [46:37:08<6:43:57, 18.20s/it]
{'loss': 0.3507, 'learning_rate': 7.960582019969375e-08, 'rewards/chosen': -1.4228159189224243, 'rewards/rejected': -2.85365891456604, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4308428764343262, 'policy_logps/rejected': -372.7698669433594, 'policy_logps/chosen': -375.79876708984375, 'referece_logps/rejected': -344.2332763671875, 'referece_logps/chosen': -361.57061767578125, 'logits/rejected': -0.24274542927742004, 'logits/chosen': -0.25241196155548096, 'epoch': 5.26}


 88%|████████▊ | 9410/10740 [46:37:48<7:02:34, 19.06s/it]
{'loss': 0.3106, 'learning_rate': 7.937015451342999e-08, 'rewards/chosen': -1.8367844820022583, 'rewards/rejected': -4.926121711730957, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0893378257751465, 'policy_logps/rejected': -526.3490600585938, 'policy_logps/chosen': -414.7797546386719, 'referece_logps/rejected': -477.0877990722656, 'referece_logps/chosen': -396.4119567871094, 'logits/rejected': -0.154450923204422, 'logits/chosen': -0.23335407674312592, 'epoch': 5.26}

 88%|████████▊ | 9411/10740 [46:38:10<7:16:45, 19.72s/it]

 88%|████████▊ | 9412/10740 [46:38:30<7:17:32, 19.77s/it]

 88%|████████▊ | 9413/10740 [46:38:49<7:16:41, 19.75s/it]

 88%|████████▊ | 9414/10740 [46:39:12<7:35:07, 20.59s/it]


 88%|████████▊ | 9416/10740 [46:39:48<7:12:46, 19.61s/it]

 88%|████████▊ | 9417/10740 [46:40:08<7:15:43, 19.76s/it]

 88%|████████▊ | 9418/10740 [46:40:24<6:50:28, 18.63s/it]
{'loss': 0.4197, 'learning_rate': 7.843084196553484e-08, 'rewards/chosen': -2.0829906463623047, 'rewards/rejected': -2.8193838596343994, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7363930344581604, 'policy_logps/rejected': -504.4757080078125, 'policy_logps/chosen': -398.7828369140625, 'referece_logps/rejected': -476.2818908691406, 'referece_logps/chosen': -377.95294189453125, 'logits/rejected': -0.7053917646408081, 'logits/chosen': -0.6529103517532349, 'epoch': 5.26}

 88%|████████▊ | 9419/10740 [46:40:41<6:38:38, 18.11s/it]

 88%|████████▊ | 9420/10740 [46:41:02<6:54:38, 18.85s/it]


 88%|████████▊ | 9422/10740 [46:41:45<7:17:58, 19.94s/it]
{'loss': 0.3509, 'learning_rate': 7.796319700551512e-08, 'rewards/chosen': -2.2182562351226807, 'rewards/rejected': -3.711616277694702, 'rewards/accuracies': 0.75, 'rewards/margins': 1.493359923362732, 'policy_logps/rejected': -424.2850341796875, 'policy_logps/chosen': -289.921630859375, 'referece_logps/rejected': -387.1689453125, 'referece_logps/chosen': -267.7390441894531, 'logits/rejected': -0.4362070560455322, 'logits/chosen': -0.39262154698371887, 'epoch': 5.26}

 88%|████████▊ | 9423/10740 [46:42:04<7:16:16, 19.88s/it]

 88%|████████▊ | 9424/10740 [46:42:22<6:58:46, 19.09s/it]


 88%|████████▊ | 9426/10740 [46:43:00<7:01:50, 19.26s/it]

 88%|████████▊ | 9427/10740 [46:43:21<7:07:06, 19.52s/it]
{'loss': 0.3395, 'learning_rate': 7.738052776878068e-08, 'rewards/chosen': -1.3765907287597656, 'rewards/rejected': -2.14475154876709, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7681608200073242, 'policy_logps/rejected': -382.2234191894531, 'policy_logps/chosen': -428.14178466796875, 'referece_logps/rejected': -360.77587890625, 'referece_logps/chosen': -414.37591552734375, 'logits/rejected': -0.4690167307853699, 'logits/chosen': -0.4688551425933838, 'epoch': 5.27}


 88%|████████▊ | 9429/10740 [46:43:57<6:47:28, 18.65s/it]
{'loss': 0.3408, 'learning_rate': 7.714804740147241e-08, 'rewards/chosen': -1.577317237854004, 'rewards/rejected': -4.833014011383057, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2556967735290527, 'policy_logps/rejected': -348.4976501464844, 'policy_logps/chosen': -279.59423828125, 'referece_logps/rejected': -300.1675109863281, 'referece_logps/chosen': -263.8210754394531, 'logits/rejected': -0.47241947054862976, 'logits/chosen': -0.580810546875, 'epoch': 5.27}

 88%|████████▊ | 9430/10740 [46:44:16<6:48:41, 18.72s/it]


 88%|████████▊ | 9432/10740 [46:44:53<6:48:55, 18.76s/it]

 88%|████████▊ | 9433/10740 [46:45:09<6:31:15, 17.96s/it]
{'loss': 0.47, 'learning_rate': 7.668409397906939e-08, 'rewards/chosen': -3.2695627212524414, 'rewards/rejected': -3.235004425048828, 'rewards/accuracies': 0.5, 'rewards/margins': -0.034558236598968506, 'policy_logps/rejected': -581.6985473632812, 'policy_logps/chosen': -352.20477294921875, 'referece_logps/rejected': -549.3485717773438, 'referece_logps/chosen': -319.5091552734375, 'logits/rejected': -0.3457498550415039, 'logits/chosen': -0.1672714352607727, 'epoch': 5.27}

 88%|████████▊ | 9434/10740 [46:45:25<6:20:49, 17.50s/it]


 88%|████████▊ | 9436/10740 [46:46:09<7:09:06, 19.74s/it]
{'loss': 0.401, 'learning_rate': 7.63370106427006e-08, 'rewards/chosen': -1.731070876121521, 'rewards/rejected': -2.815340042114258, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0842690467834473, 'policy_logps/rejected': -504.767822265625, 'policy_logps/chosen': -481.2902526855469, 'referece_logps/rejected': -476.61444091796875, 'referece_logps/chosen': -463.9795227050781, 'logits/rejected': -0.4386810064315796, 'logits/chosen': -0.42307940125465393, 'epoch': 5.27}

 88%|████████▊ | 9437/10740 [46:46:30<7:16:02, 20.08s/it]

 88%|████████▊ | 9438/10740 [46:46:43<6:34:57, 18.20s/it]

 88%|████████▊ | 9439/10740 [46:47:01<6:33:36, 18.15s/it]

 88%|████████▊ | 9440/10740 [46:47:22<6:48:17, 18.84s/it]

 88%|████████▊ | 9441/10740 [46:47:35<6:13:34, 17.26s/it]

 88%|████████▊ | 9442/10740 [46:47:54<6:21:07, 17.62s/it]

 88%|████████▊ | 9443/10740 [46:48:10<6:14:01, 17.30s/it]

 88%|████████▊ | 9444/10740 [46:48:33<6:45:22, 18.77s/it]

 88%|████████▊ | 9445/10740 [46:48:51<6:39:24, 18.51s/it]

 88%|████████▊ | 9446/10740 [46:49:08<6:32:15, 18.19s/it]

 88%|████████▊ | 9447/10740 [46:49:29<6:53:09, 19.17s/it]


 88%|████████▊ | 9449/10740 [46:50:09<6:56:25, 19.35s/it]

 88%|████████▊ | 9450/10740 [46:50:29<6:58:18, 19.46s/it]

 88%|████████▊ | 9451/10740 [46:50:49<7:01:10, 19.60s/it]
{'loss': 0.3761, 'learning_rate': 7.461294089165838e-08, 'rewards/chosen': -1.289325475692749, 'rewards/rejected': -3.1365020275115967, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8471766710281372, 'policy_logps/rejected': -341.0369567871094, 'policy_logps/chosen': -359.0069274902344, 'referece_logps/rejected': -309.6719665527344, 'referece_logps/chosen': -346.1136474609375, 'logits/rejected': -0.10767531394958496, 'logits/chosen': -0.2082967609167099, 'epoch': 5.28}

 88%|████████▊ | 9452/10740 [46:51:02<6:20:03, 17.70s/it]

 88%|████████▊ | 9453/10740 [46:51:21<6:25:22, 17.97s/it]

 88%|████████▊ | 9454/10740 [46:51:41<6:36:56, 18.52s/it]

 88%|████████▊ | 9455/10740 [46:52:02<6:54:42, 19.36s/it]

 88%|████████▊ | 9456/10740 [46:52:20<6:47:07, 19.02s/it]


 88%|████████▊ | 9458/10740 [46:52:56<6:30:52, 18.29s/it]

 88%|████████▊ | 9459/10740 [46:53:15<6:40:32, 18.76s/it]

 88%|████████▊ | 9460/10740 [46:53:27<5:57:15, 16.75s/it]

 88%|████████▊ | 9461/10740 [46:53:50<6:31:00, 18.34s/it]
{'loss': 0.2504, 'learning_rate': 7.347407965562635e-08, 'rewards/chosen': -1.165517807006836, 'rewards/rejected': -4.394705295562744, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2291877269744873, 'policy_logps/rejected': -302.6723327636719, 'policy_logps/chosen': -268.80889892578125, 'referece_logps/rejected': -258.72528076171875, 'referece_logps/chosen': -257.1536865234375, 'logits/rejected': -0.8163298964500427, 'logits/chosen': -0.9282616972923279, 'epoch': 5.29}

 88%|████████▊ | 9462/10740 [46:54:11<6:51:09, 19.30s/it]


 88%|████████▊ | 9464/10740 [46:54:40<5:50:23, 16.48s/it]
{'loss': 0.3378, 'learning_rate': 7.313406407548085e-08, 'rewards/chosen': -2.0068349838256836, 'rewards/rejected': -3.746598243713379, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7397631406784058, 'policy_logps/rejected': -352.66845703125, 'policy_logps/chosen': -296.4063415527344, 'referece_logps/rejected': -315.2024841308594, 'referece_logps/chosen': -276.3380126953125, 'logits/rejected': -0.8206096887588501, 'logits/chosen': -0.8442716598510742, 'epoch': 5.29}

 88%|████████▊ | 9465/10740 [46:54:52<5:22:58, 15.20s/it]

 88%|████████▊ | 9466/10740 [46:55:10<5:43:44, 16.19s/it]


 88%|████████▊ | 9468/10740 [46:55:54<6:45:59, 19.15s/it]
{'loss': 0.3861, 'learning_rate': 7.268189022678395e-08, 'rewards/chosen': -1.910005807876587, 'rewards/rejected': -2.9739489555358887, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0639430284500122, 'policy_logps/rejected': -438.5146789550781, 'policy_logps/chosen': -477.0348815917969, 'referece_logps/rejected': -408.7751770019531, 'referece_logps/chosen': -457.934814453125, 'logits/rejected': -0.3273536264896393, 'logits/chosen': -0.39346620440483093, 'epoch': 5.29}


 88%|████████▊ | 9470/10740 [46:56:21<5:47:53, 16.44s/it]

 88%|████████▊ | 9471/10740 [46:56:38<5:46:22, 16.38s/it]
{'loss': 0.3027, 'learning_rate': 7.234364539373972e-08, 'rewards/chosen': -1.961375117301941, 'rewards/rejected': -2.971787929534912, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0104129314422607, 'policy_logps/rejected': -483.627197265625, 'policy_logps/chosen': -432.81854248046875, 'referece_logps/rejected': -453.9093017578125, 'referece_logps/chosen': -413.20477294921875, 'logits/rejected': 0.0426064133644104, 'logits/chosen': 0.1195317804813385, 'epoch': 5.29}

 88%|████████▊ | 9472/10740 [46:56:57<6:06:45, 17.35s/it]


 88%|████████▊ | 9474/10740 [46:57:35<6:29:23, 18.45s/it]
{'loss': 0.4592, 'learning_rate': 7.200615991423353e-08, 'rewards/chosen': -1.3619810342788696, 'rewards/rejected': -2.6607933044433594, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2988121509552002, 'policy_logps/rejected': -304.9898986816406, 'policy_logps/chosen': -302.72296142578125, 'referece_logps/rejected': -278.3819580078125, 'referece_logps/chosen': -289.1031494140625, 'logits/rejected': -0.5204322934150696, 'logits/chosen': -0.5250579118728638, 'epoch': 5.29}

 88%|████████▊ | 9475/10740 [46:57:49<5:57:39, 16.96s/it]

 88%|████████▊ | 9476/10740 [46:58:09<6:15:09, 17.81s/it]

 88%|████████▊ | 9477/10740 [46:58:28<6:25:22, 18.31s/it]

 88%|████████▊ | 9478/10740 [46:58:51<6:51:08, 19.55s/it]

 88%|████████▊ | 9479/10740 [46:59:11<6:55:14, 19.76s/it]


 88%|████████▊ | 9481/10740 [46:59:42<6:13:33, 17.80s/it]
{'loss': 0.3543, 'learning_rate': 7.122164838756761e-08, 'rewards/chosen': -2.717351198196411, 'rewards/rejected': -3.353400707244873, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6360492706298828, 'policy_logps/rejected': -485.52081298828125, 'policy_logps/chosen': -517.95263671875, 'referece_logps/rejected': -451.98681640625, 'referece_logps/chosen': -490.77911376953125, 'logits/rejected': -0.08102837949991226, 'logits/chosen': -0.2219194918870926, 'epoch': 5.3}

 88%|████████▊ | 9482/10740 [47:00:03<6:34:39, 18.82s/it]

 88%|████████▊ | 9483/10740 [47:00:20<6:24:19, 18.35s/it]

 88%|████████▊ | 9484/10740 [47:00:36<6:07:27, 17.55s/it]

 88%|████████▊ | 9485/10740 [47:00:53<6:04:17, 17.42s/it]

 88%|████████▊ | 9486/10740 [47:01:10<5:57:45, 17.12s/it]

 88%|████████▊ | 9487/10740 [47:01:29<6:08:54, 17.67s/it]

 88%|████████▊ | 9488/10740 [47:01:48<6:21:04, 18.26s/it]


 88%|████████▊ | 9490/10740 [47:02:28<6:33:23, 18.88s/it]

 88%|████████▊ | 9491/10740 [47:02:50<6:51:58, 19.79s/it]
{'loss': 0.3354, 'learning_rate': 7.010809884194047e-08, 'rewards/chosen': -1.715175986289978, 'rewards/rejected': -2.068281412124634, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3531053066253662, 'policy_logps/rejected': -363.54248046875, 'policy_logps/chosen': -294.0476379394531, 'referece_logps/rejected': -342.85968017578125, 'referece_logps/chosen': -276.8958740234375, 'logits/rejected': -0.6055628061294556, 'logits/chosen': -0.6078631281852722, 'epoch': 5.3}

 88%|████████▊ | 9492/10740 [47:03:05<6:23:40, 18.45s/it]

 88%|████████▊ | 9493/10740 [47:03:22<6:09:50, 17.80s/it]

 88%|████████▊ | 9494/10740 [47:03:39<6:04:46, 17.57s/it]

 88%|████████▊ | 9495/10740 [47:03:59<6:24:16, 18.52s/it]

 88%|████████▊ | 9496/10740 [47:04:18<6:23:24, 18.49s/it]

 88%|████████▊ | 9497/10740 [47:04:37<6:30:50, 18.87s/it]

 88%|████████▊ | 9498/10740 [47:04:59<6:47:17, 19.68s/it]

 88%|████████▊ | 9499/10740 [47:05:19<6:50:41, 19.86s/it]

 88%|████████▊ | 9500/10740 [47:05:35<6:23:48, 18.57s/it]

 88%|████████▊ | 9501/10740 [47:06:07<7:48:10, 22.67s/it]

 88%|████████▊ | 9502/10740 [47:06:26<7:24:38, 21.55s/it]

 88%|████████▊ | 9503/10740 [47:06:46<7:16:37, 21.18s/it]

 88%|████████▊ | 9504/10740 [47:07:02<6:41:38, 19.50s/it]

 89%|████████▊ | 9505/10740 [47:07:22<6:44:07, 19.63s/it]

 89%|████████▊ | 9506/10740 [47:07:44<7:00:41, 20.46s/it]

 89%|████████▊ | 9507/10740 [47:08:00<6:33:19, 19.14s/it]

 89%|████████▊ | 9508/10740 [47:08:14<6:01:06, 17.59s/it]

 89%|████████▊ | 9509/10740 [47:08:34<6:11:32, 18.11s/it]

 89%|████████▊ | 9510/10740 [47:08:49<5:54:19, 17.28s/it]

 89%|████████▊ | 9511/10740 [47:09:04<5:40:19, 16.61s/it]

 89%|████████▊ | 9512/10740 [47:09:24<5:58:23, 17.51s/it]

 89%|████████▊ | 9513/10740 [47:09:43<6:11:16, 18.16s/it]

 89%|████████▊ | 9514/10740 [47:10:02<6:11:33, 18.18s/it]

 89%|████████▊ | 9515/10740 [47:10:17<5:51:45, 17.23s/it]

 89%|████████▊ | 9516/10740 [47:10:35<5:58:22, 17.57s/it]

 89%|████████▊ | 9517/10740 [47:10:53<6:02:43, 17.79s/it]

 89%|████████▊ | 9518/10740 [47:11:15<6:26:16, 18.97s/it]

 89%|████████▊ | 9519/10740 [47:11:32<6:15:23, 18.45s/it]

 89%|████████▊ | 9520/10740 [47:11:52<6:21:50, 18.78s/it]

 89%|████████▊ | 9521/10740 [47:12:08<6:04:01, 17.92s/it]

 89%|████████▊ | 9522/10740 [47:12:21<5:35:49, 16.54s/it]

 89%|████████▊ | 9523/10740 [47:12:40<5:53:39, 17.44s/it]

 89%|████████▊ | 9524/10740 [47:12:54<5:32:33, 16.41s/it]

 89%|████████▊ | 9525/10740 [47:13:16<6:05:04, 18.03s/it]

 89%|████████▊ | 9526/10740 [47:13:34<6:06:01, 18.09s/it]

 89%|████████▊ | 9527/10740 [47:13:54<6:15:17, 18.56s/it]

 89%|████████▊ | 9528/10740 [47:14:10<5:59:46, 17.81s/it]

 89%|████████▊ | 9529/10740 [47:14:30<6:11:08, 18.39s/it]

 89%|████████▊ | 9530/10740 [47:14:47<6:05:28, 18.12s/it]

 89%|████████▊ | 9531/10740 [47:15:09<6:24:35, 19.09s/it]

 89%|████████▉ | 9532/10740 [47:15:27<6:16:05, 18.68s/it]

 89%|████████▉ | 9533/10740 [47:15:43<6:02:06, 18.00s/it]

 89%|████████▉ | 9534/10740 [47:16:04<6:18:55, 18.85s/it]

 89%|████████▉ | 9535/10740 [47:16:22<6:17:32, 18.80s/it]

 89%|████████▉ | 9536/10740 [47:16:42<6:24:23, 19.16s/it]

 89%|████████▉ | 9537/10740 [47:17:02<6:27:34, 19.33s/it]

 89%|████████▉ | 9538/10740 [47:17:22<6:32:09, 19.58s/it]

 89%|████████▉ | 9539/10740 [47:17:47<7:04:22, 21.20s/it]

 89%|████████▉ | 9540/10740 [47:18:09<7:04:27, 21.22s/it]

 89%|████████▉ | 9541/10740 [47:18:28<6:50:45, 20.55s/it]

 89%|████████▉ | 9542/10740 [47:18:48<6:48:30, 20.46s/it]

 89%|████████▉ | 9543/10740 [47:19:07<6:41:59, 20.15s/it]

 89%|████████▉ | 9544/10740 [47:19:27<6:39:36, 20.05s/it]

 89%|████████▉ | 9545/10740 [47:19:46<6:30:27, 19.60s/it]

 89%|████████▉ | 9546/10740 [47:19:59<5:53:59, 17.79s/it]

 89%|████████▉ | 9547/10740 [47:20:11<5:16:07, 15.90s/it]

 89%|████████▉ | 9548/10740 [47:20:28<5:26:51, 16.45s/it]

 89%|████████▉ | 9549/10740 [47:20:49<5:48:42, 17.57s/it]

 89%|████████▉ | 9550/10740 [47:21:09<6:05:40, 18.44s/it]


 89%|████████▉ | 9552/10740 [47:21:34<5:04:34, 15.38s/it]

 89%|████████▉ | 9553/10740 [47:21:46<4:43:52, 14.35s/it]

 89%|████████▉ | 9554/10740 [47:22:04<5:05:10, 15.44s/it]

 89%|████████▉ | 9555/10740 [47:22:25<5:37:40, 17.10s/it]

 89%|████████▉ | 9556/10740 [47:22:40<5:25:32, 16.50s/it]

 89%|████████▉ | 9557/10740 [47:23:01<5:54:22, 17.97s/it]

 89%|████████▉ | 9558/10740 [47:23:14<5:25:32, 16.52s/it]

 89%|████████▉ | 9559/10740 [47:23:28<5:08:07, 15.65s/it]

 89%|████████▉ | 9560/10740 [47:23:48<5:30:53, 16.83s/it]

 89%|████████▉ | 9561/10740 [47:24:11<6:08:36, 18.76s/it]

 89%|████████▉ | 9562/10740 [47:24:28<5:57:34, 18.21s/it]

 89%|████████▉ | 9563/10740 [47:24:44<5:46:29, 17.66s/it]

 89%|████████▉ | 9564/10740 [47:25:06<6:11:12, 18.94s/it]

 89%|████████▉ | 9565/10740 [47:25:25<6:07:30, 18.77s/it]

 89%|████████▉ | 9566/10740 [47:25:46<6:23:30, 19.60s/it]
{'loss': 0.3041, 'learning_rate': 6.202674971576016e-08, 'rewards/chosen': -1.9534164667129517, 'rewards/rejected': -4.023690223693848, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0702738761901855, 'policy_logps/rejected': -360.28179931640625, 'policy_logps/chosen': -282.96136474609375, 'referece_logps/rejected': -320.0448913574219, 'referece_logps/chosen': -263.42718505859375, 'logits/rejected': -0.7208747863769531, 'logits/chosen': -0.7468584775924683, 'epoch': 5.34}


 89%|████████▉ | 9568/10740 [47:26:20<5:50:35, 17.95s/it]
{'loss': 0.3539, 'learning_rate': 6.181779792585096e-08, 'rewards/chosen': -2.138394355773926, 'rewards/rejected': -3.4333560466766357, 'rewards/accuracies': 0.625, 'rewards/margins': 1.294961929321289, 'policy_logps/rejected': -372.6189270019531, 'policy_logps/chosen': -353.6582946777344, 'referece_logps/rejected': -338.28533935546875, 'referece_logps/chosen': -332.27435302734375, 'logits/rejected': -1.1619120836257935, 'logits/chosen': -1.0879946947097778, 'epoch': 5.35}


 89%|████████▉ | 9570/10740 [47:26:51<5:28:39, 16.85s/it]

 89%|████████▉ | 9571/10740 [47:27:10<5:42:57, 17.60s/it]

 89%|████████▉ | 9572/10740 [47:27:32<6:07:16, 18.87s/it]

 89%|████████▉ | 9573/10740 [47:27:50<6:04:40, 18.75s/it]

 89%|████████▉ | 9574/10740 [47:28:06<5:43:53, 17.70s/it]

 89%|████████▉ | 9575/10740 [47:28:20<5:21:55, 16.58s/it]

 89%|████████▉ | 9576/10740 [47:28:39<5:38:33, 17.45s/it]

 89%|████████▉ | 9577/10740 [47:28:55<5:28:30, 16.95s/it]

 89%|████████▉ | 9578/10740 [47:29:13<5:35:06, 17.30s/it]

 89%|████████▉ | 9579/10740 [47:29:24<4:59:03, 15.46s/it]

 89%|████████▉ | 9580/10740 [47:29:42<5:13:09, 16.20s/it]

 89%|████████▉ | 9581/10740 [47:30:02<5:35:11, 17.35s/it]

 89%|████████▉ | 9582/10740 [47:30:15<5:10:09, 16.07s/it]

 89%|████████▉ | 9583/10740 [47:30:30<5:00:57, 15.61s/it]
{'loss': 0.3685, 'learning_rate': 6.026154430239738e-08, 'rewards/chosen': -1.3884700536727905, 'rewards/rejected': -2.3273682594299316, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9388983845710754, 'policy_logps/rejected': -310.01739501953125, 'policy_logps/chosen': -385.946044921875, 'referece_logps/rejected': -286.7436828613281, 'referece_logps/chosen': -372.0613708496094, 'logits/rejected': -0.8513748645782471, 'logits/chosen': -1.045621395111084, 'epoch': 5.35}


 89%|████████▉ | 9585/10740 [47:31:09<5:35:51, 17.45s/it]

 89%|████████▉ | 9586/10740 [47:31:28<5:44:24, 17.91s/it]

 89%|████████▉ | 9587/10740 [47:31:48<5:56:47, 18.57s/it]

 89%|████████▉ | 9588/10740 [47:32:04<5:41:15, 17.77s/it]
{'loss': 0.279, 'learning_rate': 5.974706510179305e-08, 'rewards/chosen': -1.0565621852874756, 'rewards/rejected': -3.7851340770721436, 'rewards/accuracies': 0.875, 'rewards/margins': 2.728571891784668, 'policy_logps/rejected': -413.95867919921875, 'policy_logps/chosen': -386.7312927246094, 'referece_logps/rejected': -376.1073303222656, 'referece_logps/chosen': -376.1656799316406, 'logits/rejected': -0.14751802384853363, 'logits/chosen': -0.253299355506897, 'epoch': 5.36}


 89%|████████▉ | 9590/10740 [47:32:45<6:07:10, 19.16s/it]

 89%|████████▉ | 9591/10740 [47:32:57<5:26:24, 17.04s/it]

 89%|████████▉ | 9592/10740 [47:33:17<5:42:40, 17.91s/it]

 89%|████████▉ | 9593/10740 [47:33:34<5:33:48, 17.46s/it]

 89%|████████▉ | 9594/10740 [47:33:51<5:33:47, 17.48s/it]

 89%|████████▉ | 9595/10740 [47:34:09<5:37:48, 17.70s/it]
{'loss': 0.4388, 'learning_rate': 5.903038625044332e-08, 'rewards/chosen': -1.4156813621520996, 'rewards/rejected': -4.218597412109375, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8029160499572754, 'policy_logps/rejected': -506.7166748046875, 'policy_logps/chosen': -469.84228515625, 'referece_logps/rejected': -464.53070068359375, 'referece_logps/chosen': -455.68548583984375, 'logits/rejected': 0.008738294243812561, 'logits/chosen': 0.007158145308494568, 'epoch': 5.36}


 89%|████████▉ | 9597/10740 [47:34:45<5:41:29, 17.93s/it]

 89%|████████▉ | 9598/10740 [47:35:07<6:04:17, 19.14s/it]

 89%|████████▉ | 9599/10740 [47:35:27<6:07:37, 19.33s/it]

 89%|████████▉ | 9600/10740 [47:35:40<5:31:21, 17.44s/it]

 89%|████████▉ | 9601/10740 [47:36:01<5:50:54, 18.49s/it]

 89%|████████▉ | 9602/10740 [47:36:19<5:47:42, 18.33s/it]

 89%|████████▉ | 9603/10740 [47:36:38<5:50:49, 18.51s/it]

 89%|████████▉ | 9604/10740 [47:36:55<5:44:54, 18.22s/it]
{'loss': 0.3094, 'learning_rate': 5.811510446714718e-08, 'rewards/chosen': -2.0886905193328857, 'rewards/rejected': -4.601161479949951, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5124707221984863, 'policy_logps/rejected': -574.7781982421875, 'policy_logps/chosen': -521.2842407226562, 'referece_logps/rejected': -528.7665405273438, 'referece_logps/chosen': -500.3973388671875, 'logits/rejected': 0.25859418511390686, 'logits/chosen': 0.3784139156341553, 'epoch': 5.37}


 89%|████████▉ | 9606/10740 [47:37:32<5:47:20, 18.38s/it]

 89%|████████▉ | 9607/10740 [47:37:52<5:57:35, 18.94s/it]

 89%|████████▉ | 9608/10740 [47:38:08<5:40:23, 18.04s/it]
{'loss': 0.2533, 'learning_rate': 5.7710539498416535e-08, 'rewards/chosen': -1.6580036878585815, 'rewards/rejected': -3.7984392642974854, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1404356956481934, 'policy_logps/rejected': -496.40789794921875, 'policy_logps/chosen': -471.6620788574219, 'referece_logps/rejected': -458.4234924316406, 'referece_logps/chosen': -455.0820007324219, 'logits/rejected': 0.2862154245376587, 'logits/chosen': 0.2212432622909546, 'epoch': 5.37}


 89%|████████▉ | 9610/10740 [47:38:44<5:37:17, 17.91s/it]

 89%|████████▉ | 9611/10740 [47:39:06<6:00:44, 19.17s/it]

 89%|████████▉ | 9612/10740 [47:39:24<5:54:49, 18.87s/it]

 90%|████████▉ | 9613/10740 [47:39:46<6:12:51, 19.85s/it]

 90%|████████▉ | 9613/10740 [47:39:47<6:12:51, 19.85s/it]

 90%|████████▉ | 9614/10740 [47:40:09<6:25:28, 20.54s/it]


 90%|████████▉ | 9616/10740 [47:40:42<5:48:10, 18.59s/it]

 90%|████████▉ | 9617/10740 [47:41:02<5:57:22, 19.09s/it]

 90%|████████▉ | 9618/10740 [47:41:18<5:39:15, 18.14s/it]
{'loss': 0.3461, 'learning_rate': 5.6705127606475386e-08, 'rewards/chosen': -1.6272501945495605, 'rewards/rejected': -3.053534507751465, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4262844324111938, 'policy_logps/rejected': -457.25341796875, 'policy_logps/chosen': -372.322265625, 'referece_logps/rejected': -426.7180480957031, 'referece_logps/chosen': -356.04974365234375, 'logits/rejected': 0.22047783434391022, 'logits/chosen': 0.19637709856033325, 'epoch': 5.37}


 90%|████████▉ | 9620/10740 [47:41:58<5:56:27, 19.10s/it]
{'loss': 0.4525, 'learning_rate': 5.6505074475848623e-08, 'rewards/chosen': -1.3148009777069092, 'rewards/rejected': -2.3854496479034424, 'rewards/accuracies': 0.75, 'rewards/margins': 1.070648431777954, 'policy_logps/rejected': -450.35498046875, 'policy_logps/chosen': -420.53582763671875, 'referece_logps/rejected': -426.50054931640625, 'referece_logps/chosen': -407.3877868652344, 'logits/rejected': -1.0559521913528442, 'logits/chosen': -1.5557050704956055, 'epoch': 5.37}


 90%|████████▉ | 9622/10740 [47:42:31<5:28:40, 17.64s/it]

 90%|████████▉ | 9623/10740 [47:42:52<5:49:38, 18.78s/it]

 90%|████████▉ | 9624/10740 [47:43:10<5:45:14, 18.56s/it]

 90%|████████▉ | 9625/10740 [47:43:30<5:54:00, 19.05s/it]

 90%|████████▉ | 9626/10740 [47:43:47<5:38:47, 18.25s/it]

 90%|████████▉ | 9627/10740 [47:44:08<5:53:36, 19.06s/it]
{'loss': 0.4758, 'learning_rate': 5.5807592110179936e-08, 'rewards/chosen': -1.4730424880981445, 'rewards/rejected': -3.515237331390381, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0421950817108154, 'policy_logps/rejected': -347.1903381347656, 'policy_logps/chosen': -398.1159362792969, 'referece_logps/rejected': -312.0379638671875, 'referece_logps/chosen': -383.3855285644531, 'logits/rejected': -0.5133965611457825, 'logits/chosen': -0.6020968556404114, 'epoch': 5.38}


 90%|████████▉ | 9629/10740 [47:44:36<5:11:47, 16.84s/it]

 90%|████████▉ | 9630/10740 [47:44:54<5:17:30, 17.16s/it]

 90%|████████▉ | 9631/10740 [47:45:06<4:47:20, 15.55s/it]

 90%|████████▉ | 9632/10740 [47:45:26<5:13:09, 16.96s/it]

 90%|████████▉ | 9633/10740 [47:45:38<4:47:03, 15.56s/it]

 90%|████████▉ | 9634/10740 [47:45:59<5:12:10, 16.94s/it]

 90%|████████▉ | 9635/10740 [47:46:20<5:36:45, 18.29s/it]
{'loss': 0.3853, 'learning_rate': 5.501562218197853e-08, 'rewards/chosen': -0.8577152490615845, 'rewards/rejected': -3.217512607574463, 'rewards/accuracies': 0.625, 'rewards/margins': 2.359797477722168, 'policy_logps/rejected': -446.3284606933594, 'policy_logps/chosen': -520.1102905273438, 'referece_logps/rejected': -414.1533203125, 'referece_logps/chosen': -511.5331726074219, 'logits/rejected': 0.054792165756225586, 'logits/chosen': 0.08663240075111389, 'epoch': 5.38}


 90%|████████▉ | 9637/10740 [47:46:50<5:04:00, 16.54s/it]
{'loss': 0.3523, 'learning_rate': 5.4818489006609016e-08, 'rewards/chosen': -1.8922884464263916, 'rewards/rejected': -3.3610949516296387, 'rewards/accuracies': 0.75, 'rewards/margins': 1.468806505203247, 'policy_logps/rejected': -306.7029113769531, 'policy_logps/chosen': -324.5147705078125, 'referece_logps/rejected': -273.09197998046875, 'referece_logps/chosen': -305.59185791015625, 'logits/rejected': -0.8207588195800781, 'logits/chosen': -0.8990697264671326, 'epoch': 5.38}

 90%|████████▉ | 9638/10740 [47:47:11<5:30:43, 18.01s/it]

 90%|████████▉ | 9639/10740 [47:47:33<5:51:10, 19.14s/it]


 90%|████████▉ | 9641/10740 [47:48:12<5:51:52, 19.21s/it]

 90%|████████▉ | 9642/10740 [47:48:32<5:58:23, 19.58s/it]

 90%|████████▉ | 9643/10740 [47:48:52<6:00:10, 19.70s/it]

 90%|████████▉ | 9644/10740 [47:49:14<6:11:41, 20.35s/it]

 90%|████████▉ | 9645/10740 [47:49:28<5:38:14, 18.53s/it]

 90%|████████▉ | 9646/10740 [47:49:49<5:47:21, 19.05s/it]

 90%|████████▉ | 9647/10740 [47:50:06<5:39:18, 18.63s/it]

 90%|████████▉ | 9648/10740 [47:50:28<5:58:03, 19.67s/it]

 90%|████████▉ | 9649/10740 [47:50:42<5:25:40, 17.91s/it]

 90%|████████▉ | 9650/10740 [47:51:04<5:48:28, 19.18s/it]

 90%|████████▉ | 9651/10740 [47:51:22<5:39:57, 18.73s/it]
{'loss': 0.2955, 'learning_rate': 5.344818903585679e-08, 'rewards/chosen': -1.888411283493042, 'rewards/rejected': -2.8985531330108643, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0101418495178223, 'policy_logps/rejected': -404.70111083984375, 'policy_logps/chosen': -305.098876953125, 'referece_logps/rejected': -375.715576171875, 'referece_logps/chosen': -286.21478271484375, 'logits/rejected': -0.9296975135803223, 'logits/chosen': -0.8866382241249084, 'epoch': 5.39}

 90%|████████▉ | 9652/10740 [47:51:42<5:43:40, 18.95s/it]


 90%|████████▉ | 9654/10740 [47:52:21<5:49:03, 19.29s/it]

 90%|████████▉ | 9655/10740 [47:52:39<5:42:39, 18.95s/it]

 90%|████████▉ | 9656/10740 [47:52:57<5:38:01, 18.71s/it]

 90%|████████▉ | 9657/10740 [47:53:15<5:35:07, 18.57s/it]

 90%|████████▉ | 9658/10740 [47:53:31<5:18:26, 17.66s/it]

 90%|████████▉ | 9659/10740 [47:53:52<5:37:19, 18.72s/it]
{'loss': 0.3848, 'learning_rate': 5.267273494189273e-08, 'rewards/chosen': -1.6687918901443481, 'rewards/rejected': -3.240929126739502, 'rewards/accuracies': 0.875, 'rewards/margins': 1.572137475013733, 'policy_logps/rejected': -438.5758361816406, 'policy_logps/chosen': -392.0144958496094, 'referece_logps/rejected': -406.16650390625, 'referece_logps/chosen': -375.32659912109375, 'logits/rejected': -0.27454403042793274, 'logits/chosen': -0.3130517601966858, 'epoch': 5.4}


 90%|████████▉ | 9661/10740 [47:54:33<5:55:18, 19.76s/it]

 90%|████████▉ | 9662/10740 [47:54:51<5:46:06, 19.26s/it]

 90%|████████▉ | 9663/10740 [47:55:11<5:47:53, 19.38s/it]

 90%|████████▉ | 9664/10740 [47:55:32<5:58:28, 19.99s/it]

 90%|████████▉ | 9665/10740 [47:55:49<5:38:54, 18.92s/it]

 90%|█████████ | 9666/10740 [47:56:07<5:33:55, 18.66s/it]

 90%|█████████ | 9667/10740 [47:56:19<4:57:34, 16.64s/it]

 90%|█████████ | 9668/10740 [47:56:39<5:18:25, 17.82s/it]

 90%|█████████ | 9669/10740 [47:57:01<5:37:22, 18.90s/it]

 90%|█████████ | 9670/10740 [47:57:19<5:35:10, 18.79s/it]
{'loss': 0.3726, 'learning_rate': 5.161549037577739e-08, 'rewards/chosen': -1.8654892444610596, 'rewards/rejected': -3.8557395935058594, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9902504682540894, 'policy_logps/rejected': -352.6683349609375, 'policy_logps/chosen': -299.8040771484375, 'referece_logps/rejected': -314.1109313964844, 'referece_logps/chosen': -281.1492004394531, 'logits/rejected': -1.369607925415039, 'logits/chosen': -1.3491171598434448, 'epoch': 5.4}


 90%|█████████ | 9672/10740 [47:58:02<5:56:29, 20.03s/it]
{'loss': 0.3651, 'learning_rate': 5.142438510357805e-08, 'rewards/chosen': -1.6694960594177246, 'rewards/rejected': -3.6655256748199463, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9960298538208008, 'policy_logps/rejected': -476.7811279296875, 'policy_logps/chosen': -427.72906494140625, 'referece_logps/rejected': -440.1258544921875, 'referece_logps/chosen': -411.0340576171875, 'logits/rejected': -0.4856974482536316, 'logits/chosen': -0.6016933917999268, 'epoch': 5.4}


 90%|█████████ | 9674/10740 [47:58:32<5:12:02, 17.56s/it]

 90%|█████████ | 9675/10740 [47:58:48<4:59:40, 16.88s/it]

 90%|█████████ | 9676/10740 [47:59:09<5:24:09, 18.28s/it]
{'loss': 0.3679, 'learning_rate': 5.1043209931898257e-08, 'rewards/chosen': -2.4171805381774902, 'rewards/rejected': -5.403117656707764, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9859378337860107, 'policy_logps/rejected': -518.3422241210938, 'policy_logps/chosen': -454.18194580078125, 'referece_logps/rejected': -464.31103515625, 'referece_logps/chosen': -430.0101318359375, 'logits/rejected': 0.23920397460460663, 'logits/chosen': 0.19736281037330627, 'epoch': 5.41}


 90%|█████████ | 9678/10740 [47:59:48<5:29:55, 18.64s/it]
{'loss': 0.3109, 'learning_rate': 5.085314017109288e-08, 'rewards/chosen': -1.7568672895431519, 'rewards/rejected': -3.039987564086914, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2831201553344727, 'policy_logps/rejected': -392.8581848144531, 'policy_logps/chosen': -402.2668762207031, 'referece_logps/rejected': -362.45831298828125, 'referece_logps/chosen': -384.6982421875, 'logits/rejected': -0.9507578611373901, 'logits/chosen': -1.0083298683166504, 'epoch': 5.41}


 90%|█████████ | 9680/10740 [48:00:21<5:15:23, 17.85s/it]

 90%|█████████ | 9681/10740 [48:00:37<5:07:27, 17.42s/it]

 90%|█████████ | 9682/10740 [48:00:57<5:20:21, 18.17s/it]

 90%|█████████ | 9683/10740 [48:01:16<5:22:59, 18.33s/it]

 90%|█████████ | 9684/10740 [48:01:31<5:06:11, 17.40s/it]

 90%|█████████ | 9685/10740 [48:01:44<4:41:30, 16.01s/it]

 90%|█████████ | 9686/10740 [48:01:57<4:27:02, 15.20s/it]
{'loss': 0.3753, 'learning_rate': 5.009631490678545e-08, 'rewards/chosen': -1.166007399559021, 'rewards/rejected': -2.4324896335601807, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2664819955825806, 'policy_logps/rejected': -392.49176025390625, 'policy_logps/chosen': -317.5396728515625, 'referece_logps/rejected': -368.1668701171875, 'referece_logps/chosen': -305.87957763671875, 'logits/rejected': 0.29544687271118164, 'logits/chosen': 0.2678888440132141, 'epoch': 5.41}


 90%|█████████ | 9688/10740 [48:02:33<4:47:53, 16.42s/it]
{'loss': 0.3153, 'learning_rate': 4.990797237961253e-08, 'rewards/chosen': -1.5251466035842896, 'rewards/rejected': -3.3798580169677734, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8547112941741943, 'policy_logps/rejected': -404.69378662109375, 'policy_logps/chosen': -460.61676025390625, 'referece_logps/rejected': -370.8951721191406, 'referece_logps/chosen': -445.36529541015625, 'logits/rejected': -1.1593321561813354, 'logits/chosen': -1.3279460668563843, 'epoch': 5.41}


 90%|█████████ | 9690/10740 [48:03:02<4:32:00, 15.54s/it]
{'loss': 0.4419, 'learning_rate': 4.971997550523155e-08, 'rewards/chosen': -1.5348089933395386, 'rewards/rejected': -3.969539165496826, 'rewards/accuracies': 0.875, 'rewards/margins': 2.434730052947998, 'policy_logps/rejected': -340.74212646484375, 'policy_logps/chosen': -284.5976257324219, 'referece_logps/rejected': -301.04669189453125, 'referece_logps/chosen': -269.2495422363281, 'logits/rejected': -1.02114999294281, 'logits/chosen': -0.9678865671157837, 'epoch': 5.41}

 90%|█████████ | 9691/10740 [48:03:14<4:16:47, 14.69s/it]

 90%|█████████ | 9692/10740 [48:03:34<4:43:07, 16.21s/it]

 90%|█████████ | 9693/10740 [48:03:53<4:54:07, 16.86s/it]


 90%|█████████ | 9695/10740 [48:04:24<4:39:06, 16.03s/it]

 90%|█████████ | 9696/10740 [48:04:38<4:26:23, 15.31s/it]

 90%|█████████ | 9697/10740 [48:04:50<4:10:11, 14.39s/it]

 90%|█████████ | 9698/10740 [48:05:10<4:38:30, 16.04s/it]

 90%|█████████ | 9699/10740 [48:05:27<4:47:00, 16.54s/it]

 90%|█████████ | 9700/10740 [48:05:50<5:16:13, 18.24s/it]

 90%|█████████ | 9701/10740 [48:06:09<5:22:58, 18.65s/it]

 90%|█████████ | 9702/10740 [48:06:23<4:58:38, 17.26s/it]
{'loss': 0.3086, 'learning_rate': 4.859925678889554e-08, 'rewards/chosen': -1.5098237991333008, 'rewards/rejected': -3.2020857334136963, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6922619342803955, 'policy_logps/rejected': -330.40277099609375, 'policy_logps/chosen': -402.29156494140625, 'referece_logps/rejected': -298.3819274902344, 'referece_logps/chosen': -387.19329833984375, 'logits/rejected': -0.491650253534317, 'logits/chosen': -0.34815946221351624, 'epoch': 5.42}


 90%|█████████ | 9704/10740 [48:07:04<5:21:33, 18.62s/it]

 90%|█████████ | 9705/10740 [48:07:22<5:20:47, 18.60s/it]

 90%|█████████ | 9706/10740 [48:07:34<4:46:03, 16.60s/it]
{'loss': 0.5197, 'learning_rate': 4.822845219168492e-08, 'rewards/chosen': -2.814692974090576, 'rewards/rejected': -3.3960187435150146, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5813255906105042, 'policy_logps/rejected': -554.8731079101562, 'policy_logps/chosen': -642.3359375, 'referece_logps/rejected': -520.9129028320312, 'referece_logps/chosen': -614.18896484375, 'logits/rejected': -1.3760676383972168, 'logits/chosen': -1.4061754941940308, 'epoch': 5.42}


 90%|█████████ | 9708/10740 [48:07:58<4:03:52, 14.18s/it]
{'loss': 0.3073, 'learning_rate': 4.804356925511233e-08, 'rewards/chosen': -2.525876522064209, 'rewards/rejected': -4.612821578979492, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0869452953338623, 'policy_logps/rejected': -361.1338195800781, 'policy_logps/chosen': -287.2541198730469, 'referece_logps/rejected': -315.00555419921875, 'referece_logps/chosen': -261.995361328125, 'logits/rejected': -1.282227873802185, 'logits/chosen': -1.1861498355865479, 'epoch': 5.42}

 90%|█████████ | 9709/10740 [48:08:14<4:15:33, 14.87s/it]


 90%|█████████ | 9711/10740 [48:08:42<4:08:02, 14.46s/it]
{'loss': 0.4441, 'learning_rate': 4.776689424201341e-08, 'rewards/chosen': -0.9880523681640625, 'rewards/rejected': -3.298460006713867, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3104076385498047, 'policy_logps/rejected': -369.28570556640625, 'policy_logps/chosen': -380.67919921875, 'referece_logps/rejected': -336.3011169433594, 'referece_logps/chosen': -370.79864501953125, 'logits/rejected': -0.8203887939453125, 'logits/chosen': -0.8870434165000916, 'epoch': 5.43}


 90%|█████████ | 9713/10740 [48:09:16<4:22:58, 15.36s/it]

 90%|█████████ | 9714/10740 [48:09:34<4:39:32, 16.35s/it]

 90%|█████████ | 9715/10740 [48:09:54<4:55:47, 17.31s/it]
{'loss': 0.3993, 'learning_rate': 4.7399206774634605e-08, 'rewards/chosen': -2.7980129718780518, 'rewards/rejected': -3.6343982219696045, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8363847732543945, 'policy_logps/rejected': -427.0820007324219, 'policy_logps/chosen': -423.8277587890625, 'referece_logps/rejected': -390.738037109375, 'referece_logps/chosen': -395.84765625, 'logits/rejected': 0.41589975357055664, 'logits/chosen': 0.4259452223777771, 'epoch': 5.43}

 90%|█████████ | 9716/10740 [48:10:10<4:46:35, 16.79s/it]


 90%|█████████ | 9718/10740 [48:10:50<5:14:12, 18.45s/it]

 90%|█████████ | 9719/10740 [48:11:04<4:50:36, 17.08s/it]

 91%|█████████ | 9720/10740 [48:11:22<4:57:05, 17.48s/it]
{'loss': 0.4143, 'learning_rate': 4.6941546933713546e-08, 'rewards/chosen': -1.1380468606948853, 'rewards/rejected': -3.4458518028259277, 'rewards/accuracies': 1.0, 'rewards/margins': 2.307804822921753, 'policy_logps/rejected': -275.3298034667969, 'policy_logps/chosen': -287.0502014160156, 'referece_logps/rejected': -240.87127685546875, 'referece_logps/chosen': -275.66973876953125, 'logits/rejected': -0.5101997256278992, 'logits/chosen': -0.6662546992301941, 'epoch': 5.43}

 91%|█████████ | 9721/10740 [48:11:41<5:04:37, 17.94s/it]


 91%|█████████ | 9723/10740 [48:12:16<4:52:29, 17.26s/it]
{'loss': 0.2402, 'learning_rate': 4.6667991158619524e-08, 'rewards/chosen': -1.4410943984985352, 'rewards/rejected': -3.0182645320892334, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5771700143814087, 'policy_logps/rejected': -540.1427612304688, 'policy_logps/chosen': -432.21588134765625, 'referece_logps/rejected': -509.96014404296875, 'referece_logps/chosen': -417.804931640625, 'logits/rejected': -0.09361529350280762, 'logits/chosen': -0.17595060169696808, 'epoch': 5.43}

 91%|█████████ | 9724/10740 [48:12:39<5:18:16, 18.80s/it]

 91%|█████████ | 9725/10740 [48:12:55<5:05:44, 18.07s/it]


 91%|█████████ | 9727/10740 [48:13:28<4:58:08, 17.66s/it]

 91%|█████████ | 9728/10740 [48:13:44<4:50:45, 17.24s/it]
{'loss': 0.318, 'learning_rate': 4.6213799134665386e-08, 'rewards/chosen': -1.5816779136657715, 'rewards/rejected': -4.586962699890137, 'rewards/accuracies': 1.0, 'rewards/margins': 3.005284547805786, 'policy_logps/rejected': -446.6976623535156, 'policy_logps/chosen': -347.6865539550781, 'referece_logps/rejected': -400.82806396484375, 'referece_logps/chosen': -331.8697814941406, 'logits/rejected': -1.3017277717590332, 'logits/chosen': -1.397437572479248, 'epoch': 5.43}

 91%|█████████ | 9729/10740 [48:14:04<5:01:37, 17.90s/it]


 91%|█████████ | 9731/10740 [48:14:45<5:25:39, 19.36s/it]

 91%|█████████ | 9732/10740 [48:15:06<5:35:48, 19.99s/it]
{'loss': 0.4064, 'learning_rate': 4.5852006951305086e-08, 'rewards/chosen': -1.6443408727645874, 'rewards/rejected': -4.812719345092773, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1683783531188965, 'policy_logps/rejected': -559.4368286132812, 'policy_logps/chosen': -534.0279541015625, 'referece_logps/rejected': -511.3096618652344, 'referece_logps/chosen': -517.5845947265625, 'logits/rejected': -0.3505128026008606, 'logits/chosen': -0.3149679899215698, 'epoch': 5.44}


 91%|█████████ | 9734/10740 [48:15:37<4:52:44, 17.46s/it]
{'loss': 0.3532, 'learning_rate': 4.567163151933839e-08, 'rewards/chosen': -2.7085671424865723, 'rewards/rejected': -3.600994110107422, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8924269676208496, 'policy_logps/rejected': -373.7506103515625, 'policy_logps/chosen': -449.3321533203125, 'referece_logps/rejected': -337.74066162109375, 'referece_logps/chosen': -422.2464599609375, 'logits/rejected': -0.9364427328109741, 'logits/chosen': -1.0009949207305908, 'epoch': 5.44}

 91%|█████████ | 9735/10740 [48:15:54<4:51:07, 17.38s/it]


 91%|█████████ | 9737/10740 [48:16:18<4:06:35, 14.75s/it]

 91%|█████████ | 9738/10740 [48:16:37<4:24:37, 15.85s/it]

 91%|█████████ | 9739/10740 [48:16:54<4:34:44, 16.47s/it]
{'loss': 0.2915, 'learning_rate': 4.5222212056471807e-08, 'rewards/chosen': -0.7210631370544434, 'rewards/rejected': -2.06536865234375, 'rewards/accuracies': 0.75, 'rewards/margins': 1.344305396080017, 'policy_logps/rejected': -252.6751251220703, 'policy_logps/chosen': -290.29925537109375, 'referece_logps/rejected': -232.0214385986328, 'referece_logps/chosen': -283.088623046875, 'logits/rejected': -0.8358092904090881, 'logits/chosen': -0.8203663229942322, 'epoch': 5.44}

 91%|█████████ | 9740/10740 [48:17:10<4:27:29, 16.05s/it]


 91%|█████████ | 9742/10740 [48:17:49<4:57:51, 17.91s/it]

 91%|█████████ | 9743/10740 [48:18:07<4:57:45, 17.92s/it]

 91%|█████████ | 9744/10740 [48:18:27<5:06:58, 18.49s/it]
{'loss': 0.2893, 'learning_rate': 4.477496357774357e-08, 'rewards/chosen': -1.1254265308380127, 'rewards/rejected': -4.283730506896973, 'rewards/accuracies': 0.875, 'rewards/margins': 3.15830397605896, 'policy_logps/rejected': -529.926025390625, 'policy_logps/chosen': -478.4556579589844, 'referece_logps/rejected': -487.0887145996094, 'referece_logps/chosen': -467.2013854980469, 'logits/rejected': 0.15934094786643982, 'logits/chosen': 0.17990440130233765, 'epoch': 5.44}

 91%|█████████ | 9745/10740 [48:18:44<5:00:55, 18.15s/it]

 91%|█████████ | 9746/10740 [48:19:04<5:07:23, 18.56s/it]

 91%|█████████ | 9747/10740 [48:19:24<5:15:33, 19.07s/it]


 91%|█████████ | 9749/10740 [48:19:54<4:42:53, 17.13s/it]
{'loss': 0.2998, 'learning_rate': 4.432988710011232e-08, 'rewards/chosen': -0.6502578258514404, 'rewards/rejected': -4.705606937408447, 'rewards/accuracies': 1.0, 'rewards/margins': 4.055349349975586, 'policy_logps/rejected': -446.1497802734375, 'policy_logps/chosen': -508.6445007324219, 'referece_logps/rejected': -399.09368896484375, 'referece_logps/chosen': -502.1419372558594, 'logits/rejected': -0.43982744216918945, 'logits/chosen': -0.6732621192932129, 'epoch': 5.45}


 91%|█████████ | 9751/10740 [48:20:29<4:36:01, 16.75s/it]
{'loss': 0.3097, 'learning_rate': 4.415246489622515e-08, 'rewards/chosen': -1.577041506767273, 'rewards/rejected': -2.8260607719421387, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2490192651748657, 'policy_logps/rejected': -525.44921875, 'policy_logps/chosen': -432.68878173828125, 'referece_logps/rejected': -497.18865966796875, 'referece_logps/chosen': -416.9183349609375, 'logits/rejected': -0.4489259123802185, 'logits/chosen': -0.39558106660842896, 'epoch': 5.45}


 91%|█████████ | 9753/10740 [48:21:08<5:03:23, 18.44s/it]
{'loss': 0.3084, 'learning_rate': 4.3975390439039974e-08, 'rewards/chosen': -2.2202250957489014, 'rewards/rejected': -4.383739948272705, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1635148525238037, 'policy_logps/rejected': -393.1300048828125, 'policy_logps/chosen': -377.92987060546875, 'referece_logps/rejected': -349.2926025390625, 'referece_logps/chosen': -355.72760009765625, 'logits/rejected': -0.20419929921627045, 'logits/chosen': -0.21740101277828217, 'epoch': 5.45}


 91%|█████████ | 9755/10740 [48:21:47<5:04:34, 18.55s/it]
{'loss': 0.3773, 'learning_rate': 4.3798663792978364e-08, 'rewards/chosen': -2.3914966583251953, 'rewards/rejected': -3.3306171894073486, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9391204118728638, 'policy_logps/rejected': -376.5844421386719, 'policy_logps/chosen': -463.5235900878906, 'referece_logps/rejected': -343.27825927734375, 'referece_logps/chosen': -439.6086120605469, 'logits/rejected': -0.6400467157363892, 'logits/chosen': -1.1614195108413696, 'epoch': 5.45}

 91%|█████████ | 9756/10740 [48:22:07<5:15:08, 19.22s/it]


 91%|█████████ | 9758/10740 [48:22:37<4:34:03, 16.75s/it]
{'loss': 0.5357, 'learning_rate': 4.3534226110352515e-08, 'rewards/chosen': -2.389491319656372, 'rewards/rejected': -2.4934000968933105, 'rewards/accuracies': 0.625, 'rewards/margins': 0.10390877723693848, 'policy_logps/rejected': -540.4086303710938, 'policy_logps/chosen': -624.9212646484375, 'referece_logps/rejected': -515.474609375, 'referece_logps/chosen': -601.0264282226562, 'logits/rejected': -0.5686023831367493, 'logits/chosen': -0.44104355573654175, 'epoch': 5.45}


 91%|█████████ | 9760/10740 [48:23:15<4:54:15, 18.02s/it]

 91%|█████████ | 9761/10740 [48:23:37<5:15:29, 19.34s/it]
{'loss': 0.5538, 'learning_rate': 4.327057136384926e-08, 'rewards/chosen': -1.9434475898742676, 'rewards/rejected': -2.9234089851379395, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9799613952636719, 'policy_logps/rejected': -400.5977478027344, 'policy_logps/chosen': -449.5321044921875, 'referece_logps/rejected': -371.3636779785156, 'referece_logps/chosen': -430.0976257324219, 'logits/rejected': -0.9512631297111511, 'logits/chosen': -0.7806357145309448, 'epoch': 5.45}

 91%|█████████ | 9762/10740 [48:24:00<5:33:13, 20.44s/it]

 91%|█████████ | 9763/10740 [48:24:20<5:30:31, 20.30s/it]

 91%|█████████ | 9764/10740 [48:24:42<5:37:11, 20.73s/it]


 91%|█████████ | 9766/10740 [48:25:19<5:19:51, 19.70s/it]
{'loss': 0.2674, 'learning_rate': 4.283288723028289e-08, 'rewards/chosen': -1.810166835784912, 'rewards/rejected': -3.3994033336639404, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5892366170883179, 'policy_logps/rejected': -303.65216064453125, 'policy_logps/chosen': -278.74700927734375, 'referece_logps/rejected': -269.6581115722656, 'referece_logps/chosen': -260.6453552246094, 'logits/rejected': -0.6650074124336243, 'logits/chosen': -0.6665114164352417, 'epoch': 5.46}


 91%|█████████ | 9768/10740 [48:25:53<4:57:11, 18.35s/it]
{'loss': 0.3163, 'learning_rate': 4.265842291805399e-08, 'rewards/chosen': -1.8318793773651123, 'rewards/rejected': -3.0261363983154297, 'rewards/accuracies': 0.875, 'rewards/margins': 1.194257140159607, 'policy_logps/rejected': -410.15533447265625, 'policy_logps/chosen': -361.5646667480469, 'referece_logps/rejected': -379.89398193359375, 'referece_logps/chosen': -343.245849609375, 'logits/rejected': 0.23827208578586578, 'logits/chosen': 0.20962212979793549, 'epoch': 5.46}

 91%|█████████ | 9769/10740 [48:26:14<5:10:44, 19.20s/it]

 91%|█████████ | 9770/10740 [48:26:34<5:12:03, 19.30s/it]

 91%|█████████ | 9771/10740 [48:26:54<5:14:38, 19.48s/it]

 91%|█████████ | 9772/10740 [48:27:06<4:38:43, 17.28s/it]

 91%|█████████ | 9773/10740 [48:27:26<4:50:29, 18.02s/it]

 91%|█████████ | 9774/10740 [48:27:44<4:52:23, 18.16s/it]


 91%|█████████ | 9776/10740 [48:28:21<4:50:37, 18.09s/it]
{'loss': 0.3953, 'learning_rate': 4.196404920444574e-08, 'rewards/chosen': -1.189997673034668, 'rewards/rejected': -2.545647144317627, 'rewards/accuracies': 0.875, 'rewards/margins': 1.355649471282959, 'policy_logps/rejected': -322.05816650390625, 'policy_logps/chosen': -396.0550231933594, 'referece_logps/rejected': -296.6016845703125, 'referece_logps/chosen': -384.1550598144531, 'logits/rejected': -0.3138127028942108, 'logits/chosen': -0.5172324776649475, 'epoch': 5.46}

 91%|█████████ | 9777/10740 [48:28:44<5:11:43, 19.42s/it]

 91%|█████████ | 9778/10740 [48:29:04<5:16:01, 19.71s/it]


 91%|█████████ | 9780/10740 [48:29:43<5:13:13, 19.58s/it]
{'loss': 0.4333, 'learning_rate': 4.161895335255261e-08, 'rewards/chosen': -1.7361719608306885, 'rewards/rejected': -3.2276268005371094, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4914546012878418, 'policy_logps/rejected': -303.35406494140625, 'policy_logps/chosen': -322.27178955078125, 'referece_logps/rejected': -271.0777893066406, 'referece_logps/chosen': -304.9100646972656, 'logits/rejected': -0.4203384518623352, 'logits/chosen': -0.39201515913009644, 'epoch': 5.46}

 91%|█████████ | 9781/10740 [48:29:54<4:29:24, 16.86s/it]

 91%|█████████ | 9782/10740 [48:30:14<4:45:04, 17.85s/it]

 91%|█████████ | 9783/10740 [48:30:27<4:22:05, 16.43s/it]

 91%|█████████ | 9784/10740 [48:30:47<4:37:50, 17.44s/it]

 91%|█████████ | 9785/10740 [48:31:05<4:40:42, 17.64s/it]

 91%|█████████ | 9786/10740 [48:31:25<4:52:10, 18.38s/it]

 91%|█████████ | 9787/10740 [48:31:42<4:45:32, 17.98s/it]

 91%|█████████ | 9788/10740 [48:31:59<4:39:26, 17.61s/it]

 91%|█████████ | 9789/10740 [48:32:20<4:57:23, 18.76s/it]

 91%|█████████ | 9790/10740 [48:32:41<5:05:23, 19.29s/it]


 91%|█████████ | 9792/10740 [48:33:10<4:22:34, 16.62s/it]

 91%|█████████ | 9793/10740 [48:33:28<4:28:31, 17.01s/it]

 91%|█████████ | 9794/10740 [48:33:39<4:03:56, 15.47s/it]
{'loss': 0.3443, 'learning_rate': 4.042210420245462e-08, 'rewards/chosen': -2.159517526626587, 'rewards/rejected': -3.6537230014801025, 'rewards/accuracies': 0.875, 'rewards/margins': 1.494205355644226, 'policy_logps/rejected': -434.1104736328125, 'policy_logps/chosen': -531.3758544921875, 'referece_logps/rejected': -397.5732116699219, 'referece_logps/chosen': -509.78070068359375, 'logits/rejected': -0.7131262421607971, 'logits/chosen': -0.7033657431602478, 'epoch': 5.47}

 91%|█████████ | 9795/10740 [48:33:59<4:23:35, 16.74s/it]


 91%|█████████ | 9797/10740 [48:34:32<4:19:57, 16.54s/it]

 91%|█████████ | 9798/10740 [48:34:52<4:35:28, 17.55s/it]
{'loss': 0.3381, 'learning_rate': 4.008328830701191e-08, 'rewards/chosen': -1.7178250551223755, 'rewards/rejected': -4.154724597930908, 'rewards/accuracies': 1.0, 'rewards/margins': 2.436899423599243, 'policy_logps/rejected': -471.59552001953125, 'policy_logps/chosen': -389.7604675292969, 'referece_logps/rejected': -430.04827880859375, 'referece_logps/chosen': -372.5822448730469, 'logits/rejected': -0.8864884972572327, 'logits/chosen': -0.9430714845657349, 'epoch': 5.47}

 91%|█████████ | 9799/10740 [48:35:09<4:36:01, 17.60s/it]

 91%|█████████ | 9800/10740 [48:35:29<4:45:53, 18.25s/it]


 91%|█████████▏| 9802/10740 [48:36:04<4:42:18, 18.06s/it]
{'loss': 0.2677, 'learning_rate': 3.9745869319875e-08, 'rewards/chosen': -1.0718495845794678, 'rewards/rejected': -2.900826930999756, 'rewards/accuracies': 0.875, 'rewards/margins': 1.828977346420288, 'policy_logps/rejected': -350.37322998046875, 'policy_logps/chosen': -335.1136169433594, 'referece_logps/rejected': -321.36492919921875, 'referece_logps/chosen': -324.3951416015625, 'logits/rejected': -0.7218381762504578, 'logits/chosen': -0.6664096713066101, 'epoch': 5.48}

 91%|█████████▏| 9803/10740 [48:36:19<4:26:32, 17.07s/it]

 91%|█████████▏| 9804/10740 [48:36:39<4:39:40, 17.93s/it]

 91%|█████████▏| 9805/10740 [48:36:51<4:12:05, 16.18s/it]


 91%|█████████▏| 9807/10740 [48:37:28<4:31:27, 17.46s/it]

 91%|█████████▏| 9808/10740 [48:37:48<4:43:34, 18.26s/it]
{'loss': 0.484, 'learning_rate': 3.9242361115805035e-08, 'rewards/chosen': -1.865708589553833, 'rewards/rejected': -4.329176902770996, 'rewards/accuracies': 0.875, 'rewards/margins': 2.463468074798584, 'policy_logps/rejected': -475.1383361816406, 'policy_logps/chosen': -477.1541442871094, 'referece_logps/rejected': -431.8465576171875, 'referece_logps/chosen': -458.49700927734375, 'logits/rejected': 0.564690351486206, 'logits/chosen': 0.465988427400589, 'epoch': 5.48}

 91%|█████████▏| 9809/10740 [48:38:02<4:27:20, 17.23s/it]


 91%|█████████▏| 9811/10740 [48:38:38<4:29:38, 17.42s/it]
{'loss': 0.3929, 'learning_rate': 3.899178658487645e-08, 'rewards/chosen': -2.2161433696746826, 'rewards/rejected': -3.813915252685547, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5977721214294434, 'policy_logps/rejected': -332.7678527832031, 'policy_logps/chosen': -334.26116943359375, 'referece_logps/rejected': -294.6286926269531, 'referece_logps/chosen': -312.0997314453125, 'logits/rejected': 0.07926736772060394, 'logits/chosen': 0.058286458253860474, 'epoch': 5.48}


 91%|█████████▏| 9813/10740 [48:39:04<3:56:37, 15.32s/it]
{'loss': 0.4035, 'learning_rate': 3.8825173915198464e-08, 'rewards/chosen': -1.8735421895980835, 'rewards/rejected': -3.2754788398742676, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4019365310668945, 'policy_logps/rejected': -397.951904296875, 'policy_logps/chosen': -394.6425476074219, 'referece_logps/rejected': -365.1971130371094, 'referece_logps/chosen': -375.9071350097656, 'logits/rejected': 0.3575321435928345, 'logits/chosen': 0.23844438791275024, 'epoch': 5.48}

 91%|█████████▏| 9814/10740 [48:39:23<4:15:09, 16.53s/it]

 91%|█████████▏| 9815/10740 [48:39:39<4:09:33, 16.19s/it]

 91%|█████████▏| 9816/10740 [48:39:55<4:10:50, 16.29s/it]

 91%|█████████▏| 9817/10740 [48:40:13<4:19:25, 16.86s/it]


 91%|█████████▏| 9819/10740 [48:40:48<4:22:20, 17.09s/it]
{'loss': 0.4554, 'learning_rate': 3.832743425692519e-08, 'rewards/chosen': -1.5065324306488037, 'rewards/rejected': -2.561671733856201, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0551389455795288, 'policy_logps/rejected': -375.7280578613281, 'policy_logps/chosen': -341.3720397949219, 'referece_logps/rejected': -350.1113586425781, 'referece_logps/chosen': -326.3067321777344, 'logits/rejected': -0.17372670769691467, 'logits/chosen': -0.12986046075820923, 'epoch': 5.49}


 91%|█████████▏| 9821/10740 [48:41:20<4:13:02, 16.52s/it]
{'loss': 0.42, 'learning_rate': 3.816222068895725e-08, 'rewards/chosen': -1.644856572151184, 'rewards/rejected': -3.227928400039673, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5830720663070679, 'policy_logps/rejected': -470.6752014160156, 'policy_logps/chosen': -371.8114929199219, 'referece_logps/rejected': -438.39593505859375, 'referece_logps/chosen': -355.3629150390625, 'logits/rejected': -0.12085509300231934, 'logits/chosen': -0.05856877565383911, 'epoch': 5.49}

 91%|█████████▏| 9822/10740 [48:41:36<4:09:07, 16.28s/it]

 91%|█████████▏| 9823/10740 [48:41:56<4:24:19, 17.29s/it]

 91%|█████████▏| 9824/10740 [48:42:17<4:44:22, 18.63s/it]

 91%|█████████▏| 9825/10740 [48:42:32<4:26:50, 17.50s/it]

 91%|█████████▏| 9826/10740 [48:42:51<4:30:24, 17.75s/it]

 91%|█████████▏| 9827/10740 [48:43:09<4:32:03, 17.88s/it]

 92%|█████████▏| 9828/10740 [48:43:28<4:37:29, 18.26s/it]

 92%|█████████▏| 9829/10740 [48:43:47<4:40:11, 18.45s/it]


 92%|█████████▏| 9831/10740 [48:44:25<4:39:55, 18.48s/it]
{'loss': 0.3957, 'learning_rate': 3.734140293695964e-08, 'rewards/chosen': -2.3524551391601562, 'rewards/rejected': -3.035543441772461, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6830883026123047, 'policy_logps/rejected': -333.6383056640625, 'policy_logps/chosen': -415.4141540527344, 'referece_logps/rejected': -303.28289794921875, 'referece_logps/chosen': -391.8896179199219, 'logits/rejected': -0.6484352946281433, 'logits/chosen': -0.7415701150894165, 'epoch': 5.49}

 92%|█████████▏| 9832/10740 [48:44:41<4:31:17, 17.93s/it]

 92%|█████████▏| 9833/10740 [48:44:53<4:04:36, 16.18s/it]

 92%|█████████▏| 9834/10740 [48:45:14<4:22:06, 17.36s/it]

 92%|█████████▏| 9835/10740 [48:45:29<4:13:52, 16.83s/it]

 92%|█████████▏| 9836/10740 [48:45:48<4:25:09, 17.60s/it]

 92%|█████████▏| 9837/10740 [48:46:03<4:12:31, 16.78s/it]

 92%|█████████▏| 9838/10740 [48:46:23<4:25:19, 17.65s/it]

 92%|█████████▏| 9839/10740 [48:46:43<4:37:48, 18.50s/it]

 92%|█████████▏| 9840/10740 [48:47:06<4:56:07, 19.74s/it]

 92%|█████████▏| 9841/10740 [48:47:22<4:39:05, 18.63s/it]

 92%|█████████▏| 9842/10740 [48:47:42<4:43:38, 18.95s/it]

 92%|█████████▏| 9843/10740 [48:48:01<4:46:21, 19.15s/it]

 92%|█████████▏| 9844/10740 [48:48:17<4:30:28, 18.11s/it]

 92%|█████████▏| 9845/10740 [48:48:36<4:32:52, 18.29s/it]

 92%|█████████▏| 9846/10740 [48:48:48<4:06:01, 16.51s/it]

 92%|█████████▏| 9847/10740 [48:49:10<4:27:51, 18.00s/it]

 92%|█████████▏| 9848/10740 [48:49:29<4:35:12, 18.51s/it]

 92%|█████████▏| 9849/10740 [48:49:46<4:26:01, 17.91s/it]

 92%|█████████▏| 9850/10740 [48:50:06<4:33:22, 18.43s/it]

 92%|█████████▏| 9851/10740 [48:50:22<4:22:52, 17.74s/it]


 92%|█████████▏| 9853/10740 [48:51:03<4:42:46, 19.13s/it]
{'loss': 0.2976, 'learning_rate': 3.556643402158155e-08, 'rewards/chosen': -1.2645008563995361, 'rewards/rejected': -4.052816867828369, 'rewards/accuracies': 1.0, 'rewards/margins': 2.788316249847412, 'policy_logps/rejected': -405.29412841796875, 'policy_logps/chosen': -403.2623291015625, 'referece_logps/rejected': -364.76593017578125, 'referece_logps/chosen': -390.6173400878906, 'logits/rejected': -0.7877885103225708, 'logits/chosen': -0.7426791787147522, 'epoch': 5.5}

 92%|█████████▏| 9854/10740 [48:51:16<4:17:11, 17.42s/it]

 92%|█████████▏| 9855/10740 [48:51:35<4:24:42, 17.95s/it]

 92%|█████████▏| 9856/10740 [48:51:52<4:20:14, 17.66s/it]

 92%|█████████▏| 9857/10740 [48:52:07<4:08:01, 16.85s/it]

 92%|█████████▏| 9858/10740 [48:52:28<4:24:17, 17.98s/it]

 92%|█████████▏| 9859/10740 [48:52:46<4:24:33, 18.02s/it]

 92%|█████████▏| 9860/10740 [48:53:03<4:17:37, 17.57s/it]


 92%|█████████▏| 9862/10740 [48:53:29<3:47:40, 15.56s/it]
{'loss': 0.3582, 'learning_rate': 3.485254256252035e-08, 'rewards/chosen': -1.3061847686767578, 'rewards/rejected': -2.7701454162597656, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4639606475830078, 'policy_logps/rejected': -383.7459411621094, 'policy_logps/chosen': -285.1414794921875, 'referece_logps/rejected': -356.04449462890625, 'referece_logps/chosen': -272.0796203613281, 'logits/rejected': 0.7378941774368286, 'logits/chosen': 0.7070310115814209, 'epoch': 5.51}

 92%|█████████▏| 9863/10740 [48:53:44<3:45:24, 15.42s/it]

 92%|█████████▏| 9864/10740 [48:53:59<3:42:01, 15.21s/it]

 92%|█████████▏| 9865/10740 [48:54:12<3:32:57, 14.60s/it]

 92%|█████████▏| 9866/10740 [48:54:26<3:30:38, 14.46s/it]

 92%|█████████▏| 9867/10740 [48:54:48<4:02:46, 16.69s/it]

 92%|█████████▏| 9868/10740 [48:55:02<3:50:22, 15.85s/it]

 92%|█████████▏| 9869/10740 [48:55:14<3:33:58, 14.74s/it]

 92%|█████████▏| 9870/10740 [48:55:34<3:53:42, 16.12s/it]

 92%|█████████▏| 9871/10740 [48:55:52<4:01:49, 16.70s/it]

 92%|█████████▏| 9872/10740 [48:56:10<4:10:00, 17.28s/it]

 92%|█████████▏| 9873/10740 [48:56:30<4:20:23, 18.02s/it]

 92%|█████████▏| 9874/10740 [48:56:43<4:00:22, 16.65s/it]

 92%|█████████▏| 9875/10740 [48:57:00<4:01:43, 16.77s/it]

 92%|█████████▏| 9876/10740 [48:57:17<4:00:48, 16.72s/it]

 92%|█████████▏| 9877/10740 [48:57:37<4:12:56, 17.59s/it]

 92%|█████████▏| 9878/10740 [48:57:57<4:24:17, 18.40s/it]

 92%|█████████▏| 9879/10740 [48:58:17<4:30:05, 18.82s/it]

 92%|█████████▏| 9880/10740 [48:58:33<4:18:33, 18.04s/it]

 92%|█████████▏| 9881/10740 [48:58:54<4:32:16, 19.02s/it]

 92%|█████████▏| 9882/10740 [48:59:12<4:26:13, 18.62s/it]

 92%|█████████▏| 9883/10740 [48:59:28<4:16:18, 17.95s/it]

 92%|█████████▏| 9884/10740 [48:59:42<3:57:51, 16.67s/it]

 92%|█████████▏| 9885/10740 [48:59:57<3:49:44, 16.12s/it]

 92%|█████████▏| 9886/10740 [49:00:08<3:29:34, 14.72s/it]

 92%|█████████▏| 9887/10740 [49:00:21<3:20:56, 14.13s/it]

 92%|█████████▏| 9888/10740 [49:00:32<3:05:47, 13.08s/it]

 92%|█████████▏| 9889/10740 [49:00:50<3:27:49, 14.65s/it]

 92%|█████████▏| 9890/10740 [49:01:11<3:56:10, 16.67s/it]

 92%|█████████▏| 9891/10740 [49:01:32<4:12:15, 17.83s/it]


 92%|█████████▏| 9893/10740 [49:02:14<4:35:14, 19.50s/it]
{'loss': 0.2986, 'learning_rate': 3.244804058387196e-08, 'rewards/chosen': -1.2142537832260132, 'rewards/rejected': -2.7563817501068115, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5421282052993774, 'policy_logps/rejected': -384.6024169921875, 'policy_logps/chosen': -297.63720703125, 'referece_logps/rejected': -357.0386047363281, 'referece_logps/chosen': -285.4946594238281, 'logits/rejected': -0.7311795353889465, 'logits/chosen': -0.6870853900909424, 'epoch': 5.53}

 92%|█████████▏| 9894/10740 [49:02:34<4:38:03, 19.72s/it]

 92%|█████████▏| 9895/10740 [49:02:53<4:36:33, 19.64s/it]

 92%|█████████▏| 9896/10740 [49:03:14<4:40:05, 19.91s/it]

 92%|█████████▏| 9897/10740 [49:03:30<4:21:56, 18.64s/it]

 92%|█████████▏| 9898/10740 [49:03:51<4:32:12, 19.40s/it]


 92%|█████████▏| 9900/10740 [49:04:22<3:58:41, 17.05s/it]
{'loss': 0.3741, 'learning_rate': 3.1916785242388836e-08, 'rewards/chosen': -1.897070288658142, 'rewards/rejected': -5.716425895690918, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8193559646606445, 'policy_logps/rejected': -413.1098327636719, 'policy_logps/chosen': -456.28375244140625, 'referece_logps/rejected': -355.94561767578125, 'referece_logps/chosen': -437.3130187988281, 'logits/rejected': -0.03320002183318138, 'logits/chosen': -0.016374029219150543, 'epoch': 5.53}

 92%|█████████▏| 9901/10740 [49:04:43<4:15:11, 18.25s/it]

 92%|█████████▏| 9902/10740 [49:05:04<4:25:18, 19.00s/it]

 92%|█████████▏| 9903/10740 [49:05:20<4:11:45, 18.05s/it]

 92%|█████████▏| 9904/10740 [49:05:32<3:50:27, 16.54s/it]

 92%|█████████▏| 9905/10740 [49:05:47<3:43:31, 16.06s/it]

 92%|█████████▏| 9906/10740 [49:06:04<3:45:35, 16.23s/it]

 92%|█████████▏| 9907/10740 [49:06:27<4:11:36, 18.12s/it]

 92%|█████████▏| 9908/10740 [49:06:47<4:21:50, 18.88s/it]

 92%|█████████▏| 9909/10740 [49:07:07<4:24:19, 19.09s/it]

 92%|█████████▏| 9910/10740 [49:07:26<4:23:45, 19.07s/it]

 92%|█████████▏| 9911/10740 [49:07:41<4:08:41, 18.00s/it]


 92%|█████████▏| 9913/10740 [49:08:19<4:15:54, 18.57s/it]

 92%|█████████▏| 9914/10740 [49:08:43<4:34:45, 19.96s/it]

 92%|█████████▏| 9915/10740 [49:08:57<4:11:24, 18.28s/it]

 92%|█████████▏| 9916/10740 [49:09:16<4:16:11, 18.65s/it]

 92%|█████████▏| 9917/10740 [49:09:37<4:23:39, 19.22s/it]

 92%|█████████▏| 9918/10740 [49:09:55<4:19:04, 18.91s/it]

 92%|█████████▏| 9919/10740 [49:10:11<4:05:49, 17.96s/it]

 92%|█████████▏| 9920/10740 [49:10:25<3:47:29, 16.65s/it]

 92%|█████████▏| 9921/10740 [49:10:41<3:46:43, 16.61s/it]

 92%|█████████▏| 9922/10740 [49:10:58<3:47:27, 16.68s/it]

 92%|█████████▏| 9923/10740 [49:11:13<3:40:49, 16.22s/it]

 92%|█████████▏| 9924/10740 [49:11:33<3:56:03, 17.36s/it]

 92%|█████████▏| 9925/10740 [49:11:53<4:05:40, 18.09s/it]

 92%|█████████▏| 9926/10740 [49:12:14<4:16:12, 18.88s/it]
{'loss': 0.3135, 'learning_rate': 2.998134289391519e-08, 'rewards/chosen': -1.8160239458084106, 'rewards/rejected': -4.27105188369751, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4550280570983887, 'policy_logps/rejected': -583.973388671875, 'policy_logps/chosen': -429.5508728027344, 'referece_logps/rejected': -541.2628173828125, 'referece_logps/chosen': -411.3906555175781, 'logits/rejected': -0.13136053085327148, 'logits/chosen': -0.054095715284347534, 'epoch': 5.55}


 92%|█████████▏| 9928/10740 [49:12:52<4:19:27, 19.17s/it]
{'loss': 0.3765, 'learning_rate': 2.983493152440719e-08, 'rewards/chosen': -1.8445029258728027, 'rewards/rejected': -3.90912127494812, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0646183490753174, 'policy_logps/rejected': -363.2833251953125, 'policy_logps/chosen': -376.4555358886719, 'referece_logps/rejected': -324.1921081542969, 'referece_logps/chosen': -358.0105285644531, 'logits/rejected': 0.08171819150447845, 'logits/chosen': -0.02235778421163559, 'epoch': 5.55}


 92%|█████████▏| 9930/10740 [49:13:22<3:47:11, 16.83s/it]

 92%|█████████▏| 9931/10740 [49:13:42<3:58:52, 17.72s/it]

 92%|█████████▏| 9932/10740 [49:14:00<4:02:18, 17.99s/it]

 92%|█████████▏| 9933/10740 [49:14:12<3:34:53, 15.98s/it]

 92%|█████████▏| 9934/10740 [49:14:26<3:29:10, 15.57s/it]

 93%|█████████▎| 9935/10740 [49:14:37<3:09:05, 14.09s/it]

 93%|█████████▎| 9936/10740 [49:14:55<3:24:42, 15.28s/it]

 93%|█████████▎| 9937/10740 [49:15:16<3:48:14, 17.05s/it]
{'loss': 0.3932, 'learning_rate': 2.9180448952700332e-08, 'rewards/chosen': -2.576944351196289, 'rewards/rejected': -5.683071136474609, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1061267852783203, 'policy_logps/rejected': -535.6658935546875, 'policy_logps/chosen': -449.46832275390625, 'referece_logps/rejected': -478.83514404296875, 'referece_logps/chosen': -423.6988830566406, 'logits/rejected': 0.32969537377357483, 'logits/chosen': 0.340101957321167, 'epoch': 5.55}

 93%|█████████▎| 9938/10740 [49:15:29<3:30:42, 15.76s/it]

 93%|█████████▎| 9939/10740 [49:15:45<3:31:04, 15.81s/it]


 93%|█████████▎| 9941/10740 [49:16:17<3:35:04, 16.15s/it]

 93%|█████████▎| 9942/10740 [49:16:29<3:19:26, 15.00s/it]

 93%|█████████▎| 9943/10740 [49:16:50<3:42:10, 16.73s/it]

 93%|█████████▎| 9944/10740 [49:17:04<3:30:58, 15.90s/it]

 93%|█████████▎| 9945/10740 [49:17:24<3:47:47, 17.19s/it]

 93%|█████████▎| 9946/10740 [49:17:40<3:39:37, 16.60s/it]

 93%|█████████▎| 9947/10740 [49:18:00<3:53:16, 17.65s/it]

 93%|█████████▎| 9948/10740 [49:18:20<4:04:36, 18.53s/it]

 93%|█████████▎| 9949/10740 [49:18:40<4:10:52, 19.03s/it]

 93%|█████████▎| 9950/10740 [49:19:03<4:22:39, 19.95s/it]

 93%|█████████▎| 9951/10740 [49:19:24<4:29:37, 20.50s/it]

 93%|█████████▎| 9952/10740 [49:19:44<4:27:13, 20.35s/it]

 93%|█████████▎| 9953/10740 [49:20:01<4:11:23, 19.17s/it]

 93%|█████████▎| 9954/10740 [49:20:20<4:12:57, 19.31s/it]

 93%|█████████▎| 9955/10740 [49:20:38<4:06:03, 18.81s/it]

 93%|█████████▎| 9956/10740 [49:20:52<3:47:46, 17.43s/it]

 93%|█████████▎| 9957/10740 [49:21:14<4:03:40, 18.67s/it]

 93%|█████████▎| 9958/10740 [49:21:25<3:33:29, 16.38s/it]

 93%|█████████▎| 9959/10740 [49:21:42<3:37:34, 16.71s/it]

 93%|█████████▎| 9960/10740 [49:22:03<3:52:49, 17.91s/it]
{'loss': 0.3438, 'learning_rate': 2.7540387396270515e-08, 'rewards/chosen': -1.4002333879470825, 'rewards/rejected': -3.937070846557617, 'rewards/accuracies': 0.875, 'rewards/margins': 2.536837100982666, 'policy_logps/rejected': -515.2725219726562, 'policy_logps/chosen': -456.9434509277344, 'referece_logps/rejected': -475.90179443359375, 'referece_logps/chosen': -442.94110107421875, 'logits/rejected': 0.3039309084415436, 'logits/chosen': 0.2873134911060333, 'epoch': 5.56}


 93%|█████████▎| 9962/10740 [49:22:33<3:33:12, 16.44s/it]
{'loss': 0.3186, 'learning_rate': 2.7399983410618777e-08, 'rewards/chosen': -2.1800878047943115, 'rewards/rejected': -3.1250343322753906, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9449466466903687, 'policy_logps/rejected': -483.7471618652344, 'policy_logps/chosen': -505.61138916015625, 'referece_logps/rejected': -452.4967956542969, 'referece_logps/chosen': -483.81048583984375, 'logits/rejected': -0.7603362798690796, 'logits/chosen': -0.7108299732208252, 'epoch': 5.57}

 93%|█████████▎| 9963/10740 [49:22:49<3:32:19, 16.40s/it]


 93%|█████████▎| 9965/10740 [49:23:22<3:31:27, 16.37s/it]

 93%|█████████▎| 9966/10740 [49:23:39<3:33:14, 16.53s/it]
[2024-04-03 20:37:22,492] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 9967/10740 [49:24:00<3:51:28, 17.97s/it]

 93%|█████████▎| 9968/10740 [49:24:18<3:51:51, 18.02s/it]

 93%|█████████▎| 9969/10740 [49:24:38<3:58:26, 18.56s/it]

 93%|█████████▎| 9970/10740 [49:24:56<3:56:59, 18.47s/it]

 93%|█████████▎| 9971/10740 [49:25:16<4:02:53, 18.95s/it]

 93%|█████████▎| 9972/10740 [49:25:33<3:53:53, 18.27s/it]

 93%|█████████▎| 9973/10740 [49:25:51<3:51:55, 18.14s/it]

 93%|█████████▎| 9974/10740 [49:26:04<3:31:23, 16.56s/it]
{'loss': 0.4454, 'learning_rate': 2.656499194530426e-08, 'rewards/chosen': -2.2635765075683594, 'rewards/rejected': -4.702691078186035, 'rewards/accuracies': 0.75, 'rewards/margins': 2.439114570617676, 'policy_logps/rejected': -463.2348327636719, 'policy_logps/chosen': -419.0113220214844, 'referece_logps/rejected': -416.2079162597656, 'referece_logps/chosen': -396.37554931640625, 'logits/rejected': -1.0434308052062988, 'logits/chosen': -0.9949331283569336, 'epoch': 5.57}


 93%|█████████▎| 9976/10740 [49:26:34<3:16:53, 15.46s/it]

 93%|█████████▎| 9977/10740 [49:26:54<3:32:43, 16.73s/it]

 93%|█████████▎| 9978/10740 [49:27:07<3:17:59, 15.59s/it]

 93%|█████████▎| 9979/10740 [49:27:28<3:39:08, 17.28s/it]
[2024-04-03 20:41:11,730] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3705, 'learning_rate': 2.622084100167299e-08, 'rewards/chosen': -2.023859739303589, 'rewards/rejected': -3.428158760070801, 'rewards/accuracies': 0.875, 'rewards/margins': 1.404299020767212, 'policy_logps/rejected': -349.1781921386719, 'policy_logps/chosen': -329.2034912109375, 'referece_logps/rejected': -314.8966369628906, 'referece_logps/chosen': -308.96490478515625, 'logits/rejected': -0.3967587351799011, 'logits/chosen': -0.6096000671386719, 'epoch': 5.57}


 93%|█████████▎| 9981/10740 [49:27:58<3:20:45, 15.87s/it]
[2024-04-03 20:41:41,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 9982/10740 [49:28:20<3:44:10, 17.74s/it]

 93%|█████████▎| 9983/10740 [49:28:41<3:55:30, 18.67s/it]

 93%|█████████▎| 9984/10740 [49:28:58<3:48:59, 18.17s/it]

 93%|█████████▎| 9985/10740 [49:29:20<4:04:20, 19.42s/it]

 93%|█████████▎| 9986/10740 [49:29:34<3:43:35, 17.79s/it]

 93%|█████████▎| 9987/10740 [49:29:51<3:38:27, 17.41s/it]

 93%|█████████▎| 9988/10740 [49:30:11<3:49:32, 18.31s/it]
[2024-04-03 20:43:54,960] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 9989/10740 [49:30:27<3:39:13, 17.51s/it]
[2024-04-03 20:44:10,608] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 9990/10740 [49:30:47<3:50:20, 18.43s/it]
[2024-04-03 20:44:31,166] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 9991/10740 [49:31:08<3:59:56, 19.22s/it]
[2024-04-03 20:44:52,239] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3632, 'learning_rate': 2.5403914110183054e-08, 'rewards/chosen': -2.221900463104248, 'rewards/rejected': -4.538061141967773, 'rewards/accuracies': 1.0, 'rewards/margins': 2.316160202026367, 'policy_logps/rejected': -523.1698608398438, 'policy_logps/chosen': -483.2200622558594, 'referece_logps/rejected': -477.7892761230469, 'referece_logps/chosen': -461.00103759765625, 'logits/rejected': 0.6071934700012207, 'logits/chosen': 0.4962216019630432, 'epoch': 5.58}
[2024-04-03 20:45:13,776] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 93%|█████████▎| 9993/10740 [49:31:51<4:10:33, 20.12s/it]
[2024-04-03 20:45:34,388] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 9994/10740 [49:32:10<4:08:43, 20.00s/it]

 93%|█████████▎| 9995/10740 [49:32:29<4:02:55, 19.56s/it]
{'loss': 0.329, 'learning_rate': 2.5134441160659658e-08, 'rewards/chosen': -1.632533073425293, 'rewards/rejected': -3.1252667903900146, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4927334785461426, 'policy_logps/rejected': -400.6048278808594, 'policy_logps/chosen': -355.06048583984375, 'referece_logps/rejected': -369.3521423339844, 'referece_logps/chosen': -338.7352294921875, 'logits/rejected': -0.5368300080299377, 'logits/chosen': -0.5939599275588989, 'epoch': 5.58}
[2024-04-03 20:46:35,599] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 93%|█████████▎| 9997/10740 [49:33:12<4:11:41, 20.32s/it]

 93%|█████████▎| 9998/10740 [49:33:31<4:06:06, 19.90s/it]

 93%|█████████▎| 9999/10740 [49:33:51<4:09:31, 20.20s/it]

 93%|█████████▎| 10000/10740 [49:34:09<4:00:39, 19.51s/it]
{'loss': 0.2599, 'learning_rate': 2.4799595013587305e-08, 'rewards/chosen': -2.1167545318603516, 'rewards/rejected': -5.37308931350708, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2563352584838867, 'policy_logps/rejected': -384.98828125, 'policy_logps/chosen': -371.54412841796875, 'referece_logps/rejected': -331.25738525390625, 'referece_logps/chosen': -350.3765563964844, 'logits/rejected': -0.4868376851081848, 'logits/chosen': -0.4512871503829956, 'epoch': 5.59}


 93%|█████████▎| 10002/10740 [49:34:59<4:24:31, 21.51s/it]

 93%|█████████▎| 10003/10740 [49:35:17<4:11:30, 20.48s/it]

 93%|█████████▎| 10004/10740 [49:35:37<4:09:08, 20.31s/it]

 93%|█████████▎| 10005/10740 [49:35:49<3:38:01, 17.80s/it]

 93%|█████████▎| 10006/10740 [49:36:05<3:32:22, 17.36s/it]

 93%|█████████▎| 10007/10740 [49:36:27<3:48:19, 18.69s/it]
[2024-04-03 20:50:10,818] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2985, 'learning_rate': 2.4334535845071282e-08, 'rewards/chosen': -2.9854300022125244, 'rewards/rejected': -4.863320350646973, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8778908252716064, 'policy_logps/rejected': -424.1466979980469, 'policy_logps/chosen': -488.61102294921875, 'referece_logps/rejected': -375.51348876953125, 'referece_logps/chosen': -458.75677490234375, 'logits/rejected': 0.08481515944004059, 'logits/chosen': 0.10788866877555847, 'epoch': 5.59}

 93%|█████████▎| 10008/10740 [49:36:48<3:57:42, 19.48s/it]

 93%|█████████▎| 10009/10740 [49:37:12<4:11:06, 20.61s/it]
[2024-04-03 20:50:55,395] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 10010/10740 [49:37:37<4:26:29, 21.90s/it]
[2024-04-03 20:51:20,313] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 10011/10740 [49:37:55<4:14:43, 20.96s/it]

 93%|█████████▎| 10012/10740 [49:38:15<4:11:01, 20.69s/it]

 93%|█████████▎| 10013/10740 [49:38:36<4:10:09, 20.65s/it]

 93%|█████████▎| 10014/10740 [49:38:50<3:45:52, 18.67s/it]
{'loss': 0.2827, 'learning_rate': 2.3873824894141802e-08, 'rewards/chosen': -1.4161186218261719, 'rewards/rejected': -4.150669574737549, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7345504760742188, 'policy_logps/rejected': -284.99383544921875, 'policy_logps/chosen': -280.8040771484375, 'referece_logps/rejected': -243.48712158203125, 'referece_logps/chosen': -266.64288330078125, 'logits/rejected': -0.897131085395813, 'logits/chosen': -0.9852386713027954, 'epoch': 5.59}

 93%|█████████▎| 10015/10740 [49:39:07<3:38:08, 18.05s/it]


 93%|█████████▎| 10017/10740 [49:39:42<3:37:12, 18.03s/it]
{'loss': 0.3822, 'learning_rate': 2.367770894026566e-08, 'rewards/chosen': -1.1651793718338013, 'rewards/rejected': -2.382538080215454, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2173584699630737, 'policy_logps/rejected': -315.7732849121094, 'policy_logps/chosen': -343.9006042480469, 'referece_logps/rejected': -291.94793701171875, 'referece_logps/chosen': -332.2488098144531, 'logits/rejected': -0.4573557376861572, 'logits/chosen': -0.554129421710968, 'epoch': 5.6}

 93%|█████████▎| 10018/10740 [49:40:01<3:39:13, 18.22s/it]

 93%|█████████▎| 10019/10740 [49:40:19<3:38:05, 18.15s/it]


 93%|█████████▎| 10021/10740 [49:40:46<3:06:18, 15.55s/it]
{'loss': 0.38, 'learning_rate': 2.3417464214035233e-08, 'rewards/chosen': -3.1553709506988525, 'rewards/rejected': -4.064060211181641, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9086893796920776, 'policy_logps/rejected': -475.7431640625, 'policy_logps/chosen': -497.46826171875, 'referece_logps/rejected': -435.1025695800781, 'referece_logps/chosen': -465.9145812988281, 'logits/rejected': -0.9653261303901672, 'logits/chosen': -0.8277421593666077, 'epoch': 5.6}

 93%|█████████▎| 10022/10740 [49:40:59<2:57:01, 14.79s/it]


 93%|█████████▎| 10024/10740 [49:41:34<3:09:50, 15.91s/it]
{'loss': 0.4266, 'learning_rate': 2.3223213285677668e-08, 'rewards/chosen': -1.868597388267517, 'rewards/rejected': -3.010063648223877, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1414659023284912, 'policy_logps/rejected': -321.03338623046875, 'policy_logps/chosen': -378.917236328125, 'referece_logps/rejected': -290.9327697753906, 'referece_logps/chosen': -360.2312927246094, 'logits/rejected': -0.4794341027736664, 'logits/chosen': -0.6634057760238647, 'epoch': 5.6}

 93%|█████████▎| 10025/10740 [49:41:53<3:21:56, 16.95s/it]

 93%|█████████▎| 10026/10740 [49:42:07<3:09:37, 15.94s/it]

 93%|█████████▎| 10027/10740 [49:42:25<3:17:05, 16.58s/it]


 93%|█████████▎| 10029/10740 [49:43:06<3:42:07, 18.75s/it]
[2024-04-03 20:56:49,856] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 10030/10740 [49:43:24<3:38:52, 18.50s/it]
[2024-04-03 20:57:07,775] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4027, 'learning_rate': 2.283711027376456e-08, 'rewards/chosen': -1.815517783164978, 'rewards/rejected': -2.3951380252838135, 'rewards/accuracies': 1.0, 'rewards/margins': 0.5796201229095459, 'policy_logps/rejected': -372.6618957519531, 'policy_logps/chosen': -449.1434326171875, 'referece_logps/rejected': -348.71051025390625, 'referece_logps/chosen': -430.98822021484375, 'logits/rejected': -0.5614397525787354, 'logits/chosen': -0.40041348338127136, 'epoch': 5.6}

 93%|█████████▎| 10031/10740 [49:43:43<3:41:13, 18.72s/it]

 93%|█████████▎| 10032/10740 [49:44:05<3:52:45, 19.72s/it]

 93%|█████████▎| 10033/10740 [49:44:27<3:58:47, 20.27s/it]
[2024-04-03 20:58:30,342] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 93%|█████████▎| 10034/10740 [49:44:47<3:56:34, 20.11s/it]


 93%|█████████▎| 10036/10740 [49:45:12<3:13:55, 16.53s/it]

 93%|█████████▎| 10037/10740 [49:45:32<3:24:55, 17.49s/it]

 93%|█████████▎| 10038/10740 [49:45:54<3:39:08, 18.73s/it]

 93%|█████████▎| 10039/10740 [49:46:06<3:18:18, 16.97s/it]

 93%|█████████▎| 10040/10740 [49:46:24<3:21:27, 17.27s/it]

 93%|█████████▎| 10041/10740 [49:46:46<3:37:36, 18.68s/it]
{'loss': 0.4273, 'learning_rate': 2.213756550081891e-08, 'rewards/chosen': -2.716069459915161, 'rewards/rejected': -3.4050300121307373, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6889601945877075, 'policy_logps/rejected': -486.87200927734375, 'policy_logps/chosen': -402.4538879394531, 'referece_logps/rejected': -452.82171630859375, 'referece_logps/chosen': -375.293212890625, 'logits/rejected': 0.5401285886764526, 'logits/chosen': 0.5918149352073669, 'epoch': 5.61}

 94%|█████████▎| 10042/10740 [49:47:06<3:39:34, 18.87s/it]

 94%|█████████▎| 10043/10740 [49:47:19<3:20:17, 17.24s/it]

 94%|█████████▎| 10044/10740 [49:47:31<3:02:17, 15.71s/it]

 94%|█████████▎| 10045/10740 [49:47:49<3:09:59, 16.40s/it]

 94%|█████████▎| 10046/10740 [49:48:02<2:55:43, 15.19s/it]


 94%|█████████▎| 10048/10740 [49:48:38<3:19:09, 17.27s/it]

 94%|█████████▎| 10049/10740 [49:48:56<3:19:19, 17.31s/it]
{'loss': 0.3847, 'learning_rate': 2.163556458779647e-08, 'rewards/chosen': -2.516228199005127, 'rewards/rejected': -4.35876989364624, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8425419330596924, 'policy_logps/rejected': -476.73590087890625, 'policy_logps/chosen': -451.1883239746094, 'referece_logps/rejected': -433.1482238769531, 'referece_logps/chosen': -426.02606201171875, 'logits/rejected': 0.2548385560512543, 'logits/chosen': 0.25856053829193115, 'epoch': 5.61}

 94%|█████████▎| 10050/10740 [49:49:16<3:27:38, 18.06s/it]


 94%|█████████▎| 10052/10740 [49:49:48<3:13:45, 16.90s/it]

 94%|█████████▎| 10053/10740 [49:50:08<3:24:06, 17.83s/it]

 94%|█████████▎| 10054/10740 [49:50:26<3:24:06, 17.85s/it]
{'loss': 0.405, 'learning_rate': 2.132470583262569e-08, 'rewards/chosen': -2.1225979328155518, 'rewards/rejected': -3.397498607635498, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2749007940292358, 'policy_logps/rejected': -524.5230102539062, 'policy_logps/chosen': -469.2652587890625, 'referece_logps/rejected': -490.54803466796875, 'referece_logps/chosen': -448.03924560546875, 'logits/rejected': -0.5794217586517334, 'logits/chosen': -0.39107710123062134, 'epoch': 5.62}


 94%|█████████▎| 10056/10740 [49:51:00<3:16:45, 17.26s/it]
{'loss': 0.3757, 'learning_rate': 2.1200985381458202e-08, 'rewards/chosen': -1.4019896984100342, 'rewards/rejected': -3.4370031356811523, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0350136756896973, 'policy_logps/rejected': -350.5238037109375, 'policy_logps/chosen': -331.33233642578125, 'referece_logps/rejected': -316.15374755859375, 'referece_logps/chosen': -317.3124084472656, 'logits/rejected': -1.0508629083633423, 'logits/chosen': -1.1036790609359741, 'epoch': 5.62}

 94%|█████████▎| 10057/10740 [49:51:13<3:01:00, 15.90s/it]


 94%|█████████▎| 10059/10740 [49:51:45<2:57:33, 15.64s/it]
[2024-04-03 21:05:28,458] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4115, 'learning_rate': 2.101607240000336e-08, 'rewards/chosen': -2.4862804412841797, 'rewards/rejected': -4.755718231201172, 'rewards/accuracies': 0.875, 'rewards/margins': 2.269437551498413, 'policy_logps/rejected': -479.0022277832031, 'policy_logps/chosen': -386.149169921875, 'referece_logps/rejected': -431.445068359375, 'referece_logps/chosen': -361.286376953125, 'logits/rejected': -0.4407234787940979, 'logits/chosen': -0.3768291473388672, 'epoch': 5.62}


 94%|█████████▎| 10061/10740 [49:52:21<3:09:55, 16.78s/it]
{'loss': 0.3534, 'learning_rate': 2.089324227463529e-08, 'rewards/chosen': -1.497318148612976, 'rewards/rejected': -4.251460075378418, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7541418075561523, 'policy_logps/rejected': -463.65972900390625, 'policy_logps/chosen': -376.39306640625, 'referece_logps/rejected': -421.1451110839844, 'referece_logps/chosen': -361.41986083984375, 'logits/rejected': -0.6032171249389648, 'logits/chosen': -0.869047999382019, 'epoch': 5.62}
[2024-04-03 21:06:25,383] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 94%|█████████▎| 10062/10740 [49:52:42<3:23:22, 18.00s/it]


 94%|█████████▎| 10064/10740 [49:53:13<3:06:11, 16.53s/it]

 94%|█████████▎| 10065/10740 [49:53:30<3:10:07, 16.90s/it]
{'loss': 0.4115, 'learning_rate': 2.0648650694361836e-08, 'rewards/chosen': -1.894285798072815, 'rewards/rejected': -2.473862409591675, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5795766711235046, 'policy_logps/rejected': -456.7568664550781, 'policy_logps/chosen': -380.40771484375, 'referece_logps/rejected': -432.0182800292969, 'referece_logps/chosen': -361.46490478515625, 'logits/rejected': -0.5050445199012756, 'logits/chosen': -0.4521584212779999, 'epoch': 5.62}

 94%|█████████▎| 10066/10740 [49:53:44<2:56:45, 15.74s/it]

 94%|█████████▎| 10067/10740 [49:54:04<3:12:25, 17.16s/it]

 94%|█████████▎| 10068/10740 [49:54:20<3:07:00, 16.70s/it]

 94%|█████████▍| 10069/10740 [49:54:36<3:05:01, 16.54s/it]


 94%|█████████▍| 10071/10740 [49:55:15<3:21:38, 18.08s/it]

 94%|█████████▍| 10072/10740 [49:55:29<3:06:50, 16.78s/it]
{'loss': 0.3545, 'learning_rate': 2.0224045006112856e-08, 'rewards/chosen': -1.6835553646087646, 'rewards/rejected': -3.4326014518737793, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7490460872650146, 'policy_logps/rejected': -411.1690368652344, 'policy_logps/chosen': -340.18292236328125, 'referece_logps/rejected': -376.8430480957031, 'referece_logps/chosen': -323.34735107421875, 'logits/rejected': -1.1423170566558838, 'logits/chosen': -1.259302020072937, 'epoch': 5.63}

 94%|█████████▍| 10073/10740 [49:55:48<3:13:43, 17.43s/it]


 94%|█████████▍| 10075/10740 [49:56:29<3:29:40, 18.92s/it]

 94%|█████████▍| 10076/10740 [49:56:48<3:32:07, 19.17s/it]
{'loss': 0.3211, 'learning_rate': 1.998337354954216e-08, 'rewards/chosen': -1.3174948692321777, 'rewards/rejected': -3.6411514282226562, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3236567974090576, 'policy_logps/rejected': -291.0443115234375, 'policy_logps/chosen': -400.81207275390625, 'referece_logps/rejected': -254.63278198242188, 'referece_logps/chosen': -387.6371765136719, 'logits/rejected': -0.010578162968158722, 'logits/chosen': -0.035539738833904266, 'epoch': 5.63}


 94%|█████████▍| 10078/10740 [49:57:23<3:18:18, 17.97s/it]

 94%|█████████▍| 10079/10740 [49:57:39<3:12:16, 17.45s/it]
{'loss': 0.3854, 'learning_rate': 1.9803805854545998e-08, 'rewards/chosen': -2.4762043952941895, 'rewards/rejected': -3.9479479789733887, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4717433452606201, 'policy_logps/rejected': -475.03857421875, 'policy_logps/chosen': -378.23065185546875, 'referece_logps/rejected': -435.5591125488281, 'referece_logps/chosen': -353.4685974121094, 'logits/rejected': 0.18060512840747833, 'logits/chosen': 0.0998314917087555, 'epoch': 5.63}

 94%|█████████▍| 10080/10740 [49:57:56<3:10:32, 17.32s/it]

 94%|█████████▍| 10081/10740 [49:58:14<3:10:53, 17.38s/it]

 94%|█████████▍| 10082/10740 [49:58:28<2:59:55, 16.41s/it]


 94%|█████████▍| 10084/10740 [49:59:07<3:20:17, 18.32s/it]
{'loss': 0.3773, 'learning_rate': 1.9506309460021743e-08, 'rewards/chosen': -1.399754285812378, 'rewards/rejected': -2.590122699737549, 'rewards/accuracies': 0.75, 'rewards/margins': 1.19036865234375, 'policy_logps/rejected': -385.908447265625, 'policy_logps/chosen': -389.58099365234375, 'referece_logps/rejected': -360.0072021484375, 'referece_logps/chosen': -375.58343505859375, 'logits/rejected': -0.5749913454055786, 'logits/chosen': -0.6465812921524048, 'epoch': 5.63}

 94%|█████████▍| 10085/10740 [49:59:18<2:56:01, 16.12s/it]


 94%|█████████▍| 10087/10740 [49:59:57<3:14:04, 17.83s/it]

 94%|█████████▍| 10088/10740 [50:00:13<3:10:30, 17.53s/it]
{'loss': 0.3887, 'learning_rate': 1.926991752131424e-08, 'rewards/chosen': -1.44700288772583, 'rewards/rejected': -3.563389539718628, 'rewards/accuracies': 1.0, 'rewards/margins': 2.116386651992798, 'policy_logps/rejected': -336.2905578613281, 'policy_logps/chosen': -286.3713684082031, 'referece_logps/rejected': -300.6566467285156, 'referece_logps/chosen': -271.9013366699219, 'logits/rejected': -1.3239591121673584, 'logits/chosen': -1.292109489440918, 'epoch': 5.64}


 94%|█████████▍| 10090/10740 [50:00:49<3:16:12, 18.11s/it]
[2024-04-03 21:14:33,165] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3874, 'learning_rate': 1.9152256729317417e-08, 'rewards/chosen': -1.5926140546798706, 'rewards/rejected': -6.4496750831604, 'rewards/accuracies': 0.875, 'rewards/margins': 4.857061862945557, 'policy_logps/rejected': -441.4150390625, 'policy_logps/chosen': -464.3332214355469, 'referece_logps/rejected': -376.91827392578125, 'referece_logps/chosen': -448.40704345703125, 'logits/rejected': 0.3356832265853882, 'logits/chosen': 0.36963051557540894, 'epoch': 5.64}
[2024-04-03 21:14:53,649] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 94%|█████████▍| 10092/10740 [50:01:29<3:25:20, 19.01s/it]
{'loss': 0.3044, 'learning_rate': 1.9034952779343195e-08, 'rewards/chosen': -1.537868857383728, 'rewards/rejected': -4.638350009918213, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1004812717437744, 'policy_logps/rejected': -352.0268859863281, 'policy_logps/chosen': -346.3799133300781, 'referece_logps/rejected': -305.6434020996094, 'referece_logps/chosen': -331.001220703125, 'logits/rejected': -0.5969605445861816, 'logits/chosen': -0.6075413227081299, 'epoch': 5.64}


 94%|█████████▍| 10094/10740 [50:02:11<3:33:18, 19.81s/it]

 94%|█████████▍| 10095/10740 [50:02:25<3:14:58, 18.14s/it]
{'loss': 0.318, 'learning_rate': 1.885966602649336e-08, 'rewards/chosen': -1.5545933246612549, 'rewards/rejected': -3.088592767715454, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5339995622634888, 'policy_logps/rejected': -464.76513671875, 'policy_logps/chosen': -393.4678039550781, 'referece_logps/rejected': -433.87921142578125, 'referece_logps/chosen': -377.921875, 'logits/rejected': -0.393473356962204, 'logits/chosen': -0.4628582298755646, 'epoch': 5.64}
[2024-04-03 21:16:32,316] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 94%|█████████▍| 10097/10740 [50:03:03<3:14:15, 18.13s/it]
{'loss': 0.2864, 'learning_rate': 1.874325436800084e-08, 'rewards/chosen': -2.3598625659942627, 'rewards/rejected': -4.096286773681641, 'rewards/accuracies': 0.875, 'rewards/margins': 1.736424207687378, 'policy_logps/rejected': -567.9221801757812, 'policy_logps/chosen': -583.9053955078125, 'referece_logps/rejected': -526.9592895507812, 'referece_logps/chosen': -560.3067626953125, 'logits/rejected': -0.2566191852092743, 'logits/chosen': -0.26213061809539795, 'epoch': 5.64}
[2024-04-03 21:17:01,960] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 94%|█████████▍| 10099/10740 [50:03:35<3:02:02, 17.04s/it]

 94%|█████████▍| 10100/10740 [50:03:53<3:04:40, 17.31s/it]
{'loss': 0.2887, 'learning_rate': 1.856930625125219e-08, 'rewards/chosen': -2.2777481079101562, 'rewards/rejected': -4.6459221839904785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3681743144989014, 'policy_logps/rejected': -374.7667236328125, 'policy_logps/chosen': -339.30413818359375, 'referece_logps/rejected': -328.3074951171875, 'referece_logps/chosen': -316.5266418457031, 'logits/rejected': -0.35528457164764404, 'logits/chosen': -0.3653302788734436, 'epoch': 5.64}


 94%|█████████▍| 10102/10740 [50:04:26<2:56:02, 16.56s/it]

 94%|█████████▍| 10103/10740 [50:04:47<3:11:18, 18.02s/it]
{'loss': 0.3168, 'learning_rate': 1.839616150621115e-08, 'rewards/chosen': -1.8869550228118896, 'rewards/rejected': -3.8698318004608154, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9828767776489258, 'policy_logps/rejected': -333.1343688964844, 'policy_logps/chosen': -299.84356689453125, 'referece_logps/rejected': -294.43603515625, 'referece_logps/chosen': -280.9740295410156, 'logits/rejected': -0.3900222182273865, 'logits/chosen': -0.36296558380126953, 'epoch': 5.64}


 94%|█████████▍| 10105/10740 [50:05:26<3:17:14, 18.64s/it]

 94%|█████████▍| 10106/10740 [50:05:39<3:00:47, 17.11s/it]
{'loss': 0.3828, 'learning_rate': 1.8223820274609114e-08, 'rewards/chosen': -1.6691687107086182, 'rewards/rejected': -3.086580514907837, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4174120426177979, 'policy_logps/rejected': -412.7972412109375, 'policy_logps/chosen': -327.0058898925781, 'referece_logps/rejected': -381.9314270019531, 'referece_logps/chosen': -310.314208984375, 'logits/rejected': -0.33855947852134705, 'logits/chosen': -0.3642478883266449, 'epoch': 5.65}

 94%|█████████▍| 10107/10740 [50:05:52<2:48:32, 15.98s/it]


 94%|█████████▍| 10109/10740 [50:06:21<2:41:00, 15.31s/it]
{'loss': 0.4371, 'learning_rate': 1.8052282697519905e-08, 'rewards/chosen': -1.8794682025909424, 'rewards/rejected': -3.622746467590332, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7432780265808105, 'policy_logps/rejected': -303.173828125, 'policy_logps/chosen': -320.02789306640625, 'referece_logps/rejected': -266.9463806152344, 'referece_logps/chosen': -301.23321533203125, 'logits/rejected': -0.7560117840766907, 'logits/chosen': -0.748645544052124, 'epoch': 5.65}

 94%|█████████▍| 10110/10740 [50:06:41<2:54:11, 16.59s/it]


 94%|█████████▍| 10112/10740 [50:07:12<2:47:47, 16.03s/it]

 94%|█████████▍| 10113/10740 [50:07:27<2:45:54, 15.88s/it]

 94%|█████████▍| 10114/10740 [50:07:44<2:47:30, 16.05s/it]
{'loss': 0.4876, 'learning_rate': 1.7768173015700106e-08, 'rewards/chosen': -2.875145435333252, 'rewards/rejected': -4.240669250488281, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3655235767364502, 'policy_logps/rejected': -442.80517578125, 'policy_logps/chosen': -321.2225036621094, 'referece_logps/rejected': -400.3984680175781, 'referece_logps/chosen': -292.4710693359375, 'logits/rejected': -0.22184094786643982, 'logits/chosen': -0.27616748213768005, 'epoch': 5.65}

 94%|█████████▍| 10115/10740 [50:08:00<2:49:23, 16.26s/it]

 94%|█████████▍| 10116/10740 [50:08:13<2:39:00, 15.29s/it]

 94%|█████████▍| 10117/10740 [50:08:29<2:38:45, 15.29s/it]

 94%|█████████▍| 10118/10740 [50:08:51<3:00:57, 17.46s/it]
[2024-04-03 21:22:58,154] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 94%|█████████▍| 10119/10740 [50:09:14<3:18:40, 19.20s/it]


 94%|█████████▍| 10121/10740 [50:09:50<3:11:24, 18.55s/it]
{'loss': 0.3465, 'learning_rate': 1.7374171732737143e-08, 'rewards/chosen': -1.3812241554260254, 'rewards/rejected': -3.0757956504821777, 'rewards/accuracies': 0.875, 'rewards/margins': 1.694571614265442, 'policy_logps/rejected': -369.31658935546875, 'policy_logps/chosen': -362.8196716308594, 'referece_logps/rejected': -338.55865478515625, 'referece_logps/chosen': -349.0074462890625, 'logits/rejected': -0.7953671216964722, 'logits/chosen': -0.7305575013160706, 'epoch': 5.65}

 94%|█████████▍| 10122/10740 [50:10:09<3:13:14, 18.76s/it]

 94%|█████████▍| 10123/10740 [50:10:29<3:15:43, 19.03s/it]

 94%|█████████▍| 10124/10740 [50:10:45<3:05:39, 18.08s/it]


 94%|█████████▍| 10126/10740 [50:11:22<3:07:20, 18.31s/it]
{'loss': 0.3946, 'learning_rate': 1.7095423309183233e-08, 'rewards/chosen': -2.2072043418884277, 'rewards/rejected': -4.115912914276123, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9087084531784058, 'policy_logps/rejected': -379.00555419921875, 'policy_logps/chosen': -418.8607482910156, 'referece_logps/rejected': -337.8464050292969, 'referece_logps/chosen': -396.7886657714844, 'logits/rejected': 0.19027745723724365, 'logits/chosen': 0.0717381089925766, 'epoch': 5.66}


 94%|█████████▍| 10128/10740 [50:11:56<2:54:35, 17.12s/it]
{'loss': 0.3081, 'learning_rate': 1.698454968739682e-08, 'rewards/chosen': -2.315812826156616, 'rewards/rejected': -2.711975574493408, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3961626887321472, 'policy_logps/rejected': -440.2081604003906, 'policy_logps/chosen': -383.8478088378906, 'referece_logps/rejected': -413.08837890625, 'referece_logps/chosen': -360.689697265625, 'logits/rejected': -0.17660364508628845, 'logits/chosen': -0.13253283500671387, 'epoch': 5.66}

 94%|█████████▍| 10129/10740 [50:12:07<2:35:12, 15.24s/it]

 94%|█████████▍| 10130/10740 [50:12:22<2:36:10, 15.36s/it]

 94%|█████████▍| 10131/10740 [50:12:41<2:45:35, 16.31s/it]


 94%|█████████▍| 10133/10740 [50:13:20<3:02:12, 18.01s/it]

 94%|█████████▍| 10134/10740 [50:13:34<2:49:10, 16.75s/it]
{'loss': 0.3424, 'learning_rate': 1.6654074766669137e-08, 'rewards/chosen': -1.8808820247650146, 'rewards/rejected': -3.515592336654663, 'rewards/accuracies': 0.875, 'rewards/margins': 1.634710431098938, 'policy_logps/rejected': -454.94287109375, 'policy_logps/chosen': -385.59649658203125, 'referece_logps/rejected': -419.78692626953125, 'referece_logps/chosen': -366.7876892089844, 'logits/rejected': -0.7883541584014893, 'logits/chosen': -0.7358759045600891, 'epoch': 5.66}

 94%|█████████▍| 10135/10740 [50:13:48<2:42:00, 16.07s/it]

 94%|█████████▍| 10136/10740 [50:13:59<2:25:49, 14.49s/it]

 94%|█████████▍| 10137/10740 [50:14:15<2:28:49, 14.81s/it]

 94%|█████████▍| 10138/10740 [50:14:30<2:28:42, 14.82s/it]

 94%|█████████▍| 10139/10740 [50:14:44<2:25:55, 14.57s/it]


 94%|█████████▍| 10141/10740 [50:15:14<2:31:12, 15.15s/it]
{'loss': 0.2548, 'learning_rate': 1.627259018321825e-08, 'rewards/chosen': -1.8265025615692139, 'rewards/rejected': -4.379672050476074, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5531692504882812, 'policy_logps/rejected': -469.76251220703125, 'policy_logps/chosen': -391.87530517578125, 'referece_logps/rejected': -425.9658203125, 'referece_logps/chosen': -373.6102294921875, 'logits/rejected': -0.38518330454826355, 'logits/chosen': -0.3770982623100281, 'epoch': 5.67}

 94%|█████████▍| 10142/10740 [50:15:32<2:37:20, 15.79s/it]

 94%|█████████▍| 10143/10740 [50:15:48<2:37:29, 15.83s/it]

 94%|█████████▍| 10144/10740 [50:16:05<2:42:44, 16.38s/it]

 94%|█████████▍| 10145/10740 [50:16:24<2:49:07, 17.05s/it]

 94%|█████████▍| 10146/10740 [50:16:42<2:51:48, 17.36s/it]

 94%|█████████▍| 10147/10740 [50:16:56<2:40:08, 16.20s/it]


 94%|█████████▍| 10149/10740 [50:17:30<2:46:50, 16.94s/it]
{'loss': 0.4092, 'learning_rate': 1.5841976243643696e-08, 'rewards/chosen': -1.5582493543624878, 'rewards/rejected': -3.182295799255371, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6240462064743042, 'policy_logps/rejected': -312.5236511230469, 'policy_logps/chosen': -361.7005615234375, 'referece_logps/rejected': -280.70068359375, 'referece_logps/chosen': -346.1180419921875, 'logits/rejected': -0.2628866732120514, 'logits/chosen': -0.25730323791503906, 'epoch': 5.67}

 95%|█████████▍| 10150/10740 [50:17:50<2:54:41, 17.77s/it]

 95%|█████████▍| 10151/10740 [50:18:08<2:54:31, 17.78s/it]

 95%|█████████▍| 10152/10740 [50:18:28<3:01:18, 18.50s/it]

 95%|█████████▍| 10153/10740 [50:18:48<3:04:11, 18.83s/it]

 95%|█████████▍| 10154/10740 [50:19:08<3:07:49, 19.23s/it]

 95%|█████████▍| 10155/10740 [50:19:28<3:09:12, 19.41s/it]

 95%|█████████▍| 10156/10740 [50:19:43<2:56:51, 18.17s/it]

 95%|█████████▍| 10157/10740 [50:19:57<2:45:29, 17.03s/it]


 95%|█████████▍| 10159/10740 [50:20:30<2:39:30, 16.47s/it]
{'loss': 0.3788, 'learning_rate': 1.5311765148618872e-08, 'rewards/chosen': -2.1537301540374756, 'rewards/rejected': -3.026198625564575, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8724687099456787, 'policy_logps/rejected': -346.44488525390625, 'policy_logps/chosen': -346.395751953125, 'referece_logps/rejected': -316.182861328125, 'referece_logps/chosen': -324.85845947265625, 'logits/rejected': -0.7307617664337158, 'logits/chosen': -0.6602171659469604, 'epoch': 5.68}

 95%|█████████▍| 10160/10740 [50:20:49<2:45:36, 17.13s/it]

 95%|█████████▍| 10161/10740 [50:21:06<2:44:50, 17.08s/it]

 95%|█████████▍| 10162/10740 [50:21:22<2:40:17, 16.64s/it]
[2024-04-03 21:35:28,669] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▍| 10163/10740 [50:21:45<2:59:03, 18.62s/it]
[2024-04-03 21:35:44,946] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▍| 10164/10740 [50:22:01<2:51:59, 17.92s/it]

 95%|█████████▍| 10165/10740 [50:22:22<3:00:23, 18.82s/it]

 95%|█████████▍| 10166/10740 [50:22:38<2:51:48, 17.96s/it]

 95%|█████████▍| 10167/10740 [50:22:49<2:32:33, 15.98s/it]
[2024-04-03 21:36:53,965] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▍| 10168/10740 [50:23:10<2:46:04, 17.42s/it]

 95%|█████████▍| 10169/10740 [50:23:22<2:30:11, 15.78s/it]


 95%|█████████▍| 10171/10740 [50:23:45<2:07:33, 13.45s/it]
{'loss': 0.3409, 'learning_rate': 1.4687334139252827e-08, 'rewards/chosen': -1.9927799701690674, 'rewards/rejected': -5.204922676086426, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2121429443359375, 'policy_logps/rejected': -419.47015380859375, 'policy_logps/chosen': -464.4491271972656, 'referece_logps/rejected': -367.4209289550781, 'referece_logps/chosen': -444.5213623046875, 'logits/rejected': -0.3850187361240387, 'logits/chosen': -0.5629016757011414, 'epoch': 5.68}

 95%|█████████▍| 10172/10740 [50:24:00<2:11:11, 13.86s/it]


 95%|█████████▍| 10174/10740 [50:24:27<2:12:42, 14.07s/it]

 95%|█████████▍| 10175/10740 [50:24:41<2:11:51, 14.00s/it]
{'loss': 0.3405, 'learning_rate': 1.4482057798666514e-08, 'rewards/chosen': -1.5886247158050537, 'rewards/rejected': -2.7680211067199707, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1793965101242065, 'policy_logps/rejected': -317.4768981933594, 'policy_logps/chosen': -381.58013916015625, 'referece_logps/rejected': -289.79669189453125, 'referece_logps/chosen': -365.6938781738281, 'logits/rejected': -0.1990540325641632, 'logits/chosen': -0.23560450971126556, 'epoch': 5.68}


 95%|█████████▍| 10177/10740 [50:25:13<2:20:43, 15.00s/it]
{'loss': 0.4029, 'learning_rate': 1.4379957421365996e-08, 'rewards/chosen': -1.9317398071289062, 'rewards/rejected': -2.438230276107788, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5064904093742371, 'policy_logps/rejected': -380.19622802734375, 'policy_logps/chosen': -488.42041015625, 'referece_logps/rejected': -355.81396484375, 'referece_logps/chosen': -469.10296630859375, 'logits/rejected': -0.09506450593471527, 'logits/chosen': -0.2091684341430664, 'epoch': 5.69}

 95%|█████████▍| 10178/10740 [50:25:34<2:38:14, 16.89s/it]


 95%|█████████▍| 10180/10740 [50:26:09<2:40:47, 17.23s/it]
{'loss': 0.3575, 'learning_rate': 1.4227479201172066e-08, 'rewards/chosen': -2.026853322982788, 'rewards/rejected': -2.700845718383789, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6739922165870667, 'policy_logps/rejected': -275.0664367675781, 'policy_logps/chosen': -285.36981201171875, 'referece_logps/rejected': -248.0579833984375, 'referece_logps/chosen': -265.1012878417969, 'logits/rejected': -0.665225625038147, 'logits/chosen': -0.6655106544494629, 'epoch': 5.69}

 95%|█████████▍| 10181/10740 [50:26:29<2:47:26, 17.97s/it]

 95%|█████████▍| 10182/10740 [50:26:42<2:34:03, 16.56s/it]

 95%|█████████▍| 10183/10740 [50:27:04<2:48:44, 18.18s/it]

 95%|█████████▍| 10184/10740 [50:27:23<2:52:31, 18.62s/it]


 95%|█████████▍| 10186/10740 [50:27:47<2:19:37, 15.12s/it]
{'loss': 0.3703, 'learning_rate': 1.3924943662355415e-08, 'rewards/chosen': -1.6580231189727783, 'rewards/rejected': -3.2652440071105957, 'rewards/accuracies': 1.0, 'rewards/margins': 1.607221007347107, 'policy_logps/rejected': -621.8909912109375, 'policy_logps/chosen': -417.7442321777344, 'referece_logps/rejected': -589.238525390625, 'referece_logps/chosen': -401.16400146484375, 'logits/rejected': -0.6669747829437256, 'logits/chosen': -0.6570757627487183, 'epoch': 5.69}

 95%|█████████▍| 10187/10740 [50:28:05<2:28:19, 16.09s/it]

 95%|█████████▍| 10188/10740 [50:28:25<2:37:16, 17.10s/it]

 95%|█████████▍| 10189/10740 [50:28:42<2:36:47, 17.07s/it]


 95%|█████████▍| 10191/10740 [50:29:13<2:31:53, 16.60s/it]
{'loss': 0.362, 'learning_rate': 1.3675297034443922e-08, 'rewards/chosen': -2.028515100479126, 'rewards/rejected': -3.4448204040527344, 'rewards/accuracies': 0.75, 'rewards/margins': 1.416305422782898, 'policy_logps/rejected': -358.4895324707031, 'policy_logps/chosen': -365.4139709472656, 'referece_logps/rejected': -324.0413818359375, 'referece_logps/chosen': -345.1288146972656, 'logits/rejected': -0.5580861568450928, 'logits/chosen': -0.5704420804977417, 'epoch': 5.69}

 95%|█████████▍| 10192/10740 [50:29:34<2:42:09, 17.75s/it]

 95%|█████████▍| 10193/10740 [50:29:53<2:45:19, 18.13s/it]


 95%|█████████▍| 10195/10740 [50:30:31<2:51:28, 18.88s/it]
{'loss': 0.4456, 'learning_rate': 1.3477194460456253e-08, 'rewards/chosen': -2.0633625984191895, 'rewards/rejected': -4.310520172119141, 'rewards/accuracies': 1.0, 'rewards/margins': 2.247157573699951, 'policy_logps/rejected': -632.1909790039062, 'policy_logps/chosen': -556.944580078125, 'referece_logps/rejected': -589.0858154296875, 'referece_logps/chosen': -536.3109741210938, 'logits/rejected': 0.37175896763801575, 'logits/chosen': 0.27813202142715454, 'epoch': 5.7}

 95%|█████████▍| 10196/10740 [50:30:48<2:46:00, 18.31s/it]


 95%|█████████▍| 10198/10740 [50:31:30<2:55:45, 19.46s/it]
{'loss': 0.4126, 'learning_rate': 1.3329559644190913e-08, 'rewards/chosen': -2.039468765258789, 'rewards/rejected': -3.1156375408172607, 'rewards/accuracies': 0.5, 'rewards/margins': 1.0761685371398926, 'policy_logps/rejected': -363.938720703125, 'policy_logps/chosen': -365.60284423828125, 'referece_logps/rejected': -332.7823486328125, 'referece_logps/chosen': -345.2081298828125, 'logits/rejected': -0.5954468250274658, 'logits/chosen': -0.5911349058151245, 'epoch': 5.7}


 95%|█████████▍| 10200/10740 [50:32:09<2:57:31, 19.73s/it]
{'loss': 0.2931, 'learning_rate': 1.3231585126368906e-08, 'rewards/chosen': -1.694730281829834, 'rewards/rejected': -3.304327964782715, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6095974445343018, 'policy_logps/rejected': -397.4275207519531, 'policy_logps/chosen': -446.1320495605469, 'referece_logps/rejected': -364.3842468261719, 'referece_logps/chosen': -429.184814453125, 'logits/rejected': -1.1998909711837769, 'logits/chosen': -1.2257827520370483, 'epoch': 5.7}

 95%|█████████▍| 10201/10740 [50:32:31<3:01:23, 20.19s/it]

 95%|█████████▍| 10202/10740 [50:32:48<2:53:53, 19.39s/it]

 95%|█████████▌| 10203/10740 [50:33:05<2:46:26, 18.60s/it]
[2024-04-03 21:47:08,285] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10204/10740 [50:33:25<2:49:03, 18.93s/it]

 95%|█████████▌| 10205/10740 [50:33:39<2:37:21, 17.65s/it]


 95%|█████████▌| 10207/10740 [50:34:20<2:47:50, 18.89s/it]
{'loss': 0.3256, 'learning_rate': 1.2891501640393432e-08, 'rewards/chosen': -1.5226694345474243, 'rewards/rejected': -3.804028034210205, 'rewards/accuracies': 0.875, 'rewards/margins': 2.281358480453491, 'policy_logps/rejected': -421.1909484863281, 'policy_logps/chosen': -379.51910400390625, 'referece_logps/rejected': -383.15069580078125, 'referece_logps/chosen': -364.2923583984375, 'logits/rejected': -1.615203619003296, 'logits/chosen': -1.674332618713379, 'epoch': 5.7}

 95%|█████████▌| 10208/10740 [50:34:42<2:57:10, 19.98s/it]

 95%|█████████▌| 10209/10740 [50:35:02<2:55:58, 19.88s/it]
[2024-04-03 21:49:07,777] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10210/10740 [50:35:24<3:02:01, 20.61s/it]
[2024-04-03 21:49:29,044] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10211/10740 [50:35:45<3:03:25, 20.80s/it]

 95%|█████████▌| 10212/10740 [50:36:00<2:47:38, 19.05s/it]

 95%|█████████▌| 10213/10740 [50:36:21<2:51:35, 19.54s/it]

 95%|█████████▌| 10214/10740 [50:36:35<2:36:55, 17.90s/it]

 95%|█████████▌| 10215/10740 [50:36:51<2:30:34, 17.21s/it]

 95%|█████████▌| 10216/10740 [50:37:03<2:17:48, 15.78s/it]

 95%|█████████▌| 10217/10740 [50:37:25<2:33:54, 17.66s/it]

 95%|█████████▌| 10218/10740 [50:37:45<2:40:18, 18.43s/it]

 95%|█████████▌| 10219/10740 [50:38:05<2:42:34, 18.72s/it]
[2024-04-03 21:52:08,602] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10220/10740 [50:38:25<2:45:55, 19.15s/it]
[2024-04-03 21:52:28,854] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10221/10740 [50:38:45<2:48:28, 19.48s/it]

 95%|█████████▌| 10222/10740 [50:39:04<2:47:20, 19.38s/it]

 95%|█████████▌| 10223/10740 [50:39:25<2:50:49, 19.83s/it]

 95%|█████████▌| 10224/10740 [50:39:45<2:51:07, 19.90s/it]

 95%|█████████▌| 10225/10740 [50:40:08<2:57:20, 20.66s/it]


 95%|█████████▌| 10227/10740 [50:40:44<2:44:20, 19.22s/it]

 95%|█████████▌| 10228/10740 [50:41:08<2:56:36, 20.70s/it]
[2024-04-03 21:54:51,773] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3383, 'learning_rate': 1.1897652439372575e-08, 'rewards/chosen': -1.503076195716858, 'rewards/rejected': -3.734457492828369, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2313811779022217, 'policy_logps/rejected': -488.6155700683594, 'policy_logps/chosen': -514.88232421875, 'referece_logps/rejected': -451.27099609375, 'referece_logps/chosen': -499.8515625, 'logits/rejected': -0.45046812295913696, 'logits/chosen': -0.43179935216903687, 'epoch': 5.71}
[2024-04-03 21:55:09,352] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10229/10740 [50:41:26<2:48:17, 19.76s/it]
[2024-04-03 21:55:29,354] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 95%|█████████▌| 10230/10740 [50:41:46<2:48:34, 19.83s/it]

 95%|█████████▌| 10231/10740 [50:42:05<2:45:58, 19.57s/it]

 95%|█████████▌| 10232/10740 [50:42:20<2:35:21, 18.35s/it]

 95%|█████████▌| 10233/10740 [50:42:36<2:30:01, 17.76s/it]

 95%|█████████▌| 10234/10740 [50:42:51<2:21:50, 16.82s/it]

 95%|█████████▌| 10235/10740 [50:43:06<2:16:25, 16.21s/it]

 95%|█████████▌| 10236/10740 [50:43:20<2:10:23, 15.52s/it]

 95%|█████████▌| 10237/10740 [50:43:34<2:05:59, 15.03s/it]

 95%|█████████▌| 10238/10740 [50:43:45<1:57:41, 14.07s/it]

 95%|█████████▌| 10239/10740 [50:44:03<2:06:53, 15.20s/it]

{'loss': 0.4008, 'learning_rate': 1.1347530385267701e-08, 'rewards/chosen': -2.060932159423828, 'rewards/rejected': -2.4381134510040283, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3771815001964569, 'policy_logps/rejected': -302.5914001464844, 'policy_logps/chosen': -270.7401428222656, 'referece_logps/rejected': -278.21026611328125, 'referece_logps/chosen': -250.13079833984375, 'logits/rejected': -1.1439006328582764, 'logits/chosen': -1.2757916450500488, 'epoch': 5.72}
 95%|█████████▌| 10240/10740 [50:44:22<2:15:56, 16.31s/it]

 95%|█████████▌| 10241/10740 [50:44:41<2:21:22, 17.00s/it]

 95%|█████████▌| 10242/10740 [50:44:52<2:07:14, 15.33s/it]

 95%|█████████▌| 10243/10740 [50:45:11<2:14:53, 16.28s/it]

 95%|█████████▌| 10244/10740 [50:45:30<2:22:39, 17.26s/it]

 95%|█████████▌| 10245/10740 [50:45:51<2:31:48, 18.40s/it]

 95%|█████████▌| 10246/10740 [50:46:04<2:16:42, 16.60s/it]

 95%|█████████▌| 10247/10740 [50:46:27<2:33:04, 18.63s/it]


 95%|█████████▌| 10249/10740 [50:47:07<2:35:55, 19.05s/it]

 95%|█████████▌| 10250/10740 [50:47:18<2:16:22, 16.70s/it]

 95%|█████████▌| 10251/10740 [50:47:35<2:16:56, 16.80s/it]

 95%|█████████▌| 10252/10740 [50:47:58<2:30:55, 18.56s/it]
{'loss': 0.2561, 'learning_rate': 1.0810356849933699e-08, 'rewards/chosen': -0.8832483887672424, 'rewards/rejected': -2.829335927963257, 'rewards/accuracies': 0.875, 'rewards/margins': 1.94608736038208, 'policy_logps/rejected': -289.27630615234375, 'policy_logps/chosen': -356.4455871582031, 'referece_logps/rejected': -260.98297119140625, 'referece_logps/chosen': -347.61309814453125, 'logits/rejected': -0.18440723419189453, 'logits/chosen': -0.23910969495773315, 'epoch': 5.73}


 95%|█████████▌| 10254/10740 [50:48:38<2:37:33, 19.45s/it]

 95%|█████████▌| 10255/10740 [50:48:52<2:22:41, 17.65s/it]
{'loss': 0.4111, 'learning_rate': 1.067808750035104e-08, 'rewards/chosen': -2.381005048751831, 'rewards/rejected': -3.892949104309082, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5119441747665405, 'policy_logps/rejected': -421.36651611328125, 'policy_logps/chosen': -417.616455078125, 'referece_logps/rejected': -382.43701171875, 'referece_logps/chosen': -393.806396484375, 'logits/rejected': -0.6173233985900879, 'logits/chosen': -0.6546677350997925, 'epoch': 5.73}


 96%|█████████▌| 10257/10740 [50:49:20<2:06:25, 15.71s/it]

 96%|█████████▌| 10258/10740 [50:49:31<1:54:00, 14.19s/it]

 96%|█████████▌| 10259/10740 [50:49:43<1:49:29, 13.66s/it]

 96%|█████████▌| 10260/10740 [50:50:03<2:04:34, 15.57s/it]

 96%|█████████▌| 10261/10740 [50:50:24<2:17:52, 17.27s/it]
[2024-04-03 22:04:07,939] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10262/10740 [50:50:42<2:18:01, 17.32s/it]
[2024-04-03 22:04:25,392] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10263/10740 [50:50:55<2:08:28, 16.16s/it]

 96%|█████████▌| 10264/10740 [50:51:14<2:13:53, 16.88s/it]

 96%|█████████▌| 10265/10740 [50:51:27<2:05:49, 15.89s/it]

 96%|█████████▌| 10266/10740 [50:51:45<2:08:51, 16.31s/it]

 96%|█████████▌| 10267/10740 [50:52:00<2:05:54, 15.97s/it]

 96%|█████████▌| 10268/10740 [50:52:20<2:16:02, 17.29s/it]

 96%|█████████▌| 10269/10740 [50:52:38<2:16:46, 17.42s/it]

 96%|█████████▌| 10270/10740 [50:52:49<2:02:25, 15.63s/it]

 96%|█████████▌| 10271/10740 [50:53:09<2:11:21, 16.80s/it]

 96%|█████████▌| 10272/10740 [50:53:28<2:16:48, 17.54s/it]

 96%|█████████▌| 10273/10740 [50:53:48<2:22:23, 18.29s/it]
[2024-04-03 22:07:31,848] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10274/10740 [50:54:09<2:28:23, 19.11s/it]

 96%|█████████▌| 10275/10740 [50:54:28<2:26:31, 18.91s/it]

 96%|█████████▌| 10276/10740 [50:54:47<2:26:35, 18.96s/it]
[2024-04-03 22:08:30,365] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10277/10740 [50:55:07<2:28:56, 19.30s/it]

 96%|█████████▌| 10278/10740 [50:55:28<2:33:52, 19.98s/it]

 96%|█████████▌| 10279/10740 [50:55:44<2:24:43, 18.84s/it]

 96%|█████████▌| 10280/10740 [50:56:02<2:20:42, 18.35s/it]

 96%|█████████▌| 10281/10740 [50:56:12<2:03:04, 16.09s/it]

 96%|█████████▌| 10282/10740 [50:56:34<2:14:26, 17.61s/it]

 96%|█████████▌| 10283/10740 [50:56:51<2:14:36, 17.67s/it]

 96%|█████████▌| 10284/10740 [50:57:06<2:08:04, 16.85s/it]

 96%|█████████▌| 10285/10740 [50:57:24<2:08:37, 16.96s/it]

 96%|█████████▌| 10286/10740 [50:57:37<2:00:29, 15.92s/it]
{'loss': 0.4558, 'learning_rate': 9.358743691185723e-09, 'rewards/chosen': -1.6689220666885376, 'rewards/rejected': -3.135354995727539, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4664326906204224, 'policy_logps/rejected': -416.3194885253906, 'policy_logps/chosen': -512.677978515625, 'referece_logps/rejected': -384.9659729003906, 'referece_logps/chosen': -495.9887390136719, 'logits/rejected': -0.1943202018737793, 'logits/chosen': -0.2763557434082031, 'epoch': 5.75}


 96%|█████████▌| 10288/10740 [50:58:15<2:11:44, 17.49s/it]

 96%|█████████▌| 10289/10740 [50:58:37<2:22:16, 18.93s/it]

 96%|█████████▌| 10290/10740 [50:58:50<2:09:25, 17.26s/it]
{'loss': 0.2615, 'learning_rate': 9.19481046855719e-09, 'rewards/chosen': -2.765509605407715, 'rewards/rejected': -4.885790824890137, 'rewards/accuracies': 0.875, 'rewards/margins': 2.120281457901001, 'policy_logps/rejected': -562.3107299804688, 'policy_logps/chosen': -455.5694580078125, 'referece_logps/rejected': -513.4528198242188, 'referece_logps/chosen': -427.91436767578125, 'logits/rejected': -0.22852616012096405, 'logits/chosen': -0.24867947399616241, 'epoch': 5.75}


 96%|█████████▌| 10292/10740 [50:59:30<2:19:05, 18.63s/it]

 96%|█████████▌| 10293/10740 [50:59:47<2:15:16, 18.16s/it]

 96%|█████████▌| 10294/10740 [51:00:05<2:15:24, 18.22s/it]

 96%|█████████▌| 10295/10740 [51:00:25<2:18:04, 18.62s/it]

 96%|█████████▌| 10296/10740 [51:00:44<2:17:46, 18.62s/it]

 96%|█████████▌| 10297/10740 [51:00:58<2:07:23, 17.26s/it]

 96%|█████████▌| 10298/10740 [51:01:17<2:12:23, 17.97s/it]

 96%|█████████▌| 10299/10740 [51:01:38<2:18:18, 18.82s/it]

 96%|█████████▌| 10300/10740 [51:02:01<2:26:22, 19.96s/it]

 96%|█████████▌| 10301/10740 [51:02:22<2:28:31, 20.30s/it]
[2024-04-03 22:16:05,605] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10302/10740 [51:02:42<2:27:11, 20.16s/it]
[2024-04-03 22:16:25,447] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10303/10740 [51:02:52<2:06:14, 17.33s/it]

 96%|█████████▌| 10304/10740 [51:03:10<2:07:20, 17.52s/it]

 96%|█████████▌| 10305/10740 [51:03:27<2:04:22, 17.16s/it]

 96%|█████████▌| 10306/10740 [51:03:39<1:53:49, 15.74s/it]

 96%|█████████▌| 10307/10740 [51:04:00<2:05:17, 17.36s/it]
[2024-04-03 22:17:44,024] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10308/10740 [51:04:13<1:54:50, 15.95s/it]
[2024-04-03 22:17:56,682] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10309/10740 [51:04:33<2:02:51, 17.10s/it]

 96%|█████████▌| 10310/10740 [51:04:47<1:55:28, 16.11s/it]

 96%|█████████▌| 10311/10740 [51:05:06<2:02:59, 17.20s/it]

 96%|█████████▌| 10312/10740 [51:05:18<1:52:01, 15.70s/it]

 96%|█████████▌| 10313/10740 [51:05:41<2:06:12, 17.73s/it]
[2024-04-03 22:19:24,697] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10314/10740 [51:05:59<2:06:57, 17.88s/it]

 96%|█████████▌| 10315/10740 [51:06:16<2:04:51, 17.63s/it]

 96%|█████████▌| 10316/10740 [51:06:30<1:56:19, 16.46s/it]

 96%|█████████▌| 10317/10740 [51:06:49<2:00:33, 17.10s/it]

 96%|█████████▌| 10318/10740 [51:07:07<2:02:59, 17.49s/it]
{'loss': 0.3571, 'learning_rate': 8.08766304113162e-09, 'rewards/chosen': -0.7952756285667419, 'rewards/rejected': -4.591580390930176, 'rewards/accuracies': 1.0, 'rewards/margins': 3.796304941177368, 'policy_logps/rejected': -432.05242919921875, 'policy_logps/chosen': -414.7398376464844, 'referece_logps/rejected': -386.13665771484375, 'referece_logps/chosen': -406.7870788574219, 'logits/rejected': -0.7358023524284363, 'logits/chosen': -0.7293727397918701, 'epoch': 5.76}


 96%|█████████▌| 10320/10740 [51:07:36<1:52:12, 16.03s/it]

 96%|█████████▌| 10321/10740 [51:07:56<1:59:47, 17.15s/it]

 96%|█████████▌| 10322/10740 [51:08:18<2:08:26, 18.44s/it]
[2024-04-03 22:22:01,248] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▌| 10323/10740 [51:08:31<1:58:10, 17.00s/it]

 96%|█████████▌| 10324/10740 [51:08:53<2:07:56, 18.45s/it]

 96%|█████████▌| 10325/10740 [51:09:14<2:11:55, 19.07s/it]

 96%|█████████▌| 10326/10740 [51:09:33<2:12:10, 19.16s/it]

 96%|█████████▌| 10327/10740 [51:09:47<2:02:25, 17.79s/it]

 96%|█████████▌| 10328/10740 [51:10:01<1:53:28, 16.52s/it]

 96%|█████████▌| 10329/10740 [51:10:23<2:03:52, 18.08s/it]

 96%|█████████▌| 10330/10740 [51:10:47<2:16:43, 20.01s/it]

 96%|█████████▌| 10331/10740 [51:11:08<2:18:20, 20.30s/it]

 96%|█████████▌| 10332/10740 [51:11:28<2:15:57, 19.99s/it]

 96%|█████████▌| 10333/10740 [51:11:44<2:08:07, 18.89s/it]

 96%|█████████▌| 10334/10740 [51:12:04<2:09:37, 19.16s/it]

 96%|█████████▌| 10335/10740 [51:12:23<2:09:04, 19.12s/it]
{'loss': 0.3347, 'learning_rate': 7.449969298892189e-09, 'rewards/chosen': -1.810211420059204, 'rewards/rejected': -2.716580629348755, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9063690304756165, 'policy_logps/rejected': -379.1638488769531, 'policy_logps/chosen': -368.34478759765625, 'referece_logps/rejected': -351.998046875, 'referece_logps/chosen': -350.24267578125, 'logits/rejected': -1.1881773471832275, 'logits/chosen': -1.2308939695358276, 'epoch': 5.77}


 96%|█████████▌| 10337/10740 [51:13:05<2:16:39, 20.35s/it]
[2024-04-03 22:26:48,859] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3853, 'learning_rate': 7.376661407175521e-09, 'rewards/chosen': -1.988769769668579, 'rewards/rejected': -3.3412108421325684, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3524410724639893, 'policy_logps/rejected': -339.38409423828125, 'policy_logps/chosen': -375.9106750488281, 'referece_logps/rejected': -305.97198486328125, 'referece_logps/chosen': -356.02294921875, 'logits/rejected': -0.7907556295394897, 'logits/chosen': -0.7962726950645447, 'epoch': 5.77}

 96%|█████████▋| 10338/10740 [51:13:24<2:14:05, 20.01s/it]


 96%|█████████▋| 10340/10740 [51:14:06<2:16:15, 20.44s/it]
{'loss': 0.433, 'learning_rate': 7.267376689335858e-09, 'rewards/chosen': -1.5737779140472412, 'rewards/rejected': -3.054360866546631, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4805829524993896, 'policy_logps/rejected': -641.931884765625, 'policy_logps/chosen': -522.8746337890625, 'referece_logps/rejected': -611.3882446289062, 'referece_logps/chosen': -507.1368408203125, 'logits/rejected': -1.1310193538665771, 'logits/chosen': -0.8806233406066895, 'epoch': 5.78}

 96%|█████████▋| 10341/10740 [51:14:26<2:16:04, 20.46s/it]


 96%|█████████▋| 10343/10740 [51:15:05<2:11:21, 19.85s/it]
[2024-04-03 22:28:48,771] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 96%|█████████▋| 10344/10740 [51:15:22<2:04:27, 18.86s/it]
{'loss': 0.3472, 'learning_rate': 7.122927828039249e-09, 'rewards/chosen': -1.693688988685608, 'rewards/rejected': -3.2460007667541504, 'rewards/accuracies': 0.875, 'rewards/margins': 1.552311897277832, 'policy_logps/rejected': -406.4911193847656, 'policy_logps/chosen': -435.0960998535156, 'referece_logps/rejected': -374.0311279296875, 'referece_logps/chosen': -418.1592102050781, 'logits/rejected': -0.3191887140274048, 'logits/chosen': -0.22615957260131836, 'epoch': 5.78}


 96%|█████████▋| 10346/10740 [51:15:50<1:47:38, 16.39s/it]

 96%|█████████▋| 10347/10740 [51:16:05<1:45:19, 16.08s/it]
{'loss': 0.4505, 'learning_rate': 7.015539368891432e-09, 'rewards/chosen': -1.7596640586853027, 'rewards/rejected': -4.694441318511963, 'rewards/accuracies': 1.0, 'rewards/margins': 2.934777021408081, 'policy_logps/rejected': -469.50286865234375, 'policy_logps/chosen': -483.4259033203125, 'referece_logps/rejected': -422.5584716796875, 'referece_logps/chosen': -465.8292236328125, 'logits/rejected': -0.07382619380950928, 'logits/chosen': -0.1504497528076172, 'epoch': 5.78}


 96%|█████████▋| 10349/10740 [51:16:40<1:49:06, 16.74s/it]

 96%|█████████▋| 10350/10740 [51:16:59<1:54:42, 17.65s/it]
{'loss': 0.4227, 'learning_rate': 6.908963739041373e-09, 'rewards/chosen': -1.9675720930099487, 'rewards/rejected': -3.0266060829162598, 'rewards/accuracies': 0.875, 'rewards/margins': 1.059033989906311, 'policy_logps/rejected': -373.2073974609375, 'policy_logps/chosen': -345.3087463378906, 'referece_logps/rejected': -342.94134521484375, 'referece_logps/chosen': -325.633056640625, 'logits/rejected': -0.5830622911453247, 'logits/chosen': -0.5713348984718323, 'epoch': 5.78}


 96%|█████████▋| 10352/10740 [51:17:25<1:38:28, 15.23s/it]

 96%|█████████▋| 10353/10740 [51:17:44<1:45:21, 16.33s/it]
{'loss': 0.3221, 'learning_rate': 6.803201025729066e-09, 'rewards/chosen': -1.7298011779785156, 'rewards/rejected': -3.1998233795166016, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4700219631195068, 'policy_logps/rejected': -291.2206726074219, 'policy_logps/chosen': -225.2555694580078, 'referece_logps/rejected': -259.222412109375, 'referece_logps/chosen': -207.95755004882812, 'logits/rejected': -0.44032204151153564, 'logits/chosen': -0.4646035432815552, 'epoch': 5.78}


 96%|█████████▋| 10355/10740 [51:18:20<1:51:53, 17.44s/it]
{'loss': 0.3519, 'learning_rate': 6.733144213266429e-09, 'rewards/chosen': -1.7415176630020142, 'rewards/rejected': -2.872166633605957, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1306487321853638, 'policy_logps/rejected': -392.7410583496094, 'policy_logps/chosen': -310.19268798828125, 'referece_logps/rejected': -364.0194091796875, 'referece_logps/chosen': -292.7774963378906, 'logits/rejected': -0.8947715163230896, 'logits/chosen': -0.9569498300552368, 'epoch': 5.78}

 96%|█████████▋| 10356/10740 [51:18:37<1:50:08, 17.21s/it]


 96%|█████████▋| 10358/10740 [51:19:14<1:54:12, 17.94s/it]
{'loss': 0.3102, 'learning_rate': 6.628736552909297e-09, 'rewards/chosen': -1.8410784006118774, 'rewards/rejected': -2.6271438598632812, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7860655784606934, 'policy_logps/rejected': -342.3138427734375, 'policy_logps/chosen': -307.915283203125, 'referece_logps/rejected': -316.0423889160156, 'referece_logps/chosen': -289.50445556640625, 'logits/rejected': -0.6966829895973206, 'logits/chosen': -0.7155576348304749, 'epoch': 5.79}

 96%|█████████▋| 10359/10740 [51:19:35<2:00:05, 18.91s/it]


 96%|█████████▋| 10361/10740 [51:20:08<1:52:56, 17.88s/it]

 96%|█████████▋| 10362/10740 [51:20:29<1:56:50, 18.55s/it]
{'loss': 0.3554, 'learning_rate': 6.4907912474351235e-09, 'rewards/chosen': -2.1925747394561768, 'rewards/rejected': -3.5565249919891357, 'rewards/accuracies': 0.75, 'rewards/margins': 1.363950252532959, 'policy_logps/rejected': -330.71368408203125, 'policy_logps/chosen': -505.59649658203125, 'referece_logps/rejected': -295.1484375, 'referece_logps/chosen': -483.6707458496094, 'logits/rejected': -1.3261616230010986, 'logits/chosen': -1.3030617237091064, 'epoch': 5.79}

 96%|█████████▋| 10363/10740 [51:20:47<1:56:01, 18.47s/it]

 96%|█████████▋| 10364/10740 [51:20:59<1:44:20, 16.65s/it]

 97%|█████████▋| 10365/10740 [51:21:19<1:50:34, 17.69s/it]

 97%|█████████▋| 10366/10740 [51:21:35<1:46:37, 17.11s/it]

 97%|█████████▋| 10367/10740 [51:21:55<1:51:23, 17.92s/it]


 97%|█████████▋| 10369/10740 [51:22:34<1:54:49, 18.57s/it]
{'loss': 0.4215, 'learning_rate': 6.252866022845693e-09, 'rewards/chosen': -1.5316494703292847, 'rewards/rejected': -2.41916823387146, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8875188231468201, 'policy_logps/rejected': -364.04254150390625, 'policy_logps/chosen': -350.1526184082031, 'referece_logps/rejected': -339.850830078125, 'referece_logps/chosen': -334.8360900878906, 'logits/rejected': -0.735759437084198, 'logits/chosen': -0.5944574475288391, 'epoch': 5.79}

 97%|█████████▋| 10370/10740 [51:22:51<1:52:47, 18.29s/it]

 97%|█████████▋| 10371/10740 [51:23:03<1:40:13, 16.30s/it]


 97%|█████████▋| 10373/10740 [51:23:36<1:41:04, 16.52s/it]

 97%|█████████▋| 10374/10740 [51:23:48<1:32:32, 15.17s/it]
{'loss': 0.4553, 'learning_rate': 6.085630883987169e-09, 'rewards/chosen': -2.0042054653167725, 'rewards/rejected': -3.1535184383392334, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1493127346038818, 'policy_logps/rejected': -412.7347106933594, 'policy_logps/chosen': -401.7950744628906, 'referece_logps/rejected': -381.1994934082031, 'referece_logps/chosen': -381.75299072265625, 'logits/rejected': -0.26172253489494324, 'logits/chosen': -0.2896452844142914, 'epoch': 5.8}


 97%|█████████▋| 10376/10740 [51:24:25<1:40:29, 16.57s/it]
{'loss': 0.3437, 'learning_rate': 6.019369599852275e-09, 'rewards/chosen': -1.9197309017181396, 'rewards/rejected': -3.1508402824401855, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2311097383499146, 'policy_logps/rejected': -688.4573974609375, 'policy_logps/chosen': -529.1421508789062, 'referece_logps/rejected': -656.9490966796875, 'referece_logps/chosen': -509.9447937011719, 'logits/rejected': -0.5265695452690125, 'logits/chosen': -0.47167277336120605, 'epoch': 5.8}


 97%|█████████▋| 10378/10740 [51:25:03<1:47:00, 17.74s/it]
{'loss': 0.5092, 'learning_rate': 5.953469935613076e-09, 'rewards/chosen': -1.5641348361968994, 'rewards/rejected': -3.079221725463867, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5150867700576782, 'policy_logps/rejected': -283.62017822265625, 'policy_logps/chosen': -337.5589904785156, 'referece_logps/rejected': -252.82794189453125, 'referece_logps/chosen': -321.9176025390625, 'logits/rejected': -0.7461806535720825, 'logits/chosen': -0.9846383333206177, 'epoch': 5.8}

 97%|█████████▋| 10379/10740 [51:25:25<1:55:23, 19.18s/it]

 97%|█████████▋| 10380/10740 [51:25:45<1:56:37, 19.44s/it]

 97%|█████████▋| 10381/10740 [51:25:59<1:46:43, 17.84s/it]

 97%|█████████▋| 10382/10740 [51:26:19<1:49:54, 18.42s/it]


 97%|█████████▋| 10384/10740 [51:27:01<1:56:25, 19.62s/it]
{'loss': 0.2969, 'learning_rate': 5.757940901361724e-09, 'rewards/chosen': -2.6797797679901123, 'rewards/rejected': -4.667342662811279, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9875630140304565, 'policy_logps/rejected': -458.5132141113281, 'policy_logps/chosen': -376.7460021972656, 'referece_logps/rejected': -411.8398132324219, 'referece_logps/chosen': -349.9482116699219, 'logits/rejected': -0.9588994383811951, 'logits/chosen': -0.8823366165161133, 'epoch': 5.8}


 97%|█████████▋| 10386/10740 [51:27:39<1:54:56, 19.48s/it]
{'loss': 0.3914, 'learning_rate': 5.693487955139243e-09, 'rewards/chosen': -2.195462703704834, 'rewards/rejected': -4.146773338317871, 'rewards/accuracies': 0.875, 'rewards/margins': 1.951310634613037, 'policy_logps/rejected': -379.7683410644531, 'policy_logps/chosen': -452.3261413574219, 'referece_logps/rejected': -338.30059814453125, 'referece_logps/chosen': -430.37152099609375, 'logits/rejected': -0.7612693309783936, 'logits/chosen': -0.8176180124282837, 'epoch': 5.8}

 97%|█████████▋| 10387/10740 [51:27:57<1:52:38, 19.15s/it]


 97%|█████████▋| 10389/10740 [51:28:40<1:58:21, 20.23s/it]

 97%|█████████▋| 10390/10740 [51:28:57<1:51:35, 19.13s/it]

 97%|█████████▋| 10391/10740 [51:29:14<1:48:31, 18.66s/it]

 97%|█████████▋| 10392/10740 [51:29:34<1:50:34, 19.06s/it]

 97%|█████████▋| 10393/10740 [51:29:53<1:49:34, 18.95s/it]
{'loss': 0.4816, 'learning_rate': 5.470751486385317e-09, 'rewards/chosen': -2.1813387870788574, 'rewards/rejected': -3.771742343902588, 'rewards/accuracies': 0.75, 'rewards/margins': 1.59040367603302, 'policy_logps/rejected': -429.3469543457031, 'policy_logps/chosen': -444.3184509277344, 'referece_logps/rejected': -391.6295166015625, 'referece_logps/chosen': -422.5050354003906, 'logits/rejected': -0.4957858920097351, 'logits/chosen': -0.6322544813156128, 'epoch': 5.81}

 97%|█████████▋| 10394/10740 [51:30:08<1:41:56, 17.68s/it]

 97%|█████████▋| 10395/10740 [51:30:27<1:45:08, 18.28s/it]


 97%|█████████▋| 10397/10740 [51:31:07<1:49:03, 19.08s/it]

 97%|█████████▋| 10398/10740 [51:31:25<1:47:33, 18.87s/it]
{'loss': 0.4162, 'learning_rate': 5.314367596841673e-09, 'rewards/chosen': -2.0444514751434326, 'rewards/rejected': -3.4786298274993896, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4341787099838257, 'policy_logps/rejected': -412.8321533203125, 'policy_logps/chosen': -378.2157287597656, 'referece_logps/rejected': -378.0458679199219, 'referece_logps/chosen': -357.77117919921875, 'logits/rejected': -0.7957397699356079, 'logits/chosen': -0.6303296685218811, 'epoch': 5.81}


 97%|█████████▋| 10400/10740 [51:31:57<1:40:14, 17.69s/it]

 97%|█████████▋| 10401/10740 [51:32:15<1:41:07, 17.90s/it]

 97%|█████████▋| 10402/10740 [51:32:35<1:44:04, 18.48s/it]

 97%|█████████▋| 10403/10740 [51:32:55<1:46:12, 18.91s/it]
{'loss': 0.3509, 'learning_rate': 5.160245434328137e-09, 'rewards/chosen': -1.6819286346435547, 'rewards/rejected': -2.984431028366089, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3025023937225342, 'policy_logps/rejected': -315.72332763671875, 'policy_logps/chosen': -356.96356201171875, 'referece_logps/rejected': -285.8789978027344, 'referece_logps/chosen': -340.144287109375, 'logits/rejected': -0.14755818247795105, 'logits/chosen': -0.17609156668186188, 'epoch': 5.81}

 97%|█████████▋| 10404/10740 [51:33:16<1:48:55, 19.45s/it]


 97%|█████████▋| 10406/10740 [51:33:53<1:45:14, 18.91s/it]

 97%|█████████▋| 10407/10740 [51:34:13<1:46:05, 19.12s/it]
{'loss': 0.3533, 'learning_rate': 5.038576383450399e-09, 'rewards/chosen': -2.2716946601867676, 'rewards/rejected': -3.3749635219573975, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1032689809799194, 'policy_logps/rejected': -269.4367980957031, 'policy_logps/chosen': -304.9935302734375, 'referece_logps/rejected': -235.68716430664062, 'referece_logps/chosen': -282.276611328125, 'logits/rejected': -0.5888292789459229, 'logits/chosen': -0.595760703086853, 'epoch': 5.81}


 97%|█████████▋| 10409/10740 [51:34:48<1:42:19, 18.55s/it]

 97%|█████████▋| 10410/10740 [51:35:09<1:44:43, 19.04s/it]
{'loss': 0.2682, 'learning_rate': 4.948274774486672e-09, 'rewards/chosen': -1.7993632555007935, 'rewards/rejected': -3.380265951156616, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5809025764465332, 'policy_logps/rejected': -325.80853271484375, 'policy_logps/chosen': -348.3436279296875, 'referece_logps/rejected': -292.0058898925781, 'referece_logps/chosen': -330.3500061035156, 'logits/rejected': -0.006016790866851807, 'logits/chosen': 0.03266824036836624, 'epoch': 5.82}

 97%|█████████▋| 10411/10740 [51:35:30<1:48:31, 19.79s/it]

 97%|█████████▋| 10412/10740 [51:35:50<1:47:45, 19.71s/it]


 97%|█████████▋| 10414/10740 [51:36:19<1:32:49, 17.08s/it]

 97%|█████████▋| 10415/10740 [51:36:39<1:36:58, 17.90s/it]
{'loss': 0.4757, 'learning_rate': 4.799582176865602e-09, 'rewards/chosen': -1.3468254804611206, 'rewards/rejected': -3.9076693058013916, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5608441829681396, 'policy_logps/rejected': -368.2424011230469, 'policy_logps/chosen': -385.4938659667969, 'referece_logps/rejected': -329.16571044921875, 'referece_logps/chosen': -372.0256042480469, 'logits/rejected': -1.172655463218689, 'logits/chosen': -1.2710459232330322, 'epoch': 5.82}


 97%|█████████▋| 10417/10740 [51:37:15<1:37:59, 18.20s/it]

 97%|█████████▋| 10418/10740 [51:37:31<1:33:29, 17.42s/it]

 97%|█████████▋| 10419/10740 [51:37:47<1:32:10, 17.23s/it]

 97%|█████████▋| 10420/10740 [51:38:05<1:32:24, 17.33s/it]

 97%|█████████▋| 10421/10740 [51:38:21<1:29:38, 16.86s/it]

 97%|█████████▋| 10422/10740 [51:38:41<1:34:54, 17.91s/it]
[2024-04-03 22:52:24,872] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 97%|█████████▋| 10423/10740 [51:39:01<1:37:30, 18.46s/it]
{'loss': 0.3123, 'learning_rate': 4.566380986308349e-09, 'rewards/chosen': -2.872575283050537, 'rewards/rejected': -4.491891860961914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6193162202835083, 'policy_logps/rejected': -478.78741455078125, 'policy_logps/chosen': -475.67041015625, 'referece_logps/rejected': -433.8684997558594, 'referece_logps/chosen': -446.9446105957031, 'logits/rejected': -1.0356156826019287, 'logits/chosen': -0.9994792938232422, 'epoch': 5.82}


 97%|█████████▋| 10425/10740 [51:39:25<1:19:34, 15.16s/it]

 97%|█████████▋| 10426/10740 [51:39:37<1:15:06, 14.35s/it]

 97%|█████████▋| 10427/10740 [51:39:49<1:10:29, 13.51s/it]
{'loss': 0.5571, 'learning_rate': 4.451953197561642e-09, 'rewards/chosen': -2.031398296356201, 'rewards/rejected': -3.1014657020568848, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0700674057006836, 'policy_logps/rejected': -364.2835388183594, 'policy_logps/chosen': -389.9496765136719, 'referece_logps/rejected': -333.2688903808594, 'referece_logps/chosen': -369.6357116699219, 'logits/rejected': -0.019716769456863403, 'logits/chosen': -0.03403232991695404, 'epoch': 5.83}

 97%|█████████▋| 10428/10740 [51:40:06<1:16:18, 14.68s/it]
[2024-04-03 22:54:11,600] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 97%|█████████▋| 10430/10740 [51:40:49<1:33:17, 18.06s/it]

 97%|█████████▋| 10431/10740 [51:41:01<1:24:11, 16.35s/it]

 97%|█████████▋| 10432/10740 [51:41:16<1:21:06, 15.80s/it]

 97%|█████████▋| 10433/10740 [51:41:35<1:25:55, 16.79s/it]
{'loss': 0.4489, 'learning_rate': 4.283027991684563e-09, 'rewards/chosen': -1.6531399488449097, 'rewards/rejected': -3.651118755340576, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9979790449142456, 'policy_logps/rejected': -396.2440490722656, 'policy_logps/chosen': -349.90435791015625, 'referece_logps/rejected': -359.73284912109375, 'referece_logps/chosen': -333.3729553222656, 'logits/rejected': 0.2422645390033722, 'logits/chosen': 0.2605642080307007, 'epoch': 5.83}


 97%|█████████▋| 10435/10740 [51:42:08<1:22:29, 16.23s/it]
{'loss': 0.4236, 'learning_rate': 4.227444065730945e-09, 'rewards/chosen': -2.1701478958129883, 'rewards/rejected': -3.291168689727783, 'rewards/accuracies': 0.5, 'rewards/margins': 1.121021032333374, 'policy_logps/rejected': -344.23052978515625, 'policy_logps/chosen': -297.2573547363281, 'referece_logps/rejected': -311.31884765625, 'referece_logps/chosen': -275.5558776855469, 'logits/rejected': -0.3062639832496643, 'logits/chosen': -0.1209641695022583, 'epoch': 5.83}
[2024-04-03 22:56:08,173] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 97%|█████████▋| 10437/10740 [51:42:40<1:20:38, 15.97s/it]

 97%|█████████▋| 10438/10740 [51:42:55<1:20:04, 15.91s/it]

 97%|█████████▋| 10439/10740 [51:43:15<1:25:59, 17.14s/it]
{'loss': 0.3553, 'learning_rate': 4.117363049361389e-09, 'rewards/chosen': -2.086359977722168, 'rewards/rejected': -3.8774778842926025, 'rewards/accuracies': 0.875, 'rewards/margins': 1.791117787361145, 'policy_logps/rejected': -373.887451171875, 'policy_logps/chosen': -352.8140869140625, 'referece_logps/rejected': -335.1126708984375, 'referece_logps/chosen': -331.95050048828125, 'logits/rejected': -0.5999189019203186, 'logits/chosen': -0.5784094333648682, 'epoch': 5.83}

 97%|█████████▋| 10440/10740 [51:43:32<1:25:15, 17.05s/it]


 97%|█████████▋| 10442/10740 [51:44:06<1:22:23, 16.59s/it]
{'loss': 0.5828, 'learning_rate': 4.03575334696038e-09, 'rewards/chosen': -1.8929204940795898, 'rewards/rejected': -2.7481436729431152, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8552231788635254, 'policy_logps/rejected': -371.4179992675781, 'policy_logps/chosen': -404.75823974609375, 'referece_logps/rejected': -343.9365539550781, 'referece_logps/chosen': -385.82904052734375, 'logits/rejected': -0.1511784791946411, 'logits/chosen': -0.2574007511138916, 'epoch': 5.83}


 97%|█████████▋| 10444/10740 [51:44:40<1:22:21, 16.70s/it]
{'loss': 0.3073, 'learning_rate': 3.9817998015231335e-09, 'rewards/chosen': -1.165150761604309, 'rewards/rejected': -3.4819507598876953, 'rewards/accuracies': 1.0, 'rewards/margins': 2.316800117492676, 'policy_logps/rejected': -286.75653076171875, 'policy_logps/chosen': -334.772705078125, 'referece_logps/rejected': -251.93702697753906, 'referece_logps/chosen': -323.1212158203125, 'logits/rejected': -1.2531569004058838, 'logits/chosen': -1.4617878198623657, 'epoch': 5.83}


 97%|█████████▋| 10446/10740 [51:45:16<1:25:29, 17.45s/it]
{'loss': 0.3359, 'learning_rate': 3.928208617269502e-09, 'rewards/chosen': -2.4507007598876953, 'rewards/rejected': -3.561674118041992, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1109728813171387, 'policy_logps/rejected': -388.8953857421875, 'policy_logps/chosen': -298.2431945800781, 'referece_logps/rejected': -353.27862548828125, 'referece_logps/chosen': -273.7361755371094, 'logits/rejected': -0.8786602020263672, 'logits/chosen': -0.8838098049163818, 'epoch': 5.84}

 97%|█████████▋| 10447/10740 [51:45:31<1:21:58, 16.79s/it]

 97%|█████████▋| 10448/10740 [51:45:53<1:29:28, 18.38s/it]

 97%|█████████▋| 10449/10740 [51:46:05<1:19:48, 16.46s/it]


 97%|█████████▋| 10451/10740 [51:46:46<1:29:51, 18.66s/it]
[2024-04-03 23:00:29,953] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 97%|█████████▋| 10452/10740 [51:47:06<1:31:21, 19.03s/it]

 97%|█████████▋| 10453/10740 [51:47:20<1:23:47, 17.52s/it]

 97%|█████████▋| 10454/10740 [51:47:35<1:20:27, 16.88s/it]

 97%|█████████▋| 10455/10740 [51:47:57<1:27:11, 18.36s/it]
[2024-04-03 23:01:41,040] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 97%|█████████▋| 10456/10740 [51:48:20<1:32:59, 19.64s/it]
[2024-04-03 23:02:03,691] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2644, 'learning_rate': 3.66568879153617e-09, 'rewards/chosen': -1.3248193264007568, 'rewards/rejected': -3.809814453125, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4849956035614014, 'policy_logps/rejected': -520.1661376953125, 'policy_logps/chosen': -502.16546630859375, 'referece_logps/rejected': -482.06793212890625, 'referece_logps/chosen': -488.917236328125, 'logits/rejected': -0.3948388397693634, 'logits/chosen': -0.45035791397094727, 'epoch': 5.84}


 97%|█████████▋| 10458/10740 [51:49:00<1:33:18, 19.85s/it]
{'loss': 0.3889, 'learning_rate': 3.6142721792066233e-09, 'rewards/chosen': -1.39358651638031, 'rewards/rejected': -4.702569007873535, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3089823722839355, 'policy_logps/rejected': -328.5107421875, 'policy_logps/chosen': -422.5201416015625, 'referece_logps/rejected': -281.48504638671875, 'referece_logps/chosen': -408.58428955078125, 'logits/rejected': -0.45357924699783325, 'logits/chosen': -0.5889101624488831, 'epoch': 5.84}

 97%|█████████▋| 10459/10740 [51:49:17<1:29:03, 19.02s/it]


 97%|█████████▋| 10461/10740 [51:49:54<1:26:20, 18.57s/it]

 97%|█████████▋| 10462/10740 [51:50:14<1:27:58, 18.99s/it]
[2024-04-03 23:03:57,804] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 97%|█████████▋| 10463/10740 [51:50:30<1:24:00, 18.20s/it]
{'loss': 0.4906, 'learning_rate': 3.4873166041375914e-09, 'rewards/chosen': -1.6230672597885132, 'rewards/rejected': -2.1255478858947754, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5024805068969727, 'policy_logps/rejected': -608.5310668945312, 'policy_logps/chosen': -409.250244140625, 'referece_logps/rejected': -587.2755737304688, 'referece_logps/chosen': -393.01959228515625, 'logits/rejected': -1.1029266119003296, 'logits/chosen': -1.1425273418426514, 'epoch': 5.85}

 97%|█████████▋| 10464/10740 [51:50:49<1:24:02, 18.27s/it]


 97%|█████████▋| 10466/10740 [51:51:30<1:28:13, 19.32s/it]
[2024-04-03 23:05:13,893] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4365, 'learning_rate': 3.412230863906629e-09, 'rewards/chosen': -2.365499496459961, 'rewards/rejected': -3.415888786315918, 'rewards/accuracies': 1.0, 'rewards/margins': 1.050389289855957, 'policy_logps/rejected': -540.7905883789062, 'policy_logps/chosen': -546.364990234375, 'referece_logps/rejected': -506.6317138671875, 'referece_logps/chosen': -522.7100219726562, 'logits/rejected': -0.3290945887565613, 'logits/chosen': -0.2265123426914215, 'epoch': 5.85}

 97%|█████████▋| 10467/10740 [51:51:43<1:19:36, 17.50s/it]

 97%|█████████▋| 10468/10740 [51:51:59<1:16:44, 16.93s/it]

 97%|█████████▋| 10469/10740 [51:52:13<1:13:01, 16.17s/it]


 97%|█████████▋| 10471/10740 [51:52:48<1:16:24, 17.04s/it]

 98%|█████████▊| 10472/10740 [51:53:06<1:17:13, 17.29s/it]

 98%|█████████▊| 10473/10740 [51:53:26<1:20:24, 18.07s/it]
{'loss': 0.3572, 'learning_rate': 3.2402033817154893e-09, 'rewards/chosen': -1.248887062072754, 'rewards/rejected': -3.2003109455108643, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9514238834381104, 'policy_logps/rejected': -477.8057861328125, 'policy_logps/chosen': -281.9941711425781, 'referece_logps/rejected': -445.80267333984375, 'referece_logps/chosen': -269.50531005859375, 'logits/rejected': -1.2092382907867432, 'logits/chosen': -1.2688045501708984, 'epoch': 5.85}

 98%|█████████▊| 10474/10740 [51:53:46<1:22:28, 18.60s/it]

 98%|█████████▊| 10475/10740 [51:54:06<1:23:49, 18.98s/it]

 98%|█████████▊| 10476/10740 [51:54:24<1:22:01, 18.64s/it]


 98%|█████████▊| 10478/10740 [51:55:04<1:25:58, 19.69s/it]

 98%|█████████▊| 10479/10740 [51:55:22<1:23:37, 19.23s/it]

 98%|█████████▊| 10480/10740 [51:55:45<1:27:25, 20.17s/it]
{'loss': 0.3631, 'learning_rate': 3.0726181275592123e-09, 'rewards/chosen': -2.2433254718780518, 'rewards/rejected': -3.2443854808807373, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0010597705841064, 'policy_logps/rejected': -378.0318908691406, 'policy_logps/chosen': -406.2541809082031, 'referece_logps/rejected': -345.58807373046875, 'referece_logps/chosen': -383.8208923339844, 'logits/rejected': -0.5836910009384155, 'logits/chosen': -0.6804878115653992, 'epoch': 5.85}

 98%|█████████▊| 10481/10740 [51:56:05<1:27:30, 20.27s/it]

 98%|█████████▊| 10482/10740 [51:56:24<1:25:01, 19.77s/it]

 98%|█████████▊| 10483/10740 [51:56:43<1:24:17, 19.68s/it]


 98%|█████████▊| 10485/10740 [51:57:15<1:15:14, 17.70s/it]
{'loss': 0.3852, 'learning_rate': 2.9556345205428157e-09, 'rewards/chosen': -1.9436665773391724, 'rewards/rejected': -3.2564616203308105, 'rewards/accuracies': 0.75, 'rewards/margins': 1.312794804573059, 'policy_logps/rejected': -461.7515869140625, 'policy_logps/chosen': -433.7253723144531, 'referece_logps/rejected': -429.1869812011719, 'referece_logps/chosen': -414.2887268066406, 'logits/rejected': -0.6927686929702759, 'logits/chosen': -0.6276694536209106, 'epoch': 5.86}

 98%|█████████▊| 10486/10740 [51:57:34<1:16:14, 18.01s/it]
[2024-04-03 23:11:37,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10487/10740 [51:57:53<1:18:03, 18.51s/it]

 98%|█████████▊| 10488/10740 [51:58:10<1:15:29, 17.97s/it]
[2024-04-03 23:12:16,973] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10489/10740 [51:58:33<1:21:46, 19.55s/it]
[2024-04-03 23:12:37,029] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 98%|█████████▊| 10491/10740 [51:59:07<1:13:49, 17.79s/it]
{'loss': 0.3267, 'learning_rate': 2.818246774420663e-09, 'rewards/chosen': -1.8136957883834839, 'rewards/rejected': -3.0116806030273438, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1979849338531494, 'policy_logps/rejected': -378.10064697265625, 'policy_logps/chosen': -429.9019470214844, 'referece_logps/rejected': -347.9838562011719, 'referece_logps/chosen': -411.7650146484375, 'logits/rejected': -0.5699077248573303, 'logits/chosen': -0.6494914293289185, 'epoch': 5.86}

 98%|█████████▊| 10492/10740 [51:59:20<1:08:14, 16.51s/it]

 98%|█████████▊| 10493/10740 [51:59:36<1:07:30, 16.40s/it]

 98%|█████████▊| 10494/10740 [51:59:56<1:11:18, 17.39s/it]


 98%|█████████▊| 10496/10740 [52:00:27<1:04:37, 15.89s/it]
{'loss': 0.475, 'learning_rate': 2.7062511100186538e-09, 'rewards/chosen': -3.057666778564453, 'rewards/rejected': -3.7416388988494873, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6839720606803894, 'policy_logps/rejected': -353.77142333984375, 'policy_logps/chosen': -374.7662658691406, 'referece_logps/rejected': -316.35504150390625, 'referece_logps/chosen': -344.1895751953125, 'logits/rejected': -1.4153934717178345, 'logits/chosen': -1.3789466619491577, 'epoch': 5.86}

 98%|█████████▊| 10497/10740 [52:00:46<1:08:30, 16.92s/it]

 98%|█████████▊| 10498/10740 [52:01:04<1:09:56, 17.34s/it]
[2024-04-03 23:15:08,200] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10499/10740 [52:01:24<1:13:10, 18.22s/it]
[2024-04-03 23:15:28,243] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10500/10740 [52:01:45<1:15:03, 18.77s/it]

 98%|█████████▊| 10501/10740 [52:02:21<1:36:12, 24.15s/it]
[2024-04-03 23:16:25,596] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10502/10740 [52:02:42<1:31:36, 23.10s/it]
[2024-04-03 23:16:48,162] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10503/10740 [52:03:04<1:30:36, 22.94s/it]

 98%|█████████▊| 10504/10740 [52:03:20<1:21:15, 20.66s/it]

 98%|█████████▊| 10505/10740 [52:03:37<1:17:27, 19.78s/it]

 98%|█████████▊| 10506/10740 [52:03:58<1:17:56, 19.98s/it]

 98%|█████████▊| 10507/10740 [52:04:15<1:13:39, 18.97s/it]


 98%|█████████▊| 10509/10740 [52:04:51<1:12:52, 18.93s/it]

 98%|█████████▊| 10510/10740 [52:05:09<1:11:26, 18.64s/it]
[2024-04-03 23:18:52,881] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4188, 'learning_rate': 2.404727980141463e-09, 'rewards/chosen': -0.8998138308525085, 'rewards/rejected': -2.404439926147461, 'rewards/accuracies': 0.875, 'rewards/margins': 1.504625916481018, 'policy_logps/rejected': -254.8640594482422, 'policy_logps/chosen': -302.49285888671875, 'referece_logps/rejected': -230.8196563720703, 'referece_logps/chosen': -293.4947204589844, 'logits/rejected': -1.078837275505066, 'logits/chosen': -1.0730124711990356, 'epoch': 5.87}

 98%|█████████▊| 10511/10740 [52:05:26<1:08:39, 17.99s/it]

 98%|█████████▊| 10512/10740 [52:05:40<1:04:39, 17.02s/it]

 98%|█████████▊| 10513/10740 [52:06:00<1:07:37, 17.88s/it]


 98%|█████████▊| 10515/10740 [52:06:43<1:13:51, 19.69s/it]
{'loss': 0.3778, 'learning_rate': 2.301350724718376e-09, 'rewards/chosen': -2.363658905029297, 'rewards/rejected': -4.494724750518799, 'rewards/accuracies': 0.875, 'rewards/margins': 2.131065845489502, 'policy_logps/rejected': -318.5423583984375, 'policy_logps/chosen': -367.7547302246094, 'referece_logps/rejected': -273.5951232910156, 'referece_logps/chosen': -344.1181335449219, 'logits/rejected': -0.9158095121383667, 'logits/chosen': -1.1129142045974731, 'epoch': 5.87}

 98%|█████████▊| 10516/10740 [52:07:02<1:12:17, 19.36s/it]

 98%|█████████▊| 10517/10740 [52:07:21<1:11:30, 19.24s/it]


 98%|█████████▊| 10519/10740 [52:07:53<1:04:17, 17.45s/it]
{'loss': 0.4224, 'learning_rate': 2.2202822854738846e-09, 'rewards/chosen': -1.2801339626312256, 'rewards/rejected': -2.809967279434204, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5298333168029785, 'policy_logps/rejected': -484.01446533203125, 'policy_logps/chosen': -477.45037841796875, 'referece_logps/rejected': -455.91473388671875, 'referece_logps/chosen': -464.6490173339844, 'logits/rejected': -0.4420430064201355, 'logits/chosen': -0.5682675242424011, 'epoch': 5.88}

 98%|█████████▊| 10520/10740 [52:08:04<56:43, 15.47s/it]

 98%|█████████▊| 10521/10740 [52:08:24<1:00:50, 16.67s/it]

 98%|█████████▊| 10522/10740 [52:08:39<58:36, 16.13s/it]


 98%|█████████▊| 10524/10740 [52:09:13<1:01:07, 16.98s/it]
{'loss': 0.5701, 'learning_rate': 2.120988636336918e-09, 'rewards/chosen': -1.9964346885681152, 'rewards/rejected': -3.079756736755371, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0833219289779663, 'policy_logps/rejected': -470.04412841796875, 'policy_logps/chosen': -650.4639282226562, 'referece_logps/rejected': -439.24664306640625, 'referece_logps/chosen': -630.4996948242188, 'logits/rejected': -0.9134976863861084, 'logits/chosen': -0.9901532530784607, 'epoch': 5.88}

 98%|█████████▊| 10525/10740 [52:09:32<1:02:44, 17.51s/it]

 98%|█████████▊| 10526/10740 [52:09:52<1:04:42, 18.14s/it]

 98%|█████████▊| 10527/10740 [52:10:11<1:05:23, 18.42s/it]

 98%|█████████▊| 10528/10740 [52:10:31<1:06:31, 18.83s/it]

 98%|█████████▊| 10529/10740 [52:10:48<1:05:09, 18.53s/it]


 98%|█████████▊| 10531/10740 [52:11:18<56:31, 16.23s/it]
{'loss': 0.3195, 'learning_rate': 1.98578947726169e-09, 'rewards/chosen': -1.7861859798431396, 'rewards/rejected': -4.393722057342529, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6075363159179688, 'policy_logps/rejected': -364.5927734375, 'policy_logps/chosen': -326.3567199707031, 'referece_logps/rejected': -320.655517578125, 'referece_logps/chosen': -308.494873046875, 'logits/rejected': -0.3302457630634308, 'logits/chosen': -0.4148668348789215, 'epoch': 5.88}

 98%|█████████▊| 10532/10740 [52:11:38<1:00:17, 17.39s/it]

 98%|█████████▊| 10533/10740 [52:11:54<58:47, 17.04s/it]


 98%|█████████▊| 10535/10740 [52:12:29<1:00:12, 17.62s/it]

 98%|█████████▊| 10536/10740 [52:12:49<1:02:19, 18.33s/it]
{'loss': 0.4055, 'learning_rate': 1.8919417692109253e-09, 'rewards/chosen': -1.4824846982955933, 'rewards/rejected': -2.390395402908325, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9079108834266663, 'policy_logps/rejected': -345.1715393066406, 'policy_logps/chosen': -289.5535888671875, 'referece_logps/rejected': -321.2676086425781, 'referece_logps/chosen': -274.7287902832031, 'logits/rejected': -1.487884283065796, 'logits/chosen': -1.4741753339767456, 'epoch': 5.89}

 98%|█████████▊| 10537/10740 [52:13:04<58:28, 17.28s/it]

 98%|█████████▊| 10538/10740 [52:13:22<58:58, 17.52s/it]


 98%|█████████▊| 10540/10740 [52:14:06<1:05:40, 19.70s/it]
{'loss': 0.3895, 'learning_rate': 1.8184976391657502e-09, 'rewards/chosen': -2.552948474884033, 'rewards/rejected': -3.629194736480713, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0762462615966797, 'policy_logps/rejected': -303.10736083984375, 'policy_logps/chosen': -391.3459777832031, 'referece_logps/rejected': -266.8154296875, 'referece_logps/chosen': -365.8164978027344, 'logits/rejected': -1.1464605331420898, 'logits/chosen': -1.2837523221969604, 'epoch': 5.89}

 98%|█████████▊| 10541/10740 [52:14:27<1:06:49, 20.15s/it]

 98%|█████████▊| 10542/10740 [52:14:43<1:03:06, 19.12s/it]

 98%|█████████▊| 10543/10740 [52:15:03<1:03:23, 19.31s/it]

 98%|█████████▊| 10544/10740 [52:15:16<56:55, 17.43s/it]

 98%|█████████▊| 10545/10740 [52:15:35<58:08, 17.89s/it]

 98%|█████████▊| 10546/10740 [52:15:53<57:37, 17.82s/it]

 98%|█████████▊| 10547/10740 [52:16:07<54:04, 16.81s/it]

 98%|█████████▊| 10548/10740 [52:16:23<52:22, 16.37s/it]

 98%|█████████▊| 10549/10740 [52:16:40<53:28, 16.80s/it]
[2024-04-03 23:30:42,667] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10550/10740 [52:16:59<54:48, 17.31s/it]

 98%|█████████▊| 10551/10740 [52:17:15<53:38, 17.03s/it]

 98%|█████████▊| 10552/10740 [52:17:35<55:38, 17.76s/it]

 98%|█████████▊| 10553/10740 [52:17:55<57:39, 18.50s/it]


 98%|█████████▊| 10555/10740 [52:18:32<57:30, 18.65s/it]

 98%|█████████▊| 10556/10740 [52:18:52<58:15, 19.00s/it]

 98%|█████████▊| 10557/10740 [52:19:08<55:22, 18.16s/it]
{'loss': 0.3149, 'learning_rate': 1.5225668258663783e-09, 'rewards/chosen': -1.5515992641448975, 'rewards/rejected': -3.6332995891571045, 'rewards/accuracies': 0.75, 'rewards/margins': 2.081700325012207, 'policy_logps/rejected': -533.050537109375, 'policy_logps/chosen': -476.3049621582031, 'referece_logps/rejected': -496.7175598144531, 'referece_logps/chosen': -460.78900146484375, 'logits/rejected': -0.09016221016645432, 'logits/chosen': 0.012887315824627876, 'epoch': 5.9}

 98%|█████████▊| 10558/10740 [52:19:24<53:36, 17.67s/it]

 98%|█████████▊| 10559/10740 [52:19:44<54:48, 18.17s/it]

 98%|█████████▊| 10560/10740 [52:19:58<50:39, 16.89s/it]
[2024-04-03 23:34:02,388] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10561/10740 [52:20:19<53:59, 18.10s/it]

 98%|█████████▊| 10562/10740 [52:20:41<57:53, 19.51s/it]
[2024-04-03 23:34:43,607] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10563/10740 [52:21:00<56:34, 19.18s/it]
[2024-04-03 23:35:03,812] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 98%|█████████▊| 10564/10740 [52:21:20<57:09, 19.49s/it]

 98%|█████████▊| 10565/10740 [52:21:40<57:08, 19.59s/it]

 98%|█████████▊| 10566/10740 [52:22:01<58:29, 20.17s/it]

 98%|█████████▊| 10567/10740 [52:22:21<57:25, 19.91s/it]

 98%|█████████▊| 10568/10740 [52:22:35<51:48, 18.07s/it]

 98%|█████████▊| 10569/10740 [52:22:56<54:21, 19.07s/it]

 98%|█████████▊| 10570/10740 [52:23:14<53:04, 18.74s/it]

 98%|█████████▊| 10571/10740 [52:23:34<53:32, 19.01s/it]

 98%|█████████▊| 10572/10740 [52:23:53<53:48, 19.22s/it]

 98%|█████████▊| 10573/10740 [52:24:13<53:54, 19.37s/it]

 98%|█████████▊| 10574/10740 [52:24:33<53:51, 19.47s/it]

 98%|█████████▊| 10575/10740 [52:24:46<48:42, 17.71s/it]

 98%|█████████▊| 10576/10740 [52:25:00<44:56, 16.44s/it]

 98%|█████████▊| 10577/10740 [52:25:13<41:51, 15.41s/it]

 98%|█████████▊| 10578/10740 [52:25:28<41:35, 15.41s/it]

 99%|█████████▊| 10579/10740 [52:25:42<40:26, 15.07s/it]

 99%|█████████▊| 10580/10740 [52:25:56<38:36, 14.48s/it]

 99%|█████████▊| 10581/10740 [52:26:12<39:36, 14.95s/it]

 99%|█████████▊| 10582/10740 [52:26:33<44:37, 16.95s/it]

 99%|█████████▊| 10583/10740 [52:26:46<41:00, 15.67s/it]

 99%|█████████▊| 10584/10740 [52:27:08<45:35, 17.54s/it]

 99%|█████████▊| 10585/10740 [52:27:28<47:11, 18.27s/it]

 99%|█████████▊| 10586/10740 [52:27:48<48:21, 18.84s/it]

 99%|█████████▊| 10587/10740 [52:28:06<47:30, 18.63s/it]

 99%|█████████▊| 10588/10740 [52:28:26<48:27, 19.13s/it]

 99%|█████████▊| 10589/10740 [52:28:42<45:37, 18.13s/it]

 99%|█████████▊| 10590/10740 [52:29:02<46:40, 18.67s/it]

 99%|█████████▊| 10591/10740 [52:29:16<42:46, 17.23s/it]

 99%|█████████▊| 10592/10740 [52:29:30<40:15, 16.32s/it]

 99%|█████████▊| 10593/10740 [52:29:47<40:26, 16.50s/it]

 99%|█████████▊| 10594/10740 [52:30:04<40:35, 16.68s/it]

 99%|█████████▊| 10595/10740 [52:30:23<41:40, 17.25s/it]

 99%|█████████▊| 10596/10740 [52:30:45<44:45, 18.65s/it]

 99%|█████████▊| 10597/10740 [52:31:02<43:51, 18.40s/it]

 99%|█████████▊| 10598/10740 [52:31:23<44:58, 19.00s/it]
[2024-04-03 23:45:28,209] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▊| 10599/10740 [52:31:44<46:29, 19.78s/it]
[2024-04-03 23:45:48,472] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▊| 10600/10740 [52:32:05<46:29, 19.93s/it]

 99%|█████████▊| 10601/10740 [52:32:25<46:33, 20.10s/it]

 99%|█████████▊| 10602/10740 [52:32:43<44:25, 19.31s/it]

 99%|█████████▊| 10603/10740 [52:33:01<43:23, 19.00s/it]

 99%|█████████▊| 10604/10740 [52:33:20<43:07, 19.03s/it]

 99%|█████████▊| 10605/10740 [52:33:40<43:27, 19.32s/it]

 99%|█████████▉| 10606/10740 [52:33:59<42:38, 19.09s/it]

 99%|█████████▉| 10607/10740 [52:34:19<43:04, 19.43s/it]

 99%|█████████▉| 10608/10740 [52:34:40<43:58, 19.99s/it]


 99%|█████████▉| 10610/10740 [52:35:13<40:06, 18.51s/it]

 99%|█████████▉| 10611/10740 [52:35:28<37:32, 17.46s/it]

 99%|█████████▉| 10612/10740 [52:35:46<37:45, 17.70s/it]

 99%|█████████▉| 10613/10740 [52:36:00<35:02, 16.55s/it]

 99%|█████████▉| 10614/10740 [52:36:18<35:41, 16.99s/it]

 99%|█████████▉| 10615/10740 [52:36:33<33:49, 16.24s/it]

 99%|█████████▉| 10616/10740 [52:36:52<35:42, 17.28s/it]

 99%|█████████▉| 10617/10740 [52:37:10<35:40, 17.40s/it]

 99%|█████████▉| 10618/10740 [52:37:26<34:35, 17.01s/it]

 99%|█████████▉| 10619/10740 [52:37:47<36:36, 18.15s/it]
[2024-04-03 23:51:30,634] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10620/10740 [52:38:07<37:24, 18.70s/it]

 99%|█████████▉| 10621/10740 [52:38:18<32:46, 16.52s/it]

 99%|█████████▉| 10622/10740 [52:38:36<33:08, 16.85s/it]

 99%|█████████▉| 10623/10740 [52:38:53<32:49, 16.83s/it]

 99%|█████████▉| 10624/10740 [52:39:08<31:55, 16.51s/it]

 99%|█████████▉| 10625/10740 [52:39:26<32:10, 16.79s/it]

 99%|█████████▉| 10626/10740 [52:39:43<31:57, 16.82s/it]

 99%|█████████▉| 10627/10740 [52:39:59<31:16, 16.61s/it]

 99%|█████████▉| 10628/10740 [52:40:18<32:39, 17.49s/it]

 99%|█████████▉| 10629/10740 [52:40:34<31:13, 16.87s/it]

 99%|█████████▉| 10630/10740 [52:40:52<31:42, 17.29s/it]
[2024-04-03 23:54:35,930] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10631/10740 [52:41:08<30:40, 16.89s/it]

 99%|█████████▉| 10632/10740 [52:41:26<31:01, 17.23s/it]

 99%|█████████▉| 10633/10740 [52:41:46<32:03, 17.98s/it]

 99%|█████████▉| 10634/10740 [52:42:06<32:51, 18.60s/it]

 99%|█████████▉| 10635/10740 [52:42:18<29:06, 16.64s/it]

 99%|█████████▉| 10636/10740 [52:42:39<31:13, 18.01s/it]

 99%|█████████▉| 10637/10740 [52:42:56<30:23, 17.70s/it]

 99%|█████████▉| 10638/10740 [52:43:16<31:07, 18.31s/it]

 99%|█████████▉| 10639/10740 [52:43:35<31:02, 18.44s/it]

 99%|█████████▉| 10640/10740 [52:43:55<31:43, 19.04s/it]
[2024-04-03 23:57:38,836] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10641/10740 [52:44:19<33:34, 20.35s/it]

 99%|█████████▉| 10642/10740 [52:44:37<32:15, 19.75s/it]

 99%|█████████▉| 10643/10740 [52:44:55<31:00, 19.18s/it]

 99%|█████████▉| 10644/10740 [52:45:16<31:31, 19.70s/it]
[2024-04-03 23:58:59,379] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10645/10740 [52:45:36<31:39, 20.00s/it]

 99%|█████████▉| 10646/10740 [52:45:58<32:19, 20.63s/it]

 99%|█████████▉| 10647/10740 [52:46:17<31:13, 20.15s/it]

 99%|█████████▉| 10648/10740 [52:46:37<30:42, 20.03s/it]

 99%|█████████▉| 10649/10740 [52:46:54<28:48, 18.99s/it]

 99%|█████████▉| 10650/10740 [52:47:16<29:56, 19.96s/it]
[2024-04-04 00:00:59,729] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10651/10740 [52:47:37<30:15, 20.40s/it]

 99%|█████████▉| 10652/10740 [52:47:58<30:12, 20.59s/it]

 99%|█████████▉| 10653/10740 [52:48:19<29:43, 20.50s/it]

 99%|█████████▉| 10654/10740 [52:48:35<27:23, 19.11s/it]

 99%|█████████▉| 10655/10740 [52:48:46<23:36, 16.66s/it]

 99%|█████████▉| 10656/10740 [52:49:05<24:33, 17.54s/it]

 99%|█████████▉| 10657/10740 [52:49:19<22:49, 16.50s/it]

 99%|█████████▉| 10658/10740 [52:49:39<23:51, 17.46s/it]

 99%|█████████▉| 10659/10740 [52:49:58<24:21, 18.05s/it]

 99%|█████████▉| 10660/10740 [52:50:21<25:45, 19.31s/it]

 99%|█████████▉| 10661/10740 [52:50:42<26:08, 19.85s/it]
[2024-04-04 00:04:25,464] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10662/10740 [52:51:02<25:53, 19.92s/it]
[2024-04-04 00:04:45,547] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10663/10740 [52:51:23<26:06, 20.34s/it]

 99%|█████████▉| 10664/10740 [52:51:42<25:12, 19.90s/it]

 99%|█████████▉| 10665/10740 [52:51:57<22:59, 18.39s/it]

 99%|█████████▉| 10666/10740 [52:52:15<22:23, 18.16s/it]

 99%|█████████▉| 10667/10740 [52:52:31<21:38, 17.79s/it]
[2024-04-04 00:06:15,178] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10668/10740 [52:52:53<22:34, 18.82s/it]

 99%|█████████▉| 10669/10740 [52:53:05<19:57, 16.87s/it]

 99%|█████████▉| 10670/10740 [52:53:25<20:53, 17.90s/it]
[2024-04-04 00:07:09,021] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 99%|█████████▉| 10671/10740 [52:53:42<20:00, 17.40s/it]

 99%|█████████▉| 10672/10740 [52:54:01<20:30, 18.09s/it]

 99%|█████████▉| 10673/10740 [52:54:21<20:41, 18.53s/it]

 99%|█████████▉| 10674/10740 [52:54:39<20:24, 18.55s/it]
{'loss': 0.2431, 'learning_rate': 1.9808790272590747e-10, 'rewards/chosen': -1.4332717657089233, 'rewards/rejected': -4.049138069152832, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6158664226531982, 'policy_logps/rejected': -506.17010498046875, 'policy_logps/chosen': -473.6005859375, 'referece_logps/rejected': -465.6787109375, 'referece_logps/chosen': -459.2678527832031, 'logits/rejected': -0.5311363935470581, 'logits/chosen': -0.4433821439743042, 'epoch': 5.96}


 99%|█████████▉| 10676/10740 [52:55:14<19:21, 18.15s/it]

 99%|█████████▉| 10677/10740 [52:55:34<19:37, 18.69s/it]

 99%|█████████▉| 10678/10740 [52:55:54<19:34, 18.94s/it]

 99%|█████████▉| 10679/10740 [52:56:13<19:30, 19.19s/it]

 99%|█████████▉| 10680/10740 [52:56:30<18:29, 18.49s/it]
{'loss': 0.3339, 'learning_rate': 1.637099485789495e-10, 'rewards/chosen': -2.060073137283325, 'rewards/rejected': -4.296047687530518, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2359747886657715, 'policy_logps/rejected': -526.2431640625, 'policy_logps/chosen': -470.702880859375, 'referece_logps/rejected': -483.28265380859375, 'referece_logps/chosen': -450.1021728515625, 'logits/rejected': -1.2645978927612305, 'logits/chosen': -1.2860877513885498, 'epoch': 5.97}

 99%|█████████▉| 10681/10740 [52:56:45<17:08, 17.42s/it]

 99%|█████████▉| 10682/10740 [52:56:59<15:49, 16.37s/it]

 99%|█████████▉| 10683/10740 [52:57:13<14:46, 15.56s/it]

 99%|█████████▉| 10684/10740 [52:57:31<15:17, 16.39s/it]

 99%|█████████▉| 10685/10740 [52:57:47<14:58, 16.33s/it]

 99%|█████████▉| 10686/10740 [52:57:59<13:30, 15.01s/it]


100%|█████████▉| 10688/10740 [52:58:40<15:31, 17.92s/it]

100%|█████████▉| 10689/10740 [52:58:52<13:48, 16.24s/it]
{'loss': 0.4776, 'learning_rate': 1.1828133345070933e-10, 'rewards/chosen': -1.9065015316009521, 'rewards/rejected': -3.3720827102661133, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4655811786651611, 'policy_logps/rejected': -483.7185974121094, 'policy_logps/chosen': -362.2755432128906, 'referece_logps/rejected': -449.997802734375, 'referece_logps/chosen': -343.21051025390625, 'logits/rejected': -1.2275567054748535, 'logits/chosen': -1.1918087005615234, 'epoch': 5.97}


100%|█████████▉| 10691/10740 [52:59:28<13:58, 17.11s/it]
{'loss': 0.5259, 'learning_rate': 1.0918643295487751e-10, 'rewards/chosen': -2.572190999984741, 'rewards/rejected': -2.3673255443573, 'rewards/accuracies': 0.25, 'rewards/margins': -0.20486566424369812, 'policy_logps/rejected': -536.904541015625, 'policy_logps/chosen': -589.87060546875, 'referece_logps/rejected': -513.2312622070312, 'referece_logps/chosen': -564.148681640625, 'logits/rejected': -0.3703576624393463, 'logits/chosen': -0.48947808146476746, 'epoch': 5.97}

100%|█████████▉| 10692/10740 [52:59:39<12:12, 15.27s/it]


100%|█████████▉| 10694/10740 [53:00:08<11:09, 14.56s/it]

100%|█████████▉| 10695/10740 [53:00:26<11:50, 15.78s/it]

100%|█████████▉| 10696/10740 [53:00:46<12:19, 16.81s/it]

100%|█████████▉| 10697/10740 [53:01:07<13:01, 18.17s/it]
{'loss': 0.3569, 'learning_rate': 8.408436451068368e-11, 'rewards/chosen': -1.9062926769256592, 'rewards/rejected': -3.172011375427246, 'rewards/accuracies': 0.75, 'rewards/margins': 1.265718698501587, 'policy_logps/rejected': -579.8568115234375, 'policy_logps/chosen': -538.1904907226562, 'referece_logps/rejected': -548.1367797851562, 'referece_logps/chosen': -519.1275634765625, 'logits/rejected': -1.0807214975357056, 'logits/chosen': -1.0573320388793945, 'epoch': 5.98}


100%|█████████▉| 10699/10740 [53:01:42<12:21, 18.09s/it]

100%|█████████▉| 10700/10740 [53:02:04<12:48, 19.21s/it]
{'loss': 0.4211, 'learning_rate': 7.27610801086742e-11, 'rewards/chosen': -2.2191450595855713, 'rewards/rejected': -3.405606985092163, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1864616870880127, 'policy_logps/rejected': -339.55340576171875, 'policy_logps/chosen': -374.65533447265625, 'referece_logps/rejected': -305.4972839355469, 'referece_logps/chosen': -352.4638977050781, 'logits/rejected': -0.03963851183652878, 'logits/chosen': -0.013080023229122162, 'epoch': 5.98}


100%|█████████▉| 10702/10740 [53:02:39<11:34, 18.27s/it]

100%|█████████▉| 10703/10740 [53:02:59<11:40, 18.93s/it]

100%|█████████▉| 10704/10740 [53:03:19<11:28, 19.14s/it]
{'loss': 0.369, 'learning_rate': 5.893661068567812e-11, 'rewards/chosen': -1.6361454725265503, 'rewards/rejected': -2.9163756370544434, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2802304029464722, 'policy_logps/rejected': -472.5889892578125, 'policy_logps/chosen': -437.09600830078125, 'referece_logps/rejected': -443.4252624511719, 'referece_logps/chosen': -420.73455810546875, 'logits/rejected': 0.015432417392730713, 'logits/chosen': -0.06571421027183533, 'epoch': 5.98}

100%|█████████▉| 10705/10740 [53:03:35<10:44, 18.41s/it]

100%|█████████▉| 10706/10740 [53:03:53<10:21, 18.28s/it]


100%|█████████▉| 10708/10740 [53:04:31<09:45, 18.31s/it]

100%|█████████▉| 10709/10740 [53:04:49<09:26, 18.26s/it]

100%|█████████▉| 10710/10740 [53:05:11<09:43, 19.46s/it]

100%|█████████▉| 10711/10740 [53:05:28<09:01, 18.69s/it]

100%|█████████▉| 10712/10740 [53:05:44<08:25, 18.05s/it]

100%|█████████▉| 10713/10740 [53:06:03<08:12, 18.23s/it]

100%|█████████▉| 10714/10740 [53:06:15<07:01, 16.19s/it]

100%|█████████▉| 10715/10740 [53:06:34<07:11, 17.28s/it]
{'loss': 0.2918, 'learning_rate': 2.8422506956116143e-11, 'rewards/chosen': -1.8305026292800903, 'rewards/rejected': -3.3093276023864746, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4788249731063843, 'policy_logps/rejected': -343.90716552734375, 'policy_logps/chosen': -317.3617248535156, 'referece_logps/rejected': -310.81390380859375, 'referece_logps/chosen': -299.05670166015625, 'logits/rejected': -0.6308737993240356, 'logits/chosen': -0.7105521559715271, 'epoch': 5.99}


100%|█████████▉| 10717/10740 [53:07:09<06:40, 17.42s/it]

100%|█████████▉| 10718/10740 [53:07:28<06:34, 17.92s/it]

100%|█████████▉| 10719/10740 [53:07:47<06:18, 18.01s/it]
{'loss': 0.3814, 'learning_rate': 2.005494887702408e-11, 'rewards/chosen': -1.0836843252182007, 'rewards/rejected': -3.4231340885162354, 'rewards/accuracies': 0.875, 'rewards/margins': 2.339449405670166, 'policy_logps/rejected': -400.1960144042969, 'policy_logps/chosen': -387.0321960449219, 'referece_logps/rejected': -365.9647216796875, 'referece_logps/chosen': -376.1953430175781, 'logits/rejected': 0.3076705038547516, 'logits/chosen': 0.23113228380680084, 'epoch': 5.99}
[2024-04-04 00:21:51,664] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


100%|█████████▉| 10721/10740 [53:08:21<05:24, 17.10s/it]

100%|█████████▉| 10722/10740 [53:08:41<05:23, 18.00s/it]

100%|█████████▉| 10723/10740 [53:09:01<05:15, 18.54s/it]
{'loss': 0.3481, 'learning_rate': 1.3142600686300021e-11, 'rewards/chosen': -1.2241103649139404, 'rewards/rejected': -3.8569021224975586, 'rewards/accuracies': 1.0, 'rewards/margins': 2.632791757583618, 'policy_logps/rejected': -332.1868896484375, 'policy_logps/chosen': -322.4349060058594, 'referece_logps/rejected': -293.6178894042969, 'referece_logps/chosen': -310.1938171386719, 'logits/rejected': -0.6677594780921936, 'logits/chosen': -0.7924829125404358, 'epoch': 5.99}


100%|█████████▉| 10725/10740 [53:09:36<04:31, 18.11s/it]

100%|█████████▉| 10726/10740 [53:09:56<04:21, 18.65s/it]

100%|█████████▉| 10727/10740 [53:10:17<04:09, 19.19s/it]

100%|█████████▉| 10728/10740 [53:10:39<04:00, 20.03s/it]

100%|█████████▉| 10729/10740 [53:10:57<03:34, 19.48s/it]

100%|█████████▉| 10730/10740 [53:11:16<03:11, 19.19s/it]

100%|█████████▉| 10731/10740 [53:11:36<02:55, 19.47s/it]

100%|█████████▉| 10732/10740 [53:11:57<02:40, 20.02s/it]

100%|█████████▉| 10733/10740 [53:12:15<02:16, 19.46s/it]

100%|█████████▉| 10734/10740 [53:12:33<01:53, 18.90s/it]

100%|█████████▉| 10735/10740 [53:12:45<01:24, 16.96s/it]

100%|█████████▉| 10736/10740 [53:13:06<01:13, 18.27s/it]

100%|█████████▉| 10737/10740 [53:13:27<00:56, 18.85s/it]
[2024-04-04 00:27:10,383] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4182, 'learning_rate': 4.09286011082699e-13, 'rewards/chosen': -1.8049416542053223, 'rewards/rejected': -2.997220277786255, 'rewards/accuracies': 0.875, 'rewards/margins': 1.192278504371643, 'policy_logps/rejected': -476.4045715332031, 'policy_logps/chosen': -532.9022827148438, 'referece_logps/rejected': -446.432373046875, 'referece_logps/chosen': -514.8529052734375, 'logits/rejected': -0.664415717124939, 'logits/chosen': -0.536857545375824, 'epoch': 6.0}


100%|█████████▉| 10739/10740 [53:14:01<00:17, 17.88s/it]

100%|██████████| 10740/10740 [53:14:20<00:00, 17.85s/it]
{'loss': 0.3345, 'learning_rate': 0.0, 'rewards/chosen': -1.517296314239502, 'rewards/rejected': -2.9656002521514893, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4483036994934082, 'policy_logps/rejected': -434.2344665527344, 'policy_logps/chosen': -394.2467346191406, 'referece_logps/rejected': -404.5784606933594, 'referece_logps/chosen': -379.0737609863281, 'logits/rejected': -1.360655426979065, 'logits/chosen': -1.456480622291565, 'epoch': 6.0}
{'train_runtime': 191668.8399, 'train_samples_per_second': 3.585, 'train_steps_per_second': 0.056, 'train_loss': 0.42754886539815967, 'epoch': 6.0}