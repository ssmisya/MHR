  0%|          | 0/10740 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/code/mm/MAPO/m3apo/alignment/trainer/llava_dpo_trainer.py:179: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/10740 [00:23<69:57:36, 23.45s/it]

  0%|          | 2/10740 [00:44<65:34:11, 21.98s/it]
{'loss': 0.6931, 'learning_rate': 1.2383900928792569e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'policy_logps/rejected': -361.7127990722656, 'policy_logps/chosen': -316.69866943359375, 'referece_logps/rejected': -361.7127990722656, 'referece_logps/chosen': -316.69866943359375, 'logits/rejected': -1.5316078662872314, 'logits/chosen': -1.5054025650024414, 'epoch': 0.0}


  0%|          | 4/10740 [01:20<56:59:36, 19.11s/it]
{'loss': 0.6921, 'learning_rate': 2.4767801857585138e-08, 'rewards/chosen': 0.01473083533346653, 'rewards/rejected': -0.01699218899011612, 'rewards/accuracies': 0.625, 'rewards/margins': 0.0317230224609375, 'policy_logps/rejected': -319.35302734375, 'policy_logps/chosen': -380.0462341308594, 'referece_logps/rejected': -319.18310546875, 'referece_logps/chosen': -380.19354248046875, 'logits/rejected': 0.05681214854121208, 'logits/chosen': -0.011161673814058304, 'epoch': 0.0}

  0%|          | 5/10740 [01:40<58:30:18, 19.62s/it]

  0%|          | 6/10740 [02:02<61:05:05, 20.49s/it]


  0%|          | 8/10740 [02:42<60:22:21, 20.25s/it]
[2024-04-01 19:15:59,876] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6933, 'learning_rate': 4.9535603715170276e-08, 'rewards/chosen': 0.01516580581665039, 'rewards/rejected': -0.03157520294189453, 'rewards/accuracies': 0.5, 'rewards/margins': 0.04674101248383522, 'policy_logps/rejected': -345.328857421875, 'policy_logps/chosen': -408.52972412109375, 'referece_logps/rejected': -345.0130920410156, 'referece_logps/chosen': -408.681396484375, 'logits/rejected': -0.6204758882522583, 'logits/chosen': -0.5497544407844543, 'epoch': 0.0}

  0%|          | 9/10740 [02:58<56:48:36, 19.06s/it]

  0%|          | 10/10740 [03:16<56:04:33, 18.81s/it]


  0%|          | 12/10740 [03:52<54:00:50, 18.13s/it]
{'loss': 0.6953, 'learning_rate': 7.430340557275541e-08, 'rewards/chosen': -0.0012104026973247528, 'rewards/rejected': -0.006598282139748335, 'rewards/accuracies': 0.5, 'rewards/margins': 0.005387879908084869, 'policy_logps/rejected': -325.9239807128906, 'policy_logps/chosen': -302.7826843261719, 'referece_logps/rejected': -325.85797119140625, 'referece_logps/chosen': -302.7705993652344, 'logits/rejected': -0.9085516333580017, 'logits/chosen': -0.8175564408302307, 'epoch': 0.01}

  0%|          | 13/10740 [04:07<51:08:22, 17.16s/it]

  0%|          | 14/10740 [04:17<45:08:18, 15.15s/it]


  0%|          | 16/10740 [04:46<43:22:22, 14.56s/it]
{'loss': 0.6991, 'learning_rate': 9.907120743034055e-08, 'rewards/chosen': -0.04022827371954918, 'rewards/rejected': 0.006605005823075771, 'rewards/accuracies': 0.25, 'rewards/margins': -0.04683327674865723, 'policy_logps/rejected': -325.4239501953125, 'policy_logps/chosen': -517.2354736328125, 'referece_logps/rejected': -325.4900207519531, 'referece_logps/chosen': -516.8331298828125, 'logits/rejected': -0.9405611753463745, 'logits/chosen': -1.0966160297393799, 'epoch': 0.01}


  0%|          | 18/10740 [05:28<52:44:35, 17.71s/it]
{'loss': 0.6963, 'learning_rate': 1.1145510835913312e-07, 'rewards/chosen': 0.016919422894716263, 'rewards/rejected': -0.005146313458681107, 'rewards/accuracies': 0.75, 'rewards/margins': 0.02206573635339737, 'policy_logps/rejected': -198.4834442138672, 'policy_logps/chosen': -200.7984619140625, 'referece_logps/rejected': -198.43197631835938, 'referece_logps/chosen': -200.9676971435547, 'logits/rejected': -0.7467299699783325, 'logits/chosen': -0.7315724492073059, 'epoch': 0.01}

  0%|          | 19/10740 [05:48<55:12:42, 18.54s/it]

  0%|          | 20/10740 [06:08<56:24:27, 18.94s/it]
[2024-04-01 19:19:47,427] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 21/10740 [06:29<58:10:31, 19.54s/it]

  0%|          | 22/10740 [06:46<55:51:12, 18.76s/it]


  0%|          | 24/10740 [07:14<49:15:31, 16.55s/it]
{'loss': 0.699, 'learning_rate': 1.4860681114551082e-07, 'rewards/chosen': -0.02095966413617134, 'rewards/rejected': 0.016913603991270065, 'rewards/accuracies': 0.125, 'rewards/margins': -0.037873268127441406, 'policy_logps/rejected': -365.6460876464844, 'policy_logps/chosen': -350.01654052734375, 'referece_logps/rejected': -365.8152160644531, 'referece_logps/chosen': -349.8069152832031, 'logits/rejected': -1.3811765909194946, 'logits/chosen': -1.6376363039016724, 'epoch': 0.01}

  0%|          | 25/10740 [07:34<52:49:18, 17.75s/it]

  0%|          | 26/10740 [07:51<51:24:49, 17.28s/it]

  0%|          | 27/10740 [08:02<46:35:58, 15.66s/it]


  0%|          | 29/10740 [08:42<52:10:33, 17.54s/it]

  0%|          | 30/10740 [09:02<54:11:11, 18.21s/it]
{'loss': 0.6882, 'learning_rate': 1.8575851393188855e-07, 'rewards/chosen': 0.06377629935741425, 'rewards/rejected': 0.006973552983254194, 'rewards/accuracies': 0.625, 'rewards/margins': 0.05680274963378906, 'policy_logps/rejected': -411.9883728027344, 'policy_logps/chosen': -666.7546997070312, 'referece_logps/rejected': -412.05810546875, 'referece_logps/chosen': -667.3925170898438, 'logits/rejected': 0.0535675585269928, 'logits/chosen': 0.003347426652908325, 'epoch': 0.02}

  0%|          | 31/10740 [09:20<54:43:23, 18.40s/it]

  0%|          | 32/10740 [09:38<54:17:02, 18.25s/it]

  0%|          | 33/10740 [09:57<54:16:44, 18.25s/it]

  0%|          | 34/10740 [10:14<53:02:54, 17.84s/it]

  0%|          | 35/10740 [10:30<52:07:37, 17.53s/it]

  0%|          | 36/10740 [10:50<53:39:12, 18.04s/it]

  0%|          | 37/10740 [11:04<50:40:43, 17.05s/it]

  0%|          | 38/10740 [11:23<52:04:59, 17.52s/it]

  0%|          | 39/10740 [11:39<50:21:41, 16.94s/it]


  0%|          | 41/10740 [12:16<52:45:04, 17.75s/it]
{'loss': 0.6887, 'learning_rate': 2.5386996904024765e-07, 'rewards/chosen': 0.0027060499414801598, 'rewards/rejected': 0.0011169430799782276, 'rewards/accuracies': 0.375, 'rewards/margins': 0.0015891063958406448, 'policy_logps/rejected': -399.5122375488281, 'policy_logps/chosen': -379.2442321777344, 'referece_logps/rejected': -399.5233459472656, 'referece_logps/chosen': -379.2712707519531, 'logits/rejected': -0.6458569765090942, 'logits/chosen': -0.5947045683860779, 'epoch': 0.02}

  0%|          | 42/10740 [12:31<50:03:01, 16.84s/it]

  0%|          | 43/10740 [12:46<48:58:05, 16.48s/it]


  0%|          | 45/10740 [13:24<51:44:15, 17.42s/it]

  0%|          | 46/10740 [13:44<53:53:24, 18.14s/it]
{'loss': 0.6909, 'learning_rate': 2.848297213622291e-07, 'rewards/chosen': -0.01738138310611248, 'rewards/rejected': 0.018088150769472122, 'rewards/accuracies': 0.25, 'rewards/margins': -0.03546953201293945, 'policy_logps/rejected': -286.51239013671875, 'policy_logps/chosen': -409.298583984375, 'referece_logps/rejected': -286.6932373046875, 'referece_logps/chosen': -409.124755859375, 'logits/rejected': -1.3432718515396118, 'logits/chosen': -1.443124532699585, 'epoch': 0.03}


  0%|          | 48/10740 [14:24<56:08:05, 18.90s/it]

  0%|          | 49/10740 [14:36<50:17:11, 16.93s/it]
{'loss': 0.695, 'learning_rate': 3.0340557275541794e-07, 'rewards/chosen': 0.010614014230668545, 'rewards/rejected': 0.009315871633589268, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0012981421314179897, 'policy_logps/rejected': -393.7582092285156, 'policy_logps/chosen': -327.59735107421875, 'referece_logps/rejected': -393.85137939453125, 'referece_logps/chosen': -327.7034912109375, 'logits/rejected': -0.3677516579627991, 'logits/chosen': -0.17722824215888977, 'epoch': 0.03}

  0%|          | 50/10740 [14:47<45:08:39, 15.20s/it]
[2024-04-01 19:28:20,032] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 51/10740 [15:02<44:23:36, 14.95s/it]

  0%|          | 52/10740 [15:20<47:03:42, 15.85s/it]

  0%|          | 53/10740 [15:31<42:58:13, 14.47s/it]

  1%|          | 54/10740 [15:48<45:05:46, 15.19s/it]

  1%|          | 55/10740 [16:11<51:56:41, 17.50s/it]

  1%|          | 56/10740 [16:28<51:35:03, 17.38s/it]

  1%|          | 57/10740 [16:49<54:51:17, 18.49s/it]

  1%|          | 58/10740 [17:04<51:47:12, 17.45s/it]


  1%|          | 60/10740 [17:48<59:02:43, 19.90s/it]
{'loss': 0.6956, 'learning_rate': 3.715170278637771e-07, 'rewards/chosen': -0.007112312596291304, 'rewards/rejected': 0.01714649237692356, 'rewards/accuracies': 0.25, 'rewards/margins': -0.024258803576231003, 'policy_logps/rejected': -289.00103759765625, 'policy_logps/chosen': -263.0892028808594, 'referece_logps/rejected': -289.1725158691406, 'referece_logps/chosen': -263.01806640625, 'logits/rejected': -0.6461992263793945, 'logits/chosen': -0.6327297687530518, 'epoch': 0.03}


  1%|          | 62/10740 [18:31<60:35:56, 20.43s/it]
[2024-04-01 19:31:48,891] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.694, 'learning_rate': 3.8390092879256965e-07, 'rewards/chosen': 0.021912384778261185, 'rewards/rejected': -0.006695843767374754, 'rewards/accuracies': 0.375, 'rewards/margins': 0.028608229011297226, 'policy_logps/rejected': -367.5406188964844, 'policy_logps/chosen': -647.71240234375, 'referece_logps/rejected': -367.4736633300781, 'referece_logps/chosen': -647.9314575195312, 'logits/rejected': 0.5508943796157837, 'logits/chosen': 0.7586382627487183, 'epoch': 0.03}

  1%|          | 63/10740 [18:48<57:40:09, 19.44s/it]


  1%|          | 65/10740 [19:27<57:15:04, 19.31s/it]

  1%|          | 66/10740 [19:49<59:42:59, 20.14s/it]
{'loss': 0.6934, 'learning_rate': 4.0866873065015477e-07, 'rewards/chosen': 0.006648064590990543, 'rewards/rejected': -0.022584058344364166, 'rewards/accuracies': 0.5, 'rewards/margins': 0.029232122004032135, 'policy_logps/rejected': -341.3472595214844, 'policy_logps/chosen': -404.62445068359375, 'referece_logps/rejected': -341.12139892578125, 'referece_logps/chosen': -404.69091796875, 'logits/rejected': -0.8170320391654968, 'logits/chosen': -0.6802616119384766, 'epoch': 0.04}

  1%|          | 67/10740 [20:09<59:43:41, 20.15s/it]

  1%|          | 68/10740 [20:28<58:32:07, 19.75s/it]

  1%|          | 69/10740 [20:45<56:19:25, 19.00s/it]
[2024-04-01 19:34:25,692] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 70/10740 [21:07<59:22:13, 20.03s/it]
[2024-04-01 19:34:46,035] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 71/10740 [21:28<59:38:31, 20.12s/it]

  1%|          | 72/10740 [21:43<55:41:58, 18.80s/it]


  1%|          | 74/10740 [22:19<53:05:37, 17.92s/it]

  1%|          | 75/10740 [22:35<51:34:56, 17.41s/it]
{'loss': 0.6964, 'learning_rate': 4.643962848297213e-07, 'rewards/chosen': -0.005400943569839001, 'rewards/rejected': -0.03306169435381889, 'rewards/accuracies': 0.5, 'rewards/margins': 0.027660753577947617, 'policy_logps/rejected': -310.8560485839844, 'policy_logps/chosen': -364.1610107421875, 'referece_logps/rejected': -310.52545166015625, 'referece_logps/chosen': -364.10699462890625, 'logits/rejected': -0.19864924252033234, 'logits/chosen': -0.15741407871246338, 'epoch': 0.04}

  1%|          | 76/10740 [22:56<55:15:45, 18.66s/it]


  1%|          | 78/10740 [23:41<60:03:59, 20.28s/it]

  1%|          | 79/10740 [24:01<59:52:11, 20.22s/it]
{'loss': 0.6932, 'learning_rate': 4.891640866873064e-07, 'rewards/chosen': 0.013777542859315872, 'rewards/rejected': 0.024927139282226562, 'rewards/accuracies': 0.375, 'rewards/margins': -0.011149597354233265, 'policy_logps/rejected': -302.6812438964844, 'policy_logps/chosen': -227.16502380371094, 'referece_logps/rejected': -302.9305114746094, 'referece_logps/chosen': -227.30279541015625, 'logits/rejected': -0.044038478285074234, 'logits/chosen': -0.09171856939792633, 'epoch': 0.04}


  1%|          | 81/10740 [24:41<59:24:26, 20.06s/it]

  1%|          | 82/10740 [25:03<61:04:48, 20.63s/it]
{'loss': 0.6875, 'learning_rate': 5.077399380804953e-07, 'rewards/chosen': 0.0068111419677734375, 'rewards/rejected': -0.020175551995635033, 'rewards/accuracies': 0.5, 'rewards/margins': 0.02698669582605362, 'policy_logps/rejected': -317.807861328125, 'policy_logps/chosen': -317.9012145996094, 'referece_logps/rejected': -317.6061096191406, 'referece_logps/chosen': -317.9693298339844, 'logits/rejected': -0.4208793640136719, 'logits/chosen': -0.44353416562080383, 'epoch': 0.05}

  1%|          | 83/10740 [25:20<57:51:21, 19.54s/it]


  1%|          | 85/10740 [25:45<47:13:31, 15.96s/it]
{'loss': 0.7017, 'learning_rate': 5.263157894736842e-07, 'rewards/chosen': 0.005942153744399548, 'rewards/rejected': -0.0003360749687999487, 'rewards/accuracies': 0.5, 'rewards/margins': 0.006278229411691427, 'policy_logps/rejected': -323.7955322265625, 'policy_logps/chosen': -400.75054931640625, 'referece_logps/rejected': -323.7921447753906, 'referece_logps/chosen': -400.80999755859375, 'logits/rejected': -0.9854311943054199, 'logits/chosen': -0.7263844013214111, 'epoch': 0.05}

  1%|          | 86/10740 [26:03<49:03:33, 16.58s/it]

  1%|          | 87/10740 [26:17<47:32:58, 16.07s/it]

  1%|          | 88/10740 [26:37<50:51:33, 17.19s/it]

  1%|          | 89/10740 [26:52<48:36:43, 16.43s/it]

  1%|          | 90/10740 [27:12<51:28:45, 17.40s/it]

  1%|          | 91/10740 [27:33<54:40:37, 18.48s/it]

  1%|          | 92/10740 [27:47<50:40:23, 17.13s/it]


  1%|          | 94/10740 [28:29<56:30:25, 19.11s/it]
[2024-04-01 19:41:47,113] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 95/10740 [28:49<57:23:03, 19.41s/it]
{'loss': 0.6979, 'learning_rate': 5.88235294117647e-07, 'rewards/chosen': 0.0008728969842195511, 'rewards/rejected': 0.0019309036433696747, 'rewards/accuracies': 0.625, 'rewards/margins': -0.001058007124811411, 'policy_logps/rejected': -329.20123291015625, 'policy_logps/chosen': -309.2297668457031, 'referece_logps/rejected': -329.2205505371094, 'referece_logps/chosen': -309.2385559082031, 'logits/rejected': -0.20709598064422607, 'logits/chosen': -0.12696880102157593, 'epoch': 0.05}

  1%|          | 96/10740 [29:11<60:03:36, 20.31s/it]

  1%|          | 97/10740 [29:28<56:34:15, 19.14s/it]

  1%|          | 98/10740 [29:40<50:39:28, 17.14s/it]

  1%|          | 99/10740 [29:52<45:56:39, 15.54s/it]

  1%|          | 100/10740 [30:03<42:14:31, 14.29s/it]

  1%|          | 101/10740 [30:24<47:42:00, 16.14s/it]

  1%|          | 102/10740 [30:43<49:59:02, 16.92s/it]

  1%|          | 103/10740 [31:04<53:33:44, 18.13s/it]


  1%|          | 105/10740 [31:43<56:33:39, 19.15s/it]
{'loss': 0.6876, 'learning_rate': 6.501547987616099e-07, 'rewards/chosen': -0.004857635125517845, 'rewards/rejected': -0.023494340479373932, 'rewards/accuracies': 0.625, 'rewards/margins': 0.018636703491210938, 'policy_logps/rejected': -366.2655334472656, 'policy_logps/chosen': -417.9665832519531, 'referece_logps/rejected': -366.0306396484375, 'referece_logps/chosen': -417.91802978515625, 'logits/rejected': -0.3112388253211975, 'logits/chosen': 0.024887505918741226, 'epoch': 0.06}

  1%|          | 106/10740 [32:01<54:57:22, 18.60s/it]

  1%|          | 107/10740 [32:20<55:55:24, 18.93s/it]

  1%|          | 108/10740 [32:37<53:35:25, 18.15s/it]

  1%|          | 109/10740 [32:56<54:53:41, 18.59s/it]

  1%|          | 110/10740 [33:21<60:10:02, 20.38s/it]

  1%|          | 111/10740 [33:38<57:44:02, 19.55s/it]

  1%|          | 112/10740 [33:53<53:32:49, 18.14s/it]

  1%|          | 113/10740 [34:15<56:33:40, 19.16s/it]

  1%|          | 114/10740 [34:35<57:10:18, 19.37s/it]

  1%|          | 115/10740 [34:51<54:22:40, 18.42s/it]

  1%|          | 116/10740 [35:12<57:02:25, 19.33s/it]

  1%|          | 117/10740 [35:31<56:14:31, 19.06s/it]

  1%|          | 118/10740 [35:48<54:56:36, 18.62s/it]

  1%|          | 119/10740 [36:03<51:32:59, 17.47s/it]

  1%|          | 120/10740 [36:19<49:50:10, 16.89s/it]

  1%|          | 121/10740 [36:40<53:18:20, 18.07s/it]

  1%|          | 122/10740 [36:56<52:16:31, 17.72s/it]

  1%|          | 123/10740 [37:18<55:41:39, 18.88s/it]

  1%|          | 124/10740 [37:37<56:05:53, 19.02s/it]

  1%|          | 125/10740 [37:53<53:26:06, 18.12s/it]

  1%|          | 126/10740 [38:08<50:36:38, 17.17s/it]

  1%|          | 127/10740 [38:29<53:29:22, 18.14s/it]
[2024-04-01 19:52:08,901] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 128/10740 [38:51<56:47:35, 19.27s/it]

  1%|          | 129/10740 [39:13<59:24:24, 20.16s/it]

  1%|          | 130/10740 [39:27<53:51:48, 18.28s/it]

  1%|          | 131/10740 [39:47<55:38:16, 18.88s/it]

  1%|          | 132/10740 [40:07<56:43:09, 19.25s/it]

  1%|          | 133/10740 [40:20<50:49:38, 17.25s/it]

  1%|          | 134/10740 [40:39<52:36:18, 17.86s/it]

  1%|▏         | 135/10740 [41:00<55:01:01, 18.68s/it]

  1%|▏         | 136/10740 [41:15<52:11:27, 17.72s/it]

  1%|▏         | 137/10740 [41:33<52:40:10, 17.88s/it]

  1%|▏         | 138/10740 [41:51<51:59:38, 17.66s/it]

  1%|▏         | 139/10740 [42:09<52:28:35, 17.82s/it]

  1%|▏         | 140/10740 [42:24<50:16:41, 17.08s/it]

  1%|▏         | 141/10740 [42:43<51:37:33, 17.54s/it]

  1%|▏         | 142/10740 [42:55<46:59:04, 15.96s/it]

  1%|▏         | 143/10740 [43:09<45:35:18, 15.49s/it]

  1%|▏         | 144/10740 [43:24<44:58:14, 15.28s/it]

  1%|▏         | 145/10740 [43:47<51:41:57, 17.57s/it]

  1%|▏         | 146/10740 [44:08<54:25:34, 18.49s/it]

  1%|▏         | 147/10740 [44:25<53:41:52, 18.25s/it]

  1%|▏         | 148/10740 [44:47<57:00:46, 19.38s/it]

  1%|▏         | 149/10740 [45:09<59:06:36, 20.09s/it]

  1%|▏         | 150/10740 [45:28<57:49:10, 19.66s/it]

  1%|▏         | 151/10740 [45:47<57:14:09, 19.46s/it]

  1%|▏         | 152/10740 [46:00<51:58:50, 17.67s/it]

  1%|▏         | 153/10740 [46:14<48:03:35, 16.34s/it]

  1%|▏         | 154/10740 [46:30<48:04:58, 16.35s/it]

  1%|▏         | 155/10740 [46:44<45:49:55, 15.59s/it]

  1%|▏         | 156/10740 [46:56<43:00:33, 14.63s/it]

  1%|▏         | 157/10740 [47:12<44:27:46, 15.12s/it]

  1%|▏         | 158/10740 [47:33<49:19:27, 16.78s/it]

  1%|▏         | 159/10740 [47:50<49:48:44, 16.95s/it]

  1%|▏         | 160/10740 [48:12<54:00:43, 18.38s/it]

  1%|▏         | 161/10740 [48:27<50:54:40, 17.32s/it]

  2%|▏         | 162/10740 [48:48<54:35:10, 18.58s/it]

  2%|▏         | 163/10740 [49:00<48:38:32, 16.56s/it]

  2%|▏         | 164/10740 [49:20<51:35:45, 17.56s/it]

  2%|▏         | 165/10740 [49:36<50:26:44, 17.17s/it]

  2%|▏         | 166/10740 [49:56<52:46:51, 17.97s/it]


  2%|▏         | 168/10740 [50:25<46:59:40, 16.00s/it]

  2%|▏         | 169/10740 [50:37<43:24:12, 14.78s/it]
{'loss': 0.6789, 'learning_rate': 1.0464396284829723e-06, 'rewards/chosen': -0.11188317090272903, 'rewards/rejected': -0.12139520794153214, 'rewards/accuracies': 0.5, 'rewards/margins': 0.009512043558061123, 'policy_logps/rejected': -295.07403564453125, 'policy_logps/chosen': -367.7789611816406, 'referece_logps/rejected': -293.8600769042969, 'referece_logps/chosen': -366.66015625, 'logits/rejected': -0.3733464777469635, 'logits/chosen': -0.4072708487510681, 'epoch': 0.09}

  2%|▏         | 170/10740 [50:49<41:19:02, 14.07s/it]

  2%|▏         | 171/10740 [51:11<48:12:28, 16.42s/it]

  2%|▏         | 172/10740 [51:30<50:09:44, 17.09s/it]

  2%|▏         | 173/10740 [51:43<46:20:42, 15.79s/it]


  2%|▏         | 175/10740 [52:19<49:29:13, 16.86s/it]

  2%|▏         | 176/10740 [52:37<50:26:20, 17.19s/it]
{'loss': 0.6813, 'learning_rate': 1.089783281733746e-06, 'rewards/chosen': -0.02073344960808754, 'rewards/rejected': -0.04791564866900444, 'rewards/accuracies': 0.625, 'rewards/margins': 0.0271821990609169, 'policy_logps/rejected': -295.7433776855469, 'policy_logps/chosen': -374.96307373046875, 'referece_logps/rejected': -295.26422119140625, 'referece_logps/chosen': -374.7557067871094, 'logits/rejected': -1.0026490688323975, 'logits/chosen': -0.9709987640380859, 'epoch': 0.1}

  2%|▏         | 177/10740 [52:52<48:11:22, 16.42s/it]

  2%|▏         | 178/10740 [53:13<52:13:55, 17.80s/it]

  2%|▏         | 179/10740 [53:32<53:49:46, 18.35s/it]

  2%|▏         | 180/10740 [53:54<56:27:46, 19.25s/it]

  2%|▏         | 181/10740 [54:14<57:04:27, 19.46s/it]

  2%|▏         | 182/10740 [54:32<55:52:20, 19.05s/it]

  2%|▏         | 183/10740 [54:51<56:04:13, 19.12s/it]


  2%|▏         | 185/10740 [55:35<60:57:00, 20.79s/it]
{'loss': 0.6814, 'learning_rate': 1.1455108359133125e-06, 'rewards/chosen': -0.03156471252441406, 'rewards/rejected': -0.09080009907484055, 'rewards/accuracies': 0.625, 'rewards/margins': 0.059235379099845886, 'policy_logps/rejected': -339.4980163574219, 'policy_logps/chosen': -369.7579040527344, 'referece_logps/rejected': -338.5899963378906, 'referece_logps/chosen': -369.4422607421875, 'logits/rejected': -0.6374849677085876, 'logits/chosen': -0.6097096800804138, 'epoch': 0.1}

  2%|▏         | 186/10740 [55:56<61:05:06, 20.84s/it]

  2%|▏         | 187/10740 [56:15<59:07:20, 20.17s/it]

  2%|▏         | 188/10740 [56:36<59:45:25, 20.39s/it]


  2%|▏         | 190/10740 [56:59<46:47:03, 15.96s/it]
{'loss': 0.6755, 'learning_rate': 1.176470588235294e-06, 'rewards/chosen': -0.0019260424887761474, 'rewards/rejected': -0.0558292418718338, 'rewards/accuracies': 0.75, 'rewards/margins': 0.05390319973230362, 'policy_logps/rejected': -457.5673522949219, 'policy_logps/chosen': -521.2233276367188, 'referece_logps/rejected': -457.0090637207031, 'referece_logps/chosen': -521.2040405273438, 'logits/rejected': 0.028557628393173218, 'logits/chosen': -0.16197173297405243, 'epoch': 0.11}

  2%|▏         | 191/10740 [57:10<42:11:17, 14.40s/it]

  2%|▏         | 192/10740 [57:21<38:44:15, 13.22s/it]

  2%|▏         | 193/10740 [57:33<38:06:33, 13.01s/it]

  2%|▏         | 194/10740 [57:45<37:12:37, 12.70s/it]


  2%|▏         | 196/10740 [58:18<42:03:39, 14.36s/it]
{'loss': 0.6645, 'learning_rate': 1.2136222910216717e-06, 'rewards/chosen': -0.06116484850645065, 'rewards/rejected': -0.2136957049369812, 'rewards/accuracies': 0.875, 'rewards/margins': 0.15253087878227234, 'policy_logps/rejected': -421.85968017578125, 'policy_logps/chosen': -468.67431640625, 'referece_logps/rejected': -419.7226867675781, 'referece_logps/chosen': -468.0627136230469, 'logits/rejected': -0.0879393219947815, 'logits/chosen': 0.22621247172355652, 'epoch': 0.11}

  2%|▏         | 197/10740 [58:34<44:02:10, 15.04s/it]

  2%|▏         | 198/10740 [58:53<47:02:47, 16.07s/it]

  2%|▏         | 199/10740 [59:04<43:05:41, 14.72s/it]

  2%|▏         | 200/10740 [59:17<41:17:28, 14.10s/it]


  2%|▏         | 202/10740 [59:54<46:53:12, 16.02s/it]
{'loss': 0.6661, 'learning_rate': 1.2507739938080495e-06, 'rewards/chosen': 0.017114829272031784, 'rewards/rejected': -0.09771880507469177, 'rewards/accuracies': 0.75, 'rewards/margins': 0.11483363807201385, 'policy_logps/rejected': -346.7468566894531, 'policy_logps/chosen': -342.40997314453125, 'referece_logps/rejected': -345.7696533203125, 'referece_logps/chosen': -342.58111572265625, 'logits/rejected': -0.5744153261184692, 'logits/chosen': -0.4718005061149597, 'epoch': 0.11}

  2%|▏         | 203/10740 [1:00:06<43:34:47, 14.89s/it]

  2%|▏         | 204/10740 [1:00:18<41:09:20, 14.06s/it]

  2%|▏         | 205/10740 [1:00:32<41:29:04, 14.18s/it]


  2%|▏         | 207/10740 [1:01:06<43:45:00, 14.95s/it]

  2%|▏         | 208/10740 [1:01:21<44:27:43, 15.20s/it]

  2%|▏         | 209/10740 [1:01:34<42:10:01, 14.41s/it]

  2%|▏         | 210/10740 [1:01:53<46:12:59, 15.80s/it]

  2%|▏         | 211/10740 [1:02:12<48:31:53, 16.59s/it]

  2%|▏         | 212/10740 [1:02:31<50:56:02, 17.42s/it]

  2%|▏         | 213/10740 [1:02:46<49:13:21, 16.83s/it]

  2%|▏         | 214/10740 [1:03:05<50:49:44, 17.38s/it]

  2%|▏         | 215/10740 [1:03:25<53:06:50, 18.17s/it]

  2%|▏         | 216/10740 [1:03:36<46:40:24, 15.97s/it]

  2%|▏         | 217/10740 [1:03:49<44:29:23, 15.22s/it]

  2%|▏         | 218/10740 [1:04:03<43:13:33, 14.79s/it]

  2%|▏         | 219/10740 [1:04:24<48:50:16, 16.71s/it]

  2%|▏         | 220/10740 [1:04:43<51:00:19, 17.45s/it]

  2%|▏         | 221/10740 [1:05:03<52:34:58, 18.00s/it]

  2%|▏         | 222/10740 [1:05:21<52:47:43, 18.07s/it]

  2%|▏         | 223/10740 [1:05:41<54:20:20, 18.60s/it]

  2%|▏         | 224/10740 [1:06:03<57:37:17, 19.73s/it]

  2%|▏         | 225/10740 [1:06:24<58:52:29, 20.16s/it]

  2%|▏         | 226/10740 [1:06:37<52:18:43, 17.91s/it]

  2%|▏         | 227/10740 [1:06:57<54:19:11, 18.60s/it]

  2%|▏         | 228/10740 [1:07:09<48:28:48, 16.60s/it]

  2%|▏         | 229/10740 [1:07:31<53:10:18, 18.21s/it]

  2%|▏         | 230/10740 [1:07:48<52:07:40, 17.86s/it]

  2%|▏         | 231/10740 [1:08:02<49:02:48, 16.80s/it]

  2%|▏         | 232/10740 [1:08:18<48:00:20, 16.45s/it]

  2%|▏         | 233/10740 [1:08:41<54:04:44, 18.53s/it]

  2%|▏         | 234/10740 [1:08:56<50:18:30, 17.24s/it]

  2%|▏         | 235/10740 [1:09:16<52:51:17, 18.11s/it]
[2024-04-01 20:22:34,105] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6457, 'learning_rate': 1.4551083591331269e-06, 'rewards/chosen': -0.14581060409545898, 'rewards/rejected': -0.22167378664016724, 'rewards/accuracies': 0.75, 'rewards/margins': 0.07586317509412766, 'policy_logps/rejected': -347.2326965332031, 'policy_logps/chosen': -316.0006408691406, 'referece_logps/rejected': -345.0159606933594, 'referece_logps/chosen': -314.54254150390625, 'logits/rejected': 0.12333088368177414, 'logits/chosen': -0.024160731583833694, 'epoch': 0.13}
[2024-04-01 20:22:50,748] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  2%|▏         | 237/10740 [1:09:53<53:58:37, 18.50s/it]

  2%|▏         | 238/10740 [1:10:07<49:59:11, 17.13s/it]

  2%|▏         | 239/10740 [1:10:30<55:03:51, 18.88s/it]

  2%|▏         | 240/10740 [1:10:46<52:39:23, 18.05s/it]

  2%|▏         | 241/10740 [1:11:01<49:44:44, 17.06s/it]
{'loss': 0.6476, 'learning_rate': 1.4922600619195044e-06, 'rewards/chosen': -0.11035891622304916, 'rewards/rejected': -0.19698572158813477, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0866268202662468, 'policy_logps/rejected': -268.5804443359375, 'policy_logps/chosen': -391.0563659667969, 'referece_logps/rejected': -266.610595703125, 'referece_logps/chosen': -389.95281982421875, 'logits/rejected': -0.8847476243972778, 'logits/chosen': -0.6144226789474487, 'epoch': 0.13}


  2%|▏         | 243/10740 [1:11:39<52:01:45, 17.84s/it]

  2%|▏         | 244/10740 [1:12:00<54:53:36, 18.83s/it]

  2%|▏         | 245/10740 [1:12:18<54:13:09, 18.60s/it]

  2%|▏         | 246/10740 [1:12:35<52:43:06, 18.09s/it]

  2%|▏         | 247/10740 [1:12:53<53:01:19, 18.19s/it]

  2%|▏         | 248/10740 [1:13:15<55:59:14, 19.21s/it]

  2%|▏         | 249/10740 [1:13:35<56:52:55, 19.52s/it]

  2%|▏         | 250/10740 [1:13:58<59:22:52, 20.38s/it]

  2%|▏         | 251/10740 [1:14:15<57:07:18, 19.61s/it]

  2%|▏         | 252/10740 [1:14:34<55:54:29, 19.19s/it]

  2%|▏         | 253/10740 [1:14:51<54:23:47, 18.67s/it]

  2%|▏         | 254/10740 [1:15:10<54:58:59, 18.88s/it]

  2%|▏         | 255/10740 [1:15:30<55:29:35, 19.05s/it]

  2%|▏         | 256/10740 [1:15:45<52:06:04, 17.89s/it]

  2%|▏         | 257/10740 [1:16:04<53:15:31, 18.29s/it]

  2%|▏         | 258/10740 [1:16:25<55:45:01, 19.15s/it]
{'loss': 0.6177, 'learning_rate': 1.5975232198142413e-06, 'rewards/chosen': -0.14372539520263672, 'rewards/rejected': -0.44148486852645874, 'rewards/accuracies': 1.0, 'rewards/margins': 0.297759473323822, 'policy_logps/rejected': -333.3719177246094, 'policy_logps/chosen': -314.5592041015625, 'referece_logps/rejected': -328.9570617675781, 'referece_logps/chosen': -313.1219177246094, 'logits/rejected': -0.6169095635414124, 'logits/chosen': -0.6099010109901428, 'epoch': 0.14}

  2%|▏         | 259/10740 [1:16:43<54:15:15, 18.64s/it]


  2%|▏         | 261/10740 [1:17:22<56:09:37, 19.29s/it]
[2024-04-01 20:30:40,456] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 262/10740 [1:17:40<55:01:31, 18.91s/it]

  2%|▏         | 263/10740 [1:18:02<57:55:10, 19.90s/it]

  2%|▏         | 264/10740 [1:18:16<52:42:16, 18.11s/it]

  2%|▏         | 265/10740 [1:18:38<56:06:08, 19.28s/it]

  2%|▏         | 266/10740 [1:18:56<54:58:25, 18.89s/it]

  2%|▏         | 267/10740 [1:19:19<58:04:21, 19.96s/it]

  2%|▏         | 268/10740 [1:19:42<61:03:19, 20.99s/it]
{'loss': 0.6208, 'learning_rate': 1.6594427244582043e-06, 'rewards/chosen': -0.38707417249679565, 'rewards/rejected': -0.36663612723350525, 'rewards/accuracies': 0.25, 'rewards/margins': -0.020438047125935555, 'policy_logps/rejected': -241.52340698242188, 'policy_logps/chosen': -362.123291015625, 'referece_logps/rejected': -237.8570556640625, 'referece_logps/chosen': -358.2525634765625, 'logits/rejected': 0.12638717889785767, 'logits/chosen': 0.1932671219110489, 'epoch': 0.15}


  3%|▎         | 270/10740 [1:20:19<57:02:16, 19.61s/it]

  3%|▎         | 271/10740 [1:20:41<59:22:17, 20.42s/it]
[2024-04-01 20:33:59,547] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 272/10740 [1:20:58<56:24:19, 19.40s/it]

  3%|▎         | 273/10740 [1:21:17<55:22:23, 19.04s/it]

  3%|▎         | 274/10740 [1:21:36<55:19:03, 19.03s/it]

  3%|▎         | 275/10740 [1:21:52<53:19:02, 18.34s/it]

  3%|▎         | 276/10740 [1:22:12<54:09:12, 18.63s/it]

  3%|▎         | 277/10740 [1:22:25<49:19:37, 16.97s/it]

  3%|▎         | 278/10740 [1:22:43<50:36:56, 17.42s/it]

  3%|▎         | 279/10740 [1:22:58<48:10:50, 16.58s/it]

  3%|▎         | 280/10740 [1:23:14<47:46:04, 16.44s/it]

  3%|▎         | 281/10740 [1:23:25<43:10:50, 14.86s/it]

  3%|▎         | 282/10740 [1:23:41<44:12:28, 15.22s/it]

  3%|▎         | 283/10740 [1:23:58<45:57:12, 15.82s/it]

  3%|▎         | 284/10740 [1:24:18<49:42:28, 17.11s/it]

  3%|▎         | 285/10740 [1:24:38<51:36:17, 17.77s/it]

  3%|▎         | 286/10740 [1:24:56<51:40:47, 17.80s/it]

  3%|▎         | 287/10740 [1:25:14<52:33:59, 18.10s/it]

  3%|▎         | 288/10740 [1:25:30<50:42:55, 17.47s/it]

  3%|▎         | 289/10740 [1:25:50<52:17:46, 18.01s/it]

  3%|▎         | 290/10740 [1:26:10<54:06:39, 18.64s/it]

  3%|▎         | 291/10740 [1:26:33<57:38:05, 19.86s/it]

  3%|▎         | 292/10740 [1:26:54<59:19:02, 20.44s/it]
{'loss': 0.5763, 'learning_rate': 1.808049535603715e-06, 'rewards/chosen': -0.4418676197528839, 'rewards/rejected': -0.7502899169921875, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3084222674369812, 'policy_logps/rejected': -337.66485595703125, 'policy_logps/chosen': -337.41094970703125, 'referece_logps/rejected': -330.1619567871094, 'referece_logps/chosen': -332.99224853515625, 'logits/rejected': -0.15089493989944458, 'logits/chosen': -0.04806150496006012, 'epoch': 0.16}


  3%|▎         | 294/10740 [1:27:29<55:27:00, 19.11s/it]

  3%|▎         | 295/10740 [1:27:42<50:26:36, 17.39s/it]

  3%|▎         | 296/10740 [1:27:55<47:02:41, 16.22s/it]

  3%|▎         | 297/10740 [1:28:15<50:14:04, 17.32s/it]

  3%|▎         | 298/10740 [1:28:28<46:27:36, 16.02s/it]

  3%|▎         | 299/10740 [1:28:45<46:33:19, 16.05s/it]

  3%|▎         | 300/10740 [1:29:02<47:47:15, 16.48s/it]

  3%|▎         | 301/10740 [1:29:21<49:53:15, 17.20s/it]

  3%|▎         | 302/10740 [1:29:42<53:42:44, 18.53s/it]

  3%|▎         | 303/10740 [1:30:04<55:59:22, 19.31s/it]

  3%|▎         | 304/10740 [1:30:15<48:53:20, 16.86s/it]

  3%|▎         | 305/10740 [1:30:32<48:52:23, 16.86s/it]

  3%|▎         | 306/10740 [1:30:53<52:32:42, 18.13s/it]

  3%|▎         | 307/10740 [1:31:10<51:33:59, 17.79s/it]

  3%|▎         | 308/10740 [1:31:31<54:41:50, 18.88s/it]

  3%|▎         | 309/10740 [1:31:50<54:25:27, 18.78s/it]

  3%|▎         | 310/10740 [1:32:04<50:09:10, 17.31s/it]

  3%|▎         | 311/10740 [1:32:21<49:53:42, 17.22s/it]

  3%|▎         | 312/10740 [1:32:41<52:56:36, 18.28s/it]

  3%|▎         | 313/10740 [1:32:56<50:11:31, 17.33s/it]

  3%|▎         | 314/10740 [1:33:15<51:12:26, 17.68s/it]

  3%|▎         | 315/10740 [1:33:31<50:09:42, 17.32s/it]

  3%|▎         | 316/10740 [1:33:54<54:17:43, 18.75s/it]

  3%|▎         | 317/10740 [1:34:09<51:26:29, 17.77s/it]
{'loss': 0.5732, 'learning_rate': 1.9628482972136224e-06, 'rewards/chosen': -0.3960634171962738, 'rewards/rejected': -0.5967951416969299, 'rewards/accuracies': 0.75, 'rewards/margins': 0.20073170959949493, 'policy_logps/rejected': -316.501708984375, 'policy_logps/chosen': -410.244384765625, 'referece_logps/rejected': -310.5337829589844, 'referece_logps/chosen': -406.28375244140625, 'logits/rejected': -0.8144318461418152, 'logits/chosen': -0.666916012763977, 'epoch': 0.18}


  3%|▎         | 319/10740 [1:34:33<42:50:21, 14.80s/it]

  3%|▎         | 320/10740 [1:34:48<43:09:33, 14.91s/it]

  3%|▎         | 321/10740 [1:35:10<49:16:50, 17.03s/it]

  3%|▎         | 322/10740 [1:35:25<47:34:01, 16.44s/it]
{'loss': 0.5419, 'learning_rate': 1.9938080495356037e-06, 'rewards/chosen': -0.7858858108520508, 'rewards/rejected': -1.3141515254974365, 'rewards/accuracies': 1.0, 'rewards/margins': 0.5282657742500305, 'policy_logps/rejected': -412.5115966796875, 'policy_logps/chosen': -579.6427001953125, 'referece_logps/rejected': -399.3701477050781, 'referece_logps/chosen': -571.7838134765625, 'logits/rejected': -0.7109700441360474, 'logits/chosen': -0.5885325074195862, 'epoch': 0.18}


  3%|▎         | 324/10740 [1:36:04<51:27:29, 17.79s/it]

  3%|▎         | 325/10740 [1:36:20<49:54:58, 17.25s/it]

  3%|▎         | 326/10740 [1:36:38<50:56:51, 17.61s/it]

  3%|▎         | 327/10740 [1:36:58<52:51:08, 18.27s/it]

  3%|▎         | 328/10740 [1:37:18<54:07:17, 18.71s/it]

  3%|▎         | 329/10740 [1:37:42<59:05:56, 20.44s/it]

  3%|▎         | 330/10740 [1:37:56<53:15:59, 18.42s/it]

  3%|▎         | 331/10740 [1:38:15<54:15:06, 18.76s/it]

  3%|▎         | 332/10740 [1:38:39<58:03:56, 20.08s/it]
{'loss': 0.5407, 'learning_rate': 1.99999631642791e-06, 'rewards/chosen': -0.5261392593383789, 'rewards/rejected': -0.5262423157691956, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00010303407907485962, 'policy_logps/rejected': -320.46044921875, 'policy_logps/chosen': -347.3809814453125, 'referece_logps/rejected': -315.197998046875, 'referece_logps/chosen': -342.1195983886719, 'logits/rejected': -1.4478679895401, 'logits/chosen': -1.4906178712844849, 'epoch': 0.19}


  3%|▎         | 334/10740 [1:39:03<46:49:21, 16.20s/it]

  3%|▎         | 335/10740 [1:39:15<43:03:03, 14.90s/it]

  3%|▎         | 336/10740 [1:39:34<46:25:42, 16.07s/it]

  3%|▎         | 337/10740 [1:39:49<45:41:18, 15.81s/it]

  3%|▎         | 338/10740 [1:40:07<47:37:12, 16.48s/it]

  3%|▎         | 339/10740 [1:40:26<49:38:35, 17.18s/it]

  3%|▎         | 340/10740 [1:40:46<52:02:23, 18.01s/it]
{'loss': 0.5292, 'learning_rate': 1.9999868573993133e-06, 'rewards/chosen': -0.28524160385131836, 'rewards/rejected': -1.0348320007324219, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7495905160903931, 'policy_logps/rejected': -270.7663879394531, 'policy_logps/chosen': -375.6348876953125, 'referece_logps/rejected': -260.41802978515625, 'referece_logps/chosen': -372.78240966796875, 'logits/rejected': -0.8390527367591858, 'logits/chosen': -0.9661408066749573, 'epoch': 0.19}


  3%|▎         | 342/10740 [1:41:28<56:48:05, 19.67s/it]

  3%|▎         | 343/10740 [1:41:46<55:48:59, 19.33s/it]
{'loss': 0.5093, 'learning_rate': 1.999981809564527e-06, 'rewards/chosen': -0.7888624668121338, 'rewards/rejected': -1.599604845046997, 'rewards/accuracies': 1.0, 'rewards/margins': 0.8107424378395081, 'policy_logps/rejected': -336.21044921875, 'policy_logps/chosen': -277.34185791015625, 'referece_logps/rejected': -320.2143859863281, 'referece_logps/chosen': -269.4532470703125, 'logits/rejected': -0.3314192593097687, 'logits/chosen': -0.27929526567459106, 'epoch': 0.19}

  3%|▎         | 344/10740 [1:42:07<56:35:15, 19.60s/it]


  3%|▎         | 346/10740 [1:42:39<53:01:48, 18.37s/it]

  3%|▎         | 347/10740 [1:42:56<51:29:34, 17.84s/it]

  3%|▎         | 348/10740 [1:43:16<52:59:36, 18.36s/it]

  3%|▎         | 349/10740 [1:43:38<56:06:31, 19.44s/it]

  3%|▎         | 350/10740 [1:43:58<57:07:55, 19.80s/it]
[2024-04-01 20:57:16,474] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5027, 'learning_rate': 1.9999668480140187e-06, 'rewards/chosen': -0.8906628489494324, 'rewards/rejected': -1.7154979705810547, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8248351812362671, 'policy_logps/rejected': -395.0189514160156, 'policy_logps/chosen': -325.6400451660156, 'referece_logps/rejected': -377.86395263671875, 'referece_logps/chosen': -316.7333984375, 'logits/rejected': -0.48974552750587463, 'logits/chosen': -0.5210874676704407, 'epoch': 0.2}


  3%|▎         | 352/10740 [1:44:35<56:25:29, 19.55s/it]
[2024-04-01 20:57:53,708] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 353/10740 [1:44:54<55:53:16, 19.37s/it]
{'loss': 0.5707, 'learning_rate': 1.9999590716752913e-06, 'rewards/chosen': -0.5171487927436829, 'rewards/rejected': -1.062172293663025, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5450235605239868, 'policy_logps/rejected': -238.0174560546875, 'policy_logps/chosen': -259.4884948730469, 'referece_logps/rejected': -227.39572143554688, 'referece_logps/chosen': -254.3170166015625, 'logits/rejected': -1.2689425945281982, 'logits/chosen': -1.0552153587341309, 'epoch': 0.2}

  3%|▎         | 354/10740 [1:45:09<51:33:38, 17.87s/it]


  3%|▎         | 356/10740 [1:45:53<57:30:47, 19.94s/it]

  3%|▎         | 357/10740 [1:46:05<50:33:44, 17.53s/it]

  3%|▎         | 358/10740 [1:46:24<52:25:00, 18.18s/it]
{'loss': 0.5663, 'learning_rate': 1.9999442921397052e-06, 'rewards/chosen': -0.7451631426811218, 'rewards/rejected': -1.0782324075698853, 'rewards/accuracies': 0.75, 'rewards/margins': 0.33306920528411865, 'policy_logps/rejected': -254.28594970703125, 'policy_logps/chosen': -211.53750610351562, 'referece_logps/rejected': -243.50363159179688, 'referece_logps/chosen': -204.0858612060547, 'logits/rejected': -0.7309420704841614, 'logits/chosen': -0.727996826171875, 'epoch': 0.2}


  3%|▎         | 360/10740 [1:47:13<61:04:53, 21.18s/it]

  3%|▎         | 361/10740 [1:47:34<61:20:05, 21.27s/it]

  3%|▎         | 362/10740 [1:47:54<60:18:12, 20.92s/it]

  3%|▎         | 363/10740 [1:48:11<56:37:11, 19.64s/it]

  3%|▎         | 364/10740 [1:48:34<59:32:57, 20.66s/it]
[2024-04-01 21:01:52,135] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5146, 'learning_rate': 1.9999235554371424e-06, 'rewards/chosen': -1.0772557258605957, 'rewards/rejected': -1.7944047451019287, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7171487808227539, 'policy_logps/rejected': -437.4676208496094, 'policy_logps/chosen': -368.0006103515625, 'referece_logps/rejected': -419.5235900878906, 'referece_logps/chosen': -357.2280578613281, 'logits/rejected': -0.49449053406715393, 'logits/chosen': -0.4481458365917206, 'epoch': 0.2}


  3%|▎         | 366/10740 [1:49:10<55:36:52, 19.30s/it]

  3%|▎         | 367/10740 [1:49:30<56:33:55, 19.63s/it]
[2024-04-01 21:02:48,348] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4545, 'learning_rate': 1.99991195931728e-06, 'rewards/chosen': -0.764691948890686, 'rewards/rejected': -1.6413922309875488, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8767004013061523, 'policy_logps/rejected': -436.7083740234375, 'policy_logps/chosen': -402.4804992675781, 'referece_logps/rejected': -420.29443359375, 'referece_logps/chosen': -394.8336181640625, 'logits/rejected': -1.4063916206359863, 'logits/chosen': -1.1017462015151978, 'epoch': 0.21}


  3%|▎         | 369/10740 [1:50:10<56:11:38, 19.51s/it]
{'loss': 0.5437, 'learning_rate': 1.9999037738478945e-06, 'rewards/chosen': -0.7350939512252808, 'rewards/rejected': -1.219759464263916, 'rewards/accuracies': 0.875, 'rewards/margins': 0.48466548323631287, 'policy_logps/rejected': -401.15264892578125, 'policy_logps/chosen': -486.1051940917969, 'referece_logps/rejected': -388.9550476074219, 'referece_logps/chosen': -478.7543029785156, 'logits/rejected': -0.6997237205505371, 'logits/chosen': -0.6882284283638, 'epoch': 0.21}


  3%|▎         | 371/10740 [1:50:48<54:43:44, 19.00s/it]

  3%|▎         | 372/10740 [1:51:10<57:00:31, 19.79s/it]

  3%|▎         | 373/10740 [1:51:30<57:18:47, 19.90s/it]

  3%|▎         | 374/10740 [1:51:44<52:22:39, 18.19s/it]
{'loss': 0.5494, 'learning_rate': 1.999881718666549e-06, 'rewards/chosen': -0.7000934481620789, 'rewards/rejected': -0.8650199174880981, 'rewards/accuracies': 0.625, 'rewards/margins': 0.16492649912834167, 'policy_logps/rejected': -320.2974853515625, 'policy_logps/chosen': -342.540771484375, 'referece_logps/rejected': -311.64727783203125, 'referece_logps/chosen': -335.5398254394531, 'logits/rejected': -0.5067017078399658, 'logits/chosen': -0.353685200214386, 'epoch': 0.21}


  4%|▎         | 376/10740 [1:52:14<46:59:58, 16.33s/it]

  4%|▎         | 377/10740 [1:52:34<50:02:33, 17.38s/it]

  4%|▎         | 378/10740 [1:52:56<54:10:08, 18.82s/it]
[2024-04-01 21:06:14,588] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▎         | 379/10740 [1:53:20<58:25:05, 20.30s/it]
{'loss': 0.6439, 'learning_rate': 1.9998573899432568e-06, 'rewards/chosen': -1.1222060918807983, 'rewards/rejected': -1.4053031206130981, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2830970287322998, 'policy_logps/rejected': -315.7803955078125, 'policy_logps/chosen': -357.2791442871094, 'referece_logps/rejected': -301.7273254394531, 'referece_logps/chosen': -346.0570983886719, 'logits/rejected': -0.038032904267311096, 'logits/chosen': 0.07735750824213028, 'epoch': 0.21}


  4%|▎         | 381/10740 [1:53:58<55:02:40, 19.13s/it]

  4%|▎         | 382/10740 [1:54:12<50:32:32, 17.57s/it]
{'loss': 0.5413, 'learning_rate': 1.999841701431944e-06, 'rewards/chosen': -0.6371515989303589, 'rewards/rejected': -1.588001012802124, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9508494138717651, 'policy_logps/rejected': -297.93597412109375, 'policy_logps/chosen': -320.7215576171875, 'referece_logps/rejected': -282.05596923828125, 'referece_logps/chosen': -314.35003662109375, 'logits/rejected': -1.5670922994613647, 'logits/chosen': -1.5402226448059082, 'epoch': 0.21}
[2024-04-01 21:07:51,957] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▎         | 384/10740 [1:54:54<55:26:08, 19.27s/it]
{'loss': 0.4672, 'learning_rate': 1.999830787733335e-06, 'rewards/chosen': -0.8353198170661926, 'rewards/rejected': -1.6218801736831665, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7865603566169739, 'policy_logps/rejected': -266.9637145996094, 'policy_logps/chosen': -407.5654296875, 'referece_logps/rejected': -250.74490356445312, 'referece_logps/chosen': -399.21221923828125, 'logits/rejected': -1.0464107990264893, 'logits/chosen': -0.9994656443595886, 'epoch': 0.21}


  4%|▎         | 386/10740 [1:55:25<49:26:33, 17.19s/it]

  4%|▎         | 387/10740 [1:55:39<46:05:30, 16.03s/it]

  4%|▎         | 388/10740 [1:55:57<47:59:39, 16.69s/it]
{'loss': 0.5303, 'learning_rate': 1.9998078690955004e-06, 'rewards/chosen': -0.7748139500617981, 'rewards/rejected': -1.204107403755188, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4292934238910675, 'policy_logps/rejected': -251.18582153320312, 'policy_logps/chosen': -353.1114501953125, 'referece_logps/rejected': -239.14474487304688, 'referece_logps/chosen': -345.36328125, 'logits/rejected': -1.397836446762085, 'logits/chosen': -1.4010831117630005, 'epoch': 0.22}

  4%|▎         | 389/10740 [1:56:18<51:43:23, 17.99s/it]


  4%|▎         | 391/10740 [1:56:53<52:14:45, 18.17s/it]
{'loss': 0.5536, 'learning_rate': 1.999789725298064e-06, 'rewards/chosen': -0.702695906162262, 'rewards/rejected': -1.3290469646453857, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6263510584831238, 'policy_logps/rejected': -433.6058654785156, 'policy_logps/chosen': -320.40875244140625, 'referece_logps/rejected': -420.3153991699219, 'referece_logps/chosen': -313.38177490234375, 'logits/rejected': -0.5618203282356262, 'logits/chosen': -0.7411494255065918, 'epoch': 0.22}


  4%|▎         | 393/10740 [1:57:33<55:45:54, 19.40s/it]

  4%|▎         | 394/10740 [1:57:49<52:28:45, 18.26s/it]
{'loss': 0.5316, 'learning_rate': 1.9997707631007303e-06, 'rewards/chosen': -0.546287477016449, 'rewards/rejected': -1.2009611129760742, 'rewards/accuracies': 0.875, 'rewards/margins': 0.65467369556427, 'policy_logps/rejected': -228.31826782226562, 'policy_logps/chosen': -328.8746032714844, 'referece_logps/rejected': -216.3086395263672, 'referece_logps/chosen': -323.4117431640625, 'logits/rejected': -0.380372554063797, 'logits/chosen': -0.3204130530357361, 'epoch': 0.22}

  4%|▎         | 395/10740 [1:58:08<53:10:10, 18.50s/it]


  4%|▎         | 397/10740 [1:58:47<55:14:25, 19.23s/it]

  4%|▎         | 398/10740 [1:59:01<50:10:14, 17.46s/it]

  4%|▎         | 399/10740 [1:59:19<50:40:49, 17.64s/it]
{'loss': 0.4813, 'learning_rate': 1.9997373408145306e-06, 'rewards/chosen': -0.6158332824707031, 'rewards/rejected': -1.3498482704162598, 'rewards/accuracies': 1.0, 'rewards/margins': 0.7340149879455566, 'policy_logps/rejected': -223.3697052001953, 'policy_logps/chosen': -331.9483337402344, 'referece_logps/rejected': -209.8712158203125, 'referece_logps/chosen': -325.78997802734375, 'logits/rejected': -0.31468382477760315, 'logits/chosen': -0.12962664663791656, 'epoch': 0.22}


  4%|▎         | 401/10740 [2:00:03<56:40:13, 19.73s/it]

  4%|▎         | 402/10740 [2:00:21<55:48:06, 19.43s/it]

  4%|▍         | 403/10740 [2:00:33<49:20:47, 17.19s/it]

  4%|▍         | 404/10740 [2:00:55<53:31:46, 18.64s/it]

  4%|▍         | 405/10740 [2:01:17<56:17:23, 19.61s/it]

  4%|▍         | 406/10740 [2:01:30<50:06:33, 17.46s/it]

  4%|▍         | 407/10740 [2:01:51<53:16:07, 18.56s/it]
{'loss': 0.6179, 'learning_rate': 1.9996791369057495e-06, 'rewards/chosen': -0.7643584609031677, 'rewards/rejected': -1.272389531135559, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5080310702323914, 'policy_logps/rejected': -254.02462768554688, 'policy_logps/chosen': -333.23162841796875, 'referece_logps/rejected': -241.3007354736328, 'referece_logps/chosen': -325.5880432128906, 'logits/rejected': -0.8139876127243042, 'logits/chosen': -0.649484395980835, 'epoch': 0.23}


  4%|▍         | 409/10740 [2:02:23<49:41:18, 17.31s/it]
{'loss': 0.5031, 'learning_rate': 1.999663676682318e-06, 'rewards/chosen': -0.8259647488594055, 'rewards/rejected': -1.6110048294067383, 'rewards/accuracies': 0.875, 'rewards/margins': 0.785040020942688, 'policy_logps/rejected': -251.0391387939453, 'policy_logps/chosen': -280.18798828125, 'referece_logps/rejected': -234.92909240722656, 'referece_logps/chosen': -271.9283142089844, 'logits/rejected': -0.4330993890762329, 'logits/chosen': -0.6160962581634521, 'epoch': 0.23}
[2024-04-01 21:16:02,321] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 410/10740 [2:02:44<53:06:56, 18.51s/it]


  4%|▍         | 412/10740 [2:03:26<56:28:45, 19.69s/it]
[2024-04-01 21:16:43,768] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 413/10740 [2:03:47<58:10:59, 20.28s/it]
[2024-04-01 21:17:05,441] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5242, 'learning_rate': 1.9996316651788814e-06, 'rewards/chosen': -1.0513441562652588, 'rewards/rejected': -1.4914931058883667, 'rewards/accuracies': 0.625, 'rewards/margins': 0.44014886021614075, 'policy_logps/rejected': -510.0057373046875, 'policy_logps/chosen': -395.9568786621094, 'referece_logps/rejected': -495.0908203125, 'referece_logps/chosen': -385.4433898925781, 'logits/rejected': 0.06658639758825302, 'logits/chosen': 0.020466826856136322, 'epoch': 0.23}


  4%|▍         | 415/10740 [2:04:31<60:51:22, 21.22s/it]
[2024-04-01 21:17:49,438] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5494, 'learning_rate': 1.999615113910522e-06, 'rewards/chosen': -1.2024415731430054, 'rewards/rejected': -1.5462602376937866, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3438187837600708, 'policy_logps/rejected': -294.89654541015625, 'policy_logps/chosen': -364.0973815917969, 'referece_logps/rejected': -279.4339599609375, 'referece_logps/chosen': -352.07293701171875, 'logits/rejected': -0.6250386238098145, 'logits/chosen': -0.6438845992088318, 'epoch': 0.23}
[2024-04-01 21:18:10,797] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 416/10740 [2:04:53<60:58:16, 21.26s/it]
[2024-04-01 21:18:26,868] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 418/10740 [2:05:30<57:52:09, 20.18s/it]

  4%|▍         | 419/10740 [2:05:50<57:41:11, 20.12s/it]
{'loss': 0.5528, 'learning_rate': 1.9995809203706283e-06, 'rewards/chosen': -1.0777713060379028, 'rewards/rejected': -1.5968633890151978, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5190919637680054, 'policy_logps/rejected': -486.0544738769531, 'policy_logps/chosen': -391.8671875, 'referece_logps/rejected': -470.0857849121094, 'referece_logps/chosen': -381.0895080566406, 'logits/rejected': -0.6795191168785095, 'logits/chosen': -0.36772432923316956, 'epoch': 0.23}

  4%|▍         | 420/10740 [2:06:05<53:05:28, 18.52s/it]
[2024-04-01 21:19:43,000] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 422/10740 [2:06:41<52:10:54, 18.21s/it]

  4%|▍         | 423/10740 [2:06:57<50:06:56, 17.49s/it]

  4%|▍         | 424/10740 [2:07:17<52:13:08, 18.22s/it]

  4%|▍         | 425/10740 [2:07:34<50:57:38, 17.79s/it]

  4%|▍         | 426/10740 [2:07:54<53:02:25, 18.51s/it]
[2024-04-01 21:21:12,115] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 427/10740 [2:08:12<52:34:54, 18.35s/it]

  4%|▍         | 428/10740 [2:08:33<55:00:56, 19.21s/it]
{'loss': 0.4866, 'learning_rate': 1.9994986664970437e-06, 'rewards/chosen': -1.1879371404647827, 'rewards/rejected': -2.019683599472046, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8317463994026184, 'policy_logps/rejected': -416.82012939453125, 'policy_logps/chosen': -582.5394287109375, 'referece_logps/rejected': -396.623291015625, 'referece_logps/chosen': -570.6600341796875, 'logits/rejected': 0.45718222856521606, 'logits/chosen': 0.43724995851516724, 'epoch': 0.24}


  4%|▍         | 430/10740 [2:09:13<55:45:08, 19.47s/it]
{'loss': 0.6038, 'learning_rate': 1.9994793878612753e-06, 'rewards/chosen': -1.3232097625732422, 'rewards/rejected': -1.1515209674835205, 'rewards/accuracies': 0.5, 'rewards/margins': -0.1716887503862381, 'policy_logps/rejected': -258.6363220214844, 'policy_logps/chosen': -323.31109619140625, 'referece_logps/rejected': -247.12110900878906, 'referece_logps/chosen': -310.0789794921875, 'logits/rejected': -0.5268478393554688, 'logits/chosen': -0.445222407579422, 'epoch': 0.24}
[2024-04-01 21:22:52,481] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 432/10740 [2:09:55<57:58:14, 20.25s/it]
[2024-04-01 21:23:13,403] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5429, 'learning_rate': 1.9994597456051094e-06, 'rewards/chosen': -0.6431339383125305, 'rewards/rejected': -1.2462397813796997, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6031057834625244, 'policy_logps/rejected': -240.07791137695312, 'policy_logps/chosen': -429.2477722167969, 'referece_logps/rejected': -227.61549377441406, 'referece_logps/chosen': -422.81646728515625, 'logits/rejected': 0.8737839460372925, 'logits/chosen': 0.8425601720809937, 'epoch': 0.24}

  4%|▍         | 433/10740 [2:10:16<58:52:49, 20.57s/it]
[2024-04-01 21:23:55,348] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 435/10740 [2:10:56<57:15:29, 20.00s/it]

  4%|▍         | 436/10740 [2:11:12<53:42:02, 18.76s/it]
{'loss': 0.5188, 'learning_rate': 1.999419370260301e-06, 'rewards/chosen': -1.368509292602539, 'rewards/rejected': -1.8430830240249634, 'rewards/accuracies': 0.75, 'rewards/margins': 0.47457385063171387, 'policy_logps/rejected': -486.1784973144531, 'policy_logps/chosen': -394.8667907714844, 'referece_logps/rejected': -467.7477111816406, 'referece_logps/chosen': -381.1816711425781, 'logits/rejected': -0.8136747479438782, 'logits/chosen': -0.8626751899719238, 'epoch': 0.24}

  4%|▍         | 437/10740 [2:11:33<55:54:38, 19.54s/it]
[2024-04-01 21:25:06,933] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 438/10740 [2:11:49<52:38:28, 18.40s/it]

  4%|▍         | 439/10740 [2:12:06<52:03:25, 18.19s/it]
[2024-04-01 21:25:43,143] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 441/10740 [2:12:43<52:24:07, 18.32s/it]

  4%|▍         | 442/10740 [2:12:58<49:25:49, 17.28s/it]
{'loss': 0.4959, 'learning_rate': 1.999356080273057e-06, 'rewards/chosen': -0.7665207386016846, 'rewards/rejected': -1.5060170888900757, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7394963502883911, 'policy_logps/rejected': -189.82470703125, 'policy_logps/chosen': -385.4682922363281, 'referece_logps/rejected': -174.76454162597656, 'referece_logps/chosen': -377.8031005859375, 'logits/rejected': -0.6885943412780762, 'logits/chosen': -0.681452751159668, 'epoch': 0.25}
[2024-04-01 21:26:38,717] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 443/10740 [2:13:20<53:44:44, 18.79s/it]
[2024-04-01 21:26:56,967] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 445/10740 [2:14:00<55:38:11, 19.46s/it]
{'loss': 0.5798, 'learning_rate': 1.9993232081989244e-06, 'rewards/chosen': -0.7034796476364136, 'rewards/rejected': -1.1499618291854858, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4464820921421051, 'policy_logps/rejected': -286.065185546875, 'policy_logps/chosen': -303.7461242675781, 'referece_logps/rejected': -274.5655822753906, 'referece_logps/chosen': -296.7113037109375, 'logits/rejected': -0.3586089015007019, 'logits/chosen': -0.2768295407295227, 'epoch': 0.25}


  4%|▍         | 447/10740 [2:14:21<42:46:31, 14.96s/it]

  4%|▍         | 448/10740 [2:14:32<39:05:51, 13.68s/it]
{'loss': 0.395, 'learning_rate': 1.9992895181067723e-06, 'rewards/chosen': -0.2123691439628601, 'rewards/rejected': -1.5876429080963135, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3752737045288086, 'policy_logps/rejected': -245.01895141601562, 'policy_logps/chosen': -585.14990234375, 'referece_logps/rejected': -229.1425323486328, 'referece_logps/chosen': -583.0262451171875, 'logits/rejected': 0.05252280831336975, 'logits/chosen': -0.13113802671432495, 'epoch': 0.25}
[2024-04-01 21:28:09,647] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 449/10740 [2:14:51<43:55:37, 15.37s/it]


  4%|▍         | 451/10740 [2:15:26<47:00:44, 16.45s/it]
[2024-04-01 21:28:44,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 452/10740 [2:15:48<51:23:01, 17.98s/it]

  4%|▍         | 453/10740 [2:16:10<55:01:30, 19.26s/it]

  4%|▍         | 454/10740 [2:16:34<58:59:26, 20.65s/it]
[2024-04-01 21:29:52,342] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6248, 'learning_rate': 1.999219683979391e-06, 'rewards/chosen': -1.2703227996826172, 'rewards/rejected': -1.133275032043457, 'rewards/accuracies': 0.25, 'rewards/margins': -0.1370478719472885, 'policy_logps/rejected': -231.94961547851562, 'policy_logps/chosen': -310.0370178222656, 'referece_logps/rejected': -220.6168670654297, 'referece_logps/chosen': -297.3337707519531, 'logits/rejected': -0.06990447640419006, 'logits/chosen': 0.10060958564281464, 'epoch': 0.25}


  4%|▍         | 456/10740 [2:17:10<54:43:06, 19.15s/it]
{'loss': 0.5025, 'learning_rate': 1.999195678873677e-06, 'rewards/chosen': -1.0591660737991333, 'rewards/rejected': -1.590165376663208, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5309991836547852, 'policy_logps/rejected': -309.394287109375, 'policy_logps/chosen': -405.85589599609375, 'referece_logps/rejected': -293.4926452636719, 'referece_logps/chosen': -395.2642517089844, 'logits/rejected': -1.4150605201721191, 'logits/chosen': -1.3078289031982422, 'epoch': 0.25}

  4%|▍         | 457/10740 [2:17:27<52:35:24, 18.41s/it]


  4%|▍         | 459/10740 [2:18:00<49:52:05, 17.46s/it]
{'loss': 0.4813, 'learning_rate': 1.9991589896231563e-06, 'rewards/chosen': -0.9932339191436768, 'rewards/rejected': -2.1339664459228516, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1407324075698853, 'policy_logps/rejected': -262.67669677734375, 'policy_logps/chosen': -341.83648681640625, 'referece_logps/rejected': -241.3370361328125, 'referece_logps/chosen': -331.9041748046875, 'logits/rejected': -0.11655225604772568, 'logits/chosen': -0.10860227793455124, 'epoch': 0.26}


  4%|▍         | 461/10740 [2:18:33<49:17:10, 17.26s/it]

  4%|▍         | 462/10740 [2:18:54<53:07:19, 18.61s/it]

  4%|▍         | 463/10740 [2:19:15<54:57:48, 19.25s/it]
{'loss': 0.5951, 'learning_rate': 1.9991087983643796e-06, 'rewards/chosen': -1.4336668252944946, 'rewards/rejected': -1.9694387912750244, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5357718467712402, 'policy_logps/rejected': -337.4080810546875, 'policy_logps/chosen': -288.5509033203125, 'referece_logps/rejected': -317.7137145996094, 'referece_logps/chosen': -274.2142333984375, 'logits/rejected': -1.0759660005569458, 'logits/chosen': -0.9054179191589355, 'epoch': 0.26}


  4%|▍         | 465/10740 [2:19:51<52:00:35, 18.22s/it]
{'loss': 0.4306, 'learning_rate': 1.999083157502033e-06, 'rewards/chosen': -1.127501130104065, 'rewards/rejected': -2.247293710708618, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1197925806045532, 'policy_logps/rejected': -798.6486206054688, 'policy_logps/chosen': -376.62628173828125, 'referece_logps/rejected': -776.1757202148438, 'referece_logps/chosen': -365.35125732421875, 'logits/rejected': -1.4683581590652466, 'logits/chosen': -1.0492888689041138, 'epoch': 0.26}

  4%|▍         | 466/10740 [2:20:14<56:27:22, 19.78s/it]


  4%|▍         | 468/10740 [2:20:46<50:42:44, 17.77s/it]

  4%|▍         | 469/10740 [2:21:09<54:43:26, 19.18s/it]
{'loss': 0.5602, 'learning_rate': 1.9990307853580646e-06, 'rewards/chosen': -1.123359203338623, 'rewards/rejected': -1.7565484046936035, 'rewards/accuracies': 1.0, 'rewards/margins': 0.63318932056427, 'policy_logps/rejected': -428.3001403808594, 'policy_logps/chosen': -412.2637939453125, 'referece_logps/rejected': -410.7346496582031, 'referece_logps/chosen': -401.0301513671875, 'logits/rejected': 0.21543169021606445, 'logits/chosen': 0.07955022156238556, 'epoch': 0.26}

  4%|▍         | 470/10740 [2:21:29<55:41:35, 19.52s/it]

  4%|▍         | 471/10740 [2:21:52<58:43:44, 20.59s/it]

  4%|▍         | 472/10740 [2:22:12<57:39:19, 20.21s/it]


  4%|▍         | 474/10740 [2:22:51<56:28:32, 19.80s/it]
{'loss': 0.4347, 'learning_rate': 1.9989632757407203e-06, 'rewards/chosen': -1.4630672931671143, 'rewards/rejected': -1.727997064590454, 'rewards/accuracies': 0.625, 'rewards/margins': 0.264929860830307, 'policy_logps/rejected': -326.2100830078125, 'policy_logps/chosen': -326.80462646484375, 'referece_logps/rejected': -308.93011474609375, 'referece_logps/chosen': -312.1739501953125, 'logits/rejected': -0.010329509153962135, 'logits/chosen': -0.0894627571105957, 'epoch': 0.26}

  4%|▍         | 475/10740 [2:23:02<49:40:53, 17.42s/it]
[2024-04-01 21:36:44,649] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 476/10740 [2:23:26<55:17:23, 19.39s/it]
[2024-04-01 21:37:02,062] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 477/10740 [2:23:44<53:35:29, 18.80s/it]

  4%|▍         | 478/10740 [2:24:02<53:27:51, 18.76s/it]

  4%|▍         | 479/10740 [2:24:24<56:04:25, 19.67s/it]

  4%|▍         | 480/10740 [2:24:46<58:12:33, 20.42s/it]


  4%|▍         | 482/10740 [2:25:19<51:09:01, 17.95s/it]
{'loss': 0.419, 'learning_rate': 1.9988505357953104e-06, 'rewards/chosen': -1.001365303993225, 'rewards/rejected': -1.9887182712554932, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9873529672622681, 'policy_logps/rejected': -374.3511962890625, 'policy_logps/chosen': -312.43927001953125, 'referece_logps/rejected': -354.46405029296875, 'referece_logps/chosen': -302.42559814453125, 'logits/rejected': -0.015511713922023773, 'logits/chosen': 0.22891753911972046, 'epoch': 0.27}

  4%|▍         | 483/10740 [2:25:33<48:17:57, 16.95s/it]

  5%|▍         | 484/10740 [2:25:54<51:50:37, 18.20s/it]


  5%|▍         | 486/10740 [2:26:31<52:03:57, 18.28s/it]

  5%|▍         | 487/10740 [2:26:47<50:06:18, 17.59s/it]
{'loss': 0.4543, 'learning_rate': 1.998777120730833e-06, 'rewards/chosen': -1.0142532587051392, 'rewards/rejected': -1.8476487398147583, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8333954215049744, 'policy_logps/rejected': -211.6950225830078, 'policy_logps/chosen': -318.2038879394531, 'referece_logps/rejected': -193.21853637695312, 'referece_logps/chosen': -308.06134033203125, 'logits/rejected': -0.9050136208534241, 'logits/chosen': -0.6879687309265137, 'epoch': 0.27}

  5%|▍         | 488/10740 [2:27:04<49:38:30, 17.43s/it]

  5%|▍         | 489/10740 [2:27:24<51:40:29, 18.15s/it]

  5%|▍         | 490/10740 [2:27:43<52:51:25, 18.56s/it]

  5%|▍         | 491/10740 [2:27:58<49:06:20, 17.25s/it]


  5%|▍         | 493/10740 [2:28:41<55:39:37, 19.55s/it]
{'loss': 0.5319, 'learning_rate': 1.9986860249085163e-06, 'rewards/chosen': -1.2487257719039917, 'rewards/rejected': -1.3710525035858154, 'rewards/accuracies': 0.5, 'rewards/margins': 0.12232658267021179, 'policy_logps/rejected': -198.44778442382812, 'policy_logps/chosen': -171.16229248046875, 'referece_logps/rejected': -184.7372589111328, 'referece_logps/chosen': -158.67506408691406, 'logits/rejected': -0.7307018041610718, 'logits/chosen': -0.754440426826477, 'epoch': 0.28}

  5%|▍         | 494/10740 [2:28:56<51:46:08, 18.19s/it]


  5%|▍         | 496/10740 [2:29:37<55:43:16, 19.58s/it]
{'loss': 0.5639, 'learning_rate': 1.99863925073389e-06, 'rewards/chosen': -1.0655226707458496, 'rewards/rejected': -1.7351847887039185, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6696619987487793, 'policy_logps/rejected': -290.36883544921875, 'policy_logps/chosen': -281.3505859375, 'referece_logps/rejected': -273.0169982910156, 'referece_logps/chosen': -270.69537353515625, 'logits/rejected': -0.9219033718109131, 'logits/chosen': -0.9128949046134949, 'epoch': 0.28}


  5%|▍         | 498/10740 [2:30:17<56:13:05, 19.76s/it]
{'loss': 0.4583, 'learning_rate': 1.9986076138049983e-06, 'rewards/chosen': -1.344557762145996, 'rewards/rejected': -2.2993645668029785, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9548070430755615, 'policy_logps/rejected': -350.5990905761719, 'policy_logps/chosen': -359.80853271484375, 'referece_logps/rejected': -327.6054382324219, 'referece_logps/chosen': -346.3629455566406, 'logits/rejected': -0.20203746855258942, 'logits/chosen': -0.2191413789987564, 'epoch': 0.28}


  5%|▍         | 500/10740 [2:30:57<56:34:52, 19.89s/it]
[2024-04-01 21:44:15,624] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  5%|▍         | 500/10740 [2:30:57<56:34:52, 19.89s/it]/mnt/petrelfs/songmingyang/anaconda3/envs/vcd_origin/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.4828, 'learning_rate': 1.9985594772217224e-06, 'rewards/chosen': -1.1034245491027832, 'rewards/rejected': -1.9606361389160156, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8572114706039429, 'policy_logps/rejected': -337.8536376953125, 'policy_logps/chosen': -380.61297607421875, 'referece_logps/rejected': -318.24725341796875, 'referece_logps/chosen': -369.5787048339844, 'logits/rejected': -0.7347111701965332, 'logits/chosen': -0.6023966670036316, 'epoch': 0.28}

  5%|▍         | 502/10740 [2:31:47<61:18:32, 21.56s/it]
{'loss': 0.5173, 'learning_rate': 1.9985432500491427e-06, 'rewards/chosen': -1.4108588695526123, 'rewards/rejected': -2.3834784030914307, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9726196527481079, 'policy_logps/rejected': -310.3642272949219, 'policy_logps/chosen': -327.5686950683594, 'referece_logps/rejected': -286.5294494628906, 'referece_logps/chosen': -313.4601135253906, 'logits/rejected': -1.415216326713562, 'logits/chosen': -1.2023892402648926, 'epoch': 0.28}

  5%|▍         | 503/10740 [2:32:07<59:44:49, 21.01s/it]

  5%|▍         | 504/10740 [2:32:24<56:38:56, 19.92s/it]

  5%|▍         | 505/10740 [2:32:45<57:02:27, 20.06s/it]


  5%|▍         | 507/10740 [2:33:27<59:13:07, 20.83s/it]
{'loss': 0.4325, 'learning_rate': 1.9984607519166913e-06, 'rewards/chosen': -1.0649148225784302, 'rewards/rejected': -1.479261875152588, 'rewards/accuracies': 0.5, 'rewards/margins': 0.41434717178344727, 'policy_logps/rejected': -331.1167907714844, 'policy_logps/chosen': -239.2558135986328, 'referece_logps/rejected': -316.32415771484375, 'referece_logps/chosen': -228.606689453125, 'logits/rejected': -0.6517953872680664, 'logits/chosen': -0.5764098167419434, 'epoch': 0.28}

  5%|▍         | 508/10740 [2:33:47<58:11:38, 20.47s/it]

  5%|▍         | 509/10740 [2:34:07<57:39:26, 20.29s/it]
[2024-04-01 21:47:47,861] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 510/10740 [2:34:30<59:47:58, 21.04s/it]
[2024-04-01 21:48:06,306] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  5%|▍         | 512/10740 [2:35:05<54:58:10, 19.35s/it]
{'loss': 0.4911, 'learning_rate': 1.9983759834733017e-06, 'rewards/chosen': -0.8910112977027893, 'rewards/rejected': -1.6345527172088623, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7435415983200073, 'policy_logps/rejected': -269.2035217285156, 'policy_logps/chosen': -261.21112060546875, 'referece_logps/rejected': -252.8579864501953, 'referece_logps/chosen': -252.30096435546875, 'logits/rejected': -1.0935274362564087, 'logits/chosen': -0.8823966383934021, 'epoch': 0.29}


  5%|▍         | 514/10740 [2:35:46<56:04:32, 19.74s/it]
{'loss': 0.408, 'learning_rate': 1.9983414404518853e-06, 'rewards/chosen': -1.0630439519882202, 'rewards/rejected': -2.8792929649353027, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8162486553192139, 'policy_logps/rejected': -468.1153259277344, 'policy_logps/chosen': -296.59521484375, 'referece_logps/rejected': -439.3224182128906, 'referece_logps/chosen': -285.96478271484375, 'logits/rejected': -0.8503700494766235, 'logits/chosen': -0.6826856732368469, 'epoch': 0.29}


  5%|▍         | 516/10740 [2:36:24<54:48:10, 19.30s/it]
{'loss': 0.4265, 'learning_rate': 1.9983065342240675e-06, 'rewards/chosen': -0.8358170986175537, 'rewards/rejected': -2.321622371673584, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4858055114746094, 'policy_logps/rejected': -446.8870849609375, 'policy_logps/chosen': -393.23187255859375, 'referece_logps/rejected': -423.6708068847656, 'referece_logps/chosen': -384.8736572265625, 'logits/rejected': 0.40191173553466797, 'logits/chosen': 0.49363937973976135, 'epoch': 0.29}

  5%|▍         | 517/10740 [2:36:38<50:29:26, 17.78s/it]

  5%|▍         | 518/10740 [2:36:59<53:28:01, 18.83s/it]

  5%|▍         | 519/10740 [2:37:16<51:46:37, 18.24s/it]

  5%|▍         | 520/10740 [2:37:29<47:04:30, 16.58s/it]

  5%|▍         | 521/10740 [2:37:51<51:25:33, 18.12s/it]


  5%|▍         | 523/10740 [2:38:30<53:11:47, 18.74s/it]
{'loss': 0.53, 'learning_rate': 1.998181502360834e-06, 'rewards/chosen': -0.8930651545524597, 'rewards/rejected': -1.7168540954589844, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8237889409065247, 'policy_logps/rejected': -245.9227294921875, 'policy_logps/chosen': -378.3763427734375, 'referece_logps/rejected': -228.75418090820312, 'referece_logps/chosen': -369.4457092285156, 'logits/rejected': -0.7147382497787476, 'logits/chosen': -0.5480490922927856, 'epoch': 0.29}

  5%|▍         | 524/10740 [2:38:42<48:03:54, 16.94s/it]

  5%|▍         | 525/10740 [2:38:58<47:17:38, 16.67s/it]


  5%|▍         | 527/10740 [2:39:36<50:42:10, 17.87s/it]
{'loss': 0.5269, 'learning_rate': 1.998108058230789e-06, 'rewards/chosen': -0.9692347049713135, 'rewards/rejected': -2.2391273975372314, 'rewards/accuracies': 0.75, 'rewards/margins': 1.269892930984497, 'policy_logps/rejected': -398.7646179199219, 'policy_logps/chosen': -386.07061767578125, 'referece_logps/rejected': -376.37335205078125, 'referece_logps/chosen': -376.37823486328125, 'logits/rejected': -0.7718175649642944, 'logits/chosen': -0.8749217987060547, 'epoch': 0.29}

  5%|▍         | 528/10740 [2:39:56<53:11:24, 18.75s/it]

  5%|▍         | 529/10740 [2:40:20<56:51:34, 20.05s/it]

  5%|▍         | 530/10740 [2:40:32<50:46:06, 17.90s/it]

  5%|▍         | 531/10740 [2:40:53<53:28:16, 18.86s/it]

  5%|▍         | 532/10740 [2:41:05<47:18:19, 16.68s/it]

  5%|▍         | 533/10740 [2:41:25<50:23:33, 17.77s/it]


  5%|▍         | 535/10740 [2:42:02<52:04:40, 18.37s/it]
[2024-04-01 21:55:20,089] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 536/10740 [2:42:18<50:11:38, 17.71s/it]
{'loss': 0.4326, 'learning_rate': 1.9979374984530587e-06, 'rewards/chosen': -0.8670515418052673, 'rewards/rejected': -1.8210164308547974, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9539650678634644, 'policy_logps/rejected': -332.1308898925781, 'policy_logps/chosen': -440.5533752441406, 'referece_logps/rejected': -313.9206848144531, 'referece_logps/chosen': -431.88287353515625, 'logits/rejected': 0.15043821930885315, 'logits/chosen': 0.2728555202484131, 'epoch': 0.3}


  5%|▌         | 538/10740 [2:42:52<49:37:59, 17.51s/it]
{'loss': 0.5001, 'learning_rate': 1.997898597822044e-06, 'rewards/chosen': -1.368852972984314, 'rewards/rejected': -2.698989152908325, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3301360607147217, 'policy_logps/rejected': -467.97479248046875, 'policy_logps/chosen': -359.6603088378906, 'referece_logps/rejected': -440.98486328125, 'referece_logps/chosen': -345.9718322753906, 'logits/rejected': -0.25044167041778564, 'logits/chosen': -0.1624906361103058, 'epoch': 0.3}
[2024-04-01 21:56:31,747] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 539/10740 [2:43:14<53:03:37, 18.73s/it]
[2024-04-01 21:56:52,810] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 540/10740 [2:43:35<55:02:33, 19.43s/it]


  5%|▌         | 542/10740 [2:44:04<47:53:09, 16.90s/it]
{'loss': 0.5051, 'learning_rate': 1.997819707438427e-06, 'rewards/chosen': -0.9570204615592957, 'rewards/rejected': -1.8773269653320312, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9203064441680908, 'policy_logps/rejected': -424.4029541015625, 'policy_logps/chosen': -331.419189453125, 'referece_logps/rejected': -405.6296691894531, 'referece_logps/chosen': -321.8489990234375, 'logits/rejected': -0.7392090559005737, 'logits/chosen': -0.6638981103897095, 'epoch': 0.3}


  5%|▌         | 544/10740 [2:44:44<52:26:10, 18.51s/it]

  5%|▌         | 545/10740 [2:45:06<55:21:14, 19.55s/it]
[2024-04-01 21:58:24,153] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 546/10740 [2:45:28<57:31:18, 20.31s/it]
{'loss': 0.4485, 'learning_rate': 1.997739364988584e-06, 'rewards/chosen': -1.4153989553451538, 'rewards/rejected': -1.8305517435073853, 'rewards/accuracies': 0.625, 'rewards/margins': 0.41515275835990906, 'policy_logps/rejected': -463.5445251464844, 'policy_logps/chosen': -404.7004699707031, 'referece_logps/rejected': -445.2389831542969, 'referece_logps/chosen': -390.5465087890625, 'logits/rejected': -0.6913014650344849, 'logits/chosen': -0.7373338937759399, 'epoch': 0.31}


  5%|▌         | 548/10740 [2:46:14<61:50:28, 21.84s/it]
{'loss': 0.5181, 'learning_rate': 1.9976986492752815e-06, 'rewards/chosen': -1.349334955215454, 'rewards/rejected': -1.7488348484039307, 'rewards/accuracies': 0.75, 'rewards/margins': 0.39949989318847656, 'policy_logps/rejected': -396.39306640625, 'policy_logps/chosen': -414.4743957519531, 'referece_logps/rejected': -378.9046630859375, 'referece_logps/chosen': -400.9810791015625, 'logits/rejected': -0.5396650433540344, 'logits/chosen': -0.24940814077854156, 'epoch': 0.31}

  5%|▌         | 549/10740 [2:46:35<60:35:55, 21.41s/it]

  5%|▌         | 550/10740 [2:46:56<60:17:46, 21.30s/it]

  5%|▌         | 551/10740 [2:47:19<62:00:39, 21.91s/it]

  5%|▌         | 552/10740 [2:47:38<59:30:12, 21.03s/it]

  5%|▌         | 553/10740 [2:47:55<56:29:19, 19.96s/it]


  5%|▌         | 555/10740 [2:48:37<57:31:44, 20.33s/it]
{'loss': 0.4912, 'learning_rate': 1.9975532859683124e-06, 'rewards/chosen': -1.8049259185791016, 'rewards/rejected': -2.064213752746582, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2592877745628357, 'policy_logps/rejected': -309.5485534667969, 'policy_logps/chosen': -402.084228515625, 'referece_logps/rejected': -288.90643310546875, 'referece_logps/chosen': -384.03497314453125, 'logits/rejected': -0.3542442321777344, 'logits/chosen': -0.41200917959213257, 'epoch': 0.31}

  5%|▌         | 556/10740 [2:48:57<57:43:22, 20.40s/it]

  5%|▌         | 557/10740 [2:49:19<58:27:06, 20.66s/it]

  5%|▌         | 558/10740 [2:49:41<59:54:13, 21.18s/it]
[2024-04-01 22:03:20,940] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 559/10740 [2:50:03<60:26:23, 21.37s/it]
[2024-04-01 22:03:42,571] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▌         | 560/10740 [2:50:24<60:39:13, 21.45s/it]

  5%|▌         | 561/10740 [2:50:38<54:02:45, 19.11s/it]

  5%|▌         | 562/10740 [2:50:49<47:19:40, 16.74s/it]

  5%|▌         | 563/10740 [2:51:04<45:31:59, 16.11s/it]

  5%|▌         | 564/10740 [2:51:24<48:51:05, 17.28s/it]

  5%|▌         | 565/10740 [2:51:44<51:28:05, 18.21s/it]

  5%|▌         | 566/10740 [2:51:59<48:28:31, 17.15s/it]

  5%|▌         | 567/10740 [2:52:19<50:55:24, 18.02s/it]

  5%|▌         | 568/10740 [2:52:40<53:06:10, 18.79s/it]

  5%|▌         | 569/10740 [2:53:00<54:26:52, 19.27s/it]

  5%|▌         | 570/10740 [2:53:21<55:59:56, 19.82s/it]


  5%|▌         | 572/10740 [2:54:00<55:42:10, 19.72s/it]

  5%|▌         | 573/10740 [2:54:22<57:59:05, 20.53s/it]

  5%|▌         | 574/10740 [2:54:42<57:35:37, 20.40s/it]

  5%|▌         | 575/10740 [2:54:59<54:23:21, 19.26s/it]

  5%|▌         | 576/10740 [2:55:16<52:09:52, 18.48s/it]

  5%|▌         | 577/10740 [2:55:31<49:32:08, 17.55s/it]

  5%|▌         | 578/10740 [2:55:49<50:03:10, 17.73s/it]

  5%|▌         | 579/10740 [2:56:06<49:39:48, 17.60s/it]

  5%|▌         | 580/10740 [2:56:27<52:26:21, 18.58s/it]

  5%|▌         | 581/10740 [2:56:50<55:39:53, 19.73s/it]

  5%|▌         | 582/10740 [2:57:03<50:39:41, 17.95s/it]

  5%|▌         | 583/10740 [2:57:26<54:25:40, 19.29s/it]

  5%|▌         | 584/10740 [2:57:51<59:04:20, 20.94s/it]

  5%|▌         | 585/10740 [2:58:13<60:10:46, 21.33s/it]

  5%|▌         | 586/10740 [2:58:35<60:44:34, 21.54s/it]

  5%|▌         | 587/10740 [2:58:53<57:47:40, 20.49s/it]

  5%|▌         | 588/10740 [2:59:10<55:11:54, 19.57s/it]

  5%|▌         | 589/10740 [2:59:33<57:44:07, 20.48s/it]

  5%|▌         | 590/10740 [2:59:55<58:52:21, 20.88s/it]
[2024-04-01 22:13:13,070] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 591/10740 [3:00:09<52:51:40, 18.75s/it]

  6%|▌         | 592/10740 [3:00:30<55:17:46, 19.62s/it]
[2024-04-01 22:13:48,487] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 593/10740 [3:00:48<53:41:51, 19.05s/it]

  6%|▌         | 594/10740 [3:01:09<55:43:26, 19.77s/it]

  6%|▌         | 595/10740 [3:01:30<56:16:10, 19.97s/it]

  6%|▌         | 596/10740 [3:01:50<56:24:19, 20.02s/it]

  6%|▌         | 597/10740 [3:02:10<56:39:35, 20.11s/it]

  6%|▌         | 598/10740 [3:02:32<58:17:02, 20.69s/it]

  6%|▌         | 599/10740 [3:02:52<57:27:57, 20.40s/it]
[2024-04-01 22:16:10,323] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 600/10740 [3:03:14<58:30:37, 20.77s/it]

  6%|▌         | 601/10740 [3:03:33<57:06:19, 20.28s/it]

  6%|▌         | 602/10740 [3:03:52<56:32:19, 20.08s/it]

  6%|▌         | 603/10740 [3:04:03<48:40:11, 17.28s/it]

  6%|▌         | 604/10740 [3:04:19<47:32:39, 16.89s/it]

  6%|▌         | 605/10740 [3:04:41<51:22:49, 18.25s/it]

  6%|▌         | 606/10740 [3:04:59<51:49:28, 18.41s/it]

  6%|▌         | 607/10740 [3:05:20<53:50:31, 19.13s/it]

  6%|▌         | 608/10740 [3:05:33<48:17:23, 17.16s/it]

  6%|▌         | 609/10740 [3:05:53<50:52:29, 18.08s/it]

  6%|▌         | 610/10740 [3:06:15<53:48:56, 19.13s/it]

  6%|▌         | 611/10740 [3:06:36<55:23:35, 19.69s/it]

  6%|▌         | 612/10740 [3:06:49<50:17:37, 17.88s/it]

  6%|▌         | 613/10740 [3:07:06<49:20:03, 17.54s/it]

  6%|▌         | 614/10740 [3:07:20<46:02:46, 16.37s/it]

  6%|▌         | 615/10740 [3:07:38<47:51:51, 17.02s/it]

  6%|▌         | 616/10740 [3:07:57<49:05:49, 17.46s/it]

  6%|▌         | 617/10740 [3:08:11<46:27:14, 16.52s/it]

  6%|▌         | 618/10740 [3:08:23<42:47:28, 15.22s/it]

  6%|▌         | 619/10740 [3:08:40<44:09:21, 15.71s/it]

  6%|▌         | 620/10740 [3:08:56<44:10:15, 15.71s/it]

  6%|▌         | 621/10740 [3:09:17<48:49:08, 17.37s/it]

  6%|▌         | 622/10740 [3:09:31<45:58:41, 16.36s/it]

  6%|▌         | 623/10740 [3:09:44<43:34:06, 15.50s/it]

  6%|▌         | 624/10740 [3:10:00<43:42:55, 15.56s/it]

  6%|▌         | 625/10740 [3:10:21<48:30:06, 17.26s/it]

  6%|▌         | 626/10740 [3:10:35<45:21:34, 16.15s/it]

  6%|▌         | 627/10740 [3:10:58<50:54:05, 18.12s/it]

  6%|▌         | 628/10740 [3:11:16<50:54:08, 18.12s/it]

  6%|▌         | 629/10740 [3:11:29<47:03:55, 16.76s/it]

  6%|▌         | 630/10740 [3:11:44<44:52:50, 15.98s/it]

  6%|▌         | 631/10740 [3:11:58<43:30:47, 15.50s/it]

  6%|▌         | 632/10740 [3:12:10<40:52:01, 14.55s/it]

  6%|▌         | 633/10740 [3:12:30<45:08:12, 16.08s/it]

  6%|▌         | 634/10740 [3:12:44<43:07:28, 15.36s/it]

  6%|▌         | 635/10740 [3:13:00<43:53:44, 15.64s/it]

  6%|▌         | 636/10740 [3:13:17<45:18:26, 16.14s/it]

  6%|▌         | 637/10740 [3:13:36<47:11:25, 16.82s/it]

  6%|▌         | 638/10740 [3:13:57<51:07:13, 18.22s/it]
[2024-04-01 22:27:15,265] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 639/10740 [3:14:17<52:41:47, 18.78s/it]
[2024-04-01 22:27:35,361] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 640/10740 [3:14:37<53:53:22, 19.21s/it]

  6%|▌         | 641/10740 [3:14:52<49:55:40, 17.80s/it]
{'loss': 0.5632, 'learning_rate': 1.9954047857171573e-06, 'rewards/chosen': -1.5140643119812012, 'rewards/rejected': -2.778184413909912, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2641199827194214, 'policy_logps/rejected': -326.95721435546875, 'policy_logps/chosen': -312.283203125, 'referece_logps/rejected': -299.1753845214844, 'referece_logps/chosen': -297.142578125, 'logits/rejected': -0.1924150437116623, 'logits/chosen': -0.2509886622428894, 'epoch': 0.36}


  6%|▌         | 643/10740 [3:15:28<49:35:40, 17.68s/it]

  6%|▌         | 644/10740 [3:15:47<50:15:13, 17.92s/it]
[2024-04-01 22:29:05,173] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 645/10740 [3:16:03<48:19:53, 17.24s/it]

  6%|▌         | 646/10740 [3:16:19<47:34:28, 16.97s/it]

  6%|▌         | 647/10740 [3:16:40<51:08:15, 18.24s/it]
[2024-04-01 22:29:58,364] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 648/10740 [3:17:04<55:46:17, 19.89s/it]

  6%|▌         | 649/10740 [3:17:17<50:27:41, 18.00s/it]

  6%|▌         | 650/10740 [3:17:36<50:45:04, 18.11s/it]

  6%|▌         | 651/10740 [3:17:57<53:40:34, 19.15s/it]

  6%|▌         | 652/10740 [3:18:14<51:27:52, 18.37s/it]

  6%|▌         | 653/10740 [3:18:37<55:25:18, 19.78s/it]
[2024-04-01 22:31:55,261] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 654/10740 [3:18:58<56:32:07, 20.18s/it]
[2024-04-01 22:32:16,372] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 655/10740 [3:19:18<56:15:25, 20.08s/it]

  6%|▌         | 656/10740 [3:19:37<55:30:18, 19.82s/it]

  6%|▌         | 657/10740 [3:19:55<53:27:35, 19.09s/it]
[2024-04-01 22:33:12,808] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 658/10740 [3:20:07<48:02:38, 17.16s/it]

  6%|▌         | 659/10740 [3:20:24<47:19:14, 16.90s/it]

  6%|▌         | 660/10740 [3:20:42<48:18:27, 17.25s/it]

  6%|▌         | 661/10740 [3:20:56<45:56:46, 16.41s/it]

  6%|▌         | 662/10740 [3:21:17<50:02:19, 17.87s/it]
[2024-04-01 22:34:35,570] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 663/10740 [3:21:42<55:19:46, 19.77s/it]
[2024-04-01 22:34:59,751] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 664/10740 [3:22:03<56:31:00, 20.19s/it]
{'loss': 0.5417, 'learning_rate': 1.9947166377851117e-06, 'rewards/chosen': -1.4604146480560303, 'rewards/rejected': -2.6654915809631348, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2050766944885254, 'policy_logps/rejected': -473.65264892578125, 'policy_logps/chosen': -427.2481689453125, 'referece_logps/rejected': -446.9977722167969, 'referece_logps/chosen': -412.6440124511719, 'logits/rejected': -0.1426575630903244, 'logits/chosen': -0.15813080966472626, 'epoch': 0.37}
[2024-04-01 22:35:43,166] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 665/10740 [3:22:25<58:13:12, 20.80s/it]


  6%|▌         | 667/10740 [3:22:56<49:40:22, 17.75s/it]

  6%|▌         | 668/10740 [3:23:16<51:31:31, 18.42s/it]

  6%|▌         | 669/10740 [3:23:36<53:23:26, 19.09s/it]

  6%|▌         | 670/10740 [3:23:57<54:42:52, 19.56s/it]

  6%|▌         | 671/10740 [3:24:10<49:01:09, 17.53s/it]

  6%|▋         | 672/10740 [3:24:30<51:19:03, 18.35s/it]

  6%|▋         | 673/10740 [3:24:54<56:03:51, 20.05s/it]
{'loss': 0.5215, 'learning_rate': 1.9944343326986247e-06, 'rewards/chosen': -1.9225897789001465, 'rewards/rejected': -2.4580013751983643, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5354116559028625, 'policy_logps/rejected': -529.7025146484375, 'policy_logps/chosen': -417.0921630859375, 'referece_logps/rejected': -505.12255859375, 'referece_logps/chosen': -397.86627197265625, 'logits/rejected': -0.43720415234565735, 'logits/chosen': -0.41011568903923035, 'epoch': 0.38}


  6%|▋         | 675/10740 [3:25:34<56:36:22, 20.25s/it]

  6%|▋         | 676/10740 [3:25:52<54:08:02, 19.36s/it]
{'loss': 0.4099, 'learning_rate': 1.994338602867512e-06, 'rewards/chosen': -0.723293125629425, 'rewards/rejected': -1.6605908870697021, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9372978806495667, 'policy_logps/rejected': -312.9612731933594, 'policy_logps/chosen': -350.1522216796875, 'referece_logps/rejected': -296.35540771484375, 'referece_logps/chosen': -342.9193115234375, 'logits/rejected': -0.23978382349014282, 'logits/chosen': -0.14796462655067444, 'epoch': 0.38}


  6%|▋         | 678/10740 [3:26:29<53:24:00, 19.11s/it]

  6%|▋         | 679/10740 [3:26:43<49:09:21, 17.59s/it]
{'loss': 0.5061, 'learning_rate': 1.9942420590986384e-06, 'rewards/chosen': -0.8819612264633179, 'rewards/rejected': -2.434011697769165, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5520503520965576, 'policy_logps/rejected': -387.9539489746094, 'policy_logps/chosen': -359.50518798828125, 'referece_logps/rejected': -363.61383056640625, 'referece_logps/chosen': -350.68560791015625, 'logits/rejected': -0.43510884046554565, 'logits/chosen': -0.31666359305381775, 'epoch': 0.38}

  6%|▋         | 680/10740 [3:27:04<51:42:37, 18.50s/it]


  6%|▋         | 682/10740 [3:27:38<49:44:09, 17.80s/it]

  6%|▋         | 683/10740 [3:27:59<52:04:56, 18.64s/it]
{'loss': 0.4991, 'learning_rate': 1.9941120680847552e-06, 'rewards/chosen': -0.9661361575126648, 'rewards/rejected': -2.2060375213623047, 'rewards/accuracies': 0.75, 'rewards/margins': 1.239901065826416, 'policy_logps/rejected': -238.844482421875, 'policy_logps/chosen': -292.651611328125, 'referece_logps/rejected': -216.78411865234375, 'referece_logps/chosen': -282.9902648925781, 'logits/rejected': 0.08192082494497299, 'logits/chosen': 0.1663072556257248, 'epoch': 0.38}


  6%|▋         | 685/10740 [3:28:31<48:30:58, 17.37s/it]
{'loss': 0.4897, 'learning_rate': 1.994046530064387e-06, 'rewards/chosen': -1.817705512046814, 'rewards/rejected': -2.3130621910095215, 'rewards/accuracies': 0.625, 'rewards/margins': 0.49535661935806274, 'policy_logps/rejected': -265.96392822265625, 'policy_logps/chosen': -329.65948486328125, 'referece_logps/rejected': -242.83334350585938, 'referece_logps/chosen': -311.482421875, 'logits/rejected': -0.8953648209571838, 'logits/chosen': -0.8563632965087891, 'epoch': 0.38}


  6%|▋         | 687/10740 [3:29:10<52:50:25, 18.92s/it]

  6%|▋         | 688/10740 [3:29:34<56:48:41, 20.35s/it]
{'loss': 0.482, 'learning_rate': 1.9939475449590634e-06, 'rewards/chosen': -1.7569847106933594, 'rewards/rejected': -2.3230032920837402, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5660187005996704, 'policy_logps/rejected': -394.8345947265625, 'policy_logps/chosen': -399.2467041015625, 'referece_logps/rejected': -371.60455322265625, 'referece_logps/chosen': -381.6768493652344, 'logits/rejected': 0.006699085235595703, 'logits/chosen': -0.012293100357055664, 'epoch': 0.38}


  6%|▋         | 690/10740 [3:30:11<54:38:57, 19.58s/it]

  6%|▋         | 691/10740 [3:30:34<57:41:31, 20.67s/it]
[2024-04-01 22:43:52,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▋         | 692/10740 [3:30:54<57:30:09, 20.60s/it]

  6%|▋         | 693/10740 [3:31:13<55:38:40, 19.94s/it]
{'loss': 0.4676, 'learning_rate': 1.9937807617846132e-06, 'rewards/chosen': -0.8667981624603271, 'rewards/rejected': -2.1693780422210693, 'rewards/accuracies': 0.875, 'rewards/margins': 1.302579641342163, 'policy_logps/rejected': -400.3050537109375, 'policy_logps/chosen': -390.3389587402344, 'referece_logps/rejected': -378.61126708984375, 'referece_logps/chosen': -381.67095947265625, 'logits/rejected': -0.3420412540435791, 'logits/chosen': -0.3197615444660187, 'epoch': 0.39}

  6%|▋         | 694/10740 [3:31:32<54:56:06, 19.69s/it]


  6%|▋         | 696/10740 [3:32:17<59:16:07, 21.24s/it]

  6%|▋         | 697/10740 [3:32:33<54:51:50, 19.67s/it]
{'loss': 0.4581, 'learning_rate': 1.9936457082646187e-06, 'rewards/chosen': -1.1745383739471436, 'rewards/rejected': -2.824578046798706, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6500396728515625, 'policy_logps/rejected': -309.488037109375, 'policy_logps/chosen': -328.5526123046875, 'referece_logps/rejected': -281.2422790527344, 'referece_logps/chosen': -316.8072509765625, 'logits/rejected': -0.16045020520687103, 'logits/chosen': -0.11647405475378036, 'epoch': 0.39}


  7%|▋         | 699/10740 [3:33:12<54:34:07, 19.56s/it]
{'loss': 0.5065, 'learning_rate': 1.9935776392452336e-06, 'rewards/chosen': -1.5702362060546875, 'rewards/rejected': -2.7375617027282715, 'rewards/accuracies': 0.625, 'rewards/margins': 1.167325496673584, 'policy_logps/rejected': -319.90289306640625, 'policy_logps/chosen': -257.54095458984375, 'referece_logps/rejected': -292.5273132324219, 'referece_logps/chosen': -241.83859252929688, 'logits/rejected': 0.2650397717952728, 'logits/chosen': 0.1562480926513672, 'epoch': 0.39}
[2024-04-01 22:46:54,203] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 700/10740 [3:33:36<57:55:46, 20.77s/it]


  7%|▋         | 702/10740 [3:34:20<59:38:44, 21.39s/it]

  7%|▋         | 703/10740 [3:34:37<55:47:41, 20.01s/it]

  7%|▋         | 704/10740 [3:34:58<56:42:33, 20.34s/it]

  7%|▋         | 705/10740 [3:35:17<55:22:30, 19.87s/it]

  7%|▋         | 706/10740 [3:35:35<53:30:59, 19.20s/it]

  7%|▋         | 707/10740 [3:35:53<52:19:31, 18.78s/it]

  7%|▋         | 708/10740 [3:36:07<48:59:11, 17.58s/it]

  7%|▋         | 709/10740 [3:36:24<48:37:17, 17.45s/it]

  7%|▋         | 710/10740 [3:36:42<48:20:55, 17.35s/it]

  7%|▋         | 711/10740 [3:37:03<51:22:56, 18.44s/it]
[2024-04-01 22:50:20,810] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 712/10740 [3:37:24<53:38:18, 19.26s/it]

  7%|▋         | 713/10740 [3:37:46<55:44:32, 20.01s/it]

  7%|▋         | 714/10740 [3:38:07<56:56:43, 20.45s/it]

  7%|▋         | 715/10740 [3:38:21<51:47:46, 18.60s/it]
[2024-04-01 22:51:39,491] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 716/10740 [3:38:42<53:14:29, 19.12s/it]

  7%|▋         | 717/10740 [3:39:01<53:38:50, 19.27s/it]
{'loss': 0.4428, 'learning_rate': 1.992948754788098e-06, 'rewards/chosen': -1.2865945100784302, 'rewards/rejected': -2.465670108795166, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1790754795074463, 'policy_logps/rejected': -252.91558837890625, 'policy_logps/chosen': -430.3650207519531, 'referece_logps/rejected': -228.2589111328125, 'referece_logps/chosen': -417.49908447265625, 'logits/rejected': -0.6372485756874084, 'logits/chosen': -0.6596876978874207, 'epoch': 0.4}


  7%|▋         | 719/10740 [3:39:37<51:39:03, 18.56s/it]
[2024-04-01 22:52:55,464] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 720/10740 [3:39:56<51:41:19, 18.57s/it]

  7%|▋         | 721/10740 [3:40:09<47:18:36, 17.00s/it]

  7%|▋         | 722/10740 [3:40:31<51:32:38, 18.52s/it]

  7%|▋         | 723/10740 [3:40:50<51:29:45, 18.51s/it]

  7%|▋         | 724/10740 [3:41:11<53:36:53, 19.27s/it]

  7%|▋         | 725/10740 [3:41:27<50:54:26, 18.30s/it]
{'loss': 0.449, 'learning_rate': 1.992659857118057e-06, 'rewards/chosen': -1.3893243074417114, 'rewards/rejected': -2.063438892364502, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6741147041320801, 'policy_logps/rejected': -320.1744079589844, 'policy_logps/chosen': -435.88336181640625, 'referece_logps/rejected': -299.53997802734375, 'referece_logps/chosen': -421.9900817871094, 'logits/rejected': 0.0458359532058239, 'logits/chosen': 0.04919050633907318, 'epoch': 0.41}
[2024-04-01 22:55:04,377] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 727/10740 [3:42:03<50:07:53, 18.02s/it]
{'loss': 0.5264, 'learning_rate': 1.9925867297860646e-06, 'rewards/chosen': -1.8907454013824463, 'rewards/rejected': -2.6837165355682373, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7929710745811462, 'policy_logps/rejected': -423.0417785644531, 'policy_logps/chosen': -399.7745361328125, 'referece_logps/rejected': -396.2046203613281, 'referece_logps/chosen': -380.8670654296875, 'logits/rejected': -0.10444450378417969, 'logits/chosen': -0.10310479998588562, 'epoch': 0.41}


  7%|▋         | 729/10740 [3:42:33<45:54:44, 16.51s/it]

  7%|▋         | 730/10740 [3:42:47<43:44:03, 15.73s/it]

  7%|▋         | 731/10740 [3:43:05<46:01:28, 16.55s/it]
{'loss': 0.4357, 'learning_rate': 1.9924393918104725e-06, 'rewards/chosen': -1.4966857433319092, 'rewards/rejected': -2.8814077377319336, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3847216367721558, 'policy_logps/rejected': -465.3268737792969, 'policy_logps/chosen': -431.0481262207031, 'referece_logps/rejected': -436.5127868652344, 'referece_logps/chosen': -416.0812683105469, 'logits/rejected': -0.49409911036491394, 'logits/chosen': -0.41119930148124695, 'epoch': 0.41}


  7%|▋         | 733/10740 [3:43:38<45:59:36, 16.55s/it]

  7%|▋         | 734/10740 [3:43:59<49:32:10, 17.82s/it]
{'loss': 0.4711, 'learning_rate': 1.992327940536718e-06, 'rewards/chosen': -1.1416903734207153, 'rewards/rejected': -2.5639657974243164, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4222753047943115, 'policy_logps/rejected': -304.870361328125, 'policy_logps/chosen': -296.72100830078125, 'referece_logps/rejected': -279.230712890625, 'referece_logps/chosen': -285.3041076660156, 'logits/rejected': -0.9072175025939941, 'logits/chosen': -0.9822396636009216, 'epoch': 0.41}


  7%|▋         | 736/10740 [3:44:39<52:53:00, 19.03s/it]

  7%|▋         | 737/10740 [3:45:01<55:41:17, 20.04s/it]
[2024-04-01 22:58:19,614] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4099, 'learning_rate': 1.9922156769710736e-06, 'rewards/chosen': -1.6026506423950195, 'rewards/rejected': -2.907323122024536, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3046727180480957, 'policy_logps/rejected': -484.035888671875, 'policy_logps/chosen': -437.4212646484375, 'referece_logps/rejected': -454.9626770019531, 'referece_logps/chosen': -421.394775390625, 'logits/rejected': 0.9294969439506531, 'logits/chosen': 0.961970329284668, 'epoch': 0.41}


  7%|▋         | 739/10740 [3:45:37<51:44:20, 18.62s/it]

  7%|▋         | 740/10740 [3:45:51<47:46:24, 17.20s/it]

  7%|▋         | 741/10740 [3:46:12<50:43:26, 18.26s/it]

  7%|▋         | 742/10740 [3:46:29<50:19:47, 18.12s/it]

  7%|▋         | 743/10740 [3:46:52<54:20:10, 19.57s/it]
{'loss': 0.427, 'learning_rate': 1.991988713332367e-06, 'rewards/chosen': -1.5622988939285278, 'rewards/rejected': -2.3320415019989014, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7697427868843079, 'policy_logps/rejected': -300.1306457519531, 'policy_logps/chosen': -412.9305419921875, 'referece_logps/rejected': -276.81024169921875, 'referece_logps/chosen': -397.3075256347656, 'logits/rejected': -0.6969289779663086, 'logits/chosen': -0.7277173399925232, 'epoch': 0.42}


  7%|▋         | 745/10740 [3:47:36<57:32:46, 20.73s/it]
{'loss': 0.4777, 'learning_rate': 1.991912336958868e-06, 'rewards/chosen': -0.8699163794517517, 'rewards/rejected': -2.142031192779541, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2721147537231445, 'policy_logps/rejected': -343.6325378417969, 'policy_logps/chosen': -298.13470458984375, 'referece_logps/rejected': -322.21221923828125, 'referece_logps/chosen': -289.435546875, 'logits/rejected': -0.5413460731506348, 'logits/chosen': -0.4971436560153961, 'epoch': 0.42}
[2024-04-01 23:01:16,744] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 746/10740 [3:47:59<58:54:10, 21.22s/it]
[2024-04-01 23:01:34,801] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 748/10740 [3:48:36<55:27:20, 19.98s/it]
{'loss': 0.4346, 'learning_rate': 1.991797095780907e-06, 'rewards/chosen': -1.902564525604248, 'rewards/rejected': -2.930349826812744, 'rewards/accuracies': 0.375, 'rewards/margins': 1.027785301208496, 'policy_logps/rejected': -291.2451171875, 'policy_logps/chosen': -350.5828552246094, 'referece_logps/rejected': -261.9416198730469, 'referece_logps/chosen': -331.5572204589844, 'logits/rejected': -0.7548449039459229, 'logits/chosen': -0.7616747617721558, 'epoch': 0.42}

  7%|▋         | 749/10740 [3:48:49<49:20:01, 17.78s/it]

  7%|▋         | 750/10740 [3:49:05<48:33:28, 17.50s/it]

  7%|▋         | 751/10740 [3:49:19<45:26:07, 16.37s/it]

  7%|▋         | 752/10740 [3:49:43<51:49:36, 18.68s/it]

  7%|▋         | 753/10740 [3:49:55<46:14:10, 16.67s/it]

  7%|▋         | 754/10740 [3:50:12<46:02:13, 16.60s/it]

  7%|▋         | 755/10740 [3:50:27<44:59:24, 16.22s/it]

  7%|▋         | 756/10740 [3:50:41<43:36:05, 15.72s/it]


  7%|▋         | 758/10740 [3:51:23<50:31:49, 18.22s/it]

  7%|▋         | 759/10740 [3:51:38<48:14:06, 17.40s/it]
{'loss': 0.4617, 'learning_rate': 1.9913675996289105e-06, 'rewards/chosen': -1.4193260669708252, 'rewards/rejected': -2.9757468700408936, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5564206838607788, 'policy_logps/rejected': -437.54608154296875, 'policy_logps/chosen': -419.16937255859375, 'referece_logps/rejected': -407.78857421875, 'referece_logps/chosen': -404.97613525390625, 'logits/rejected': -0.7481307983398438, 'logits/chosen': -0.9189664721488953, 'epoch': 0.42}

  7%|▋         | 760/10740 [3:51:55<47:58:56, 17.31s/it]

  7%|▋         | 761/10740 [3:52:15<50:19:00, 18.15s/it]


  7%|▋         | 763/10740 [3:52:48<47:37:14, 17.18s/it]

  7%|▋         | 764/10740 [3:53:10<51:41:56, 18.66s/it]
{'loss': 0.4983, 'learning_rate': 1.9911687671274496e-06, 'rewards/chosen': -1.4024136066436768, 'rewards/rejected': -2.8295726776123047, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4271588325500488, 'policy_logps/rejected': -411.16400146484375, 'policy_logps/chosen': -335.9661865234375, 'referece_logps/rejected': -382.8682556152344, 'referece_logps/chosen': -321.9420166015625, 'logits/rejected': -0.6374399662017822, 'logits/chosen': -0.567422091960907, 'epoch': 0.43}


  7%|▋         | 766/10740 [3:53:55<56:15:12, 20.30s/it]
{'loss': 0.5238, 'learning_rate': 1.991088603056879e-06, 'rewards/chosen': -1.5608290433883667, 'rewards/rejected': -1.5618952512741089, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0010662972927093506, 'policy_logps/rejected': -325.32568359375, 'policy_logps/chosen': -283.6187438964844, 'referece_logps/rejected': -309.7067565917969, 'referece_logps/chosen': -268.0104675292969, 'logits/rejected': -0.032481975853443146, 'logits/chosen': 0.045887500047683716, 'epoch': 0.43}

  7%|▋         | 767/10740 [3:54:16<56:51:05, 20.52s/it]

  7%|▋         | 768/10740 [3:54:39<59:20:56, 21.43s/it]


  7%|▋         | 770/10740 [3:55:06<48:29:58, 17.51s/it]

  7%|▋         | 771/10740 [3:55:26<50:43:27, 18.32s/it]

  7%|▋         | 772/10740 [3:55:40<47:06:16, 17.01s/it]
{'loss': 0.5739, 'learning_rate': 1.9908459475559937e-06, 'rewards/chosen': -1.0049817562103271, 'rewards/rejected': -1.8144867420196533, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8095048666000366, 'policy_logps/rejected': -361.25958251953125, 'policy_logps/chosen': -336.6562194824219, 'referece_logps/rejected': -343.11474609375, 'referece_logps/chosen': -326.6064453125, 'logits/rejected': -0.2129938006401062, 'logits/chosen': -0.19364485144615173, 'epoch': 0.43}

  7%|▋         | 773/10740 [3:55:58<47:24:59, 17.13s/it]
[2024-04-01 23:09:40,200] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 774/10740 [3:56:22<53:16:14, 19.24s/it]


  7%|▋         | 776/10740 [3:57:01<53:57:19, 19.49s/it]
{'loss': 0.4072, 'learning_rate': 1.9906823747756883e-06, 'rewards/chosen': -1.3009282350540161, 'rewards/rejected': -2.418252944946289, 'rewards/accuracies': 0.875, 'rewards/margins': 1.117324709892273, 'policy_logps/rejected': -325.8074951171875, 'policy_logps/chosen': -543.7781982421875, 'referece_logps/rejected': -301.625, 'referece_logps/chosen': -530.7689208984375, 'logits/rejected': -0.8580854535102844, 'logits/chosen': -0.9419152736663818, 'epoch': 0.43}

  7%|▋         | 777/10740 [3:57:19<52:54:48, 19.12s/it]
[2024-04-01 23:10:58,226] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 778/10740 [3:57:40<54:28:09, 19.68s/it]
[2024-04-01 23:11:17,577] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 779/10740 [3:57:59<54:11:15, 19.58s/it]


  7%|▋         | 781/10740 [3:58:33<49:41:48, 17.96s/it]
{'loss': 0.4728, 'learning_rate': 1.990475881466322e-06, 'rewards/chosen': -1.3008222579956055, 'rewards/rejected': -2.600552797317505, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2997307777404785, 'policy_logps/rejected': -572.560791015625, 'policy_logps/chosen': -381.04998779296875, 'referece_logps/rejected': -546.5552978515625, 'referece_logps/chosen': -368.0417785644531, 'logits/rejected': -1.3277301788330078, 'logits/chosen': -0.9874259233474731, 'epoch': 0.44}

  7%|▋         | 782/10740 [3:58:54<52:27:55, 18.97s/it]

  7%|▋         | 783/10740 [3:59:14<53:09:21, 19.22s/it]


  7%|▋         | 785/10740 [3:59:43<46:12:21, 16.71s/it]
{'loss': 0.5702, 'learning_rate': 1.9903090652446256e-06, 'rewards/chosen': -0.8425686359405518, 'rewards/rejected': -1.2632778882980347, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4207092523574829, 'policy_logps/rejected': -385.397705078125, 'policy_logps/chosen': -436.127197265625, 'referece_logps/rejected': -372.76495361328125, 'referece_logps/chosen': -427.7015380859375, 'logits/rejected': -0.3501817286014557, 'logits/chosen': -0.32698822021484375, 'epoch': 0.44}


  7%|▋         | 787/10740 [4:00:07<39:45:31, 14.38s/it]
{'loss': 0.475, 'learning_rate': 1.990225116692356e-06, 'rewards/chosen': -1.0920826196670532, 'rewards/rejected': -1.8001950979232788, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7081124186515808, 'policy_logps/rejected': -205.6707000732422, 'policy_logps/chosen': -225.00579833984375, 'referece_logps/rejected': -187.66876220703125, 'referece_logps/chosen': -214.08499145507812, 'logits/rejected': 0.002377063035964966, 'logits/chosen': 0.010045960545539856, 'epoch': 0.44}


  7%|▋         | 789/10740 [4:00:47<47:13:38, 17.09s/it]
{'loss': 0.4526, 'learning_rate': 1.990140807886483e-06, 'rewards/chosen': -1.2141785621643066, 'rewards/rejected': -2.1841511726379395, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9699726700782776, 'policy_logps/rejected': -442.84234619140625, 'policy_logps/chosen': -485.4847106933594, 'referece_logps/rejected': -421.0008544921875, 'referece_logps/chosen': -473.3428955078125, 'logits/rejected': -1.0150997638702393, 'logits/chosen': -0.9325259923934937, 'epoch': 0.44}
[2024-04-01 23:14:26,564] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 790/10740 [4:01:08<50:40:37, 18.34s/it]


  7%|▋         | 792/10740 [4:01:51<54:46:41, 19.82s/it]
{'loss': 0.4531, 'learning_rate': 1.9900136692692997e-06, 'rewards/chosen': -1.7059730291366577, 'rewards/rejected': -2.405496835708618, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6995238065719604, 'policy_logps/rejected': -442.1583557128906, 'policy_logps/chosen': -452.0090637207031, 'referece_logps/rejected': -418.1033935546875, 'referece_logps/chosen': -434.9493408203125, 'logits/rejected': -0.03934498131275177, 'logits/chosen': 0.0038557294756174088, 'epoch': 0.44}


  7%|▋         | 794/10740 [4:02:27<52:46:48, 19.10s/it]
{'loss': 0.3921, 'learning_rate': 1.9899284599638974e-06, 'rewards/chosen': -0.981450080871582, 'rewards/rejected': -1.5910893678665161, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6096392869949341, 'policy_logps/rejected': -381.7647705078125, 'policy_logps/chosen': -394.133056640625, 'referece_logps/rejected': -365.8538818359375, 'referece_logps/chosen': -384.3185729980469, 'logits/rejected': -0.28973981738090515, 'logits/chosen': -0.3365136981010437, 'epoch': 0.44}
[2024-04-01 23:16:05,634] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 795/10740 [4:02:47<54:10:47, 19.61s/it]

  7%|▋         | 796/10740 [4:03:00<48:15:05, 17.47s/it]

  7%|▋         | 797/10740 [4:03:16<47:30:21, 17.20s/it]

  7%|▋         | 798/10740 [4:03:34<47:46:48, 17.30s/it]
[2024-04-01 23:17:15,853] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 799/10740 [4:03:58<53:01:42, 19.20s/it]

  7%|▋         | 800/10740 [4:04:11<48:36:25, 17.60s/it]


  7%|▋         | 802/10740 [4:04:57<55:42:43, 20.18s/it]

  7%|▋         | 803/10740 [4:05:17<55:30:46, 20.11s/it]
[2024-04-01 23:18:34,995] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4251, 'learning_rate': 1.989540561737478e-06, 'rewards/chosen': -2.280707597732544, 'rewards/rejected': -4.064090251922607, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7833824157714844, 'policy_logps/rejected': -439.7981872558594, 'policy_logps/chosen': -565.8342895507812, 'referece_logps/rejected': -399.1573181152344, 'referece_logps/chosen': -543.0271606445312, 'logits/rejected': -0.46836140751838684, 'logits/chosen': -0.6035417914390564, 'epoch': 0.45}


  7%|▋         | 805/10740 [4:05:51<52:21:40, 18.97s/it]
{'loss': 0.4641, 'learning_rate': 1.9894533720179927e-06, 'rewards/chosen': -0.8979960083961487, 'rewards/rejected': -2.352876663208008, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4548808336257935, 'policy_logps/rejected': -396.42706298828125, 'policy_logps/chosen': -359.2887268066406, 'referece_logps/rejected': -372.89825439453125, 'referece_logps/chosen': -350.30877685546875, 'logits/rejected': 1.173313021659851, 'logits/chosen': 1.097244381904602, 'epoch': 0.45}

  8%|▊         | 806/10740 [4:06:06<48:34:20, 17.60s/it]

  8%|▊         | 807/10740 [4:06:26<50:26:26, 18.28s/it]

  8%|▊         | 808/10740 [4:06:41<47:40:41, 17.28s/it]


  8%|▊         | 810/10740 [4:07:17<49:09:37, 17.82s/it]
{'loss': 0.5021, 'learning_rate': 1.9892338229078394e-06, 'rewards/chosen': -1.1397514343261719, 'rewards/rejected': -2.0626938343048096, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9229426383972168, 'policy_logps/rejected': -255.1568603515625, 'policy_logps/chosen': -379.4237976074219, 'referece_logps/rejected': -234.52992248535156, 'referece_logps/chosen': -368.0262756347656, 'logits/rejected': -0.22739854454994202, 'logits/chosen': -0.28768837451934814, 'epoch': 0.45}


  8%|▊         | 812/10740 [4:07:54<49:10:49, 17.83s/it]
{'loss': 0.402, 'learning_rate': 1.989145373423069e-06, 'rewards/chosen': -1.8805018663406372, 'rewards/rejected': -2.376624584197998, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4961225986480713, 'policy_logps/rejected': -403.4508361816406, 'policy_logps/chosen': -518.9832763671875, 'referece_logps/rejected': -379.6845703125, 'referece_logps/chosen': -500.17828369140625, 'logits/rejected': -0.8561774492263794, 'logits/chosen': -0.6905544996261597, 'epoch': 0.45}

  8%|▊         | 813/10740 [4:08:14<51:35:49, 18.71s/it]
[2024-04-01 23:21:52,909] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 814/10740 [4:08:35<52:55:55, 19.20s/it]

  8%|▊         | 815/10740 [4:08:51<50:11:14, 18.20s/it]

  8%|▊         | 816/10740 [4:09:08<49:47:46, 18.06s/it]

  8%|▊         | 817/10740 [4:09:30<52:42:05, 19.12s/it]


  8%|▊         | 819/10740 [4:10:15<57:46:48, 20.97s/it]
[2024-04-01 23:23:33,559] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4661, 'learning_rate': 1.988832966535071e-06, 'rewards/chosen': -1.9569470882415771, 'rewards/rejected': -2.983494281768799, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0265471935272217, 'policy_logps/rejected': -460.5420227050781, 'policy_logps/chosen': -357.90643310546875, 'referece_logps/rejected': -430.70703125, 'referece_logps/chosen': -338.3369445800781, 'logits/rejected': -0.0551086962223053, 'logits/chosen': -0.20721831917762756, 'epoch': 0.46}
[2024-04-01 23:23:56,556] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 820/10740 [4:10:38<59:27:06, 21.58s/it]


  8%|▊         | 822/10740 [4:11:15<54:47:24, 19.89s/it]

  8%|▊         | 823/10740 [4:11:34<53:37:38, 19.47s/it]
{'loss': 0.4933, 'learning_rate': 1.9886524696147324e-06, 'rewards/chosen': -1.3231528997421265, 'rewards/rejected': -2.042304515838623, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7191516160964966, 'policy_logps/rejected': -307.41522216796875, 'policy_logps/chosen': -408.7300109863281, 'referece_logps/rejected': -286.9921875, 'referece_logps/chosen': -395.4985046386719, 'logits/rejected': -0.541366457939148, 'logits/chosen': -0.4425116181373596, 'epoch': 0.46}


  8%|▊         | 825/10740 [4:12:07<50:00:03, 18.15s/it]
{'loss': 0.3593, 'learning_rate': 1.9885616816159256e-06, 'rewards/chosen': -1.1978659629821777, 'rewards/rejected': -2.27099871635437, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0731329917907715, 'policy_logps/rejected': -264.7521667480469, 'policy_logps/chosen': -242.40670776367188, 'referece_logps/rejected': -242.0421905517578, 'referece_logps/chosen': -230.42803955078125, 'logits/rejected': -0.4203958213329315, 'logits/chosen': -0.376435250043869, 'epoch': 0.46}


  8%|▊         | 827/10740 [4:12:50<54:22:50, 19.75s/it]
{'loss': 0.4956, 'learning_rate': 1.98847053396869e-06, 'rewards/chosen': -1.7108447551727295, 'rewards/rejected': -2.2694389820098877, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5585941672325134, 'policy_logps/rejected': -291.9464416503906, 'policy_logps/chosen': -317.6766662597656, 'referece_logps/rejected': -269.2520446777344, 'referece_logps/chosen': -300.5682373046875, 'logits/rejected': -0.4640118181705475, 'logits/chosen': -0.3767746090888977, 'epoch': 0.46}

  8%|▊         | 828/10740 [4:13:08<53:03:51, 19.27s/it]


  8%|▊         | 830/10740 [4:13:43<49:57:51, 18.15s/it]
[2024-04-01 23:27:01,545] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 831/10740 [4:13:55<45:01:14, 16.36s/it]

  8%|▊         | 832/10740 [4:14:09<42:55:16, 15.60s/it]

  8%|▊         | 833/10740 [4:14:22<40:28:40, 14.71s/it]
{'loss': 0.3898, 'learning_rate': 1.988194933468667e-06, 'rewards/chosen': -1.3280889987945557, 'rewards/rejected': -2.4824907779693604, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1544016599655151, 'policy_logps/rejected': -291.46771240234375, 'policy_logps/chosen': -327.3543701171875, 'referece_logps/rejected': -266.6427917480469, 'referece_logps/chosen': -314.0734558105469, 'logits/rejected': -0.2171963006258011, 'logits/chosen': -0.16409000754356384, 'epoch': 0.47}

  8%|▊         | 834/10740 [4:14:39<42:17:05, 15.37s/it]


  8%|▊         | 836/10740 [4:15:14<46:25:03, 16.87s/it]
{'loss': 0.4181, 'learning_rate': 1.9880559197990027e-06, 'rewards/chosen': -1.5129753351211548, 'rewards/rejected': -2.33565092086792, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8226755261421204, 'policy_logps/rejected': -290.47808837890625, 'policy_logps/chosen': -294.04278564453125, 'referece_logps/rejected': -267.1216125488281, 'referece_logps/chosen': -278.91302490234375, 'logits/rejected': -1.3493813276290894, 'logits/chosen': -1.2590411901474, 'epoch': 0.47}

  8%|▊         | 837/10740 [4:15:29<45:05:18, 16.39s/it]


  8%|▊         | 839/10740 [4:16:01<44:58:54, 16.36s/it]
{'loss': 0.4472, 'learning_rate': 1.987916097334406e-06, 'rewards/chosen': -1.5820575952529907, 'rewards/rejected': -3.348568916320801, 'rewards/accuracies': 0.875, 'rewards/margins': 1.766511082649231, 'policy_logps/rejected': -311.2926025390625, 'policy_logps/chosen': -353.76666259765625, 'referece_logps/rejected': -277.80694580078125, 'referece_logps/chosen': -337.9460754394531, 'logits/rejected': -0.9200875163078308, 'logits/chosen': -0.9029305577278137, 'epoch': 0.47}


  8%|▊         | 841/10740 [4:16:26<38:47:01, 14.10s/it]
{'loss': 0.4995, 'learning_rate': 1.987822433083984e-06, 'rewards/chosen': -1.2094337940216064, 'rewards/rejected': -2.162039041519165, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9526051878929138, 'policy_logps/rejected': -250.98666381835938, 'policy_logps/chosen': -506.9205322265625, 'referece_logps/rejected': -229.36627197265625, 'referece_logps/chosen': -494.826171875, 'logits/rejected': -0.10455794632434845, 'logits/chosen': 0.05504680424928665, 'epoch': 0.47}

  8%|▊         | 842/10740 [4:16:42<40:57:11, 14.90s/it]
[2024-04-01 23:30:22,446] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 843/10740 [4:17:04<46:39:06, 16.97s/it]


  8%|▊         | 845/10740 [4:17:34<44:26:02, 16.17s/it]

  8%|▊         | 846/10740 [4:17:56<49:00:24, 17.83s/it]

  8%|▊         | 847/10740 [4:18:16<50:35:03, 18.41s/it]

  8%|▊         | 848/10740 [4:18:28<45:48:41, 16.67s/it]
{'loss': 0.4712, 'learning_rate': 1.98749177831888e-06, 'rewards/chosen': -0.8151299953460693, 'rewards/rejected': -1.6742753982543945, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8591454029083252, 'policy_logps/rejected': -180.30340576171875, 'policy_logps/chosen': -315.1232604980469, 'referece_logps/rejected': -163.56068420410156, 'referece_logps/chosen': -306.9719543457031, 'logits/rejected': -1.060647964477539, 'logits/chosen': -1.1375057697296143, 'epoch': 0.47}

  8%|▊         | 849/10740 [4:18:48<48:42:34, 17.73s/it]

  8%|▊         | 850/10740 [4:19:11<52:17:31, 19.03s/it]


  8%|▊         | 852/10740 [4:19:46<49:26:10, 18.00s/it]
{'loss': 0.3986, 'learning_rate': 1.987300856718635e-06, 'rewards/chosen': -1.5917366743087769, 'rewards/rejected': -3.043362855911255, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4516260623931885, 'policy_logps/rejected': -540.6611328125, 'policy_logps/chosen': -319.58721923828125, 'referece_logps/rejected': -510.22747802734375, 'referece_logps/chosen': -303.66986083984375, 'logits/rejected': -0.9510691165924072, 'logits/chosen': -0.5195581912994385, 'epoch': 0.48}


  8%|▊         | 854/10740 [4:20:18<48:03:12, 17.50s/it]
{'loss': 0.4935, 'learning_rate': 1.9872048571165225e-06, 'rewards/chosen': -0.8445113897323608, 'rewards/rejected': -1.884299635887146, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0397882461547852, 'policy_logps/rejected': -304.4363708496094, 'policy_logps/chosen': -420.46588134765625, 'referece_logps/rejected': -285.5933837890625, 'referece_logps/chosen': -412.0207824707031, 'logits/rejected': -0.45592647790908813, 'logits/chosen': -0.28415775299072266, 'epoch': 0.48}

  8%|▊         | 855/10740 [4:20:37<48:40:22, 17.73s/it]

  8%|▊         | 856/10740 [4:20:59<52:40:17, 19.18s/it]

  8%|▊         | 857/10740 [4:21:18<52:18:01, 19.05s/it]

  8%|▊         | 858/10740 [4:21:34<49:56:13, 18.19s/it]


  8%|▊         | 860/10740 [4:22:08<47:57:56, 17.48s/it]
{'loss': 0.4008, 'learning_rate': 1.9869147035217215e-06, 'rewards/chosen': -1.6912994384765625, 'rewards/rejected': -2.7637479305267334, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0724481344223022, 'policy_logps/rejected': -465.2022705078125, 'policy_logps/chosen': -519.7714233398438, 'referece_logps/rejected': -437.5648193359375, 'referece_logps/chosen': -502.8583984375, 'logits/rejected': -0.057705968618392944, 'logits/chosen': -0.10784268379211426, 'epoch': 0.48}

  8%|▊         | 861/10740 [4:22:25<47:10:58, 17.19s/it]
[2024-04-01 23:36:01,889] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 862/10740 [4:22:44<48:23:43, 17.64s/it]

  8%|▊         | 863/10740 [4:23:00<47:13:04, 17.21s/it]

  8%|▊         | 864/10740 [4:23:18<48:01:37, 17.51s/it]

  8%|▊         | 865/10740 [4:23:33<46:07:37, 16.82s/it]

  8%|▊         | 866/10740 [4:23:57<52:01:15, 18.97s/it]

  8%|▊         | 867/10740 [4:24:14<50:25:04, 18.38s/it]

  8%|▊         | 868/10740 [4:24:34<51:38:50, 18.83s/it]

  8%|▊         | 869/10740 [4:24:56<54:03:36, 19.72s/it]

  8%|▊         | 870/10740 [4:25:17<55:11:36, 20.13s/it]

  8%|▊         | 871/10740 [4:25:31<50:03:55, 18.26s/it]

  8%|▊         | 872/10740 [4:25:46<47:01:40, 17.16s/it]

  8%|▊         | 873/10740 [4:26:05<49:09:52, 17.94s/it]
[2024-04-01 23:39:44,311] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 874/10740 [4:26:26<51:30:41, 18.80s/it]

  8%|▊         | 875/10740 [4:26:49<54:54:55, 20.04s/it]

  8%|▊         | 876/10740 [4:27:09<55:11:22, 20.14s/it]

  8%|▊         | 877/10740 [4:27:31<56:33:46, 20.65s/it]
[2024-04-01 23:41:12,403] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 878/10740 [4:27:54<58:27:02, 21.34s/it]

  8%|▊         | 879/10740 [4:28:15<58:24:18, 21.32s/it]

  8%|▊         | 880/10740 [4:28:27<50:36:38, 18.48s/it]

  8%|▊         | 881/10740 [4:28:42<47:47:22, 17.45s/it]

  8%|▊         | 882/10740 [4:28:55<44:06:32, 16.11s/it]

  8%|▊         | 883/10740 [4:29:18<49:05:16, 17.93s/it]

  8%|▊         | 884/10740 [4:29:30<44:21:36, 16.20s/it]

  8%|▊         | 885/10740 [4:29:48<46:03:46, 16.83s/it]

  8%|▊         | 886/10740 [4:30:09<49:50:26, 18.21s/it]

  8%|▊         | 887/10740 [4:30:20<43:38:55, 15.95s/it]

  8%|▊         | 888/10740 [4:30:40<46:32:13, 17.01s/it]

  8%|▊         | 889/10740 [4:30:58<47:24:53, 17.33s/it]

  8%|▊         | 890/10740 [4:31:09<42:42:06, 15.61s/it]

  8%|▊         | 891/10740 [4:31:31<47:37:10, 17.41s/it]

  8%|▊         | 892/10740 [4:31:44<44:27:32, 16.25s/it]

  8%|▊         | 893/10740 [4:32:03<46:19:14, 16.93s/it]

  8%|▊         | 894/10740 [4:32:22<48:10:11, 17.61s/it]

  8%|▊         | 895/10740 [4:32:38<46:56:40, 17.17s/it]

  8%|▊         | 896/10740 [4:32:58<49:03:18, 17.94s/it]


  8%|▊         | 898/10740 [4:33:31<46:44:28, 17.10s/it]
{'loss': 0.4101, 'learning_rate': 1.9850020629459953e-06, 'rewards/chosen': -1.7720824480056763, 'rewards/rejected': -3.3033361434936523, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5312540531158447, 'policy_logps/rejected': -330.0111083984375, 'policy_logps/chosen': -403.0882263183594, 'referece_logps/rejected': -296.9777526855469, 'referece_logps/chosen': -385.3674011230469, 'logits/rejected': -1.1986145973205566, 'logits/chosen': -1.1082489490509033, 'epoch': 0.5}
[2024-04-01 23:47:11,694] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 899/10740 [4:33:53<50:56:30, 18.64s/it]
[2024-04-01 23:47:31,933] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 900/10740 [4:34:14<52:15:07, 19.12s/it]

  8%|▊         | 901/10740 [4:34:35<54:21:01, 19.89s/it]

  8%|▊         | 902/10740 [4:34:50<49:52:58, 18.25s/it]

  8%|▊         | 903/10740 [4:35:10<51:34:22, 18.87s/it]
[2024-04-01 23:48:47,811] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 904/10740 [4:35:30<52:01:26, 19.04s/it]

  8%|▊         | 905/10740 [4:35:54<56:15:17, 20.59s/it]

  8%|▊         | 906/10740 [4:36:13<54:54:37, 20.10s/it]

  8%|▊         | 907/10740 [4:36:29<51:26:12, 18.83s/it]

  8%|▊         | 908/10740 [4:36:43<47:58:57, 17.57s/it]

  8%|▊         | 909/10740 [4:36:56<44:11:16, 16.18s/it]
[2024-04-01 23:50:32,482] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 910/10740 [4:37:14<45:43:46, 16.75s/it]

  8%|▊         | 911/10740 [4:37:35<49:01:52, 17.96s/it]

  8%|▊         | 912/10740 [4:37:56<51:14:49, 18.77s/it]

  9%|▊         | 913/10740 [4:38:18<53:50:44, 19.73s/it]

  9%|▊         | 914/10740 [4:38:39<55:11:12, 20.22s/it]


  9%|▊         | 916/10740 [4:39:08<46:04:49, 16.89s/it]
{'loss': 0.4031, 'learning_rate': 1.9840509072433776e-06, 'rewards/chosen': -1.3965448141098022, 'rewards/rejected': -2.297771692276001, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9012269377708435, 'policy_logps/rejected': -315.7545471191406, 'policy_logps/chosen': -344.68890380859375, 'referece_logps/rejected': -292.7768249511719, 'referece_logps/chosen': -330.72344970703125, 'logits/rejected': -0.42489659786224365, 'logits/chosen': -0.27337902784347534, 'epoch': 0.51}

  9%|▊         | 917/10740 [4:39:26<47:28:58, 17.40s/it]

  9%|▊         | 918/10740 [4:39:48<51:23:22, 18.84s/it]


  9%|▊         | 920/10740 [4:40:22<47:39:13, 17.47s/it]

  9%|▊         | 921/10740 [4:40:42<49:23:52, 18.11s/it]

  9%|▊         | 922/10740 [4:41:03<51:55:25, 19.04s/it]
[2024-04-01 23:54:21,163] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▊         | 923/10740 [4:41:23<52:52:34, 19.39s/it]

  9%|▊         | 924/10740 [4:41:43<53:38:08, 19.67s/it]

  9%|▊         | 925/10740 [4:42:01<52:04:14, 19.10s/it]

  9%|▊         | 926/10740 [4:42:22<53:47:01, 19.73s/it]

  9%|▊         | 927/10740 [4:42:34<46:51:56, 17.19s/it]

  9%|▊         | 928/10740 [4:42:52<47:58:10, 17.60s/it]
[2024-04-01 23:56:10,486] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▊         | 929/10740 [4:43:12<49:26:18, 18.14s/it]

  9%|▊         | 930/10740 [4:43:31<50:02:27, 18.36s/it]

  9%|▊         | 931/10740 [4:43:50<51:11:01, 18.78s/it]

  9%|▊         | 932/10740 [4:44:10<51:39:40, 18.96s/it]

  9%|▊         | 933/10740 [4:44:20<44:52:56, 16.48s/it]

  9%|▊         | 934/10740 [4:44:34<42:52:33, 15.74s/it]

  9%|▊         | 935/10740 [4:44:49<42:20:08, 15.54s/it]

  9%|▊         | 936/10740 [4:45:01<38:59:29, 14.32s/it]

  9%|▊         | 937/10740 [4:45:12<36:33:24, 13.42s/it]

  9%|▊         | 938/10740 [4:45:26<37:10:31, 13.65s/it]

  9%|▊         | 939/10740 [4:45:42<38:53:47, 14.29s/it]

  9%|▉         | 940/10740 [4:45:55<37:35:00, 13.81s/it]

  9%|▉         | 941/10740 [4:46:14<41:41:38, 15.32s/it]

  9%|▉         | 942/10740 [4:46:32<44:05:37, 16.20s/it]

  9%|▉         | 943/10740 [4:46:50<45:56:57, 16.88s/it]

  9%|▉         | 944/10740 [4:47:12<49:42:54, 18.27s/it]

  9%|▉         | 945/10740 [4:47:33<52:13:13, 19.19s/it]

  9%|▉         | 946/10740 [4:47:48<48:34:44, 17.86s/it]

  9%|▉         | 947/10740 [4:47:59<43:05:34, 15.84s/it]

  9%|▉         | 948/10740 [4:48:20<46:58:21, 17.27s/it]

  9%|▉         | 949/10740 [4:48:41<50:24:17, 18.53s/it]

  9%|▉         | 950/10740 [4:49:01<51:01:15, 18.76s/it]

  9%|▉         | 951/10740 [4:49:15<47:45:13, 17.56s/it]

  9%|▉         | 952/10740 [4:49:29<44:15:21, 16.28s/it]

  9%|▉         | 953/10740 [4:49:50<48:17:48, 17.77s/it]

  9%|▉         | 954/10740 [4:50:05<46:01:15, 16.93s/it]

  9%|▉         | 955/10740 [4:50:25<48:24:28, 17.81s/it]

  9%|▉         | 956/10740 [4:50:44<49:51:35, 18.35s/it]

  9%|▉         | 957/10740 [4:51:01<48:11:26, 17.73s/it]

  9%|▉         | 958/10740 [4:51:17<47:11:49, 17.37s/it]

  9%|▉         | 959/10740 [4:51:36<48:46:15, 17.95s/it]
[2024-04-02 00:04:54,678] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 960/10740 [4:51:58<51:50:35, 19.08s/it]

  9%|▉         | 961/10740 [4:52:18<52:17:53, 19.25s/it]

  9%|▉         | 962/10740 [4:52:31<47:39:27, 17.55s/it]
{'loss': 0.426, 'learning_rate': 1.981488497934297e-06, 'rewards/chosen': -1.8079211711883545, 'rewards/rejected': -3.043473482131958, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2355520725250244, 'policy_logps/rejected': -470.30902099609375, 'policy_logps/chosen': -503.48004150390625, 'referece_logps/rejected': -439.874267578125, 'referece_logps/chosen': -485.40081787109375, 'logits/rejected': -0.6020786762237549, 'logits/chosen': -0.47439685463905334, 'epoch': 0.54}


  9%|▉         | 964/10740 [4:53:10<49:43:30, 18.31s/it]

  9%|▉         | 965/10740 [4:53:31<51:53:52, 19.11s/it]

  9%|▉         | 966/10740 [4:53:49<51:05:23, 18.82s/it]

  9%|▉         | 967/10740 [4:54:03<47:02:12, 17.33s/it]

  9%|▉         | 968/10740 [4:54:22<48:05:03, 17.71s/it]

  9%|▉         | 969/10740 [4:54:40<48:51:06, 18.00s/it]

  9%|▉         | 970/10740 [4:54:55<46:17:05, 17.05s/it]
{'loss': 0.5341, 'learning_rate': 1.9810235653665438e-06, 'rewards/chosen': -1.3418678045272827, 'rewards/rejected': -2.172731876373291, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8308639526367188, 'policy_logps/rejected': -245.31008911132812, 'policy_logps/chosen': -281.0311584472656, 'referece_logps/rejected': -223.58277893066406, 'referece_logps/chosen': -267.6124572753906, 'logits/rejected': -0.1760418564081192, 'logits/chosen': -0.0619928240776062, 'epoch': 0.54}


  9%|▉         | 972/10740 [4:55:32<48:52:44, 18.01s/it]
[2024-04-02 00:08:50,282] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 973/10740 [4:55:51<49:42:16, 18.32s/it]
{'loss': 0.4229, 'learning_rate': 1.9808477432706823e-06, 'rewards/chosen': -0.9938610792160034, 'rewards/rejected': -2.7676005363464355, 'rewards/accuracies': 1.0, 'rewards/margins': 1.773739218711853, 'policy_logps/rejected': -331.71697998046875, 'policy_logps/chosen': -415.4713134765625, 'referece_logps/rejected': -304.0409851074219, 'referece_logps/chosen': -405.53265380859375, 'logits/rejected': -0.7681208252906799, 'logits/chosen': -0.8066099882125854, 'epoch': 0.54}

  9%|▉         | 974/10740 [4:56:13<52:30:12, 19.35s/it]
[2024-04-02 00:09:45,124] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  9%|▉         | 976/10740 [4:56:44<47:55:18, 17.67s/it]

  9%|▉         | 977/10740 [4:56:58<44:53:19, 16.55s/it]

  9%|▉         | 978/10740 [4:57:11<41:27:28, 15.29s/it]

  9%|▉         | 979/10740 [4:57:29<44:14:13, 16.32s/it]

  9%|▉         | 980/10740 [4:57:51<48:16:37, 17.81s/it]
[2024-04-02 00:11:08,865] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 981/10740 [4:58:12<51:29:30, 18.99s/it]
[2024-04-02 00:11:30,632] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 982/10740 [4:58:32<51:53:37, 19.15s/it]

  9%|▉         | 983/10740 [4:58:54<54:34:46, 20.14s/it]
[2024-04-02 00:12:12,583] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4497, 'learning_rate': 1.9802558717485413e-06, 'rewards/chosen': -2.150214910507202, 'rewards/rejected': -3.6538686752319336, 'rewards/accuracies': 0.875, 'rewards/margins': 1.503653645515442, 'policy_logps/rejected': -502.90313720703125, 'policy_logps/chosen': -466.7284240722656, 'referece_logps/rejected': -466.3644104003906, 'referece_logps/chosen': -445.22625732421875, 'logits/rejected': 0.5304757952690125, 'logits/chosen': 0.5633012056350708, 'epoch': 0.55}

  9%|▉         | 984/10740 [4:59:09<50:11:15, 18.52s/it]


  9%|▉         | 986/10740 [4:59:40<45:55:21, 16.95s/it]

  9%|▉         | 987/10740 [4:59:53<42:41:14, 15.76s/it]
{'loss': 0.3858, 'learning_rate': 1.9800166264504577e-06, 'rewards/chosen': -1.508851170539856, 'rewards/rejected': -2.6905460357666016, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1816949844360352, 'policy_logps/rejected': -346.1671142578125, 'policy_logps/chosen': -428.96441650390625, 'referece_logps/rejected': -319.26165771484375, 'referece_logps/chosen': -413.8758850097656, 'logits/rejected': -0.20600105822086334, 'logits/chosen': -0.1533341407775879, 'epoch': 0.55}


  9%|▉         | 989/10740 [5:00:18<37:56:18, 14.01s/it]

  9%|▉         | 990/10740 [5:00:32<37:50:40, 13.97s/it]

  9%|▉         | 991/10740 [5:00:42<35:01:44, 12.94s/it]

  9%|▉         | 992/10740 [5:00:55<34:39:28, 12.80s/it]
{'loss': 0.4977, 'learning_rate': 1.979715564333478e-06, 'rewards/chosen': -1.4884556531906128, 'rewards/rejected': -1.7291313409805298, 'rewards/accuracies': 0.625, 'rewards/margins': 0.240675687789917, 'policy_logps/rejected': -302.0968933105469, 'policy_logps/chosen': -356.22711181640625, 'referece_logps/rejected': -284.8055419921875, 'referece_logps/chosen': -341.342529296875, 'logits/rejected': -0.16234861314296722, 'logits/chosen': -0.21499790251255035, 'epoch': 0.55}


  9%|▉         | 994/10740 [5:01:34<44:11:43, 16.33s/it]

  9%|▉         | 995/10740 [5:01:54<46:48:38, 17.29s/it]

  9%|▉         | 996/10740 [5:02:14<49:03:23, 18.12s/it]

  9%|▉         | 997/10740 [5:02:36<52:22:03, 19.35s/it]

  9%|▉         | 998/10740 [5:02:52<49:44:02, 18.38s/it]

  9%|▉         | 999/10740 [5:03:14<52:05:41, 19.25s/it]
{'loss': 0.4111, 'learning_rate': 1.9792903350082967e-06, 'rewards/chosen': -1.3553457260131836, 'rewards/rejected': -2.3707213401794434, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0153756141662598, 'policy_logps/rejected': -422.0444030761719, 'policy_logps/chosen': -370.413330078125, 'referece_logps/rejected': -398.337158203125, 'referece_logps/chosen': -356.85986328125, 'logits/rejected': -0.4641852080821991, 'logits/chosen': -0.48932451009750366, 'epoch': 0.56}


  9%|▉         | 1001/10740 [5:04:06<62:40:51, 23.17s/it]

  9%|▉         | 1002/10740 [5:04:22<56:52:53, 21.03s/it]

  9%|▉         | 1003/10740 [5:04:40<54:16:07, 20.06s/it]

  9%|▉         | 1004/10740 [5:04:57<51:48:32, 19.16s/it]

  9%|▉         | 1005/10740 [5:05:18<53:28:34, 19.78s/it]

  9%|▉         | 1006/10740 [5:05:41<55:42:46, 20.60s/it]

  9%|▉         | 1007/10740 [5:06:02<56:07:30, 20.76s/it]
{'loss': 0.4122, 'learning_rate': 1.9787990146185416e-06, 'rewards/chosen': -1.0601747035980225, 'rewards/rejected': -3.027242660522461, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9670679569244385, 'policy_logps/rejected': -218.9136505126953, 'policy_logps/chosen': -432.5367431640625, 'referece_logps/rejected': -188.64125061035156, 'referece_logps/chosen': -421.93499755859375, 'logits/rejected': -0.8369037508964539, 'logits/chosen': -0.7704410552978516, 'epoch': 0.56}


  9%|▉         | 1009/10740 [5:06:30<46:51:02, 17.33s/it]

  9%|▉         | 1010/10740 [5:06:52<50:34:33, 18.71s/it]
[2024-04-02 00:20:10,484] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 1011/10740 [5:07:13<52:24:32, 19.39s/it]
[2024-04-02 00:20:31,465] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 1012/10740 [5:07:30<50:16:34, 18.61s/it]

  9%|▉         | 1013/10740 [5:07:49<50:26:30, 18.67s/it]

  9%|▉         | 1014/10740 [5:08:09<51:28:21, 19.05s/it]
{'loss': 0.385, 'learning_rate': 1.9783644354122037e-06, 'rewards/chosen': -1.116376519203186, 'rewards/rejected': -3.5049703121185303, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3885934352874756, 'policy_logps/rejected': -343.5245361328125, 'policy_logps/chosen': -454.390625, 'referece_logps/rejected': -308.474853515625, 'referece_logps/chosen': -443.22686767578125, 'logits/rejected': -0.9972314238548279, 'logits/chosen': -0.9114092588424683, 'epoch': 0.57}


  9%|▉         | 1016/10740 [5:08:45<49:48:22, 18.44s/it]

  9%|▉         | 1017/10740 [5:09:04<50:40:34, 18.76s/it]

  9%|▉         | 1018/10740 [5:09:24<51:46:00, 19.17s/it]

  9%|▉         | 1019/10740 [5:09:40<48:42:36, 18.04s/it]

  9%|▉         | 1020/10740 [5:09:53<44:31:18, 16.49s/it]

 10%|▉         | 1021/10740 [5:10:13<47:34:29, 17.62s/it]

 10%|▉         | 1022/10740 [5:10:35<51:09:55, 18.95s/it]

 10%|▉         | 1023/10740 [5:10:54<51:17:17, 19.00s/it]

 10%|▉         | 1024/10740 [5:11:15<52:46:36, 19.56s/it]

 10%|▉         | 1025/10740 [5:11:30<49:22:28, 18.30s/it]

 10%|▉         | 1026/10740 [5:11:46<47:22:08, 17.55s/it]

 10%|▉         | 1027/10740 [5:12:02<46:20:03, 17.17s/it]
{'loss': 0.3331, 'learning_rate': 1.9775457932259923e-06, 'rewards/chosen': -1.263617753982544, 'rewards/rejected': -2.4180476665496826, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1544299125671387, 'policy_logps/rejected': -248.6372528076172, 'policy_logps/chosen': -257.1165771484375, 'referece_logps/rejected': -224.45675659179688, 'referece_logps/chosen': -244.4803924560547, 'logits/rejected': -0.5796992778778076, 'logits/chosen': -0.31808412075042725, 'epoch': 0.57}


 10%|▉         | 1029/10740 [5:12:33<42:37:00, 15.80s/it]

 10%|▉         | 1030/10740 [5:12:50<43:22:25, 16.08s/it]

 10%|▉         | 1031/10740 [5:13:13<49:17:56, 18.28s/it]
[2024-04-02 00:26:31,256] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4116, 'learning_rate': 1.9772908797896335e-06, 'rewards/chosen': -1.3328105211257935, 'rewards/rejected': -2.0575976371765137, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7247868776321411, 'policy_logps/rejected': -342.1419677734375, 'policy_logps/chosen': -292.35662841796875, 'referece_logps/rejected': -321.56597900390625, 'referece_logps/chosen': -279.0285339355469, 'logits/rejected': -0.8078023791313171, 'logits/chosen': -0.772733747959137, 'epoch': 0.58}

 10%|▉         | 1032/10740 [5:13:28<46:42:31, 17.32s/it]


 10%|▉         | 1034/10740 [5:14:07<49:00:54, 18.18s/it]

 10%|▉         | 1035/10740 [5:14:27<50:44:34, 18.82s/it]

 10%|▉         | 1036/10740 [5:14:46<50:30:00, 18.73s/it]

 10%|▉         | 1037/10740 [5:15:05<51:09:23, 18.98s/it]

 10%|▉         | 1038/10740 [5:15:25<51:51:04, 19.24s/it]

 10%|▉         | 1039/10740 [5:15:47<53:50:09, 19.98s/it]

 10%|▉         | 1040/10740 [5:16:08<54:38:23, 20.28s/it]

 10%|▉         | 1041/10740 [5:16:23<50:33:30, 18.77s/it]

 10%|▉         | 1042/10740 [5:16:40<48:41:35, 18.08s/it]

 10%|▉         | 1043/10740 [5:16:56<47:00:06, 17.45s/it]

 10%|▉         | 1044/10740 [5:17:15<48:34:56, 18.04s/it]

 10%|▉         | 1045/10740 [5:17:37<51:22:35, 19.08s/it]

 10%|▉         | 1046/10740 [5:17:55<50:49:03, 18.87s/it]

 10%|▉         | 1047/10740 [5:18:14<50:42:53, 18.84s/it]
{'loss': 0.3814, 'learning_rate': 1.976257007866096e-06, 'rewards/chosen': -1.5342227220535278, 'rewards/rejected': -2.417611837387085, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8833889365196228, 'policy_logps/rejected': -302.0144348144531, 'policy_logps/chosen': -325.4406433105469, 'referece_logps/rejected': -277.83831787109375, 'referece_logps/chosen': -310.0984191894531, 'logits/rejected': -0.8552113175392151, 'logits/chosen': -0.7867591977119446, 'epoch': 0.58}


 10%|▉         | 1049/10740 [5:18:51<50:05:32, 18.61s/it]

 10%|▉         | 1050/10740 [5:19:03<45:26:14, 16.88s/it]

 10%|▉         | 1051/10740 [5:19:23<47:40:20, 17.71s/it]

 10%|▉         | 1052/10740 [5:19:39<46:35:10, 17.31s/it]

 10%|▉         | 1053/10740 [5:19:59<48:21:31, 17.97s/it]

 10%|▉         | 1054/10740 [5:20:17<48:20:20, 17.97s/it]

 10%|▉         | 1055/10740 [5:20:37<50:02:39, 18.60s/it]

 10%|▉         | 1056/10740 [5:20:53<47:44:34, 17.75s/it]

 10%|▉         | 1057/10740 [5:21:07<45:04:54, 16.76s/it]

 10%|▉         | 1058/10740 [5:21:29<49:07:25, 18.27s/it]

 10%|▉         | 1059/10740 [5:21:44<46:41:14, 17.36s/it]
{'loss': 0.3927, 'learning_rate': 1.9754666853971057e-06, 'rewards/chosen': -2.1640076637268066, 'rewards/rejected': -3.270366907119751, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1063588857650757, 'policy_logps/rejected': -303.5259094238281, 'policy_logps/chosen': -354.5701599121094, 'referece_logps/rejected': -270.822265625, 'referece_logps/chosen': -332.9300537109375, 'logits/rejected': 0.4979098439216614, 'logits/chosen': 0.42420873045921326, 'epoch': 0.59}


 10%|▉         | 1061/10740 [5:22:29<53:39:35, 19.96s/it]

 10%|▉         | 1062/10740 [5:22:52<56:15:59, 20.93s/it]

 10%|▉         | 1063/10740 [5:23:13<56:22:14, 20.97s/it]
{'loss': 0.4819, 'learning_rate': 1.9752004049864126e-06, 'rewards/chosen': -1.390404224395752, 'rewards/rejected': -2.5320777893066406, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1416735649108887, 'policy_logps/rejected': -261.3602600097656, 'policy_logps/chosen': -315.65966796875, 'referece_logps/rejected': -236.03952026367188, 'referece_logps/chosen': -301.7556457519531, 'logits/rejected': -0.3947339653968811, 'logits/chosen': -0.3889588713645935, 'epoch': 0.59}


 10%|▉         | 1065/10740 [5:23:45<49:43:55, 18.50s/it]
{'loss': 0.4362, 'learning_rate': 1.9750667325756165e-06, 'rewards/chosen': -1.2547119855880737, 'rewards/rejected': -2.1297903060913086, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8750783205032349, 'policy_logps/rejected': -293.7406311035156, 'policy_logps/chosen': -269.8798522949219, 'referece_logps/rejected': -272.4427185058594, 'referece_logps/chosen': -257.332763671875, 'logits/rejected': -1.2999662160873413, 'logits/chosen': -1.3283659219741821, 'epoch': 0.59}

 10%|▉         | 1066/10740 [5:24:07<52:03:41, 19.37s/it]

 10%|▉         | 1067/10740 [5:24:23<49:39:37, 18.48s/it]

 10%|▉         | 1068/10740 [5:24:37<46:02:05, 17.13s/it]


 10%|▉         | 1070/10740 [5:25:10<44:19:57, 16.50s/it]
{'loss': 0.3741, 'learning_rate': 1.9747309996729243e-06, 'rewards/chosen': -1.8450419902801514, 'rewards/rejected': -2.7233283519744873, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8782860636711121, 'policy_logps/rejected': -245.2760467529297, 'policy_logps/chosen': -364.3787841796875, 'referece_logps/rejected': -218.04275512695312, 'referece_logps/chosen': -345.9283752441406, 'logits/rejected': -1.0925846099853516, 'logits/chosen': -0.9609901905059814, 'epoch': 0.6}
[2024-04-02 00:38:49,511] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 10%|▉         | 1072/10740 [5:25:54<52:07:56, 19.41s/it]
[2024-04-02 00:39:12,459] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|▉         | 1073/10740 [5:26:10<49:32:23, 18.45s/it]
[2024-04-02 00:39:28,660] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|█         | 1074/10740 [5:26:32<51:42:42, 19.26s/it]
{'loss': 0.3897, 'learning_rate': 1.9744608175391013e-06, 'rewards/chosen': -1.4041963815689087, 'rewards/rejected': -2.81095027923584, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4067537784576416, 'policy_logps/rejected': -358.6646728515625, 'policy_logps/chosen': -535.024658203125, 'referece_logps/rejected': -330.55517578125, 'referece_logps/chosen': -520.9827270507812, 'logits/rejected': -0.23088029026985168, 'logits/chosen': -0.5112544894218445, 'epoch': 0.6}


 10%|█         | 1076/10740 [5:27:10<51:04:09, 19.02s/it]

 10%|█         | 1077/10740 [5:27:28<50:23:25, 18.77s/it]

 10%|█         | 1078/10740 [5:27:49<51:41:31, 19.26s/it]

 10%|█         | 1079/10740 [5:28:02<46:42:59, 17.41s/it]

 10%|█         | 1080/10740 [5:28:16<44:36:20, 16.62s/it]

 10%|█         | 1081/10740 [5:28:40<50:06:18, 18.67s/it]
[2024-04-02 00:41:58,117] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|█         | 1082/10740 [5:28:59<50:12:33, 18.72s/it]

 10%|█         | 1083/10740 [5:29:22<54:03:32, 20.15s/it]
[2024-04-02 00:42:40,429] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|█         | 1084/10740 [5:29:36<49:00:06, 18.27s/it]
[2024-04-02 00:42:54,307] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3694, 'learning_rate': 1.9737791589983266e-06, 'rewards/chosen': -1.8320508003234863, 'rewards/rejected': -3.2575645446777344, 'rewards/accuracies': 0.625, 'rewards/margins': 1.425513505935669, 'policy_logps/rejected': -367.4627685546875, 'policy_logps/chosen': -438.6872863769531, 'referece_logps/rejected': -334.8870849609375, 'referece_logps/chosen': -420.36676025390625, 'logits/rejected': -0.7791746854782104, 'logits/chosen': -0.6915143132209778, 'epoch': 0.61}


 10%|█         | 1086/10740 [5:30:18<52:32:56, 19.60s/it]
{'loss': 0.3963, 'learning_rate': 1.973641764279925e-06, 'rewards/chosen': -2.175412654876709, 'rewards/rejected': -3.7497873306274414, 'rewards/accuracies': 0.875, 'rewards/margins': 1.574374794960022, 'policy_logps/rejected': -491.7912902832031, 'policy_logps/chosen': -638.968017578125, 'referece_logps/rejected': -454.2933654785156, 'referece_logps/chosen': -617.2138671875, 'logits/rejected': -0.4787524938583374, 'logits/chosen': -0.5284255743026733, 'epoch': 0.61}


 10%|█         | 1088/10740 [5:30:58<53:09:32, 19.83s/it]

 10%|█         | 1089/10740 [5:31:18<53:34:51, 19.99s/it]

 10%|█         | 1090/10740 [5:31:34<50:17:13, 18.76s/it]

 10%|█         | 1091/10740 [5:31:55<51:55:36, 19.37s/it]

 10%|█         | 1092/10740 [5:32:15<52:10:25, 19.47s/it]
{'loss': 0.4542, 'learning_rate': 1.9732274550028034e-06, 'rewards/chosen': -1.4607751369476318, 'rewards/rejected': -2.566248655319214, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1054733991622925, 'policy_logps/rejected': -288.4705505371094, 'policy_logps/chosen': -506.3358154296875, 'referece_logps/rejected': -262.80804443359375, 'referece_logps/chosen': -491.72802734375, 'logits/rejected': -0.07253625243902206, 'logits/chosen': -0.10981647670269012, 'epoch': 0.61}


 10%|█         | 1094/10740 [5:32:53<52:08:26, 19.46s/it]

 10%|█         | 1095/10740 [5:33:09<49:07:15, 18.33s/it]
{'loss': 0.3672, 'learning_rate': 1.9730191052941455e-06, 'rewards/chosen': -1.685025930404663, 'rewards/rejected': -3.5836918354034424, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8986659049987793, 'policy_logps/rejected': -342.8798828125, 'policy_logps/chosen': -334.27630615234375, 'referece_logps/rejected': -307.0429992675781, 'referece_logps/chosen': -317.4260559082031, 'logits/rejected': 0.497852087020874, 'logits/chosen': 0.45288920402526855, 'epoch': 0.61}

 10%|█         | 1096/10740 [5:33:30<51:22:40, 19.18s/it]


 10%|█         | 1098/10740 [5:34:04<49:51:37, 18.62s/it]

 10%|█         | 1099/10740 [5:34:23<49:52:33, 18.62s/it]
{'loss': 0.4519, 'learning_rate': 1.972740066733614e-06, 'rewards/chosen': -1.5702167749404907, 'rewards/rejected': -3.197587251663208, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6273704767227173, 'policy_logps/rejected': -383.8566589355469, 'policy_logps/chosen': -549.3323364257812, 'referece_logps/rejected': -351.88079833984375, 'referece_logps/chosen': -533.6302490234375, 'logits/rejected': -0.5254276990890503, 'logits/chosen': -0.5926751494407654, 'epoch': 0.61}


 10%|█         | 1101/10740 [5:35:03<51:19:24, 19.17s/it]
{'loss': 0.4202, 'learning_rate': 1.9726000165893812e-06, 'rewards/chosen': -2.4449710845947266, 'rewards/rejected': -4.091835975646973, 'rewards/accuracies': 0.75, 'rewards/margins': 1.646864652633667, 'policy_logps/rejected': -430.90374755859375, 'policy_logps/chosen': -567.4259033203125, 'referece_logps/rejected': -389.9853820800781, 'referece_logps/chosen': -542.9761962890625, 'logits/rejected': -0.41128796339035034, 'logits/chosen': -0.5029953718185425, 'epoch': 0.62}


 10%|█         | 1103/10740 [5:35:40<50:37:23, 18.91s/it]

 10%|█         | 1104/10740 [5:35:59<50:12:51, 18.76s/it]
{'loss': 0.3585, 'learning_rate': 1.9723892779363288e-06, 'rewards/chosen': -2.3879990577697754, 'rewards/rejected': -3.3239190578460693, 'rewards/accuracies': 0.875, 'rewards/margins': 0.935920000076294, 'policy_logps/rejected': -407.5030212402344, 'policy_logps/chosen': -372.95245361328125, 'referece_logps/rejected': -374.2638244628906, 'referece_logps/chosen': -349.07244873046875, 'logits/rejected': -0.2806926667690277, 'logits/chosen': -0.36087000370025635, 'epoch': 0.62}

 10%|█         | 1105/10740 [5:36:11<45:20:46, 16.94s/it]

 10%|█         | 1106/10740 [5:36:25<42:57:13, 16.05s/it]


 10%|█         | 1108/10740 [5:37:01<44:33:37, 16.65s/it]

 10%|█         | 1109/10740 [5:37:21<47:02:29, 17.58s/it]

 10%|█         | 1110/10740 [5:37:41<49:03:11, 18.34s/it]
[2024-04-02 00:50:58,947] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4045, 'learning_rate': 1.971965412891408e-06, 'rewards/chosen': -1.1227176189422607, 'rewards/rejected': -2.909240245819092, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7865225076675415, 'policy_logps/rejected': -499.81085205078125, 'policy_logps/chosen': -324.6203918457031, 'referece_logps/rejected': -470.718505859375, 'referece_logps/chosen': -313.3932189941406, 'logits/rejected': -0.30409175157546997, 'logits/chosen': -0.037321314215660095, 'epoch': 0.62}
[2024-04-02 00:51:19,840] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|█         | 1111/10740 [5:38:02<51:05:55, 19.10s/it]


 10%|█         | 1113/10740 [5:38:33<46:28:54, 17.38s/it]

 10%|█         | 1114/10740 [5:38:49<45:05:05, 16.86s/it]

 10%|█         | 1115/10740 [5:39:09<47:48:37, 17.88s/it]

 10%|█         | 1116/10740 [5:39:31<50:34:39, 18.92s/it]

 10%|█         | 1117/10740 [5:39:49<50:25:40, 18.87s/it]
{'loss': 0.3899, 'learning_rate': 1.9714668814514018e-06, 'rewards/chosen': -1.6395328044891357, 'rewards/rejected': -2.7577853202819824, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1182522773742676, 'policy_logps/rejected': -457.3979187011719, 'policy_logps/chosen': -529.1997680664062, 'referece_logps/rejected': -429.82012939453125, 'referece_logps/chosen': -512.804443359375, 'logits/rejected': -0.5732724070549011, 'logits/chosen': -0.3858311176300049, 'epoch': 0.62}


 10%|█         | 1119/10740 [5:40:27<50:35:33, 18.93s/it]

 10%|█         | 1120/10740 [5:40:45<49:30:36, 18.53s/it]

 10%|█         | 1121/10740 [5:40:57<44:14:30, 16.56s/it]

 10%|█         | 1122/10740 [5:41:15<45:58:47, 17.21s/it]
{'loss': 0.4294, 'learning_rate': 1.9711081367174745e-06, 'rewards/chosen': -1.6508430242538452, 'rewards/rejected': -2.599372625350952, 'rewards/accuracies': 0.875, 'rewards/margins': 0.948529839515686, 'policy_logps/rejected': -302.217529296875, 'policy_logps/chosen': -423.6178894042969, 'referece_logps/rejected': -276.22381591796875, 'referece_logps/chosen': -407.1094970703125, 'logits/rejected': -0.3397170901298523, 'logits/chosen': -0.21080757677555084, 'epoch': 0.63}


 10%|█         | 1124/10740 [5:41:49<46:15:42, 17.32s/it]
{'loss': 0.3957, 'learning_rate': 1.9709640205055006e-06, 'rewards/chosen': -1.3668744564056396, 'rewards/rejected': -2.2479448318481445, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8810704946517944, 'policy_logps/rejected': -301.758544921875, 'policy_logps/chosen': -348.8194274902344, 'referece_logps/rejected': -279.2790832519531, 'referece_logps/chosen': -335.15069580078125, 'logits/rejected': -0.9751717448234558, 'logits/chosen': -0.9414384365081787, 'epoch': 0.63}

 10%|█         | 1125/10740 [5:42:08<47:55:31, 17.94s/it]

 10%|█         | 1126/10740 [5:42:28<49:32:20, 18.55s/it]


 11%|█         | 1128/10740 [5:43:07<50:58:54, 19.09s/it]

 11%|█         | 1129/10740 [5:43:21<47:09:22, 17.66s/it]

 11%|█         | 1130/10740 [5:43:42<49:17:30, 18.47s/it]
{'loss': 0.2699, 'learning_rate': 1.970529552602581e-06, 'rewards/chosen': -1.5217381715774536, 'rewards/rejected': -3.380866765975952, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8591283559799194, 'policy_logps/rejected': -277.12554931640625, 'policy_logps/chosen': -274.7130126953125, 'referece_logps/rejected': -243.31686401367188, 'referece_logps/chosen': -259.49566650390625, 'logits/rejected': -1.3099169731140137, 'logits/chosen': -1.224998950958252, 'epoch': 0.63}

 11%|█         | 1131/10740 [5:43:58<48:02:11, 18.00s/it]

 11%|█         | 1132/10740 [5:44:18<49:08:38, 18.41s/it]


 11%|█         | 1134/10740 [5:44:53<47:13:06, 17.70s/it]

 11%|█         | 1135/10740 [5:45:12<47:33:08, 17.82s/it]

 11%|█         | 1136/10740 [5:45:33<50:32:53, 18.95s/it]
{'loss': 0.3083, 'learning_rate': 1.9700919069069576e-06, 'rewards/chosen': -1.3566802740097046, 'rewards/rejected': -2.772143840789795, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4154634475708008, 'policy_logps/rejected': -449.9172668457031, 'policy_logps/chosen': -468.7827453613281, 'referece_logps/rejected': -422.1958312988281, 'referece_logps/chosen': -455.2159118652344, 'logits/rejected': -0.35175621509552, 'logits/chosen': -0.3478005826473236, 'epoch': 0.63}


 11%|█         | 1138/10740 [5:46:12<51:00:08, 19.12s/it]

 11%|█         | 1139/10740 [5:46:31<51:32:16, 19.32s/it]

 11%|█         | 1140/10740 [5:46:52<52:11:52, 19.57s/it]

 11%|█         | 1141/10740 [5:47:11<51:57:16, 19.49s/it]

 11%|█         | 1142/10740 [5:47:31<52:34:02, 19.72s/it]

 11%|█         | 1143/10740 [5:47:45<47:50:28, 17.95s/it]
{'loss': 0.3386, 'learning_rate': 1.969577305797496e-06, 'rewards/chosen': -1.5664414167404175, 'rewards/rejected': -2.876716375350952, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3102749586105347, 'policy_logps/rejected': -215.51644897460938, 'policy_logps/chosen': -294.77337646484375, 'referece_logps/rejected': -186.74929809570312, 'referece_logps/chosen': -279.10894775390625, 'logits/rejected': -0.13713636994361877, 'logits/chosen': -0.19951380789279938, 'epoch': 0.64}

 11%|█         | 1144/10740 [5:48:03<47:35:08, 17.85s/it]

 11%|█         | 1145/10740 [5:48:23<49:17:40, 18.50s/it]

 11%|█         | 1146/10740 [5:48:44<51:42:41, 19.40s/it]


 11%|█         | 1148/10740 [5:49:18<47:09:18, 17.70s/it]
{'loss': 0.3226, 'learning_rate': 1.9692070878799123e-06, 'rewards/chosen': -1.9247368574142456, 'rewards/rejected': -3.2323646545410156, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3076276779174805, 'policy_logps/rejected': -238.8066864013672, 'policy_logps/chosen': -224.33387756347656, 'referece_logps/rejected': -206.48301696777344, 'referece_logps/chosen': -205.08651733398438, 'logits/rejected': -0.700404942035675, 'logits/chosen': -0.3747023344039917, 'epoch': 0.64}


 11%|█         | 1150/10740 [5:49:59<51:24:13, 19.30s/it]

 11%|█         | 1151/10740 [5:50:19<52:04:11, 19.55s/it]

 11%|█         | 1152/10740 [5:50:42<54:16:39, 20.38s/it]
[2024-04-02 01:03:59,842] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 11%|█         | 1153/10740 [5:51:03<55:14:03, 20.74s/it]
{'loss': 0.5141, 'learning_rate': 1.9688346661686916e-06, 'rewards/chosen': -1.9607139825820923, 'rewards/rejected': -3.1129844188690186, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1522703170776367, 'policy_logps/rejected': -276.6298828125, 'policy_logps/chosen': -445.2418518066406, 'referece_logps/rejected': -245.5000457763672, 'referece_logps/chosen': -425.63470458984375, 'logits/rejected': -0.3169182240962982, 'logits/chosen': -0.4073026478290558, 'epoch': 0.64}

 11%|█         | 1154/10740 [5:51:25<55:48:34, 20.96s/it]

 11%|█         | 1155/10740 [5:51:45<55:02:56, 20.68s/it]

 11%|█         | 1156/10740 [5:52:02<52:42:26, 19.80s/it]


 11%|█         | 1158/10740 [5:52:40<50:34:53, 19.00s/it]

 11%|█         | 1159/10740 [5:52:56<48:22:25, 18.18s/it]

 11%|█         | 1160/10740 [5:53:12<46:32:56, 17.49s/it]
{'loss': 0.4964, 'learning_rate': 1.968309575012963e-06, 'rewards/chosen': -1.8335316181182861, 'rewards/rejected': -2.42185378074646, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5883224010467529, 'policy_logps/rejected': -500.29339599609375, 'policy_logps/chosen': -390.9549560546875, 'referece_logps/rejected': -476.0748291015625, 'referece_logps/chosen': -372.61962890625, 'logits/rejected': -0.952571451663971, 'logits/chosen': -0.8362690806388855, 'epoch': 0.65}

 11%|█         | 1161/10740 [5:53:33<49:46:36, 18.71s/it]


 11%|█         | 1163/10740 [5:54:00<42:46:42, 16.08s/it]

 11%|█         | 1164/10740 [5:54:16<43:03:26, 16.19s/it]
{'loss': 0.3243, 'learning_rate': 1.968007585230917e-06, 'rewards/chosen': -1.7393898963928223, 'rewards/rejected': -3.091750144958496, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3523602485656738, 'policy_logps/rejected': -474.207763671875, 'policy_logps/chosen': -441.0820007324219, 'referece_logps/rejected': -443.290283203125, 'referece_logps/chosen': -423.6881103515625, 'logits/rejected': -0.3613017797470093, 'logits/chosen': -0.4025762677192688, 'epoch': 0.65}

 11%|█         | 1165/10740 [5:54:32<42:32:49, 16.00s/it]

 11%|█         | 1166/10740 [5:54:43<38:25:40, 14.45s/it]


 11%|█         | 1168/10740 [5:55:22<45:35:19, 17.15s/it]

 11%|█         | 1169/10740 [5:55:40<46:38:01, 17.54s/it]

 11%|█         | 1170/10740 [5:55:58<46:40:08, 17.56s/it]
{'loss': 0.3868, 'learning_rate': 1.967551959416128e-06, 'rewards/chosen': -1.500125765800476, 'rewards/rejected': -2.0348904132843018, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5347646474838257, 'policy_logps/rejected': -323.52630615234375, 'policy_logps/chosen': -386.6795654296875, 'referece_logps/rejected': -303.1773986816406, 'referece_logps/chosen': -371.6783142089844, 'logits/rejected': -0.3097193241119385, 'logits/chosen': -0.2867097556591034, 'epoch': 0.65}


 11%|█         | 1172/10740 [5:56:36<48:08:09, 18.11s/it]
{'loss': 0.4368, 'learning_rate': 1.9673993800609596e-06, 'rewards/chosen': -2.181196928024292, 'rewards/rejected': -3.0030629634857178, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8218660950660706, 'policy_logps/rejected': -354.3692626953125, 'policy_logps/chosen': -345.99395751953125, 'referece_logps/rejected': -324.338623046875, 'referece_logps/chosen': -324.18194580078125, 'logits/rejected': -0.33769989013671875, 'logits/chosen': -0.31062790751457214, 'epoch': 0.65}


 11%|█         | 1174/10740 [5:57:14<49:03:19, 18.46s/it]

 11%|█         | 1175/10740 [5:57:26<43:52:00, 16.51s/it]
{'loss': 0.3867, 'learning_rate': 1.967169851140508e-06, 'rewards/chosen': -1.2703211307525635, 'rewards/rejected': -2.5729782581329346, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3026570081710815, 'policy_logps/rejected': -416.032470703125, 'policy_logps/chosen': -325.411376953125, 'referece_logps/rejected': -390.302734375, 'referece_logps/chosen': -312.70819091796875, 'logits/rejected': -1.3562144041061401, 'logits/chosen': -1.1508417129516602, 'epoch': 0.66}

 11%|█         | 1176/10740 [5:57:45<45:41:28, 17.20s/it]


 11%|█         | 1178/10740 [5:58:16<44:10:37, 16.63s/it]
{'loss': 0.4902, 'learning_rate': 1.9669395305218754e-06, 'rewards/chosen': -1.4506471157073975, 'rewards/rejected': -2.6180129051208496, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1673660278320312, 'policy_logps/rejected': -303.0749816894531, 'policy_logps/chosen': -315.5821838378906, 'referece_logps/rejected': -276.8948669433594, 'referece_logps/chosen': -301.07574462890625, 'logits/rejected': -0.9280545711517334, 'logits/chosen': -0.9356784820556641, 'epoch': 0.66}


 11%|█         | 1180/10740 [5:58:44<41:17:03, 15.55s/it]
{'loss': 0.3982, 'learning_rate': 1.966785543703541e-06, 'rewards/chosen': -1.7485133409500122, 'rewards/rejected': -2.7576348781585693, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0091215372085571, 'policy_logps/rejected': -278.0078430175781, 'policy_logps/chosen': -358.1075744628906, 'referece_logps/rejected': -250.4314727783203, 'referece_logps/chosen': -340.6224365234375, 'logits/rejected': -0.5979626774787903, 'logits/chosen': -0.661695122718811, 'epoch': 0.66}


 11%|█         | 1182/10740 [5:59:20<45:24:41, 17.10s/it]

 11%|█         | 1183/10740 [5:59:35<43:11:43, 16.27s/it]
{'loss': 0.3747, 'learning_rate': 1.966553904007225e-06, 'rewards/chosen': -1.5044795274734497, 'rewards/rejected': -3.1292922496795654, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6248127222061157, 'policy_logps/rejected': -278.4524841308594, 'policy_logps/chosen': -365.1035461425781, 'referece_logps/rejected': -247.15957641601562, 'referece_logps/chosen': -350.0587158203125, 'logits/rejected': -0.04263034462928772, 'logits/chosen': 0.07192362844944, 'epoch': 0.66}

 11%|█         | 1184/10740 [5:59:54<45:43:51, 17.23s/it]

 11%|█         | 1185/10740 [6:00:14<47:47:20, 18.01s/it]

 11%|█         | 1186/10740 [6:00:34<49:01:41, 18.47s/it]

 11%|█         | 1187/10740 [6:00:47<45:25:22, 17.12s/it]


 11%|█         | 1189/10740 [6:01:28<50:01:26, 18.86s/it]

 11%|█         | 1190/10740 [6:01:49<51:12:50, 19.31s/it]

 11%|█         | 1191/10740 [6:02:06<49:57:53, 18.84s/it]

 11%|█         | 1192/10740 [6:02:23<47:46:26, 18.01s/it]
{'loss': 0.3177, 'learning_rate': 1.965854238516068e-06, 'rewards/chosen': -0.8250744938850403, 'rewards/rejected': -2.877174139022827, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0520994663238525, 'policy_logps/rejected': -220.08506774902344, 'policy_logps/chosen': -332.4870300292969, 'referece_logps/rejected': -191.31332397460938, 'referece_logps/chosen': -324.23626708984375, 'logits/rejected': 0.41222772002220154, 'logits/chosen': 0.6499528884887695, 'epoch': 0.67}


 11%|█         | 1194/10740 [6:03:05<52:48:28, 19.92s/it]
{'loss': 0.3817, 'learning_rate': 1.965697790798961e-06, 'rewards/chosen': -2.2940337657928467, 'rewards/rejected': -3.6689059734344482, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3748722076416016, 'policy_logps/rejected': -345.7868957519531, 'policy_logps/chosen': -490.0007629394531, 'referece_logps/rejected': -309.09783935546875, 'referece_logps/chosen': -467.0604248046875, 'logits/rejected': -0.4094085693359375, 'logits/chosen': -0.5036723017692566, 'epoch': 0.67}

 11%|█         | 1195/10740 [6:03:21<50:08:54, 18.91s/it]

 11%|█         | 1196/10740 [6:03:40<49:45:38, 18.77s/it]

 11%|█         | 1197/10740 [6:03:56<47:40:09, 17.98s/it]

 11%|█         | 1198/10740 [6:04:15<48:46:00, 18.40s/it]

 11%|█         | 1199/10740 [6:04:30<45:47:20, 17.28s/it]

 11%|█         | 1200/10740 [6:04:50<47:42:55, 18.01s/it]


 11%|█         | 1202/10740 [6:05:29<49:26:37, 18.66s/it]
{'loss': 0.407, 'learning_rate': 1.96506848719841e-06, 'rewards/chosen': -1.155299186706543, 'rewards/rejected': -3.2322165966033936, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0769174098968506, 'policy_logps/rejected': -387.0411071777344, 'policy_logps/chosen': -373.24664306640625, 'referece_logps/rejected': -354.7189636230469, 'referece_logps/chosen': -361.69366455078125, 'logits/rejected': -0.306067556142807, 'logits/chosen': -0.21199354529380798, 'epoch': 0.67}

 11%|█         | 1203/10740 [6:05:47<49:10:54, 18.57s/it]

 11%|█         | 1204/10740 [6:06:10<52:44:26, 19.91s/it]

 11%|█         | 1205/10740 [6:06:32<54:32:43, 20.59s/it]

 11%|█         | 1206/10740 [6:06:53<54:59:46, 20.77s/it]

 11%|█         | 1207/10740 [6:07:08<49:38:51, 18.75s/it]

 11%|█         | 1208/10740 [6:07:30<52:15:51, 19.74s/it]

 11%|█▏        | 1209/10740 [6:07:44<48:18:57, 18.25s/it]


 11%|█▏        | 1211/10740 [6:08:23<49:25:43, 18.67s/it]
{'loss': 0.3933, 'learning_rate': 1.964353806020767e-06, 'rewards/chosen': -1.955759048461914, 'rewards/rejected': -3.5502305030822754, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5944712162017822, 'policy_logps/rejected': -422.9239807128906, 'policy_logps/chosen': -420.14312744140625, 'referece_logps/rejected': -387.4216613769531, 'referece_logps/chosen': -400.58551025390625, 'logits/rejected': -0.33736881613731384, 'logits/chosen': -0.17847205698490143, 'epoch': 0.68}

 11%|█▏        | 1212/10740 [6:08:38<46:58:38, 17.75s/it]

 11%|█▏        | 1213/10740 [6:08:56<47:07:47, 17.81s/it]

 11%|█▏        | 1214/10740 [6:09:18<50:06:45, 18.94s/it]


 11%|█▏        | 1216/10740 [6:09:59<52:25:02, 19.81s/it]

 11%|█▏        | 1217/10740 [6:10:17<51:01:24, 19.29s/it]
{'loss': 0.3436, 'learning_rate': 1.963873404612103e-06, 'rewards/chosen': -1.4597405195236206, 'rewards/rejected': -3.3113396167755127, 'rewards/accuracies': 1.0, 'rewards/margins': 1.851599097251892, 'policy_logps/rejected': -317.97259521484375, 'policy_logps/chosen': -393.72332763671875, 'referece_logps/rejected': -284.8592224121094, 'referece_logps/chosen': -379.1259460449219, 'logits/rejected': -0.19354258477687836, 'logits/chosen': -0.11165522038936615, 'epoch': 0.68}


 11%|█▏        | 1219/10740 [6:10:53<48:36:24, 18.38s/it]

 11%|█▏        | 1220/10740 [6:11:13<49:44:15, 18.81s/it]
{'loss': 0.5641, 'learning_rate': 1.9636320203095954e-06, 'rewards/chosen': -2.153740167617798, 'rewards/rejected': -3.387249708175659, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2335096597671509, 'policy_logps/rejected': -360.4898681640625, 'policy_logps/chosen': -430.0506896972656, 'referece_logps/rejected': -326.61737060546875, 'referece_logps/chosen': -408.5132751464844, 'logits/rejected': -0.1524534523487091, 'logits/chosen': -0.10271570086479187, 'epoch': 0.68}

 11%|█▏        | 1221/10740 [6:11:32<49:53:52, 18.87s/it]

 11%|█▏        | 1222/10740 [6:11:54<52:09:40, 19.73s/it]

 11%|█▏        | 1223/10740 [6:12:12<51:02:10, 19.31s/it]

 11%|█▏        | 1224/10740 [6:12:29<49:00:19, 18.54s/it]

 11%|█▏        | 1225/10740 [6:12:44<46:16:47, 17.51s/it]

 11%|█▏        | 1226/10740 [6:13:06<49:43:17, 18.81s/it]

 11%|█▏        | 1227/10740 [6:13:22<47:49:02, 18.10s/it]

 11%|█▏        | 1228/10740 [6:13:46<51:51:03, 19.62s/it]


 11%|█▏        | 1230/10740 [6:14:27<53:23:59, 20.21s/it]
{'loss': 0.3412, 'learning_rate': 1.962821710177801e-06, 'rewards/chosen': -2.7809696197509766, 'rewards/rejected': -4.307950019836426, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5269801616668701, 'policy_logps/rejected': -427.6648254394531, 'policy_logps/chosen': -443.81024169921875, 'referece_logps/rejected': -384.5853271484375, 'referece_logps/chosen': -416.00054931640625, 'logits/rejected': -1.3751670122146606, 'logits/chosen': -1.2654410600662231, 'epoch': 0.69}

 11%|█▏        | 1231/10740 [6:14:46<52:29:44, 19.87s/it]

 11%|█▏        | 1232/10740 [6:15:06<52:16:27, 19.79s/it]

 11%|█▏        | 1233/10740 [6:15:22<49:19:22, 18.68s/it]

 11%|█▏        | 1234/10740 [6:15:34<44:23:49, 16.81s/it]

 11%|█▏        | 1235/10740 [6:15:48<41:39:51, 15.78s/it]


 12%|█▏        | 1237/10740 [6:16:30<48:31:47, 18.38s/it]
{'loss': 0.4908, 'learning_rate': 1.9622492821734934e-06, 'rewards/chosen': -1.7157673835754395, 'rewards/rejected': -2.6989808082580566, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9832136631011963, 'policy_logps/rejected': -506.8575439453125, 'policy_logps/chosen': -456.1029968261719, 'referece_logps/rejected': -479.8677062988281, 'referece_logps/chosen': -438.94537353515625, 'logits/rejected': -0.4072129726409912, 'logits/chosen': -0.45760709047317505, 'epoch': 0.69}

 12%|█▏        | 1238/10740 [6:16:53<52:40:30, 19.96s/it]

 12%|█▏        | 1239/10740 [6:17:11<50:49:53, 19.26s/it]


 12%|█▏        | 1241/10740 [6:17:50<51:05:55, 19.37s/it]
{'loss': 0.3914, 'learning_rate': 1.961920254876524e-06, 'rewards/chosen': -1.8881092071533203, 'rewards/rejected': -2.708925724029541, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8208163976669312, 'policy_logps/rejected': -273.49444580078125, 'policy_logps/chosen': -361.5218505859375, 'referece_logps/rejected': -246.40518188476562, 'referece_logps/chosen': -342.6407775878906, 'logits/rejected': 0.39987578988075256, 'logits/chosen': 0.3759363889694214, 'epoch': 0.69}

 12%|█▏        | 1242/10740 [6:18:09<51:25:45, 19.49s/it]

 12%|█▏        | 1243/10740 [6:18:32<54:10:36, 20.54s/it]


 12%|█▏        | 1245/10740 [6:19:08<50:38:47, 19.20s/it]
{'loss': 0.3302, 'learning_rate': 1.961589827755615e-06, 'rewards/chosen': -2.0195765495300293, 'rewards/rejected': -4.046238899230957, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0266623497009277, 'policy_logps/rejected': -482.63818359375, 'policy_logps/chosen': -406.9740905761719, 'referece_logps/rejected': -442.1758117675781, 'referece_logps/chosen': -386.7783508300781, 'logits/rejected': -0.07447093725204468, 'logits/chosen': 0.10095399618148804, 'epoch': 0.7}

 12%|█▏        | 1246/10740 [6:19:25<48:59:50, 18.58s/it]

 12%|█▏        | 1247/10740 [6:19:43<48:44:17, 18.48s/it]

 12%|█▏        | 1248/10740 [6:20:05<51:33:31, 19.55s/it]

 12%|█▏        | 1249/10740 [6:20:22<49:45:21, 18.87s/it]

 12%|█▏        | 1250/10740 [6:20:41<49:19:51, 18.71s/it]

 12%|█▏        | 1251/10740 [6:21:03<51:43:28, 19.62s/it]

 12%|█▏        | 1252/10740 [6:21:22<51:57:14, 19.71s/it]


 12%|█▏        | 1254/10740 [6:21:58<48:57:08, 18.58s/it]
{'loss': 0.4479, 'learning_rate': 1.960841251121238e-06, 'rewards/chosen': -2.4690639972686768, 'rewards/rejected': -4.347029209136963, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8779652118682861, 'policy_logps/rejected': -398.3168029785156, 'policy_logps/chosen': -427.1736145019531, 'referece_logps/rejected': -354.846435546875, 'referece_logps/chosen': -402.4830017089844, 'logits/rejected': -0.4534054398536682, 'logits/chosen': -0.488429456949234, 'epoch': 0.7}

 12%|█▏        | 1255/10740 [6:22:20<51:55:36, 19.71s/it]


 12%|█▏        | 1257/10740 [6:22:54<48:01:35, 18.23s/it]

 12%|█▏        | 1258/10740 [6:23:14<49:37:37, 18.84s/it]
{'loss': 0.5416, 'learning_rate': 1.960506277903943e-06, 'rewards/chosen': -2.1770670413970947, 'rewards/rejected': -2.7964305877685547, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6193633675575256, 'policy_logps/rejected': -361.2322998046875, 'policy_logps/chosen': -240.69061279296875, 'referece_logps/rejected': -333.26800537109375, 'referece_logps/chosen': -218.91993713378906, 'logits/rejected': -1.0659573078155518, 'logits/chosen': -0.8537663817405701, 'epoch': 0.7}

 12%|█▏        | 1259/10740 [6:23:33<49:42:38, 18.88s/it]

 12%|█▏        | 1260/10740 [6:23:47<45:39:34, 17.34s/it]

 12%|█▏        | 1261/10740 [6:24:09<49:47:21, 18.91s/it]

 12%|█▏        | 1262/10740 [6:24:29<50:27:04, 19.16s/it]
{'loss': 0.4804, 'learning_rate': 1.9600855958308314e-06, 'rewards/chosen': -1.3419904708862305, 'rewards/rejected': -3.155027151107788, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8130367994308472, 'policy_logps/rejected': -468.7882385253906, 'policy_logps/chosen': -679.3560791015625, 'referece_logps/rejected': -437.23797607421875, 'referece_logps/chosen': -665.9360961914062, 'logits/rejected': -0.8382160663604736, 'logits/chosen': -0.9680439829826355, 'epoch': 0.71}
[2024-04-02 01:38:25,223] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1264/10740 [6:25:07<50:50:16, 19.31s/it]
[2024-04-02 01:38:39,675] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1265/10740 [6:25:21<46:59:35, 17.85s/it]

 12%|█▏        | 1266/10740 [6:25:34<42:44:22, 16.24s/it]

 12%|█▏        | 1267/10740 [6:25:47<40:12:10, 15.28s/it]

 12%|█▏        | 1268/10740 [6:26:06<43:33:49, 16.56s/it]

 12%|█▏        | 1269/10740 [6:26:28<47:44:23, 18.15s/it]

 12%|█▏        | 1270/10740 [6:26:49<49:22:57, 18.77s/it]

 12%|█▏        | 1271/10740 [6:27:08<50:03:11, 19.03s/it]

 12%|█▏        | 1272/10740 [6:27:28<50:29:25, 19.20s/it]

 12%|█▏        | 1273/10740 [6:27:47<50:51:05, 19.34s/it]

 12%|█▏        | 1274/10740 [6:28:04<48:35:27, 18.48s/it]


 12%|█▏        | 1276/10740 [6:28:34<44:02:10, 16.75s/it]

 12%|█▏        | 1277/10740 [6:28:50<43:35:45, 16.59s/it]

 12%|█▏        | 1278/10740 [6:29:06<43:31:52, 16.56s/it]

 12%|█▏        | 1279/10740 [6:29:18<39:43:59, 15.12s/it]

 12%|█▏        | 1280/10740 [6:29:34<40:09:16, 15.28s/it]

 12%|█▏        | 1281/10740 [6:29:49<40:21:36, 15.36s/it]

 12%|█▏        | 1282/10740 [6:30:04<39:43:05, 15.12s/it]

 12%|█▏        | 1283/10740 [6:30:17<38:04:57, 14.50s/it]

 12%|█▏        | 1284/10740 [6:30:32<38:21:14, 14.60s/it]
{'loss': 0.4069, 'learning_rate': 1.9582949034155364e-06, 'rewards/chosen': -1.7151846885681152, 'rewards/rejected': -2.4252045154571533, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7100200653076172, 'policy_logps/rejected': -450.8784484863281, 'policy_logps/chosen': -287.60955810546875, 'referece_logps/rejected': -426.6264343261719, 'referece_logps/chosen': -270.4576721191406, 'logits/rejected': -0.22887557744979858, 'logits/chosen': -0.1267865151166916, 'epoch': 0.72}


 12%|█▏        | 1286/10740 [6:31:07<42:23:21, 16.14s/it]

 12%|█▏        | 1287/10740 [6:31:21<40:15:51, 15.33s/it]

 12%|█▏        | 1288/10740 [6:31:41<44:16:04, 16.86s/it]

 12%|█▏        | 1289/10740 [6:31:52<39:22:10, 15.00s/it]

 12%|█▏        | 1290/10740 [6:32:12<43:38:11, 16.62s/it]

 12%|█▏        | 1291/10740 [6:32:33<46:42:42, 17.80s/it]

 12%|█▏        | 1292/10740 [6:32:48<44:18:24, 16.88s/it]
{'loss': 0.3928, 'learning_rate': 1.957602620486272e-06, 'rewards/chosen': -1.2070133686065674, 'rewards/rejected': -2.496403217315674, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2893898487091064, 'policy_logps/rejected': -279.38897705078125, 'policy_logps/chosen': -295.20294189453125, 'referece_logps/rejected': -254.42495727539062, 'referece_logps/chosen': -283.1327819824219, 'logits/rejected': -0.16006770730018616, 'logits/chosen': -0.2274288833141327, 'epoch': 0.72}


 12%|█▏        | 1294/10740 [6:33:28<48:07:30, 18.34s/it]

 12%|█▏        | 1295/10740 [6:33:46<48:24:16, 18.45s/it]

 12%|█▏        | 1296/10740 [6:34:09<51:35:30, 19.67s/it]

 12%|█▏        | 1297/10740 [6:34:30<52:51:42, 20.15s/it]

 12%|█▏        | 1298/10740 [6:34:54<55:39:02, 21.22s/it]
[2024-04-02 01:48:12,026] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1299/10740 [6:35:12<53:39:06, 20.46s/it]
[2024-04-02 01:48:30,711] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1300/10740 [6:35:26<48:11:18, 18.38s/it]

 12%|█▏        | 1301/10740 [6:35:45<48:19:45, 18.43s/it]

 12%|█▏        | 1302/10740 [6:36:06<50:38:39, 19.32s/it]

 12%|█▏        | 1303/10740 [6:36:29<53:13:20, 20.30s/it]
[2024-04-02 01:49:46,779] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 1304/10740 [6:36:43<48:28:16, 18.49s/it]

 12%|█▏        | 1305/10740 [6:36:57<45:25:57, 17.34s/it]

 12%|█▏        | 1306/10740 [6:37:14<44:36:12, 17.02s/it]

 12%|█▏        | 1307/10740 [6:37:28<42:17:20, 16.14s/it]

 12%|█▏        | 1308/10740 [6:37:48<45:13:21, 17.26s/it]
{'loss': 0.4777, 'learning_rate': 1.9562013362070214e-06, 'rewards/chosen': -1.491916298866272, 'rewards/rejected': -3.3496909141540527, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8577743768692017, 'policy_logps/rejected': -408.70477294921875, 'policy_logps/chosen': -556.731689453125, 'referece_logps/rejected': -375.2078552246094, 'referece_logps/chosen': -541.8126220703125, 'logits/rejected': 0.9195745587348938, 'logits/chosen': 0.9615907669067383, 'epoch': 0.73}


 12%|█▏        | 1310/10740 [6:38:21<45:12:27, 17.26s/it]

 12%|█▏        | 1311/10740 [6:38:33<40:44:49, 15.56s/it]

 12%|█▏        | 1312/10740 [6:38:48<40:47:30, 15.58s/it]

 12%|█▏        | 1313/10740 [6:39:01<38:24:53, 14.67s/it]

 12%|█▏        | 1314/10740 [6:39:22<43:15:02, 16.52s/it]

 12%|█▏        | 1315/10740 [6:39:39<44:11:19, 16.88s/it]

 12%|█▏        | 1316/10740 [6:39:54<42:25:54, 16.21s/it]

 12%|█▏        | 1317/10740 [6:40:13<44:39:07, 17.06s/it]

 12%|█▏        | 1318/10740 [6:40:27<42:02:28, 16.06s/it]

 12%|█▏        | 1319/10740 [6:40:48<46:06:24, 17.62s/it]

 12%|█▏        | 1320/10740 [6:41:04<44:38:57, 17.06s/it]

 12%|█▏        | 1321/10740 [6:41:25<48:14:17, 18.44s/it]

 12%|█▏        | 1322/10740 [6:41:50<52:37:18, 20.11s/it]
{'loss': 0.3884, 'learning_rate': 1.954956947910176e-06, 'rewards/chosen': -1.8220528364181519, 'rewards/rejected': -3.601571798324585, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7795190811157227, 'policy_logps/rejected': -319.6671142578125, 'policy_logps/chosen': -349.7666931152344, 'referece_logps/rejected': -283.6513671875, 'referece_logps/chosen': -331.546142578125, 'logits/rejected': -0.7556368708610535, 'logits/chosen': -0.8678354620933533, 'epoch': 0.74}


 12%|█▏        | 1324/10740 [6:42:22<47:20:29, 18.10s/it]

 12%|█▏        | 1325/10740 [6:42:41<47:33:33, 18.19s/it]

 12%|█▏        | 1326/10740 [6:43:02<49:57:14, 19.10s/it]

 12%|█▏        | 1327/10740 [6:43:23<51:21:32, 19.64s/it]

 12%|█▏        | 1328/10740 [6:43:41<49:59:00, 19.12s/it]

 12%|█▏        | 1329/10740 [6:44:01<50:57:40, 19.49s/it]

 12%|█▏        | 1330/10740 [6:44:21<51:06:13, 19.55s/it]

 12%|█▏        | 1331/10740 [6:44:39<49:43:53, 19.03s/it]

 12%|█▏        | 1332/10740 [6:44:54<46:44:00, 17.88s/it]

 12%|█▏        | 1333/10740 [6:45:07<43:08:48, 16.51s/it]

 12%|█▏        | 1334/10740 [6:45:23<42:37:29, 16.31s/it]

 12%|█▏        | 1335/10740 [6:45:41<43:40:59, 16.72s/it]

 12%|█▏        | 1336/10740 [6:46:00<46:03:19, 17.63s/it]

 12%|█▏        | 1337/10740 [6:46:19<46:31:43, 17.81s/it]

 12%|█▏        | 1338/10740 [6:46:40<49:09:50, 18.82s/it]

 12%|█▏        | 1339/10740 [6:47:01<51:01:56, 19.54s/it]

 12%|█▏        | 1340/10740 [6:47:21<51:27:38, 19.71s/it]

 12%|█▏        | 1341/10740 [6:47:43<52:48:43, 20.23s/it]

 12%|█▏        | 1342/10740 [6:47:58<48:57:53, 18.76s/it]

 13%|█▎        | 1343/10740 [6:48:12<45:23:15, 17.39s/it]

 13%|█▎        | 1344/10740 [6:48:32<47:40:01, 18.26s/it]
{'loss': 0.4048, 'learning_rate': 1.9529670944308484e-06, 'rewards/chosen': -0.9617933630943298, 'rewards/rejected': -2.408369541168213, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4465761184692383, 'policy_logps/rejected': -286.412109375, 'policy_logps/chosen': -276.939453125, 'referece_logps/rejected': -262.32843017578125, 'referece_logps/chosen': -267.3215637207031, 'logits/rejected': -0.08587145805358887, 'logits/chosen': -0.1676877737045288, 'epoch': 0.75}
[2024-04-02 02:02:14,144] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1346/10740 [6:49:16<51:43:51, 19.82s/it]
[2024-04-02 02:02:33,984] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1347/10740 [6:49:28<45:37:58, 17.49s/it]

 13%|█▎        | 1348/10740 [6:49:50<48:59:44, 18.78s/it]
[2024-04-02 02:03:07,817] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1349/10740 [6:50:09<49:49:36, 19.10s/it]
[2024-04-02 02:03:27,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1350/10740 [6:50:23<45:24:52, 17.41s/it]

 13%|█▎        | 1351/10740 [6:50:40<45:04:08, 17.28s/it]

 13%|█▎        | 1352/10740 [6:50:56<44:20:16, 17.00s/it]

 13%|█▎        | 1353/10740 [6:51:19<48:48:01, 18.72s/it]

 13%|█▎        | 1354/10740 [6:51:31<43:39:10, 16.74s/it]

 13%|█▎        | 1355/10740 [6:51:43<39:44:36, 15.25s/it]

 13%|█▎        | 1356/10740 [6:52:04<44:32:03, 17.08s/it]

 13%|█▎        | 1357/10740 [6:52:23<45:51:48, 17.60s/it]

 13%|█▎        | 1358/10740 [6:52:42<47:14:35, 18.13s/it]

 13%|█▎        | 1359/10740 [6:53:04<49:59:11, 19.18s/it]
[2024-04-02 02:06:22,247] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1360/10740 [6:53:23<49:35:11, 19.03s/it]

 13%|█▎        | 1361/10740 [6:53:44<51:24:55, 19.74s/it]

 13%|█▎        | 1362/10740 [6:54:05<52:39:06, 20.21s/it]

 13%|█▎        | 1363/10740 [6:54:25<52:10:39, 20.03s/it]

 13%|█▎        | 1364/10740 [6:54:45<51:50:37, 19.91s/it]

 13%|█▎        | 1365/10740 [6:55:02<49:34:22, 19.04s/it]

 13%|█▎        | 1366/10740 [6:55:23<51:31:52, 19.79s/it]

 13%|█▎        | 1367/10740 [6:55:37<46:50:07, 17.99s/it]
{'loss': 0.3537, 'learning_rate': 1.950841940517838e-06, 'rewards/chosen': -1.689753770828247, 'rewards/rejected': -3.764932870864868, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0751793384552, 'policy_logps/rejected': -225.3818817138672, 'policy_logps/chosen': -224.07510375976562, 'referece_logps/rejected': -187.7325439453125, 'referece_logps/chosen': -207.1775665283203, 'logits/rejected': -0.8204250335693359, 'logits/chosen': -0.6835420727729797, 'epoch': 0.76}


 13%|█▎        | 1369/10740 [6:56:13<46:48:10, 17.98s/it]

 13%|█▎        | 1370/10740 [6:56:35<49:54:00, 19.17s/it]

 13%|█▎        | 1371/10740 [6:56:55<50:13:49, 19.30s/it]

 13%|█▎        | 1372/10740 [6:57:13<49:15:55, 18.93s/it]

 13%|█▎        | 1373/10740 [6:57:35<51:40:12, 19.86s/it]

 13%|█▎        | 1374/10740 [6:57:51<48:40:15, 18.71s/it]

 13%|█▎        | 1375/10740 [6:58:08<47:24:32, 18.22s/it]
{'loss': 0.4149, 'learning_rate': 1.9500920276203874e-06, 'rewards/chosen': -3.1857523918151855, 'rewards/rejected': -4.314279556274414, 'rewards/accuracies': 0.75, 'rewards/margins': 1.128527045249939, 'policy_logps/rejected': -331.6981506347656, 'policy_logps/chosen': -368.22930908203125, 'referece_logps/rejected': -288.55535888671875, 'referece_logps/chosen': -336.3717956542969, 'logits/rejected': 0.14936545491218567, 'logits/chosen': 0.11795933544635773, 'epoch': 0.77}


 13%|█▎        | 1377/10740 [6:58:47<49:35:19, 19.07s/it]
[2024-04-02 02:12:05,678] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1378/10740 [6:59:09<51:44:04, 19.89s/it]

 13%|█▎        | 1379/10740 [6:59:26<49:31:35, 19.05s/it]

 13%|█▎        | 1380/10740 [6:59:44<48:40:16, 18.72s/it]

 13%|█▎        | 1381/10740 [7:00:06<50:54:15, 19.58s/it]
{'loss': 0.3547, 'learning_rate': 1.9495259633547022e-06, 'rewards/chosen': -2.388955593109131, 'rewards/rejected': -4.041027545928955, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6520718336105347, 'policy_logps/rejected': -344.622314453125, 'policy_logps/chosen': -442.5870361328125, 'referece_logps/rejected': -304.2120361328125, 'referece_logps/chosen': -418.697509765625, 'logits/rejected': -0.6588889360427856, 'logits/chosen': -0.8550142645835876, 'epoch': 0.77}


 13%|█▎        | 1383/10740 [7:00:44<49:06:05, 18.89s/it]

 13%|█▎        | 1384/10740 [7:01:04<50:09:10, 19.30s/it]
{'loss': 0.51, 'learning_rate': 1.949241765222778e-06, 'rewards/chosen': -1.7770771980285645, 'rewards/rejected': -2.05017352104187, 'rewards/accuracies': 0.5, 'rewards/margins': 0.27309632301330566, 'policy_logps/rejected': -264.7018127441406, 'policy_logps/chosen': -268.52435302734375, 'referece_logps/rejected': -244.20004272460938, 'referece_logps/chosen': -250.7535858154297, 'logits/rejected': -0.576210618019104, 'logits/chosen': -0.5307296514511108, 'epoch': 0.77}


 13%|█▎        | 1386/10740 [7:01:40<47:25:30, 18.25s/it]
{'loss': 0.3961, 'learning_rate': 1.949051868107802e-06, 'rewards/chosen': -2.6550235748291016, 'rewards/rejected': -4.800792217254639, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1457691192626953, 'policy_logps/rejected': -348.53704833984375, 'policy_logps/chosen': -448.3573913574219, 'referece_logps/rejected': -300.5290832519531, 'referece_logps/chosen': -421.8071594238281, 'logits/rejected': -0.3288726806640625, 'logits/chosen': -0.47465988993644714, 'epoch': 0.77}


 13%|█▎        | 1388/10740 [7:02:22<51:27:06, 19.81s/it]
{'loss': 0.3964, 'learning_rate': 1.9488616257184543e-06, 'rewards/chosen': -1.9142343997955322, 'rewards/rejected': -3.0473709106445312, 'rewards/accuracies': 0.875, 'rewards/margins': 1.13313627243042, 'policy_logps/rejected': -378.29229736328125, 'policy_logps/chosen': -322.96453857421875, 'referece_logps/rejected': -347.818603515625, 'referece_logps/chosen': -303.8221740722656, 'logits/rejected': 0.18096596002578735, 'logits/chosen': 0.12751036882400513, 'epoch': 0.78}

 13%|█▎        | 1389/10740 [7:02:45<53:48:56, 20.72s/it]
[2024-04-02 02:16:25,591] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1391/10740 [7:03:31<56:36:00, 21.79s/it]

 13%|█▎        | 1392/10740 [7:03:51<55:56:25, 21.54s/it]
[2024-04-02 02:17:09,703] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1393/10740 [7:04:12<55:21:12, 21.32s/it]
[2024-04-02 02:17:30,500] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3318, 'learning_rate': 1.9483845096242225e-06, 'rewards/chosen': -1.4789985418319702, 'rewards/rejected': -3.457379102706909, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9783804416656494, 'policy_logps/rejected': -426.7773742675781, 'policy_logps/chosen': -412.0003662109375, 'referece_logps/rejected': -392.20361328125, 'referece_logps/chosen': -397.2103271484375, 'logits/rejected': -0.27658164501190186, 'logits/chosen': -0.2045968621969223, 'epoch': 0.78}
[2024-04-02 02:17:49,069] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1395/10740 [7:04:50<52:32:54, 20.24s/it]

 13%|█▎        | 1396/10740 [7:05:03<46:10:30, 17.79s/it]

 13%|█▎        | 1397/10740 [7:05:22<47:21:47, 18.25s/it]

 13%|█▎        | 1398/10740 [7:05:38<45:35:49, 17.57s/it]

 13%|█▎        | 1399/10740 [7:05:58<47:55:18, 18.47s/it]
{'loss': 0.3075, 'learning_rate': 1.9478091238968757e-06, 'rewards/chosen': -1.1336323022842407, 'rewards/rejected': -3.656423330307007, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5227911472320557, 'policy_logps/rejected': -359.0846252441406, 'policy_logps/chosen': -384.71405029296875, 'referece_logps/rejected': -322.5203857421875, 'referece_logps/chosen': -373.37774658203125, 'logits/rejected': -0.5956583023071289, 'logits/chosen': -0.6466703414916992, 'epoch': 0.78}

 13%|█▎        | 1400/10740 [7:06:20<49:57:18, 19.25s/it]

 13%|█▎        | 1401/10740 [7:06:35<46:56:09, 18.09s/it]
[2024-04-02 02:20:13,316] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1403/10740 [7:07:15<49:07:34, 18.94s/it]

 13%|█▎        | 1404/10740 [7:07:36<50:55:05, 19.63s/it]

 13%|█▎        | 1405/10740 [7:07:54<50:10:24, 19.35s/it]

 13%|█▎        | 1406/10740 [7:08:15<50:50:55, 19.61s/it]

 13%|█▎        | 1407/10740 [7:08:32<49:05:06, 18.93s/it]

 13%|█▎        | 1408/10740 [7:08:43<42:38:43, 16.45s/it]

 13%|█▎        | 1409/10740 [7:09:02<45:07:15, 17.41s/it]
{'loss': 0.2694, 'learning_rate': 1.9468432521762806e-06, 'rewards/chosen': -1.8942378759384155, 'rewards/rejected': -3.50447940826416, 'rewards/accuracies': 0.875, 'rewards/margins': 1.610241413116455, 'policy_logps/rejected': -380.659912109375, 'policy_logps/chosen': -325.8565673828125, 'referece_logps/rejected': -345.6151428222656, 'referece_logps/chosen': -306.9141845703125, 'logits/rejected': -0.4028506875038147, 'logits/chosen': -0.22983835637569427, 'epoch': 0.79}

 13%|█▎        | 1410/10740 [7:09:22<46:28:00, 17.93s/it]


 13%|█▎        | 1412/10740 [7:10:03<50:17:44, 19.41s/it]
{'loss': 0.4578, 'learning_rate': 1.9465518109641433e-06, 'rewards/chosen': -1.7125648260116577, 'rewards/rejected': -2.5774800777435303, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8649150133132935, 'policy_logps/rejected': -195.44544982910156, 'policy_logps/chosen': -266.503662109375, 'referece_logps/rejected': -169.67063903808594, 'referece_logps/chosen': -249.37803649902344, 'logits/rejected': -0.5759372711181641, 'logits/chosen': -0.5202962160110474, 'epoch': 0.79}


 13%|█▎        | 1414/10740 [7:10:43<51:38:17, 19.93s/it]

 13%|█▎        | 1415/10740 [7:10:59<48:24:58, 18.69s/it]
{'loss': 0.3932, 'learning_rate': 1.9462595949311756e-06, 'rewards/chosen': -1.6511116027832031, 'rewards/rejected': -3.769758701324463, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1186466217041016, 'policy_logps/rejected': -355.933349609375, 'policy_logps/chosen': -344.9704895019531, 'referece_logps/rejected': -318.23577880859375, 'referece_logps/chosen': -328.4593811035156, 'logits/rejected': -0.062414661049842834, 'logits/chosen': 0.021128349006175995, 'epoch': 0.79}


 13%|█▎        | 1417/10740 [7:11:35<46:53:23, 18.11s/it]

 13%|█▎        | 1418/10740 [7:11:57<49:43:29, 19.20s/it]
[2024-04-02 02:25:14,988] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1419/10740 [7:12:10<45:26:12, 17.55s/it]
{'loss': 0.3702, 'learning_rate': 1.9458687686905627e-06, 'rewards/chosen': -2.0592668056488037, 'rewards/rejected': -2.906623601913452, 'rewards/accuracies': 0.75, 'rewards/margins': 0.847356915473938, 'policy_logps/rejected': -351.8940124511719, 'policy_logps/chosen': -346.4151916503906, 'referece_logps/rejected': -322.8277587890625, 'referece_logps/chosen': -325.8225402832031, 'logits/rejected': -0.42210084199905396, 'logits/chosen': -0.3798408508300781, 'epoch': 0.79}

 13%|█▎        | 1420/10740 [7:12:32<48:17:52, 18.66s/it]

 13%|█▎        | 1421/10740 [7:12:49<47:32:51, 18.37s/it]


 13%|█▎        | 1423/10740 [7:13:28<48:33:58, 18.77s/it]

 13%|█▎        | 1424/10740 [7:13:42<44:52:46, 17.34s/it]
[2024-04-02 02:27:00,343] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1425/10740 [7:13:55<41:18:01, 15.96s/it]

 13%|█▎        | 1426/10740 [7:14:09<40:04:02, 15.49s/it]

 13%|█▎        | 1427/10740 [7:14:23<38:28:09, 14.87s/it]
{'loss': 0.3603, 'learning_rate': 1.945082987384514e-06, 'rewards/chosen': -2.091036319732666, 'rewards/rejected': -3.761911392211914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.670875072479248, 'policy_logps/rejected': -381.25439453125, 'policy_logps/chosen': -381.948974609375, 'referece_logps/rejected': -343.6352844238281, 'referece_logps/chosen': -361.0386047363281, 'logits/rejected': -0.055992335081100464, 'logits/chosen': -0.0230005644261837, 'epoch': 0.8}

 13%|█▎        | 1428/10740 [7:14:38<38:41:15, 14.96s/it]

 13%|█▎        | 1429/10740 [7:14:56<41:04:49, 15.88s/it]
[2024-04-02 02:28:34,204] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1430/10740 [7:15:16<44:21:15, 17.15s/it]


 13%|█▎        | 1432/10740 [7:15:53<46:35:23, 18.02s/it]
[2024-04-02 02:29:11,654] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3771, 'learning_rate': 1.9445890801553896e-06, 'rewards/chosen': -2.331143379211426, 'rewards/rejected': -4.599777698516846, 'rewards/accuracies': 0.75, 'rewards/margins': 2.26863431930542, 'policy_logps/rejected': -363.4862060546875, 'policy_logps/chosen': -499.38458251953125, 'referece_logps/rejected': -317.4884338378906, 'referece_logps/chosen': -476.07318115234375, 'logits/rejected': 0.054955706000328064, 'logits/chosen': 0.052239224314689636, 'epoch': 0.8}
[2024-04-02 02:29:33,901] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1434/10740 [7:16:38<52:30:28, 20.31s/it]
{'loss': 0.4468, 'learning_rate': 1.9443909158119405e-06, 'rewards/chosen': -1.885737419128418, 'rewards/rejected': -2.6720874309539795, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7863501310348511, 'policy_logps/rejected': -352.6669921875, 'policy_logps/chosen': -395.6318359375, 'referece_logps/rejected': -325.9461669921875, 'referece_logps/chosen': -376.7744445800781, 'logits/rejected': -0.195010244846344, 'logits/chosen': -0.17212048172950745, 'epoch': 0.8}

 13%|█▎        | 1435/10740 [7:17:00<53:38:32, 20.75s/it]


 13%|█▎        | 1437/10740 [7:17:35<48:42:30, 18.85s/it]
{'loss': 0.3831, 'learning_rate': 1.9440930251093216e-06, 'rewards/chosen': -1.2650890350341797, 'rewards/rejected': -4.379398822784424, 'rewards/accuracies': 1.0, 'rewards/margins': 3.114309787750244, 'policy_logps/rejected': -494.5176086425781, 'policy_logps/chosen': -466.0159912109375, 'referece_logps/rejected': -450.7236022949219, 'referece_logps/chosen': -453.3650817871094, 'logits/rejected': -0.18275606632232666, 'logits/chosen': -0.0630861222743988, 'epoch': 0.8}
[2024-04-02 02:31:15,911] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 13%|█▎        | 1439/10740 [7:18:19<52:21:33, 20.27s/it]
[2024-04-02 02:31:36,788] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 1440/10740 [7:18:36<50:29:27, 19.54s/it]
{'loss': 0.3299, 'learning_rate': 1.943794361598566e-06, 'rewards/chosen': -1.357494592666626, 'rewards/rejected': -3.687415361404419, 'rewards/accuracies': 0.875, 'rewards/margins': 2.329920768737793, 'policy_logps/rejected': -328.82281494140625, 'policy_logps/chosen': -279.9702453613281, 'referece_logps/rejected': -291.94866943359375, 'referece_logps/chosen': -266.39532470703125, 'logits/rejected': -0.7889838218688965, 'logits/chosen': -0.6483627557754517, 'epoch': 0.8}


 13%|█▎        | 1442/10740 [7:19:17<51:22:12, 19.89s/it]

 13%|█▎        | 1443/10740 [7:19:40<53:21:57, 20.66s/it]

 13%|█▎        | 1444/10740 [7:19:59<52:04:45, 20.17s/it]
{'loss': 0.4938, 'learning_rate': 1.94339494186088e-06, 'rewards/chosen': -2.8758435249328613, 'rewards/rejected': -3.85685133934021, 'rewards/accuracies': 0.75, 'rewards/margins': 0.981007993221283, 'policy_logps/rejected': -312.0601806640625, 'policy_logps/chosen': -351.4985656738281, 'referece_logps/rejected': -273.49169921875, 'referece_logps/chosen': -322.7400817871094, 'logits/rejected': -0.836637020111084, 'logits/chosen': -0.7947245240211487, 'epoch': 0.81}

 13%|█▎        | 1445/10740 [7:20:20<53:18:13, 20.64s/it]


 13%|█▎        | 1447/10740 [7:20:51<44:55:43, 17.40s/it]

 13%|█▎        | 1448/10740 [7:21:09<46:00:31, 17.83s/it]

 13%|█▎        | 1449/10740 [7:21:31<48:58:59, 18.98s/it]

 14%|█▎        | 1450/10740 [7:21:53<51:23:16, 19.91s/it]

 14%|█▎        | 1451/10740 [7:22:14<51:45:14, 20.06s/it]
{'loss': 0.4126, 'learning_rate': 1.9426926542138727e-06, 'rewards/chosen': -1.980830430984497, 'rewards/rejected': -3.7447903156280518, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7639597654342651, 'policy_logps/rejected': -242.30111694335938, 'policy_logps/chosen': -202.10421752929688, 'referece_logps/rejected': -204.85321044921875, 'referece_logps/chosen': -182.29592895507812, 'logits/rejected': -0.9236964583396912, 'logits/chosen': -1.0495351552963257, 'epoch': 0.81}


 14%|█▎        | 1453/10740 [7:22:56<53:03:54, 20.57s/it]
[2024-04-02 02:36:13,791] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▎        | 1454/10740 [7:23:10<48:01:01, 18.62s/it]
[2024-04-02 02:36:27,844] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2444, 'learning_rate': 1.9423903875079006e-06, 'rewards/chosen': -2.260357141494751, 'rewards/rejected': -4.139673233032227, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8793164491653442, 'policy_logps/rejected': -293.03009033203125, 'policy_logps/chosen': -363.8656005859375, 'referece_logps/rejected': -251.6333465576172, 'referece_logps/chosen': -341.2619934082031, 'logits/rejected': -0.940788984298706, 'logits/chosen': -0.88899827003479, 'epoch': 0.81}

 14%|█▎        | 1455/10740 [7:23:30<49:28:42, 19.18s/it]


 14%|█▎        | 1457/10740 [7:24:17<54:59:35, 21.33s/it]
[2024-04-02 02:37:35,299] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▎        | 1458/10740 [7:24:37<53:50:05, 20.88s/it]
{'loss': 0.2871, 'learning_rate': 1.9419861652981644e-06, 'rewards/chosen': -1.420964241027832, 'rewards/rejected': -3.2793030738830566, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8583388328552246, 'policy_logps/rejected': -211.45724487304688, 'policy_logps/chosen': -218.43890380859375, 'referece_logps/rejected': -178.6642303466797, 'referece_logps/chosen': -204.22926330566406, 'logits/rejected': -0.2687448561191559, 'logits/chosen': -0.3423879146575928, 'epoch': 0.81}


 14%|█▎        | 1460/10740 [7:25:12<48:40:21, 18.88s/it]

 14%|█▎        | 1461/10740 [7:25:30<48:16:40, 18.73s/it]
{'loss': 0.3683, 'learning_rate': 1.9416820990112252e-06, 'rewards/chosen': -1.7091772556304932, 'rewards/rejected': -3.20739483833313, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4982175827026367, 'policy_logps/rejected': -244.2085418701172, 'policy_logps/chosen': -306.98919677734375, 'referece_logps/rejected': -212.13455200195312, 'referece_logps/chosen': -289.89739990234375, 'logits/rejected': -0.6477510333061218, 'logits/chosen': -0.6420615315437317, 'epoch': 0.82}

 14%|█▎        | 1462/10740 [7:25:49<48:37:03, 18.86s/it]

 14%|█▎        | 1463/10740 [7:26:09<49:09:51, 19.08s/it]


 14%|█▎        | 1465/10740 [7:26:42<46:11:00, 17.93s/it]

 14%|█▎        | 1466/10740 [7:26:54<41:53:17, 16.26s/it]
{'loss': 0.3003, 'learning_rate': 1.941173609023703e-06, 'rewards/chosen': -2.425602436065674, 'rewards/rejected': -3.819511651992798, 'rewards/accuracies': 0.875, 'rewards/margins': 1.393909215927124, 'policy_logps/rejected': -343.42974853515625, 'policy_logps/chosen': -329.38140869140625, 'referece_logps/rejected': -305.2346496582031, 'referece_logps/chosen': -305.12542724609375, 'logits/rejected': -0.3840681314468384, 'logits/chosen': -0.2978217899799347, 'epoch': 0.82}


 14%|█▎        | 1468/10740 [7:27:27<42:31:56, 16.51s/it]
{'loss': 0.4266, 'learning_rate': 1.9409696137495567e-06, 'rewards/chosen': -1.4536194801330566, 'rewards/rejected': -4.7794060707092285, 'rewards/accuracies': 0.875, 'rewards/margins': 3.325786590576172, 'policy_logps/rejected': -641.3270874023438, 'policy_logps/chosen': -484.4449462890625, 'referece_logps/rejected': -593.532958984375, 'referece_logps/chosen': -469.90875244140625, 'logits/rejected': -0.27652785181999207, 'logits/chosen': -0.1324891597032547, 'epoch': 0.82}

 14%|█▎        | 1469/10740 [7:27:47<44:32:47, 17.30s/it]

 14%|█▎        | 1470/10740 [7:28:03<43:38:48, 16.95s/it]

 14%|█▎        | 1471/10740 [7:28:15<40:16:26, 15.64s/it]

 14%|█▎        | 1472/10740 [7:28:35<43:22:53, 16.85s/it]

 14%|█▎        | 1473/10740 [7:28:57<47:08:19, 18.31s/it]
[2024-04-02 02:42:37,080] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▎        | 1474/10740 [7:29:19<50:06:59, 19.47s/it]
[2024-04-02 02:42:57,015] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▎        | 1475/10740 [7:29:39<50:28:10, 19.61s/it]

 14%|█▎        | 1476/10740 [7:29:55<47:35:38, 18.50s/it]


 14%|█▍        | 1478/10740 [7:30:36<50:38:27, 19.68s/it]
{'loss': 0.3124, 'learning_rate': 1.9399445038579686e-06, 'rewards/chosen': -2.160067558288574, 'rewards/rejected': -3.790234088897705, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6301665306091309, 'policy_logps/rejected': -365.67840576171875, 'policy_logps/chosen': -376.80694580078125, 'referece_logps/rejected': -327.7760314941406, 'referece_logps/chosen': -355.2062683105469, 'logits/rejected': -0.6393151879310608, 'logits/chosen': -0.4746015667915344, 'epoch': 0.83}

 14%|█▍        | 1479/10740 [7:30:59<53:36:24, 20.84s/it]


 14%|█▍        | 1481/10740 [7:31:38<51:25:34, 20.00s/it]
[2024-04-02 02:44:56,543] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4014, 'learning_rate': 1.9396353034054568e-06, 'rewards/chosen': -1.899354100227356, 'rewards/rejected': -4.40653657913208, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5071823596954346, 'policy_logps/rejected': -453.0704040527344, 'policy_logps/chosen': -367.32708740234375, 'referece_logps/rejected': -409.0050354003906, 'referece_logps/chosen': -348.3335266113281, 'logits/rejected': 0.518862783908844, 'logits/chosen': 0.5816805362701416, 'epoch': 0.83}


 14%|█▍        | 1483/10740 [7:32:02<40:43:59, 15.84s/it]
{'loss': 0.3791, 'learning_rate': 1.9394287424441492e-06, 'rewards/chosen': -1.2869340181350708, 'rewards/rejected': -3.4187166690826416, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1317827701568604, 'policy_logps/rejected': -328.1301574707031, 'policy_logps/chosen': -431.65423583984375, 'referece_logps/rejected': -293.9429931640625, 'referece_logps/chosen': -418.784912109375, 'logits/rejected': -0.7202770709991455, 'logits/chosen': -0.7207245826721191, 'epoch': 0.83}

 14%|█▍        | 1484/10740 [7:32:21<43:44:00, 17.01s/it]


 14%|█▍        | 1486/10740 [7:32:48<38:23:24, 14.93s/it]

 14%|█▍        | 1487/10740 [7:33:04<39:43:50, 15.46s/it]

 14%|█▍        | 1488/10740 [7:33:23<41:52:24, 16.29s/it]

 14%|█▍        | 1489/10740 [7:33:40<42:43:29, 16.63s/it]

 14%|█▍        | 1490/10740 [7:33:55<41:04:27, 15.99s/it]

 14%|█▍        | 1491/10740 [7:34:14<43:44:00, 17.02s/it]

 14%|█▍        | 1492/10740 [7:34:31<43:24:44, 16.90s/it]
{'loss': 0.5347, 'learning_rate': 1.9384949897605145e-06, 'rewards/chosen': -2.868074417114258, 'rewards/rejected': -3.8709537982940674, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0028795003890991, 'policy_logps/rejected': -289.3885498046875, 'policy_logps/chosen': -349.92974853515625, 'referece_logps/rejected': -250.6790313720703, 'referece_logps/chosen': -321.2490234375, 'logits/rejected': -1.5928261280059814, 'logits/chosen': -1.6974258422851562, 'epoch': 0.83}


 14%|█▍        | 1494/10740 [7:35:12<48:25:55, 18.86s/it]
{'loss': 0.3594, 'learning_rate': 1.9382865499788943e-06, 'rewards/chosen': -2.7198944091796875, 'rewards/rejected': -4.467589378356934, 'rewards/accuracies': 0.75, 'rewards/margins': 1.747694969177246, 'policy_logps/rejected': -472.28021240234375, 'policy_logps/chosen': -392.12799072265625, 'referece_logps/rejected': -427.60430908203125, 'referece_logps/chosen': -364.9290466308594, 'logits/rejected': 0.23199152946472168, 'logits/chosen': 0.3149105906486511, 'epoch': 0.83}

 14%|█▍        | 1495/10740 [7:35:30<47:40:01, 18.56s/it]


 14%|█▍        | 1497/10740 [7:36:12<51:23:41, 20.02s/it]

 14%|█▍        | 1498/10740 [7:36:34<52:48:28, 20.57s/it]
{'loss': 0.309, 'learning_rate': 1.937868646418081e-06, 'rewards/chosen': -1.4888722896575928, 'rewards/rejected': -4.9349365234375, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4460642337799072, 'policy_logps/rejected': -323.0076599121094, 'policy_logps/chosen': -227.6037139892578, 'referece_logps/rejected': -273.6582946777344, 'referece_logps/chosen': -212.71498107910156, 'logits/rejected': -0.5533884763717651, 'logits/chosen': -0.5962308049201965, 'epoch': 0.84}


 14%|█▍        | 1500/10740 [7:37:13<51:06:07, 19.91s/it]

 14%|█▍        | 1501/10740 [7:37:41<57:21:05, 22.35s/it]

 14%|█▍        | 1502/10740 [7:38:02<56:56:15, 22.19s/it]

 14%|█▍        | 1503/10740 [7:38:20<53:47:44, 20.97s/it]

 14%|█▍        | 1504/10740 [7:38:39<51:50:03, 20.20s/it]

 14%|█▍        | 1505/10740 [7:39:00<52:40:22, 20.53s/it]
{'loss': 0.3514, 'learning_rate': 1.9371340314480338e-06, 'rewards/chosen': -1.8260431289672852, 'rewards/rejected': -3.0036251544952393, 'rewards/accuracies': 0.875, 'rewards/margins': 1.177581787109375, 'policy_logps/rejected': -377.4378967285156, 'policy_logps/chosen': -291.22137451171875, 'referece_logps/rejected': -347.4016418457031, 'referece_logps/chosen': -272.9609375, 'logits/rejected': 0.11739277839660645, 'logits/chosen': 0.04405997693538666, 'epoch': 0.84}
[2024-04-02 02:52:37,401] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 1506/10740 [7:39:19<51:27:13, 20.06s/it]
[2024-04-02 02:52:59,988] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 14%|█▍        | 1508/10740 [7:40:03<53:23:08, 20.82s/it]
{'loss': 0.3718, 'learning_rate': 1.9368179177502e-06, 'rewards/chosen': -2.057720184326172, 'rewards/rejected': -4.787175178527832, 'rewards/accuracies': 1.0, 'rewards/margins': 2.72945499420166, 'policy_logps/rejected': -406.9908447265625, 'policy_logps/chosen': -456.27508544921875, 'referece_logps/rejected': -359.119140625, 'referece_logps/chosen': -435.6978454589844, 'logits/rejected': -0.6997985243797302, 'logits/chosen': -0.7211713194847107, 'epoch': 0.84}

 14%|█▍        | 1509/10740 [7:40:26<55:32:18, 21.66s/it]

 14%|█▍        | 1510/10740 [7:40:48<55:18:14, 21.57s/it]


 14%|█▍        | 1512/10740 [7:41:20<48:30:46, 18.93s/it]

 14%|█▍        | 1513/10740 [7:41:31<42:14:53, 16.48s/it]
{'loss': 0.3679, 'learning_rate': 1.936289357597574e-06, 'rewards/chosen': -1.482329249382019, 'rewards/rejected': -2.8093643188476562, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3270349502563477, 'policy_logps/rejected': -302.30499267578125, 'policy_logps/chosen': -359.1209716796875, 'referece_logps/rejected': -274.2113342285156, 'referece_logps/chosen': -344.2976989746094, 'logits/rejected': -0.000855959951877594, 'logits/chosen': 0.052892088890075684, 'epoch': 0.85}


 14%|█▍        | 1515/10740 [7:42:11<46:12:31, 18.03s/it]
{'loss': 0.4696, 'learning_rate': 1.936077337364473e-06, 'rewards/chosen': -1.783130168914795, 'rewards/rejected': -2.1350302696228027, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3519001007080078, 'policy_logps/rejected': -277.0877685546875, 'policy_logps/chosen': -275.59014892578125, 'referece_logps/rejected': -255.7374725341797, 'referece_logps/chosen': -257.7588806152344, 'logits/rejected': -0.2664724290370941, 'logits/chosen': -0.1914074420928955, 'epoch': 0.85}


 14%|█▍        | 1517/10740 [7:42:43<44:19:53, 17.30s/it]

 14%|█▍        | 1518/10740 [7:42:55<40:01:52, 15.63s/it]

 14%|█▍        | 1519/10740 [7:43:10<40:14:35, 15.71s/it]
{'loss': 0.3397, 'learning_rate': 1.9356522753132e-06, 'rewards/chosen': -1.9276171922683716, 'rewards/rejected': -3.898831367492676, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9712140560150146, 'policy_logps/rejected': -295.57879638671875, 'policy_logps/chosen': -306.0086975097656, 'referece_logps/rejected': -256.59051513671875, 'referece_logps/chosen': -286.7325134277344, 'logits/rejected': -0.23446020483970642, 'logits/chosen': -0.10275262594223022, 'epoch': 0.85}

 14%|█▍        | 1520/10740 [7:43:28<41:45:26, 16.30s/it]


 14%|█▍        | 1522/10740 [7:44:01<42:13:26, 16.49s/it]
{'loss': 0.3572, 'learning_rate': 1.9353325851923187e-06, 'rewards/chosen': -2.1458795070648193, 'rewards/rejected': -2.698054790496826, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5521754026412964, 'policy_logps/rejected': -311.28387451171875, 'policy_logps/chosen': -413.61962890625, 'referece_logps/rejected': -284.3033447265625, 'referece_logps/chosen': -392.16082763671875, 'logits/rejected': -0.7472851872444153, 'logits/chosen': -0.8488475680351257, 'epoch': 0.85}


 14%|█▍        | 1524/10740 [7:44:35<44:30:36, 17.39s/it]

 14%|█▍        | 1525/10740 [7:44:53<44:50:44, 17.52s/it]

 14%|█▍        | 1526/10740 [7:45:11<45:27:56, 17.76s/it]
{'loss': 0.3834, 'learning_rate': 1.9349051407521097e-06, 'rewards/chosen': -1.7156120538711548, 'rewards/rejected': -3.033021926879883, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3174099922180176, 'policy_logps/rejected': -327.951904296875, 'policy_logps/chosen': -271.38116455078125, 'referece_logps/rejected': -297.6217041015625, 'referece_logps/chosen': -254.22506713867188, 'logits/rejected': -0.21342983841896057, 'logits/chosen': -0.1345500946044922, 'epoch': 0.85}


 14%|█▍        | 1528/10740 [7:45:49<46:29:35, 18.17s/it]

 14%|█▍        | 1529/10740 [7:46:11<49:36:09, 19.39s/it]
{'loss': 0.4247, 'learning_rate': 1.934583664552851e-06, 'rewards/chosen': -1.161804437637329, 'rewards/rejected': -2.7796339988708496, 'rewards/accuracies': 0.625, 'rewards/margins': 1.61782968044281, 'policy_logps/rejected': -394.44403076171875, 'policy_logps/chosen': -405.0101623535156, 'referece_logps/rejected': -366.6476745605469, 'referece_logps/chosen': -393.3921203613281, 'logits/rejected': 0.7592290639877319, 'logits/chosen': 0.6632322072982788, 'epoch': 0.85}

 14%|█▍        | 1530/10740 [7:46:30<48:50:25, 19.09s/it]

 14%|█▍        | 1531/10740 [7:46:44<45:30:31, 17.79s/it]

 14%|█▍        | 1532/10740 [7:47:06<48:27:04, 18.94s/it]


 14%|█▍        | 1534/10740 [7:47:37<43:38:23, 17.07s/it]

 14%|█▍        | 1535/10740 [7:47:55<44:54:04, 17.56s/it]
{'loss': 0.3981, 'learning_rate': 1.933938417345991e-06, 'rewards/chosen': -1.8699429035186768, 'rewards/rejected': -3.3926339149475098, 'rewards/accuracies': 0.875, 'rewards/margins': 1.522691011428833, 'policy_logps/rejected': -322.872314453125, 'policy_logps/chosen': -311.5711669921875, 'referece_logps/rejected': -288.9460144042969, 'referece_logps/chosen': -292.8717346191406, 'logits/rejected': -1.1641556024551392, 'logits/chosen': -1.1331310272216797, 'epoch': 0.86}

 14%|█▍        | 1536/10740 [7:48:17<47:39:01, 18.64s/it]


 14%|█▍        | 1538/10740 [7:48:51<44:57:23, 17.59s/it]

 14%|█▍        | 1539/10740 [7:49:11<46:29:51, 18.19s/it]

 14%|█▍        | 1540/10740 [7:49:33<49:27:05, 19.35s/it]

 14%|█▍        | 1541/10740 [7:49:49<47:08:03, 18.45s/it]
{'loss': 0.2632, 'learning_rate': 1.9332901121563215e-06, 'rewards/chosen': -1.6895473003387451, 'rewards/rejected': -3.375047206878662, 'rewards/accuracies': 1.0, 'rewards/margins': 1.685499668121338, 'policy_logps/rejected': -285.37017822265625, 'policy_logps/chosen': -517.4951171875, 'referece_logps/rejected': -251.61972045898438, 'referece_logps/chosen': -500.5996398925781, 'logits/rejected': -1.4860442876815796, 'logits/chosen': -1.5381474494934082, 'epoch': 0.86}

 14%|█▍        | 1542/10740 [7:50:11<49:25:25, 19.34s/it]

 14%|█▍        | 1543/10740 [7:50:32<50:54:49, 19.93s/it]

 14%|█▍        | 1544/10740 [7:50:44<45:05:21, 17.65s/it]

 14%|█▍        | 1545/10740 [7:51:06<48:30:35, 18.99s/it]

 14%|█▍        | 1546/10740 [7:51:26<49:11:35, 19.26s/it]

 14%|█▍        | 1547/10740 [7:51:45<48:30:29, 19.00s/it]

 14%|█▍        | 1548/10740 [7:51:59<44:42:12, 17.51s/it]

 14%|█▍        | 1549/10740 [7:52:18<46:06:16, 18.06s/it]


 14%|█▍        | 1551/10740 [7:52:58<48:23:16, 18.96s/it]
{'loss': 0.3671, 'learning_rate': 1.9322028137595076e-06, 'rewards/chosen': -1.8867688179016113, 'rewards/rejected': -3.1784729957580566, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2917040586471558, 'policy_logps/rejected': -288.6845703125, 'policy_logps/chosen': -293.02606201171875, 'referece_logps/rejected': -256.8998718261719, 'referece_logps/chosen': -274.1583557128906, 'logits/rejected': -1.3330577611923218, 'logits/chosen': -1.2311731576919556, 'epoch': 0.87}

 14%|█▍        | 1552/10740 [7:53:21<51:18:08, 20.10s/it]

 14%|█▍        | 1553/10740 [7:53:39<49:56:38, 19.57s/it]

 14%|█▍        | 1554/10740 [7:53:56<48:19:21, 18.94s/it]

 14%|█▍        | 1555/10740 [7:54:09<43:41:35, 17.13s/it]

 14%|█▍        | 1556/10740 [7:54:24<42:03:26, 16.49s/it]


 15%|█▍        | 1558/10740 [7:54:58<41:47:22, 16.38s/it]
{'loss': 0.4297, 'learning_rate': 1.931436659519157e-06, 'rewards/chosen': -3.2228331565856934, 'rewards/rejected': -3.3997621536254883, 'rewards/accuracies': 0.375, 'rewards/margins': 0.1769287884235382, 'policy_logps/rejected': -514.1979370117188, 'policy_logps/chosen': -564.6016235351562, 'referece_logps/rejected': -480.2002868652344, 'referece_logps/chosen': -532.3733520507812, 'logits/rejected': -0.013874247670173645, 'logits/chosen': -0.10924957692623138, 'epoch': 0.87}

 15%|█▍        | 1559/10740 [7:55:16<42:40:15, 16.73s/it]

 15%|█▍        | 1560/10740 [7:55:37<46:11:49, 18.12s/it]

 15%|█▍        | 1561/10740 [7:55:57<47:42:42, 18.71s/it]

 15%|█▍        | 1562/10740 [7:56:11<43:48:07, 17.18s/it]
[2024-04-02 03:09:51,369] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 15%|█▍        | 1564/10740 [7:56:54<49:36:01, 19.46s/it]
{'loss': 0.4391, 'learning_rate': 1.93077665181437e-06, 'rewards/chosen': -2.1015141010284424, 'rewards/rejected': -2.7200100421905518, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6184959411621094, 'policy_logps/rejected': -342.62261962890625, 'policy_logps/chosen': -527.5048217773438, 'referece_logps/rejected': -315.4225158691406, 'referece_logps/chosen': -506.4897155761719, 'logits/rejected': 0.5749845504760742, 'logits/chosen': 0.580743134021759, 'epoch': 0.87}

 15%|█▍        | 1565/10740 [7:57:08<45:36:12, 17.89s/it]


 15%|█▍        | 1567/10740 [7:57:44<46:21:56, 18.20s/it]
{'loss': 0.3078, 'learning_rate': 1.9304455049651653e-06, 'rewards/chosen': -2.6220145225524902, 'rewards/rejected': -3.6791739463806152, 'rewards/accuracies': 0.625, 'rewards/margins': 1.057159185409546, 'policy_logps/rejected': -402.0952453613281, 'policy_logps/chosen': -415.6789245605469, 'referece_logps/rejected': -365.30352783203125, 'referece_logps/chosen': -389.45880126953125, 'logits/rejected': -0.5155316591262817, 'logits/chosen': -0.37219148874282837, 'epoch': 0.88}


 15%|█▍        | 1569/10740 [7:58:20<45:12:31, 17.75s/it]
{'loss': 0.3858, 'learning_rate': 1.930224317250794e-06, 'rewards/chosen': -2.341107130050659, 'rewards/rejected': -3.954944610595703, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6138370037078857, 'policy_logps/rejected': -375.419921875, 'policy_logps/chosen': -481.8877258300781, 'referece_logps/rejected': -335.8704833984375, 'referece_logps/chosen': -458.4766845703125, 'logits/rejected': -0.25959286093711853, 'logits/chosen': -0.2183670848608017, 'epoch': 0.88}

 15%|█▍        | 1570/10740 [7:58:33<41:13:58, 16.19s/it]

 15%|█▍        | 1571/10740 [7:58:51<43:00:26, 16.89s/it]


 15%|█▍        | 1573/10740 [7:59:30<46:13:05, 18.15s/it]
{'loss': 0.3413, 'learning_rate': 1.9297809266284715e-06, 'rewards/chosen': -0.9767707586288452, 'rewards/rejected': -2.665668487548828, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6888976097106934, 'policy_logps/rejected': -297.09246826171875, 'policy_logps/chosen': -462.64080810546875, 'referece_logps/rejected': -270.435791015625, 'referece_logps/chosen': -452.87310791015625, 'logits/rejected': -0.3175280690193176, 'logits/chosen': -0.2989117205142975, 'epoch': 0.88}


 15%|█▍        | 1575/10740 [8:00:02<42:41:54, 16.77s/it]
{'loss': 0.3614, 'learning_rate': 1.929558723881831e-06, 'rewards/chosen': -1.5700966119766235, 'rewards/rejected': -2.5072808265686035, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9371842741966248, 'policy_logps/rejected': -457.0436706542969, 'policy_logps/chosen': -478.65106201171875, 'referece_logps/rejected': -431.97088623046875, 'referece_logps/chosen': -462.9500732421875, 'logits/rejected': -0.7967590093612671, 'logits/chosen': -0.686059296131134, 'epoch': 0.88}

 15%|█▍        | 1576/10740 [8:00:16<40:18:54, 15.84s/it]

 15%|█▍        | 1577/10740 [8:00:36<43:20:11, 17.03s/it]

 15%|█▍        | 1578/10740 [8:00:52<42:52:52, 16.85s/it]


 15%|█▍        | 1580/10740 [8:01:30<46:25:58, 18.25s/it]
{'loss': 0.3825, 'learning_rate': 1.9290017376436044e-06, 'rewards/chosen': -3.4902658462524414, 'rewards/rejected': -6.585048198699951, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0947821140289307, 'policy_logps/rejected': -512.0142211914062, 'policy_logps/chosen': -436.3009338378906, 'referece_logps/rejected': -446.1636962890625, 'referece_logps/chosen': -401.3982849121094, 'logits/rejected': -0.6033705472946167, 'logits/chosen': -0.6099152565002441, 'epoch': 0.88}

 15%|█▍        | 1581/10740 [8:01:52<48:53:42, 19.22s/it]

 15%|█▍        | 1582/10740 [8:02:13<50:20:17, 19.79s/it]


 15%|█▍        | 1584/10740 [8:02:51<48:56:12, 19.24s/it]
{'loss': 0.3649, 'learning_rate': 1.9285546276825924e-06, 'rewards/chosen': -2.3477911949157715, 'rewards/rejected': -4.502631664276123, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1548404693603516, 'policy_logps/rejected': -354.80279541015625, 'policy_logps/chosen': -324.90399169921875, 'referece_logps/rejected': -309.7764892578125, 'referece_logps/chosen': -301.4260559082031, 'logits/rejected': 0.773270845413208, 'logits/chosen': 0.9123140573501587, 'epoch': 0.88}
[2024-04-02 03:16:32,016] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 15%|█▍        | 1586/10740 [8:03:35<52:16:54, 20.56s/it]
{'loss': 0.4487, 'learning_rate': 1.9283305659354785e-06, 'rewards/chosen': -1.865342617034912, 'rewards/rejected': -3.154698610305786, 'rewards/accuracies': 0.875, 'rewards/margins': 1.289355993270874, 'policy_logps/rejected': -330.13104248046875, 'policy_logps/chosen': -295.77764892578125, 'referece_logps/rejected': -298.58404541015625, 'referece_logps/chosen': -277.1242370605469, 'logits/rejected': -0.2774331271648407, 'logits/chosen': -0.17837734520435333, 'epoch': 0.89}


 15%|█▍        | 1588/10740 [8:04:10<48:17:12, 18.99s/it]
{'loss': 0.4008, 'learning_rate': 1.9281061664526052e-06, 'rewards/chosen': -1.8746670484542847, 'rewards/rejected': -4.18434476852417, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3096776008605957, 'policy_logps/rejected': -333.19195556640625, 'policy_logps/chosen': -492.04779052734375, 'referece_logps/rejected': -291.3485107421875, 'referece_logps/chosen': -473.30108642578125, 'logits/rejected': -0.22980345785617828, 'logits/chosen': -0.2982054650783539, 'epoch': 0.89}


 15%|█▍        | 1590/10740 [8:04:51<49:32:23, 19.49s/it]

 15%|█▍        | 1591/10740 [8:05:06<46:45:48, 18.40s/it]

 15%|█▍        | 1592/10740 [8:05:28<49:28:39, 19.47s/it]

 15%|█▍        | 1593/10740 [8:05:48<49:55:53, 19.65s/it]
{'loss': 0.2387, 'learning_rate': 1.927543690687534e-06, 'rewards/chosen': -1.8963406085968018, 'rewards/rejected': -4.953962802886963, 'rewards/accuracies': 0.75, 'rewards/margins': 3.057621955871582, 'policy_logps/rejected': -347.8084411621094, 'policy_logps/chosen': -432.2606506347656, 'referece_logps/rejected': -298.268798828125, 'referece_logps/chosen': -413.2972412109375, 'logits/rejected': -0.487643837928772, 'logits/chosen': -0.4421382248401642, 'epoch': 0.89}

 15%|█▍        | 1594/10740 [8:06:08<49:54:03, 19.64s/it]

 15%|█▍        | 1595/10740 [8:06:25<48:03:11, 18.92s/it]

 15%|█▍        | 1596/10740 [8:06:37<42:33:16, 16.75s/it]


 15%|█▍        | 1598/10740 [8:07:09<40:25:10, 15.92s/it]
{'loss': 0.3231, 'learning_rate': 1.9269791058635114e-06, 'rewards/chosen': -2.181175947189331, 'rewards/rejected': -4.533206939697266, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3520309925079346, 'policy_logps/rejected': -264.7405090332031, 'policy_logps/chosen': -299.26141357421875, 'referece_logps/rejected': -219.40843200683594, 'referece_logps/chosen': -277.44964599609375, 'logits/rejected': -0.7573108673095703, 'logits/chosen': -0.8542713522911072, 'epoch': 0.89}

 15%|█▍        | 1599/10740 [8:07:28<43:05:39, 16.97s/it]

 15%|█▍        | 1600/10740 [8:07:44<42:19:47, 16.67s/it]

 15%|█▍        | 1601/10740 [8:07:58<40:10:41, 15.83s/it]

 15%|█▍        | 1602/10740 [8:08:19<44:06:23, 17.38s/it]

 15%|█▍        | 1603/10740 [8:08:39<46:06:21, 18.17s/it]

 15%|█▍        | 1604/10740 [8:08:57<46:21:20, 18.27s/it]

 15%|█▍        | 1605/10740 [8:09:14<44:43:59, 17.63s/it]

 15%|█▍        | 1606/10740 [8:09:31<44:40:28, 17.61s/it]
[2024-04-02 03:23:11,908] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▍        | 1607/10740 [8:09:54<48:24:11, 19.08s/it]
[2024-04-02 03:23:25,611] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▍        | 1608/10740 [8:10:07<44:18:21, 17.47s/it]

 15%|█▍        | 1609/10740 [8:10:29<47:34:55, 18.76s/it]

 15%|█▍        | 1610/10740 [8:10:51<49:36:04, 19.56s/it]

 15%|█▌        | 1611/10740 [8:11:12<50:57:37, 20.10s/it]

 15%|█▌        | 1612/10740 [8:11:36<54:09:17, 21.36s/it]


 15%|█▌        | 1614/10740 [8:12:17<53:28:30, 21.09s/it]
{'loss': 0.3485, 'learning_rate': 1.9251582765344126e-06, 'rewards/chosen': -2.619011402130127, 'rewards/rejected': -3.808458089828491, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1894468069076538, 'policy_logps/rejected': -304.3236999511719, 'policy_logps/chosen': -350.03240966796875, 'referece_logps/rejected': -266.2391052246094, 'referece_logps/chosen': -323.8423156738281, 'logits/rejected': -0.060773223638534546, 'logits/chosen': -0.163068950176239, 'epoch': 0.9}

 15%|█▌        | 1615/10740 [8:12:38<53:26:56, 21.09s/it]


 15%|█▌        | 1617/10740 [8:13:17<51:18:09, 20.24s/it]
{'loss': 0.3284, 'learning_rate': 1.9248144716089716e-06, 'rewards/chosen': -2.4603428840637207, 'rewards/rejected': -3.332411050796509, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8720681667327881, 'policy_logps/rejected': -277.024658203125, 'policy_logps/chosen': -298.64556884765625, 'referece_logps/rejected': -243.7005615234375, 'referece_logps/chosen': -274.04217529296875, 'logits/rejected': -0.6304849982261658, 'logits/chosen': -0.6185420751571655, 'epoch': 0.9}

 15%|█▌        | 1618/10740 [8:13:38<51:42:10, 20.40s/it]

 15%|█▌        | 1619/10740 [8:13:58<51:23:23, 20.28s/it]

 15%|█▌        | 1620/10740 [8:14:15<49:12:00, 19.42s/it]

 15%|█▌        | 1621/10740 [8:14:34<48:20:00, 19.08s/it]

 15%|█▌        | 1622/10740 [8:14:50<45:53:01, 18.12s/it]

 15%|█▌        | 1623/10740 [8:15:04<43:24:56, 17.14s/it]
[2024-04-02 03:28:44,457] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1624/10740 [8:15:26<46:55:21, 18.53s/it]

 15%|█▌        | 1625/10740 [8:15:42<44:53:15, 17.73s/it]

 15%|█▌        | 1626/10740 [8:16:03<46:59:41, 18.56s/it]

 15%|█▌        | 1627/10740 [8:16:16<42:56:54, 16.97s/it]

 15%|█▌        | 1628/10740 [8:16:29<40:00:58, 15.81s/it]

 15%|█▌        | 1629/10740 [8:16:48<42:35:05, 16.83s/it]


 15%|█▌        | 1631/10740 [8:17:17<40:07:23, 15.86s/it]

 15%|█▌        | 1632/10740 [8:17:35<41:10:12, 16.27s/it]

 15%|█▌        | 1633/10740 [8:17:55<44:36:08, 17.63s/it]

 15%|█▌        | 1634/10740 [8:18:12<43:32:45, 17.22s/it]

 15%|█▌        | 1635/10740 [8:18:32<45:54:37, 18.15s/it]

 15%|█▌        | 1636/10740 [8:18:52<47:41:32, 18.86s/it]

 15%|█▌        | 1637/10740 [8:19:14<49:49:34, 19.70s/it]

 15%|█▌        | 1638/10740 [8:19:36<51:18:12, 20.29s/it]

 15%|█▌        | 1639/10740 [8:19:57<52:00:14, 20.57s/it]

 15%|█▌        | 1640/10740 [8:20:09<45:27:07, 17.98s/it]

 15%|█▌        | 1641/10740 [8:20:29<46:37:02, 18.44s/it]

 15%|█▌        | 1642/10740 [8:20:44<44:38:23, 17.66s/it]
{'loss': 0.3005, 'learning_rate': 1.921920017520178e-06, 'rewards/chosen': -1.6482762098312378, 'rewards/rejected': -3.7669894695281982, 'rewards/accuracies': 0.75, 'rewards/margins': 2.118713140487671, 'policy_logps/rejected': -505.0107421875, 'policy_logps/chosen': -426.3371887207031, 'referece_logps/rejected': -467.34088134765625, 'referece_logps/chosen': -409.8544616699219, 'logits/rejected': 0.18528318405151367, 'logits/chosen': 0.16922859847545624, 'epoch': 0.92}


 15%|█▌        | 1644/10740 [8:21:18<43:31:09, 17.22s/it]

 15%|█▌        | 1645/10740 [8:21:38<46:07:48, 18.26s/it]

 15%|█▌        | 1646/10740 [8:22:00<48:30:48, 19.20s/it]

 15%|█▌        | 1647/10740 [8:22:19<48:44:26, 19.30s/it]

 15%|█▌        | 1648/10740 [8:22:35<46:23:19, 18.37s/it]

 15%|█▌        | 1649/10740 [8:22:53<45:48:50, 18.14s/it]

 15%|█▌        | 1650/10740 [8:23:17<50:00:26, 19.80s/it]

 15%|█▌        | 1651/10740 [8:23:34<48:00:15, 19.01s/it]

 15%|█▌        | 1652/10740 [8:23:55<49:53:00, 19.76s/it]

 15%|█▌        | 1653/10740 [8:24:17<51:41:36, 20.48s/it]

 15%|█▌        | 1654/10740 [8:24:35<49:19:28, 19.54s/it]

 15%|█▌        | 1655/10740 [8:24:55<50:07:02, 19.86s/it]

 15%|█▌        | 1656/10740 [8:25:15<49:53:04, 19.77s/it]
{'loss': 0.3669, 'learning_rate': 1.9202762213836522e-06, 'rewards/chosen': -2.33355712890625, 'rewards/rejected': -4.228936672210693, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8953793048858643, 'policy_logps/rejected': -330.9794616699219, 'policy_logps/chosen': -371.6042785644531, 'referece_logps/rejected': -288.6900634765625, 'referece_logps/chosen': -348.2686462402344, 'logits/rejected': -0.09380599111318588, 'logits/chosen': -0.012080181390047073, 'epoch': 0.93}


 15%|█▌        | 1658/10740 [8:25:56<50:59:12, 20.21s/it]
[2024-04-02 03:39:14,026] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1659/10740 [8:26:18<52:17:40, 20.73s/it]

 15%|█▌        | 1660/10740 [8:26:40<53:08:52, 21.07s/it]

 15%|█▌        | 1661/10740 [8:26:54<47:46:44, 18.95s/it]

 15%|█▌        | 1662/10740 [8:27:11<46:26:12, 18.42s/it]

 15%|█▌        | 1663/10740 [8:27:28<45:14:34, 17.94s/it]
[2024-04-02 03:40:45,844] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 15%|█▌        | 1664/10740 [8:27:47<46:01:06, 18.25s/it]
[2024-04-02 03:41:04,821] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 16%|█▌        | 1665/10740 [8:28:07<47:43:13, 18.93s/it]
{'loss': 0.3388, 'learning_rate': 1.919210830361644e-06, 'rewards/chosen': -2.590644121170044, 'rewards/rejected': -4.377095699310303, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7864513397216797, 'policy_logps/rejected': -492.67633056640625, 'policy_logps/chosen': -413.587646484375, 'referece_logps/rejected': -448.9054260253906, 'referece_logps/chosen': -387.68121337890625, 'logits/rejected': -0.18546101450920105, 'logits/chosen': 0.01890825293958187, 'epoch': 0.93}


 16%|█▌        | 1667/10740 [8:28:37<42:35:59, 16.90s/it]

 16%|█▌        | 1668/10740 [8:28:50<39:19:54, 15.61s/it]

 16%|█▌        | 1669/10740 [8:29:08<41:33:13, 16.49s/it]

 16%|█▌        | 1670/10740 [8:29:24<40:50:14, 16.21s/it]

 16%|█▌        | 1671/10740 [8:29:36<37:57:39, 15.07s/it]

 16%|█▌        | 1672/10740 [8:30:00<44:21:19, 17.61s/it]
[2024-04-02 03:43:17,947] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 16%|█▌        | 1673/10740 [8:30:15<42:46:38, 16.98s/it]

 16%|█▌        | 1674/10740 [8:30:27<38:51:30, 15.43s/it]

 16%|█▌        | 1675/10740 [8:30:44<40:06:33, 15.93s/it]

 16%|█▌        | 1676/10740 [8:31:03<42:25:06, 16.85s/it]

 16%|█▌        | 1677/10740 [8:31:24<45:43:05, 18.16s/it]

 16%|█▌        | 1678/10740 [8:31:41<44:23:28, 17.64s/it]

 16%|█▌        | 1679/10740 [8:31:58<44:22:16, 17.63s/it]

 16%|█▌        | 1680/10740 [8:32:20<47:10:53, 18.75s/it]

 16%|█▌        | 1681/10740 [8:32:39<47:29:19, 18.87s/it]

 16%|█▌        | 1682/10740 [8:33:00<49:24:24, 19.64s/it]

 16%|█▌        | 1683/10740 [8:33:16<46:15:36, 18.39s/it]

 16%|█▌        | 1684/10740 [8:33:33<45:42:53, 18.17s/it]

 16%|█▌        | 1685/10740 [8:33:54<47:22:41, 18.84s/it]

 16%|█▌        | 1686/10740 [8:34:12<47:03:59, 18.71s/it]

 16%|█▌        | 1687/10740 [8:34:31<47:09:19, 18.75s/it]

 16%|█▌        | 1688/10740 [8:34:54<50:32:04, 20.10s/it]

 16%|█▌        | 1689/10740 [8:35:06<44:13:03, 17.59s/it]

 16%|█▌        | 1690/10740 [8:35:31<49:56:01, 19.86s/it]
[2024-04-02 03:48:49,489] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 16%|█▌        | 1691/10740 [8:35:48<47:14:18, 18.79s/it]
[2024-04-02 03:49:05,784] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2888, 'learning_rate': 1.916095022036457e-06, 'rewards/chosen': -1.7119226455688477, 'rewards/rejected': -2.8627796173095703, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1508569717407227, 'policy_logps/rejected': -424.33221435546875, 'policy_logps/chosen': -360.65264892578125, 'referece_logps/rejected': -395.70440673828125, 'referece_logps/chosen': -343.5334167480469, 'logits/rejected': -0.2763862609863281, 'logits/chosen': -0.21928033232688904, 'epoch': 0.94}


 16%|█▌        | 1693/10740 [8:36:21<45:38:02, 18.16s/it]

 16%|█▌        | 1694/10740 [8:36:39<45:13:20, 18.00s/it]

 16%|█▌        | 1695/10740 [8:36:59<46:26:49, 18.49s/it]

 16%|█▌        | 1696/10740 [8:37:14<44:01:28, 17.52s/it]

 16%|█▌        | 1697/10740 [8:37:26<40:06:50, 15.97s/it]

 16%|█▌        | 1698/10740 [8:37:49<45:21:53, 18.06s/it]

 16%|█▌        | 1699/10740 [8:38:01<40:54:58, 16.29s/it]

 16%|█▌        | 1700/10740 [8:38:20<42:56:45, 17.10s/it]

 16%|█▌        | 1701/10740 [8:38:37<42:45:23, 17.03s/it]

 16%|█▌        | 1702/10740 [8:38:56<43:45:32, 17.43s/it]

 16%|█▌        | 1703/10740 [8:39:06<38:51:08, 15.48s/it]

 16%|█▌        | 1704/10740 [8:39:28<43:11:10, 17.21s/it]

 16%|█▌        | 1705/10740 [8:39:47<45:05:46, 17.97s/it]

 16%|█▌        | 1706/10740 [8:40:08<46:54:11, 18.69s/it]
[2024-04-02 03:53:26,033] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 16%|█▌        | 1707/10740 [8:40:28<47:51:28, 19.07s/it]

 16%|█▌        | 1708/10740 [8:40:41<43:41:04, 17.41s/it]

 16%|█▌        | 1709/10740 [8:40:58<43:10:23, 17.21s/it]

 16%|█▌        | 1710/10740 [8:41:17<44:16:32, 17.65s/it]

 16%|█▌        | 1711/10740 [8:41:35<44:57:27, 17.93s/it]

 16%|█▌        | 1712/10740 [8:41:58<48:11:09, 19.21s/it]

 16%|█▌        | 1713/10740 [8:42:14<46:22:06, 18.49s/it]

 16%|█▌        | 1714/10740 [8:42:32<45:56:35, 18.32s/it]

 16%|█▌        | 1715/10740 [8:42:48<44:19:16, 17.68s/it]

 16%|█▌        | 1716/10740 [8:43:04<43:03:54, 17.18s/it]

 16%|█▌        | 1717/10740 [8:43:26<46:20:18, 18.49s/it]

 16%|█▌        | 1718/10740 [8:43:48<48:44:37, 19.45s/it]

 16%|█▌        | 1719/10740 [8:44:06<47:47:55, 19.08s/it]

 16%|█▌        | 1720/10740 [8:44:22<45:35:45, 18.20s/it]

 16%|█▌        | 1721/10740 [8:44:44<48:09:33, 19.22s/it]

 16%|█▌        | 1722/10740 [8:45:06<50:21:51, 20.11s/it]
{'loss': 0.366, 'learning_rate': 1.9123064222784474e-06, 'rewards/chosen': -2.484184741973877, 'rewards/rejected': -3.2076973915100098, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7235125303268433, 'policy_logps/rejected': -420.4959411621094, 'policy_logps/chosen': -487.8133239746094, 'referece_logps/rejected': -388.4189758300781, 'referece_logps/chosen': -462.97149658203125, 'logits/rejected': -0.012106835842132568, 'logits/chosen': -0.15678898990154266, 'epoch': 0.96}


 16%|█▌        | 1724/10740 [8:45:49<52:51:22, 21.11s/it]

 16%|█▌        | 1725/10740 [8:46:09<51:53:53, 20.72s/it]

 16%|█▌        | 1726/10740 [8:46:25<48:20:00, 19.30s/it]

 16%|█▌        | 1727/10740 [8:46:37<42:37:25, 17.02s/it]

 16%|█▌        | 1728/10740 [8:46:50<39:29:57, 15.78s/it]

 16%|█▌        | 1729/10740 [8:47:03<37:23:54, 14.94s/it]

 16%|█▌        | 1730/10740 [8:47:17<37:01:47, 14.80s/it]

 16%|█▌        | 1731/10740 [8:47:32<37:10:43, 14.86s/it]

 16%|█▌        | 1732/10740 [8:47:50<39:20:36, 15.72s/it]

 16%|█▌        | 1733/10740 [8:48:09<42:14:56, 16.89s/it]

 16%|█▌        | 1734/10740 [8:48:29<43:55:02, 17.56s/it]

 16%|█▌        | 1735/10740 [8:48:50<47:04:32, 18.82s/it]

 16%|█▌        | 1736/10740 [8:49:09<47:02:56, 18.81s/it]

 16%|█▌        | 1737/10740 [8:49:32<50:03:59, 20.02s/it]

 16%|█▌        | 1738/10740 [8:49:49<48:06:06, 19.24s/it]

 16%|█▌        | 1739/10740 [8:50:05<45:30:44, 18.20s/it]

 16%|█▌        | 1740/10740 [8:50:24<46:17:34, 18.52s/it]

 16%|█▌        | 1741/10740 [8:50:35<40:33:57, 16.23s/it]

 16%|█▌        | 1742/10740 [8:50:58<45:25:38, 18.17s/it]
{'loss': 0.3794, 'learning_rate': 1.9098198255840827e-06, 'rewards/chosen': -1.7434698343276978, 'rewards/rejected': -4.038359642028809, 'rewards/accuracies': 0.875, 'rewards/margins': 2.294889450073242, 'policy_logps/rejected': -393.1165466308594, 'policy_logps/chosen': -366.30230712890625, 'referece_logps/rejected': -352.7329406738281, 'referece_logps/chosen': -348.8676452636719, 'logits/rejected': -0.39115530252456665, 'logits/chosen': -0.44021278619766235, 'epoch': 0.97}


 16%|█▌        | 1744/10740 [8:51:21<36:30:44, 14.61s/it]

 16%|█▌        | 1745/10740 [8:51:41<41:11:09, 16.48s/it]

 16%|█▋        | 1746/10740 [8:51:55<38:41:05, 15.48s/it]
{'loss': 0.3793, 'learning_rate': 1.9093185313246098e-06, 'rewards/chosen': -2.4887776374816895, 'rewards/rejected': -6.154449939727783, 'rewards/accuracies': 1.0, 'rewards/margins': 3.665673017501831, 'policy_logps/rejected': -507.633544921875, 'policy_logps/chosen': -480.12701416015625, 'referece_logps/rejected': -446.08905029296875, 'referece_logps/chosen': -455.2392272949219, 'logits/rejected': -0.023999154567718506, 'logits/chosen': 0.1600828468799591, 'epoch': 0.98}


 16%|█▋        | 1748/10740 [8:52:27<40:26:38, 16.19s/it]
{'loss': 0.334, 'learning_rate': 1.909067387920758e-06, 'rewards/chosen': -3.6960995197296143, 'rewards/rejected': -5.510344505310059, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8142454624176025, 'policy_logps/rejected': -309.67999267578125, 'policy_logps/chosen': -493.56396484375, 'referece_logps/rejected': -254.57656860351562, 'referece_logps/chosen': -456.60296630859375, 'logits/rejected': -0.5082459449768066, 'logits/chosen': -0.7728369832038879, 'epoch': 0.98}

 16%|█▋        | 1749/10740 [8:52:38<37:00:31, 14.82s/it]

 16%|█▋        | 1750/10740 [8:52:58<40:33:55, 16.24s/it]


 16%|█▋        | 1752/10740 [8:53:36<43:36:40, 17.47s/it]

 16%|█▋        | 1753/10740 [8:53:53<43:19:41, 17.36s/it]
{'loss': 0.3655, 'learning_rate': 1.9084380826779297e-06, 'rewards/chosen': -1.784580111503601, 'rewards/rejected': -4.231191158294678, 'rewards/accuracies': 0.875, 'rewards/margins': 2.446611166000366, 'policy_logps/rejected': -362.8416748046875, 'policy_logps/chosen': -477.3655700683594, 'referece_logps/rejected': -320.5297546386719, 'referece_logps/chosen': -459.5197448730469, 'logits/rejected': -0.2990438938140869, 'logits/chosen': -0.13309377431869507, 'epoch': 0.98}


 16%|█▋        | 1755/10740 [8:54:31<45:29:32, 18.23s/it]

 16%|█▋        | 1756/10740 [8:54:52<47:19:00, 18.96s/it]
[2024-04-02 04:08:10,021] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 16%|█▋        | 1757/10740 [8:55:04<41:54:23, 16.79s/it]

 16%|█▋        | 1758/10740 [8:55:20<41:18:35, 16.56s/it]

 16%|█▋        | 1759/10740 [8:55:32<38:01:40, 15.24s/it]
{'loss': 0.3852, 'learning_rate': 1.907680189899124e-06, 'rewards/chosen': -1.7866090536117554, 'rewards/rejected': -4.1419358253479, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3553268909454346, 'policy_logps/rejected': -577.195068359375, 'policy_logps/chosen': -383.5337829589844, 'referece_logps/rejected': -535.7757568359375, 'referece_logps/chosen': -365.6676940917969, 'logits/rejected': -0.34869951009750366, 'logits/chosen': -0.011955559253692627, 'epoch': 0.98}


 16%|█▋        | 1761/10740 [8:56:05<40:09:29, 16.10s/it]

 16%|█▋        | 1762/10740 [8:56:18<37:13:38, 14.93s/it]
{'loss': 0.2359, 'learning_rate': 1.9073001288520592e-06, 'rewards/chosen': -1.791099190711975, 'rewards/rejected': -4.560877323150635, 'rewards/accuracies': 1.0, 'rewards/margins': 2.769778251647949, 'policy_logps/rejected': -323.69140625, 'policy_logps/chosen': -343.3524475097656, 'referece_logps/rejected': -278.0826416015625, 'referece_logps/chosen': -325.44146728515625, 'logits/rejected': -0.5317472815513611, 'logits/chosen': -0.3262470066547394, 'epoch': 0.98}

 16%|█▋        | 1763/10740 [8:56:39<41:49:32, 16.77s/it]


 16%|█▋        | 1765/10740 [8:57:16<44:05:09, 17.68s/it]

 16%|█▋        | 1766/10740 [8:57:29<40:49:52, 16.38s/it]

 16%|█▋        | 1767/10740 [8:57:41<37:52:45, 15.20s/it]

 16%|█▋        | 1768/10740 [8:57:59<39:44:52, 15.95s/it]
{'loss': 0.3235, 'learning_rate': 1.9065377789981408e-06, 'rewards/chosen': -2.5074386596679688, 'rewards/rejected': -4.650720596313477, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1432814598083496, 'policy_logps/rejected': -486.29132080078125, 'policy_logps/chosen': -549.7340087890625, 'referece_logps/rejected': -439.78411865234375, 'referece_logps/chosen': -524.6596069335938, 'logits/rejected': -0.14558559656143188, 'logits/chosen': -0.24924609065055847, 'epoch': 0.99}

 16%|█▋        | 1769/10740 [8:58:17<40:58:42, 16.44s/it]


 16%|█▋        | 1771/10740 [8:58:54<44:23:56, 17.82s/it]

 16%|█▋        | 1772/10740 [8:59:10<42:39:44, 17.13s/it]

 17%|█▋        | 1773/10740 [8:59:29<44:36:16, 17.91s/it]

 17%|█▋        | 1774/10740 [8:59:49<46:14:32, 18.57s/it]

 17%|█▋        | 1775/10740 [9:00:09<47:09:41, 18.94s/it]

 17%|█▋        | 1776/10740 [9:00:30<48:21:29, 19.42s/it]

 17%|█▋        | 1777/10740 [9:00:50<49:06:11, 19.72s/it]
{'loss': 0.3831, 'learning_rate': 1.9053886895026357e-06, 'rewards/chosen': -1.867318034172058, 'rewards/rejected': -4.552636623382568, 'rewards/accuracies': 1.0, 'rewards/margins': 2.685318946838379, 'policy_logps/rejected': -218.8304901123047, 'policy_logps/chosen': -295.3092346191406, 'referece_logps/rejected': -173.3041229248047, 'referece_logps/chosen': -276.63604736328125, 'logits/rejected': -0.36782780289649963, 'logits/chosen': -0.229662224650383, 'epoch': 0.99}


 17%|█▋        | 1779/10740 [9:01:22<43:56:34, 17.65s/it]

 17%|█▋        | 1780/10740 [9:01:41<45:26:05, 18.26s/it]

 17%|█▋        | 1781/10740 [9:02:00<45:40:44, 18.36s/it]

 17%|█▋        | 1782/10740 [9:02:14<41:59:35, 16.88s/it]
{'loss': 0.3991, 'learning_rate': 1.9047474237511384e-06, 'rewards/chosen': -2.3370044231414795, 'rewards/rejected': -3.614304304122925, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2773001194000244, 'policy_logps/rejected': -437.9732360839844, 'policy_logps/chosen': -405.3804016113281, 'referece_logps/rejected': -401.8302001953125, 'referece_logps/chosen': -382.0103454589844, 'logits/rejected': -0.8576952219009399, 'logits/chosen': -0.804214358329773, 'epoch': 1.0}


 17%|█▋        | 1784/10740 [9:02:47<42:13:37, 16.97s/it]
{'loss': 0.3447, 'learning_rate': 1.9044903413458532e-06, 'rewards/chosen': -1.909920573234558, 'rewards/rejected': -5.081918716430664, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1719985008239746, 'policy_logps/rejected': -331.9874267578125, 'policy_logps/chosen': -383.0544128417969, 'referece_logps/rejected': -281.168212890625, 'referece_logps/chosen': -363.9552001953125, 'logits/rejected': 0.08097316324710846, 'logits/chosen': 0.07515652477741241, 'epoch': 1.0}

 17%|█▋        | 1785/10740 [9:03:08<45:26:34, 18.27s/it]


 17%|█▋        | 1787/10740 [9:03:48<47:41:44, 19.18s/it]

 17%|█▋        | 1788/10740 [9:04:03<44:45:41, 18.00s/it]

 17%|█▋        | 1789/10740 [9:04:20<43:32:59, 17.52s/it]

 17%|█▋        | 1790/10740 [9:04:37<43:37:34, 17.55s/it]

 17%|█▋        | 1791/10740 [9:04:54<42:40:32, 17.17s/it]
{'loss': 0.3421, 'learning_rate': 1.9035879621754155e-06, 'rewards/chosen': -2.2276382446289062, 'rewards/rejected': -4.178325653076172, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9506874084472656, 'policy_logps/rejected': -247.348388671875, 'policy_logps/chosen': -310.6089172363281, 'referece_logps/rejected': -205.56512451171875, 'referece_logps/chosen': -288.3325500488281, 'logits/rejected': -0.62347012758255, 'logits/chosen': -0.7230800986289978, 'epoch': 1.0}


 17%|█▋        | 1793/10740 [9:05:28<43:25:24, 17.47s/it]

 17%|█▋        | 1794/10740 [9:05:46<43:54:21, 17.67s/it]

 17%|█▋        | 1795/10740 [9:06:08<46:34:23, 18.74s/it]

 17%|█▋        | 1796/10740 [9:06:26<46:26:54, 18.70s/it]

 17%|█▋        | 1797/10740 [9:06:40<43:05:52, 17.35s/it]
{'loss': 0.297, 'learning_rate': 1.9028112890051934e-06, 'rewards/chosen': -2.0511529445648193, 'rewards/rejected': -4.254388332366943, 'rewards/accuracies': 0.875, 'rewards/margins': 2.203235626220703, 'policy_logps/rejected': -712.435791015625, 'policy_logps/chosen': -347.5662536621094, 'referece_logps/rejected': -669.891845703125, 'referece_logps/chosen': -327.05474853515625, 'logits/rejected': -0.8648718595504761, 'logits/chosen': -0.5890496969223022, 'epoch': 1.0}


 17%|█▋        | 1799/10740 [9:07:14<42:35:28, 17.15s/it]
{'loss': 0.4598, 'learning_rate': 1.902551740919559e-06, 'rewards/chosen': -3.546227216720581, 'rewards/rejected': -4.9685211181640625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.422294020652771, 'policy_logps/rejected': -372.0074157714844, 'policy_logps/chosen': -354.716552734375, 'referece_logps/rejected': -322.3221740722656, 'referece_logps/chosen': -319.2542724609375, 'logits/rejected': -0.48535051941871643, 'logits/chosen': -0.5372145175933838, 'epoch': 1.01}

 17%|█▋        | 1800/10740 [9:07:29<40:39:30, 16.37s/it]

 17%|█▋        | 1801/10740 [9:07:51<45:15:43, 18.23s/it]


 17%|█▋        | 1803/10740 [9:08:24<41:50:58, 16.86s/it]

 17%|█▋        | 1804/10740 [9:08:36<38:07:51, 15.36s/it]
{'loss': 0.3071, 'learning_rate': 1.9019014343497067e-06, 'rewards/chosen': -1.9928679466247559, 'rewards/rejected': -4.606272220611572, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6134040355682373, 'policy_logps/rejected': -283.0594482421875, 'policy_logps/chosen': -459.5207214355469, 'referece_logps/rejected': -236.99668884277344, 'referece_logps/chosen': -439.592041015625, 'logits/rejected': 0.03027200698852539, 'logits/chosen': -0.0811222568154335, 'epoch': 1.01}

 17%|█▋        | 1805/10740 [9:08:57<42:51:44, 17.27s/it]

 17%|█▋        | 1806/10740 [9:09:09<38:59:29, 15.71s/it]


 17%|█▋        | 1808/10740 [9:09:42<39:06:25, 15.76s/it]

 17%|█▋        | 1809/10740 [9:10:04<43:49:22, 17.66s/it]

 17%|█▋        | 1810/10740 [9:10:26<46:48:39, 18.87s/it]

 17%|█▋        | 1811/10740 [9:10:38<41:35:13, 16.77s/it]

 17%|█▋        | 1812/10740 [9:10:55<41:35:18, 16.77s/it]

 17%|█▋        | 1813/10740 [9:11:10<40:20:53, 16.27s/it]
{'loss': 0.2859, 'learning_rate': 1.900725715622279e-06, 'rewards/chosen': -1.8163611888885498, 'rewards/rejected': -4.711650371551514, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8952889442443848, 'policy_logps/rejected': -231.69723510742188, 'policy_logps/chosen': -303.6424560546875, 'referece_logps/rejected': -184.58071899414062, 'referece_logps/chosen': -285.4788513183594, 'logits/rejected': -0.2541617453098297, 'logits/chosen': -0.2507372796535492, 'epoch': 1.01}


 17%|█▋        | 1815/10740 [9:11:35<35:23:05, 14.27s/it]

 17%|█▋        | 1816/10740 [9:11:47<33:35:02, 13.55s/it]
{'loss': 0.2483, 'learning_rate': 1.9003323343337204e-06, 'rewards/chosen': -2.287778377532959, 'rewards/rejected': -6.0782856941223145, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7905070781707764, 'policy_logps/rejected': -276.3929748535156, 'policy_logps/chosen': -353.9828186035156, 'referece_logps/rejected': -215.61012268066406, 'referece_logps/chosen': -331.1050720214844, 'logits/rejected': -0.8062232732772827, 'logits/chosen': -0.8394117951393127, 'epoch': 1.01}


 17%|█▋        | 1818/10740 [9:12:19<36:21:45, 14.67s/it]

 17%|█▋        | 1819/10740 [9:12:32<35:06:10, 14.17s/it]

 17%|█▋        | 1820/10740 [9:12:53<40:11:00, 16.22s/it]
{'loss': 0.5542, 'learning_rate': 1.8998066795807146e-06, 'rewards/chosen': -2.6052935123443604, 'rewards/rejected': -4.844658851623535, 'rewards/accuracies': 0.75, 'rewards/margins': 2.239365339279175, 'policy_logps/rejected': -407.59564208984375, 'policy_logps/chosen': -373.90301513671875, 'referece_logps/rejected': -359.14910888671875, 'referece_logps/chosen': -347.85009765625, 'logits/rejected': -0.5795838832855225, 'logits/chosen': -0.6137233972549438, 'epoch': 1.02}


 17%|█▋        | 1822/10740 [9:13:29<42:38:31, 17.21s/it]

 17%|█▋        | 1823/10740 [9:13:46<42:37:34, 17.21s/it]

 17%|█▋        | 1824/10740 [9:14:06<44:44:47, 18.07s/it]

 17%|█▋        | 1825/10740 [9:14:29<48:17:34, 19.50s/it]
{'loss': 0.3566, 'learning_rate': 1.8991477698379467e-06, 'rewards/chosen': -1.5128144025802612, 'rewards/rejected': -3.7544193267822266, 'rewards/accuracies': 1.0, 'rewards/margins': 2.241605043411255, 'policy_logps/rejected': -227.23280334472656, 'policy_logps/chosen': -267.8949890136719, 'referece_logps/rejected': -189.6885986328125, 'referece_logps/chosen': -252.76681518554688, 'logits/rejected': -0.1871941089630127, 'logits/chosen': -0.2324337214231491, 'epoch': 1.02}

 17%|█▋        | 1826/10740 [9:14:46<46:24:36, 18.74s/it]


 17%|█▋        | 1828/10740 [9:15:20<45:19:23, 18.31s/it]
{'loss': 0.3473, 'learning_rate': 1.898751442540047e-06, 'rewards/chosen': -1.4674025774002075, 'rewards/rejected': -3.597443103790283, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1300406455993652, 'policy_logps/rejected': -383.58331298828125, 'policy_logps/chosen': -253.74566650390625, 'referece_logps/rejected': -347.6088562011719, 'referece_logps/chosen': -239.07164001464844, 'logits/rejected': 0.10104361921548843, 'logits/chosen': 0.022213801741600037, 'epoch': 1.02}

 17%|█▋        | 1829/10740 [9:15:33<41:14:00, 16.66s/it]


 17%|█▋        | 1831/10740 [9:16:09<43:09:17, 17.44s/it]

 17%|█▋        | 1832/10740 [9:16:29<44:52:48, 18.14s/it]
{'loss': 0.3191, 'learning_rate': 1.8982218617880067e-06, 'rewards/chosen': -1.5543131828308105, 'rewards/rejected': -3.4259543418884277, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8716413974761963, 'policy_logps/rejected': -269.1658935546875, 'policy_logps/chosen': -359.52392578125, 'referece_logps/rejected': -234.90638732910156, 'referece_logps/chosen': -343.98077392578125, 'logits/rejected': -0.9371174573898315, 'logits/chosen': -0.8495727777481079, 'epoch': 1.02}

 17%|█▋        | 1833/10740 [9:16:43<42:16:47, 17.09s/it]

 17%|█▋        | 1834/10740 [9:17:06<46:14:47, 18.69s/it]


 17%|█▋        | 1836/10740 [9:17:41<44:47:40, 18.11s/it]

 17%|█▋        | 1837/10740 [9:18:00<45:29:29, 18.39s/it]

 17%|█▋        | 1838/10740 [9:18:20<46:55:34, 18.98s/it]

 17%|█▋        | 1839/10740 [9:18:39<46:22:10, 18.75s/it]
{'loss': 0.4443, 'learning_rate': 1.897291950660947e-06, 'rewards/chosen': -2.0001580715179443, 'rewards/rejected': -3.9378252029418945, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9376671314239502, 'policy_logps/rejected': -259.2926330566406, 'policy_logps/chosen': -511.45135498046875, 'referece_logps/rejected': -219.91439819335938, 'referece_logps/chosen': -491.4497375488281, 'logits/rejected': 0.40127286314964294, 'logits/chosen': 0.36472874879837036, 'epoch': 1.03}


 17%|█▋        | 1841/10740 [9:19:11<42:57:26, 17.38s/it]

 17%|█▋        | 1842/10740 [9:19:31<44:53:58, 18.17s/it]
{'loss': 0.3481, 'learning_rate': 1.8968921929153905e-06, 'rewards/chosen': -1.5793095827102661, 'rewards/rejected': -3.769821882247925, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1905126571655273, 'policy_logps/rejected': -359.67633056640625, 'policy_logps/chosen': -344.3575439453125, 'referece_logps/rejected': -321.9781188964844, 'referece_logps/chosen': -328.5644226074219, 'logits/rejected': -0.13881106674671173, 'logits/chosen': -0.09344488382339478, 'epoch': 1.03}

 17%|█▋        | 1843/10740 [9:19:48<43:45:26, 17.71s/it]

 17%|█▋        | 1844/10740 [9:20:08<45:13:12, 18.30s/it]

 17%|█▋        | 1845/10740 [9:20:22<42:10:22, 17.07s/it]


 17%|█▋        | 1847/10740 [9:20:49<37:50:20, 15.32s/it]
{'loss': 0.321, 'learning_rate': 1.8962242986772058e-06, 'rewards/chosen': -2.061387777328491, 'rewards/rejected': -3.5641555786132812, 'rewards/accuracies': 0.875, 'rewards/margins': 1.50276780128479, 'policy_logps/rejected': -364.37799072265625, 'policy_logps/chosen': -373.1480407714844, 'referece_logps/rejected': -328.73638916015625, 'referece_logps/chosen': -352.5341491699219, 'logits/rejected': -0.2908616364002228, 'logits/chosen': -0.21547876298427582, 'epoch': 1.03}


 17%|█▋        | 1849/10740 [9:21:27<42:56:36, 17.39s/it]

 17%|█▋        | 1850/10740 [9:21:51<47:49:44, 19.37s/it]
[2024-04-02 04:35:09,590] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.249, 'learning_rate': 1.8958225838715111e-06, 'rewards/chosen': -2.532881259918213, 'rewards/rejected': -4.170651912689209, 'rewards/accuracies': 0.875, 'rewards/margins': 1.637770652770996, 'policy_logps/rejected': -488.48419189453125, 'policy_logps/chosen': -459.3872985839844, 'referece_logps/rejected': -446.77764892578125, 'referece_logps/chosen': -434.0584716796875, 'logits/rejected': -0.048746298998594284, 'logits/chosen': -0.011567188426852226, 'epoch': 1.03}

 17%|█▋        | 1851/10740 [9:22:10<47:21:05, 19.18s/it]
[2024-04-02 04:35:49,942] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 17%|█▋        | 1853/10740 [9:22:43<42:45:20, 17.32s/it]

 17%|█▋        | 1854/10740 [9:23:05<45:52:52, 18.59s/it]

 17%|█▋        | 1855/10740 [9:23:23<45:43:14, 18.53s/it]

 17%|█▋        | 1856/10740 [9:23:35<40:45:57, 16.52s/it]

 17%|█▋        | 1857/10740 [9:23:53<42:14:03, 17.12s/it]
{'loss': 0.2696, 'learning_rate': 1.8948823981904064e-06, 'rewards/chosen': -3.4984753131866455, 'rewards/rejected': -5.435392379760742, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9369169473648071, 'policy_logps/rejected': -369.70281982421875, 'policy_logps/chosen': -408.4562072753906, 'referece_logps/rejected': -315.3489074707031, 'referece_logps/chosen': -373.471435546875, 'logits/rejected': 0.5462916493415833, 'logits/chosen': 0.4974210858345032, 'epoch': 1.04}
[2024-04-02 04:37:32,801] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 1858/10740 [9:24:15<45:20:01, 18.37s/it]
[2024-04-02 04:37:54,854] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 17%|█▋        | 1860/10740 [9:24:58<49:10:43, 19.94s/it]
[2024-04-02 04:38:15,863] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3884, 'learning_rate': 1.8944782403491962e-06, 'rewards/chosen': -3.2129600048065186, 'rewards/rejected': -5.165706157684326, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9527456760406494, 'policy_logps/rejected': -476.4031982421875, 'policy_logps/chosen': -401.72369384765625, 'referece_logps/rejected': -424.74609375, 'referece_logps/chosen': -369.5941162109375, 'logits/rejected': 0.21760134398937225, 'logits/chosen': 0.2590716481208801, 'epoch': 1.04}
[2024-04-02 04:38:38,196] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 17%|█▋        | 1861/10740 [9:25:20<50:56:42, 20.66s/it]

 17%|█▋        | 1862/10740 [9:25:39<49:29:54, 20.07s/it]

 17%|█▋        | 1863/10740 [9:25:57<48:04:41, 19.50s/it]
[2024-04-02 04:39:36,474] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 17%|█▋        | 1865/10740 [9:26:36<47:36:43, 19.31s/it]
{'loss': 0.4383, 'learning_rate': 1.8938030170111307e-06, 'rewards/chosen': -3.4872498512268066, 'rewards/rejected': -4.628655910491943, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1414058208465576, 'policy_logps/rejected': -383.2496337890625, 'policy_logps/chosen': -373.6654052734375, 'referece_logps/rejected': -336.96307373046875, 'referece_logps/chosen': -338.7929382324219, 'logits/rejected': -1.0819406509399414, 'logits/chosen': -1.0792893171310425, 'epoch': 1.04}

 17%|█▋        | 1866/10740 [9:26:57<48:52:12, 19.83s/it]


 17%|█▋        | 1868/10740 [9:27:32<45:40:29, 18.53s/it]
{'loss': 0.4147, 'learning_rate': 1.8933969073870992e-06, 'rewards/chosen': -1.4638655185699463, 'rewards/rejected': -3.0671191215515137, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6032536029815674, 'policy_logps/rejected': -279.29254150390625, 'policy_logps/chosen': -217.79396057128906, 'referece_logps/rejected': -248.621337890625, 'referece_logps/chosen': -203.15530395507812, 'logits/rejected': -0.6688305139541626, 'logits/chosen': -0.5498285293579102, 'epoch': 1.04}


 17%|█▋        | 1870/10740 [9:28:08<46:02:15, 18.68s/it]

 17%|█▋        | 1871/10740 [9:28:24<43:59:18, 17.86s/it]

 17%|█▋        | 1872/10740 [9:28:40<42:35:25, 17.29s/it]
{'loss': 0.3953, 'learning_rate': 1.8928542903530466e-06, 'rewards/chosen': -1.5483789443969727, 'rewards/rejected': -2.339050054550171, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7906709313392639, 'policy_logps/rejected': -316.7083740234375, 'policy_logps/chosen': -338.738525390625, 'referece_logps/rejected': -293.31787109375, 'referece_logps/chosen': -323.2547302246094, 'logits/rejected': 0.5890501737594604, 'logits/chosen': 0.6002234220504761, 'epoch': 1.05}


 17%|█▋        | 1874/10740 [9:29:19<46:34:12, 18.91s/it]
{'loss': 0.3636, 'learning_rate': 1.8925824945429254e-06, 'rewards/chosen': -1.7000874280929565, 'rewards/rejected': -2.7950806617736816, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0949931144714355, 'policy_logps/rejected': -337.903076171875, 'policy_logps/chosen': -373.88946533203125, 'referece_logps/rejected': -309.9522705078125, 'referece_logps/chosen': -356.88861083984375, 'logits/rejected': -0.8638023734092712, 'logits/chosen': -0.9413400888442993, 'epoch': 1.05}


 17%|█▋        | 1876/10740 [9:29:58<46:11:02, 18.76s/it]
{'loss': 0.4219, 'learning_rate': 1.892310374002544e-06, 'rewards/chosen': -1.413522720336914, 'rewards/rejected': -2.958468437194824, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5449455976486206, 'policy_logps/rejected': -176.45265197753906, 'policy_logps/chosen': -402.0003662109375, 'referece_logps/rejected': -146.8679656982422, 'referece_logps/chosen': -387.8651428222656, 'logits/rejected': -0.43240267038345337, 'logits/chosen': -0.4701044261455536, 'epoch': 1.05}

 17%|█▋        | 1877/10740 [9:30:19<48:12:21, 19.58s/it]


 17%|█▋        | 1879/10740 [9:31:02<50:39:00, 20.58s/it]
[2024-04-02 04:44:19,817] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2879, 'learning_rate': 1.8919015845393295e-06, 'rewards/chosen': -2.237837076187134, 'rewards/rejected': -3.0097262859344482, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7718890309333801, 'policy_logps/rejected': -423.9652404785156, 'policy_logps/chosen': -503.12457275390625, 'referece_logps/rejected': -393.86798095703125, 'referece_logps/chosen': -480.7462158203125, 'logits/rejected': -0.011571034789085388, 'logits/chosen': 0.017577387392520905, 'epoch': 1.05}

 18%|█▊        | 1880/10740 [9:31:23<51:05:31, 20.76s/it]

 18%|█▊        | 1881/10740 [9:31:40<48:09:18, 19.57s/it]

 18%|█▊        | 1882/10740 [9:31:57<46:41:08, 18.97s/it]

 18%|█▊        | 1883/10740 [9:32:17<47:13:19, 19.19s/it]

 18%|█▊        | 1884/10740 [9:32:34<45:20:42, 18.43s/it]
[2024-04-02 04:46:13,827] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 1885/10740 [9:32:56<48:02:08, 19.53s/it]

 18%|█▊        | 1886/10740 [9:33:17<49:24:40, 20.09s/it]
[2024-04-02 04:46:47,855] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 1887/10740 [9:33:30<43:53:57, 17.85s/it]


 18%|█▊        | 1889/10740 [9:34:00<39:35:49, 16.11s/it]
{'loss': 0.2635, 'learning_rate': 1.8905336820383834e-06, 'rewards/chosen': -2.815831184387207, 'rewards/rejected': -4.4778032302856445, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6619720458984375, 'policy_logps/rejected': -299.7216491699219, 'policy_logps/chosen': -336.3324279785156, 'referece_logps/rejected': -254.94363403320312, 'referece_logps/chosen': -308.17413330078125, 'logits/rejected': -0.7196422815322876, 'logits/chosen': -0.6923296451568604, 'epoch': 1.06}


 18%|█▊        | 1891/10740 [9:34:30<37:28:15, 15.24s/it]
{'loss': 0.3594, 'learning_rate': 1.8902591291848334e-06, 'rewards/chosen': -2.405392646789551, 'rewards/rejected': -5.063356399536133, 'rewards/accuracies': 0.625, 'rewards/margins': 2.657963275909424, 'policy_logps/rejected': -349.451171875, 'policy_logps/chosen': -438.06134033203125, 'referece_logps/rejected': -298.817626953125, 'referece_logps/chosen': -414.0074157714844, 'logits/rejected': -1.443669080734253, 'logits/chosen': -1.4842467308044434, 'epoch': 1.06}

 18%|█▊        | 1892/10740 [9:34:47<38:20:26, 15.60s/it]

 18%|█▊        | 1893/10740 [9:35:04<40:03:18, 16.30s/it]


 18%|█▊        | 1895/10740 [9:35:44<44:33:21, 18.13s/it]

 18%|█▊        | 1896/10740 [9:36:06<47:22:51, 19.29s/it]

 18%|█▊        | 1897/10740 [9:36:24<46:17:10, 18.84s/it]

 18%|█▊        | 1898/10740 [9:36:42<45:34:24, 18.56s/it]
[2024-04-02 04:50:00,255] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3666, 'learning_rate': 1.8892956442596155e-06, 'rewards/chosen': -2.189727783203125, 'rewards/rejected': -3.4726014137268066, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2828737497329712, 'policy_logps/rejected': -410.15399169921875, 'policy_logps/chosen': -395.1607666015625, 'referece_logps/rejected': -375.427978515625, 'referece_logps/chosen': -373.2634582519531, 'logits/rejected': 0.39156675338745117, 'logits/chosen': 0.4604817032814026, 'epoch': 1.06}

 18%|█▊        | 1899/10740 [9:36:56<42:00:12, 17.10s/it]
[2024-04-02 04:50:33,827] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 18%|█▊        | 1901/10740 [9:37:38<47:26:18, 19.32s/it]
[2024-04-02 04:50:56,396] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4187, 'learning_rate': 1.8888815086439522e-06, 'rewards/chosen': -2.9189364910125732, 'rewards/rejected': -3.768996000289917, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8500593900680542, 'policy_logps/rejected': -364.7944030761719, 'policy_logps/chosen': -405.3131103515625, 'referece_logps/rejected': -327.10443115234375, 'referece_logps/chosen': -376.1237487792969, 'logits/rejected': 1.0809977054595947, 'logits/chosen': 1.1130183935165405, 'epoch': 1.06}


 18%|█▊        | 1903/10740 [9:38:12<43:31:24, 17.73s/it]
{'loss': 0.2789, 'learning_rate': 1.8886050139828186e-06, 'rewards/chosen': -1.9225198030471802, 'rewards/rejected': -2.866244316101074, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9437243938446045, 'policy_logps/rejected': -341.16510009765625, 'policy_logps/chosen': -367.868408203125, 'referece_logps/rejected': -312.50262451171875, 'referece_logps/chosen': -348.6432189941406, 'logits/rejected': -0.42546218633651733, 'logits/chosen': -0.4358697831630707, 'epoch': 1.06}

 18%|█▊        | 1904/10740 [9:38:26<40:22:23, 16.45s/it]

 18%|█▊        | 1905/10740 [9:38:41<39:53:41, 16.26s/it]
[2024-04-02 04:52:21,312] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 1906/10740 [9:39:03<43:51:41, 17.87s/it]

 18%|█▊        | 1907/10740 [9:39:25<46:44:57, 19.05s/it]


 18%|█▊        | 1909/10740 [9:40:06<49:12:14, 20.06s/it]
[2024-04-02 04:53:24,609] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 1910/10740 [9:40:26<49:02:27, 19.99s/it]
{'loss': 0.2677, 'learning_rate': 1.8876347374747347e-06, 'rewards/chosen': -2.383394956588745, 'rewards/rejected': -6.037197589874268, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6538023948669434, 'policy_logps/rejected': -387.3124084472656, 'policy_logps/chosen': -355.1686096191406, 'referece_logps/rejected': -326.9403991699219, 'referece_logps/chosen': -331.3346862792969, 'logits/rejected': -0.48826056718826294, 'logits/chosen': -0.4811789095401764, 'epoch': 1.07}

 18%|█▊        | 1911/10740 [9:40:46<48:48:28, 19.90s/it]


 18%|█▊        | 1913/10740 [9:41:24<47:51:25, 19.52s/it]
[2024-04-02 04:54:42,529] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3783, 'learning_rate': 1.887217693444889e-06, 'rewards/chosen': -2.1638474464416504, 'rewards/rejected': -4.794339656829834, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6304919719696045, 'policy_logps/rejected': -320.5362548828125, 'policy_logps/chosen': -415.9522705078125, 'referece_logps/rejected': -272.59283447265625, 'referece_logps/chosen': -394.31378173828125, 'logits/rejected': -0.1850205957889557, 'logits/chosen': -0.19462552666664124, 'epoch': 1.07}

 18%|█▊        | 1914/10740 [9:41:41<45:53:50, 18.72s/it]


 18%|█▊        | 1916/10740 [9:42:23<48:38:51, 19.85s/it]

 18%|█▊        | 1917/10740 [9:42:44<50:02:13, 20.42s/it]
{'loss': 0.3173, 'learning_rate': 1.8866605050728354e-06, 'rewards/chosen': -2.8261425495147705, 'rewards/rejected': -4.797057628631592, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9709148406982422, 'policy_logps/rejected': -344.39971923828125, 'policy_logps/chosen': -299.3927001953125, 'referece_logps/rejected': -296.42913818359375, 'referece_logps/chosen': -271.13128662109375, 'logits/rejected': -0.5420022010803223, 'logits/chosen': -0.29379233717918396, 'epoch': 1.07}


 18%|█▊        | 1919/10740 [9:43:20<46:09:25, 18.84s/it]
{'loss': 0.3306, 'learning_rate': 1.8863814269724284e-06, 'rewards/chosen': -2.1054491996765137, 'rewards/rejected': -4.437014579772949, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3315653800964355, 'policy_logps/rejected': -351.72760009765625, 'policy_logps/chosen': -278.3202819824219, 'referece_logps/rejected': -307.35748291015625, 'referece_logps/chosen': -257.2657775878906, 'logits/rejected': -0.526022732257843, 'logits/chosen': -0.5774866342544556, 'epoch': 1.07}


 18%|█▊        | 1921/10740 [9:43:54<43:55:43, 17.93s/it]
{'loss': 0.2873, 'learning_rate': 1.8861020263977702e-06, 'rewards/chosen': -2.591764211654663, 'rewards/rejected': -6.07789421081543, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4861299991607666, 'policy_logps/rejected': -408.9073791503906, 'policy_logps/chosen': -341.6630554199219, 'referece_logps/rejected': -348.12841796875, 'referece_logps/chosen': -315.74542236328125, 'logits/rejected': -0.5233049988746643, 'logits/chosen': -0.4135040044784546, 'epoch': 1.07}

 18%|█▊        | 1922/10740 [9:44:12<43:33:04, 17.78s/it]

 18%|█▊        | 1923/10740 [9:44:25<40:28:02, 16.52s/it]

 18%|█▊        | 1924/10740 [9:44:40<39:05:50, 15.97s/it]
[2024-04-02 04:58:18,287] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 18%|█▊        | 1926/10740 [9:45:19<42:58:13, 17.55s/it]
{'loss': 0.2798, 'learning_rate': 1.885402114803635e-06, 'rewards/chosen': -2.251437187194824, 'rewards/rejected': -4.0409393310546875, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7895017862319946, 'policy_logps/rejected': -308.78521728515625, 'policy_logps/chosen': -465.74993896484375, 'referece_logps/rejected': -268.3758544921875, 'referece_logps/chosen': -443.2355651855469, 'logits/rejected': -0.6214518547058105, 'logits/chosen': -0.6872130632400513, 'epoch': 1.08}

 18%|█▊        | 1927/10740 [9:45:36<42:52:52, 17.52s/it]


 18%|█▊        | 1929/10740 [9:46:09<40:55:19, 16.72s/it]

 18%|█▊        | 1930/10740 [9:46:29<43:28:10, 17.76s/it]

 18%|█▊        | 1931/10740 [9:46:51<46:31:52, 19.02s/it]
[2024-04-02 05:00:09,023] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3561, 'learning_rate': 1.8847001899725233e-06, 'rewards/chosen': -2.5429885387420654, 'rewards/rejected': -3.9512665271759033, 'rewards/accuracies': 0.75, 'rewards/margins': 1.408277988433838, 'policy_logps/rejected': -457.7493591308594, 'policy_logps/chosen': -533.2147216796875, 'referece_logps/rejected': -418.23675537109375, 'referece_logps/chosen': -507.784912109375, 'logits/rejected': -0.7751621007919312, 'logits/chosen': -0.6040643453598022, 'epoch': 1.08}
[2024-04-02 05:00:23,517] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 1932/10740 [9:47:05<43:12:23, 17.66s/it]

 18%|█▊        | 1933/10740 [9:47:22<42:42:38, 17.46s/it]

 18%|█▊        | 1934/10740 [9:47:43<45:21:12, 18.54s/it]

 18%|█▊        | 1935/10740 [9:48:03<46:27:22, 18.99s/it]


 18%|█▊        | 1937/10740 [9:48:43<47:12:51, 19.31s/it]
{'loss': 0.2883, 'learning_rate': 1.883855224949932e-06, 'rewards/chosen': -2.8121562004089355, 'rewards/rejected': -4.871029853820801, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0588736534118652, 'policy_logps/rejected': -428.9463195800781, 'policy_logps/chosen': -389.89324951171875, 'referece_logps/rejected': -380.2359924316406, 'referece_logps/chosen': -361.771728515625, 'logits/rejected': -0.46122777462005615, 'logits/chosen': -0.48965558409690857, 'epoch': 1.08}

 18%|█▊        | 1938/10740 [9:49:00<45:34:50, 18.64s/it]

 18%|█▊        | 1939/10740 [9:49:17<43:59:41, 18.00s/it]


 18%|█▊        | 1941/10740 [9:49:53<44:38:54, 18.27s/it]
{'loss': 0.3594, 'learning_rate': 1.8832903069881205e-06, 'rewards/chosen': -4.026947498321533, 'rewards/rejected': -7.180943965911865, 'rewards/accuracies': 1.0, 'rewards/margins': 3.153996467590332, 'policy_logps/rejected': -466.9632263183594, 'policy_logps/chosen': -532.3651733398438, 'referece_logps/rejected': -395.15380859375, 'referece_logps/chosen': -492.0957336425781, 'logits/rejected': -0.8394994735717773, 'logits/chosen': -0.8227344155311584, 'epoch': 1.08}
[2024-04-02 05:03:34,063] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 18%|█▊        | 1942/10740 [9:50:16<47:55:18, 19.61s/it]

 18%|█▊        | 1943/10740 [9:50:36<48:00:07, 19.64s/it]

 18%|█▊        | 1944/10740 [9:50:57<48:59:05, 20.05s/it]

 18%|█▊        | 1945/10740 [9:51:16<48:49:02, 19.98s/it]


 18%|█▊        | 1947/10740 [9:51:57<49:26:42, 20.24s/it]
{'loss': 0.3427, 'learning_rate': 1.8824405201805034e-06, 'rewards/chosen': -2.8213706016540527, 'rewards/rejected': -4.808884143829346, 'rewards/accuracies': 0.75, 'rewards/margins': 1.987513542175293, 'policy_logps/rejected': -398.7352294921875, 'policy_logps/chosen': -379.9617919921875, 'referece_logps/rejected': -350.64642333984375, 'referece_logps/chosen': -351.7480773925781, 'logits/rejected': -0.6633977890014648, 'logits/chosen': -0.7576299905776978, 'epoch': 1.09}


 18%|█▊        | 1949/10740 [9:52:33<45:32:54, 18.65s/it]
{'loss': 0.2937, 'learning_rate': 1.8821566156927552e-06, 'rewards/chosen': -2.8351454734802246, 'rewards/rejected': -5.0116424560546875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.176496982574463, 'policy_logps/rejected': -386.1020202636719, 'policy_logps/chosen': -368.0174865722656, 'referece_logps/rejected': -335.9856262207031, 'referece_logps/chosen': -339.666015625, 'logits/rejected': -1.0805554389953613, 'logits/chosen': -1.0819644927978516, 'epoch': 1.09}

 18%|█▊        | 1950/10740 [9:52:50<44:29:54, 18.22s/it]


 18%|█▊        | 1952/10740 [9:53:28<45:10:22, 18.51s/it]
{'loss': 0.348, 'learning_rate': 1.8817301572361491e-06, 'rewards/chosen': -2.5606918334960938, 'rewards/rejected': -3.1885738372802734, 'rewards/accuracies': 0.625, 'rewards/margins': 0.627881646156311, 'policy_logps/rejected': -448.7369079589844, 'policy_logps/chosen': -477.0135192871094, 'referece_logps/rejected': -416.8511657714844, 'referece_logps/chosen': -451.4066467285156, 'logits/rejected': -0.06269991397857666, 'logits/chosen': -0.06361177563667297, 'epoch': 1.09}

 18%|█▊        | 1953/10740 [9:53:44<43:51:37, 17.97s/it]

 18%|█▊        | 1954/10740 [9:54:07<47:13:46, 19.35s/it]


 18%|█▊        | 1956/10740 [9:54:46<47:10:13, 19.33s/it]

 18%|█▊        | 1957/10740 [9:55:04<46:13:00, 18.94s/it]
{'loss': 0.2803, 'learning_rate': 1.8810177894041547e-06, 'rewards/chosen': -2.1184072494506836, 'rewards/rejected': -3.5659186840057373, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4475113153457642, 'policy_logps/rejected': -257.0660095214844, 'policy_logps/chosen': -307.6420593261719, 'referece_logps/rejected': -221.4068145751953, 'referece_logps/chosen': -286.4579772949219, 'logits/rejected': -0.6277074813842773, 'logits/chosen': -0.6145378351211548, 'epoch': 1.09}

 18%|█▊        | 1958/10740 [9:55:17<42:01:19, 17.23s/it]

 18%|█▊        | 1959/10740 [9:55:34<42:12:55, 17.31s/it]

 18%|█▊        | 1960/10740 [9:55:49<40:23:13, 16.56s/it]

 18%|█▊        | 1961/10740 [9:56:07<41:30:30, 17.02s/it]

 18%|█▊        | 1962/10740 [9:56:18<36:48:59, 15.10s/it]

 18%|█▊        | 1963/10740 [9:56:36<38:48:26, 15.92s/it]

 18%|█▊        | 1964/10740 [9:56:51<38:03:30, 15.61s/it]


 18%|█▊        | 1966/10740 [9:57:30<43:43:48, 17.94s/it]
{'loss': 0.3585, 'learning_rate': 1.8797304801629901e-06, 'rewards/chosen': -1.9048528671264648, 'rewards/rejected': -5.787137031555176, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8822836875915527, 'policy_logps/rejected': -261.992919921875, 'policy_logps/chosen': -348.3650817871094, 'referece_logps/rejected': -204.12156677246094, 'referece_logps/chosen': -329.3165588378906, 'logits/rejected': -0.1876862794160843, 'logits/chosen': -0.1683824211359024, 'epoch': 1.1}

 18%|█▊        | 1967/10740 [9:57:42<39:35:27, 16.25s/it]

 18%|█▊        | 1968/10740 [9:57:57<38:47:12, 15.92s/it]

 18%|█▊        | 1969/10740 [9:58:12<37:45:03, 15.49s/it]

 18%|█▊        | 1970/10740 [9:58:31<40:18:52, 16.55s/it]

 18%|█▊        | 1971/10740 [9:58:48<40:29:39, 16.62s/it]

 18%|█▊        | 1972/10740 [9:59:09<44:00:16, 18.07s/it]

 18%|█▊        | 1973/10740 [9:59:27<44:07:34, 18.12s/it]

 18%|█▊        | 1974/10740 [9:59:48<45:47:04, 18.80s/it]

 18%|█▊        | 1975/10740 [10:00:08<47:08:58, 19.37s/it]

 18%|█▊        | 1976/10740 [10:00:29<48:20:03, 19.85s/it]

 18%|█▊        | 1977/10740 [10:00:49<47:59:19, 19.71s/it]

 18%|█▊        | 1978/10740 [10:01:08<47:53:54, 19.68s/it]

 18%|█▊        | 1979/10740 [10:01:19<41:11:54, 16.93s/it]

 18%|█▊        | 1980/10740 [10:01:30<36:51:08, 15.14s/it]

 18%|█▊        | 1981/10740 [10:01:41<33:38:23, 13.83s/it]

 18%|█▊        | 1982/10740 [10:01:51<31:24:36, 12.91s/it]

 18%|█▊        | 1983/10740 [10:02:08<33:50:54, 13.92s/it]

 18%|█▊        | 1984/10740 [10:02:27<38:00:40, 15.63s/it]

 18%|█▊        | 1985/10740 [10:02:45<39:47:54, 16.36s/it]

 18%|█▊        | 1986/10740 [10:02:58<37:03:16, 15.24s/it]

 19%|█▊        | 1987/10740 [10:03:09<33:55:37, 13.95s/it]

 19%|█▊        | 1988/10740 [10:03:27<36:47:14, 15.13s/it]

 19%|█▊        | 1989/10740 [10:03:39<34:59:31, 14.40s/it]

 19%|█▊        | 1990/10740 [10:03:51<33:09:22, 13.64s/it]


 19%|█▊        | 1992/10740 [10:04:27<39:22:50, 16.21s/it]

 19%|█▊        | 1993/10740 [10:04:49<43:03:31, 17.72s/it]

 19%|█▊        | 1994/10740 [10:04:59<38:04:38, 15.67s/it]

 19%|█▊        | 1995/10740 [10:05:17<39:19:41, 16.19s/it]

 19%|█▊        | 1996/10740 [10:05:33<39:28:21, 16.25s/it]

 19%|█▊        | 1997/10740 [10:05:52<41:29:17, 17.08s/it]

 19%|█▊        | 1998/10740 [10:06:04<37:51:37, 15.59s/it]

 19%|█▊        | 1999/10740 [10:06:19<37:30:25, 15.45s/it]

 19%|█▊        | 2000/10740 [10:06:33<36:10:03, 14.90s/it]

 19%|█▊        | 2001/10740 [10:07:02<46:25:44, 19.13s/it]

 19%|█▊        | 2002/10740 [10:07:13<40:19:27, 16.61s/it]

 19%|█▊        | 2003/10740 [10:07:32<42:27:54, 17.50s/it]

 19%|█▊        | 2004/10740 [10:07:44<38:20:50, 15.80s/it]

 19%|█▊        | 2005/10740 [10:08:04<40:58:29, 16.89s/it]

 19%|█▊        | 2006/10740 [10:08:21<40:58:16, 16.89s/it]

 19%|█▊        | 2007/10740 [10:08:34<38:30:23, 15.87s/it]

 19%|█▊        | 2008/10740 [10:08:54<41:18:48, 17.03s/it]

 19%|█▊        | 2009/10740 [10:09:11<41:08:25, 16.96s/it]

 19%|█▊        | 2010/10740 [10:09:32<44:07:55, 18.20s/it]

 19%|█▊        | 2011/10740 [10:09:46<41:04:03, 16.94s/it]

 19%|█▊        | 2012/10740 [10:10:02<40:34:37, 16.74s/it]

 19%|█▊        | 2013/10740 [10:10:15<38:12:33, 15.76s/it]

 19%|█▉        | 2014/10740 [10:10:36<41:26:07, 17.09s/it]

 19%|█▉        | 2015/10740 [10:10:54<42:32:53, 17.56s/it]

 19%|█▉        | 2016/10740 [10:11:15<44:58:03, 18.56s/it]

 19%|█▉        | 2017/10740 [10:11:38<47:53:59, 19.77s/it]
[2024-04-02 05:24:55,970] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2018/10740 [10:11:58<47:55:45, 19.78s/it]
{'loss': 0.268, 'learning_rate': 1.8721660905011905e-06, 'rewards/chosen': -2.4455976486206055, 'rewards/rejected': -4.859529972076416, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4139323234558105, 'policy_logps/rejected': -217.54296875, 'policy_logps/chosen': -332.5379943847656, 'referece_logps/rejected': -168.94766235351562, 'referece_logps/chosen': -308.08203125, 'logits/rejected': -0.452709823846817, 'logits/chosen': -0.43602344393730164, 'epoch': 1.13}
[2024-04-02 05:25:36,977] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 19%|█▉        | 2020/10740 [10:12:34<45:01:54, 18.59s/it]
[2024-04-02 05:25:51,803] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2021/10740 [10:12:53<45:45:04, 18.89s/it]

 19%|█▉        | 2022/10740 [10:13:10<43:54:43, 18.13s/it]

 19%|█▉        | 2023/10740 [10:13:26<43:00:09, 17.76s/it]

 19%|█▉        | 2024/10740 [10:13:46<44:21:15, 18.32s/it]

 19%|█▉        | 2025/10740 [10:14:06<45:33:45, 18.82s/it]
[2024-04-02 05:27:24,263] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2026/10740 [10:14:26<46:13:52, 19.10s/it]

 19%|█▉        | 2027/10740 [10:14:42<44:15:51, 18.29s/it]

 19%|█▉        | 2028/10740 [10:14:58<42:17:01, 17.47s/it]

 19%|█▉        | 2029/10740 [10:15:16<42:30:17, 17.57s/it]

 19%|█▉        | 2030/10740 [10:15:28<38:56:05, 16.09s/it]

 19%|█▉        | 2031/10740 [10:15:46<39:53:31, 16.49s/it]

 19%|█▉        | 2032/10740 [10:15:59<37:56:24, 15.68s/it]

 19%|█▉        | 2033/10740 [10:16:13<36:31:25, 15.10s/it]

 19%|█▉        | 2034/10740 [10:16:32<38:54:16, 16.09s/it]

 19%|█▉        | 2035/10740 [10:16:49<39:54:25, 16.50s/it]

 19%|█▉        | 2036/10740 [10:17:07<40:55:54, 16.93s/it]

 19%|█▉        | 2037/10740 [10:17:25<41:36:39, 17.21s/it]

 19%|█▉        | 2038/10740 [10:17:45<43:48:06, 18.12s/it]

 19%|█▉        | 2039/10740 [10:18:00<41:20:13, 17.10s/it]

 19%|█▉        | 2040/10740 [10:18:17<41:12:42, 17.05s/it]

 19%|█▉        | 2041/10740 [10:18:31<39:10:31, 16.21s/it]

 19%|█▉        | 2042/10740 [10:18:49<40:43:34, 16.86s/it]

 19%|█▉        | 2043/10740 [10:19:05<39:51:30, 16.50s/it]

 19%|█▉        | 2044/10740 [10:19:26<43:14:07, 17.90s/it]
[2024-04-02 05:32:44,382] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2045/10740 [10:19:46<44:51:41, 18.57s/it]
[2024-04-02 05:33:04,532] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2046/10740 [10:20:03<43:12:26, 17.89s/it]

 19%|█▉        | 2047/10740 [10:20:24<45:51:37, 18.99s/it]

 19%|█▉        | 2048/10740 [10:20:43<46:02:15, 19.07s/it]

 19%|█▉        | 2049/10740 [10:21:02<46:01:14, 19.06s/it]
[2024-04-02 05:34:20,686] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2050/10740 [10:21:24<48:08:34, 19.94s/it]

 19%|█▉        | 2051/10740 [10:21:47<50:13:44, 20.81s/it]

 19%|█▉        | 2052/10740 [10:22:08<50:20:49, 20.86s/it]
[2024-04-02 05:35:26,501] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2053/10740 [10:22:28<49:33:43, 20.54s/it]

 19%|█▉        | 2054/10740 [10:22:47<48:05:11, 19.93s/it]

 19%|█▉        | 2055/10740 [10:23:09<49:41:13, 20.60s/it]

 19%|█▉        | 2056/10740 [10:23:32<51:52:37, 21.51s/it]

 19%|█▉        | 2057/10740 [10:23:46<45:54:46, 19.04s/it]

 19%|█▉        | 2058/10740 [10:24:03<44:41:01, 18.53s/it]

 19%|█▉        | 2059/10740 [10:24:23<45:41:33, 18.95s/it]

 19%|█▉        | 2060/10740 [10:24:42<46:08:20, 19.14s/it]

 19%|█▉        | 2061/10740 [10:25:04<47:38:36, 19.76s/it]

 19%|█▉        | 2062/10740 [10:25:24<48:15:45, 20.02s/it]

 19%|█▉        | 2063/10740 [10:25:46<49:20:41, 20.47s/it]

 19%|█▉        | 2064/10740 [10:26:04<47:28:46, 19.70s/it]

 19%|█▉        | 2065/10740 [10:26:20<44:39:44, 18.53s/it]

 19%|█▉        | 2066/10740 [10:26:37<43:41:01, 18.13s/it]

 19%|█▉        | 2067/10740 [10:26:52<41:52:07, 17.38s/it]

 19%|█▉        | 2068/10740 [10:27:10<42:17:31, 17.56s/it]

 19%|█▉        | 2069/10740 [10:27:29<43:15:53, 17.96s/it]

 19%|█▉        | 2070/10740 [10:27:49<44:13:32, 18.36s/it]

 19%|█▉        | 2071/10740 [10:28:10<46:42:09, 19.39s/it]

 19%|█▉        | 2072/10740 [10:28:30<46:38:30, 19.37s/it]

 19%|█▉        | 2073/10740 [10:28:43<41:56:32, 17.42s/it]
{'loss': 0.4288, 'learning_rate': 1.8639319225914884e-06, 'rewards/chosen': -1.9709479808807373, 'rewards/rejected': -4.862181663513184, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8912336826324463, 'policy_logps/rejected': -480.1427917480469, 'policy_logps/chosen': -488.9968566894531, 'referece_logps/rejected': -431.52093505859375, 'referece_logps/chosen': -469.287353515625, 'logits/rejected': -0.3886721432209015, 'logits/chosen': -0.3944661319255829, 'epoch': 1.16}


 19%|█▉        | 2075/10740 [10:29:18<43:03:43, 17.89s/it]
[2024-04-02 05:42:35,989] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3682, 'learning_rate': 1.8636280056191191e-06, 'rewards/chosen': -2.293478488922119, 'rewards/rejected': -5.0979509353637695, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8044724464416504, 'policy_logps/rejected': -509.03485107421875, 'policy_logps/chosen': -410.0540466308594, 'referece_logps/rejected': -458.0553283691406, 'referece_logps/chosen': -387.1192626953125, 'logits/rejected': -0.3333102762699127, 'logits/chosen': -0.28135165572166443, 'epoch': 1.16}
[2024-04-02 05:42:54,637] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 19%|█▉        | 2077/10740 [10:29:57<45:44:36, 19.01s/it]

 19%|█▉        | 2078/10740 [10:30:19<47:26:18, 19.72s/it]

 19%|█▉        | 2079/10740 [10:30:39<47:28:54, 19.74s/it]

 19%|█▉        | 2080/10740 [10:30:53<43:43:48, 18.18s/it]
{'loss': 0.3668, 'learning_rate': 1.8628668388213869e-06, 'rewards/chosen': -2.41788649559021, 'rewards/rejected': -3.9014084339141846, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4835220575332642, 'policy_logps/rejected': -317.9090270996094, 'policy_logps/chosen': -270.8925476074219, 'referece_logps/rejected': -278.8949279785156, 'referece_logps/chosen': -246.7136688232422, 'logits/rejected': 0.011736616492271423, 'logits/chosen': 0.08592550456523895, 'epoch': 1.16}

 19%|█▉        | 2081/10740 [10:31:13<44:33:13, 18.52s/it]


 19%|█▉        | 2083/10740 [10:31:51<45:14:48, 18.82s/it]

 19%|█▉        | 2084/10740 [10:32:11<45:57:05, 19.11s/it]

 19%|█▉        | 2085/10740 [10:32:29<45:17:24, 18.84s/it]

 19%|█▉        | 2086/10740 [10:32:45<43:06:09, 17.93s/it]

 19%|█▉        | 2087/10740 [10:33:01<42:05:53, 17.51s/it]

 19%|█▉        | 2088/10740 [10:33:21<43:57:27, 18.29s/it]
[2024-04-02 05:46:39,586] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2089/10740 [10:33:38<42:46:47, 17.80s/it]

 19%|█▉        | 2090/10740 [10:34:01<46:24:56, 19.32s/it]
[2024-04-02 05:47:19,101] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2091/10740 [10:34:21<46:57:39, 19.55s/it]

 19%|█▉        | 2092/10740 [10:34:38<45:03:01, 18.75s/it]
[2024-04-02 05:47:56,087] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 19%|█▉        | 2093/10740 [10:34:57<45:38:22, 19.00s/it]

 19%|█▉        | 2094/10740 [10:35:14<43:51:04, 18.26s/it]
{'loss': 0.2593, 'learning_rate': 1.860725139510544e-06, 'rewards/chosen': -2.2297475337982178, 'rewards/rejected': -4.095870018005371, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8661223649978638, 'policy_logps/rejected': -388.21331787109375, 'policy_logps/chosen': -413.911376953125, 'referece_logps/rejected': -347.2545471191406, 'referece_logps/chosen': -391.6138610839844, 'logits/rejected': 0.33321672677993774, 'logits/chosen': 0.3823924958705902, 'epoch': 1.17}


 20%|█▉        | 2096/10740 [10:35:44<39:12:31, 16.33s/it]

 20%|█▉        | 2097/10740 [10:36:01<39:53:12, 16.61s/it]

 20%|█▉        | 2098/10740 [10:36:24<43:49:49, 18.26s/it]

 20%|█▉        | 2099/10740 [10:36:42<43:46:38, 18.24s/it]
{'loss': 0.3521, 'learning_rate': 1.8599565263734785e-06, 'rewards/chosen': -2.4399023056030273, 'rewards/rejected': -4.941105365753174, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5012030601501465, 'policy_logps/rejected': -240.46868896484375, 'policy_logps/chosen': -381.96905517578125, 'referece_logps/rejected': -191.05763244628906, 'referece_logps/chosen': -357.57000732421875, 'logits/rejected': 0.0464695543050766, 'logits/chosen': 0.23902663588523865, 'epoch': 1.17}
[2024-04-02 05:50:20,581] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 20%|█▉        | 2101/10740 [10:37:24<47:08:38, 19.65s/it]

 20%|█▉        | 2102/10740 [10:37:39<44:10:38, 18.41s/it]

 20%|█▉        | 2103/10740 [10:37:59<45:27:15, 18.95s/it]
{'loss': 0.2873, 'learning_rate': 1.859340227907253e-06, 'rewards/chosen': -2.8130617141723633, 'rewards/rejected': -4.965872287750244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1528103351593018, 'policy_logps/rejected': -529.0529174804688, 'policy_logps/chosen': -273.298583984375, 'referece_logps/rejected': -479.3941955566406, 'referece_logps/chosen': -245.16796875, 'logits/rejected': -1.1094599962234497, 'logits/chosen': -0.6934636235237122, 'epoch': 1.17}

 20%|█▉        | 2104/10740 [10:38:21<47:23:34, 19.76s/it]


 20%|█▉        | 2106/10740 [10:38:54<43:32:00, 18.15s/it]

 20%|█▉        | 2107/10740 [10:39:10<42:09:02, 17.58s/it]

 20%|█▉        | 2108/10740 [10:39:24<39:39:57, 16.54s/it]

 20%|█▉        | 2109/10740 [10:39:40<39:20:03, 16.41s/it]

 20%|█▉        | 2110/10740 [10:39:54<37:17:37, 15.56s/it]
{'loss': 0.3157, 'learning_rate': 1.8582586970068492e-06, 'rewards/chosen': -2.6102652549743652, 'rewards/rejected': -4.271536827087402, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6612712144851685, 'policy_logps/rejected': -436.366943359375, 'policy_logps/chosen': -331.38482666015625, 'referece_logps/rejected': -393.65155029296875, 'referece_logps/chosen': -305.2821960449219, 'logits/rejected': 0.11956772208213806, 'logits/chosen': 0.21142998337745667, 'epoch': 1.18}

 20%|█▉        | 2111/10740 [10:40:09<36:54:10, 15.40s/it]

 20%|█▉        | 2112/10740 [10:40:27<38:51:05, 16.21s/it]


 20%|█▉        | 2114/10740 [10:41:08<43:41:31, 18.23s/it]

 20%|█▉        | 2115/10740 [10:41:24<41:48:56, 17.45s/it]

 20%|█▉        | 2116/10740 [10:41:38<39:44:16, 16.59s/it]
{'loss': 0.3101, 'learning_rate': 1.8573286259701527e-06, 'rewards/chosen': -2.6170246601104736, 'rewards/rejected': -3.4469082355499268, 'rewards/accuracies': 0.625, 'rewards/margins': 0.829883337020874, 'policy_logps/rejected': -599.451171875, 'policy_logps/chosen': -346.36370849609375, 'referece_logps/rejected': -564.9821166992188, 'referece_logps/chosen': -320.1934509277344, 'logits/rejected': -0.8002054691314697, 'logits/chosen': -0.5865233540534973, 'epoch': 1.18}


 20%|█▉        | 2118/10740 [10:42:12<40:50:30, 17.05s/it]
{'loss': 0.3171, 'learning_rate': 1.85701797833164e-06, 'rewards/chosen': -2.165813684463501, 'rewards/rejected': -2.4689323902130127, 'rewards/accuracies': 0.625, 'rewards/margins': 0.30311885476112366, 'policy_logps/rejected': -313.2865295410156, 'policy_logps/chosen': -345.081298828125, 'referece_logps/rejected': -288.5971984863281, 'referece_logps/chosen': -323.4231872558594, 'logits/rejected': -0.020310580730438232, 'logits/chosen': 0.017711520195007324, 'epoch': 1.18}


 20%|█▉        | 2120/10740 [10:42:48<43:01:52, 17.97s/it]

 20%|█▉        | 2121/10740 [10:43:08<44:20:45, 18.52s/it]

 20%|█▉        | 2122/10740 [10:43:27<44:18:10, 18.51s/it]

 20%|█▉        | 2123/10740 [10:43:44<43:20:03, 18.10s/it]

 20%|█▉        | 2124/10740 [10:44:02<43:35:26, 18.21s/it]

 20%|█▉        | 2125/10740 [10:44:22<44:23:59, 18.55s/it]

 20%|█▉        | 2126/10740 [10:44:43<46:12:20, 19.31s/it]
[2024-04-02 05:58:00,865] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|█▉        | 2127/10740 [10:45:00<44:29:18, 18.60s/it]
[2024-04-02 05:58:17,790] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|█▉        | 2128/10740 [10:45:19<44:57:09, 18.79s/it]

 20%|█▉        | 2129/10740 [10:45:42<48:18:11, 20.19s/it]
[2024-04-02 05:59:00,507] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|█▉        | 2130/10740 [10:46:02<47:52:24, 20.02s/it]

 20%|█▉        | 2131/10740 [10:46:23<48:21:16, 20.22s/it]

 20%|█▉        | 2132/10740 [10:46:37<43:50:50, 18.34s/it]

 20%|█▉        | 2133/10740 [10:46:58<45:56:10, 19.21s/it]
{'loss': 0.2858, 'learning_rate': 1.8546781905112514e-06, 'rewards/chosen': -2.9511539936065674, 'rewards/rejected': -5.1088409423828125, 'rewards/accuracies': 1.0, 'rewards/margins': 2.157686233520508, 'policy_logps/rejected': -403.9471130371094, 'policy_logps/chosen': -514.8599243164062, 'referece_logps/rejected': -352.8586730957031, 'referece_logps/chosen': -485.348388671875, 'logits/rejected': -0.10297498106956482, 'logits/chosen': -0.05511021614074707, 'epoch': 1.19}

 20%|█▉        | 2134/10740 [10:47:15<44:34:29, 18.65s/it]
[2024-04-02 06:00:45,538] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 20%|█▉        | 2136/10740 [10:47:51<44:55:10, 18.79s/it]

 20%|█▉        | 2137/10740 [10:48:08<43:48:29, 18.33s/it]
{'loss': 0.2814, 'learning_rate': 1.8540512911794113e-06, 'rewards/chosen': -1.580909252166748, 'rewards/rejected': -5.225327968597412, 'rewards/accuracies': 0.875, 'rewards/margins': 3.644418716430664, 'policy_logps/rejected': -430.66668701171875, 'policy_logps/chosen': -310.3412170410156, 'referece_logps/rejected': -378.4133605957031, 'referece_logps/chosen': -294.53216552734375, 'logits/rejected': -0.6901094913482666, 'logits/chosen': -0.6963794231414795, 'epoch': 1.19}

 20%|█▉        | 2138/10740 [10:48:23<41:29:06, 17.36s/it]


 20%|█▉        | 2140/10740 [10:49:05<45:36:05, 19.09s/it]

 20%|█▉        | 2141/10740 [10:49:24<45:48:29, 19.18s/it]

 20%|█▉        | 2142/10740 [10:49:39<42:57:05, 17.98s/it]

 20%|█▉        | 2143/10740 [10:50:01<45:25:53, 19.02s/it]

 20%|█▉        | 2144/10740 [10:50:22<47:22:41, 19.84s/it]

 20%|█▉        | 2145/10740 [10:50:41<46:24:59, 19.44s/it]

 20%|█▉        | 2146/10740 [10:50:56<43:16:28, 18.13s/it]

 20%|█▉        | 2147/10740 [10:51:09<39:38:28, 16.61s/it]

 20%|██        | 2148/10740 [10:51:32<44:16:02, 18.55s/it]
[2024-04-02 06:04:50,239] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.369, 'learning_rate': 1.8523209123281292e-06, 'rewards/chosen': -2.977534294128418, 'rewards/rejected': -5.389760971069336, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4122262001037598, 'policy_logps/rejected': -672.6343994140625, 'policy_logps/chosen': -479.36090087890625, 'referece_logps/rejected': -618.73681640625, 'referece_logps/chosen': -449.5855407714844, 'logits/rejected': -0.4204826354980469, 'logits/chosen': -0.4147278368473053, 'epoch': 1.2}


 20%|██        | 2150/10740 [10:52:09<43:46:47, 18.35s/it]

 20%|██        | 2151/10740 [10:52:22<40:12:54, 16.86s/it]

 20%|██        | 2152/10740 [10:52:40<41:03:13, 17.21s/it]

 20%|██        | 2153/10740 [10:53:02<44:21:22, 18.60s/it]
{'loss': 0.3045, 'learning_rate': 1.8515312745139621e-06, 'rewards/chosen': -2.028474807739258, 'rewards/rejected': -3.905294895172119, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8768199682235718, 'policy_logps/rejected': -325.701416015625, 'policy_logps/chosen': -547.9791870117188, 'referece_logps/rejected': -286.6484375, 'referece_logps/chosen': -527.6944580078125, 'logits/rejected': 0.3590713143348694, 'logits/chosen': 0.2127152383327484, 'epoch': 1.2}

 20%|██        | 2154/10740 [10:53:16<40:58:18, 17.18s/it]


 20%|██        | 2156/10740 [10:53:49<39:28:07, 16.55s/it]

 20%|██        | 2157/10740 [10:54:07<40:23:20, 16.94s/it]
{'loss': 0.3732, 'learning_rate': 1.8508981700971115e-06, 'rewards/chosen': -1.9361058473587036, 'rewards/rejected': -6.435805797576904, 'rewards/accuracies': 1.0, 'rewards/margins': 4.499700546264648, 'policy_logps/rejected': -500.57635498046875, 'policy_logps/chosen': -417.8817138671875, 'referece_logps/rejected': -436.2182312011719, 'referece_logps/chosen': -398.5206298828125, 'logits/rejected': -0.17932412028312683, 'logits/chosen': -0.016651533544063568, 'epoch': 1.21}


 20%|██        | 2159/10740 [10:54:32<35:20:56, 14.83s/it]

 20%|██        | 2160/10740 [10:54:51<38:14:59, 16.05s/it]

 20%|██        | 2161/10740 [10:55:13<41:54:58, 17.59s/it]
[2024-04-02 06:08:30,793] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|██        | 2162/10740 [10:55:35<45:22:54, 19.05s/it]
[2024-04-02 06:08:53,237] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.338, 'learning_rate': 1.8501050483806898e-06, 'rewards/chosen': -2.6111607551574707, 'rewards/rejected': -5.26710844039917, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6559481620788574, 'policy_logps/rejected': -445.4507141113281, 'policy_logps/chosen': -449.74835205078125, 'referece_logps/rejected': -392.77960205078125, 'referece_logps/chosen': -423.6367492675781, 'logits/rejected': -0.43060171604156494, 'logits/chosen': -0.41508084535598755, 'epoch': 1.21}


 20%|██        | 2164/10740 [10:56:11<43:42:56, 18.35s/it]

 20%|██        | 2165/10740 [10:56:27<42:28:11, 17.83s/it]

 20%|██        | 2166/10740 [10:56:47<43:51:13, 18.41s/it]

 20%|██        | 2167/10740 [10:57:05<43:24:15, 18.23s/it]

 20%|██        | 2168/10740 [10:57:28<46:30:43, 19.53s/it]
{'loss': 0.2908, 'learning_rate': 1.8491507509488718e-06, 'rewards/chosen': -2.9236838817596436, 'rewards/rejected': -4.97986364364624, 'rewards/accuracies': 0.875, 'rewards/margins': 2.056180000305176, 'policy_logps/rejected': -523.5459594726562, 'policy_logps/chosen': -647.0784912109375, 'referece_logps/rejected': -473.74725341796875, 'referece_logps/chosen': -617.8416748046875, 'logits/rejected': 0.31169041991233826, 'logits/chosen': 0.2066926807165146, 'epoch': 1.21}

 20%|██        | 2169/10740 [10:57:40<41:19:26, 17.36s/it]


 20%|██        | 2171/10740 [10:58:19<44:00:21, 18.49s/it]

 20%|██        | 2172/10740 [10:58:39<45:10:13, 18.98s/it]
[2024-04-02 06:11:56,886] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|██        | 2173/10740 [10:58:59<46:25:09, 19.51s/it]
[2024-04-02 06:12:17,622] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|██        | 2174/10740 [10:59:19<46:32:13, 19.56s/it]

 20%|██        | 2175/10740 [10:59:37<45:14:04, 19.01s/it]
{'loss': 0.2431, 'learning_rate': 1.8480338900943032e-06, 'rewards/chosen': -2.006060838699341, 'rewards/rejected': -4.560831069946289, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5547702312469482, 'policy_logps/rejected': -386.19439697265625, 'policy_logps/chosen': -373.28106689453125, 'referece_logps/rejected': -340.5860595703125, 'referece_logps/chosen': -353.220458984375, 'logits/rejected': 0.00021595507860183716, 'logits/chosen': 0.04592340439558029, 'epoch': 1.22}


 20%|██        | 2177/10740 [11:00:20<48:07:22, 20.23s/it]
{'loss': 0.4355, 'learning_rate': 1.847714092598297e-06, 'rewards/chosen': -1.6830945014953613, 'rewards/rejected': -5.007568359375, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3244740962982178, 'policy_logps/rejected': -267.77569580078125, 'policy_logps/chosen': -291.8860778808594, 'referece_logps/rejected': -217.7000274658203, 'referece_logps/chosen': -275.05511474609375, 'logits/rejected': -0.13055047392845154, 'logits/chosen': -0.14040127396583557, 'epoch': 1.22}
[2024-04-02 06:13:54,527] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 20%|██        | 2178/10740 [11:00:36<45:37:00, 19.18s/it]


 20%|██        | 2180/10740 [11:01:12<44:06:59, 18.55s/it]

 20%|██        | 2181/10740 [11:01:31<44:52:54, 18.88s/it]
{'loss': 0.2118, 'learning_rate': 1.8470735725026553e-06, 'rewards/chosen': -1.6100819110870361, 'rewards/rejected': -3.71431565284729, 'rewards/accuracies': 1.0, 'rewards/margins': 2.104234218597412, 'policy_logps/rejected': -359.3334045410156, 'policy_logps/chosen': -412.30584716796875, 'referece_logps/rejected': -322.19024658203125, 'referece_logps/chosen': -396.20501708984375, 'logits/rejected': 0.1348590850830078, 'logits/chosen': 0.012135356664657593, 'epoch': 1.22}

 20%|██        | 2182/10740 [11:01:50<44:51:33, 18.87s/it]

 20%|██        | 2183/10740 [11:02:08<44:12:58, 18.60s/it]
[2024-04-02 06:15:46,852] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 20%|██        | 2185/10740 [11:02:45<43:46:01, 18.42s/it]

 20%|██        | 2186/10740 [11:03:04<43:52:47, 18.47s/it]
{'loss': 0.422, 'learning_rate': 1.8462711890157796e-06, 'rewards/chosen': -2.105738401412964, 'rewards/rejected': -4.390178203582764, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2844398021698, 'policy_logps/rejected': -407.69244384765625, 'policy_logps/chosen': -360.66400146484375, 'referece_logps/rejected': -363.79071044921875, 'referece_logps/chosen': -339.60662841796875, 'logits/rejected': -0.3666343092918396, 'logits/chosen': -0.37682968378067017, 'epoch': 1.22}

 20%|██        | 2187/10740 [11:03:27<47:13:33, 19.88s/it]


 20%|██        | 2189/10740 [11:04:04<44:51:16, 18.88s/it]

 20%|██        | 2190/10740 [11:04:19<42:34:21, 17.93s/it]
{'loss': 0.2889, 'learning_rate': 1.8456278966708888e-06, 'rewards/chosen': -2.4598701000213623, 'rewards/rejected': -4.333927154541016, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8740572929382324, 'policy_logps/rejected': -502.73553466796875, 'policy_logps/chosen': -510.46685791015625, 'referece_logps/rejected': -459.396240234375, 'referece_logps/chosen': -485.8681640625, 'logits/rejected': -1.0119837522506714, 'logits/chosen': -1.0059080123901367, 'epoch': 1.22}


 20%|██        | 2192/10740 [11:04:59<45:13:25, 19.05s/it]
{'loss': 0.3294, 'learning_rate': 1.8453057889683304e-06, 'rewards/chosen': -3.015735626220703, 'rewards/rejected': -3.5766794681549072, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5609438419342041, 'policy_logps/rejected': -412.918701171875, 'policy_logps/chosen': -373.2701416015625, 'referece_logps/rejected': -377.1519470214844, 'referece_logps/chosen': -343.1127624511719, 'logits/rejected': -0.46726709604263306, 'logits/chosen': -0.37526029348373413, 'epoch': 1.22}

 20%|██        | 2193/10740 [11:05:20<46:44:50, 19.69s/it]


 20%|██        | 2195/10740 [11:05:56<44:57:55, 18.94s/it]

 20%|██        | 2196/10740 [11:06:14<44:18:09, 18.67s/it]
{'loss': 0.2994, 'learning_rate': 1.844660651088918e-06, 'rewards/chosen': -1.4117916822433472, 'rewards/rejected': -3.892982006072998, 'rewards/accuracies': 0.875, 'rewards/margins': 2.481189727783203, 'policy_logps/rejected': -355.128173828125, 'policy_logps/chosen': -482.4891052246094, 'referece_logps/rejected': -316.1983642578125, 'referece_logps/chosen': -468.37115478515625, 'logits/rejected': 0.098843052983284, 'logits/chosen': 0.026456430554389954, 'epoch': 1.23}

 20%|██        | 2197/10740 [11:06:33<44:28:40, 18.74s/it]

 20%|██        | 2198/10740 [11:06:53<45:29:27, 19.17s/it]

 20%|██        | 2199/10740 [11:07:09<43:42:00, 18.42s/it]

 20%|██        | 2200/10740 [11:07:29<44:21:12, 18.70s/it]

 20%|██        | 2201/10740 [11:07:49<45:34:31, 19.21s/it]


 21%|██        | 2203/10740 [11:08:22<42:13:05, 17.80s/it]
{'loss': 0.2892, 'learning_rate': 1.843528702643686e-06, 'rewards/chosen': -2.527334213256836, 'rewards/rejected': -4.656679153442383, 'rewards/accuracies': 0.875, 'rewards/margins': 2.129344940185547, 'policy_logps/rejected': -286.4276123046875, 'policy_logps/chosen': -283.7331237792969, 'referece_logps/rejected': -239.86083984375, 'referece_logps/chosen': -258.4598388671875, 'logits/rejected': -0.6521619558334351, 'logits/chosen': -0.4957237243652344, 'epoch': 1.23}


 21%|██        | 2205/10740 [11:08:56<41:54:49, 17.68s/it]

 21%|██        | 2206/10740 [11:09:14<42:08:02, 17.77s/it]
{'loss': 0.3876, 'learning_rate': 1.8430424307720531e-06, 'rewards/chosen': -2.2910733222961426, 'rewards/rejected': -4.953189373016357, 'rewards/accuracies': 0.75, 'rewards/margins': 2.662116050720215, 'policy_logps/rejected': -282.195068359375, 'policy_logps/chosen': -260.75201416015625, 'referece_logps/rejected': -232.66314697265625, 'referece_logps/chosen': -237.84130859375, 'logits/rejected': -0.625549852848053, 'logits/chosen': -0.6620972752571106, 'epoch': 1.23}

 21%|██        | 2207/10740 [11:09:36<44:39:54, 18.84s/it]

 21%|██        | 2208/10740 [11:09:55<45:05:08, 19.02s/it]

 21%|██        | 2209/10740 [11:10:07<40:22:16, 17.04s/it]

 21%|██        | 2210/10740 [11:10:25<40:49:22, 17.23s/it]


 21%|██        | 2212/10740 [11:10:56<38:50:52, 16.40s/it]

 21%|██        | 2213/10740 [11:11:14<39:58:31, 16.88s/it]
{'loss': 0.2872, 'learning_rate': 1.8419051134069355e-06, 'rewards/chosen': -2.0312230587005615, 'rewards/rejected': -4.411189556121826, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3799662590026855, 'policy_logps/rejected': -428.1968078613281, 'policy_logps/chosen': -383.49432373046875, 'referece_logps/rejected': -384.0849609375, 'referece_logps/chosen': -363.18212890625, 'logits/rejected': -0.263203501701355, 'logits/chosen': -0.2877733111381531, 'epoch': 1.24}

 21%|██        | 2214/10740 [11:11:37<44:12:07, 18.66s/it]
[2024-04-02 06:25:17,148] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 21%|██        | 2216/10740 [11:12:20<47:30:13, 20.06s/it]

 21%|██        | 2217/10740 [11:12:40<47:55:52, 20.25s/it]
{'loss': 0.3364, 'learning_rate': 1.8412535328311812e-06, 'rewards/chosen': -2.206615447998047, 'rewards/rejected': -4.170124530792236, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9635090827941895, 'policy_logps/rejected': -322.23529052734375, 'policy_logps/chosen': -430.77618408203125, 'referece_logps/rejected': -280.5340576171875, 'referece_logps/chosen': -408.7100830078125, 'logits/rejected': 0.3776628375053406, 'logits/chosen': 0.2227441370487213, 'epoch': 1.24}

 21%|██        | 2218/10740 [11:12:55<44:06:47, 18.64s/it]


 21%|██        | 2220/10740 [11:13:28<41:48:47, 17.67s/it]
{'loss': 0.4, 'learning_rate': 1.8407640439497794e-06, 'rewards/chosen': -2.352994441986084, 'rewards/rejected': -3.889370918273926, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5363765954971313, 'policy_logps/rejected': -261.48236083984375, 'policy_logps/chosen': -316.9735107421875, 'referece_logps/rejected': -222.5886688232422, 'referece_logps/chosen': -293.4435729980469, 'logits/rejected': -0.1530894637107849, 'logits/chosen': -0.28125447034835815, 'epoch': 1.24}


 21%|██        | 2222/10740 [11:14:01<40:48:25, 17.25s/it]
{'loss': 0.393, 'learning_rate': 1.840437335656353e-06, 'rewards/chosen': -3.571737051010132, 'rewards/rejected': -5.061129093170166, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4893923997879028, 'policy_logps/rejected': -388.9285583496094, 'policy_logps/chosen': -520.9436645507812, 'referece_logps/rejected': -338.3172302246094, 'referece_logps/chosen': -485.2262878417969, 'logits/rejected': -0.06506577134132385, 'logits/chosen': -0.21218979358673096, 'epoch': 1.24}

 21%|██        | 2223/10740 [11:14:14<37:48:04, 15.98s/it]


 21%|██        | 2225/10740 [11:14:51<40:59:19, 17.33s/it]

 21%|██        | 2226/10740 [11:15:08<41:18:08, 17.46s/it]

 21%|██        | 2227/10740 [11:15:25<40:19:35, 17.05s/it]
{'loss': 0.3969, 'learning_rate': 1.839619227485953e-06, 'rewards/chosen': -2.6234514713287354, 'rewards/rejected': -4.98170280456543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3582513332366943, 'policy_logps/rejected': -251.4207763671875, 'policy_logps/chosen': -262.0476379394531, 'referece_logps/rejected': -201.60372924804688, 'referece_logps/chosen': -235.81314086914062, 'logits/rejected': -0.22883212566375732, 'logits/chosen': -0.07923039048910141, 'epoch': 1.24}

 21%|██        | 2228/10740 [11:15:46<43:20:56, 18.33s/it]


 21%|██        | 2230/10740 [11:16:23<43:16:20, 18.31s/it]

 21%|██        | 2231/10740 [11:16:40<42:56:26, 18.17s/it]

 21%|██        | 2232/10740 [11:16:54<39:52:26, 16.87s/it]
{'loss': 0.2771, 'learning_rate': 1.8387992101802041e-06, 'rewards/chosen': -1.8101515769958496, 'rewards/rejected': -4.277926921844482, 'rewards/accuracies': 0.75, 'rewards/margins': 2.467775821685791, 'policy_logps/rejected': -358.63458251953125, 'policy_logps/chosen': -502.7001953125, 'referece_logps/rejected': -315.85528564453125, 'referece_logps/chosen': -484.5987854003906, 'logits/rejected': 0.23987816274166107, 'logits/chosen': 0.037126779556274414, 'epoch': 1.25}


 21%|██        | 2234/10740 [11:17:22<35:35:51, 15.07s/it]
{'loss': 0.2105, 'learning_rate': 1.8384706691175236e-06, 'rewards/chosen': -2.161187171936035, 'rewards/rejected': -5.472928524017334, 'rewards/accuracies': 0.75, 'rewards/margins': 3.311741352081299, 'policy_logps/rejected': -336.870849609375, 'policy_logps/chosen': -360.122802734375, 'referece_logps/rejected': -282.1416015625, 'referece_logps/chosen': -338.51092529296875, 'logits/rejected': -0.47227030992507935, 'logits/chosen': -0.3666538894176483, 'epoch': 1.25}
[2024-04-02 06:31:01,728] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 2235/10740 [11:17:43<40:04:12, 16.96s/it]

 21%|██        | 2236/10740 [11:17:58<38:03:27, 16.11s/it]

 21%|██        | 2237/10740 [11:18:15<38:55:32, 16.48s/it]

 21%|██        | 2238/10740 [11:18:26<34:48:22, 14.74s/it]


 21%|██        | 2240/10740 [11:19:00<38:37:48, 16.36s/it]
{'loss': 0.2319, 'learning_rate': 1.837483216145057e-06, 'rewards/chosen': -3.119413137435913, 'rewards/rejected': -4.604532718658447, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4851198196411133, 'policy_logps/rejected': -371.3152160644531, 'policy_logps/chosen': -390.1557922363281, 'referece_logps/rejected': -325.2698669433594, 'referece_logps/chosen': -358.9616394042969, 'logits/rejected': 1.029222846031189, 'logits/chosen': 1.0342755317687988, 'epoch': 1.25}

 21%|██        | 2241/10740 [11:19:20<41:09:10, 17.43s/it]


 21%|██        | 2243/10740 [11:19:53<39:14:46, 16.63s/it]
{'loss': 0.4297, 'learning_rate': 1.8369884611461133e-06, 'rewards/chosen': -3.0750951766967773, 'rewards/rejected': -5.096554756164551, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0214595794677734, 'policy_logps/rejected': -314.068603515625, 'policy_logps/chosen': -355.52606201171875, 'referece_logps/rejected': -263.1030578613281, 'referece_logps/chosen': -324.7751159667969, 'logits/rejected': -0.4242432117462158, 'logits/chosen': -0.4645139276981354, 'epoch': 1.25}

 21%|██        | 2244/10740 [11:20:09<39:14:53, 16.63s/it]

 21%|██        | 2245/10740 [11:20:26<38:52:42, 16.48s/it]

 21%|██        | 2246/10740 [11:20:45<41:17:34, 17.50s/it]

 21%|██        | 2247/10740 [11:21:03<41:30:41, 17.60s/it]

 21%|██        | 2248/10740 [11:21:19<40:15:30, 17.07s/it]


 21%|██        | 2250/10740 [11:22:02<45:52:09, 19.45s/it]
{'loss': 0.3056, 'learning_rate': 1.8358313691010818e-06, 'rewards/chosen': -1.6461905241012573, 'rewards/rejected': -3.005298614501953, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3591079711914062, 'policy_logps/rejected': -337.49542236328125, 'policy_logps/chosen': -394.9510498046875, 'referece_logps/rejected': -307.4424133300781, 'referece_logps/chosen': -378.4891052246094, 'logits/rejected': -0.4548494219779968, 'logits/chosen': -0.6052182912826538, 'epoch': 1.26}


 21%|██        | 2252/10740 [11:22:43<46:43:05, 19.81s/it]

 21%|██        | 2253/10740 [11:23:03<46:50:46, 19.87s/it]
{'loss': 0.3865, 'learning_rate': 1.8353343318958019e-06, 'rewards/chosen': -2.323235034942627, 'rewards/rejected': -4.84424352645874, 'rewards/accuracies': 1.0, 'rewards/margins': 2.521008253097534, 'policy_logps/rejected': -389.6739807128906, 'policy_logps/chosen': -336.597412109375, 'referece_logps/rejected': -341.2315368652344, 'referece_logps/chosen': -313.36505126953125, 'logits/rejected': -0.009786024689674377, 'logits/chosen': 0.07813693583011627, 'epoch': 1.26}

 21%|██        | 2254/10740 [11:23:22<46:14:40, 19.62s/it]


 21%|██        | 2256/10740 [11:24:03<47:45:27, 20.26s/it]
[2024-04-02 06:37:21,428] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 2257/10740 [11:24:19<44:18:09, 18.80s/it]
[2024-04-02 06:37:36,813] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 2258/10740 [11:24:39<45:37:19, 19.36s/it]

 21%|██        | 2259/10740 [11:24:57<44:23:54, 18.85s/it]
{'loss': 0.2931, 'learning_rate': 1.834338206548723e-06, 'rewards/chosen': -2.5444560050964355, 'rewards/rejected': -3.033691644668579, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4892355501651764, 'policy_logps/rejected': -268.96685791015625, 'policy_logps/chosen': -407.5945739746094, 'referece_logps/rejected': -238.62997436523438, 'referece_logps/chosen': -382.1499938964844, 'logits/rejected': -0.8910040855407715, 'logits/chosen': -0.8638540506362915, 'epoch': 1.26}

 21%|██        | 2260/10740 [11:25:08<38:42:30, 16.43s/it]

 21%|██        | 2261/10740 [11:25:22<36:53:58, 15.67s/it]

 21%|██        | 2262/10740 [11:25:43<40:42:26, 17.29s/it]

 21%|██        | 2263/10740 [11:26:02<42:24:00, 18.01s/it]

 21%|██        | 2264/10740 [11:26:20<42:27:01, 18.03s/it]

 21%|██        | 2265/10740 [11:26:35<39:44:25, 16.88s/it]


 21%|██        | 2267/10740 [11:27:13<42:07:54, 17.90s/it]
{'loss': 0.3442, 'learning_rate': 1.8330057904194955e-06, 'rewards/chosen': -2.9357821941375732, 'rewards/rejected': -4.007749557495117, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0719671249389648, 'policy_logps/rejected': -335.9742431640625, 'policy_logps/chosen': -400.80328369140625, 'referece_logps/rejected': -295.89678955078125, 'referece_logps/chosen': -371.4454650878906, 'logits/rejected': -0.1852179318666458, 'logits/chosen': -0.2123759388923645, 'epoch': 1.27}


 21%|██        | 2269/10740 [11:27:53<44:07:18, 18.75s/it]

 21%|██        | 2270/10740 [11:28:11<43:45:30, 18.60s/it]
[2024-04-02 06:41:29,275] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 2271/10740 [11:28:31<45:01:50, 19.14s/it]

 21%|██        | 2272/10740 [11:28:45<41:24:43, 17.61s/it]
{'loss': 0.3664, 'learning_rate': 1.8321705675190237e-06, 'rewards/chosen': -2.140787124633789, 'rewards/rejected': -5.4263176918029785, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2855310440063477, 'policy_logps/rejected': -379.39312744140625, 'policy_logps/chosen': -361.4645080566406, 'referece_logps/rejected': -325.1299743652344, 'referece_logps/chosen': -340.0566711425781, 'logits/rejected': 0.061035022139549255, 'logits/chosen': 0.19859860837459564, 'epoch': 1.27}

 21%|██        | 2273/10740 [11:28:58<37:59:22, 16.15s/it]

 21%|██        | 2274/10740 [11:29:18<40:31:56, 17.24s/it]


 21%|██        | 2276/10740 [11:29:51<40:41:59, 17.31s/it]
{'loss': 0.2423, 'learning_rate': 1.8315010267244197e-06, 'rewards/chosen': -3.3119311332702637, 'rewards/rejected': -6.377295970916748, 'rewards/accuracies': 1.0, 'rewards/margins': 3.065364360809326, 'policy_logps/rejected': -473.9095458984375, 'policy_logps/chosen': -433.4955749511719, 'referece_logps/rejected': -410.1365966796875, 'referece_logps/chosen': -400.376220703125, 'logits/rejected': -0.17043936252593994, 'logits/chosen': -0.25947821140289307, 'epoch': 1.27}

 21%|██        | 2277/10740 [11:30:07<39:20:51, 16.74s/it]


 21%|██        | 2279/10740 [11:30:41<40:44:42, 17.34s/it]
[2024-04-02 06:43:59,505] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2227, 'learning_rate': 1.830998076991102e-06, 'rewards/chosen': -2.7332019805908203, 'rewards/rejected': -6.180487632751465, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4472851753234863, 'policy_logps/rejected': -567.7547607421875, 'policy_logps/chosen': -586.0584106445312, 'referece_logps/rejected': -505.94989013671875, 'referece_logps/chosen': -558.726318359375, 'logits/rejected': 0.32446885108947754, 'logits/chosen': 0.337912380695343, 'epoch': 1.27}
[2024-04-02 06:44:18,912] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██        | 2280/10740 [11:31:01<42:12:00, 17.96s/it]

 21%|██        | 2281/10740 [11:31:17<40:54:13, 17.41s/it]

 21%|██        | 2282/10740 [11:31:37<42:33:24, 18.11s/it]

 21%|██▏       | 2283/10740 [11:31:59<45:30:37, 19.37s/it]

 21%|██▏       | 2284/10740 [11:32:17<44:33:21, 18.97s/it]

 21%|██▏       | 2285/10740 [11:32:38<46:02:28, 19.60s/it]

 21%|██▏       | 2286/10740 [11:32:58<46:18:48, 19.72s/it]

 21%|██▏       | 2287/10740 [11:33:18<46:13:58, 19.69s/it]

 21%|██▏       | 2288/10740 [11:33:37<46:18:09, 19.72s/it]

 21%|██▏       | 2289/10740 [11:33:59<47:27:00, 20.21s/it]

 21%|██▏       | 2290/10740 [11:34:21<48:47:12, 20.78s/it]

 21%|██▏       | 2291/10740 [11:34:37<45:31:14, 19.40s/it]

 21%|██▏       | 2292/10740 [11:34:51<41:32:14, 17.70s/it]

 21%|██▏       | 2293/10740 [11:35:10<42:26:22, 18.09s/it]
{'loss': 0.3232, 'learning_rate': 1.8286419907281873e-06, 'rewards/chosen': -2.7047691345214844, 'rewards/rejected': -4.745713710784912, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0409445762634277, 'policy_logps/rejected': -455.6065368652344, 'policy_logps/chosen': -544.5361328125, 'referece_logps/rejected': -408.1494140625, 'referece_logps/chosen': -517.4884033203125, 'logits/rejected': 0.26235881447792053, 'logits/chosen': 0.07631790637969971, 'epoch': 1.28}

 21%|██▏       | 2294/10740 [11:35:28<42:15:30, 18.01s/it]

 21%|██▏       | 2295/10740 [11:35:41<39:21:02, 16.77s/it]


 21%|██▏       | 2297/10740 [11:36:18<40:27:50, 17.25s/it]
{'loss': 0.3613, 'learning_rate': 1.8279661081723716e-06, 'rewards/chosen': -2.471745014190674, 'rewards/rejected': -4.4931182861328125, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0213727951049805, 'policy_logps/rejected': -402.6282958984375, 'policy_logps/chosen': -598.4971923828125, 'referece_logps/rejected': -357.6971435546875, 'referece_logps/chosen': -573.77978515625, 'logits/rejected': -0.41716551780700684, 'logits/chosen': -0.683232843875885, 'epoch': 1.28}


 21%|██▏       | 2299/10740 [11:37:00<44:54:29, 19.15s/it]
{'loss': 0.3063, 'learning_rate': 1.8276277149996853e-06, 'rewards/chosen': -3.033891439437866, 'rewards/rejected': -5.055872440338135, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0219812393188477, 'policy_logps/rejected': -365.47210693359375, 'policy_logps/chosen': -380.2471618652344, 'referece_logps/rejected': -314.91339111328125, 'referece_logps/chosen': -349.9082946777344, 'logits/rejected': -0.4675499498844147, 'logits/chosen': -0.34605637192726135, 'epoch': 1.28}

 21%|██▏       | 2300/10740 [11:37:17<42:59:38, 18.34s/it]

 21%|██▏       | 2301/10740 [11:37:36<43:32:52, 18.58s/it]

 21%|██▏       | 2302/10740 [11:37:57<45:31:40, 19.42s/it]

 21%|██▏       | 2303/10740 [11:38:16<45:06:31, 19.25s/it]

 21%|██▏       | 2304/10740 [11:38:36<45:26:51, 19.39s/it]

 21%|██▏       | 2305/10740 [11:38:54<44:28:06, 18.98s/it]

 21%|██▏       | 2306/10740 [11:39:09<41:58:12, 17.91s/it]

 21%|██▏       | 2307/10740 [11:39:31<44:29:23, 18.99s/it]
[2024-04-02 06:53:09,531] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██▏       | 2308/10740 [11:39:51<45:42:43, 19.52s/it]
[2024-04-02 06:53:27,964] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 21%|██▏       | 2309/10740 [11:40:10<44:56:35, 19.19s/it]

 22%|██▏       | 2310/10740 [11:40:24<41:14:41, 17.61s/it]


 22%|██▏       | 2312/10740 [11:41:04<44:15:11, 18.90s/it]
{'loss': 0.3897, 'learning_rate': 1.8254208255991887e-06, 'rewards/chosen': -2.6086649894714355, 'rewards/rejected': -4.959150791168213, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3504858016967773, 'policy_logps/rejected': -347.24383544921875, 'policy_logps/chosen': -308.44110107421875, 'referece_logps/rejected': -297.65234375, 'referece_logps/chosen': -282.3544616699219, 'logits/rejected': -0.08963121473789215, 'logits/chosen': 0.008861151523888111, 'epoch': 1.29}
[2024-04-02 06:54:43,195] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 22%|██▏       | 2314/10740 [11:41:46<46:57:41, 20.06s/it]
{'loss': 0.4172, 'learning_rate': 1.825080177191828e-06, 'rewards/chosen': -2.2463431358337402, 'rewards/rejected': -4.9357171058654785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6893742084503174, 'policy_logps/rejected': -371.2669372558594, 'policy_logps/chosen': -420.85382080078125, 'referece_logps/rejected': -321.9097595214844, 'referece_logps/chosen': -398.3903503417969, 'logits/rejected': -0.8863635659217834, 'logits/chosen': -0.8441286087036133, 'epoch': 1.29}


 22%|██▏       | 2316/10740 [11:42:18<42:05:04, 17.98s/it]
{'loss': 0.3195, 'learning_rate': 1.824739228612212e-06, 'rewards/chosen': -3.0696768760681152, 'rewards/rejected': -5.222630023956299, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1529531478881836, 'policy_logps/rejected': -404.8052978515625, 'policy_logps/chosen': -485.7269592285156, 'referece_logps/rejected': -352.5790100097656, 'referece_logps/chosen': -455.0301818847656, 'logits/rejected': -0.3074048161506653, 'logits/chosen': -0.2305387258529663, 'epoch': 1.29}

 22%|██▏       | 2317/10740 [11:42:40<44:47:04, 19.14s/it]

 22%|██▏       | 2318/10740 [11:43:02<46:23:40, 19.83s/it]

 22%|██▏       | 2319/10740 [11:43:19<44:22:05, 18.97s/it]

 22%|██▏       | 2320/10740 [11:43:34<41:40:54, 17.82s/it]

 22%|██▏       | 2321/10740 [11:43:53<42:40:07, 18.25s/it]

 22%|██▏       | 2322/10740 [11:44:09<41:11:02, 17.61s/it]

 22%|██▏       | 2323/10740 [11:44:27<41:27:31, 17.73s/it]
[2024-04-02 06:58:07,637] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2324/10740 [11:44:49<44:42:29, 19.12s/it]

 22%|██▏       | 2325/10740 [11:45:12<47:03:05, 20.13s/it]

 22%|██▏       | 2326/10740 [11:45:31<46:37:24, 19.95s/it]

 22%|██▏       | 2327/10740 [11:45:51<46:25:38, 19.87s/it]

 22%|██▏       | 2328/10740 [11:46:12<47:23:52, 20.28s/it]

 22%|██▏       | 2329/10740 [11:46:30<45:16:08, 19.38s/it]
[2024-04-02 07:00:09,730] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2330/10740 [11:46:51<47:01:59, 20.13s/it]
[2024-04-02 07:00:32,495] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2331/10740 [11:47:14<48:52:18, 20.92s/it]

 22%|██▏       | 2332/10740 [11:47:30<45:22:34, 19.43s/it]

 22%|██▏       | 2333/10740 [11:47:49<44:49:25, 19.19s/it]

 22%|██▏       | 2334/10740 [11:48:08<44:38:27, 19.12s/it]
[2024-04-02 07:01:49,518] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 2335/10740 [11:48:31<47:41:59, 20.43s/it]

 22%|██▏       | 2336/10740 [11:48:48<45:04:15, 19.31s/it]

 22%|██▏       | 2337/10740 [11:49:09<46:18:44, 19.84s/it]

 22%|██▏       | 2338/10740 [11:49:26<44:25:04, 19.03s/it]

 22%|██▏       | 2339/10740 [11:49:45<44:00:39, 18.86s/it]

 22%|██▏       | 2340/10740 [11:50:02<42:37:07, 18.27s/it]

 22%|██▏       | 2341/10740 [11:50:23<45:11:22, 19.37s/it]

 22%|██▏       | 2342/10740 [11:50:42<44:28:43, 19.07s/it]

 22%|██▏       | 2343/10740 [11:51:01<44:41:05, 19.16s/it]

 22%|██▏       | 2344/10740 [11:51:16<41:42:46, 17.89s/it]

 22%|██▏       | 2345/10740 [11:51:36<42:54:24, 18.40s/it]

 22%|██▏       | 2346/10740 [11:51:49<39:37:09, 16.99s/it]

 22%|██▏       | 2347/10740 [11:52:06<39:16:43, 16.85s/it]

 22%|██▏       | 2348/10740 [11:52:19<36:23:23, 15.61s/it]

 22%|██▏       | 2349/10740 [11:52:38<38:54:40, 16.69s/it]

 22%|██▏       | 2350/10740 [11:52:54<38:47:05, 16.64s/it]


 22%|██▏       | 2352/10740 [11:53:34<42:23:02, 18.19s/it]

 22%|██▏       | 2353/10740 [11:53:46<38:28:41, 16.52s/it]

 22%|██▏       | 2354/10740 [11:54:06<40:55:59, 17.57s/it]

 22%|██▏       | 2355/10740 [11:54:26<42:32:38, 18.27s/it]

 22%|██▏       | 2356/10740 [11:54:40<39:36:39, 17.01s/it]

 22%|██▏       | 2357/10740 [11:54:59<40:46:37, 17.51s/it]

 22%|██▏       | 2358/10740 [11:55:11<36:46:01, 15.79s/it]

 22%|██▏       | 2359/10740 [11:55:26<36:25:53, 15.65s/it]

 22%|██▏       | 2360/10740 [11:55:46<39:28:36, 16.96s/it]

 22%|██▏       | 2361/10740 [11:56:06<41:21:18, 17.77s/it]

 22%|██▏       | 2362/10740 [11:56:27<44:03:25, 18.93s/it]

 22%|██▏       | 2363/10740 [11:56:49<45:37:15, 19.61s/it]

 22%|██▏       | 2364/10740 [11:57:05<43:25:43, 18.67s/it]

 22%|██▏       | 2365/10740 [11:57:26<45:17:19, 19.47s/it]

 22%|██▏       | 2366/10740 [11:57:41<41:32:18, 17.86s/it]

 22%|██▏       | 2367/10740 [11:58:00<42:50:15, 18.42s/it]

 22%|██▏       | 2368/10740 [11:58:15<40:22:06, 17.36s/it]

 22%|██▏       | 2369/10740 [11:58:32<39:52:21, 17.15s/it]

 22%|██▏       | 2370/10740 [11:58:52<41:39:23, 17.92s/it]

 22%|██▏       | 2371/10740 [11:59:05<38:41:43, 16.65s/it]

 22%|██▏       | 2372/10740 [11:59:25<40:53:07, 17.59s/it]

 22%|██▏       | 2373/10740 [11:59:43<41:15:37, 17.75s/it]

 22%|██▏       | 2374/10740 [12:00:06<44:49:18, 19.29s/it]

 22%|██▏       | 2375/10740 [12:00:28<46:36:57, 20.06s/it]

 22%|██▏       | 2376/10740 [12:00:48<46:31:07, 20.02s/it]

 22%|██▏       | 2377/10740 [12:01:03<43:00:57, 18.52s/it]

 22%|██▏       | 2378/10740 [12:01:21<42:55:31, 18.48s/it]

 22%|██▏       | 2379/10740 [12:01:36<40:15:02, 17.33s/it]

 22%|██▏       | 2380/10740 [12:01:54<41:02:48, 17.68s/it]
{'loss': 0.4412, 'learning_rate': 1.813671130421409e-06, 'rewards/chosen': -3.6539852619171143, 'rewards/rejected': -4.637690544128418, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9837056398391724, 'policy_logps/rejected': -524.7451171875, 'policy_logps/chosen': -593.1107177734375, 'referece_logps/rejected': -478.36822509765625, 'referece_logps/chosen': -556.5709228515625, 'logits/rejected': 0.5046636462211609, 'logits/chosen': 0.5324393510818481, 'epoch': 1.33}


 22%|██▏       | 2382/10740 [12:02:35<44:21:36, 19.11s/it]

 22%|██▏       | 2383/10740 [12:02:51<42:06:00, 18.14s/it]
{'loss': 0.4017, 'learning_rate': 1.8131448434138361e-06, 'rewards/chosen': -2.3132424354553223, 'rewards/rejected': -4.617974758148193, 'rewards/accuracies': 0.875, 'rewards/margins': 2.304731845855713, 'policy_logps/rejected': -371.87200927734375, 'policy_logps/chosen': -432.9078369140625, 'referece_logps/rejected': -325.6922607421875, 'referece_logps/chosen': -409.775390625, 'logits/rejected': -0.49976250529289246, 'logits/chosen': -0.5777334570884705, 'epoch': 1.33}

 22%|██▏       | 2384/10740 [12:03:10<42:48:35, 18.44s/it]


 22%|██▏       | 2386/10740 [12:03:51<45:24:53, 19.57s/it]
{'loss': 0.3655, 'learning_rate': 1.8126178907886445e-06, 'rewards/chosen': -3.951629400253296, 'rewards/rejected': -4.641583442687988, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6899539232254028, 'policy_logps/rejected': -492.50860595703125, 'policy_logps/chosen': -361.9986572265625, 'referece_logps/rejected': -446.0928039550781, 'referece_logps/chosen': -322.48236083984375, 'logits/rejected': -0.7284719944000244, 'logits/chosen': -0.6529315710067749, 'epoch': 1.33}


 22%|██▏       | 2388/10740 [12:04:29<44:31:10, 19.19s/it]

 22%|██▏       | 2389/10740 [12:04:40<39:08:47, 16.88s/it]

 22%|██▏       | 2390/10740 [12:05:01<41:34:47, 17.93s/it]

 22%|██▏       | 2391/10740 [12:05:11<36:27:32, 15.72s/it]

 22%|██▏       | 2392/10740 [12:05:26<36:07:38, 15.58s/it]
{'loss': 0.2252, 'learning_rate': 1.8115619904113437e-06, 'rewards/chosen': -2.8091635704040527, 'rewards/rejected': -4.8874616622924805, 'rewards/accuracies': 0.875, 'rewards/margins': 2.078298330307007, 'policy_logps/rejected': -411.4463195800781, 'policy_logps/chosen': -690.46484375, 'referece_logps/rejected': -362.57171630859375, 'referece_logps/chosen': -662.3731689453125, 'logits/rejected': -0.4135940372943878, 'logits/chosen': -0.6074360609054565, 'epoch': 1.34}


 22%|██▏       | 2394/10740 [12:05:53<33:22:06, 14.39s/it]

 22%|██▏       | 2395/10740 [12:06:13<37:00:00, 15.96s/it]

 22%|██▏       | 2396/10740 [12:06:30<37:44:16, 16.28s/it]

 22%|██▏       | 2397/10740 [12:06:50<40:13:25, 17.36s/it]

 22%|██▏       | 2398/10740 [12:07:06<39:14:49, 16.94s/it]

 22%|██▏       | 2399/10740 [12:07:28<42:52:05, 18.50s/it]

 22%|██▏       | 2400/10740 [12:07:49<45:06:52, 19.47s/it]

 22%|██▏       | 2401/10740 [12:08:06<42:47:25, 18.47s/it]

 22%|██▏       | 2402/10740 [12:08:25<43:11:32, 18.65s/it]

 22%|██▏       | 2403/10740 [12:08:44<43:32:36, 18.80s/it]
{'loss': 0.3452, 'learning_rate': 1.809619273987857e-06, 'rewards/chosen': -1.6487016677856445, 'rewards/rejected': -5.06529426574707, 'rewards/accuracies': 1.0, 'rewards/margins': 3.416592836380005, 'policy_logps/rejected': -261.9680480957031, 'policy_logps/chosen': -244.8065948486328, 'referece_logps/rejected': -211.31509399414062, 'referece_logps/chosen': -228.319580078125, 'logits/rejected': -1.0441116094589233, 'logits/chosen': -0.9958471059799194, 'epoch': 1.34}


 22%|██▏       | 2405/10740 [12:09:21<43:10:03, 18.64s/it]

 22%|██▏       | 2406/10740 [12:09:40<43:14:33, 18.68s/it]

 22%|██▏       | 2407/10740 [12:09:57<42:06:59, 18.20s/it]

 22%|██▏       | 2408/10740 [12:10:17<43:39:50, 18.87s/it]
{'loss': 0.3191, 'learning_rate': 1.8087332743077583e-06, 'rewards/chosen': -4.045543193817139, 'rewards/rejected': -5.4090375900268555, 'rewards/accuracies': 0.625, 'rewards/margins': 1.363494634628296, 'policy_logps/rejected': -503.18377685546875, 'policy_logps/chosen': -385.8233642578125, 'referece_logps/rejected': -449.0933837890625, 'referece_logps/chosen': -345.367919921875, 'logits/rejected': 0.3548545241355896, 'logits/chosen': 0.4198305308818817, 'epoch': 1.35}


 22%|██▏       | 2410/10740 [12:10:56<43:47:00, 18.92s/it]

 22%|██▏       | 2411/10740 [12:11:14<43:26:28, 18.78s/it]
{'loss': 0.2979, 'learning_rate': 1.8082007916954557e-06, 'rewards/chosen': -1.6931707859039307, 'rewards/rejected': -3.7692043781280518, 'rewards/accuracies': 0.875, 'rewards/margins': 2.076033592224121, 'policy_logps/rejected': -297.6286926269531, 'policy_logps/chosen': -329.3966064453125, 'referece_logps/rejected': -259.9366455078125, 'referece_logps/chosen': -312.4649353027344, 'logits/rejected': 0.44091251492500305, 'logits/chosen': 0.29327529668807983, 'epoch': 1.35}


 22%|██▏       | 2413/10740 [12:11:52<43:41:40, 18.89s/it]

 22%|██▏       | 2414/10740 [12:12:12<44:16:42, 19.15s/it]

 22%|██▏       | 2415/10740 [12:12:32<44:44:09, 19.35s/it]

 22%|██▏       | 2416/10740 [12:12:49<43:23:39, 18.77s/it]

 23%|██▎       | 2417/10740 [12:13:02<39:16:22, 16.99s/it]

 23%|██▎       | 2418/10740 [12:13:19<39:28:47, 17.08s/it]

 23%|██▎       | 2419/10740 [12:13:31<36:03:26, 15.60s/it]

 23%|██▎       | 2420/10740 [12:13:45<35:02:54, 15.17s/it]

 23%|██▎       | 2421/10740 [12:13:57<32:36:34, 14.11s/it]

 23%|██▎       | 2422/10740 [12:14:12<33:20:23, 14.43s/it]

 23%|██▎       | 2423/10740 [12:14:28<34:23:50, 14.89s/it]

 23%|██▎       | 2424/10740 [12:14:46<36:18:51, 15.72s/it]

 23%|██▎       | 2425/10740 [12:15:06<39:43:24, 17.20s/it]

 23%|██▎       | 2426/10740 [12:15:22<38:34:00, 16.70s/it]

 23%|██▎       | 2427/10740 [12:15:40<39:33:41, 17.13s/it]

 23%|██▎       | 2428/10740 [12:16:00<41:18:15, 17.89s/it]
{'loss': 0.4161, 'learning_rate': 1.8051709067306943e-06, 'rewards/chosen': -2.6097347736358643, 'rewards/rejected': -5.248145580291748, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6384105682373047, 'policy_logps/rejected': -407.2076110839844, 'policy_logps/chosen': -313.0882873535156, 'referece_logps/rejected': -354.7261657714844, 'referece_logps/chosen': -286.99090576171875, 'logits/rejected': 0.2666778266429901, 'logits/chosen': 0.42537152767181396, 'epoch': 1.36}


 23%|██▎       | 2430/10740 [12:16:38<43:24:07, 18.80s/it]
[2024-04-02 07:29:56,578] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2431/10740 [12:16:55<42:12:37, 18.29s/it]

 23%|██▎       | 2432/10740 [12:17:15<43:09:43, 18.70s/it]
{'loss': 0.3089, 'learning_rate': 1.8044549139109681e-06, 'rewards/chosen': -2.4910945892333984, 'rewards/rejected': -4.263466835021973, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7723727226257324, 'policy_logps/rejected': -191.66741943359375, 'policy_logps/chosen': -241.25711059570312, 'referece_logps/rejected': -149.03277587890625, 'referece_logps/chosen': -216.34616088867188, 'logits/rejected': -0.5586717128753662, 'logits/chosen': -0.6815568208694458, 'epoch': 1.36}


 23%|██▎       | 2434/10740 [12:17:47<40:07:41, 17.39s/it]
{'loss': 0.2694, 'learning_rate': 1.8040964784330837e-06, 'rewards/chosen': -2.775785207748413, 'rewards/rejected': -5.101654052734375, 'rewards/accuracies': 1.0, 'rewards/margins': 2.325868844985962, 'policy_logps/rejected': -313.4393310546875, 'policy_logps/chosen': -360.82861328125, 'referece_logps/rejected': -262.42279052734375, 'referece_logps/chosen': -333.07080078125, 'logits/rejected': -0.6213404536247253, 'logits/chosen': -0.6040725708007812, 'epoch': 1.36}


 23%|██▎       | 2436/10740 [12:18:22<39:51:04, 17.28s/it]

 23%|██▎       | 2437/10740 [12:18:39<39:13:34, 17.01s/it]

 23%|██▎       | 2438/10740 [12:18:56<39:37:04, 17.18s/it]
{'loss': 0.301, 'learning_rate': 1.8033787299932825e-06, 'rewards/chosen': -3.7306361198425293, 'rewards/rejected': -5.847238063812256, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1166012287139893, 'policy_logps/rejected': -363.43377685546875, 'policy_logps/chosen': -398.36968994140625, 'referece_logps/rejected': -304.96142578125, 'referece_logps/chosen': -361.0633544921875, 'logits/rejected': -0.5422902703285217, 'logits/chosen': -0.40075206756591797, 'epoch': 1.36}


 23%|██▎       | 2440/10740 [12:19:25<35:53:09, 15.57s/it]
{'loss': 0.2051, 'learning_rate': 1.8030194172924904e-06, 'rewards/chosen': -1.8765734434127808, 'rewards/rejected': -5.856802940368652, 'rewards/accuracies': 1.0, 'rewards/margins': 3.980229616165161, 'policy_logps/rejected': -416.68780517578125, 'policy_logps/chosen': -341.3000793457031, 'referece_logps/rejected': -358.11981201171875, 'referece_logps/chosen': -322.5343322753906, 'logits/rejected': -0.4657425284385681, 'logits/chosen': -0.3840904235839844, 'epoch': 1.36}


 23%|██▎       | 2442/10740 [12:19:59<36:53:40, 16.01s/it]

 23%|██▎       | 2443/10740 [12:20:18<39:34:20, 17.17s/it]
[2024-04-02 07:33:36,670] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2444/10740 [12:20:37<40:11:44, 17.44s/it]
{'loss': 0.233, 'learning_rate': 1.8022999155827291e-06, 'rewards/chosen': -2.4693639278411865, 'rewards/rejected': -5.590390205383301, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1210262775421143, 'policy_logps/rejected': -301.3551330566406, 'policy_logps/chosen': -419.53179931640625, 'referece_logps/rejected': -245.45123291015625, 'referece_logps/chosen': -394.838134765625, 'logits/rejected': -0.128461554646492, 'logits/chosen': -0.3885329067707062, 'epoch': 1.37}


 23%|██▎       | 2446/10740 [12:21:04<35:26:12, 15.38s/it]

 23%|██▎       | 2447/10740 [12:21:19<35:05:04, 15.23s/it]
{'loss': 0.343, 'learning_rate': 1.80175952304615e-06, 'rewards/chosen': -2.728947639465332, 'rewards/rejected': -5.251319408416748, 'rewards/accuracies': 0.875, 'rewards/margins': 2.522371292114258, 'policy_logps/rejected': -300.24603271484375, 'policy_logps/chosen': -423.95465087890625, 'referece_logps/rejected': -247.73281860351562, 'referece_logps/chosen': -396.6651916503906, 'logits/rejected': 0.06114383041858673, 'logits/chosen': 0.07317591458559036, 'epoch': 1.37}


 23%|██▎       | 2449/10740 [12:21:54<38:39:29, 16.79s/it]
[2024-04-02 07:35:12,208] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2450/10740 [12:22:16<42:03:49, 18.27s/it]

 23%|██▎       | 2451/10740 [12:22:37<44:27:39, 19.31s/it]
[2024-04-02 07:35:55,675] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3573, 'learning_rate': 1.8010379788327106e-06, 'rewards/chosen': -1.9558353424072266, 'rewards/rejected': -5.483325958251953, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5274908542633057, 'policy_logps/rejected': -414.72137451171875, 'policy_logps/chosen': -387.747314453125, 'referece_logps/rejected': -359.8881530761719, 'referece_logps/chosen': -368.1889953613281, 'logits/rejected': -0.23261798918247223, 'logits/chosen': -0.27977561950683594, 'epoch': 1.37}


 23%|██▎       | 2453/10740 [12:23:18<45:44:11, 19.87s/it]
[2024-04-02 07:36:35,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2454/10740 [12:23:34<43:03:57, 18.71s/it]

 23%|██▎       | 2455/10740 [12:23:53<43:27:39, 18.88s/it]

 23%|██▎       | 2456/10740 [12:24:09<41:38:12, 18.09s/it]

 23%|██▎       | 2457/10740 [12:24:29<42:46:44, 18.59s/it]

 23%|██▎       | 2458/10740 [12:24:46<41:41:23, 18.12s/it]
{'loss': 0.3255, 'learning_rate': 1.7997724721219266e-06, 'rewards/chosen': -2.66752290725708, 'rewards/rejected': -4.356297016143799, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6887738704681396, 'policy_logps/rejected': -252.79495239257812, 'policy_logps/chosen': -432.8931884765625, 'referece_logps/rejected': -209.23199462890625, 'referece_logps/chosen': -406.2179870605469, 'logits/rejected': 0.8194019794464111, 'logits/chosen': 0.595819354057312, 'epoch': 1.37}


 23%|██▎       | 2460/10740 [12:25:22<41:14:22, 17.93s/it]

 23%|██▎       | 2461/10740 [12:25:33<36:28:25, 15.86s/it]
{'loss': 0.1498, 'learning_rate': 1.799229020654978e-06, 'rewards/chosen': -3.234823226928711, 'rewards/rejected': -4.847160816192627, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6123374700546265, 'policy_logps/rejected': -266.7105712890625, 'policy_logps/chosen': -513.8401489257812, 'referece_logps/rejected': -218.23895263671875, 'referece_logps/chosen': -481.4919738769531, 'logits/rejected': -0.7119877338409424, 'logits/chosen': -0.8889318704605103, 'epoch': 1.37}
[2024-04-02 07:39:11,706] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2463/10740 [12:26:17<43:57:42, 19.12s/it]
[2024-04-02 07:39:35,140] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2464/10740 [12:26:36<43:56:33, 19.11s/it]

 23%|██▎       | 2465/10740 [12:26:56<44:36:23, 19.41s/it]

 23%|██▎       | 2466/10740 [12:27:16<45:05:31, 19.62s/it]

 23%|██▎       | 2467/10740 [12:27:31<41:28:04, 18.04s/it]

 23%|██▎       | 2468/10740 [12:27:52<44:03:21, 19.17s/it]

 23%|██▎       | 2469/10740 [12:28:09<42:28:06, 18.48s/it]

 23%|██▎       | 2470/10740 [12:28:30<44:06:15, 19.20s/it]

 23%|██▎       | 2471/10740 [12:28:44<40:40:48, 17.71s/it]
{'loss': 0.2887, 'learning_rate': 1.7974127932987522e-06, 'rewards/chosen': -3.4174485206604004, 'rewards/rejected': -4.150712966918945, 'rewards/accuracies': 0.75, 'rewards/margins': 0.733264684677124, 'policy_logps/rejected': -292.3638916015625, 'policy_logps/chosen': -323.400390625, 'referece_logps/rejected': -250.85678100585938, 'referece_logps/chosen': -289.22589111328125, 'logits/rejected': 0.1818893849849701, 'logits/chosen': 0.10181838274002075, 'epoch': 1.38}


 23%|██▎       | 2473/10740 [12:29:25<43:23:17, 18.89s/it]
[2024-04-02 07:42:43,088] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2474/10740 [12:29:41<41:44:47, 18.18s/it]

 23%|██▎       | 2475/10740 [12:30:03<44:13:46, 19.27s/it]

 23%|██▎       | 2476/10740 [12:30:23<44:52:35, 19.55s/it]
{'loss': 0.3623, 'learning_rate': 1.79650195883832e-06, 'rewards/chosen': -2.576937437057495, 'rewards/rejected': -5.559645652770996, 'rewards/accuracies': 1.0, 'rewards/margins': 2.982707977294922, 'policy_logps/rejected': -450.78216552734375, 'policy_logps/chosen': -340.9277038574219, 'referece_logps/rejected': -395.1856994628906, 'referece_logps/chosen': -315.1583557128906, 'logits/rejected': -0.545699954032898, 'logits/chosen': -0.4812241792678833, 'epoch': 1.38}
[2024-04-02 07:44:01,799] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 23%|██▎       | 2478/10740 [12:31:00<43:05:38, 18.78s/it]

 23%|██▎       | 2479/10740 [12:31:19<43:06:52, 18.79s/it]

 23%|██▎       | 2480/10740 [12:31:37<42:38:43, 18.59s/it]
{'loss': 0.2293, 'learning_rate': 1.7957719871821446e-06, 'rewards/chosen': -3.0672435760498047, 'rewards/rejected': -6.212890148162842, 'rewards/accuracies': 1.0, 'rewards/margins': 3.145646333694458, 'policy_logps/rejected': -392.7992248535156, 'policy_logps/chosen': -494.6882629394531, 'referece_logps/rejected': -330.67034912109375, 'referece_logps/chosen': -464.01580810546875, 'logits/rejected': -0.17686636745929718, 'logits/chosen': -0.22276675701141357, 'epoch': 1.39}
[2024-04-02 07:45:15,948] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2481/10740 [12:31:58<44:05:14, 19.22s/it]


 23%|██▎       | 2483/10740 [12:32:33<42:00:00, 18.31s/it]

 23%|██▎       | 2484/10740 [12:32:49<40:44:32, 17.77s/it]
{'loss': 0.3338, 'learning_rate': 1.7950408574874802e-06, 'rewards/chosen': -3.5937461853027344, 'rewards/rejected': -4.985295295715332, 'rewards/accuracies': 0.75, 'rewards/margins': 1.39154851436615, 'policy_logps/rejected': -576.1737060546875, 'policy_logps/chosen': -526.6194458007812, 'referece_logps/rejected': -526.3207397460938, 'referece_logps/chosen': -490.6820068359375, 'logits/rejected': -0.5790642499923706, 'logits/chosen': -0.4465065598487854, 'epoch': 1.39}


 23%|██▎       | 2486/10740 [12:33:19<37:04:06, 16.17s/it]

 23%|██▎       | 2487/10740 [12:33:39<39:36:34, 17.28s/it]
{'loss': 0.2288, 'learning_rate': 1.7944917508937035e-06, 'rewards/chosen': -1.8330742120742798, 'rewards/rejected': -3.999093532562256, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1660194396972656, 'policy_logps/rejected': -300.61865234375, 'policy_logps/chosen': -405.6994934082031, 'referece_logps/rejected': -260.6277160644531, 'referece_logps/chosen': -387.3687438964844, 'logits/rejected': -0.31863129138946533, 'logits/chosen': -0.3456035852432251, 'epoch': 1.39}
[2024-04-02 07:47:18,726] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2488/10740 [12:34:00<42:23:00, 18.49s/it]

 23%|██▎       | 2489/10740 [12:34:14<39:08:38, 17.08s/it]


 23%|██▎       | 2491/10740 [12:34:55<43:00:04, 18.77s/it]
{'loss': 0.3244, 'learning_rate': 1.79375859719287e-06, 'rewards/chosen': -1.7265973091125488, 'rewards/rejected': -3.797372341156006, 'rewards/accuracies': 0.75, 'rewards/margins': 2.070775032043457, 'policy_logps/rejected': -246.32122802734375, 'policy_logps/chosen': -353.8442077636719, 'referece_logps/rejected': -208.34750366210938, 'referece_logps/chosen': -336.5782470703125, 'logits/rejected': 0.5989186763763428, 'logits/chosen': 0.6608920693397522, 'epoch': 1.39}


 23%|██▎       | 2493/10740 [12:35:35<43:55:51, 19.18s/it]
{'loss': 0.3726, 'learning_rate': 1.7933915871100083e-06, 'rewards/chosen': -2.577718734741211, 'rewards/rejected': -4.254157066345215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6764384508132935, 'policy_logps/rejected': -353.5202331542969, 'policy_logps/chosen': -249.42788696289062, 'referece_logps/rejected': -310.9786376953125, 'referece_logps/chosen': -223.6507110595703, 'logits/rejected': 0.33644306659698486, 'logits/chosen': 0.2706902027130127, 'epoch': 1.39}

 23%|██▎       | 2494/10740 [12:35:54<44:10:43, 19.29s/it]


 23%|██▎       | 2496/10740 [12:36:30<42:11:17, 18.42s/it]
{'loss': 0.2036, 'learning_rate': 1.792840530820653e-06, 'rewards/chosen': -2.2929577827453613, 'rewards/rejected': -4.167365550994873, 'rewards/accuracies': 0.875, 'rewards/margins': 1.87440824508667, 'policy_logps/rejected': -289.52716064453125, 'policy_logps/chosen': -260.3882751464844, 'referece_logps/rejected': -247.85350036621094, 'referece_logps/chosen': -237.45867919921875, 'logits/rejected': -0.5079549551010132, 'logits/chosen': -0.5988813042640686, 'epoch': 1.39}


 23%|██▎       | 2498/10740 [12:37:04<39:44:28, 17.36s/it]

 23%|██▎       | 2499/10740 [12:37:23<41:15:55, 18.03s/it]
{'loss': 0.2612, 'learning_rate': 1.7922888255342218e-06, 'rewards/chosen': -1.7710297107696533, 'rewards/rejected': -4.226082801818848, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4550528526306152, 'policy_logps/rejected': -278.4137878417969, 'policy_logps/chosen': -422.179443359375, 'referece_logps/rejected': -236.1529541015625, 'referece_logps/chosen': -404.46917724609375, 'logits/rejected': 0.18467700481414795, 'logits/chosen': 0.20712389051914215, 'epoch': 1.4}

 23%|██▎       | 2500/10740 [12:37:43<42:27:32, 18.55s/it]


 23%|██▎       | 2502/10740 [12:38:30<47:05:07, 20.58s/it]

 23%|██▎       | 2503/10740 [12:38:50<46:42:57, 20.42s/it]

 23%|██▎       | 2504/10740 [12:39:12<47:57:38, 20.96s/it]

 23%|██▎       | 2505/10740 [12:39:32<47:12:00, 20.63s/it]
{'loss': 0.3308, 'learning_rate': 1.7911834697771017e-06, 'rewards/chosen': -2.8158321380615234, 'rewards/rejected': -4.94814395904541, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1323118209838867, 'policy_logps/rejected': -259.8052673339844, 'policy_logps/chosen': -314.7106628417969, 'referece_logps/rejected': -210.32382202148438, 'referece_logps/chosen': -286.5523681640625, 'logits/rejected': -0.9831522703170776, 'logits/chosen': -0.9426788091659546, 'epoch': 1.4}

 23%|██▎       | 2506/10740 [12:39:52<47:06:21, 20.60s/it]


 23%|██▎       | 2508/10740 [12:40:26<41:48:32, 18.28s/it]

 23%|██▎       | 2509/10740 [12:40:42<39:54:12, 17.45s/it]

 23%|██▎       | 2510/10740 [12:40:59<40:11:34, 17.58s/it]

 23%|██▎       | 2511/10740 [12:41:12<36:35:58, 16.01s/it]
{'loss': 0.3491, 'learning_rate': 1.7900755234579008e-06, 'rewards/chosen': -2.6344335079193115, 'rewards/rejected': -4.591097354888916, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9566640853881836, 'policy_logps/rejected': -388.79095458984375, 'policy_logps/chosen': -366.6337890625, 'referece_logps/rejected': -342.8800048828125, 'referece_logps/chosen': -340.28948974609375, 'logits/rejected': -1.258101224899292, 'logits/chosen': -1.4494528770446777, 'epoch': 1.4}

 23%|██▎       | 2512/10740 [12:41:28<36:53:37, 16.14s/it]


 23%|██▎       | 2514/10740 [12:42:01<37:45:26, 16.52s/it]
{'loss': 0.3582, 'learning_rate': 1.789520579970856e-06, 'rewards/chosen': -2.9966135025024414, 'rewards/rejected': -6.589759349822998, 'rewards/accuracies': 0.75, 'rewards/margins': 3.5931451320648193, 'policy_logps/rejected': -333.4246826171875, 'policy_logps/chosen': -269.9692687988281, 'referece_logps/rejected': -267.5270690917969, 'referece_logps/chosen': -240.0031280517578, 'logits/rejected': -0.42045173048973083, 'logits/chosen': -0.48374953866004944, 'epoch': 1.4}
[2024-04-02 07:55:36,416] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 23%|██▎       | 2515/10740 [12:42:18<37:59:32, 16.63s/it]

 23%|██▎       | 2516/10740 [12:42:37<39:28:47, 17.28s/it]

 23%|██▎       | 2517/10740 [12:42:56<40:51:30, 17.89s/it]

 23%|██▎       | 2518/10740 [12:43:17<42:50:18, 18.76s/it]

 23%|██▎       | 2519/10740 [12:43:37<43:35:20, 19.09s/it]

 23%|██▎       | 2520/10740 [12:43:50<39:40:37, 17.38s/it]


 23%|██▎       | 2522/10740 [12:44:20<36:49:07, 16.13s/it]

 23%|██▎       | 2523/10740 [12:44:40<39:56:47, 17.50s/it]
{'loss': 0.2748, 'learning_rate': 1.7878518736526654e-06, 'rewards/chosen': -2.4391043186187744, 'rewards/rejected': -4.185088157653809, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7459837198257446, 'policy_logps/rejected': -418.3446044921875, 'policy_logps/chosen': -365.918212890625, 'referece_logps/rejected': -376.49371337890625, 'referece_logps/chosen': -341.52716064453125, 'logits/rejected': 0.43881601095199585, 'logits/chosen': 0.3936190903186798, 'epoch': 1.41}

 24%|██▎       | 2524/10740 [12:44:53<36:29:36, 15.99s/it]

 24%|██▎       | 2525/10740 [12:45:11<38:09:56, 16.73s/it]

 24%|██▎       | 2526/10740 [12:45:23<34:32:31, 15.14s/it]


 24%|██▎       | 2528/10740 [12:45:58<37:10:00, 16.29s/it]

 24%|██▎       | 2529/10740 [12:46:14<37:16:12, 16.34s/it]

 24%|██▎       | 2530/10740 [12:46:36<41:20:25, 18.13s/it]

 24%|██▎       | 2531/10740 [12:46:56<42:21:13, 18.57s/it]

 24%|██▎       | 2532/10740 [12:47:14<41:57:12, 18.40s/it]

 24%|██▎       | 2533/10740 [12:47:34<43:03:43, 18.89s/it]

 24%|██▎       | 2534/10740 [12:47:52<42:12:57, 18.52s/it]

 24%|██▎       | 2535/10740 [12:48:14<44:44:51, 19.63s/it]
{'loss': 0.2976, 'learning_rate': 1.7856179052419663e-06, 'rewards/chosen': -2.708883285522461, 'rewards/rejected': -5.092006206512451, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3831229209899902, 'policy_logps/rejected': -278.98614501953125, 'policy_logps/chosen': -318.82635498046875, 'referece_logps/rejected': -228.0660858154297, 'referece_logps/chosen': -291.7375183105469, 'logits/rejected': -1.0291258096694946, 'logits/chosen': -0.9270119071006775, 'epoch': 1.42}
[2024-04-02 08:01:49,603] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▎       | 2536/10740 [12:48:31<43:18:37, 19.01s/it]

 24%|██▎       | 2537/10740 [12:48:51<44:03:03, 19.33s/it]


 24%|██▎       | 2539/10740 [12:49:30<43:58:44, 19.31s/it]
{'loss': 0.3251, 'learning_rate': 1.7848709611347857e-06, 'rewards/chosen': -1.8443245887756348, 'rewards/rejected': -5.455117702484131, 'rewards/accuracies': 1.0, 'rewards/margins': 3.610793113708496, 'policy_logps/rejected': -288.85491943359375, 'policy_logps/chosen': -240.1731719970703, 'referece_logps/rejected': -234.30372619628906, 'referece_logps/chosen': -221.7299346923828, 'logits/rejected': -0.5096647143363953, 'logits/chosen': -0.4719446003437042, 'epoch': 1.42}


 24%|██▎       | 2541/10740 [12:50:06<42:34:52, 18.70s/it]

 24%|██▎       | 2542/10740 [12:50:24<42:08:13, 18.50s/it]

 24%|██▎       | 2543/10740 [12:50:39<39:26:42, 17.32s/it]
{'loss': 0.359, 'learning_rate': 1.7841228748527146e-06, 'rewards/chosen': -2.3748576641082764, 'rewards/rejected': -4.854573726654053, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4797158241271973, 'policy_logps/rejected': -354.8813781738281, 'policy_logps/chosen': -482.83026123046875, 'referece_logps/rejected': -306.33563232421875, 'referece_logps/chosen': -459.0816650390625, 'logits/rejected': -0.8353691101074219, 'logits/chosen': -0.9187645316123962, 'epoch': 1.42}


 24%|██▎       | 2545/10740 [12:51:02<33:18:00, 14.63s/it]

 24%|██▎       | 2546/10740 [12:51:14<31:09:34, 13.69s/it]
{'loss': 0.2556, 'learning_rate': 1.7835610612437152e-06, 'rewards/chosen': -2.117009162902832, 'rewards/rejected': -6.040860176086426, 'rewards/accuracies': 0.75, 'rewards/margins': 3.9238507747650146, 'policy_logps/rejected': -500.2213134765625, 'policy_logps/chosen': -397.10498046875, 'referece_logps/rejected': -439.81268310546875, 'referece_logps/chosen': -375.9349060058594, 'logits/rejected': -1.198332667350769, 'logits/chosen': -1.0227195024490356, 'epoch': 1.42}


 24%|██▎       | 2548/10740 [12:51:50<36:44:47, 16.15s/it]
{'loss': 0.2754, 'learning_rate': 1.7831861624753266e-06, 'rewards/chosen': -2.0308616161346436, 'rewards/rejected': -5.26614236831665, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2352805137634277, 'policy_logps/rejected': -266.9773864746094, 'policy_logps/chosen': -384.62060546875, 'referece_logps/rejected': -214.31597900390625, 'referece_logps/chosen': -364.31201171875, 'logits/rejected': 0.2310970425605774, 'logits/chosen': 0.29606789350509644, 'epoch': 1.42}


 24%|██▎       | 2550/10740 [12:52:24<36:58:47, 16.25s/it]
{'loss': 0.305, 'learning_rate': 1.782810978776136e-06, 'rewards/chosen': -2.563751220703125, 'rewards/rejected': -4.4325361251831055, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8687849044799805, 'policy_logps/rejected': -245.88211059570312, 'policy_logps/chosen': -298.3060302734375, 'referece_logps/rejected': -201.55677795410156, 'referece_logps/chosen': -272.6684875488281, 'logits/rejected': -0.49282991886138916, 'logits/chosen': -0.5011905431747437, 'epoch': 1.42}

 24%|██▍       | 2551/10740 [12:52:41<37:38:07, 16.55s/it]

 24%|██▍       | 2552/10740 [12:52:59<38:49:58, 17.07s/it]

 24%|██▍       | 2553/10740 [12:53:19<40:44:12, 17.91s/it]


 24%|██▍       | 2555/10740 [12:53:52<39:17:14, 17.28s/it]

 24%|██▍       | 2556/10740 [12:54:10<39:48:40, 17.51s/it]

 24%|██▍       | 2557/10740 [12:54:24<37:20:07, 16.43s/it]

 24%|██▍       | 2558/10740 [12:54:43<38:36:53, 16.99s/it]

 24%|██▍       | 2559/10740 [12:54:58<37:51:55, 16.66s/it]

 24%|██▍       | 2560/10740 [12:55:16<38:35:59, 16.99s/it]

 24%|██▍       | 2561/10740 [12:55:31<36:56:54, 16.26s/it]
{'loss': 0.3594, 'learning_rate': 1.7807423813973474e-06, 'rewards/chosen': -2.241358757019043, 'rewards/rejected': -4.052521705627441, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8111629486083984, 'policy_logps/rejected': -366.87628173828125, 'policy_logps/chosen': -350.124755859375, 'referece_logps/rejected': -326.3510437011719, 'referece_logps/chosen': -327.7111511230469, 'logits/rejected': -0.1537674367427826, 'logits/chosen': -0.06006217002868652, 'epoch': 1.43}


 24%|██▍       | 2563/10740 [12:56:05<38:15:16, 16.84s/it]
{'loss': 0.2432, 'learning_rate': 1.780365348979648e-06, 'rewards/chosen': -3.2095673084259033, 'rewards/rejected': -5.166626930236816, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9570595026016235, 'policy_logps/rejected': -339.6727294921875, 'policy_logps/chosen': -261.25836181640625, 'referece_logps/rejected': -288.0065002441406, 'referece_logps/chosen': -229.16265869140625, 'logits/rejected': -0.7522724270820618, 'logits/chosen': -0.7983397841453552, 'epoch': 1.43}

 24%|██▍       | 2564/10740 [12:56:25<40:44:33, 17.94s/it]


 24%|██▍       | 2566/10740 [12:57:03<41:55:04, 18.46s/it]
{'loss': 0.3462, 'learning_rate': 1.779799268074937e-06, 'rewards/chosen': -2.681110143661499, 'rewards/rejected': -3.536766529083252, 'rewards/accuracies': 0.75, 'rewards/margins': 0.855656623840332, 'policy_logps/rejected': -268.94781494140625, 'policy_logps/chosen': -380.19805908203125, 'referece_logps/rejected': -233.5801544189453, 'referece_logps/chosen': -353.3869323730469, 'logits/rejected': 0.7812502980232239, 'logits/chosen': 0.7257445454597473, 'epoch': 1.43}
[2024-04-02 08:10:44,100] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2567/10740 [12:57:26<44:58:11, 19.81s/it]
[2024-04-02 08:11:01,492] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 24%|██▍       | 2569/10740 [12:58:01<42:14:01, 18.61s/it]
{'loss': 0.3286, 'learning_rate': 1.7792325488483621e-06, 'rewards/chosen': -3.7541959285736084, 'rewards/rejected': -6.274999141693115, 'rewards/accuracies': 0.75, 'rewards/margins': 2.520803689956665, 'policy_logps/rejected': -432.4652099609375, 'policy_logps/chosen': -393.4776611328125, 'referece_logps/rejected': -369.7152099609375, 'referece_logps/chosen': -355.9356994628906, 'logits/rejected': -0.9653209447860718, 'logits/chosen': -0.7680379152297974, 'epoch': 1.44}

 24%|██▍       | 2570/10740 [12:58:20<42:27:40, 18.71s/it]


 24%|██▍       | 2572/10740 [12:59:01<44:19:41, 19.54s/it]
{'loss': 0.2367, 'learning_rate': 1.778665191763824e-06, 'rewards/chosen': -2.799280881881714, 'rewards/rejected': -5.8463592529296875, 'rewards/accuracies': 0.75, 'rewards/margins': 3.0470786094665527, 'policy_logps/rejected': -336.6507873535156, 'policy_logps/chosen': -459.3043212890625, 'referece_logps/rejected': -278.1872253417969, 'referece_logps/chosen': -431.31146240234375, 'logits/rejected': -0.086680568754673, 'logits/chosen': -0.20389729738235474, 'epoch': 1.44}


 24%|██▍       | 2574/10740 [12:59:39<44:07:40, 19.45s/it]

 24%|██▍       | 2575/10740 [12:59:55<41:29:13, 18.29s/it]
{'loss': 0.2221, 'learning_rate': 1.7780971972857452e-06, 'rewards/chosen': -2.736807346343994, 'rewards/rejected': -5.940566539764404, 'rewards/accuracies': 0.875, 'rewards/margins': 3.203758955001831, 'policy_logps/rejected': -495.8804931640625, 'policy_logps/chosen': -372.0317687988281, 'referece_logps/rejected': -436.474853515625, 'referece_logps/chosen': -344.6636962890625, 'logits/rejected': 0.2232191264629364, 'logits/chosen': 0.2971699833869934, 'epoch': 1.44}


 24%|██▍       | 2577/10740 [13:00:17<33:10:41, 14.63s/it]

 24%|██▍       | 2578/10740 [13:00:29<31:38:57, 13.96s/it]
{'loss': 0.2235, 'learning_rate': 1.7775285658790704e-06, 'rewards/chosen': -2.893409252166748, 'rewards/rejected': -6.695178985595703, 'rewards/accuracies': 0.75, 'rewards/margins': 3.801769733428955, 'policy_logps/rejected': -431.5270080566406, 'policy_logps/chosen': -381.19061279296875, 'referece_logps/rejected': -364.5751953125, 'referece_logps/chosen': -352.2565002441406, 'logits/rejected': 0.045881882309913635, 'logits/chosen': 0.13841409981250763, 'epoch': 1.44}

 24%|██▍       | 2579/10740 [13:00:49<35:26:29, 15.63s/it]
[2024-04-02 08:14:28,117] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2580/10740 [13:01:10<39:07:23, 17.26s/it]

 24%|██▍       | 2581/10740 [13:01:30<40:47:56, 18.00s/it]

 24%|██▍       | 2582/10740 [13:01:52<43:28:55, 19.19s/it]
[2024-04-02 08:15:25,155] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2583/10740 [13:02:07<40:52:01, 18.04s/it]

 24%|██▍       | 2584/10740 [13:02:20<37:35:28, 16.59s/it]

 24%|██▍       | 2585/10740 [13:02:36<37:05:05, 16.37s/it]

 24%|██▍       | 2586/10740 [13:02:57<40:09:39, 17.73s/it]

 24%|██▍       | 2587/10740 [13:03:14<40:01:45, 17.68s/it]

 24%|██▍       | 2588/10740 [13:03:32<40:11:29, 17.75s/it]

 24%|██▍       | 2589/10740 [13:03:54<42:58:20, 18.98s/it]

 24%|██▍       | 2590/10740 [13:04:14<43:23:34, 19.17s/it]


 24%|██▍       | 2592/10740 [13:04:48<40:25:18, 17.86s/it]
{'loss': 0.2961, 'learning_rate': 1.7748665447271574e-06, 'rewards/chosen': -1.8432424068450928, 'rewards/rejected': -4.779931545257568, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9366888999938965, 'policy_logps/rejected': -379.08245849609375, 'policy_logps/chosen': -397.2162780761719, 'referece_logps/rejected': -331.28314208984375, 'referece_logps/chosen': -378.78387451171875, 'logits/rejected': -0.5441288352012634, 'logits/chosen': -0.4206795394420624, 'epoch': 1.45}

 24%|██▍       | 2593/10740 [13:05:04<39:42:04, 17.54s/it]

 24%|██▍       | 2594/10740 [13:05:25<41:27:27, 18.32s/it]

 24%|██▍       | 2595/10740 [13:05:38<37:56:57, 16.77s/it]

 24%|██▍       | 2596/10740 [13:05:50<34:47:02, 15.38s/it]

 24%|██▍       | 2597/10740 [13:06:02<32:30:15, 14.37s/it]

 24%|██▍       | 2598/10740 [13:06:21<35:41:21, 15.78s/it]

 24%|██▍       | 2599/10740 [13:06:37<35:57:02, 15.90s/it]


 24%|██▍       | 2601/10740 [13:07:10<35:35:37, 15.74s/it]

 24%|██▍       | 2602/10740 [13:07:30<38:28:31, 17.02s/it]
{'loss': 0.3096, 'learning_rate': 1.772956641161411e-06, 'rewards/chosen': -2.773977041244507, 'rewards/rejected': -5.160976409912109, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3869991302490234, 'policy_logps/rejected': -277.9253845214844, 'policy_logps/chosen': -330.53851318359375, 'referece_logps/rejected': -226.31561279296875, 'referece_logps/chosen': -302.79876708984375, 'logits/rejected': -0.9092745780944824, 'logits/chosen': -0.8933262825012207, 'epoch': 1.45}

 24%|██▍       | 2603/10740 [13:07:49<40:02:04, 17.71s/it]
[2024-04-02 08:21:30,149] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 24%|██▍       | 2605/10740 [13:08:34<45:19:38, 20.06s/it]
{'loss': 0.3284, 'learning_rate': 1.7723822984057679e-06, 'rewards/chosen': -2.4251885414123535, 'rewards/rejected': -4.212794303894043, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7876051664352417, 'policy_logps/rejected': -485.03765869140625, 'policy_logps/chosen': -350.836669921875, 'referece_logps/rejected': -442.9096984863281, 'referece_logps/chosen': -326.5848388671875, 'logits/rejected': -1.0203195810317993, 'logits/chosen': -0.9041302800178528, 'epoch': 1.46}

 24%|██▍       | 2606/10740 [13:08:55<46:14:27, 20.47s/it]

 24%|██▍       | 2607/10740 [13:09:15<45:58:18, 20.35s/it]
[2024-04-02 08:22:47,173] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 24%|██▍       | 2609/10740 [13:09:42<37:35:18, 16.64s/it]
{'loss': 0.4242, 'learning_rate': 1.7716155246454045e-06, 'rewards/chosen': -5.1818318367004395, 'rewards/rejected': -6.105274200439453, 'rewards/accuracies': 0.5, 'rewards/margins': 0.9234423041343689, 'policy_logps/rejected': -503.8826904296875, 'policy_logps/chosen': -537.75439453125, 'referece_logps/rejected': -442.8299560546875, 'referece_logps/chosen': -485.93609619140625, 'logits/rejected': 0.10760600864887238, 'logits/chosen': 0.19659635424613953, 'epoch': 1.46}

 24%|██▍       | 2610/10740 [13:09:58<37:42:06, 16.69s/it]

 24%|██▍       | 2611/10740 [13:10:14<37:12:39, 16.48s/it]

 24%|██▍       | 2612/10740 [13:10:27<34:40:35, 15.36s/it]

 24%|██▍       | 2613/10740 [13:10:47<37:39:58, 16.68s/it]

 24%|██▍       | 2614/10740 [13:11:04<37:59:57, 16.83s/it]

 24%|██▍       | 2615/10740 [13:11:17<35:22:32, 15.67s/it]

 24%|██▍       | 2616/10740 [13:11:39<39:54:47, 17.69s/it]
[2024-04-02 08:25:17,530] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2617/10740 [13:11:59<41:23:18, 18.34s/it]
[2024-04-02 08:25:38,420] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 24%|██▍       | 2618/10740 [13:12:20<43:06:27, 19.11s/it]


 24%|██▍       | 2620/10740 [13:12:44<34:37:52, 15.35s/it]
{'loss': 0.3839, 'learning_rate': 1.7695011102903794e-06, 'rewards/chosen': -2.7686729431152344, 'rewards/rejected': -4.231597900390625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4629249572753906, 'policy_logps/rejected': -238.03663635253906, 'policy_logps/chosen': -261.4077453613281, 'referece_logps/rejected': -195.72064208984375, 'referece_logps/chosen': -233.7210235595703, 'logits/rejected': -0.4865299463272095, 'logits/chosen': -0.44653022289276123, 'epoch': 1.46}


 24%|██▍       | 2622/10740 [13:13:16<34:26:54, 15.28s/it]

 24%|██▍       | 2623/10740 [13:13:34<36:28:11, 16.17s/it]
{'loss': 0.3926, 'learning_rate': 1.7689229811017115e-06, 'rewards/chosen': -3.0489206314086914, 'rewards/rejected': -6.005183219909668, 'rewards/accuracies': 0.75, 'rewards/margins': 2.9562618732452393, 'policy_logps/rejected': -268.9132385253906, 'policy_logps/chosen': -325.462890625, 'referece_logps/rejected': -208.86141967773438, 'referece_logps/chosen': -294.97369384765625, 'logits/rejected': -0.608019232749939, 'logits/chosen': -0.5590341091156006, 'epoch': 1.47}

 24%|██▍       | 2624/10740 [13:13:50<35:58:17, 15.96s/it]


 24%|██▍       | 2626/10740 [13:14:28<39:45:57, 17.64s/it]

 24%|██▍       | 2627/10740 [13:14:46<40:01:37, 17.76s/it]

 24%|██▍       | 2628/10740 [13:15:06<41:20:10, 18.34s/it]
{'loss': 0.2197, 'learning_rate': 1.7679580339793314e-06, 'rewards/chosen': -2.05291748046875, 'rewards/rejected': -3.968397617340088, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9154798984527588, 'policy_logps/rejected': -367.93585205078125, 'policy_logps/chosen': -346.4238586425781, 'referece_logps/rejected': -328.2518615722656, 'referece_logps/chosen': -325.89471435546875, 'logits/rejected': -0.5174533128738403, 'logits/chosen': -0.5852957963943481, 'epoch': 1.47}

 24%|██▍       | 2629/10740 [13:15:27<43:06:24, 19.13s/it]

 24%|██▍       | 2630/10740 [13:15:41<39:52:26, 17.70s/it]

 24%|██▍       | 2631/10740 [13:15:59<40:16:43, 17.88s/it]

 25%|██▍       | 2632/10740 [13:16:19<41:40:57, 18.51s/it]


 25%|██▍       | 2634/10740 [13:16:54<40:54:06, 18.17s/it]

 25%|██▍       | 2635/10740 [13:17:14<41:57:33, 18.64s/it]
{'loss': 0.3477, 'learning_rate': 1.766604174898774e-06, 'rewards/chosen': -3.0153560638427734, 'rewards/rejected': -5.233709335327148, 'rewards/accuracies': 0.875, 'rewards/margins': 2.218353271484375, 'policy_logps/rejected': -445.419189453125, 'policy_logps/chosen': -416.24224853515625, 'referece_logps/rejected': -393.08209228515625, 'referece_logps/chosen': -386.08868408203125, 'logits/rejected': 0.5834234356880188, 'logits/chosen': 0.5075910687446594, 'epoch': 1.47}

 25%|██▍       | 2636/10740 [13:17:27<38:12:09, 16.97s/it]
[2024-04-02 08:31:07,772] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2637/10740 [13:17:50<41:52:47, 18.61s/it]

 25%|██▍       | 2638/10740 [13:18:08<41:44:37, 18.55s/it]


 25%|██▍       | 2640/10740 [13:18:48<43:38:43, 19.40s/it]
{'loss': 0.4451, 'learning_rate': 1.7656350406108225e-06, 'rewards/chosen': -2.78652286529541, 'rewards/rejected': -4.319294452667236, 'rewards/accuracies': 0.5, 'rewards/margins': 1.5327719449996948, 'policy_logps/rejected': -234.631591796875, 'policy_logps/chosen': -187.5816192626953, 'referece_logps/rejected': -191.43865966796875, 'referece_logps/chosen': -159.7163848876953, 'logits/rejected': -0.6591907739639282, 'logits/chosen': -0.6561375260353088, 'epoch': 1.47}

 25%|██▍       | 2641/10740 [13:19:08<43:46:08, 19.46s/it]
[2024-04-02 08:32:47,976] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2642/10740 [13:19:30<45:24:47, 20.19s/it]
[2024-04-02 08:33:03,940] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2643/10740 [13:19:46<42:33:26, 18.92s/it]

 25%|██▍       | 2644/10740 [13:20:05<43:02:06, 19.14s/it]

 25%|██▍       | 2645/10740 [13:20:24<42:34:39, 18.94s/it]

 25%|██▍       | 2646/10740 [13:20:44<43:19:33, 19.27s/it]

 25%|██▍       | 2647/10740 [13:20:59<40:44:07, 18.12s/it]

 25%|██▍       | 2648/10740 [13:21:16<39:53:20, 17.75s/it]
[2024-04-02 08:34:55,084] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▍       | 2649/10740 [13:21:37<41:51:37, 18.63s/it]

 25%|██▍       | 2650/10740 [13:21:56<41:58:24, 18.68s/it]

 25%|██▍       | 2651/10740 [13:22:13<41:21:57, 18.41s/it]

 25%|██▍       | 2652/10740 [13:22:30<40:19:10, 17.95s/it]

 25%|██▍       | 2653/10740 [13:22:46<38:59:01, 17.35s/it]

 25%|██▍       | 2654/10740 [13:22:57<34:26:39, 15.34s/it]

 25%|██▍       | 2655/10740 [13:23:12<34:04:26, 15.17s/it]

 25%|██▍       | 2656/10740 [13:23:29<35:24:23, 15.77s/it]

 25%|██▍       | 2657/10740 [13:23:46<36:22:56, 16.20s/it]

 25%|██▍       | 2658/10740 [13:24:06<38:42:31, 17.24s/it]

 25%|██▍       | 2659/10740 [13:24:24<39:20:40, 17.53s/it]

 25%|██▍       | 2660/10740 [13:24:41<39:08:39, 17.44s/it]


 25%|██▍       | 2662/10740 [13:25:17<38:53:25, 17.33s/it]

 25%|██▍       | 2663/10740 [13:25:39<42:03:05, 18.74s/it]

 25%|██▍       | 2664/10740 [13:25:55<40:13:31, 17.93s/it]

 25%|██▍       | 2665/10740 [13:26:09<37:40:42, 16.80s/it]
{'loss': 0.1703, 'learning_rate': 1.7607632997425935e-06, 'rewards/chosen': -3.63212251663208, 'rewards/rejected': -5.533005714416504, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9008830785751343, 'policy_logps/rejected': -385.6755676269531, 'policy_logps/chosen': -377.9856262207031, 'referece_logps/rejected': -330.34552001953125, 'referece_logps/chosen': -341.6643981933594, 'logits/rejected': -0.3651925325393677, 'logits/chosen': -0.597686231136322, 'epoch': 1.49}

 25%|██▍       | 2666/10740 [13:26:28<39:38:01, 17.67s/it]

 25%|██▍       | 2667/10740 [13:26:46<39:35:00, 17.65s/it]

 25%|██▍       | 2668/10740 [13:27:04<39:34:30, 17.65s/it]

 25%|██▍       | 2669/10740 [13:27:23<40:58:29, 18.28s/it]

 25%|██▍       | 2670/10740 [13:27:41<40:10:26, 17.92s/it]

 25%|██▍       | 2671/10740 [13:28:00<41:31:24, 18.53s/it]


 25%|██▍       | 2673/10740 [13:28:31<36:54:02, 16.47s/it]

 25%|██▍       | 2674/10740 [13:28:53<40:30:59, 18.08s/it]
{'loss': 0.329, 'learning_rate': 1.7589988719685475e-06, 'rewards/chosen': -2.762284517288208, 'rewards/rejected': -7.09905481338501, 'rewards/accuracies': 1.0, 'rewards/margins': 4.336770534515381, 'policy_logps/rejected': -371.3572998046875, 'policy_logps/chosen': -340.01385498046875, 'referece_logps/rejected': -300.36676025390625, 'referece_logps/chosen': -312.3910217285156, 'logits/rejected': -0.3489668071269989, 'logits/chosen': -0.19912832975387573, 'epoch': 1.49}

 25%|██▍       | 2675/10740 [13:29:14<42:39:51, 19.04s/it]


 25%|██▍       | 2677/10740 [13:29:47<38:56:01, 17.38s/it]

 25%|██▍       | 2678/10740 [13:30:07<40:51:03, 18.24s/it]
{'loss': 0.2245, 'learning_rate': 1.7582128862206353e-06, 'rewards/chosen': -2.7436070442199707, 'rewards/rejected': -5.463479518890381, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7198729515075684, 'policy_logps/rejected': -416.8157653808594, 'policy_logps/chosen': -399.3065490722656, 'referece_logps/rejected': -362.180908203125, 'referece_logps/chosen': -371.8704833984375, 'logits/rejected': -0.9745239019393921, 'logits/chosen': -1.1524403095245361, 'epoch': 1.5}

 25%|██▍       | 2679/10740 [13:30:23<39:34:19, 17.67s/it]

 25%|██▍       | 2680/10740 [13:30:43<40:47:25, 18.22s/it]

 25%|██▍       | 2681/10740 [13:30:55<36:42:02, 16.39s/it]

 25%|██▍       | 2682/10740 [13:31:16<39:59:32, 17.87s/it]

 25%|██▍       | 2683/10740 [13:31:36<41:20:15, 18.47s/it]

 25%|██▍       | 2684/10740 [13:31:53<39:53:49, 17.83s/it]

 25%|██▌       | 2685/10740 [13:32:12<41:14:57, 18.44s/it]

 25%|██▌       | 2686/10740 [13:32:30<40:39:12, 18.17s/it]

 25%|██▌       | 2687/10740 [13:32:50<41:35:03, 18.59s/it]

 25%|██▌       | 2688/10740 [13:33:10<42:40:39, 19.08s/it]

 25%|██▌       | 2689/10740 [13:33:22<38:22:03, 17.16s/it]

 25%|██▌       | 2690/10740 [13:33:40<38:33:42, 17.24s/it]

 25%|██▌       | 2691/10740 [13:34:01<40:52:15, 18.28s/it]

 25%|██▌       | 2692/10740 [13:34:23<43:36:45, 19.51s/it]

 25%|██▌       | 2693/10740 [13:34:37<39:55:47, 17.86s/it]

 25%|██▌       | 2694/10740 [13:34:58<42:11:30, 18.88s/it]

 25%|██▌       | 2695/10740 [13:35:16<41:38:59, 18.64s/it]

 25%|██▌       | 2696/10740 [13:35:36<42:13:45, 18.90s/it]

 25%|██▌       | 2697/10740 [13:35:56<43:15:16, 19.36s/it]

 25%|██▌       | 2698/10740 [13:36:13<41:38:18, 18.64s/it]

 25%|██▌       | 2699/10740 [13:36:32<41:35:12, 18.62s/it]

 25%|██▌       | 2700/10740 [13:36:46<38:16:28, 17.14s/it]

 25%|██▌       | 2701/10740 [13:36:59<36:06:03, 16.17s/it]

 25%|██▌       | 2702/10740 [13:37:15<35:28:31, 15.89s/it]

 25%|██▌       | 2703/10740 [13:37:36<39:05:42, 17.51s/it]

 25%|██▌       | 2704/10740 [13:37:55<40:15:21, 18.03s/it]
[2024-04-02 08:51:33,595] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 25%|██▌       | 2706/10740 [13:38:31<39:40:04, 17.77s/it]

 25%|██▌       | 2707/10740 [13:38:51<40:54:51, 18.34s/it]
{'loss': 0.2853, 'learning_rate': 1.752481563054724e-06, 'rewards/chosen': -2.6229512691497803, 'rewards/rejected': -4.998790264129639, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3758389949798584, 'policy_logps/rejected': -263.5351257324219, 'policy_logps/chosen': -358.73095703125, 'referece_logps/rejected': -213.54721069335938, 'referece_logps/chosen': -332.50146484375, 'logits/rejected': 0.28979238867759705, 'logits/chosen': 0.29838359355926514, 'epoch': 1.51}

 25%|██▌       | 2708/10740 [13:39:04<37:27:30, 16.79s/it]


 25%|██▌       | 2710/10740 [13:39:38<38:32:38, 17.28s/it]

 25%|██▌       | 2711/10740 [13:39:57<40:12:57, 18.03s/it]

 25%|██▌       | 2712/10740 [13:40:18<41:36:02, 18.65s/it]

 25%|██▌       | 2713/10740 [13:40:33<39:42:00, 17.80s/it]

 25%|██▌       | 2714/10740 [13:40:55<42:01:25, 18.85s/it]

 25%|██▌       | 2715/10740 [13:41:08<38:04:44, 17.08s/it]
{'loss': 0.2683, 'learning_rate': 1.7508903605975358e-06, 'rewards/chosen': -3.530186176300049, 'rewards/rejected': -5.649845600128174, 'rewards/accuracies': 0.875, 'rewards/margins': 2.119659662246704, 'policy_logps/rejected': -293.9332275390625, 'policy_logps/chosen': -348.15240478515625, 'referece_logps/rejected': -237.43478393554688, 'referece_logps/chosen': -312.8505859375, 'logits/rejected': -0.015526235103607178, 'logits/chosen': 0.054489195346832275, 'epoch': 1.52}


 25%|██▌       | 2717/10740 [13:41:44<39:31:08, 17.73s/it]
{'loss': 0.2358, 'learning_rate': 1.7504918766679983e-06, 'rewards/chosen': -3.507357597351074, 'rewards/rejected': -6.579776763916016, 'rewards/accuracies': 0.75, 'rewards/margins': 3.0724194049835205, 'policy_logps/rejected': -390.7532043457031, 'policy_logps/chosen': -321.0693664550781, 'referece_logps/rejected': -324.9554748535156, 'referece_logps/chosen': -285.99578857421875, 'logits/rejected': -0.844327449798584, 'logits/chosen': -0.813317060470581, 'epoch': 1.52}


 25%|██▌       | 2719/10740 [13:42:18<39:14:41, 17.61s/it]

 25%|██▌       | 2720/10740 [13:42:39<41:31:33, 18.64s/it]
[2024-04-02 08:55:57,624] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1797, 'learning_rate': 1.7498936388759595e-06, 'rewards/chosen': -3.1912951469421387, 'rewards/rejected': -5.936980247497559, 'rewards/accuracies': 1.0, 'rewards/margins': 2.74568510055542, 'policy_logps/rejected': -627.976806640625, 'policy_logps/chosen': -550.45849609375, 'referece_logps/rejected': -568.60693359375, 'referece_logps/chosen': -518.5455322265625, 'logits/rejected': -0.15823417901992798, 'logits/chosen': -0.08897519111633301, 'epoch': 1.52}
[2024-04-02 08:56:18,285] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 25%|██▌       | 2722/10740 [13:43:16<40:33:48, 18.21s/it]

 25%|██▌       | 2723/10740 [13:43:30<37:49:59, 16.99s/it]

 25%|██▌       | 2724/10740 [13:43:46<37:23:48, 16.80s/it]

 25%|██▌       | 2725/10740 [13:43:58<34:01:55, 15.29s/it]

 25%|██▌       | 2726/10740 [13:44:10<31:29:32, 14.15s/it]

 25%|██▌       | 2727/10740 [13:44:29<34:55:44, 15.69s/it]

 25%|██▌       | 2728/10740 [13:44:45<34:54:03, 15.68s/it]

 25%|██▌       | 2729/10740 [13:45:00<34:44:23, 15.61s/it]

 25%|██▌       | 2730/10740 [13:45:16<35:17:37, 15.86s/it]
[2024-04-02 08:58:34,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2107, 'learning_rate': 1.7478950823540576e-06, 'rewards/chosen': -3.7695980072021484, 'rewards/rejected': -6.944058895111084, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1744608879089355, 'policy_logps/rejected': -387.34027099609375, 'policy_logps/chosen': -358.1156921386719, 'referece_logps/rejected': -317.899658203125, 'referece_logps/chosen': -320.4197082519531, 'logits/rejected': -0.6281641125679016, 'logits/chosen': -0.5355015993118286, 'epoch': 1.53}


 25%|██▌       | 2732/10740 [13:45:52<37:47:06, 16.99s/it]
{'loss': 0.2614, 'learning_rate': 1.7474945541928221e-06, 'rewards/chosen': -3.316096305847168, 'rewards/rejected': -6.760456085205078, 'rewards/accuracies': 0.875, 'rewards/margins': 3.444359302520752, 'policy_logps/rejected': -355.4373779296875, 'policy_logps/chosen': -416.9443359375, 'referece_logps/rejected': -287.8328552246094, 'referece_logps/chosen': -383.7833251953125, 'logits/rejected': -0.7282565832138062, 'logits/chosen': -0.7430460453033447, 'epoch': 1.53}
[2024-04-02 08:59:28,530] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 25%|██▌       | 2734/10740 [13:46:30<39:56:35, 17.96s/it]

 25%|██▌       | 2735/10740 [13:46:51<42:04:49, 18.92s/it]
[2024-04-02 09:00:09,360] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 25%|██▌       | 2736/10740 [13:47:03<37:20:58, 16.80s/it]

 25%|██▌       | 2737/10740 [13:47:22<39:01:13, 17.55s/it]

 25%|██▌       | 2738/10740 [13:47:36<36:17:02, 16.32s/it]

 26%|██▌       | 2739/10740 [13:47:47<33:08:55, 14.92s/it]

 26%|██▌       | 2740/10740 [13:48:07<36:25:09, 16.39s/it]

 26%|██▌       | 2741/10740 [13:48:28<39:12:09, 17.64s/it]

 26%|██▌       | 2742/10740 [13:48:50<42:08:23, 18.97s/it]

 26%|██▌       | 2743/10740 [13:49:10<42:43:12, 19.23s/it]

 26%|██▌       | 2744/10740 [13:49:28<42:00:29, 18.91s/it]
{'loss': 0.2078, 'learning_rate': 1.7450856794696413e-06, 'rewards/chosen': -1.9212228059768677, 'rewards/rejected': -6.208689212799072, 'rewards/accuracies': 1.0, 'rewards/margins': 4.287466526031494, 'policy_logps/rejected': -439.806640625, 'policy_logps/chosen': -499.0279541015625, 'referece_logps/rejected': -377.7197265625, 'referece_logps/chosen': -479.81573486328125, 'logits/rejected': -0.13979282975196838, 'logits/chosen': -0.3224453926086426, 'epoch': 1.53}


 26%|██▌       | 2746/10740 [13:49:58<37:05:15, 16.70s/it]

 26%|██▌       | 2747/10740 [13:50:17<38:50:14, 17.49s/it]

 26%|██▌       | 2748/10740 [13:50:37<40:22:32, 18.19s/it]
{'loss': 0.2727, 'learning_rate': 1.7442805511134186e-06, 'rewards/chosen': -2.9603097438812256, 'rewards/rejected': -5.961390495300293, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0010805130004883, 'policy_logps/rejected': -329.25616455078125, 'policy_logps/chosen': -702.264892578125, 'referece_logps/rejected': -269.64227294921875, 'referece_logps/chosen': -672.661865234375, 'logits/rejected': -0.50667405128479, 'logits/chosen': -0.7655193209648132, 'epoch': 1.54}


 26%|██▌       | 2750/10740 [13:51:14<41:11:13, 18.56s/it]

 26%|██▌       | 2751/10740 [13:51:38<44:18:56, 19.97s/it]
[2024-04-02 09:04:55,763] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3032, 'learning_rate': 1.7436759939937132e-06, 'rewards/chosen': -2.6006343364715576, 'rewards/rejected': -4.708182334899902, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1075477600097656, 'policy_logps/rejected': -367.7168884277344, 'policy_logps/chosen': -385.56732177734375, 'referece_logps/rejected': -320.63507080078125, 'referece_logps/chosen': -359.5609436035156, 'logits/rejected': -0.36408984661102295, 'logits/chosen': -0.3278094530105591, 'epoch': 1.54}


 26%|██▌       | 2753/10740 [13:52:14<42:51:27, 19.32s/it]

 26%|██▌       | 2754/10740 [13:52:30<40:35:50, 18.30s/it]
[2024-04-02 09:05:48,635] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2755/10740 [13:52:48<39:54:30, 17.99s/it]

 26%|██▌       | 2756/10740 [13:53:06<39:50:39, 17.97s/it]

 26%|██▌       | 2757/10740 [13:53:19<36:50:20, 16.61s/it]

 26%|██▌       | 2758/10740 [13:53:35<36:24:59, 16.42s/it]

 26%|██▌       | 2759/10740 [13:53:52<36:29:10, 16.46s/it]

 26%|██▌       | 2760/10740 [13:54:11<38:27:26, 17.35s/it]
{'loss': 0.2389, 'learning_rate': 1.7418586721024093e-06, 'rewards/chosen': -3.497785806655884, 'rewards/rejected': -6.587069988250732, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0892844200134277, 'policy_logps/rejected': -361.7122802734375, 'policy_logps/chosen': -475.21221923828125, 'referece_logps/rejected': -295.8415832519531, 'referece_logps/chosen': -440.234375, 'logits/rejected': 0.5874722599983215, 'logits/chosen': 0.4572717547416687, 'epoch': 1.54}


 26%|██▌       | 2762/10740 [13:54:47<39:46:26, 17.95s/it]

 26%|██▌       | 2763/10740 [13:55:02<37:52:35, 17.09s/it]

 26%|██▌       | 2764/10740 [13:55:24<41:06:38, 18.56s/it]

 26%|██▌       | 2765/10740 [13:55:37<37:00:31, 16.71s/it]

 26%|██▌       | 2766/10740 [13:55:48<33:11:26, 14.98s/it]

 26%|██▌       | 2767/10740 [13:56:06<35:22:38, 15.97s/it]

 26%|██▌       | 2768/10740 [13:56:24<36:36:03, 16.53s/it]

 26%|██▌       | 2769/10740 [13:56:43<38:38:04, 17.45s/it]

 26%|██▌       | 2770/10740 [13:57:03<40:01:15, 18.08s/it]

 26%|██▌       | 2771/10740 [13:57:22<40:56:43, 18.50s/it]

 26%|██▌       | 2772/10740 [13:57:36<38:00:35, 17.17s/it]

 26%|██▌       | 2773/10740 [13:57:59<41:28:03, 18.74s/it]
[2024-04-02 09:11:17,084] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2774/10740 [13:58:19<42:15:55, 19.10s/it]
[2024-04-02 09:11:37,033] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2775/10740 [13:58:38<42:14:35, 19.09s/it]
{'loss': 0.2829, 'learning_rate': 1.7388176636249504e-06, 'rewards/chosen': -2.9208860397338867, 'rewards/rejected': -5.448873996734619, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5279881954193115, 'policy_logps/rejected': -324.0804443359375, 'policy_logps/chosen': -331.93414306640625, 'referece_logps/rejected': -269.5916748046875, 'referece_logps/chosen': -302.7252502441406, 'logits/rejected': -0.013539731502532959, 'logits/chosen': 0.09831637144088745, 'epoch': 1.55}


 26%|██▌       | 2777/10740 [13:59:19<44:21:37, 20.05s/it]

 26%|██▌       | 2778/10740 [13:59:36<42:04:49, 19.03s/it]

 26%|██▌       | 2779/10740 [13:59:55<42:22:12, 19.16s/it]

 26%|██▌       | 2780/10740 [14:00:06<36:43:39, 16.61s/it]

 26%|██▌       | 2781/10740 [14:00:21<35:34:34, 16.09s/it]

 26%|██▌       | 2782/10740 [14:00:32<32:00:56, 14.48s/it]

 26%|██▌       | 2783/10740 [14:00:46<31:36:34, 14.30s/it]

 26%|██▌       | 2784/10740 [14:01:02<32:59:20, 14.93s/it]

 26%|██▌       | 2785/10740 [14:01:21<35:36:39, 16.12s/it]

 26%|██▌       | 2786/10740 [14:01:43<39:44:03, 17.98s/it]
{'loss': 0.2516, 'learning_rate': 1.7365779780193028e-06, 'rewards/chosen': -3.097383975982666, 'rewards/rejected': -7.878807544708252, 'rewards/accuracies': 1.0, 'rewards/margins': 4.781423568725586, 'policy_logps/rejected': -495.35479736328125, 'policy_logps/chosen': -408.14190673828125, 'referece_logps/rejected': -416.5667419433594, 'referece_logps/chosen': -377.1680908203125, 'logits/rejected': -0.33961811661720276, 'logits/chosen': -0.032038018107414246, 'epoch': 1.56}


 26%|██▌       | 2788/10740 [14:02:22<41:36:27, 18.84s/it]

 26%|██▌       | 2789/10740 [14:02:39<40:06:35, 18.16s/it]

 26%|██▌       | 2790/10740 [14:02:57<40:08:15, 18.18s/it]

 26%|██▌       | 2791/10740 [14:03:13<38:47:05, 17.57s/it]

 26%|██▌       | 2792/10740 [14:03:25<35:16:55, 15.98s/it]

 26%|██▌       | 2793/10740 [14:03:45<37:41:21, 17.07s/it]

 26%|██▌       | 2794/10740 [14:04:05<39:24:13, 17.85s/it]

 26%|██▌       | 2795/10740 [14:04:21<38:22:25, 17.39s/it]

 26%|██▌       | 2796/10740 [14:04:32<34:20:31, 15.56s/it]

 26%|██▌       | 2797/10740 [14:04:49<34:59:08, 15.86s/it]

 26%|██▌       | 2798/10740 [14:05:05<35:11:57, 15.96s/it]
{'loss': 0.3924, 'learning_rate': 1.7341254403808288e-06, 'rewards/chosen': -3.98441219329834, 'rewards/rejected': -6.053839683532715, 'rewards/accuracies': 0.875, 'rewards/margins': 2.069427490234375, 'policy_logps/rejected': -470.5154724121094, 'policy_logps/chosen': -412.0535583496094, 'referece_logps/rejected': -409.97705078125, 'referece_logps/chosen': -372.20947265625, 'logits/rejected': -0.37298789620399475, 'logits/chosen': -0.29348742961883545, 'epoch': 1.56}


 26%|██▌       | 2800/10740 [14:05:39<37:09:18, 16.85s/it]

 26%|██▌       | 2801/10740 [14:05:53<35:07:44, 15.93s/it]

 26%|██▌       | 2802/10740 [14:06:09<35:01:53, 15.89s/it]

 26%|██▌       | 2803/10740 [14:06:28<37:18:15, 16.92s/it]
{'loss': 0.2546, 'learning_rate': 1.7331007101068191e-06, 'rewards/chosen': -2.1202995777130127, 'rewards/rejected': -4.57956600189209, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4592666625976562, 'policy_logps/rejected': -305.54046630859375, 'policy_logps/chosen': -337.527099609375, 'referece_logps/rejected': -259.7447814941406, 'referece_logps/chosen': -316.3241271972656, 'logits/rejected': 0.47052761912345886, 'logits/chosen': 0.47631630301475525, 'epoch': 1.57}


 26%|██▌       | 2805/10740 [14:07:02<37:16:39, 16.91s/it]
{'loss': 0.3968, 'learning_rate': 1.7326903511255381e-06, 'rewards/chosen': -3.068176507949829, 'rewards/rejected': -4.514708042144775, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4465312957763672, 'policy_logps/rejected': -334.1402893066406, 'policy_logps/chosen': -291.79376220703125, 'referece_logps/rejected': -288.99322509765625, 'referece_logps/chosen': -261.11199951171875, 'logits/rejected': 0.031019672751426697, 'logits/chosen': -0.03479848802089691, 'epoch': 1.57}

 26%|██▌       | 2806/10740 [14:07:14<33:29:24, 15.20s/it]


 26%|██▌       | 2808/10740 [14:07:46<34:52:11, 15.83s/it]
{'loss': 0.2584, 'learning_rate': 1.7320743129004262e-06, 'rewards/chosen': -2.690185070037842, 'rewards/rejected': -5.082266807556152, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3920817375183105, 'policy_logps/rejected': -375.474853515625, 'policy_logps/chosen': -476.139404296875, 'referece_logps/rejected': -324.6522216796875, 'referece_logps/chosen': -449.237548828125, 'logits/rejected': -0.7108786106109619, 'logits/chosen': -0.8236894607543945, 'epoch': 1.57}

 26%|██▌       | 2809/10740 [14:08:06<37:32:28, 17.04s/it]
[2024-04-02 09:21:46,190] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▌       | 2810/10740 [14:08:28<40:36:03, 18.43s/it]
[2024-04-02 09:22:06,214] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 26%|██▌       | 2812/10740 [14:09:05<40:10:21, 18.24s/it]

 26%|██▌       | 2813/10740 [14:09:16<35:53:10, 16.30s/it]

 26%|██▌       | 2814/10740 [14:09:33<35:52:49, 16.30s/it]

 26%|██▌       | 2815/10740 [14:09:49<35:46:20, 16.25s/it]

 26%|██▌       | 2816/10740 [14:10:07<36:55:08, 16.77s/it]

 26%|██▌       | 2817/10740 [14:10:23<36:45:43, 16.70s/it]

 26%|██▌       | 2818/10740 [14:10:43<38:25:08, 17.46s/it]

 26%|██▌       | 2819/10740 [14:11:03<40:16:03, 18.30s/it]

 26%|██▋       | 2820/10740 [14:11:23<41:13:18, 18.74s/it]

 26%|██▋       | 2821/10740 [14:11:43<42:18:54, 19.24s/it]
[2024-04-02 09:25:01,275] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2822/10740 [14:12:01<41:28:12, 18.85s/it]

 26%|██▋       | 2823/10740 [14:12:21<42:27:19, 19.31s/it]

 26%|██▋       | 2824/10740 [14:12:35<38:54:49, 17.70s/it]

 26%|██▋       | 2825/10740 [14:12:47<35:05:21, 15.96s/it]

 26%|██▋       | 2826/10740 [14:13:07<37:47:43, 17.19s/it]
{'loss': 0.3115, 'learning_rate': 1.7283655168670129e-06, 'rewards/chosen': -2.692122459411621, 'rewards/rejected': -8.338678359985352, 'rewards/accuracies': 0.875, 'rewards/margins': 5.646554946899414, 'policy_logps/rejected': -650.183349609375, 'policy_logps/chosen': -428.7631530761719, 'referece_logps/rejected': -566.7965698242188, 'referece_logps/chosen': -401.8419494628906, 'logits/rejected': -0.8740355968475342, 'logits/chosen': -0.7654955983161926, 'epoch': 1.58}


 26%|██▋       | 2828/10740 [14:13:47<41:11:23, 18.74s/it]
{'loss': 0.2818, 'learning_rate': 1.727952101484429e-06, 'rewards/chosen': -3.0815377235412598, 'rewards/rejected': -5.425004959106445, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3434672355651855, 'policy_logps/rejected': -256.9844970703125, 'policy_logps/chosen': -372.1689453125, 'referece_logps/rejected': -202.7344207763672, 'referece_logps/chosen': -341.35357666015625, 'logits/rejected': -0.7326488494873047, 'logits/chosen': -0.9536572098731995, 'epoch': 1.58}

 26%|██▋       | 2829/10740 [14:14:04<40:10:03, 18.28s/it]


 26%|██▋       | 2831/10740 [14:14:38<38:35:01, 17.56s/it]

 26%|██▋       | 2832/10740 [14:14:57<40:01:02, 18.22s/it]

 26%|██▋       | 2833/10740 [14:15:12<37:37:55, 17.13s/it]
{'loss': 0.2817, 'learning_rate': 1.7269174046992375e-06, 'rewards/chosen': -1.9733972549438477, 'rewards/rejected': -5.002936363220215, 'rewards/accuracies': 1.0, 'rewards/margins': 3.029539108276367, 'policy_logps/rejected': -308.53839111328125, 'policy_logps/chosen': -276.1180114746094, 'referece_logps/rejected': -258.5090637207031, 'referece_logps/chosen': -256.384033203125, 'logits/rejected': -0.4788057506084442, 'logits/chosen': -0.5786736607551575, 'epoch': 1.58}


 26%|██▋       | 2835/10740 [14:15:45<36:54:39, 16.81s/it]

 26%|██▋       | 2836/10740 [14:16:01<36:32:29, 16.64s/it]

 26%|██▋       | 2837/10740 [14:16:18<36:40:40, 16.71s/it]
[2024-04-02 09:29:36,247] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2838/10740 [14:16:35<36:46:07, 16.75s/it]
{'loss': 0.3247, 'learning_rate': 1.7258810550413298e-06, 'rewards/chosen': -3.30849552154541, 'rewards/rejected': -4.390793800354004, 'rewards/accuracies': 0.875, 'rewards/margins': 1.082298994064331, 'policy_logps/rejected': -261.7286682128906, 'policy_logps/chosen': -251.88290405273438, 'referece_logps/rejected': -217.8207244873047, 'referece_logps/chosen': -218.79795837402344, 'logits/rejected': -0.1363900750875473, 'logits/chosen': -0.14009247720241547, 'epoch': 1.59}


 26%|██▋       | 2840/10740 [14:17:13<39:22:28, 17.94s/it]

 26%|██▋       | 2841/10740 [14:17:31<39:22:32, 17.95s/it]
[2024-04-02 09:30:49,373] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2842/10740 [14:17:50<39:43:49, 18.11s/it]

 26%|██▋       | 2843/10740 [14:18:09<40:35:08, 18.50s/it]
[2024-04-02 09:31:27,282] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2844/10740 [14:18:30<42:16:03, 19.27s/it]
[2024-04-02 09:31:48,348] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2845/10740 [14:18:51<43:27:20, 19.82s/it]
[2024-04-02 09:32:09,433] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 26%|██▋       | 2846/10740 [14:19:10<42:54:50, 19.57s/it]

 27%|██▋       | 2847/10740 [14:19:27<41:19:04, 18.85s/it]
{'loss': 0.3411, 'learning_rate': 1.7240114679420782e-06, 'rewards/chosen': -2.3250443935394287, 'rewards/rejected': -4.413145065307617, 'rewards/accuracies': 0.625, 'rewards/margins': 2.088101387023926, 'policy_logps/rejected': -283.8295593261719, 'policy_logps/chosen': -258.1200256347656, 'referece_logps/rejected': -239.6980743408203, 'referece_logps/chosen': -234.8695831298828, 'logits/rejected': -0.6769247055053711, 'logits/chosen': -0.7439767718315125, 'epoch': 1.59}
[2024-04-02 09:33:07,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 27%|██▋       | 2849/10740 [14:20:09<43:37:48, 19.90s/it]

 27%|██▋       | 2850/10740 [14:20:29<43:37:01, 19.90s/it]
{'loss': 0.2785, 'learning_rate': 1.7233870862506055e-06, 'rewards/chosen': -2.819845676422119, 'rewards/rejected': -6.234894752502441, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4150490760803223, 'policy_logps/rejected': -363.8712158203125, 'policy_logps/chosen': -470.7218017578125, 'referece_logps/rejected': -301.5223083496094, 'referece_logps/chosen': -442.52337646484375, 'logits/rejected': -0.04901427775621414, 'logits/chosen': 0.07836124300956726, 'epoch': 1.59}


 27%|██▋       | 2852/10740 [14:21:04<40:54:46, 18.67s/it]

 27%|██▋       | 2853/10740 [14:21:16<36:53:21, 16.84s/it]

 27%|██▋       | 2854/10740 [14:21:34<37:38:29, 17.18s/it]
{'loss': 0.3311, 'learning_rate': 1.722553656303492e-06, 'rewards/chosen': -2.785825252532959, 'rewards/rejected': -4.024502754211426, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2386775016784668, 'policy_logps/rejected': -329.8636779785156, 'policy_logps/chosen': -289.1483459472656, 'referece_logps/rejected': -289.61865234375, 'referece_logps/chosen': -261.2900695800781, 'logits/rejected': -0.21874529123306274, 'logits/chosen': -0.10851489752531052, 'epoch': 1.59}


 27%|██▋       | 2856/10740 [14:22:17<42:18:22, 19.32s/it]
[2024-04-02 09:35:35,547] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2857/10740 [14:22:34<40:47:29, 18.63s/it]
{'loss': 0.2632, 'learning_rate': 1.7219278937375665e-06, 'rewards/chosen': -2.7452054023742676, 'rewards/rejected': -4.681702613830566, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9364969730377197, 'policy_logps/rejected': -492.59954833984375, 'policy_logps/chosen': -426.8174743652344, 'referece_logps/rejected': -445.78253173828125, 'referece_logps/chosen': -399.3653869628906, 'logits/rejected': -1.1272923946380615, 'logits/chosen': -1.0137830972671509, 'epoch': 1.6}


 27%|██▋       | 2859/10740 [14:23:09<40:13:47, 18.38s/it]

 27%|██▋       | 2860/10740 [14:23:26<39:14:48, 17.93s/it]

 27%|██▋       | 2861/10740 [14:23:46<40:24:49, 18.47s/it]
{'loss': 0.2958, 'learning_rate': 1.7210926244827517e-06, 'rewards/chosen': -3.961245059967041, 'rewards/rejected': -5.318267345428467, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3570220470428467, 'policy_logps/rejected': -334.01220703125, 'policy_logps/chosen': -380.5582580566406, 'referece_logps/rejected': -280.82952880859375, 'referece_logps/chosen': -340.94580078125, 'logits/rejected': -1.3137876987457275, 'logits/chosen': -1.167989730834961, 'epoch': 1.6}

 27%|██▋       | 2862/10740 [14:24:01<37:54:41, 17.32s/it]


 27%|██▋       | 2864/10740 [14:24:34<37:42:14, 17.23s/it]

 27%|██▋       | 2865/10740 [14:24:52<38:14:27, 17.48s/it]

 27%|██▋       | 2866/10740 [14:25:12<39:37:27, 18.12s/it]
{'loss': 0.3481, 'learning_rate': 1.7200470623912806e-06, 'rewards/chosen': -2.208660125732422, 'rewards/rejected': -5.083355903625488, 'rewards/accuracies': 0.875, 'rewards/margins': 2.874695301055908, 'policy_logps/rejected': -410.6395568847656, 'policy_logps/chosen': -375.40667724609375, 'referece_logps/rejected': -359.8060302734375, 'referece_logps/chosen': -353.3200988769531, 'logits/rejected': -0.03569899499416351, 'logits/chosen': 0.03366928547620773, 'epoch': 1.6}


 27%|██▋       | 2868/10740 [14:25:48<39:22:30, 18.01s/it]

 27%|██▋       | 2869/10740 [14:26:02<36:39:32, 16.77s/it]
{'loss': 0.3564, 'learning_rate': 1.7194189391036775e-06, 'rewards/chosen': -2.701916456222534, 'rewards/rejected': -6.417962074279785, 'rewards/accuracies': 0.875, 'rewards/margins': 3.716045618057251, 'policy_logps/rejected': -502.82379150390625, 'policy_logps/chosen': -451.5716857910156, 'referece_logps/rejected': -438.6441650390625, 'referece_logps/chosen': -424.5524597167969, 'logits/rejected': -0.3687170743942261, 'logits/chosen': -0.3069791793823242, 'epoch': 1.6}

 27%|██▋       | 2870/10740 [14:26:15<34:28:52, 15.77s/it]


 27%|██▋       | 2872/10740 [14:26:52<37:07:51, 16.99s/it]

 27%|██▋       | 2873/10740 [14:27:12<38:50:57, 17.78s/it]

 27%|██▋       | 2874/10740 [14:27:28<37:43:55, 17.27s/it]
{'loss': 0.2335, 'learning_rate': 1.7183707585535415e-06, 'rewards/chosen': -4.611891746520996, 'rewards/rejected': -7.1173176765441895, 'rewards/accuracies': 0.875, 'rewards/margins': 2.505425453186035, 'policy_logps/rejected': -381.7712097167969, 'policy_logps/chosen': -464.8816833496094, 'referece_logps/rejected': -310.5980224609375, 'referece_logps/chosen': -418.76275634765625, 'logits/rejected': 0.18489712476730347, 'logits/chosen': 0.025779426097869873, 'epoch': 1.61}

 27%|██▋       | 2875/10740 [14:27:45<37:51:48, 17.33s/it]

 27%|██▋       | 2876/10740 [14:28:01<36:53:00, 16.88s/it]

 27%|██▋       | 2877/10740 [14:28:19<37:29:30, 17.17s/it]


 27%|██▋       | 2879/10740 [14:28:49<34:16:37, 15.70s/it]

 27%|██▋       | 2880/10740 [14:29:07<35:38:42, 16.33s/it]

 27%|██▋       | 2881/10740 [14:29:25<36:56:29, 16.92s/it]

 27%|██▋       | 2882/10740 [14:29:44<38:39:10, 17.71s/it]

 27%|██▋       | 2883/10740 [14:30:04<40:10:07, 18.40s/it]

 27%|██▋       | 2884/10740 [14:30:20<38:14:56, 17.53s/it]

 27%|██▋       | 2885/10740 [14:30:34<36:19:12, 16.65s/it]
{'loss': 0.228, 'learning_rate': 1.7160590149983191e-06, 'rewards/chosen': -1.9397560358047485, 'rewards/rejected': -4.638181209564209, 'rewards/accuracies': 0.875, 'rewards/margins': 2.698425054550171, 'policy_logps/rejected': -257.3568420410156, 'policy_logps/chosen': -343.13641357421875, 'referece_logps/rejected': -210.9750213623047, 'referece_logps/chosen': -323.7388610839844, 'logits/rejected': -0.047832027077674866, 'logits/chosen': -0.06447368860244751, 'epoch': 1.61}


 27%|██▋       | 2887/10740 [14:31:18<42:16:01, 19.38s/it]
[2024-04-02 09:44:36,437] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2888/10740 [14:31:36<41:30:00, 19.03s/it]

 27%|██▋       | 2889/10740 [14:31:49<37:07:17, 17.02s/it]

 27%|██▋       | 2890/10740 [14:32:05<36:39:02, 16.81s/it]
{'loss': 0.1996, 'learning_rate': 1.715005615850293e-06, 'rewards/chosen': -2.9206089973449707, 'rewards/rejected': -6.924942493438721, 'rewards/accuracies': 1.0, 'rewards/margins': 4.004333019256592, 'policy_logps/rejected': -335.6029968261719, 'policy_logps/chosen': -477.08477783203125, 'referece_logps/rejected': -266.3535461425781, 'referece_logps/chosen': -447.87872314453125, 'logits/rejected': 0.774933934211731, 'logits/chosen': 0.6843360066413879, 'epoch': 1.61}

 27%|██▋       | 2891/10740 [14:32:24<38:08:50, 17.50s/it]

 27%|██▋       | 2892/10740 [14:32:46<40:39:04, 18.65s/it]

 27%|██▋       | 2893/10740 [14:33:03<40:10:11, 18.43s/it]

 27%|██▋       | 2894/10740 [14:33:20<39:15:54, 18.02s/it]


 27%|██▋       | 2896/10740 [14:33:55<38:51:40, 17.84s/it]
{'loss': 0.369, 'learning_rate': 1.7137393910441218e-06, 'rewards/chosen': -2.4670939445495605, 'rewards/rejected': -4.534907341003418, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0678136348724365, 'policy_logps/rejected': -338.5738220214844, 'policy_logps/chosen': -347.55853271484375, 'referece_logps/rejected': -293.2247619628906, 'referece_logps/chosen': -322.8876037597656, 'logits/rejected': -1.416657567024231, 'logits/chosen': -1.3349997997283936, 'epoch': 1.62}

 27%|██▋       | 2897/10740 [14:34:14<39:47:21, 18.26s/it]


 27%|██▋       | 2899/10740 [14:34:45<36:11:30, 16.62s/it]
{'loss': 0.2952, 'learning_rate': 1.7131054020111474e-06, 'rewards/chosen': -3.0186009407043457, 'rewards/rejected': -5.8321099281311035, 'rewards/accuracies': 1.0, 'rewards/margins': 2.813508987426758, 'policy_logps/rejected': -309.8564453125, 'policy_logps/chosen': -324.51324462890625, 'referece_logps/rejected': -251.53533935546875, 'referece_logps/chosen': -294.3272399902344, 'logits/rejected': 0.2092137187719345, 'logits/chosen': 0.18872126936912537, 'epoch': 1.62}

 27%|██▋       | 2900/10740 [14:35:04<37:48:44, 17.36s/it]


 27%|██▋       | 2902/10740 [14:35:33<35:25:01, 16.27s/it]
{'loss': 0.3506, 'learning_rate': 1.7124708292500425e-06, 'rewards/chosen': -3.614405393600464, 'rewards/rejected': -5.655239105224609, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0408334732055664, 'policy_logps/rejected': -578.2777099609375, 'policy_logps/chosen': -522.2030029296875, 'referece_logps/rejected': -521.725341796875, 'referece_logps/chosen': -486.0589599609375, 'logits/rejected': -0.656173825263977, 'logits/chosen': -0.7784600853919983, 'epoch': 1.62}


 27%|██▋       | 2904/10740 [14:36:01<33:04:03, 15.19s/it]

 27%|██▋       | 2905/10740 [14:36:19<34:53:44, 16.03s/it]
{'loss': 0.3394, 'learning_rate': 1.71183567328025e-06, 'rewards/chosen': -2.662860155105591, 'rewards/rejected': -4.649990558624268, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9871301651000977, 'policy_logps/rejected': -348.26495361328125, 'policy_logps/chosen': -424.25921630859375, 'referece_logps/rejected': -301.76507568359375, 'referece_logps/chosen': -397.6306457519531, 'logits/rejected': -0.8097458481788635, 'logits/chosen': -0.9065181612968445, 'epoch': 1.62}

 27%|██▋       | 2906/10740 [14:36:37<36:02:07, 16.56s/it]

 27%|██▋       | 2907/10740 [14:36:56<38:01:50, 17.48s/it]

 27%|██▋       | 2908/10740 [14:37:14<37:50:07, 17.39s/it]


 27%|██▋       | 2910/10740 [14:37:49<38:37:41, 17.76s/it]
{'loss': 0.3039, 'learning_rate': 1.7107757853903038e-06, 'rewards/chosen': -2.385251045227051, 'rewards/rejected': -5.6902313232421875, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3049802780151367, 'policy_logps/rejected': -397.6082458496094, 'policy_logps/chosen': -318.9115905761719, 'referece_logps/rejected': -340.7059326171875, 'referece_logps/chosen': -295.05914306640625, 'logits/rejected': -0.947973906993866, 'logits/chosen': -0.7631707787513733, 'epoch': 1.63}


 27%|██▋       | 2912/10740 [14:38:27<39:57:24, 18.38s/it]

 27%|██▋       | 2913/10740 [14:38:47<41:13:14, 18.96s/it]

 27%|██▋       | 2914/10740 [14:39:05<40:35:37, 18.67s/it]
{'loss': 0.3419, 'learning_rate': 1.709926711320341e-06, 'rewards/chosen': -1.6937525272369385, 'rewards/rejected': -4.575608253479004, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8818557262420654, 'policy_logps/rejected': -305.7267761230469, 'policy_logps/chosen': -401.4914855957031, 'referece_logps/rejected': -259.970703125, 'referece_logps/chosen': -384.553955078125, 'logits/rejected': -0.2870368957519531, 'logits/chosen': -0.3405013382434845, 'epoch': 1.63}

 27%|██▋       | 2915/10740 [14:39:26<41:37:29, 19.15s/it]

 27%|██▋       | 2916/10740 [14:39:46<42:28:54, 19.55s/it]

 27%|██▋       | 2917/10740 [14:40:06<42:52:10, 19.73s/it]

 27%|██▋       | 2918/10740 [14:40:26<43:08:44, 19.86s/it]


 27%|██▋       | 2920/10740 [14:40:59<39:20:36, 18.11s/it]
{'loss': 0.3337, 'learning_rate': 1.7086511635148923e-06, 'rewards/chosen': -2.391010284423828, 'rewards/rejected': -5.319783687591553, 'rewards/accuracies': 0.75, 'rewards/margins': 2.928774118423462, 'policy_logps/rejected': -292.89141845703125, 'policy_logps/chosen': -401.908203125, 'referece_logps/rejected': -239.6935577392578, 'referece_logps/chosen': -377.99810791015625, 'logits/rejected': 0.15528351068496704, 'logits/chosen': 0.029702097177505493, 'epoch': 1.63}

 27%|██▋       | 2921/10740 [14:41:20<41:22:31, 19.05s/it]


 27%|██▋       | 2923/10740 [14:41:47<35:15:22, 16.24s/it]
{'loss': 0.2281, 'learning_rate': 1.7080125192279935e-06, 'rewards/chosen': -2.8505070209503174, 'rewards/rejected': -4.729218482971191, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8787113428115845, 'policy_logps/rejected': -376.4969787597656, 'policy_logps/chosen': -349.6341552734375, 'referece_logps/rejected': -329.204833984375, 'referece_logps/chosen': -321.12908935546875, 'logits/rejected': -0.2178690880537033, 'logits/chosen': -0.1278541386127472, 'epoch': 1.63}

 27%|██▋       | 2924/10740 [14:41:58<31:55:08, 14.70s/it]

 27%|██▋       | 2925/10740 [14:42:10<30:08:07, 13.88s/it]


 27%|██▋       | 2927/10740 [14:42:44<34:06:16, 15.71s/it]
{'loss': 0.2107, 'learning_rate': 1.707160092065958e-06, 'rewards/chosen': -2.3516685962677, 'rewards/rejected': -5.522963047027588, 'rewards/accuracies': 1.0, 'rewards/margins': 3.171294927597046, 'policy_logps/rejected': -478.4884338378906, 'policy_logps/chosen': -453.3128967285156, 'referece_logps/rejected': -423.2587890625, 'referece_logps/chosen': -429.79620361328125, 'logits/rejected': -0.071348637342453, 'logits/chosen': -0.07142917066812515, 'epoch': 1.64}

 27%|██▋       | 2928/10740 [14:42:59<33:52:22, 15.61s/it]

 27%|██▋       | 2929/10740 [14:43:16<34:46:33, 16.03s/it]

 27%|██▋       | 2930/10740 [14:43:38<38:43:51, 17.85s/it]

 27%|██▋       | 2931/10740 [14:43:50<34:54:37, 16.09s/it]

 27%|██▋       | 2932/10740 [14:44:08<36:14:37, 16.71s/it]

 27%|██▋       | 2933/10740 [14:44:28<38:11:31, 17.61s/it]

 27%|██▋       | 2934/10740 [14:44:49<40:30:39, 18.68s/it]


 27%|██▋       | 2936/10740 [14:45:22<37:22:31, 17.24s/it]
{'loss': 0.3942, 'learning_rate': 1.7052383702442712e-06, 'rewards/chosen': -3.1208701133728027, 'rewards/rejected': -6.090379238128662, 'rewards/accuracies': 0.75, 'rewards/margins': 2.9695091247558594, 'policy_logps/rejected': -295.4826965332031, 'policy_logps/chosen': -313.9671325683594, 'referece_logps/rejected': -234.5789031982422, 'referece_logps/chosen': -282.7584533691406, 'logits/rejected': -0.3715735375881195, 'logits/chosen': -0.21972547471523285, 'epoch': 1.64}
[2024-04-02 09:59:01,318] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 2937/10740 [14:45:43<39:58:14, 18.44s/it]

 27%|██▋       | 2938/10740 [14:45:59<38:15:04, 17.65s/it]

 27%|██▋       | 2939/10740 [14:46:17<38:41:15, 17.85s/it]

 27%|██▋       | 2940/10740 [14:46:37<39:46:46, 18.36s/it]

 27%|██▋       | 2941/10740 [14:46:53<38:35:40, 17.82s/it]

 27%|██▋       | 2942/10740 [14:47:13<39:50:27, 18.39s/it]

 27%|██▋       | 2943/10740 [14:47:31<39:41:47, 18.33s/it]


 27%|██▋       | 2945/10740 [14:48:10<40:09:26, 18.55s/it]
{'loss': 0.2771, 'learning_rate': 1.7033114528298302e-06, 'rewards/chosen': -2.6838064193725586, 'rewards/rejected': -4.074642181396484, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3908360004425049, 'policy_logps/rejected': -252.2282257080078, 'policy_logps/chosen': -399.2120666503906, 'referece_logps/rejected': -211.4818115234375, 'referece_logps/chosen': -372.37396240234375, 'logits/rejected': -0.5688598155975342, 'logits/chosen': -0.60573410987854, 'epoch': 1.65}

 27%|██▋       | 2946/10740 [14:48:29<40:29:55, 18.71s/it]

 27%|██▋       | 2947/10740 [14:48:46<39:50:05, 18.40s/it]


 27%|██▋       | 2949/10740 [14:49:24<40:33:54, 18.74s/it]
{'loss': 0.3585, 'learning_rate': 1.7024533810804279e-06, 'rewards/chosen': -2.3252456188201904, 'rewards/rejected': -3.8663790225982666, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5411336421966553, 'policy_logps/rejected': -409.4144287109375, 'policy_logps/chosen': -326.29193115234375, 'referece_logps/rejected': -370.7506408691406, 'referece_logps/chosen': -303.0395202636719, 'logits/rejected': 0.8188190460205078, 'logits/chosen': 0.7887210845947266, 'epoch': 1.65}


 27%|██▋       | 2951/10740 [14:49:56<36:59:42, 17.10s/it]
{'loss': 0.262, 'learning_rate': 1.7020239617885222e-06, 'rewards/chosen': -2.353266716003418, 'rewards/rejected': -4.501656532287598, 'rewards/accuracies': 0.75, 'rewards/margins': 2.148390293121338, 'policy_logps/rejected': -324.158935546875, 'policy_logps/chosen': -434.971923828125, 'referece_logps/rejected': -279.142333984375, 'referece_logps/chosen': -411.4393005371094, 'logits/rejected': -0.4281701445579529, 'logits/chosen': -0.30354252457618713, 'epoch': 1.65}

 27%|██▋       | 2952/10740 [14:50:09<34:02:57, 15.74s/it]

 27%|██▋       | 2953/10740 [14:50:28<36:33:31, 16.90s/it]


 28%|██▊       | 2955/10740 [14:51:06<38:11:16, 17.66s/it]

 28%|██▊       | 2956/10740 [14:51:24<38:32:41, 17.83s/it]

 28%|██▊       | 2957/10740 [14:51:36<34:37:14, 16.01s/it]

 28%|██▊       | 2958/10740 [14:51:56<37:13:45, 17.22s/it]
{'loss': 0.239, 'learning_rate': 1.7005189839927444e-06, 'rewards/chosen': -2.597822427749634, 'rewards/rejected': -5.740788459777832, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1429665088653564, 'policy_logps/rejected': -444.02569580078125, 'policy_logps/chosen': -325.68701171875, 'referece_logps/rejected': -386.6177978515625, 'referece_logps/chosen': -299.70880126953125, 'logits/rejected': -0.43101245164871216, 'logits/chosen': -0.07778853178024292, 'epoch': 1.65}

 28%|██▊       | 2959/10740 [14:52:15<38:43:16, 17.91s/it]
[2024-04-02 10:05:55,105] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2960/10740 [14:52:37<40:57:34, 18.95s/it]
[2024-04-02 10:06:09,852] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2961/10740 [14:52:52<38:13:43, 17.69s/it]

 28%|██▊       | 2962/10740 [14:53:05<35:16:07, 16.32s/it]

 28%|██▊       | 2963/10740 [14:53:19<34:02:26, 15.76s/it]

 28%|██▊       | 2964/10740 [14:53:36<34:25:29, 15.94s/it]

 28%|██▊       | 2965/10740 [14:53:53<35:07:29, 16.26s/it]
[2024-04-02 10:07:29,013] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 2966/10740 [14:54:11<36:22:49, 16.85s/it]

 28%|██▊       | 2967/10740 [14:54:28<36:20:41, 16.83s/it]

 28%|██▊       | 2968/10740 [14:54:43<35:44:46, 16.56s/it]


 28%|██▊       | 2970/10740 [14:55:15<35:19:00, 16.36s/it]
{'loss': 0.2401, 'learning_rate': 1.6979317623954472e-06, 'rewards/chosen': -3.711780071258545, 'rewards/rejected': -5.521265029907227, 'rewards/accuracies': 0.5, 'rewards/margins': 1.8094851970672607, 'policy_logps/rejected': -438.56854248046875, 'policy_logps/chosen': -376.9037780761719, 'referece_logps/rejected': -383.35589599609375, 'referece_logps/chosen': -339.7859802246094, 'logits/rejected': -0.6668131351470947, 'logits/chosen': -0.6490239500999451, 'epoch': 1.66}

 28%|██▊       | 2971/10740 [14:55:32<35:54:18, 16.64s/it]


 28%|██▊       | 2973/10740 [14:56:10<38:42:44, 17.94s/it]
{'loss': 0.2663, 'learning_rate': 1.6972835274027744e-06, 'rewards/chosen': -3.058311939239502, 'rewards/rejected': -7.93223237991333, 'rewards/accuracies': 1.0, 'rewards/margins': 4.873919486999512, 'policy_logps/rejected': -402.806884765625, 'policy_logps/chosen': -404.97479248046875, 'referece_logps/rejected': -323.48455810546875, 'referece_logps/chosen': -374.39166259765625, 'logits/rejected': 0.0018502157181501389, 'logits/chosen': -0.06925762444734573, 'epoch': 1.66}

 28%|██▊       | 2974/10740 [14:56:27<37:51:08, 17.55s/it]


 28%|██▊       | 2976/10740 [14:57:04<38:54:20, 18.04s/it]
{'loss': 0.2031, 'learning_rate': 1.6966347216333147e-06, 'rewards/chosen': -2.8301477432250977, 'rewards/rejected': -6.217977046966553, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3878285884857178, 'policy_logps/rejected': -302.9652099609375, 'policy_logps/chosen': -480.4836120605469, 'referece_logps/rejected': -240.78543090820312, 'referece_logps/chosen': -452.18212890625, 'logits/rejected': -0.27469438314437866, 'logits/chosen': -0.5087119936943054, 'epoch': 1.66}


 28%|██▊       | 2978/10740 [14:57:40<38:30:07, 17.86s/it]
{'loss': 0.201, 'learning_rate': 1.6962018676177094e-06, 'rewards/chosen': -3.39493989944458, 'rewards/rejected': -7.995560646057129, 'rewards/accuracies': 0.875, 'rewards/margins': 4.600621223449707, 'policy_logps/rejected': -495.9400329589844, 'policy_logps/chosen': -349.23895263671875, 'referece_logps/rejected': -415.9844665527344, 'referece_logps/chosen': -315.2895812988281, 'logits/rejected': -0.8494744300842285, 'logits/chosen': -0.5967851281166077, 'epoch': 1.66}


 28%|██▊       | 2980/10740 [14:58:18<40:04:48, 18.59s/it]
{'loss': 0.2358, 'learning_rate': 1.695768760317041e-06, 'rewards/chosen': -3.167757034301758, 'rewards/rejected': -4.77453088760376, 'rewards/accuracies': 0.625, 'rewards/margins': 1.606774091720581, 'policy_logps/rejected': -231.4806671142578, 'policy_logps/chosen': -515.316650390625, 'referece_logps/rejected': -183.73536682128906, 'referece_logps/chosen': -483.6390686035156, 'logits/rejected': -0.6536581516265869, 'logits/chosen': -0.8340147733688354, 'epoch': 1.66}

 28%|██▊       | 2981/10740 [14:58:40<41:53:25, 19.44s/it]

 28%|██▊       | 2982/10740 [14:58:57<40:32:24, 18.81s/it]

 28%|██▊       | 2983/10740 [14:59:17<41:09:29, 19.10s/it]


 28%|██▊       | 2985/10740 [14:59:51<38:44:37, 17.99s/it]
{'loss': 0.1697, 'learning_rate': 1.694684884977489e-06, 'rewards/chosen': -1.8148444890975952, 'rewards/rejected': -4.100409507751465, 'rewards/accuracies': 0.75, 'rewards/margins': 2.28556489944458, 'policy_logps/rejected': -260.3912048339844, 'policy_logps/chosen': -291.6927185058594, 'referece_logps/rejected': -219.38711547851562, 'referece_logps/chosen': -273.54425048828125, 'logits/rejected': -1.330536961555481, 'logits/chosen': -1.4025334119796753, 'epoch': 1.67}

 28%|██▊       | 2986/10740 [15:00:10<39:32:38, 18.36s/it]

 28%|██▊       | 2987/10740 [15:00:29<40:22:07, 18.74s/it]

 28%|██▊       | 2988/10740 [15:00:48<40:04:49, 18.61s/it]
[2024-04-02 10:14:25,781] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 28%|██▊       | 2990/10740 [15:01:25<39:37:56, 18.41s/it]
{'loss': 0.2619, 'learning_rate': 1.693599430055876e-06, 'rewards/chosen': -2.249924421310425, 'rewards/rejected': -4.837889194488525, 'rewards/accuracies': 1.0, 'rewards/margins': 2.587965250015259, 'policy_logps/rejected': -289.26995849609375, 'policy_logps/chosen': -408.770263671875, 'referece_logps/rejected': -240.8910675048828, 'referece_logps/chosen': -386.27099609375, 'logits/rejected': -0.8647982478141785, 'logits/chosen': -0.9882673621177673, 'epoch': 1.67}

 28%|██▊       | 2991/10740 [15:01:41<38:23:19, 17.83s/it]

 28%|██▊       | 2992/10740 [15:02:00<38:46:24, 18.02s/it]

 28%|██▊       | 2993/10740 [15:02:14<36:31:50, 16.98s/it]


 28%|██▊       | 2995/10740 [15:02:49<36:33:31, 16.99s/it]
{'loss': 0.334, 'learning_rate': 1.692512398020321e-06, 'rewards/chosen': -2.5180585384368896, 'rewards/rejected': -4.973888874053955, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4558300971984863, 'policy_logps/rejected': -299.3376159667969, 'policy_logps/chosen': -250.45147705078125, 'referece_logps/rejected': -249.5987091064453, 'referece_logps/chosen': -225.27088928222656, 'logits/rejected': -0.5775853395462036, 'logits/chosen': -0.5720995664596558, 'epoch': 1.67}

 28%|██▊       | 2996/10740 [15:03:02<34:10:18, 15.89s/it]

 28%|██▊       | 2997/10740 [15:03:18<33:50:04, 15.73s/it]

 28%|██▊       | 2998/10740 [15:03:34<34:21:37, 15.98s/it]

 28%|██▊       | 2999/10740 [15:03:48<32:42:39, 15.21s/it]

 28%|██▊       | 3000/10740 [15:04:02<32:02:47, 14.91s/it]

 28%|██▊       | 3001/10740 [15:04:35<43:44:50, 20.35s/it]

 28%|██▊       | 3002/10740 [15:04:48<38:58:44, 18.13s/it]

 28%|██▊       | 3003/10740 [15:05:03<36:47:06, 17.12s/it]


 28%|██▊       | 3005/10740 [15:05:41<39:25:27, 18.35s/it]
{'loss': 0.2504, 'learning_rate': 1.6903336124977857e-06, 'rewards/chosen': -3.0900332927703857, 'rewards/rejected': -5.6571478843688965, 'rewards/accuracies': 1.0, 'rewards/margins': 2.56711483001709, 'policy_logps/rejected': -283.4527282714844, 'policy_logps/chosen': -296.52386474609375, 'referece_logps/rejected': -226.88124084472656, 'referece_logps/chosen': -265.62353515625, 'logits/rejected': -0.1946711540222168, 'logits/chosen': -0.2803580164909363, 'epoch': 1.68}

 28%|██▊       | 3006/10740 [15:06:01<40:13:15, 18.72s/it]

 28%|██▊       | 3007/10740 [15:06:20<40:45:01, 18.97s/it]

 28%|██▊       | 3008/10740 [15:06:34<37:10:17, 17.31s/it]

 28%|██▊       | 3009/10740 [15:06:49<35:48:56, 16.68s/it]

 28%|██▊       | 3010/10740 [15:07:00<32:01:47, 14.92s/it]

 28%|██▊       | 3011/10740 [15:07:20<35:06:10, 16.35s/it]

 28%|██▊       | 3012/10740 [15:07:35<34:23:28, 16.02s/it]

 28%|██▊       | 3013/10740 [15:07:47<31:59:06, 14.90s/it]

 28%|██▊       | 3014/10740 [15:08:07<35:13:43, 16.42s/it]
[2024-04-02 10:21:46,361] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3015/10740 [15:08:28<38:15:46, 17.83s/it]
[2024-04-02 10:22:04,638] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3016/10740 [15:08:46<38:32:40, 17.96s/it]

 28%|██▊       | 3017/10740 [15:09:03<37:38:18, 17.54s/it]

 28%|██▊       | 3018/10740 [15:09:20<37:36:19, 17.53s/it]

 28%|██▊       | 3019/10740 [15:09:39<38:06:33, 17.77s/it]

 28%|██▊       | 3020/10740 [15:09:52<34:52:09, 16.26s/it]

 28%|██▊       | 3021/10740 [15:10:10<36:18:51, 16.94s/it]

 28%|██▊       | 3022/10740 [15:10:31<38:34:45, 18.00s/it]

 28%|██▊       | 3023/10740 [15:10:46<37:15:49, 17.38s/it]

 28%|██▊       | 3024/10740 [15:11:08<39:59:46, 18.66s/it]

 28%|██▊       | 3025/10740 [15:11:26<39:37:14, 18.49s/it]


 28%|██▊       | 3027/10740 [15:12:06<40:55:55, 19.10s/it]
{'loss': 0.2849, 'learning_rate': 1.6855182111386075e-06, 'rewards/chosen': -2.633373737335205, 'rewards/rejected': -5.412992477416992, 'rewards/accuracies': 0.875, 'rewards/margins': 2.779618501663208, 'policy_logps/rejected': -342.69415283203125, 'policy_logps/chosen': -380.14447021484375, 'referece_logps/rejected': -288.56427001953125, 'referece_logps/chosen': -353.81072998046875, 'logits/rejected': -0.4345412850379944, 'logits/chosen': -0.6568241119384766, 'epoch': 1.69}

 28%|██▊       | 3028/10740 [15:12:18<36:41:07, 17.12s/it]

 28%|██▊       | 3029/10740 [15:12:39<38:57:16, 18.19s/it]
[2024-04-02 10:26:18,065] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3030/10740 [15:13:00<40:49:40, 19.06s/it]

 28%|██▊       | 3031/10740 [15:13:20<41:23:04, 19.33s/it]

 28%|██▊       | 3032/10740 [15:13:35<39:00:09, 18.22s/it]

 28%|██▊       | 3033/10740 [15:13:49<35:51:10, 16.75s/it]

 28%|██▊       | 3034/10740 [15:14:09<37:58:38, 17.74s/it]

 28%|██▊       | 3035/10740 [15:14:29<39:34:26, 18.49s/it]

 28%|██▊       | 3036/10740 [15:14:46<38:23:37, 17.94s/it]

 28%|██▊       | 3037/10740 [15:15:01<36:49:35, 17.21s/it]

 28%|██▊       | 3038/10740 [15:15:22<39:26:04, 18.43s/it]

 28%|██▊       | 3039/10740 [15:15:41<39:25:11, 18.43s/it]


 28%|██▊       | 3041/10740 [15:16:14<37:59:08, 17.76s/it]
{'loss': 0.3022, 'learning_rate': 1.6824381393535202e-06, 'rewards/chosen': -2.1884069442749023, 'rewards/rejected': -6.111607551574707, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9232006072998047, 'policy_logps/rejected': -493.6005859375, 'policy_logps/chosen': -380.40350341796875, 'referece_logps/rejected': -432.4845275878906, 'referece_logps/chosen': -358.5194091796875, 'logits/rejected': -0.0154448002576828, 'logits/chosen': -0.02080029994249344, 'epoch': 1.7}

 28%|██▊       | 3042/10740 [15:16:35<40:15:35, 18.83s/it]
[2024-04-02 10:30:14,313] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3043/10740 [15:16:56<41:33:39, 19.44s/it]
[2024-04-02 10:30:36,193] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 3044/10740 [15:17:18<43:07:19, 20.17s/it]

 28%|██▊       | 3045/10740 [15:17:38<43:06:54, 20.17s/it]

 28%|██▊       | 3046/10740 [15:17:58<42:40:36, 19.97s/it]

 28%|██▊       | 3047/10740 [15:18:17<42:12:45, 19.75s/it]

 28%|██▊       | 3048/10740 [15:18:34<40:41:16, 19.04s/it]

 28%|██▊       | 3049/10740 [15:18:54<40:59:06, 19.18s/it]

 28%|██▊       | 3050/10740 [15:19:09<38:31:03, 18.03s/it]

 28%|██▊       | 3051/10740 [15:19:23<36:06:53, 16.91s/it]


 28%|██▊       | 3053/10740 [15:20:02<39:11:27, 18.35s/it]
{'loss': 0.3311, 'learning_rate': 1.6797883929209463e-06, 'rewards/chosen': -3.074723720550537, 'rewards/rejected': -6.766539096832275, 'rewards/accuracies': 1.0, 'rewards/margins': 3.691815137863159, 'policy_logps/rejected': -519.841552734375, 'policy_logps/chosen': -371.255859375, 'referece_logps/rejected': -452.1761779785156, 'referece_logps/chosen': -340.5086364746094, 'logits/rejected': -1.5978407859802246, 'logits/chosen': -1.2890034914016724, 'epoch': 1.71}

 28%|██▊       | 3054/10740 [15:20:13<34:15:27, 16.05s/it]

 28%|██▊       | 3055/10740 [15:20:32<36:14:37, 16.98s/it]

 28%|██▊       | 3056/10740 [15:20:46<34:12:25, 16.03s/it]

 28%|██▊       | 3057/10740 [15:20:58<31:46:42, 14.89s/it]

 28%|██▊       | 3058/10740 [15:21:17<34:30:45, 16.17s/it]

 28%|██▊       | 3059/10740 [15:21:36<36:25:04, 17.07s/it]

 28%|██▊       | 3060/10740 [15:21:50<34:32:33, 16.19s/it]

 29%|██▊       | 3061/10740 [15:22:08<35:08:52, 16.48s/it]

 29%|██▊       | 3062/10740 [15:22:28<37:37:21, 17.64s/it]

 29%|██▊       | 3063/10740 [15:22:48<39:00:56, 18.30s/it]

 29%|██▊       | 3064/10740 [15:23:09<40:45:43, 19.12s/it]

 29%|██▊       | 3065/10740 [15:23:29<41:06:58, 19.29s/it]

 29%|██▊       | 3066/10740 [15:23:47<40:28:07, 18.98s/it]

 29%|██▊       | 3067/10740 [15:24:08<41:41:24, 19.56s/it]

 29%|██▊       | 3068/10740 [15:24:19<36:05:18, 16.93s/it]

 29%|██▊       | 3069/10740 [15:24:37<36:49:05, 17.28s/it]

 29%|██▊       | 3070/10740 [15:24:51<35:16:07, 16.55s/it]

 29%|██▊       | 3071/10740 [15:25:06<33:47:20, 15.86s/it]

 29%|██▊       | 3072/10740 [15:25:25<36:09:31, 16.98s/it]

 29%|██▊       | 3073/10740 [15:25:44<36:59:14, 17.37s/it]


 29%|██▊       | 3075/10740 [15:26:12<34:29:09, 16.20s/it]

 29%|██▊       | 3076/10740 [15:26:29<34:52:25, 16.38s/it]

 29%|██▊       | 3077/10740 [15:26:46<34:58:08, 16.43s/it]

 29%|██▊       | 3078/10740 [15:27:05<36:55:04, 17.35s/it]

 29%|██▊       | 3079/10740 [15:27:25<38:20:25, 18.02s/it]

 29%|██▊       | 3080/10740 [15:27:47<40:56:02, 19.24s/it]

 29%|██▊       | 3081/10740 [15:28:08<42:03:59, 19.77s/it]
[2024-04-02 10:41:26,230] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▊       | 3082/10740 [15:28:25<40:11:03, 18.89s/it]
[2024-04-02 10:41:43,062] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▊       | 3083/10740 [15:28:42<39:14:10, 18.45s/it]

 29%|██▊       | 3084/10740 [15:29:04<41:26:52, 19.49s/it]

 29%|██▊       | 3085/10740 [15:29:18<37:59:36, 17.87s/it]

 29%|██▊       | 3086/10740 [15:29:34<36:49:55, 17.32s/it]

 29%|██▊       | 3087/10740 [15:29:54<38:34:35, 18.15s/it]
[2024-04-02 10:43:12,601] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▉       | 3088/10740 [15:30:11<37:33:04, 17.67s/it]

 29%|██▉       | 3089/10740 [15:30:28<37:06:08, 17.46s/it]

 29%|██▉       | 3090/10740 [15:30:46<37:27:41, 17.63s/it]

 29%|██▉       | 3091/10740 [15:31:05<38:40:03, 18.20s/it]

 29%|██▉       | 3092/10740 [15:31:24<38:51:54, 18.29s/it]

 29%|██▉       | 3093/10740 [15:31:44<39:39:23, 18.67s/it]

 29%|██▉       | 3094/10740 [15:32:06<41:58:46, 19.77s/it]

 29%|██▉       | 3095/10740 [15:32:19<37:45:36, 17.78s/it]

 29%|██▉       | 3096/10740 [15:32:38<38:23:12, 18.08s/it]

 29%|██▉       | 3097/10740 [15:32:55<37:39:00, 17.73s/it]

 29%|██▉       | 3098/10740 [15:33:15<39:25:59, 18.58s/it]

 29%|██▉       | 3099/10740 [15:33:31<37:36:14, 17.72s/it]

 29%|██▉       | 3100/10740 [15:33:53<40:25:16, 19.05s/it]
{'loss': 0.2956, 'learning_rate': 1.6693248208576688e-06, 'rewards/chosen': -2.833408832550049, 'rewards/rejected': -6.363898754119873, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5304901599884033, 'policy_logps/rejected': -365.591796875, 'policy_logps/chosen': -370.7884826660156, 'referece_logps/rejected': -301.9527893066406, 'referece_logps/chosen': -342.4543762207031, 'logits/rejected': -0.5215073823928833, 'logits/chosen': -0.7683200240135193, 'epoch': 1.73}

 29%|██▉       | 3101/10740 [15:34:05<36:10:17, 17.05s/it]


 29%|██▉       | 3103/10740 [15:34:41<36:34:44, 17.24s/it]

 29%|██▉       | 3104/10740 [15:35:03<39:28:49, 18.61s/it]

 29%|██▉       | 3105/10740 [15:35:15<35:09:06, 16.57s/it]

 29%|██▉       | 3106/10740 [15:35:35<37:34:02, 17.72s/it]

 29%|██▉       | 3107/10740 [15:35:55<38:52:40, 18.34s/it]

 29%|██▉       | 3108/10740 [15:36:12<38:06:56, 17.98s/it]

 29%|██▉       | 3109/10740 [15:36:32<39:16:14, 18.53s/it]

 29%|██▉       | 3110/10740 [15:36:44<35:24:22, 16.71s/it]

 29%|██▉       | 3111/10740 [15:37:04<37:19:34, 17.61s/it]

 29%|██▉       | 3112/10740 [15:37:22<37:30:37, 17.70s/it]

 29%|██▉       | 3113/10740 [15:37:42<39:04:50, 18.45s/it]
{'loss': 0.2118, 'learning_rate': 1.6664068096660867e-06, 'rewards/chosen': -2.746476888656616, 'rewards/rejected': -5.210907459259033, 'rewards/accuracies': 0.875, 'rewards/margins': 2.464430093765259, 'policy_logps/rejected': -417.7277526855469, 'policy_logps/chosen': -394.6614074707031, 'referece_logps/rejected': -365.61871337890625, 'referece_logps/chosen': -367.1966552734375, 'logits/rejected': 0.13093625009059906, 'logits/chosen': 0.2308972030878067, 'epoch': 1.74}

 29%|██▉       | 3114/10740 [15:37:56<35:56:40, 16.97s/it]


 29%|██▉       | 3116/10740 [15:38:30<35:42:17, 16.86s/it]
{'loss': 0.2639, 'learning_rate': 1.665731966161441e-06, 'rewards/chosen': -2.7344980239868164, 'rewards/rejected': -5.714033603668213, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9795353412628174, 'policy_logps/rejected': -304.10565185546875, 'policy_logps/chosen': -378.5489501953125, 'referece_logps/rejected': -246.9652862548828, 'referece_logps/chosen': -351.2039794921875, 'logits/rejected': -0.5491383075714111, 'logits/chosen': -0.6529810428619385, 'epoch': 1.74}


 29%|██▉       | 3118/10740 [15:39:07<38:11:40, 18.04s/it]

 29%|██▉       | 3119/10740 [15:39:29<40:23:20, 19.08s/it]

 29%|██▉       | 3120/10740 [15:39:47<39:53:42, 18.85s/it]
{'loss': 0.2758, 'learning_rate': 1.6648313272181376e-06, 'rewards/chosen': -3.619201183319092, 'rewards/rejected': -5.889363765716553, 'rewards/accuracies': 0.875, 'rewards/margins': 2.270162343978882, 'policy_logps/rejected': -330.53271484375, 'policy_logps/chosen': -394.6094970703125, 'referece_logps/rejected': -271.6390686035156, 'referece_logps/chosen': -358.41748046875, 'logits/rejected': -0.887030839920044, 'logits/chosen': -0.9633190631866455, 'epoch': 1.74}

 29%|██▉       | 3121/10740 [15:40:00<36:00:28, 17.01s/it]


 29%|██▉       | 3123/10740 [15:40:30<33:59:26, 16.06s/it]
{'loss': 0.2577, 'learning_rate': 1.664155213024587e-06, 'rewards/chosen': -2.1803512573242188, 'rewards/rejected': -7.407782077789307, 'rewards/accuracies': 1.0, 'rewards/margins': 5.227430820465088, 'policy_logps/rejected': -504.9228515625, 'policy_logps/chosen': -475.0936584472656, 'referece_logps/rejected': -430.84503173828125, 'referece_logps/chosen': -453.2901611328125, 'logits/rejected': -0.634459376335144, 'logits/chosen': -0.7037184238433838, 'epoch': 1.74}


 29%|██▉       | 3125/10740 [15:41:09<37:28:12, 17.71s/it]

 29%|██▉       | 3126/10740 [15:41:28<38:36:02, 18.25s/it]

 29%|██▉       | 3127/10740 [15:41:48<39:35:13, 18.72s/it]

 29%|██▉       | 3128/10740 [15:42:05<38:10:58, 18.06s/it]

 29%|██▉       | 3129/10740 [15:42:23<38:08:30, 18.04s/it]
{'loss': 0.3174, 'learning_rate': 1.6628013542147517e-06, 'rewards/chosen': -3.02335262298584, 'rewards/rejected': -4.09857702255249, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0752241611480713, 'policy_logps/rejected': -534.3612670898438, 'policy_logps/chosen': -462.49139404296875, 'referece_logps/rejected': -493.37548828125, 'referece_logps/chosen': -432.2578430175781, 'logits/rejected': -0.2747914493083954, 'logits/chosen': -0.33695873618125916, 'epoch': 1.75}


 29%|██▉       | 3131/10740 [15:43:01<39:25:01, 18.65s/it]

 29%|██▉       | 3132/10740 [15:43:19<39:14:00, 18.56s/it]

 29%|██▉       | 3133/10740 [15:43:41<41:24:15, 19.59s/it]

 29%|██▉       | 3134/10740 [15:43:59<40:19:18, 19.08s/it]
{'loss': 0.1889, 'learning_rate': 1.6616714805582519e-06, 'rewards/chosen': -2.575584888458252, 'rewards/rejected': -5.725607395172119, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1500227451324463, 'policy_logps/rejected': -350.36474609375, 'policy_logps/chosen': -337.4012145996094, 'referece_logps/rejected': -293.1086730957031, 'referece_logps/chosen': -311.64532470703125, 'logits/rejected': -0.714956521987915, 'logits/chosen': -0.634594202041626, 'epoch': 1.75}


 29%|██▉       | 3136/10740 [15:44:40<41:57:06, 19.86s/it]
{'loss': 0.2837, 'learning_rate': 1.6612191096872362e-06, 'rewards/chosen': -4.356791973114014, 'rewards/rejected': -7.194788455963135, 'rewards/accuracies': 0.875, 'rewards/margins': 2.837996006011963, 'policy_logps/rejected': -617.6148071289062, 'policy_logps/chosen': -621.3796997070312, 'referece_logps/rejected': -545.6669311523438, 'referece_logps/chosen': -577.811767578125, 'logits/rejected': 0.3207298815250397, 'logits/chosen': 0.5117183923721313, 'epoch': 1.75}


 29%|██▉       | 3138/10740 [15:45:17<40:19:27, 19.10s/it]

 29%|██▉       | 3139/10740 [15:45:39<41:40:31, 19.74s/it]

 29%|██▉       | 3140/10740 [15:45:59<42:12:45, 20.00s/it]

 29%|██▉       | 3141/10740 [15:46:14<39:09:40, 18.55s/it]
{'loss': 0.2545, 'learning_rate': 1.6600871304287044e-06, 'rewards/chosen': -2.378809928894043, 'rewards/rejected': -4.045792579650879, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6669824123382568, 'policy_logps/rejected': -332.0206298828125, 'policy_logps/chosen': -345.8157958984375, 'referece_logps/rejected': -291.5627136230469, 'referece_logps/chosen': -322.0277099609375, 'logits/rejected': -0.6217453479766846, 'logits/chosen': -0.5181663632392883, 'epoch': 1.75}


 29%|██▉       | 3143/10740 [15:46:45<34:42:43, 16.45s/it]

 29%|██▉       | 3144/10740 [15:47:01<34:17:58, 16.26s/it]

 29%|██▉       | 3145/10740 [15:47:13<31:33:50, 14.96s/it]

 29%|██▉       | 3146/10740 [15:47:26<30:26:37, 14.43s/it]

 29%|██▉       | 3147/10740 [15:47:43<32:15:50, 15.30s/it]
{'loss': 0.2281, 'learning_rate': 1.6587267743396414e-06, 'rewards/chosen': -3.1503524780273438, 'rewards/rejected': -5.758343696594238, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6079907417297363, 'policy_logps/rejected': -346.1133728027344, 'policy_logps/chosen': -408.09869384765625, 'referece_logps/rejected': -288.52996826171875, 'referece_logps/chosen': -376.5951843261719, 'logits/rejected': 0.19352313876152039, 'logits/chosen': 0.21117350459098816, 'epoch': 1.76}

 29%|██▉       | 3148/10740 [15:48:00<33:26:58, 15.86s/it]


 29%|██▉       | 3150/10740 [15:48:32<33:46:06, 16.02s/it]

 29%|██▉       | 3151/10740 [15:48:51<35:46:47, 16.97s/it]
{'loss': 0.3438, 'learning_rate': 1.6578186717490305e-06, 'rewards/chosen': -1.3522906303405762, 'rewards/rejected': -5.357388973236084, 'rewards/accuracies': 1.0, 'rewards/margins': 4.005098819732666, 'policy_logps/rejected': -254.11270141601562, 'policy_logps/chosen': -250.3238067626953, 'referece_logps/rejected': -200.53878784179688, 'referece_logps/chosen': -236.8009033203125, 'logits/rejected': -0.344946026802063, 'logits/chosen': -0.23963522911071777, 'epoch': 1.76}


 29%|██▉       | 3153/10740 [15:49:34<40:51:17, 19.39s/it]
[2024-04-02 11:02:51,986] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▉       | 3154/10740 [15:49:55<41:53:31, 19.88s/it]

 29%|██▉       | 3155/10740 [15:50:15<42:05:03, 19.97s/it]

 29%|██▉       | 3156/10740 [15:50:35<42:05:37, 19.98s/it]
{'loss': 0.2395, 'learning_rate': 1.6566821974859344e-06, 'rewards/chosen': -2.570943593978882, 'rewards/rejected': -7.901018142700195, 'rewards/accuracies': 0.875, 'rewards/margins': 5.330075263977051, 'policy_logps/rejected': -511.28619384765625, 'policy_logps/chosen': -436.27496337890625, 'referece_logps/rejected': -432.2759704589844, 'referece_logps/chosen': -410.5655517578125, 'logits/rejected': 0.08994694799184799, 'logits/chosen': 0.11569944769144058, 'epoch': 1.76}


 29%|██▉       | 3158/10740 [15:51:12<39:48:35, 18.90s/it]
{'loss': 0.2749, 'learning_rate': 1.6562271895479524e-06, 'rewards/chosen': -4.461637020111084, 'rewards/rejected': -5.878350257873535, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4167134761810303, 'policy_logps/rejected': -422.70135498046875, 'policy_logps/chosen': -432.1577453613281, 'referece_logps/rejected': -363.9178771972656, 'referece_logps/chosen': -387.5413513183594, 'logits/rejected': -0.16320905089378357, 'logits/chosen': -0.13663172721862793, 'epoch': 1.76}


 29%|██▉       | 3160/10740 [15:51:54<42:04:08, 19.98s/it]

 29%|██▉       | 3161/10740 [15:52:15<43:04:33, 20.46s/it]

 29%|██▉       | 3162/10740 [15:52:37<43:57:10, 20.88s/it]

 29%|██▉       | 3163/10740 [15:52:51<39:47:45, 18.91s/it]

 29%|██▉       | 3164/10740 [15:53:12<40:34:50, 19.28s/it]
[2024-04-02 11:06:29,781] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▉       | 3165/10740 [15:53:31<40:54:42, 19.44s/it]

 29%|██▉       | 3166/10740 [15:53:51<40:58:52, 19.48s/it]

 29%|██▉       | 3167/10740 [15:54:07<38:51:24, 18.47s/it]
{'loss': 0.3589, 'learning_rate': 1.6541767017881932e-06, 'rewards/chosen': -3.020878791809082, 'rewards/rejected': -4.292219638824463, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2713403701782227, 'policy_logps/rejected': -447.8349304199219, 'policy_logps/chosen': -407.37335205078125, 'referece_logps/rejected': -404.91278076171875, 'referece_logps/chosen': -377.1645812988281, 'logits/rejected': 0.5069133639335632, 'logits/chosen': 0.48926496505737305, 'epoch': 1.77}


 30%|██▉       | 3169/10740 [15:54:42<38:18:39, 18.22s/it]

 30%|██▉       | 3170/10740 [15:55:03<39:59:20, 19.02s/it]

 30%|██▉       | 3171/10740 [15:55:22<39:37:43, 18.85s/it]

 30%|██▉       | 3172/10740 [15:55:41<40:08:22, 19.09s/it]

 30%|██▉       | 3173/10740 [15:55:58<38:33:01, 18.34s/it]
{'loss': 0.2933, 'learning_rate': 1.6528070315621405e-06, 'rewards/chosen': -3.1451592445373535, 'rewards/rejected': -4.333983421325684, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1888236999511719, 'policy_logps/rejected': -228.1343231201172, 'policy_logps/chosen': -198.75518798828125, 'referece_logps/rejected': -184.79452514648438, 'referece_logps/chosen': -167.30360412597656, 'logits/rejected': -0.5127944946289062, 'logits/chosen': -0.5554509162902832, 'epoch': 1.77}


 30%|██▉       | 3175/10740 [15:56:33<38:39:15, 18.39s/it]
{'loss': 0.2599, 'learning_rate': 1.6523499996033768e-06, 'rewards/chosen': -3.7982187271118164, 'rewards/rejected': -5.061716556549072, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2634978294372559, 'policy_logps/rejected': -334.5149230957031, 'policy_logps/chosen': -369.4558410644531, 'referece_logps/rejected': -283.8977355957031, 'referece_logps/chosen': -331.4736328125, 'logits/rejected': 0.3112996816635132, 'logits/chosen': 0.2055997997522354, 'epoch': 1.77}


 30%|██▉       | 3177/10740 [15:57:10<38:03:40, 18.12s/it]

 30%|██▉       | 3178/10740 [15:57:30<39:09:27, 18.64s/it]
{'loss': 0.1386, 'learning_rate': 1.651664006720983e-06, 'rewards/chosen': -3.369269609451294, 'rewards/rejected': -6.859577178955078, 'rewards/accuracies': 1.0, 'rewards/margins': 3.490307569503784, 'policy_logps/rejected': -347.3381042480469, 'policy_logps/chosen': -336.9354248046875, 'referece_logps/rejected': -278.7423400878906, 'referece_logps/chosen': -303.24267578125, 'logits/rejected': -0.23248985409736633, 'logits/chosen': -0.1986255943775177, 'epoch': 1.78}


 30%|██▉       | 3180/10740 [15:58:14<42:43:43, 20.35s/it]
[2024-04-02 11:11:32,297] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|██▉       | 3181/10740 [15:58:32<40:54:47, 19.49s/it]
[2024-04-02 11:11:49,771] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|██▉       | 3182/10740 [15:58:46<37:27:34, 17.84s/it]

 30%|██▉       | 3183/10740 [15:59:06<39:18:17, 18.72s/it]
{'loss': 0.3401, 'learning_rate': 1.6505195001191065e-06, 'rewards/chosen': -3.017388105392456, 'rewards/rejected': -5.140457630157471, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1230695247650146, 'policy_logps/rejected': -514.1978759765625, 'policy_logps/chosen': -501.439208984375, 'referece_logps/rejected': -462.7933349609375, 'referece_logps/chosen': -471.26531982421875, 'logits/rejected': -0.6405501365661621, 'logits/chosen': -0.40325430035591125, 'epoch': 1.78}

 30%|██▉       | 3184/10740 [15:59:25<39:15:50, 18.71s/it]


 30%|██▉       | 3186/10740 [15:59:56<35:39:22, 16.99s/it]

 30%|██▉       | 3187/10740 [16:00:16<37:25:18, 17.84s/it]

 30%|██▉       | 3188/10740 [16:00:35<38:09:25, 18.19s/it]

 30%|██▉       | 3189/10740 [16:00:52<37:37:30, 17.94s/it]

 30%|██▉       | 3190/10740 [16:01:15<40:30:45, 19.32s/it]

 30%|██▉       | 3191/10740 [16:01:32<39:25:18, 18.80s/it]

 30%|██▉       | 3192/10740 [16:01:52<39:55:06, 19.04s/it]

 30%|██▉       | 3193/10740 [16:02:12<40:53:39, 19.51s/it]

 30%|██▉       | 3194/10740 [16:02:34<42:01:31, 20.05s/it]
[2024-04-02 11:15:51,883] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|██▉       | 3195/10740 [16:02:52<40:38:31, 19.39s/it]
{'loss': 0.2646, 'learning_rate': 1.6477666542706205e-06, 'rewards/chosen': -3.486740827560425, 'rewards/rejected': -6.416902542114258, 'rewards/accuracies': 1.0, 'rewards/margins': 2.930161714553833, 'policy_logps/rejected': -314.1473083496094, 'policy_logps/chosen': -384.102783203125, 'referece_logps/rejected': -249.97830200195312, 'referece_logps/chosen': -349.2353820800781, 'logits/rejected': -0.49751996994018555, 'logits/chosen': -0.45550674200057983, 'epoch': 1.78}


 30%|██▉       | 3197/10740 [16:03:31<40:51:09, 19.50s/it]

 30%|██▉       | 3198/10740 [16:03:52<42:00:16, 20.05s/it]
{'loss': 0.3031, 'learning_rate': 1.647077115789885e-06, 'rewards/chosen': -2.1811728477478027, 'rewards/rejected': -4.739939212799072, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5587663650512695, 'policy_logps/rejected': -329.1799011230469, 'policy_logps/chosen': -354.75830078125, 'referece_logps/rejected': -281.780517578125, 'referece_logps/chosen': -332.9466247558594, 'logits/rejected': 0.05837080255150795, 'logits/chosen': -0.0011234497651457787, 'epoch': 1.79}


 30%|██▉       | 3200/10740 [16:04:19<34:30:38, 16.48s/it]
{'loss': 0.2648, 'learning_rate': 1.6466171291683e-06, 'rewards/chosen': -2.503443956375122, 'rewards/rejected': -5.188399791717529, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6849563121795654, 'policy_logps/rejected': -515.8594360351562, 'policy_logps/chosen': -332.9832763671875, 'referece_logps/rejected': -463.9754638671875, 'referece_logps/chosen': -307.9488525390625, 'logits/rejected': 0.1655941605567932, 'logits/chosen': 0.27583402395248413, 'epoch': 1.79}
[2024-04-02 11:17:57,704] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|██▉       | 3202/10740 [16:05:00<38:39:04, 18.46s/it]
[2024-04-02 11:18:17,839] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2411, 'learning_rate': 1.6461569073010657e-06, 'rewards/chosen': -3.013787031173706, 'rewards/rejected': -5.472439289093018, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4586520195007324, 'policy_logps/rejected': -278.7614440917969, 'policy_logps/chosen': -280.71795654296875, 'referece_logps/rejected': -224.03707885742188, 'referece_logps/chosen': -250.58009338378906, 'logits/rejected': 0.44505274295806885, 'logits/chosen': 0.6242618560791016, 'epoch': 1.79}

 30%|██▉       | 3203/10740 [16:05:19<39:19:10, 18.78s/it]


 30%|██▉       | 3205/10740 [16:06:03<42:39:09, 20.38s/it]

 30%|██▉       | 3206/10740 [16:06:23<42:08:58, 20.14s/it]
{'loss': 0.3541, 'learning_rate': 1.6452357584994672e-06, 'rewards/chosen': -2.7364280223846436, 'rewards/rejected': -4.259289741516113, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5228620767593384, 'policy_logps/rejected': -256.7105712890625, 'policy_logps/chosen': -238.7008819580078, 'referece_logps/rejected': -214.11770629882812, 'referece_logps/chosen': -211.33660888671875, 'logits/rejected': -0.8351621627807617, 'logits/chosen': -0.9362423419952393, 'epoch': 1.79}

 30%|██▉       | 3207/10740 [16:06:35<37:30:27, 17.92s/it]


 30%|██▉       | 3209/10740 [16:07:05<34:07:56, 16.32s/it]

 30%|██▉       | 3210/10740 [16:07:24<36:16:28, 17.34s/it]
{'loss': 0.1951, 'learning_rate': 1.6443136707255817e-06, 'rewards/chosen': -4.574016571044922, 'rewards/rejected': -6.794087886810303, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2200710773468018, 'policy_logps/rejected': -454.56744384765625, 'policy_logps/chosen': -499.2061462402344, 'referece_logps/rejected': -386.6265869140625, 'referece_logps/chosen': -453.4659729003906, 'logits/rejected': 0.019384443759918213, 'logits/chosen': 0.17934928834438324, 'epoch': 1.79}

 30%|██▉       | 3211/10740 [16:07:41<36:06:48, 17.27s/it]


 30%|██▉       | 3213/10740 [16:08:22<39:33:15, 18.92s/it]
[2024-04-02 11:21:40,529] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.335, 'learning_rate': 1.6436214895017638e-06, 'rewards/chosen': -2.9282448291778564, 'rewards/rejected': -5.209855556488037, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2816107273101807, 'policy_logps/rejected': -298.1033630371094, 'policy_logps/chosen': -552.9622802734375, 'referece_logps/rejected': -246.00482177734375, 'referece_logps/chosen': -523.6798706054688, 'logits/rejected': -0.03885791450738907, 'logits/chosen': -0.007900424301624298, 'epoch': 1.79}

 30%|██▉       | 3214/10740 [16:08:42<40:05:29, 19.18s/it]


 30%|██▉       | 3216/10740 [16:09:05<31:38:57, 15.14s/it]
{'loss': 0.2574, 'learning_rate': 1.6429287814274021e-06, 'rewards/chosen': -2.5314908027648926, 'rewards/rejected': -5.896755218505859, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3652641773223877, 'policy_logps/rejected': -219.0955047607422, 'policy_logps/chosen': -711.05615234375, 'referece_logps/rejected': -160.12794494628906, 'referece_logps/chosen': -685.7412719726562, 'logits/rejected': -0.09282978624105453, 'logits/chosen': -0.06646217405796051, 'epoch': 1.8}


 30%|██▉       | 3218/10740 [16:09:37<33:20:51, 15.96s/it]
{'loss': 0.1481, 'learning_rate': 1.6424666836297454e-06, 'rewards/chosen': -2.54642653465271, 'rewards/rejected': -5.558767795562744, 'rewards/accuracies': 1.0, 'rewards/margins': 3.012341260910034, 'policy_logps/rejected': -444.89923095703125, 'policy_logps/chosen': -580.0101318359375, 'referece_logps/rejected': -389.31158447265625, 'referece_logps/chosen': -554.5458374023438, 'logits/rejected': 0.014202460646629333, 'logits/chosen': 0.2335301786661148, 'epoch': 1.8}


 30%|██▉       | 3220/10740 [16:10:14<36:24:58, 17.43s/it]

 30%|██▉       | 3221/10740 [16:10:31<36:01:59, 17.25s/it]

 30%|███       | 3222/10740 [16:10:49<36:13:39, 17.35s/it]

 30%|███       | 3223/10740 [16:11:11<39:10:22, 18.76s/it]
{'loss': 0.2688, 'learning_rate': 1.6413104169099806e-06, 'rewards/chosen': -2.808868646621704, 'rewards/rejected': -4.9238152503967285, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1149468421936035, 'policy_logps/rejected': -365.34564208984375, 'policy_logps/chosen': -326.865966796875, 'referece_logps/rejected': -316.10748291015625, 'referece_logps/chosen': -298.77728271484375, 'logits/rejected': -1.0025382041931152, 'logits/chosen': -0.9309961795806885, 'epoch': 1.8}


 30%|███       | 3225/10740 [16:11:41<34:30:29, 16.53s/it]

 30%|███       | 3226/10740 [16:11:59<35:34:42, 17.05s/it]

 30%|███       | 3227/10740 [16:12:15<34:46:41, 16.66s/it]

 30%|███       | 3228/10740 [16:12:31<34:41:06, 16.62s/it]
{'loss': 0.3672, 'learning_rate': 1.6401526919716018e-06, 'rewards/chosen': -2.952543258666992, 'rewards/rejected': -5.5414533615112305, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5889103412628174, 'policy_logps/rejected': -338.84979248046875, 'policy_logps/chosen': -327.79522705078125, 'referece_logps/rejected': -283.43524169921875, 'referece_logps/chosen': -298.2698059082031, 'logits/rejected': 0.014635991305112839, 'logits/chosen': 0.046229615807533264, 'epoch': 1.8}

 30%|███       | 3229/10740 [16:12:44<32:28:47, 15.57s/it]

 30%|███       | 3230/10740 [16:13:04<35:03:02, 16.80s/it]

 30%|███       | 3231/10740 [16:13:22<35:55:15, 17.22s/it]


 30%|███       | 3233/10740 [16:13:59<37:22:00, 17.92s/it]
{'loss': 0.2655, 'learning_rate': 1.638993511447056e-06, 'rewards/chosen': -2.8023722171783447, 'rewards/rejected': -4.4357991218566895, 'rewards/accuracies': 0.625, 'rewards/margins': 1.633426547050476, 'policy_logps/rejected': -276.10687255859375, 'policy_logps/chosen': -416.23150634765625, 'referece_logps/rejected': -231.74888610839844, 'referece_logps/chosen': -388.207763671875, 'logits/rejected': -0.7124449014663696, 'logits/chosen': -0.9395346641540527, 'epoch': 1.81}

 30%|███       | 3234/10740 [16:14:15<35:46:18, 17.16s/it]

 30%|███       | 3235/10740 [16:14:34<37:26:49, 17.96s/it]


 30%|███       | 3237/10740 [16:15:15<40:08:03, 19.26s/it]
[2024-04-02 11:28:33,401] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3238/10740 [16:15:35<40:39:19, 19.51s/it]
[2024-04-02 11:28:53,500] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3239/10740 [16:15:55<40:51:49, 19.61s/it]

 30%|███       | 3240/10740 [16:16:14<40:07:30, 19.26s/it]
{'loss': 0.1304, 'learning_rate': 1.6373682183470433e-06, 'rewards/chosen': -2.6886396408081055, 'rewards/rejected': -5.191959857940674, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5033204555511475, 'policy_logps/rejected': -393.7022399902344, 'policy_logps/chosen': -352.2251281738281, 'referece_logps/rejected': -341.7826843261719, 'referece_logps/chosen': -325.3387145996094, 'logits/rejected': 0.2891170084476471, 'logits/chosen': 0.31644123792648315, 'epoch': 1.81}

 30%|███       | 3241/10740 [16:16:28<37:00:37, 17.77s/it]


 30%|███       | 3243/10740 [16:17:03<36:24:39, 17.48s/it]

 30%|███       | 3244/10740 [16:17:15<32:55:47, 15.81s/it]

 30%|███       | 3245/10740 [16:17:35<35:34:50, 17.09s/it]
{'loss': 0.2236, 'learning_rate': 1.6362055551758765e-06, 'rewards/chosen': -2.7398407459259033, 'rewards/rejected': -5.2223944664001465, 'rewards/accuracies': 0.75, 'rewards/margins': 2.482553482055664, 'policy_logps/rejected': -318.1432189941406, 'policy_logps/chosen': -323.34515380859375, 'referece_logps/rejected': -265.9192199707031, 'referece_logps/chosen': -295.9467468261719, 'logits/rejected': -0.011521995067596436, 'logits/chosen': -0.059824422001838684, 'epoch': 1.81}

 30%|███       | 3246/10740 [16:17:57<38:16:12, 18.38s/it]


 30%|███       | 3248/10740 [16:18:33<38:49:57, 18.66s/it]
{'loss': 0.2269, 'learning_rate': 1.6355072627305049e-06, 'rewards/chosen': -2.102358341217041, 'rewards/rejected': -6.750491142272949, 'rewards/accuracies': 1.0, 'rewards/margins': 4.64813232421875, 'policy_logps/rejected': -451.5652160644531, 'policy_logps/chosen': -553.029052734375, 'referece_logps/rejected': -384.0603332519531, 'referece_logps/chosen': -532.0054931640625, 'logits/rejected': 0.46072590351104736, 'logits/chosen': 0.5018216371536255, 'epoch': 1.81}

 30%|███       | 3249/10740 [16:18:51<38:18:39, 18.41s/it]
[2024-04-02 11:32:30,648] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3250/10740 [16:19:12<40:12:08, 19.32s/it]


 30%|███       | 3252/10740 [16:19:47<37:35:35, 18.07s/it]
{'loss': 0.3552, 'learning_rate': 1.6345753970223646e-06, 'rewards/chosen': -1.6817235946655273, 'rewards/rejected': -4.828290939331055, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1465671062469482, 'policy_logps/rejected': -303.73876953125, 'policy_logps/chosen': -280.9216613769531, 'referece_logps/rejected': -255.455810546875, 'referece_logps/chosen': -264.10443115234375, 'logits/rejected': -0.1498865783214569, 'logits/chosen': -0.19388152658939362, 'epoch': 1.82}

 30%|███       | 3253/10740 [16:20:04<37:05:27, 17.83s/it]
[2024-04-02 11:33:43,237] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 3255/10740 [16:20:40<36:16:20, 17.45s/it]
[2024-04-02 11:33:57,889] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3256/10740 [16:20:56<35:21:23, 17.01s/it]

 30%|███       | 3257/10740 [16:21:15<36:57:19, 17.78s/it]
{'loss': 0.2222, 'learning_rate': 1.633409266432138e-06, 'rewards/chosen': -2.688037395477295, 'rewards/rejected': -4.159900665283203, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4718631505966187, 'policy_logps/rejected': -234.0656280517578, 'policy_logps/chosen': -363.8908386230469, 'referece_logps/rejected': -192.46661376953125, 'referece_logps/chosen': -337.0104675292969, 'logits/rejected': -0.8649621605873108, 'logits/chosen': -0.918171763420105, 'epoch': 1.82}

 30%|███       | 3258/10740 [16:21:33<36:53:19, 17.75s/it]


 30%|███       | 3260/10740 [16:22:13<39:45:34, 19.14s/it]
{'loss': 0.2926, 'learning_rate': 1.6327088965867776e-06, 'rewards/chosen': -2.013881206512451, 'rewards/rejected': -3.311777114868164, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2978960275650024, 'policy_logps/rejected': -242.78372192382812, 'policy_logps/chosen': -360.1309814453125, 'referece_logps/rejected': -209.6659698486328, 'referece_logps/chosen': -339.9922180175781, 'logits/rejected': 0.07897184789180756, 'logits/chosen': -0.046363502740859985, 'epoch': 1.82}


 30%|███       | 3262/10740 [16:22:48<37:37:39, 18.11s/it]

 30%|███       | 3263/10740 [16:23:05<37:20:56, 17.98s/it]
{'loss': 0.2406, 'learning_rate': 1.6320080088236163e-06, 'rewards/chosen': -2.890772819519043, 'rewards/rejected': -6.477809906005859, 'rewards/accuracies': 0.75, 'rewards/margins': 3.5870370864868164, 'policy_logps/rejected': -367.48187255859375, 'policy_logps/chosen': -480.7397155761719, 'referece_logps/rejected': -302.7037658691406, 'referece_logps/chosen': -451.83203125, 'logits/rejected': -0.22769726812839508, 'logits/chosen': -0.45143020153045654, 'epoch': 1.82}

 30%|███       | 3264/10740 [16:23:23<37:07:12, 17.87s/it]


 30%|███       | 3266/10740 [16:24:00<37:47:59, 18.21s/it]

 30%|███       | 3267/10740 [16:24:12<33:40:43, 16.22s/it]
{'loss': 0.2996, 'learning_rate': 1.6310726871478433e-06, 'rewards/chosen': -2.9235780239105225, 'rewards/rejected': -5.263643741607666, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3400654792785645, 'policy_logps/rejected': -507.6828308105469, 'policy_logps/chosen': -458.3519287109375, 'referece_logps/rejected': -455.04638671875, 'referece_logps/chosen': -429.11614990234375, 'logits/rejected': -1.13374924659729, 'logits/chosen': -0.8709812164306641, 'epoch': 1.83}
[2024-04-02 11:37:50,636] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 3269/10740 [16:24:44<32:52:29, 15.84s/it]
[2024-04-02 11:38:02,306] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 3270/10740 [16:25:00<33:04:38, 15.94s/it]

 30%|███       | 3271/10740 [16:25:12<30:26:33, 14.67s/it]
{'loss': 0.2773, 'learning_rate': 1.6301364471104379e-06, 'rewards/chosen': -3.143998861312866, 'rewards/rejected': -5.581347465515137, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4373488426208496, 'policy_logps/rejected': -314.281982421875, 'policy_logps/chosen': -374.6271667480469, 'referece_logps/rejected': -258.468505859375, 'referece_logps/chosen': -343.1871643066406, 'logits/rejected': -1.7602609395980835, 'logits/chosen': -1.772856593132019, 'epoch': 1.83}

 30%|███       | 3272/10740 [16:25:31<33:09:36, 15.99s/it]


 30%|███       | 3274/10740 [16:26:08<35:34:20, 17.15s/it]

 30%|███       | 3275/10740 [16:26:30<38:36:56, 18.62s/it]

 31%|███       | 3276/10740 [16:26:48<38:04:19, 18.36s/it]
{'loss': 0.1733, 'learning_rate': 1.6289648576933747e-06, 'rewards/chosen': -3.1791152954101562, 'rewards/rejected': -7.552509307861328, 'rewards/accuracies': 1.0, 'rewards/margins': 4.373393535614014, 'policy_logps/rejected': -612.1827392578125, 'policy_logps/chosen': -383.8437194824219, 'referece_logps/rejected': -536.6576538085938, 'referece_logps/chosen': -352.05255126953125, 'logits/rejected': 0.11933892220258713, 'logits/chosen': 0.18303363025188446, 'epoch': 1.83}


 31%|███       | 3278/10740 [16:27:22<35:58:54, 17.36s/it]

 31%|███       | 3279/10740 [16:27:38<35:10:23, 16.97s/it]
{'loss': 0.2478, 'learning_rate': 1.6282612174018769e-06, 'rewards/chosen': -3.3051178455352783, 'rewards/rejected': -5.826123237609863, 'rewards/accuracies': 0.75, 'rewards/margins': 2.521005630493164, 'policy_logps/rejected': -423.9248046875, 'policy_logps/chosen': -427.4583435058594, 'referece_logps/rejected': -365.6636047363281, 'referece_logps/chosen': -394.40716552734375, 'logits/rejected': 0.2599874436855316, 'logits/chosen': 0.3968757688999176, 'epoch': 1.83}

 31%|███       | 3280/10740 [16:27:59<37:52:01, 18.27s/it]
[2024-04-02 11:41:37,126] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 31%|███       | 3282/10740 [16:28:36<37:37:27, 18.16s/it]

 31%|███       | 3283/10740 [16:28:54<37:50:12, 18.27s/it]

 31%|███       | 3284/10740 [16:29:12<37:24:52, 18.07s/it]
{'loss': 0.3005, 'learning_rate': 1.6270873410294384e-06, 'rewards/chosen': -3.1211392879486084, 'rewards/rejected': -5.48942232131958, 'rewards/accuracies': 0.625, 'rewards/margins': 2.3682823181152344, 'policy_logps/rejected': -416.9328308105469, 'policy_logps/chosen': -565.6605224609375, 'referece_logps/rejected': -362.03857421875, 'referece_logps/chosen': -534.4490966796875, 'logits/rejected': 0.04541139304637909, 'logits/chosen': -0.03638733923435211, 'epoch': 1.83}

 31%|███       | 3285/10740 [16:29:29<36:59:52, 17.87s/it]

 31%|███       | 3286/10740 [16:29:50<38:27:14, 18.57s/it]

 31%|███       | 3287/10740 [16:30:07<37:35:47, 18.16s/it]


 31%|███       | 3289/10740 [16:30:44<38:35:27, 18.65s/it]
{'loss': 0.3755, 'learning_rate': 1.6259120387789705e-06, 'rewards/chosen': -3.0925493240356445, 'rewards/rejected': -5.871101379394531, 'rewards/accuracies': 0.875, 'rewards/margins': 2.778552293777466, 'policy_logps/rejected': -318.3668212890625, 'policy_logps/chosen': -231.38111877441406, 'referece_logps/rejected': -259.6557922363281, 'referece_logps/chosen': -200.4556121826172, 'logits/rejected': -0.6999511122703552, 'logits/chosen': -0.7350078821182251, 'epoch': 1.84}


 31%|███       | 3291/10740 [16:31:20<38:27:17, 18.58s/it]
[2024-04-02 11:44:38,505] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3292/10740 [16:31:35<35:46:37, 17.29s/it]

 31%|███       | 3293/10740 [16:31:47<32:30:21, 15.71s/it]
{'loss': 0.3349, 'learning_rate': 1.624970772142169e-06, 'rewards/chosen': -3.154313325881958, 'rewards/rejected': -5.919292449951172, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7649788856506348, 'policy_logps/rejected': -363.7873840332031, 'policy_logps/chosen': -300.5364990234375, 'referece_logps/rejected': -304.5944519042969, 'referece_logps/chosen': -268.99334716796875, 'logits/rejected': 0.20597751438617706, 'logits/chosen': 0.19493481516838074, 'epoch': 1.84}

 31%|███       | 3294/10740 [16:32:05<34:08:48, 16.51s/it]

 31%|███       | 3295/10740 [16:32:22<34:15:23, 16.56s/it]

 31%|███       | 3296/10740 [16:32:42<36:26:34, 17.62s/it]
[2024-04-02 11:46:21,589] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3297/10740 [16:33:03<38:54:59, 18.82s/it]
[2024-04-02 11:46:37,420] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3298/10740 [16:33:19<37:03:21, 17.93s/it]

 31%|███       | 3299/10740 [16:33:39<38:31:14, 18.64s/it]

 31%|███       | 3300/10740 [16:34:02<40:50:13, 19.76s/it]

 31%|███       | 3301/10740 [16:34:23<41:52:33, 20.27s/it]

 31%|███       | 3302/10740 [16:34:44<41:51:48, 20.26s/it]


 31%|███       | 3304/10740 [16:35:14<37:15:21, 18.04s/it]

 31%|███       | 3305/10740 [16:35:36<39:40:43, 19.21s/it]
{'loss': 0.2307, 'learning_rate': 1.6221415208261292e-06, 'rewards/chosen': -2.6876237392425537, 'rewards/rejected': -5.080414295196533, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3927903175354004, 'policy_logps/rejected': -285.700439453125, 'policy_logps/chosen': -457.83453369140625, 'referece_logps/rejected': -234.89633178710938, 'referece_logps/chosen': -430.9582824707031, 'logits/rejected': 0.3764960765838623, 'logits/chosen': 0.35320669412612915, 'epoch': 1.85}

 31%|███       | 3306/10740 [16:35:55<39:36:44, 19.18s/it]

 31%|███       | 3307/10740 [16:36:17<41:12:57, 19.96s/it]

 31%|███       | 3308/10740 [16:36:35<40:07:44, 19.44s/it]

 31%|███       | 3309/10740 [16:36:55<40:20:30, 19.54s/it]


 31%|███       | 3311/10740 [16:37:32<40:01:05, 19.39s/it]

 31%|███       | 3312/10740 [16:37:52<40:27:26, 19.61s/it]
[2024-04-02 11:51:10,602] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3313/10740 [16:38:07<37:19:01, 18.09s/it]
[2024-04-02 11:51:25,145] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2105, 'learning_rate': 1.620250824171494e-06, 'rewards/chosen': -2.903785228729248, 'rewards/rejected': -6.11370325088501, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2099180221557617, 'policy_logps/rejected': -376.5127868652344, 'policy_logps/chosen': -354.9533386230469, 'referece_logps/rejected': -315.37579345703125, 'referece_logps/chosen': -325.91546630859375, 'logits/rejected': 0.19397395849227905, 'logits/chosen': 0.18248304724693298, 'epoch': 1.85}

 31%|███       | 3314/10740 [16:38:22<35:34:33, 17.25s/it]

 31%|███       | 3315/10740 [16:38:42<36:53:34, 17.89s/it]

 31%|███       | 3316/10740 [16:39:01<38:05:33, 18.47s/it]

 31%|███       | 3317/10740 [16:39:22<39:06:40, 18.97s/it]

 31%|███       | 3318/10740 [16:39:37<36:47:38, 17.85s/it]

 31%|███       | 3319/10740 [16:39:55<37:12:00, 18.05s/it]

 31%|███       | 3320/10740 [16:40:12<36:23:54, 17.66s/it]


 31%|███       | 3322/10740 [16:40:43<34:17:08, 16.64s/it]
{'loss': 0.1768, 'learning_rate': 1.618119475366024e-06, 'rewards/chosen': -2.4139418601989746, 'rewards/rejected': -7.702118396759033, 'rewards/accuracies': 0.875, 'rewards/margins': 5.288176536560059, 'policy_logps/rejected': -367.0513000488281, 'policy_logps/chosen': -528.5186767578125, 'referece_logps/rejected': -290.03009033203125, 'referece_logps/chosen': -504.3792724609375, 'logits/rejected': 0.19772931933403015, 'logits/chosen': 0.029423296451568604, 'epoch': 1.86}

 31%|███       | 3323/10740 [16:41:02<35:46:26, 17.36s/it]


 31%|███       | 3325/10740 [16:41:35<34:52:36, 16.93s/it]
{'loss': 0.3278, 'learning_rate': 1.6174080130377621e-06, 'rewards/chosen': -3.164508104324341, 'rewards/rejected': -5.943538665771484, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7790305614471436, 'policy_logps/rejected': -530.2315063476562, 'policy_logps/chosen': -463.0115966796875, 'referece_logps/rejected': -470.796142578125, 'referece_logps/chosen': -431.36651611328125, 'logits/rejected': -0.017094120383262634, 'logits/chosen': -0.08111155778169632, 'epoch': 1.86}

 31%|███       | 3326/10740 [16:41:58<38:10:33, 18.54s/it]

 31%|███       | 3327/10740 [16:42:12<35:29:26, 17.24s/it]


 31%|███       | 3329/10740 [16:42:47<36:46:57, 17.87s/it]
{'loss': 0.2451, 'learning_rate': 1.6164586105340247e-06, 'rewards/chosen': -2.415130853652954, 'rewards/rejected': -7.2179155349731445, 'rewards/accuracies': 1.0, 'rewards/margins': 4.8027849197387695, 'policy_logps/rejected': -321.1477966308594, 'policy_logps/chosen': -338.31109619140625, 'referece_logps/rejected': -248.96865844726562, 'referece_logps/chosen': -314.1597595214844, 'logits/rejected': -0.5320672988891602, 'logits/chosen': -0.5122032165527344, 'epoch': 1.86}

 31%|███       | 3330/10740 [16:43:00<33:45:07, 16.40s/it]

 31%|███       | 3331/10740 [16:43:21<36:22:21, 17.67s/it]


 31%|███       | 3333/10740 [16:43:53<34:34:27, 16.80s/it]
{'loss': 0.3037, 'learning_rate': 1.6155083109356306e-06, 'rewards/chosen': -3.1073570251464844, 'rewards/rejected': -7.402908802032471, 'rewards/accuracies': 1.0, 'rewards/margins': 4.295551776885986, 'policy_logps/rejected': -497.67449951171875, 'policy_logps/chosen': -431.6252746582031, 'referece_logps/rejected': -423.6454162597656, 'referece_logps/chosen': -400.55169677734375, 'logits/rejected': -0.5189723968505859, 'logits/chosen': -0.5046296715736389, 'epoch': 1.86}

 31%|███       | 3334/10740 [16:44:07<32:58:45, 16.03s/it]


 31%|███       | 3336/10740 [16:44:47<36:40:53, 17.84s/it]
{'loss': 0.1909, 'learning_rate': 1.6147949983503257e-06, 'rewards/chosen': -2.736389398574829, 'rewards/rejected': -6.313431262969971, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5770418643951416, 'policy_logps/rejected': -375.814208984375, 'policy_logps/chosen': -292.10906982421875, 'referece_logps/rejected': -312.6799011230469, 'referece_logps/chosen': -264.7451477050781, 'logits/rejected': -0.10764148831367493, 'logits/chosen': 0.055726706981658936, 'epoch': 1.86}


 31%|███       | 3338/10740 [16:45:23<37:05:20, 18.04s/it]
{'loss': 0.389, 'learning_rate': 1.614319177005182e-06, 'rewards/chosen': -2.968766689300537, 'rewards/rejected': -4.853368759155273, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8846020698547363, 'policy_logps/rejected': -443.0465087890625, 'policy_logps/chosen': -606.5560913085938, 'referece_logps/rejected': -394.5128173828125, 'referece_logps/chosen': -576.868408203125, 'logits/rejected': -0.3284134864807129, 'logits/chosen': -0.4414501190185547, 'epoch': 1.86}

 31%|███       | 3339/10740 [16:45:42<37:28:22, 18.23s/it]

 31%|███       | 3340/10740 [16:46:01<37:51:19, 18.42s/it]

 31%|███       | 3341/10740 [16:46:15<35:12:22, 17.13s/it]

 31%|███       | 3342/10740 [16:46:36<37:37:44, 18.31s/it]

 31%|███       | 3343/10740 [16:46:55<38:07:05, 18.55s/it]

 31%|███       | 3344/10740 [16:47:13<37:50:09, 18.42s/it]

 31%|███       | 3345/10740 [16:47:33<38:51:49, 18.92s/it]


 31%|███       | 3347/10740 [16:48:12<38:56:57, 18.97s/it]

 31%|███       | 3348/10740 [16:48:26<35:57:31, 17.51s/it]
{'loss': 0.2358, 'learning_rate': 1.6119367213144105e-06, 'rewards/chosen': -2.867988109588623, 'rewards/rejected': -5.473615646362305, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6056275367736816, 'policy_logps/rejected': -365.826416015625, 'policy_logps/chosen': -359.38507080078125, 'referece_logps/rejected': -311.09027099609375, 'referece_logps/chosen': -330.7051696777344, 'logits/rejected': -0.6252216100692749, 'logits/chosen': -0.6302466988563538, 'epoch': 1.87}

 31%|███       | 3349/10740 [16:48:44<36:34:32, 17.82s/it]

 31%|███       | 3350/10740 [16:49:05<38:31:47, 18.77s/it]
[2024-04-02 12:02:42,307] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 3351/10740 [16:49:24<38:36:10, 18.81s/it]

 31%|███       | 3352/10740 [16:49:38<35:42:50, 17.40s/it]

 31%|███       | 3353/10740 [16:49:53<34:09:17, 16.65s/it]

 31%|███       | 3354/10740 [16:50:13<36:04:46, 17.59s/it]

 31%|███       | 3355/10740 [16:50:28<34:33:33, 16.85s/it]

 31%|███       | 3356/10740 [16:50:42<32:30:43, 15.85s/it]

 31%|███▏      | 3357/10740 [16:51:01<34:47:41, 16.97s/it]

 31%|███▏      | 3358/10740 [16:51:16<33:14:30, 16.21s/it]

 31%|███▏      | 3359/10740 [16:51:31<33:01:47, 16.11s/it]

 31%|███▏      | 3360/10740 [16:51:49<33:39:31, 16.42s/it]
[2024-04-02 12:05:27,011] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3361/10740 [16:52:09<36:00:14, 17.57s/it]

 31%|███▏      | 3362/10740 [16:52:27<36:33:52, 17.84s/it]

 31%|███▏      | 3363/10740 [16:52:43<35:00:40, 17.09s/it]

 31%|███▏      | 3364/10740 [16:52:55<32:00:04, 15.62s/it]

 31%|███▏      | 3365/10740 [16:53:15<34:54:38, 17.04s/it]

 31%|███▏      | 3366/10740 [16:53:32<35:00:00, 17.09s/it]


 31%|███▏      | 3368/10740 [16:54:10<36:58:58, 18.06s/it]

 31%|███▏      | 3369/10740 [16:54:26<35:35:28, 17.38s/it]

 31%|███▏      | 3370/10740 [16:54:42<34:48:49, 17.01s/it]
{'loss': 0.3639, 'learning_rate': 1.6066757580789684e-06, 'rewards/chosen': -3.0756020545959473, 'rewards/rejected': -5.293033599853516, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2174313068389893, 'policy_logps/rejected': -591.3560180664062, 'policy_logps/chosen': -344.575439453125, 'referece_logps/rejected': -538.4257202148438, 'referece_logps/chosen': -313.8193664550781, 'logits/rejected': 0.4622490406036377, 'logits/chosen': 0.672620952129364, 'epoch': 1.88}

 31%|███▏      | 3371/10740 [16:55:03<37:15:05, 18.20s/it]

 31%|███▏      | 3372/10740 [16:55:25<39:22:04, 19.24s/it]
[2024-04-02 12:09:05,782] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3373/10740 [16:55:48<41:38:24, 20.35s/it]
[2024-04-02 12:09:27,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3374/10740 [16:56:10<42:37:17, 20.83s/it]
[2024-04-02 12:09:44,715] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 3375/10740 [16:56:26<40:15:01, 19.67s/it]

 31%|███▏      | 3376/10740 [16:56:47<40:38:10, 19.87s/it]

 31%|███▏      | 3377/10740 [16:57:00<36:15:44, 17.73s/it]

 31%|███▏      | 3378/10740 [16:57:19<37:26:19, 18.31s/it]

 31%|███▏      | 3379/10740 [16:57:39<38:20:00, 18.75s/it]
[2024-04-02 12:11:21,146] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 31%|███▏      | 3381/10740 [16:58:20<39:40:21, 19.41s/it]
{'loss': 0.3184, 'learning_rate': 1.6040352470527552e-06, 'rewards/chosen': -2.8268840312957764, 'rewards/rejected': -5.232805252075195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.405921697616577, 'policy_logps/rejected': -617.7916870117188, 'policy_logps/chosen': -430.4409484863281, 'referece_logps/rejected': -565.463623046875, 'referece_logps/chosen': -402.172119140625, 'logits/rejected': -0.1917446106672287, 'logits/chosen': -0.22780312597751617, 'epoch': 1.89}

 31%|███▏      | 3382/10740 [16:58:38<38:29:37, 18.83s/it]

 31%|███▏      | 3383/10740 [16:58:58<39:14:29, 19.20s/it]

 32%|███▏      | 3384/10740 [16:59:20<41:03:19, 20.09s/it]

 32%|███▏      | 3385/10740 [16:59:43<42:47:51, 20.95s/it]

 32%|███▏      | 3386/10740 [17:00:01<40:44:56, 19.95s/it]

 32%|███▏      | 3387/10740 [17:00:11<35:01:32, 17.15s/it]
[2024-04-02 12:13:50,209] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3388/10740 [17:00:32<37:17:20, 18.26s/it]
[2024-04-02 12:14:10,311] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3389/10740 [17:00:52<38:24:47, 18.81s/it]

 32%|███▏      | 3390/10740 [17:01:13<39:46:46, 19.48s/it]

 32%|███▏      | 3391/10740 [17:01:26<35:34:12, 17.42s/it]

 32%|███▏      | 3392/10740 [17:01:41<34:27:02, 16.88s/it]

 32%|███▏      | 3393/10740 [17:01:54<31:58:19, 15.67s/it]

 32%|███▏      | 3394/10740 [17:02:10<32:06:32, 15.74s/it]

 32%|███▏      | 3395/10740 [17:02:27<32:38:52, 16.00s/it]

 32%|███▏      | 3396/10740 [17:02:47<35:16:51, 17.29s/it]


 32%|███▏      | 3398/10740 [17:03:18<34:26:46, 16.89s/it]
{'loss': 0.2865, 'learning_rate': 1.5999413923525503e-06, 'rewards/chosen': -2.4611473083496094, 'rewards/rejected': -5.844615936279297, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3834683895111084, 'policy_logps/rejected': -272.11505126953125, 'policy_logps/chosen': -470.6642150878906, 'referece_logps/rejected': -213.6688995361328, 'referece_logps/chosen': -446.052734375, 'logits/rejected': 0.11490786075592041, 'logits/chosen': 0.09584051370620728, 'epoch': 1.9}


 32%|███▏      | 3400/10740 [17:03:55<35:50:10, 17.58s/it]
{'loss': 0.1982, 'learning_rate': 1.599458723548398e-06, 'rewards/chosen': -3.3910391330718994, 'rewards/rejected': -6.106825828552246, 'rewards/accuracies': 0.875, 'rewards/margins': 2.715787172317505, 'policy_logps/rejected': -340.59051513671875, 'policy_logps/chosen': -388.336181640625, 'referece_logps/rejected': -279.5223083496094, 'referece_logps/chosen': -354.42578125, 'logits/rejected': -0.42870184779167175, 'logits/chosen': -0.3368423283100128, 'epoch': 1.9}

 32%|███▏      | 3401/10740 [17:04:08<33:09:27, 16.26s/it]

 32%|███▏      | 3402/10740 [17:04:28<35:27:39, 17.40s/it]

 32%|███▏      | 3403/10740 [17:04:43<34:20:54, 16.85s/it]


 32%|███▏      | 3405/10740 [17:05:15<32:56:16, 16.17s/it]
{'loss': 0.2208, 'learning_rate': 1.5982510977831406e-06, 'rewards/chosen': -3.3341028690338135, 'rewards/rejected': -6.193060874938965, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8589577674865723, 'policy_logps/rejected': -414.6875, 'policy_logps/chosen': -272.6059265136719, 'referece_logps/rejected': -352.75689697265625, 'referece_logps/chosen': -239.26492309570312, 'logits/rejected': 0.033458855003118515, 'logits/chosen': -0.08597645908594131, 'epoch': 1.9}

 32%|███▏      | 3406/10740 [17:05:36<35:57:53, 17.65s/it]


 32%|███▏      | 3408/10740 [17:06:15<37:48:21, 18.56s/it]
{'loss': 0.2161, 'learning_rate': 1.5975258691993834e-06, 'rewards/chosen': -3.315298080444336, 'rewards/rejected': -6.157811164855957, 'rewards/accuracies': 0.75, 'rewards/margins': 2.842513084411621, 'policy_logps/rejected': -357.94635009765625, 'policy_logps/chosen': -525.4859008789062, 'referece_logps/rejected': -296.3682556152344, 'referece_logps/chosen': -492.33294677734375, 'logits/rejected': 0.2786848247051239, 'logits/chosen': 0.2160331904888153, 'epoch': 1.9}


 32%|███▏      | 3410/10740 [17:06:51<36:33:30, 17.96s/it]
{'loss': 0.223, 'learning_rate': 1.5970421117080176e-06, 'rewards/chosen': -2.9356448650360107, 'rewards/rejected': -5.922696113586426, 'rewards/accuracies': 1.0, 'rewards/margins': 2.987051010131836, 'policy_logps/rejected': -473.04180908203125, 'policy_logps/chosen': -369.256103515625, 'referece_logps/rejected': -413.8148498535156, 'referece_logps/chosen': -339.899658203125, 'logits/rejected': 0.5907574892044067, 'logits/chosen': 0.4345712661743164, 'epoch': 1.91}

 32%|███▏      | 3411/10740 [17:07:05<34:13:41, 16.81s/it]


 32%|███▏      | 3413/10740 [17:07:45<37:25:03, 18.38s/it]
{'loss': 0.297, 'learning_rate': 1.5963160682576665e-06, 'rewards/chosen': -4.005965232849121, 'rewards/rejected': -5.9390153884887695, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9330508708953857, 'policy_logps/rejected': -318.511474609375, 'policy_logps/chosen': -318.34228515625, 'referece_logps/rejected': -259.12127685546875, 'referece_logps/chosen': -278.28265380859375, 'logits/rejected': -1.1609678268432617, 'logits/chosen': -1.1713370084762573, 'epoch': 1.91}

 32%|███▏      | 3414/10740 [17:08:05<38:41:03, 19.01s/it]

 32%|███▏      | 3415/10740 [17:08:26<39:52:46, 19.60s/it]

 32%|███▏      | 3416/10740 [17:08:41<37:08:51, 18.26s/it]

 32%|███▏      | 3417/10740 [17:09:04<39:51:44, 19.60s/it]

 32%|███▏      | 3418/10740 [17:09:20<37:34:58, 18.48s/it]

 32%|███▏      | 3419/10740 [17:09:36<36:14:40, 17.82s/it]

 32%|███▏      | 3420/10740 [17:09:52<34:46:11, 17.10s/it]

 32%|███▏      | 3421/10740 [17:10:10<35:44:26, 17.58s/it]

 32%|███▏      | 3422/10740 [17:10:26<34:25:30, 16.93s/it]
[2024-04-02 12:24:06,524] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3423/10740 [17:10:48<37:45:56, 18.58s/it]

 32%|███▏      | 3424/10740 [17:11:04<36:08:54, 17.79s/it]

 32%|███▏      | 3425/10740 [17:11:15<31:51:32, 15.68s/it]

 32%|███▏      | 3426/10740 [17:11:36<35:20:45, 17.40s/it]

 32%|███▏      | 3427/10740 [17:11:56<36:56:55, 18.19s/it]

 32%|███▏      | 3428/10740 [17:12:16<37:49:06, 18.62s/it]

 32%|███▏      | 3429/10740 [17:12:37<39:02:05, 19.22s/it]

 32%|███▏      | 3430/10740 [17:12:51<35:45:35, 17.61s/it]

 32%|███▏      | 3431/10740 [17:13:09<36:20:47, 17.90s/it]

 32%|███▏      | 3432/10740 [17:13:26<35:35:30, 17.53s/it]

 32%|███▏      | 3433/10740 [17:13:47<37:42:59, 18.58s/it]

 32%|███▏      | 3434/10740 [17:14:00<34:17:58, 16.90s/it]

 32%|███▏      | 3435/10740 [17:14:21<36:47:33, 18.13s/it]


 32%|███▏      | 3437/10740 [17:15:06<41:05:14, 20.25s/it]

 32%|███▏      | 3438/10740 [17:15:27<41:43:28, 20.57s/it]

 32%|███▏      | 3439/10740 [17:15:42<38:15:24, 18.86s/it]

 32%|███▏      | 3440/10740 [17:16:02<39:13:48, 19.35s/it]

 32%|███▏      | 3441/10740 [17:16:21<38:29:29, 18.98s/it]

 32%|███▏      | 3442/10740 [17:16:43<40:49:35, 20.14s/it]
[2024-04-02 12:30:01,596] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3443/10740 [17:17:04<41:15:06, 20.35s/it]
{'loss': 0.2718, 'learning_rate': 1.589028884993426e-06, 'rewards/chosen': -3.809936285018921, 'rewards/rejected': -6.256243705749512, 'rewards/accuracies': 0.75, 'rewards/margins': 2.44630765914917, 'policy_logps/rejected': -370.08819580078125, 'policy_logps/chosen': -331.3400573730469, 'referece_logps/rejected': -307.5257568359375, 'referece_logps/chosen': -293.2406921386719, 'logits/rejected': -0.32912516593933105, 'logits/chosen': -0.24035421013832092, 'epoch': 1.92}


 32%|███▏      | 3445/10740 [17:17:40<39:18:14, 19.40s/it]

 32%|███▏      | 3446/10740 [17:17:58<38:34:45, 19.04s/it]

 32%|███▏      | 3447/10740 [17:18:15<37:08:35, 18.33s/it]
[2024-04-02 12:31:33,407] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3448/10740 [17:18:34<37:26:22, 18.48s/it]

 32%|███▏      | 3449/10740 [17:18:48<34:51:14, 17.21s/it]

 32%|███▏      | 3450/10740 [17:19:08<36:30:02, 18.03s/it]

 32%|███▏      | 3451/10740 [17:19:22<33:44:13, 16.66s/it]
{'loss': 0.232, 'learning_rate': 1.5870774682691712e-06, 'rewards/chosen': -2.467252016067505, 'rewards/rejected': -5.400622367858887, 'rewards/accuracies': 0.875, 'rewards/margins': 2.933370590209961, 'policy_logps/rejected': -394.3426818847656, 'policy_logps/chosen': -342.973388671875, 'referece_logps/rejected': -340.33642578125, 'referece_logps/chosen': -318.3009033203125, 'logits/rejected': -0.039830904453992844, 'logits/chosen': 0.08032018691301346, 'epoch': 1.93}


 32%|███▏      | 3453/10740 [17:19:49<30:20:28, 14.99s/it]

 32%|███▏      | 3454/10740 [17:20:08<33:05:41, 16.35s/it]

 32%|███▏      | 3455/10740 [17:20:19<29:47:32, 14.72s/it]

 32%|███▏      | 3456/10740 [17:20:37<31:32:15, 15.59s/it]

 32%|███▏      | 3457/10740 [17:20:49<29:27:22, 14.56s/it]
{'loss': 0.2635, 'learning_rate': 1.5856116624674102e-06, 'rewards/chosen': -3.474799156188965, 'rewards/rejected': -6.892911911010742, 'rewards/accuracies': 0.875, 'rewards/margins': 3.418112277984619, 'policy_logps/rejected': -463.39599609375, 'policy_logps/chosen': -468.21380615234375, 'referece_logps/rejected': -394.46685791015625, 'referece_logps/chosen': -433.46575927734375, 'logits/rejected': -1.0659370422363281, 'logits/chosen': -1.0917834043502808, 'epoch': 1.93}


 32%|███▏      | 3459/10740 [17:21:28<34:09:15, 16.89s/it]

 32%|███▏      | 3460/10740 [17:21:43<32:59:14, 16.31s/it]

 32%|███▏      | 3461/10740 [17:21:55<30:33:23, 15.11s/it]
{'loss': 0.2543, 'learning_rate': 1.584633393046993e-06, 'rewards/chosen': -3.1243693828582764, 'rewards/rejected': -7.189809322357178, 'rewards/accuracies': 1.0, 'rewards/margins': 4.0654401779174805, 'policy_logps/rejected': -442.62432861328125, 'policy_logps/chosen': -419.7508544921875, 'referece_logps/rejected': -370.7261962890625, 'referece_logps/chosen': -388.5071716308594, 'logits/rejected': -1.1767390966415405, 'logits/chosen': -1.103514552116394, 'epoch': 1.93}


 32%|███▏      | 3463/10740 [17:22:25<29:28:19, 14.58s/it]

 32%|███▏      | 3464/10740 [17:22:35<27:04:35, 13.40s/it]

 32%|███▏      | 3465/10740 [17:22:56<31:42:46, 15.69s/it]

 32%|███▏      | 3466/10740 [17:23:15<33:43:51, 16.69s/it]

 32%|███▏      | 3467/10740 [17:23:26<30:17:42, 15.00s/it]
{'loss': 0.2306, 'learning_rate': 1.5831643941465407e-06, 'rewards/chosen': -2.5466089248657227, 'rewards/rejected': -4.439106464385986, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8924976587295532, 'policy_logps/rejected': -441.21820068359375, 'policy_logps/chosen': -360.9039611816406, 'referece_logps/rejected': -396.8271484375, 'referece_logps/chosen': -335.4378662109375, 'logits/rejected': 0.9646679759025574, 'logits/chosen': 0.9701467752456665, 'epoch': 1.94}


 32%|███▏      | 3469/10740 [17:23:59<31:52:28, 15.78s/it]

 32%|███▏      | 3470/10740 [17:24:17<32:57:53, 16.32s/it]
{'loss': 0.255, 'learning_rate': 1.5824291783525104e-06, 'rewards/chosen': -2.477167844772339, 'rewards/rejected': -5.492466449737549, 'rewards/accuracies': 0.875, 'rewards/margins': 3.015298366546631, 'policy_logps/rejected': -359.2117919921875, 'policy_logps/chosen': -458.181640625, 'referece_logps/rejected': -304.2871398925781, 'referece_logps/chosen': -433.40997314453125, 'logits/rejected': -0.34854817390441895, 'logits/chosen': -0.24078841507434845, 'epoch': 1.94}
[2024-04-02 12:37:56,254] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 32%|███▏      | 3472/10740 [17:25:01<38:55:48, 19.28s/it]
{'loss': 0.2228, 'learning_rate': 1.581938769585863e-06, 'rewards/chosen': -2.876474618911743, 'rewards/rejected': -6.653208255767822, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7767333984375, 'policy_logps/rejected': -329.9532470703125, 'policy_logps/chosen': -497.05194091796875, 'referece_logps/rejected': -263.42120361328125, 'referece_logps/chosen': -468.28717041015625, 'logits/rejected': -0.7029628753662109, 'logits/chosen': -0.7584570050239563, 'epoch': 1.94}


 32%|███▏      | 3474/10740 [17:25:42<40:05:21, 19.86s/it]

 32%|███▏      | 3475/10740 [17:26:01<39:52:12, 19.76s/it]
[2024-04-02 12:39:19,615] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2381, 'learning_rate': 1.5812027595259903e-06, 'rewards/chosen': -2.5968499183654785, 'rewards/rejected': -6.540771961212158, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9439220428466797, 'policy_logps/rejected': -375.75213623046875, 'policy_logps/chosen': -781.5250244140625, 'referece_logps/rejected': -310.3443908691406, 'referece_logps/chosen': -755.5565185546875, 'logits/rejected': -0.8830721974372864, 'logits/chosen': -1.267723560333252, 'epoch': 1.94}

 32%|███▏      | 3476/10740 [17:26:22<40:32:51, 20.10s/it]


 32%|███▏      | 3478/10740 [17:27:05<41:47:08, 20.71s/it]

 32%|███▏      | 3479/10740 [17:27:19<37:22:31, 18.53s/it]
{'loss': 0.2482, 'learning_rate': 1.5802206728182969e-06, 'rewards/chosen': -2.237783432006836, 'rewards/rejected': -5.333486557006836, 'rewards/accuracies': 0.875, 'rewards/margins': 3.095702648162842, 'policy_logps/rejected': -308.73626708984375, 'policy_logps/chosen': -371.03607177734375, 'referece_logps/rejected': -255.40139770507812, 'referece_logps/chosen': -348.658203125, 'logits/rejected': -0.3483433723449707, 'logits/chosen': -0.24098080396652222, 'epoch': 1.94}


 32%|███▏      | 3481/10740 [17:27:50<35:14:12, 17.48s/it]

 32%|███▏      | 3482/10740 [17:28:11<37:18:11, 18.50s/it]

 32%|███▏      | 3483/10740 [17:28:31<38:30:02, 19.10s/it]
[2024-04-02 12:41:49,655] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 3484/10740 [17:28:53<39:48:08, 19.75s/it]

 32%|███▏      | 3485/10740 [17:29:07<36:18:24, 18.02s/it]

 32%|███▏      | 3486/10740 [17:29:26<36:59:49, 18.36s/it]

 32%|███▏      | 3487/10740 [17:29:42<35:42:10, 17.72s/it]

 32%|███▏      | 3488/10740 [17:29:57<34:08:19, 16.95s/it]

 32%|███▏      | 3489/10740 [17:30:18<36:26:45, 18.09s/it]
{'loss': 0.2648, 'learning_rate': 1.5777617651042995e-06, 'rewards/chosen': -2.491506576538086, 'rewards/rejected': -5.541703224182129, 'rewards/accuracies': 0.875, 'rewards/margins': 3.050196647644043, 'policy_logps/rejected': -232.20066833496094, 'policy_logps/chosen': -253.20555114746094, 'referece_logps/rejected': -176.78363037109375, 'referece_logps/chosen': -228.29043579101562, 'logits/rejected': -0.18703985214233398, 'logits/chosen': -0.2167879194021225, 'epoch': 1.95}


 33%|███▎      | 3491/10740 [17:30:50<34:15:00, 17.01s/it]
{'loss': 0.2623, 'learning_rate': 1.5772693522592778e-06, 'rewards/chosen': -3.219773530960083, 'rewards/rejected': -5.486886024475098, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2671120166778564, 'policy_logps/rejected': -367.68536376953125, 'policy_logps/chosen': -356.72540283203125, 'referece_logps/rejected': -312.8165283203125, 'referece_logps/chosen': -324.52764892578125, 'logits/rejected': 0.22282537817955017, 'logits/chosen': 0.24003136157989502, 'epoch': 1.95}


 33%|███▎      | 3493/10740 [17:31:24<34:16:10, 17.02s/it]

 33%|███▎      | 3494/10740 [17:31:42<34:39:44, 17.22s/it]

 33%|███▎      | 3495/10740 [17:32:01<36:05:15, 17.93s/it]

 33%|███▎      | 3496/10740 [17:32:20<36:30:06, 18.14s/it]

 33%|███▎      | 3497/10740 [17:32:41<38:35:06, 19.18s/it]

 33%|███▎      | 3498/10740 [17:32:56<35:28:16, 17.63s/it]
{'loss': 0.2193, 'learning_rate': 1.5755442546000718e-06, 'rewards/chosen': -2.6845085620880127, 'rewards/rejected': -6.442765712738037, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7582571506500244, 'policy_logps/rejected': -408.03326416015625, 'policy_logps/chosen': -343.62261962890625, 'referece_logps/rejected': -343.6056213378906, 'referece_logps/chosen': -316.77752685546875, 'logits/rejected': -0.9424973130226135, 'logits/chosen': -0.8671070337295532, 'epoch': 1.95}


 33%|███▎      | 3500/10740 [17:33:34<37:00:24, 18.40s/it]

 33%|███▎      | 3501/10740 [17:34:08<46:14:16, 22.99s/it]

 33%|███▎      | 3502/10740 [17:34:29<45:19:02, 22.54s/it]

 33%|███▎      | 3503/10740 [17:34:49<43:51:26, 21.82s/it]

 33%|███▎      | 3504/10740 [17:35:05<40:02:22, 19.92s/it]

 33%|███▎      | 3505/10740 [17:35:24<39:49:31, 19.82s/it]
{'loss': 0.1963, 'learning_rate': 1.5738165919308894e-06, 'rewards/chosen': -2.721942663192749, 'rewards/rejected': -7.08591365814209, 'rewards/accuracies': 0.875, 'rewards/margins': 4.36397123336792, 'policy_logps/rejected': -286.72235107421875, 'policy_logps/chosen': -312.3797912597656, 'referece_logps/rejected': -215.8632049560547, 'referece_logps/chosen': -285.1603698730469, 'logits/rejected': 0.258978009223938, 'logits/chosen': 0.3196798264980316, 'epoch': 1.96}


 33%|███▎      | 3507/10740 [17:35:59<36:51:18, 18.34s/it]
{'loss': 0.2851, 'learning_rate': 1.5733225039783398e-06, 'rewards/chosen': -3.3234729766845703, 'rewards/rejected': -5.55573034286499, 'rewards/accuracies': 0.875, 'rewards/margins': 2.23225736618042, 'policy_logps/rejected': -231.40992736816406, 'policy_logps/chosen': -480.6220703125, 'referece_logps/rejected': -175.85260009765625, 'referece_logps/chosen': -447.3873291015625, 'logits/rejected': -0.37145283818244934, 'logits/chosen': -0.6680002212524414, 'epoch': 1.96}


 33%|███▎      | 3509/10740 [17:36:38<38:25:26, 19.13s/it]
{'loss': 0.262, 'learning_rate': 1.5728282074454438e-06, 'rewards/chosen': -2.665957450866699, 'rewards/rejected': -6.1554083824157715, 'rewards/accuracies': 0.875, 'rewards/margins': 3.489450454711914, 'policy_logps/rejected': -435.0606689453125, 'policy_logps/chosen': -347.7709655761719, 'referece_logps/rejected': -373.506591796875, 'referece_logps/chosen': -321.1114196777344, 'logits/rejected': -0.6147534251213074, 'logits/chosen': -0.6032390594482422, 'epoch': 1.96}


 33%|███▎      | 3511/10740 [17:37:15<37:28:39, 18.66s/it]

 33%|███▎      | 3512/10740 [17:37:35<38:27:09, 19.15s/it]

 33%|███▎      | 3513/10740 [17:37:50<35:31:56, 17.70s/it]
{'loss': 0.3443, 'learning_rate': 1.5718389893580084e-06, 'rewards/chosen': -2.2058331966400146, 'rewards/rejected': -5.731472015380859, 'rewards/accuracies': 0.875, 'rewards/margins': 3.525639057159424, 'policy_logps/rejected': -402.6916198730469, 'policy_logps/chosen': -317.5622863769531, 'referece_logps/rejected': -345.376953125, 'referece_logps/chosen': -295.50390625, 'logits/rejected': -1.1067287921905518, 'logits/chosen': -0.954513669013977, 'epoch': 1.96}

 33%|███▎      | 3514/10740 [17:38:07<35:14:54, 17.56s/it]


 33%|███▎      | 3516/10740 [17:38:46<37:14:00, 18.55s/it]
[2024-04-02 12:52:04,389] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3517/10740 [17:39:02<35:25:51, 17.66s/it]

 33%|███▎      | 3518/10740 [17:39:25<38:41:34, 19.29s/it]
[2024-04-02 12:52:43,045] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3519/10740 [17:39:45<39:00:05, 19.44s/it]
[2024-04-02 12:53:02,854] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3520/10740 [17:40:02<37:29:55, 18.70s/it]

 33%|███▎      | 3521/10740 [17:40:23<38:51:47, 19.38s/it]

 33%|███▎      | 3522/10740 [17:40:45<40:34:51, 20.24s/it]
{'loss': 0.1991, 'learning_rate': 1.5696102082623775e-06, 'rewards/chosen': -2.188000440597534, 'rewards/rejected': -6.360726833343506, 'rewards/accuracies': 0.875, 'rewards/margins': 4.172726154327393, 'policy_logps/rejected': -425.911376953125, 'policy_logps/chosen': -418.095458984375, 'referece_logps/rejected': -362.3040466308594, 'referece_logps/chosen': -396.2154235839844, 'logits/rejected': 1.0709103345870972, 'logits/chosen': 1.0059701204299927, 'epoch': 1.97}

 33%|███▎      | 3523/10740 [17:41:01<38:26:13, 19.17s/it]


 33%|███▎      | 3525/10740 [17:41:36<36:48:40, 18.37s/it]

 33%|███▎      | 3526/10740 [17:41:49<33:23:48, 16.67s/it]

 33%|███▎      | 3527/10740 [17:42:09<35:19:21, 17.63s/it]
[2024-04-02 12:55:26,955] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3528/10740 [17:42:27<35:44:53, 17.84s/it]

 33%|███▎      | 3529/10740 [17:42:45<35:35:15, 17.77s/it]
{'loss': 0.1995, 'learning_rate': 1.567873809797499e-06, 'rewards/chosen': -2.1802399158477783, 'rewards/rejected': -6.408331871032715, 'rewards/accuracies': 1.0, 'rewards/margins': 4.228091716766357, 'policy_logps/rejected': -328.0469055175781, 'policy_logps/chosen': -302.52606201171875, 'referece_logps/rejected': -263.9635925292969, 'referece_logps/chosen': -280.7236633300781, 'logits/rejected': 0.49585509300231934, 'logits/chosen': 0.36877354979515076, 'epoch': 1.97}


 33%|███▎      | 3531/10740 [17:43:22<36:36:04, 18.28s/it]

 33%|███▎      | 3532/10740 [17:43:41<36:37:48, 18.29s/it]

 33%|███▎      | 3533/10740 [17:43:59<36:40:06, 18.32s/it]
{'loss': 0.1963, 'learning_rate': 1.5668804453175358e-06, 'rewards/chosen': -1.6885454654693604, 'rewards/rejected': -3.983436107635498, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2948904037475586, 'policy_logps/rejected': -329.3492736816406, 'policy_logps/chosen': -357.3407897949219, 'referece_logps/rejected': -289.51495361328125, 'referece_logps/chosen': -340.455322265625, 'logits/rejected': -0.16878516972064972, 'logits/chosen': -0.08845272660255432, 'epoch': 1.97}


 33%|███▎      | 3535/10740 [17:44:21<29:12:48, 14.60s/it]
{'loss': 0.2882, 'learning_rate': 1.566383453632193e-06, 'rewards/chosen': -3.4005022048950195, 'rewards/rejected': -6.508760929107666, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1082587242126465, 'policy_logps/rejected': -452.0174255371094, 'policy_logps/chosen': -374.3660583496094, 'referece_logps/rejected': -386.9298400878906, 'referece_logps/chosen': -340.36102294921875, 'logits/rejected': -0.7185168266296387, 'logits/chosen': -0.5948171615600586, 'epoch': 1.97}


 33%|███▎      | 3537/10740 [17:44:54<31:24:46, 15.70s/it]

 33%|███▎      | 3538/10740 [17:45:06<29:08:24, 14.57s/it]

 33%|███▎      | 3539/10740 [17:45:17<26:50:11, 13.42s/it]
{'loss': 0.2316, 'learning_rate': 1.5653888522748377e-06, 'rewards/chosen': -2.7941997051239014, 'rewards/rejected': -5.625549793243408, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8313498497009277, 'policy_logps/rejected': -279.2780456542969, 'policy_logps/chosen': -381.4385681152344, 'referece_logps/rejected': -223.02256774902344, 'referece_logps/chosen': -353.49658203125, 'logits/rejected': -0.10045743733644485, 'logits/chosen': -0.3319094479084015, 'epoch': 1.98}

 33%|███▎      | 3540/10740 [17:45:38<31:17:07, 15.64s/it]


 33%|███▎      | 3542/10740 [17:46:19<36:11:52, 18.10s/it]

 33%|███▎      | 3543/10740 [17:46:37<36:29:13, 18.25s/it]

 33%|███▎      | 3544/10740 [17:46:51<33:59:38, 17.01s/it]

 33%|███▎      | 3545/10740 [17:47:11<35:29:42, 17.76s/it]

 33%|███▎      | 3546/10740 [17:47:30<36:35:21, 18.31s/it]

 33%|███▎      | 3547/10740 [17:47:51<37:59:48, 19.02s/it]
{'loss': 0.2942, 'learning_rate': 1.5633971826808585e-06, 'rewards/chosen': -3.536505937576294, 'rewards/rejected': -6.802037239074707, 'rewards/accuracies': 0.875, 'rewards/margins': 3.265531539916992, 'policy_logps/rejected': -386.1705322265625, 'policy_logps/chosen': -358.1371765136719, 'referece_logps/rejected': -318.1501770019531, 'referece_logps/chosen': -322.7721252441406, 'logits/rejected': -0.15332579612731934, 'logits/chosen': -0.023562103509902954, 'epoch': 1.98}


 33%|███▎      | 3549/10740 [17:48:23<35:00:14, 17.52s/it]

 33%|███▎      | 3550/10740 [17:48:37<32:21:55, 16.21s/it]

 33%|███▎      | 3551/10740 [17:48:51<31:26:42, 15.75s/it]

 33%|███▎      | 3552/10740 [17:49:13<35:11:53, 17.63s/it]

 33%|███▎      | 3553/10740 [17:49:33<36:18:15, 18.19s/it]

 33%|███▎      | 3554/10740 [17:49:55<38:29:10, 19.28s/it]
[2024-04-02 13:03:12,796] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3555/10740 [17:50:16<39:31:30, 19.80s/it]
[2024-04-02 13:03:33,821] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4282, 'learning_rate': 1.5614022335776965e-06, 'rewards/chosen': -3.5933470726013184, 'rewards/rejected': -5.9998016357421875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.406454563140869, 'policy_logps/rejected': -366.4798583984375, 'policy_logps/chosen': -328.1345520019531, 'referece_logps/rejected': -306.48187255859375, 'referece_logps/chosen': -292.20111083984375, 'logits/rejected': -0.40782392024993896, 'logits/chosen': -0.30777883529663086, 'epoch': 1.99}
[2024-04-02 13:03:54,786] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 33%|███▎      | 3557/10740 [17:50:55<39:26:00, 19.76s/it]

 33%|███▎      | 3558/10740 [17:51:11<37:10:15, 18.63s/it]
{'loss': 0.3491, 'learning_rate': 1.5606532845350733e-06, 'rewards/chosen': -2.719247579574585, 'rewards/rejected': -6.2325921058654785, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5133447647094727, 'policy_logps/rejected': -371.4927978515625, 'policy_logps/chosen': -354.2283935546875, 'referece_logps/rejected': -309.16693115234375, 'referece_logps/chosen': -327.0359191894531, 'logits/rejected': -0.0028602704405784607, 'logits/chosen': -0.008844949305057526, 'epoch': 1.99}

 33%|███▎      | 3559/10740 [17:51:26<34:55:49, 17.51s/it]


 33%|███▎      | 3561/10740 [17:52:08<38:13:48, 19.17s/it]
{'loss': 0.1748, 'learning_rate': 1.559903876557357e-06, 'rewards/chosen': -3.0444555282592773, 'rewards/rejected': -6.6081929206848145, 'rewards/accuracies': 1.0, 'rewards/margins': 3.563737392425537, 'policy_logps/rejected': -456.56683349609375, 'policy_logps/chosen': -350.1600646972656, 'referece_logps/rejected': -390.4849548339844, 'referece_logps/chosen': -319.7155456542969, 'logits/rejected': -0.3012005686759949, 'logits/chosen': -0.16542783379554749, 'epoch': 1.99}


 33%|███▎      | 3563/10740 [17:52:47<38:34:13, 19.35s/it]

 33%|███▎      | 3564/10740 [17:53:05<37:51:59, 19.00s/it]
{'loss': 0.2437, 'learning_rate': 1.5591540102579923e-06, 'rewards/chosen': -2.407428026199341, 'rewards/rejected': -4.278465270996094, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8710371255874634, 'policy_logps/rejected': -279.01568603515625, 'policy_logps/chosen': -308.10504150390625, 'referece_logps/rejected': -236.23106384277344, 'referece_logps/chosen': -284.03076171875, 'logits/rejected': -0.4381076991558075, 'logits/chosen': -0.35637661814689636, 'epoch': 1.99}

 33%|███▎      | 3565/10740 [17:53:24<37:48:13, 18.97s/it]

 33%|███▎      | 3566/10740 [17:53:40<36:01:13, 18.08s/it]

 33%|███▎      | 3567/10740 [17:54:00<37:09:05, 18.65s/it]


 33%|███▎      | 3569/10740 [17:54:37<37:42:09, 18.93s/it]

 33%|███▎      | 3570/10740 [17:54:51<34:38:26, 17.39s/it]

 33%|███▎      | 3571/10740 [17:55:09<34:45:36, 17.46s/it]
{'loss': 0.3228, 'learning_rate': 1.5574025433128753e-06, 'rewards/chosen': -3.6289265155792236, 'rewards/rejected': -6.3637213706970215, 'rewards/accuracies': 0.875, 'rewards/margins': 2.734794855117798, 'policy_logps/rejected': -489.37835693359375, 'policy_logps/chosen': -400.4039306640625, 'referece_logps/rejected': -425.7411193847656, 'referece_logps/chosen': -364.1146545410156, 'logits/rejected': -0.3676244616508484, 'logits/chosen': -0.47685572504997253, 'epoch': 1.99}

 33%|███▎      | 3572/10740 [17:55:29<36:12:33, 18.19s/it]


 33%|███▎      | 3574/10740 [17:56:06<36:43:05, 18.45s/it]

 33%|███▎      | 3575/10740 [17:56:17<32:33:40, 16.36s/it]

 33%|███▎      | 3576/10740 [17:56:40<36:04:46, 18.13s/it]

 33%|███▎      | 3577/10740 [17:57:00<37:15:16, 18.72s/it]

 33%|███▎      | 3578/10740 [17:57:19<37:33:08, 18.88s/it]
{'loss': 0.2378, 'learning_rate': 1.5556485922093754e-06, 'rewards/chosen': -2.755650758743286, 'rewards/rejected': -7.516550064086914, 'rewards/accuracies': 1.0, 'rewards/margins': 4.760898590087891, 'policy_logps/rejected': -329.1714172363281, 'policy_logps/chosen': -439.5078430175781, 'referece_logps/rejected': -254.00592041015625, 'referece_logps/chosen': -411.95135498046875, 'logits/rejected': 0.04090959578752518, 'logits/chosen': -0.041275374591350555, 'epoch': 2.0}

 33%|███▎      | 3579/10740 [17:57:32<34:17:30, 17.24s/it]

 33%|███▎      | 3580/10740 [17:57:55<37:14:22, 18.72s/it]

 33%|███▎      | 3581/10740 [17:58:15<38:08:43, 19.18s/it]


 33%|███▎      | 3583/10740 [17:58:44<34:01:11, 17.11s/it]

 33%|███▎      | 3584/10740 [17:59:00<33:22:59, 16.79s/it]
{'loss': 0.2442, 'learning_rate': 1.5541432342869045e-06, 'rewards/chosen': -3.1292152404785156, 'rewards/rejected': -7.146657466888428, 'rewards/accuracies': 1.0, 'rewards/margins': 4.017441749572754, 'policy_logps/rejected': -387.2740478515625, 'policy_logps/chosen': -350.8382263183594, 'referece_logps/rejected': -315.8074951171875, 'referece_logps/chosen': -319.54608154296875, 'logits/rejected': -0.6974821090698242, 'logits/chosen': -0.5686714053153992, 'epoch': 2.0}


 33%|███▎      | 3586/10740 [17:59:32<32:01:11, 16.11s/it]
{'loss': 0.3175, 'learning_rate': 1.5536410448637877e-06, 'rewards/chosen': -2.4300055503845215, 'rewards/rejected': -4.237850189208984, 'rewards/accuracies': 0.875, 'rewards/margins': 1.807844877243042, 'policy_logps/rejected': -337.41009521484375, 'policy_logps/chosen': -271.05322265625, 'referece_logps/rejected': -295.0316162109375, 'referece_logps/chosen': -246.75315856933594, 'logits/rejected': -0.18058809638023376, 'logits/chosen': -0.21123141050338745, 'epoch': 2.0}


 33%|███▎      | 3588/10740 [18:00:08<34:36:28, 17.42s/it]

 33%|███▎      | 3589/10740 [18:00:26<34:35:03, 17.41s/it]

 33%|███▎      | 3590/10740 [18:00:41<33:30:45, 16.87s/it]
{'loss': 0.3823, 'learning_rate': 1.5526360619402134e-06, 'rewards/chosen': -2.5349438190460205, 'rewards/rejected': -5.733052730560303, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1981091499328613, 'policy_logps/rejected': -442.7409973144531, 'policy_logps/chosen': -597.7890014648438, 'referece_logps/rejected': -385.4104919433594, 'referece_logps/chosen': -572.4395751953125, 'logits/rejected': 0.3509948253631592, 'logits/chosen': 0.3568328619003296, 'epoch': 2.01}
[2024-04-02 13:14:17,110] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3591/10740 [18:00:59<33:57:59, 17.10s/it]
[2024-04-02 13:14:37,368] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 3592/10740 [18:01:19<35:50:28, 18.05s/it]

 33%|███▎      | 3593/10740 [18:01:39<36:54:39, 18.59s/it]


 33%|███▎      | 3595/10740 [18:02:10<33:24:59, 16.84s/it]

 33%|███▎      | 3596/10740 [18:02:26<32:30:19, 16.38s/it]

 33%|███▎      | 3597/10740 [18:02:46<35:06:17, 17.69s/it]

 34%|███▎      | 3598/10740 [18:03:08<37:16:09, 18.79s/it]
{'loss': 0.2765, 'learning_rate': 1.5506236849039533e-06, 'rewards/chosen': -4.080085754394531, 'rewards/rejected': -9.089534759521484, 'rewards/accuracies': 0.875, 'rewards/margins': 5.009449005126953, 'policy_logps/rejected': -390.5807800292969, 'policy_logps/chosen': -571.8372192382812, 'referece_logps/rejected': -299.6853942871094, 'referece_logps/chosen': -531.036376953125, 'logits/rejected': 0.15915845334529877, 'logits/chosen': 0.3509884476661682, 'epoch': 2.01}

 34%|███▎      | 3599/10740 [18:03:27<37:41:28, 19.00s/it]


 34%|███▎      | 3601/10740 [18:04:02<36:37:50, 18.47s/it]

 34%|███▎      | 3602/10740 [18:04:18<34:56:34, 17.62s/it]
{'loss': 0.1862, 'learning_rate': 1.5496162937197574e-06, 'rewards/chosen': -3.3422484397888184, 'rewards/rejected': -7.429937362670898, 'rewards/accuracies': 0.875, 'rewards/margins': 4.08768892288208, 'policy_logps/rejected': -549.62255859375, 'policy_logps/chosen': -402.68267822265625, 'referece_logps/rejected': -475.3231506347656, 'referece_logps/chosen': -369.26019287109375, 'logits/rejected': -0.8154858350753784, 'logits/chosen': -0.566207230091095, 'epoch': 2.01}

 34%|███▎      | 3603/10740 [18:04:35<34:37:33, 17.47s/it]

 34%|███▎      | 3604/10740 [18:04:53<34:47:50, 17.55s/it]


 34%|███▎      | 3606/10740 [18:05:18<29:25:11, 14.85s/it]
{'loss': 0.3561, 'learning_rate': 1.5486081027124556e-06, 'rewards/chosen': -3.4136717319488525, 'rewards/rejected': -8.311570167541504, 'rewards/accuracies': 0.875, 'rewards/margins': 4.897898197174072, 'policy_logps/rejected': -346.87933349609375, 'policy_logps/chosen': -326.69097900390625, 'referece_logps/rejected': -263.7636413574219, 'referece_logps/chosen': -292.55426025390625, 'logits/rejected': -0.47430747747421265, 'logits/chosen': -0.5404691100120544, 'epoch': 2.01}


 34%|███▎      | 3608/10740 [18:05:56<34:22:16, 17.35s/it]

 34%|███▎      | 3609/10740 [18:06:12<33:22:50, 16.85s/it]

 34%|███▎      | 3610/10740 [18:06:32<35:34:03, 17.96s/it]
{'loss': 0.336, 'learning_rate': 1.5475991133492066e-06, 'rewards/chosen': -2.6340017318725586, 'rewards/rejected': -5.445348739624023, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8113467693328857, 'policy_logps/rejected': -332.50103759765625, 'policy_logps/chosen': -255.18014526367188, 'referece_logps/rejected': -278.0475769042969, 'referece_logps/chosen': -228.8401336669922, 'logits/rejected': -0.6000145673751831, 'logits/chosen': -0.7002792358398438, 'epoch': 2.02}

 34%|███▎      | 3611/10740 [18:06:55<38:07:59, 19.26s/it]

 34%|███▎      | 3612/10740 [18:07:15<38:58:28, 19.68s/it]

 34%|███▎      | 3613/10740 [18:07:37<40:03:38, 20.24s/it]


 34%|███▎      | 3615/10740 [18:08:17<39:41:10, 20.05s/it]

 34%|███▎      | 3616/10740 [18:08:34<37:56:29, 19.17s/it]
{'loss': 0.2633, 'learning_rate': 1.5460841355992015e-06, 'rewards/chosen': -3.27485990524292, 'rewards/rejected': -5.234201908111572, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9593422412872314, 'policy_logps/rejected': -412.9931335449219, 'policy_logps/chosen': -487.900634765625, 'referece_logps/rejected': -360.651123046875, 'referece_logps/chosen': -455.1520690917969, 'logits/rejected': 0.0708092749118805, 'logits/chosen': -0.07514521479606628, 'epoch': 2.02}


 34%|███▎      | 3618/10740 [18:09:04<34:19:34, 17.35s/it]

 34%|███▎      | 3619/10740 [18:09:24<35:46:51, 18.09s/it]

 34%|███▎      | 3620/10740 [18:09:44<36:45:29, 18.59s/it]
{'loss': 0.2094, 'learning_rate': 1.5450731567725246e-06, 'rewards/chosen': -2.34623384475708, 'rewards/rejected': -5.1910176277160645, 'rewards/accuracies': 0.75, 'rewards/margins': 2.844783306121826, 'policy_logps/rejected': -224.15478515625, 'policy_logps/chosen': -278.40771484375, 'referece_logps/rejected': -172.24461364746094, 'referece_logps/chosen': -254.94540405273438, 'logits/rejected': -0.8202393054962158, 'logits/chosen': -0.9162155389785767, 'epoch': 2.02}


 34%|███▎      | 3622/10740 [18:10:19<35:10:12, 17.79s/it]

 34%|███▎      | 3623/10740 [18:10:34<33:39:13, 17.02s/it]

 34%|███▎      | 3624/10740 [18:10:48<32:05:42, 16.24s/it]
{'loss': 0.2358, 'learning_rate': 1.5440613847340922e-06, 'rewards/chosen': -3.523153305053711, 'rewards/rejected': -5.9448370933532715, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4216833114624023, 'policy_logps/rejected': -378.0499572753906, 'policy_logps/chosen': -450.7193603515625, 'referece_logps/rejected': -318.6015930175781, 'referece_logps/chosen': -415.4877624511719, 'logits/rejected': -0.6523182392120361, 'logits/chosen': -0.843281090259552, 'epoch': 2.02}

 34%|███▍      | 3625/10740 [18:11:04<31:52:30, 16.13s/it]

 34%|███▍      | 3626/10740 [18:11:24<33:59:04, 17.20s/it]

 34%|███▍      | 3627/10740 [18:11:39<32:51:42, 16.63s/it]


 34%|███▍      | 3629/10740 [18:12:05<28:39:45, 14.51s/it]
{'loss': 0.1376, 'learning_rate': 1.5427955564751942e-06, 'rewards/chosen': -3.2105915546417236, 'rewards/rejected': -6.4735870361328125, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2629950046539307, 'policy_logps/rejected': -313.09942626953125, 'policy_logps/chosen': -430.7722473144531, 'referece_logps/rejected': -248.36354064941406, 'referece_logps/chosen': -398.6663818359375, 'logits/rejected': -0.1379965990781784, 'logits/chosen': -0.28112390637397766, 'epoch': 2.03}


 34%|███▍      | 3631/10740 [18:12:27<25:07:41, 12.72s/it]
{'loss': 0.1976, 'learning_rate': 1.5422888794303123e-06, 'rewards/chosen': -3.394580125808716, 'rewards/rejected': -7.482924461364746, 'rewards/accuracies': 1.0, 'rewards/margins': 4.088344097137451, 'policy_logps/rejected': -354.4338073730469, 'policy_logps/chosen': -478.4064025878906, 'referece_logps/rejected': -279.60455322265625, 'referece_logps/chosen': -444.4605712890625, 'logits/rejected': 0.10027387738227844, 'logits/chosen': 0.11046969890594482, 'epoch': 2.03}

 34%|███▍      | 3632/10740 [18:12:38<23:54:44, 12.11s/it]

 34%|███▍      | 3633/10740 [18:12:56<27:21:45, 13.86s/it]

 34%|███▍      | 3634/10740 [18:13:18<32:17:38, 16.36s/it]

 34%|███▍      | 3635/10740 [18:13:34<32:18:11, 16.37s/it]

 34%|███▍      | 3636/10740 [18:13:45<29:07:08, 14.76s/it]

 34%|███▍      | 3637/10740 [18:14:04<31:48:33, 16.12s/it]

 34%|███▍      | 3638/10740 [18:14:20<31:36:54, 16.03s/it]

 34%|███▍      | 3639/10740 [18:14:36<31:18:13, 15.87s/it]


 34%|███▍      | 3641/10740 [18:15:09<31:26:49, 15.95s/it]

 34%|███▍      | 3642/10740 [18:15:27<32:49:10, 16.65s/it]
{'loss': 0.2007, 'learning_rate': 1.5394986340706035e-06, 'rewards/chosen': -3.1371943950653076, 'rewards/rejected': -5.249330520629883, 'rewards/accuracies': 0.875, 'rewards/margins': 2.112135648727417, 'policy_logps/rejected': -341.3623962402344, 'policy_logps/chosen': -438.8010559082031, 'referece_logps/rejected': -288.8691101074219, 'referece_logps/chosen': -407.4290771484375, 'logits/rejected': -0.6191095113754272, 'logits/chosen': -0.6624628305435181, 'epoch': 2.03}

 34%|███▍      | 3643/10740 [18:15:46<33:56:36, 17.22s/it]


 34%|███▍      | 3645/10740 [18:16:21<34:23:11, 17.45s/it]
{'loss': 0.192, 'learning_rate': 1.5387366263276138e-06, 'rewards/chosen': -2.276817798614502, 'rewards/rejected': -6.716036796569824, 'rewards/accuracies': 1.0, 'rewards/margins': 4.439218521118164, 'policy_logps/rejected': -406.0302734375, 'policy_logps/chosen': -353.75054931640625, 'referece_logps/rejected': -338.8699035644531, 'referece_logps/chosen': -330.9823913574219, 'logits/rejected': -0.09140580892562866, 'logits/chosen': -0.06280176341533661, 'epoch': 2.04}

 34%|███▍      | 3646/10740 [18:16:36<33:18:41, 16.90s/it]

 34%|███▍      | 3647/10740 [18:16:54<33:33:52, 17.04s/it]


 34%|███▍      | 3649/10740 [18:17:33<36:26:06, 18.50s/it]

 34%|███▍      | 3650/10740 [18:17:53<37:09:52, 18.87s/it]
{'loss': 0.2563, 'learning_rate': 1.5374656337425354e-06, 'rewards/chosen': -2.105593681335449, 'rewards/rejected': -3.8477272987365723, 'rewards/accuracies': 0.75, 'rewards/margins': 1.742133378982544, 'policy_logps/rejected': -257.827392578125, 'policy_logps/chosen': -234.382568359375, 'referece_logps/rejected': -219.35015869140625, 'referece_logps/chosen': -213.32666015625, 'logits/rejected': -0.1797407865524292, 'logits/chosen': -0.20952920615673065, 'epoch': 2.04}

 34%|███▍      | 3651/10740 [18:18:11<36:23:26, 18.48s/it]

 34%|███▍      | 3652/10740 [18:18:30<37:10:36, 18.88s/it]


 34%|███▍      | 3654/10740 [18:19:09<37:26:24, 19.02s/it]
{'loss': 0.2862, 'learning_rate': 1.5364479596271043e-06, 'rewards/chosen': -3.1863794326782227, 'rewards/rejected': -4.185591697692871, 'rewards/accuracies': 0.75, 'rewards/margins': 0.999212384223938, 'policy_logps/rejected': -365.8083190917969, 'policy_logps/chosen': -397.97698974609375, 'referece_logps/rejected': -323.952392578125, 'referece_logps/chosen': -366.1131896972656, 'logits/rejected': -0.5986210107803345, 'logits/chosen': -0.5320062637329102, 'epoch': 2.04}


 34%|███▍      | 3656/10740 [18:19:41<33:52:45, 17.22s/it]
{'loss': 0.3856, 'learning_rate': 1.535938829729273e-06, 'rewards/chosen': -2.2522940635681152, 'rewards/rejected': -3.583102226257324, 'rewards/accuracies': 0.75, 'rewards/margins': 1.330808162689209, 'policy_logps/rejected': -301.5987854003906, 'policy_logps/chosen': -326.99078369140625, 'referece_logps/rejected': -265.76776123046875, 'referece_logps/chosen': -304.46783447265625, 'logits/rejected': 0.2493257224559784, 'logits/chosen': 0.2773653268814087, 'epoch': 2.04}

 34%|███▍      | 3657/10740 [18:20:01<35:17:40, 17.94s/it]

 34%|███▍      | 3658/10740 [18:20:21<36:22:17, 18.49s/it]


 34%|███▍      | 3660/10740 [18:21:01<38:32:21, 19.60s/it]
{'loss': 0.2779, 'learning_rate': 1.5349199851795095e-06, 'rewards/chosen': -1.9385061264038086, 'rewards/rejected': -3.1960835456848145, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2575775384902954, 'policy_logps/rejected': -340.141845703125, 'policy_logps/chosen': -384.7029724121094, 'referece_logps/rejected': -308.1810607910156, 'referece_logps/chosen': -365.31787109375, 'logits/rejected': 0.04078269749879837, 'logits/chosen': -0.03094097599387169, 'epoch': 2.04}


 34%|███▍      | 3662/10740 [18:21:37<37:37:34, 19.14s/it]

 34%|███▍      | 3663/10740 [18:21:57<37:59:04, 19.32s/it]

 34%|███▍      | 3664/10740 [18:22:09<33:45:58, 17.18s/it]
{'loss': 0.2553, 'learning_rate': 1.5339003621932826e-06, 'rewards/chosen': -2.279968738555908, 'rewards/rejected': -5.616147518157959, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3361783027648926, 'policy_logps/rejected': -391.31158447265625, 'policy_logps/chosen': -394.6027526855469, 'referece_logps/rejected': -335.150146484375, 'referece_logps/chosen': -371.8030700683594, 'logits/rejected': 0.35776224732398987, 'logits/chosen': 0.24015271663665771, 'epoch': 2.05}

 34%|███▍      | 3665/10740 [18:22:31<36:05:38, 18.37s/it]

 34%|███▍      | 3666/10740 [18:22:44<33:30:12, 17.05s/it]


 34%|███▍      | 3668/10740 [18:23:16<31:37:37, 16.10s/it]
{'loss': 0.1912, 'learning_rate': 1.5328799622543874e-06, 'rewards/chosen': -3.255554676055908, 'rewards/rejected': -6.817296981811523, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5617423057556152, 'policy_logps/rejected': -655.6788940429688, 'policy_logps/chosen': -431.46710205078125, 'referece_logps/rejected': -587.5059814453125, 'referece_logps/chosen': -398.91156005859375, 'logits/rejected': -0.3971906304359436, 'logits/chosen': -0.24305269122123718, 'epoch': 2.05}

 34%|███▍      | 3669/10740 [18:23:32<31:43:05, 16.15s/it]

 34%|███▍      | 3670/10740 [18:23:48<31:54:04, 16.24s/it]

 34%|███▍      | 3671/10740 [18:24:07<33:10:12, 16.89s/it]

 34%|███▍      | 3672/10740 [18:24:27<34:51:47, 17.76s/it]


 34%|███▍      | 3674/10740 [18:25:06<36:55:03, 18.81s/it]
{'loss': 0.2977, 'learning_rate': 1.5313479088083754e-06, 'rewards/chosen': -3.820819139480591, 'rewards/rejected': -5.859065532684326, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0382466316223145, 'policy_logps/rejected': -267.8525085449219, 'policy_logps/chosen': -332.92535400390625, 'referece_logps/rejected': -209.2618408203125, 'referece_logps/chosen': -294.7171936035156, 'logits/rejected': -0.7390364408493042, 'logits/chosen': -0.6988347768783569, 'epoch': 2.05}


 34%|███▍      | 3676/10740 [18:25:38<34:21:12, 17.51s/it]
{'loss': 0.2459, 'learning_rate': 1.5308368374594238e-06, 'rewards/chosen': -3.149238348007202, 'rewards/rejected': -6.356567859649658, 'rewards/accuracies': 1.0, 'rewards/margins': 3.207329511642456, 'policy_logps/rejected': -419.1405944824219, 'policy_logps/chosen': -411.57122802734375, 'referece_logps/rejected': -355.5749206542969, 'referece_logps/chosen': -380.07879638671875, 'logits/rejected': -1.3704382181167603, 'logits/chosen': -1.4040447473526, 'epoch': 2.05}

 34%|███▍      | 3677/10740 [18:25:58<36:02:31, 18.37s/it]

 34%|███▍      | 3678/10740 [18:26:11<33:00:05, 16.82s/it]

 34%|███▍      | 3679/10740 [18:26:26<31:43:14, 16.17s/it]

 34%|███▍      | 3680/10740 [18:26:47<34:19:53, 17.51s/it]

 34%|███▍      | 3681/10740 [18:27:03<33:32:51, 17.11s/it]


 34%|███▍      | 3683/10740 [18:27:34<32:16:43, 16.47s/it]
{'loss': 0.3219, 'learning_rate': 1.5290465681102113e-06, 'rewards/chosen': -3.184401512145996, 'rewards/rejected': -7.74065637588501, 'rewards/accuracies': 0.75, 'rewards/margins': 4.5562543869018555, 'policy_logps/rejected': -401.7845458984375, 'policy_logps/chosen': -310.5039978027344, 'referece_logps/rejected': -324.37799072265625, 'referece_logps/chosen': -278.6600036621094, 'logits/rejected': -0.8497661352157593, 'logits/chosen': -0.8458762168884277, 'epoch': 2.06}

 34%|███▍      | 3684/10740 [18:27:53<33:59:10, 17.34s/it]

 34%|███▍      | 3685/10740 [18:28:13<35:21:54, 18.05s/it]

 34%|███▍      | 3686/10740 [18:28:33<36:36:22, 18.68s/it]

 34%|███▍      | 3687/10740 [18:28:53<37:27:57, 19.12s/it]

 34%|███▍      | 3688/10740 [18:29:15<38:47:16, 19.80s/it]

 34%|███▍      | 3689/10740 [18:29:36<40:01:21, 20.43s/it]


 34%|███▍      | 3691/10740 [18:30:10<36:45:08, 18.77s/it]

 34%|███▍      | 3692/10740 [18:30:26<35:15:11, 18.01s/it]
{'loss': 0.3004, 'learning_rate': 1.526741329849703e-06, 'rewards/chosen': -3.9534709453582764, 'rewards/rejected': -6.73848819732666, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7850170135498047, 'policy_logps/rejected': -367.1846008300781, 'policy_logps/chosen': -341.0984191894531, 'referece_logps/rejected': -299.7996826171875, 'referece_logps/chosen': -301.563720703125, 'logits/rejected': 0.1444496512413025, 'logits/chosen': -0.049232400953769684, 'epoch': 2.06}


 34%|███▍      | 3694/10740 [18:30:58<33:42:33, 17.22s/it]
{'loss': 0.1693, 'learning_rate': 1.5262285270899483e-06, 'rewards/chosen': -3.5189743041992188, 'rewards/rejected': -6.770652770996094, 'rewards/accuracies': 1.0, 'rewards/margins': 3.251677989959717, 'policy_logps/rejected': -377.5418701171875, 'policy_logps/chosen': -373.1922302246094, 'referece_logps/rejected': -309.8353271484375, 'referece_logps/chosen': -338.00250244140625, 'logits/rejected': -0.20828041434288025, 'logits/chosen': -0.21294724941253662, 'epoch': 2.06}


 34%|███▍      | 3696/10740 [18:31:32<33:43:18, 17.23s/it]
{'loss': 0.1746, 'learning_rate': 1.5257155328830976e-06, 'rewards/chosen': -2.1828668117523193, 'rewards/rejected': -5.814634323120117, 'rewards/accuracies': 0.75, 'rewards/margins': 3.631767511367798, 'policy_logps/rejected': -460.20953369140625, 'policy_logps/chosen': -327.012451171875, 'referece_logps/rejected': -402.06317138671875, 'referece_logps/chosen': -305.18377685546875, 'logits/rejected': 0.3011448383331299, 'logits/chosen': 0.28888481855392456, 'epoch': 2.06}

 34%|███▍      | 3697/10740 [18:31:47<32:46:15, 16.75s/it]

 34%|███▍      | 3698/10740 [18:32:08<34:50:48, 17.81s/it]


 34%|███▍      | 3700/10740 [18:32:44<35:11:26, 18.00s/it]
{'loss': 0.2005, 'learning_rate': 1.5246889708747076e-06, 'rewards/chosen': -3.819465160369873, 'rewards/rejected': -6.1593017578125, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3398361206054688, 'policy_logps/rejected': -345.15032958984375, 'policy_logps/chosen': -359.98248291015625, 'referece_logps/rejected': -283.55731201171875, 'referece_logps/chosen': -321.7877502441406, 'logits/rejected': -0.6171562671661377, 'logits/chosen': -0.5733562111854553, 'epoch': 2.07}

 34%|███▍      | 3701/10740 [18:33:06<37:15:17, 19.05s/it]

 34%|███▍      | 3702/10740 [18:33:23<36:15:45, 18.55s/it]

 34%|███▍      | 3703/10740 [18:33:43<37:18:42, 19.09s/it]

 34%|███▍      | 3704/10740 [18:34:06<39:13:41, 20.07s/it]

 34%|███▍      | 3705/10740 [18:34:24<38:22:33, 19.64s/it]


 35%|███▍      | 3707/10740 [18:34:58<36:28:50, 18.67s/it]
{'loss': 0.2303, 'learning_rate': 1.522890650972207e-06, 'rewards/chosen': -3.1137850284576416, 'rewards/rejected': -6.680465221405029, 'rewards/accuracies': 1.0, 'rewards/margins': 3.566680431365967, 'policy_logps/rejected': -344.872314453125, 'policy_logps/chosen': -433.29412841796875, 'referece_logps/rejected': -278.067626953125, 'referece_logps/chosen': -402.15625, 'logits/rejected': -0.4136224091053009, 'logits/chosen': -0.36771196126937866, 'epoch': 2.07}

 35%|███▍      | 3708/10740 [18:35:16<35:39:25, 18.25s/it]

 35%|███▍      | 3709/10740 [18:35:35<36:29:04, 18.68s/it]


 35%|███▍      | 3711/10740 [18:36:12<36:22:27, 18.63s/it]
{'loss': 0.2294, 'learning_rate': 1.5218619928051361e-06, 'rewards/chosen': -3.5629630088806152, 'rewards/rejected': -7.906607151031494, 'rewards/accuracies': 0.875, 'rewards/margins': 4.343644142150879, 'policy_logps/rejected': -319.07452392578125, 'policy_logps/chosen': -353.83807373046875, 'referece_logps/rejected': -240.00845336914062, 'referece_logps/chosen': -318.20843505859375, 'logits/rejected': -0.6851193904876709, 'logits/chosen': -0.4470090866088867, 'epoch': 2.07}

 35%|███▍      | 3712/10740 [18:36:26<33:10:17, 16.99s/it]

 35%|███▍      | 3713/10740 [18:36:39<30:52:23, 15.82s/it]

 35%|███▍      | 3714/10740 [18:36:57<32:19:39, 16.56s/it]

 35%|███▍      | 3715/10740 [18:37:08<28:53:05, 14.80s/it]

 35%|███▍      | 3716/10740 [18:37:27<31:40:15, 16.23s/it]

 35%|███▍      | 3717/10740 [18:37:47<33:44:12, 17.29s/it]

 35%|███▍      | 3718/10740 [18:38:07<35:37:01, 18.26s/it]

 35%|███▍      | 3719/10740 [18:38:27<36:27:36, 18.69s/it]

 35%|███▍      | 3720/10740 [18:38:44<35:15:49, 18.08s/it]

 35%|███▍      | 3721/10740 [18:39:07<38:24:33, 19.70s/it]


 35%|███▍      | 3723/10740 [18:39:32<31:12:46, 16.01s/it]
{'loss': 0.3277, 'learning_rate': 1.5187714676934438e-06, 'rewards/chosen': -3.0047967433929443, 'rewards/rejected': -6.933352947235107, 'rewards/accuracies': 0.875, 'rewards/margins': 3.928556442260742, 'policy_logps/rejected': -277.65521240234375, 'policy_logps/chosen': -566.1522216796875, 'referece_logps/rejected': -208.3217010498047, 'referece_logps/chosen': -536.104248046875, 'logits/rejected': -0.3531992435455322, 'logits/chosen': -0.2043711543083191, 'epoch': 2.08}

 35%|███▍      | 3724/10740 [18:39:52<33:15:13, 17.06s/it]


 35%|███▍      | 3726/10740 [18:40:31<35:59:06, 18.47s/it]

 35%|███▍      | 3727/10740 [18:40:49<35:32:37, 18.25s/it]

 35%|███▍      | 3728/10740 [18:41:08<36:30:34, 18.74s/it]

 35%|███▍      | 3729/10740 [18:41:29<37:17:54, 19.15s/it]
{'loss': 0.1752, 'learning_rate': 1.5172236546960513e-06, 'rewards/chosen': -2.988562822341919, 'rewards/rejected': -5.1804351806640625, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1918723583221436, 'policy_logps/rejected': -350.4127502441406, 'policy_logps/chosen': -453.61395263671875, 'referece_logps/rejected': -298.6083984375, 'referece_logps/chosen': -423.7283020019531, 'logits/rejected': -0.28999847173690796, 'logits/chosen': -0.2087511271238327, 'epoch': 2.08}

 35%|███▍      | 3730/10740 [18:41:46<36:03:57, 18.52s/it]

 35%|███▍      | 3731/10740 [18:42:01<34:01:30, 17.48s/it]

 35%|███▍      | 3732/10740 [18:42:14<31:48:53, 16.34s/it]

 35%|███▍      | 3733/10740 [18:42:32<32:44:26, 16.82s/it]

 35%|███▍      | 3734/10740 [18:42:52<34:25:36, 17.69s/it]

 35%|███▍      | 3735/10740 [18:43:12<35:59:04, 18.49s/it]

 35%|███▍      | 3736/10740 [18:43:31<35:58:06, 18.49s/it]

 35%|███▍      | 3737/10740 [18:43:52<37:23:39, 19.22s/it]

 35%|███▍      | 3738/10740 [18:44:06<34:20:47, 17.66s/it]

 35%|███▍      | 3739/10740 [18:44:19<31:51:26, 16.38s/it]

 35%|███▍      | 3740/10740 [18:44:40<34:27:14, 17.72s/it]

 35%|███▍      | 3741/10740 [18:45:00<35:32:01, 18.28s/it]

 35%|███▍      | 3742/10740 [18:45:17<35:03:46, 18.04s/it]

 35%|███▍      | 3743/10740 [18:45:37<36:08:01, 18.59s/it]

 35%|███▍      | 3744/10740 [18:45:56<36:32:47, 18.81s/it]

 35%|███▍      | 3745/10740 [18:46:16<37:02:35, 19.06s/it]

 35%|███▍      | 3746/10740 [18:46:36<37:28:36, 19.29s/it]

 35%|███▍      | 3747/10740 [18:46:52<35:50:03, 18.45s/it]

 35%|███▍      | 3748/10740 [18:47:16<38:41:39, 19.92s/it]

 35%|███▍      | 3749/10740 [18:47:34<37:32:30, 19.33s/it]

 35%|███▍      | 3750/10740 [18:47:54<37:57:14, 19.55s/it]

 35%|███▍      | 3751/10740 [18:48:11<36:41:39, 18.90s/it]

 35%|███▍      | 3752/10740 [18:48:31<37:07:27, 19.13s/it]

 35%|███▍      | 3753/10740 [18:48:51<37:54:20, 19.53s/it]

 35%|███▍      | 3754/10740 [18:49:12<38:41:30, 19.94s/it]

 35%|███▍      | 3755/10740 [18:49:28<36:05:57, 18.61s/it]

 35%|███▍      | 3756/10740 [18:49:47<36:40:41, 18.91s/it]

 35%|███▍      | 3757/10740 [18:50:07<37:07:19, 19.14s/it]

 35%|███▍      | 3758/10740 [18:50:28<38:24:43, 19.81s/it]

 35%|███▌      | 3759/10740 [18:50:45<36:34:51, 18.86s/it]

 35%|███▌      | 3760/10740 [18:51:04<36:29:05, 18.82s/it]

 35%|███▌      | 3761/10740 [18:51:18<33:58:34, 17.53s/it]

 35%|███▌      | 3762/10740 [18:51:35<33:45:17, 17.41s/it]

 35%|███▌      | 3763/10740 [18:51:52<33:39:59, 17.37s/it]

 35%|███▌      | 3764/10740 [18:52:08<32:51:33, 16.96s/it]

 35%|███▌      | 3765/10740 [18:52:29<34:40:53, 17.90s/it]

 35%|███▌      | 3766/10740 [18:52:48<35:37:47, 18.39s/it]

 35%|███▌      | 3767/10740 [18:53:08<36:45:57, 18.98s/it]

 35%|███▌      | 3768/10740 [18:53:27<36:36:29, 18.90s/it]

 35%|███▌      | 3769/10740 [18:53:39<32:23:43, 16.73s/it]

 35%|███▌      | 3770/10740 [18:53:50<28:53:09, 14.92s/it]

 35%|███▌      | 3771/10740 [18:54:00<26:29:51, 13.69s/it]

 35%|███▌      | 3772/10740 [18:54:12<25:18:14, 13.07s/it]

 35%|███▌      | 3773/10740 [18:54:28<27:03:32, 13.98s/it]

 35%|███▌      | 3774/10740 [18:54:48<30:36:16, 15.82s/it]

 35%|███▌      | 3775/10740 [18:55:02<29:43:24, 15.36s/it]

 35%|███▌      | 3776/10740 [18:55:21<31:16:44, 16.17s/it]

 35%|███▌      | 3777/10740 [18:55:40<33:22:41, 17.26s/it]

 35%|███▌      | 3778/10740 [18:55:56<32:22:48, 16.74s/it]

 35%|███▌      | 3779/10740 [18:56:09<30:15:00, 15.64s/it]

 35%|███▌      | 3780/10740 [18:56:27<31:25:11, 16.25s/it]

 35%|███▌      | 3781/10740 [18:56:44<31:52:15, 16.49s/it]

 35%|███▌      | 3782/10740 [18:56:55<29:02:00, 15.02s/it]

 35%|███▌      | 3783/10740 [18:57:08<27:32:53, 14.26s/it]

 35%|███▌      | 3784/10740 [18:57:29<31:29:27, 16.30s/it]

 35%|███▌      | 3785/10740 [18:57:40<28:14:20, 14.62s/it]

 35%|███▌      | 3786/10740 [18:57:51<26:40:30, 13.81s/it]

 35%|███▌      | 3787/10740 [18:58:13<30:58:45, 16.04s/it]

 35%|███▌      | 3788/10740 [18:58:26<29:11:28, 15.12s/it]

 35%|███▌      | 3789/10740 [18:58:45<31:38:15, 16.39s/it]

 35%|███▌      | 3790/10740 [18:58:56<28:40:52, 14.86s/it]

 35%|███▌      | 3791/10740 [18:59:12<29:00:29, 15.03s/it]

 35%|███▌      | 3792/10740 [18:59:27<29:25:16, 15.24s/it]

 35%|███▌      | 3793/10740 [18:59:44<30:11:34, 15.65s/it]

 35%|███▌      | 3794/10740 [18:59:58<29:17:27, 15.18s/it]

 35%|███▌      | 3795/10740 [19:00:18<32:06:20, 16.64s/it]

 35%|███▌      | 3796/10740 [19:00:35<32:23:56, 16.80s/it]

 35%|███▌      | 3797/10740 [19:00:55<33:58:18, 17.61s/it]

 35%|███▌      | 3798/10740 [19:01:13<34:30:48, 17.90s/it]

 35%|███▌      | 3799/10740 [19:01:33<35:37:55, 18.48s/it]

 35%|███▌      | 3800/10740 [19:01:47<32:57:12, 17.09s/it]

 35%|███▌      | 3801/10740 [19:02:06<34:00:19, 17.64s/it]

 35%|███▌      | 3802/10740 [19:02:20<31:45:48, 16.48s/it]

 35%|███▌      | 3803/10740 [19:02:32<29:32:11, 15.33s/it]

 35%|███▌      | 3804/10740 [19:02:53<32:24:50, 16.82s/it]

 35%|███▌      | 3805/10740 [19:03:11<33:14:33, 17.26s/it]


 35%|███▌      | 3807/10740 [19:03:47<33:51:15, 17.58s/it]

 35%|███▌      | 3808/10740 [19:04:08<36:02:34, 18.72s/it]

 35%|███▌      | 3809/10740 [19:04:28<36:41:34, 19.06s/it]

 35%|███▌      | 3810/10740 [19:04:47<37:03:48, 19.25s/it]

 35%|███▌      | 3811/10740 [19:05:04<35:17:40, 18.34s/it]

 35%|███▌      | 3812/10740 [19:05:19<33:17:55, 17.30s/it]

 36%|███▌      | 3813/10740 [19:05:39<34:48:10, 18.09s/it]

 36%|███▌      | 3814/10740 [19:05:55<34:07:20, 17.74s/it]

 36%|███▌      | 3815/10740 [19:06:13<34:18:08, 17.83s/it]

 36%|███▌      | 3816/10740 [19:06:34<35:51:22, 18.64s/it]

 36%|███▌      | 3817/10740 [19:06:54<36:44:27, 19.11s/it]

 36%|███▌      | 3818/10740 [19:07:17<38:37:12, 20.09s/it]

 36%|███▌      | 3819/10740 [19:07:33<36:16:54, 18.87s/it]

 36%|███▌      | 3820/10740 [19:07:53<37:16:28, 19.39s/it]

 36%|███▌      | 3821/10740 [19:08:11<36:20:29, 18.91s/it]

 36%|███▌      | 3822/10740 [19:08:31<36:50:33, 19.17s/it]

 36%|███▌      | 3823/10740 [19:08:52<37:59:00, 19.77s/it]

 36%|███▌      | 3824/10740 [19:09:08<35:45:52, 18.62s/it]

 36%|███▌      | 3825/10740 [19:09:25<34:54:45, 18.18s/it]

 36%|███▌      | 3826/10740 [19:09:47<37:02:46, 19.29s/it]

 36%|███▌      | 3827/10740 [19:09:58<32:35:10, 16.97s/it]
{'loss': 0.1547, 'learning_rate': 1.4917066641793153e-06, 'rewards/chosen': -2.340254783630371, 'rewards/rejected': -6.531021595001221, 'rewards/accuracies': 1.0, 'rewards/margins': 4.190766334533691, 'policy_logps/rejected': -422.23907470703125, 'policy_logps/chosen': -374.7984924316406, 'referece_logps/rejected': -356.92889404296875, 'referece_logps/chosen': -351.39593505859375, 'logits/rejected': 0.4089715778827667, 'logits/chosen': 0.3997518718242645, 'epoch': 2.14}


 36%|███▌      | 3829/10740 [19:10:33<33:07:47, 17.26s/it]

 36%|███▌      | 3830/10740 [19:10:55<35:57:09, 18.73s/it]

 36%|███▌      | 3831/10740 [19:11:13<35:45:16, 18.63s/it]

 36%|███▌      | 3832/10740 [19:11:34<37:11:05, 19.38s/it]

 36%|███▌      | 3833/10740 [19:11:56<38:48:06, 20.22s/it]

 36%|███▌      | 3834/10740 [19:12:16<38:12:23, 19.92s/it]

 36%|███▌      | 3835/10740 [19:12:32<35:57:37, 18.75s/it]
{'loss': 0.2661, 'learning_rate': 1.4896043799243347e-06, 'rewards/chosen': -3.399346351623535, 'rewards/rejected': -6.541467666625977, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1421217918395996, 'policy_logps/rejected': -486.0972900390625, 'policy_logps/chosen': -432.81817626953125, 'referece_logps/rejected': -420.6826171875, 'referece_logps/chosen': -398.82470703125, 'logits/rejected': 0.028875499963760376, 'logits/chosen': 0.08792656660079956, 'epoch': 2.14}


 36%|███▌      | 3837/10740 [19:13:15<38:37:10, 20.14s/it]

 36%|███▌      | 3838/10740 [19:13:34<38:25:56, 20.05s/it]
{'loss': 0.2379, 'learning_rate': 1.4888152879143965e-06, 'rewards/chosen': -2.505108118057251, 'rewards/rejected': -5.728888511657715, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2237801551818848, 'policy_logps/rejected': -255.57395935058594, 'policy_logps/chosen': -348.9881896972656, 'referece_logps/rejected': -198.2850799560547, 'referece_logps/chosen': -323.9371032714844, 'logits/rejected': -0.5351533889770508, 'logits/chosen': -0.572835385799408, 'epoch': 2.14}


 36%|███▌      | 3840/10740 [19:14:14<37:56:14, 19.79s/it]

 36%|███▌      | 3841/10740 [19:14:32<37:04:22, 19.35s/it]
{'loss': 0.3006, 'learning_rate': 1.4880257957739398e-06, 'rewards/chosen': -3.049948215484619, 'rewards/rejected': -6.7580108642578125, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7080626487731934, 'policy_logps/rejected': -384.3440856933594, 'policy_logps/chosen': -284.367431640625, 'referece_logps/rejected': -316.76397705078125, 'referece_logps/chosen': -253.8679656982422, 'logits/rejected': -0.7157633304595947, 'logits/chosen': -0.7213935256004333, 'epoch': 2.15}


 36%|███▌      | 3843/10740 [19:15:09<35:58:15, 18.78s/it]
{'loss': 0.2665, 'learning_rate': 1.4874992457046944e-06, 'rewards/chosen': -2.504160165786743, 'rewards/rejected': -6.330256462097168, 'rewards/accuracies': 0.875, 'rewards/margins': 3.826096296310425, 'policy_logps/rejected': -367.0694580078125, 'policy_logps/chosen': -379.0512390136719, 'referece_logps/rejected': -303.7668762207031, 'referece_logps/chosen': -354.0096435546875, 'logits/rejected': -0.8023533821105957, 'logits/chosen': -0.8630669713020325, 'epoch': 2.15}

 36%|███▌      | 3844/10740 [19:15:26<34:42:26, 18.12s/it]

 36%|███▌      | 3845/10740 [19:15:37<30:56:38, 16.16s/it]


 36%|███▌      | 3847/10740 [19:16:10<31:39:25, 16.53s/it]

 36%|███▌      | 3848/10740 [19:16:30<33:30:01, 17.50s/it]

 36%|███▌      | 3849/10740 [19:16:50<34:58:13, 18.27s/it]
{'loss': 0.1979, 'learning_rate': 1.4859185321215155e-06, 'rewards/chosen': -2.1969332695007324, 'rewards/rejected': -3.854188919067383, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6572556495666504, 'policy_logps/rejected': -314.4827880859375, 'policy_logps/chosen': -387.14117431640625, 'referece_logps/rejected': -275.94091796875, 'referece_logps/chosen': -365.1718444824219, 'logits/rejected': -0.16001081466674805, 'logits/chosen': -0.06153295189142227, 'epoch': 2.15}

 36%|███▌      | 3850/10740 [19:17:11<36:44:04, 19.19s/it]


 36%|███▌      | 3852/10740 [19:17:48<35:29:39, 18.55s/it]

 36%|███▌      | 3853/10740 [19:18:05<34:48:57, 18.20s/it]
{'loss': 0.1929, 'learning_rate': 1.4848638388369508e-06, 'rewards/chosen': -2.9914562702178955, 'rewards/rejected': -4.92055606842041, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9290995597839355, 'policy_logps/rejected': -315.6216735839844, 'policy_logps/chosen': -333.2886657714844, 'referece_logps/rejected': -266.4161071777344, 'referece_logps/chosen': -303.3740539550781, 'logits/rejected': -1.1374837160110474, 'logits/chosen': -1.4664710760116577, 'epoch': 2.15}


 36%|███▌      | 3855/10740 [19:18:40<33:49:19, 17.68s/it]
{'loss': 0.2259, 'learning_rate': 1.4843362275014002e-06, 'rewards/chosen': -3.589132070541382, 'rewards/rejected': -6.463891506195068, 'rewards/accuracies': 1.0, 'rewards/margins': 2.87475848197937, 'policy_logps/rejected': -373.5245361328125, 'policy_logps/chosen': -448.8084411621094, 'referece_logps/rejected': -308.8856201171875, 'referece_logps/chosen': -412.9170837402344, 'logits/rejected': 0.09295123815536499, 'logits/chosen': -0.01562674343585968, 'epoch': 2.15}


 36%|███▌      | 3857/10740 [19:19:15<33:13:47, 17.38s/it]
{'loss': 0.3177, 'learning_rate': 1.4838084399595828e-06, 'rewards/chosen': -3.2500877380371094, 'rewards/rejected': -5.1706953048706055, 'rewards/accuracies': 1.0, 'rewards/margins': 1.920607328414917, 'policy_logps/rejected': -394.4375, 'policy_logps/chosen': -477.6318054199219, 'referece_logps/rejected': -342.7305603027344, 'referece_logps/chosen': -445.13092041015625, 'logits/rejected': 0.3957914412021637, 'logits/chosen': 0.3413783311843872, 'epoch': 2.15}


 36%|███▌      | 3859/10740 [19:19:55<36:07:19, 18.90s/it]
{'loss': 0.3086, 'learning_rate': 1.4832804764035127e-06, 'rewards/chosen': -3.6750025749206543, 'rewards/rejected': -7.347119331359863, 'rewards/accuracies': 0.875, 'rewards/margins': 3.672116279602051, 'policy_logps/rejected': -489.5873107910156, 'policy_logps/chosen': -360.0175476074219, 'referece_logps/rejected': -416.1161193847656, 'referece_logps/chosen': -323.26751708984375, 'logits/rejected': -0.4394981265068054, 'logits/chosen': -0.25352779030799866, 'epoch': 2.16}


 36%|███▌      | 3861/10740 [19:20:27<32:52:08, 17.20s/it]

 36%|███▌      | 3862/10740 [19:20:50<36:06:35, 18.90s/it]

 36%|███▌      | 3863/10740 [19:21:11<37:21:40, 19.56s/it]
[2024-04-02 14:34:29,343] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 3864/10740 [19:21:29<36:35:06, 19.15s/it]

 36%|███▌      | 3865/10740 [19:21:49<37:00:56, 19.38s/it]

 36%|███▌      | 3866/10740 [19:22:09<37:08:58, 19.46s/it]
{'loss': 0.2806, 'learning_rate': 1.4814312206187375e-06, 'rewards/chosen': -4.8908371925354, 'rewards/rejected': -7.580070495605469, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6892335414886475, 'policy_logps/rejected': -444.6264343261719, 'policy_logps/chosen': -425.17327880859375, 'referece_logps/rejected': -368.82574462890625, 'referece_logps/chosen': -376.2649230957031, 'logits/rejected': -0.6785162687301636, 'logits/chosen': -0.5551878809928894, 'epoch': 2.16}

 36%|███▌      | 3867/10740 [19:22:26<35:34:28, 18.63s/it]


 36%|███▌      | 3869/10740 [19:22:59<33:55:19, 17.77s/it]

 36%|███▌      | 3870/10740 [19:23:17<34:25:58, 18.04s/it]

 36%|███▌      | 3871/10740 [19:23:34<33:53:44, 17.76s/it]

 36%|███▌      | 3872/10740 [19:23:55<35:31:25, 18.62s/it]
{'loss': 0.26, 'learning_rate': 1.479844436213132e-06, 'rewards/chosen': -3.0341265201568604, 'rewards/rejected': -4.79241418838501, 'rewards/accuracies': 1.0, 'rewards/margins': 1.758288025856018, 'policy_logps/rejected': -254.04051208496094, 'policy_logps/chosen': -181.5943603515625, 'referece_logps/rejected': -206.11634826660156, 'referece_logps/chosen': -151.25308227539062, 'logits/rejected': -0.047831133008003235, 'logits/chosen': -0.18051669001579285, 'epoch': 2.16}


 36%|███▌      | 3874/10740 [19:24:24<31:56:19, 16.75s/it]
{'loss': 0.2231, 'learning_rate': 1.4793151586770755e-06, 'rewards/chosen': -3.017503023147583, 'rewards/rejected': -5.900404930114746, 'rewards/accuracies': 0.875, 'rewards/margins': 2.882902145385742, 'policy_logps/rejected': -351.96563720703125, 'policy_logps/chosen': -474.3354797363281, 'referece_logps/rejected': -292.96160888671875, 'referece_logps/chosen': -444.16046142578125, 'logits/rejected': 0.3416268825531006, 'logits/chosen': 0.3525417745113373, 'epoch': 2.16}


 36%|███▌      | 3876/10740 [19:24:53<29:01:27, 15.22s/it]

 36%|███▌      | 3877/10740 [19:25:12<30:57:44, 16.24s/it]
{'loss': 0.3724, 'learning_rate': 1.4785209154715215e-06, 'rewards/chosen': -3.380096435546875, 'rewards/rejected': -5.049763202667236, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6696664094924927, 'policy_logps/rejected': -355.7443542480469, 'policy_logps/chosen': -298.51849365234375, 'referece_logps/rejected': -305.2467041015625, 'referece_logps/chosen': -264.7174987792969, 'logits/rejected': -0.3597196042537689, 'logits/chosen': -0.29991716146469116, 'epoch': 2.17}

 36%|███▌      | 3878/10740 [19:25:32<33:21:13, 17.50s/it]


 36%|███▌      | 3880/10740 [19:26:06<33:39:45, 17.67s/it]

 36%|███▌      | 3881/10740 [19:26:27<35:17:01, 18.52s/it]

 36%|███▌      | 3882/10740 [19:26:47<36:05:37, 18.95s/it]

 36%|███▌      | 3883/10740 [19:26:58<31:50:11, 16.71s/it]

 36%|███▌      | 3884/10740 [19:27:17<32:57:35, 17.31s/it]

 36%|███▌      | 3885/10740 [19:27:33<32:09:13, 16.89s/it]

 36%|███▌      | 3886/10740 [19:27:51<32:56:51, 17.31s/it]

 36%|███▌      | 3887/10740 [19:28:11<34:21:25, 18.05s/it]
{'loss': 0.212, 'learning_rate': 1.4758706128019952e-06, 'rewards/chosen': -3.7595949172973633, 'rewards/rejected': -7.450019359588623, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6904244422912598, 'policy_logps/rejected': -377.04119873046875, 'policy_logps/chosen': -444.414794921875, 'referece_logps/rejected': -302.5409851074219, 'referece_logps/chosen': -406.8188171386719, 'logits/rejected': -0.3859095573425293, 'logits/chosen': -0.48826682567596436, 'epoch': 2.17}


 36%|███▌      | 3889/10740 [19:28:45<32:38:47, 17.15s/it]

 36%|███▌      | 3890/10740 [19:29:00<31:07:31, 16.36s/it]

 36%|███▌      | 3891/10740 [19:29:22<34:20:51, 18.05s/it]

 36%|███▌      | 3892/10740 [19:29:33<30:43:39, 16.15s/it]

 36%|███▌      | 3893/10740 [19:29:47<29:27:44, 15.49s/it]

 36%|███▋      | 3894/10740 [19:30:08<32:15:12, 16.96s/it]

 36%|███▋      | 3895/10740 [19:30:29<34:37:05, 18.21s/it]

 36%|███▋      | 3896/10740 [19:30:47<34:51:49, 18.34s/it]

 36%|███▋      | 3897/10740 [19:31:07<35:31:26, 18.69s/it]

 36%|███▋      | 3898/10740 [19:31:27<36:15:08, 19.07s/it]

 36%|███▋      | 3899/10740 [19:31:47<36:55:57, 19.44s/it]
[2024-04-02 14:45:05,469] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▋      | 3900/10740 [19:32:08<37:36:01, 19.79s/it]
{'loss': 0.2382, 'learning_rate': 1.472418752345952e-06, 'rewards/chosen': -3.666640520095825, 'rewards/rejected': -6.460720062255859, 'rewards/accuracies': 0.875, 'rewards/margins': 2.794079303741455, 'policy_logps/rejected': -395.3913879394531, 'policy_logps/chosen': -476.6575622558594, 'referece_logps/rejected': -330.7842102050781, 'referece_logps/chosen': -439.99114990234375, 'logits/rejected': -1.0766849517822266, 'logits/chosen': -1.2016690969467163, 'epoch': 2.18}


 36%|███▋      | 3902/10740 [19:32:45<36:46:24, 19.36s/it]

 36%|███▋      | 3903/10740 [19:33:01<35:02:04, 18.45s/it]
{'loss': 0.1986, 'learning_rate': 1.4716211360073413e-06, 'rewards/chosen': -2.9712584018707275, 'rewards/rejected': -5.438267230987549, 'rewards/accuracies': 1.0, 'rewards/margins': 2.467008590698242, 'policy_logps/rejected': -345.4323425292969, 'policy_logps/chosen': -217.9445037841797, 'referece_logps/rejected': -291.0496826171875, 'referece_logps/chosen': -188.23191833496094, 'logits/rejected': -0.01653870940208435, 'logits/chosen': 0.06558048725128174, 'epoch': 2.18}


 36%|███▋      | 3905/10740 [19:33:42<36:53:46, 19.43s/it]
{'loss': 0.276, 'learning_rate': 1.4710891772658032e-06, 'rewards/chosen': -2.526310682296753, 'rewards/rejected': -5.128390312194824, 'rewards/accuracies': 0.75, 'rewards/margins': 2.6020796298980713, 'policy_logps/rejected': -376.26324462890625, 'policy_logps/chosen': -293.7188720703125, 'referece_logps/rejected': -324.97930908203125, 'referece_logps/chosen': -268.4557800292969, 'logits/rejected': -0.42018574476242065, 'logits/chosen': -0.23235201835632324, 'epoch': 2.18}

 36%|███▋      | 3906/10740 [19:34:03<37:41:30, 19.86s/it]


 36%|███▋      | 3908/10740 [19:34:36<34:36:33, 18.24s/it]

 36%|███▋      | 3909/10740 [19:34:51<33:05:00, 17.44s/it]

 36%|███▋      | 3910/10740 [19:35:09<33:19:40, 17.57s/it]
{'loss': 0.2403, 'learning_rate': 1.4697585310179492e-06, 'rewards/chosen': -2.266583204269409, 'rewards/rejected': -6.437584400177002, 'rewards/accuracies': 1.0, 'rewards/margins': 4.171001434326172, 'policy_logps/rejected': -333.9407653808594, 'policy_logps/chosen': -257.65386962890625, 'referece_logps/rejected': -269.56494140625, 'referece_logps/chosen': -234.98802185058594, 'logits/rejected': -0.6449358463287354, 'logits/chosen': -0.6990270018577576, 'epoch': 2.18}


 36%|███▋      | 3912/10740 [19:35:40<30:21:33, 16.01s/it]

 36%|███▋      | 3913/10740 [19:36:00<32:50:37, 17.32s/it]

 36%|███▋      | 3914/10740 [19:36:18<33:06:15, 17.46s/it]

 36%|███▋      | 3915/10740 [19:36:30<30:03:08, 15.85s/it]

 36%|███▋      | 3916/10740 [19:36:45<29:50:41, 15.74s/it]

 36%|███▋      | 3917/10740 [19:37:05<32:07:00, 16.95s/it]

 36%|███▋      | 3918/10740 [19:37:20<30:48:30, 16.26s/it]

 36%|███▋      | 3919/10740 [19:37:36<30:40:03, 16.19s/it]
{'loss': 0.2577, 'learning_rate': 1.4673606780885842e-06, 'rewards/chosen': -2.7205684185028076, 'rewards/rejected': -7.679575443267822, 'rewards/accuracies': 1.0, 'rewards/margins': 4.959007263183594, 'policy_logps/rejected': -417.52203369140625, 'policy_logps/chosen': -410.5847473144531, 'referece_logps/rejected': -340.726318359375, 'referece_logps/chosen': -383.37908935546875, 'logits/rejected': -0.28479743003845215, 'logits/chosen': -0.15944816172122955, 'epoch': 2.19}


 37%|███▋      | 3921/10740 [19:38:11<32:39:27, 17.24s/it]
{'loss': 0.2456, 'learning_rate': 1.4668273536762848e-06, 'rewards/chosen': -4.186990261077881, 'rewards/rejected': -7.3745880126953125, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1875967979431152, 'policy_logps/rejected': -416.9206237792969, 'policy_logps/chosen': -508.13787841796875, 'referece_logps/rejected': -343.1747131347656, 'referece_logps/chosen': -466.26800537109375, 'logits/rejected': -0.4537144601345062, 'logits/chosen': -0.5022936463356018, 'epoch': 2.19}

 37%|███▋      | 3922/10740 [19:38:25<30:29:45, 16.10s/it]

 37%|███▋      | 3923/10740 [19:38:41<30:37:14, 16.17s/it]


 37%|███▋      | 3925/10740 [19:39:14<30:27:04, 16.09s/it]

 37%|███▋      | 3926/10740 [19:39:26<28:33:20, 15.09s/it]

 37%|███▋      | 3927/10740 [19:39:42<28:54:11, 15.27s/it]

 37%|███▋      | 3928/10740 [19:40:02<31:25:06, 16.60s/it]

 37%|███▋      | 3929/10740 [19:40:22<33:18:08, 17.60s/it]
{'loss': 0.2255, 'learning_rate': 1.4646923596046327e-06, 'rewards/chosen': -3.514967918395996, 'rewards/rejected': -5.40313720703125, 'rewards/accuracies': 0.75, 'rewards/margins': 1.888169288635254, 'policy_logps/rejected': -327.73736572265625, 'policy_logps/chosen': -302.7511291503906, 'referece_logps/rejected': -273.70599365234375, 'referece_logps/chosen': -267.6014404296875, 'logits/rejected': -1.3588309288024902, 'logits/chosen': -1.418623685836792, 'epoch': 2.19}

 37%|███▋      | 3930/10740 [19:40:41<34:20:54, 18.16s/it]


 37%|███▋      | 3932/10740 [19:41:19<34:14:40, 18.11s/it]

 37%|███▋      | 3933/10740 [19:41:36<34:07:20, 18.05s/it]

 37%|███▋      | 3934/10740 [19:41:56<34:52:33, 18.45s/it]

 37%|███▋      | 3935/10740 [19:42:16<35:56:45, 19.02s/it]
{'loss': 0.2482, 'learning_rate': 1.4630893382453002e-06, 'rewards/chosen': -2.6529743671417236, 'rewards/rejected': -4.84099006652832, 'rewards/accuracies': 0.75, 'rewards/margins': 2.188015937805176, 'policy_logps/rejected': -427.84368896484375, 'policy_logps/chosen': -384.9132080078125, 'referece_logps/rejected': -379.43377685546875, 'referece_logps/chosen': -358.3834533691406, 'logits/rejected': 0.116285040974617, 'logits/chosen': 0.1836911141872406, 'epoch': 2.2}

 37%|███▋      | 3936/10740 [19:42:39<38:10:37, 20.20s/it]


 37%|███▋      | 3938/10740 [19:43:17<37:00:16, 19.58s/it]
{'loss': 0.3184, 'learning_rate': 1.462287258629545e-06, 'rewards/chosen': -4.010927200317383, 'rewards/rejected': -7.365916728973389, 'rewards/accuracies': 0.875, 'rewards/margins': 3.354990005493164, 'policy_logps/rejected': -417.767578125, 'policy_logps/chosen': -376.6180114746094, 'referece_logps/rejected': -344.1083984375, 'referece_logps/chosen': -336.50872802734375, 'logits/rejected': -0.7640203237533569, 'logits/chosen': -0.7504222393035889, 'epoch': 2.2}

 37%|███▋      | 3939/10740 [19:43:37<37:03:04, 19.61s/it]

 37%|███▋      | 3940/10740 [19:43:55<36:13:25, 19.18s/it]


 37%|███▋      | 3942/10740 [19:44:28<33:42:12, 17.85s/it]
{'loss': 0.2366, 'learning_rate': 1.461217230609193e-06, 'rewards/chosen': -2.8175086975097656, 'rewards/rejected': -6.21128511428833, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3937759399414062, 'policy_logps/rejected': -421.208984375, 'policy_logps/chosen': -417.5389099121094, 'referece_logps/rejected': -359.0961608886719, 'referece_logps/chosen': -389.36376953125, 'logits/rejected': -0.6320160031318665, 'logits/chosen': -0.8255150318145752, 'epoch': 2.2}


 37%|███▋      | 3944/10740 [19:44:59<30:35:55, 16.21s/it]

 37%|███▋      | 3945/10740 [19:45:12<29:09:00, 15.44s/it]
{'loss': 0.2423, 'learning_rate': 1.4604142690459842e-06, 'rewards/chosen': -4.023427963256836, 'rewards/rejected': -6.310063362121582, 'rewards/accuracies': 0.875, 'rewards/margins': 2.286635637283325, 'policy_logps/rejected': -263.7768249511719, 'policy_logps/chosen': -363.4505615234375, 'referece_logps/rejected': -200.6761932373047, 'referece_logps/chosen': -323.21630859375, 'logits/rejected': -0.14707554876804352, 'logits/chosen': -0.21641522645950317, 'epoch': 2.2}


 37%|███▋      | 3947/10740 [19:45:48<31:59:09, 16.95s/it]
{'loss': 0.26, 'learning_rate': 1.459878751917571e-06, 'rewards/chosen': -3.927077531814575, 'rewards/rejected': -6.830387592315674, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9033098220825195, 'policy_logps/rejected': -370.74676513671875, 'policy_logps/chosen': -433.54827880859375, 'referece_logps/rejected': -302.44287109375, 'referece_logps/chosen': -394.27752685546875, 'logits/rejected': 0.6057472229003906, 'logits/chosen': 0.5104516744613647, 'epoch': 2.21}

 37%|███▋      | 3948/10740 [19:46:02<30:08:11, 15.97s/it]

 37%|███▋      | 3949/10740 [19:46:15<28:40:20, 15.20s/it]


 37%|███▋      | 3951/10740 [19:46:48<29:44:03, 15.77s/it]

 37%|███▋      | 3952/10740 [19:47:05<30:10:31, 16.00s/it]

 37%|███▋      | 3953/10740 [19:47:25<32:20:55, 17.16s/it]
{'loss': 0.1879, 'learning_rate': 1.4582711974615566e-06, 'rewards/chosen': -2.506507396697998, 'rewards/rejected': -6.325238227844238, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8187313079833984, 'policy_logps/rejected': -405.25732421875, 'policy_logps/chosen': -465.41021728515625, 'referece_logps/rejected': -342.00494384765625, 'referece_logps/chosen': -440.34515380859375, 'logits/rejected': 0.3933160901069641, 'logits/chosen': 0.30408862233161926, 'epoch': 2.21}

 37%|███▋      | 3954/10740 [19:47:46<34:21:54, 18.23s/it]

 37%|███▋      | 3955/10740 [19:48:05<35:14:44, 18.70s/it]

 37%|███▋      | 3956/10740 [19:48:25<35:52:41, 19.04s/it]

 37%|███▋      | 3957/10740 [19:48:47<37:33:45, 19.94s/it]


 37%|███▋      | 3959/10740 [19:49:26<37:22:46, 19.84s/it]
{'loss': 0.3145, 'learning_rate': 1.4566621424939262e-06, 'rewards/chosen': -2.9576165676116943, 'rewards/rejected': -4.675418853759766, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7178025245666504, 'policy_logps/rejected': -387.3083801269531, 'policy_logps/chosen': -374.01129150390625, 'referece_logps/rejected': -340.55419921875, 'referece_logps/chosen': -344.43511962890625, 'logits/rejected': -1.0655657052993774, 'logits/chosen': -0.9577920436859131, 'epoch': 2.21}


 37%|███▋      | 3961/10740 [19:49:55<32:08:32, 17.07s/it]
{'loss': 0.2058, 'learning_rate': 1.4561254583014888e-06, 'rewards/chosen': -3.1215856075286865, 'rewards/rejected': -5.817602157592773, 'rewards/accuracies': 0.875, 'rewards/margins': 2.696016788482666, 'policy_logps/rejected': -429.4607238769531, 'policy_logps/chosen': -396.3918151855469, 'referece_logps/rejected': -371.2846984863281, 'referece_logps/chosen': -365.176025390625, 'logits/rejected': 0.10674400627613068, 'logits/chosen': 0.18380919098854065, 'epoch': 2.21}


 37%|███▋      | 3963/10740 [19:50:35<34:48:47, 18.49s/it]

 37%|███▋      | 3964/10740 [19:50:51<33:20:20, 17.71s/it]
{'loss': 0.3628, 'learning_rate': 1.4553201209309042e-06, 'rewards/chosen': -1.9939451217651367, 'rewards/rejected': -5.075264930725098, 'rewards/accuracies': 0.875, 'rewards/margins': 3.081319808959961, 'policy_logps/rejected': -361.5754699707031, 'policy_logps/chosen': -344.3015441894531, 'referece_logps/rejected': -310.8227844238281, 'referece_logps/chosen': -324.36212158203125, 'logits/rejected': 0.2832071781158447, 'logits/chosen': 0.3816240727901459, 'epoch': 2.21}

 37%|███▋      | 3965/10740 [19:51:11<34:49:53, 18.51s/it]


 37%|███▋      | 3967/10740 [19:51:37<29:29:27, 15.68s/it]

 37%|███▋      | 3968/10740 [19:51:57<31:45:37, 16.88s/it]

 37%|███▋      | 3969/10740 [19:52:17<33:25:18, 17.77s/it]
{'loss': 0.1897, 'learning_rate': 1.4539770640560297e-06, 'rewards/chosen': -3.4399099349975586, 'rewards/rejected': -5.41770601272583, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9777960777282715, 'policy_logps/rejected': -338.2207336425781, 'policy_logps/chosen': -383.93017578125, 'referece_logps/rejected': -284.04364013671875, 'referece_logps/chosen': -349.5310974121094, 'logits/rejected': 0.30227047204971313, 'logits/chosen': 0.24249818921089172, 'epoch': 2.22}

 37%|███▋      | 3970/10740 [19:52:36<34:26:57, 18.32s/it]

 37%|███▋      | 3971/10740 [19:52:57<35:59:30, 19.14s/it]


 37%|███▋      | 3973/10740 [19:53:33<34:50:24, 18.53s/it]
{'loss': 0.2631, 'learning_rate': 1.4529018751837096e-06, 'rewards/chosen': -3.000669479370117, 'rewards/rejected': -5.3606133460998535, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3599441051483154, 'policy_logps/rejected': -419.8087463378906, 'policy_logps/chosen': -402.35186767578125, 'referece_logps/rejected': -366.2025451660156, 'referece_logps/chosen': -372.3451843261719, 'logits/rejected': -0.11976593732833862, 'logits/chosen': -0.15191750228405, 'epoch': 2.22}

 37%|███▋      | 3974/10740 [19:53:53<35:10:09, 18.71s/it]

 37%|███▋      | 3975/10740 [19:54:11<34:45:17, 18.49s/it]

 37%|███▋      | 3976/10740 [19:54:28<34:16:10, 18.24s/it]

 37%|███▋      | 3977/10740 [19:54:42<31:55:52, 17.00s/it]


 37%|███▋      | 3979/10740 [19:55:19<32:50:37, 17.49s/it]

 37%|███▋      | 3980/10740 [19:55:33<30:57:15, 16.48s/it]

 37%|███▋      | 3981/10740 [19:55:53<32:54:02, 17.52s/it]

 37%|███▋      | 3982/10740 [19:56:15<35:34:13, 18.95s/it]

 37%|███▋      | 3983/10740 [19:56:35<36:12:42, 19.29s/it]
{'loss': 0.2864, 'learning_rate': 1.4502110229507458e-06, 'rewards/chosen': -2.493441343307495, 'rewards/rejected': -5.501709461212158, 'rewards/accuracies': 1.0, 'rewards/margins': 3.008268117904663, 'policy_logps/rejected': -280.9537353515625, 'policy_logps/chosen': -323.01995849609375, 'referece_logps/rejected': -225.93667602539062, 'referece_logps/chosen': -298.08551025390625, 'logits/rejected': -0.5226100087165833, 'logits/chosen': -0.6460645794868469, 'epoch': 2.23}

 37%|███▋      | 3984/10740 [19:56:52<34:42:03, 18.49s/it]


 37%|███▋      | 3986/10740 [19:57:29<35:13:24, 18.77s/it]
{'loss': 0.3155, 'learning_rate': 1.4494029676848805e-06, 'rewards/chosen': -3.5206668376922607, 'rewards/rejected': -4.966370105743408, 'rewards/accuracies': 0.5, 'rewards/margins': 1.4457030296325684, 'policy_logps/rejected': -313.6239013671875, 'policy_logps/chosen': -317.3463134765625, 'referece_logps/rejected': -263.960205078125, 'referece_logps/chosen': -282.1396789550781, 'logits/rejected': 0.01813427358865738, 'logits/chosen': -0.03636510670185089, 'epoch': 2.23}

 37%|███▋      | 3987/10740 [19:57:49<35:46:20, 19.07s/it]

 37%|███▋      | 3988/10740 [19:58:08<36:02:17, 19.21s/it]


 37%|███▋      | 3990/10740 [19:58:46<35:36:04, 18.99s/it]
{'loss': 0.2377, 'learning_rate': 1.4483249885379013e-06, 'rewards/chosen': -4.510392189025879, 'rewards/rejected': -5.166803359985352, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6564115881919861, 'policy_logps/rejected': -437.7008056640625, 'policy_logps/chosen': -474.58966064453125, 'referece_logps/rejected': -386.03277587890625, 'referece_logps/chosen': -429.4857177734375, 'logits/rejected': 0.631325364112854, 'logits/chosen': 0.5745328068733215, 'epoch': 2.23}

 37%|███▋      | 3991/10740 [19:59:08<37:27:18, 19.98s/it]

 37%|███▋      | 3992/10740 [19:59:28<37:28:13, 19.99s/it]

 37%|███▋      | 3993/10740 [19:59:46<36:37:36, 19.54s/it]


 37%|███▋      | 3995/10740 [20:00:25<36:40:52, 19.58s/it]
{'loss': 0.2351, 'learning_rate': 1.4469765973224193e-06, 'rewards/chosen': -3.110008955001831, 'rewards/rejected': -5.730970859527588, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6209616661071777, 'policy_logps/rejected': -372.9722900390625, 'policy_logps/chosen': -524.7905883789062, 'referece_logps/rejected': -315.66259765625, 'referece_logps/chosen': -493.6905212402344, 'logits/rejected': 0.3847052752971649, 'logits/chosen': 0.1599765568971634, 'epoch': 2.23}

 37%|███▋      | 3996/10740 [20:00:49<38:43:17, 20.67s/it]
[2024-04-02 15:14:26,982] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 3997/10740 [20:01:09<38:26:54, 20.53s/it]
[2024-04-02 15:14:46,913] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 37%|███▋      | 3999/10740 [20:01:48<37:15:52, 19.90s/it]

 37%|███▋      | 4000/10740 [20:02:00<32:58:51, 17.62s/it]
{'loss': 0.2447, 'learning_rate': 1.4456271897666796e-06, 'rewards/chosen': -3.399502992630005, 'rewards/rejected': -5.69306755065918, 'rewards/accuracies': 0.75, 'rewards/margins': 2.293564558029175, 'policy_logps/rejected': -635.2554321289062, 'policy_logps/chosen': -472.2620849609375, 'referece_logps/rejected': -578.3248291015625, 'referece_logps/chosen': -438.2670593261719, 'logits/rejected': -0.12970498204231262, 'logits/chosen': 0.009743224829435349, 'epoch': 2.23}


 37%|███▋      | 4002/10740 [20:02:54<40:26:13, 21.60s/it]
{'loss': 0.2493, 'learning_rate': 1.4450871428563328e-06, 'rewards/chosen': -2.649991989135742, 'rewards/rejected': -5.752626419067383, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1026341915130615, 'policy_logps/rejected': -494.3294372558594, 'policy_logps/chosen': -568.3406982421875, 'referece_logps/rejected': -436.8031921386719, 'referece_logps/chosen': -541.8408203125, 'logits/rejected': 0.10961653292179108, 'logits/chosen': 0.16316209733486176, 'epoch': 2.24}

 37%|███▋      | 4003/10740 [20:03:11<37:44:52, 20.17s/it]

 37%|███▋      | 4004/10740 [20:03:30<37:37:47, 20.11s/it]

 37%|███▋      | 4005/10740 [20:03:49<36:45:53, 19.65s/it]

 37%|███▋      | 4006/10740 [20:04:09<36:45:21, 19.65s/it]

 37%|███▋      | 4007/10740 [20:04:26<35:41:07, 19.08s/it]

 37%|███▋      | 4008/10740 [20:04:47<36:32:02, 19.54s/it]

 37%|███▋      | 4009/10740 [20:05:07<36:54:52, 19.74s/it]

 37%|███▋      | 4010/10740 [20:05:27<36:49:02, 19.69s/it]

 37%|███▋      | 4011/10740 [20:05:42<34:28:09, 18.44s/it]


 37%|███▋      | 4013/10740 [20:06:18<33:26:24, 17.90s/it]
{'loss': 0.2, 'learning_rate': 1.4421139956740654e-06, 'rewards/chosen': -3.446659564971924, 'rewards/rejected': -6.523952960968018, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0772931575775146, 'policy_logps/rejected': -370.1875, 'policy_logps/chosen': -353.8105773925781, 'referece_logps/rejected': -304.9479675292969, 'referece_logps/chosen': -319.343994140625, 'logits/rejected': 0.181900292634964, 'logits/chosen': 0.29475706815719604, 'epoch': 2.24}

 37%|███▋      | 4014/10740 [20:06:37<34:25:32, 18.43s/it]


 37%|███▋      | 4016/10740 [20:07:06<30:34:01, 16.37s/it]
{'loss': 0.2499, 'learning_rate': 1.4413022915360616e-06, 'rewards/chosen': -3.2150843143463135, 'rewards/rejected': -7.541370391845703, 'rewards/accuracies': 0.875, 'rewards/margins': 4.326285362243652, 'policy_logps/rejected': -400.9662170410156, 'policy_logps/chosen': -362.5634460449219, 'referece_logps/rejected': -325.5525207519531, 'referece_logps/chosen': -330.41259765625, 'logits/rejected': -0.4373585283756256, 'logits/chosen': -0.35672587156295776, 'epoch': 2.24}

 37%|███▋      | 4017/10740 [20:07:19<28:55:13, 15.49s/it]

 37%|███▋      | 4018/10740 [20:07:37<30:12:11, 16.18s/it]

 37%|███▋      | 4019/10740 [20:07:51<28:58:45, 15.52s/it]


 37%|███▋      | 4021/10740 [20:08:22<27:59:15, 15.00s/it]

 37%|███▋      | 4022/10740 [20:08:38<28:52:26, 15.47s/it]
{'loss': 0.3596, 'learning_rate': 1.4396778002116603e-06, 'rewards/chosen': -4.735815525054932, 'rewards/rejected': -5.087243556976318, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3514280319213867, 'policy_logps/rejected': -430.63726806640625, 'policy_logps/chosen': -379.9178466796875, 'referece_logps/rejected': -379.76483154296875, 'referece_logps/chosen': -332.5596923828125, 'logits/rejected': -0.4754519462585449, 'logits/chosen': -0.3709854483604431, 'epoch': 2.25}


 37%|███▋      | 4024/10740 [20:09:04<26:44:41, 14.34s/it]
{'loss': 0.1976, 'learning_rate': 1.4391359829226124e-06, 'rewards/chosen': -3.346559524536133, 'rewards/rejected': -7.016938209533691, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6703782081604004, 'policy_logps/rejected': -317.1978454589844, 'policy_logps/chosen': -409.3563232421875, 'referece_logps/rejected': -247.02845764160156, 'referece_logps/chosen': -375.8907470703125, 'logits/rejected': -0.8329312801361084, 'logits/chosen': -0.9347391724586487, 'epoch': 2.25}

 37%|███▋      | 4025/10740 [20:09:15<24:41:24, 13.24s/it]

 37%|███▋      | 4026/10740 [20:09:31<26:10:16, 14.03s/it]


 38%|███▊      | 4028/10740 [20:09:52<22:54:35, 12.29s/it]
{'loss': 0.3426, 'learning_rate': 1.4380518692557693e-06, 'rewards/chosen': -3.9707703590393066, 'rewards/rejected': -8.955333709716797, 'rewards/accuracies': 0.875, 'rewards/margins': 4.98456335067749, 'policy_logps/rejected': -358.4660949707031, 'policy_logps/chosen': -340.52203369140625, 'referece_logps/rejected': -268.9127502441406, 'referece_logps/chosen': -300.8143005371094, 'logits/rejected': -0.8197863698005676, 'logits/chosen': -1.0037741661071777, 'epoch': 2.25}


 38%|███▊      | 4030/10740 [20:10:26<27:55:54, 14.99s/it]
[2024-04-02 15:23:44,389] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4031/10740 [20:10:46<30:54:47, 16.59s/it]
{'loss': 0.2833, 'learning_rate': 1.4372383655795083e-06, 'rewards/chosen': -3.2274515628814697, 'rewards/rejected': -6.456794261932373, 'rewards/accuracies': 0.75, 'rewards/margins': 3.229342460632324, 'policy_logps/rejected': -275.97283935546875, 'policy_logps/chosen': -426.2006530761719, 'referece_logps/rejected': -211.4049072265625, 'referece_logps/chosen': -393.9261474609375, 'logits/rejected': -0.40243542194366455, 'logits/chosen': -0.5480803847312927, 'epoch': 2.25}

 38%|███▊      | 4032/10740 [20:11:05<31:46:40, 17.05s/it]

 38%|███▊      | 4033/10740 [20:11:23<32:28:33, 17.43s/it]

 38%|███▊      | 4034/10740 [20:11:44<34:19:58, 18.43s/it]


 38%|███▊      | 4036/10740 [20:12:17<32:33:50, 17.49s/it]
{'loss': 0.195, 'learning_rate': 1.4358817310900667e-06, 'rewards/chosen': -3.0721752643585205, 'rewards/rejected': -8.507400512695312, 'rewards/accuracies': 1.0, 'rewards/margins': 5.435224533081055, 'policy_logps/rejected': -350.63885498046875, 'policy_logps/chosen': -360.30975341796875, 'referece_logps/rejected': -265.5648498535156, 'referece_logps/chosen': -329.5880126953125, 'logits/rejected': -0.5597578883171082, 'logits/chosen': -0.6755421161651611, 'epoch': 2.25}


 38%|███▊      | 4038/10740 [20:12:54<33:50:26, 18.18s/it]

 38%|███▊      | 4039/10740 [20:13:06<30:19:51, 16.29s/it]
{'loss': 0.1399, 'learning_rate': 1.4350672744648226e-06, 'rewards/chosen': -3.0316555500030518, 'rewards/rejected': -7.311135292053223, 'rewards/accuracies': 1.0, 'rewards/margins': 4.279480457305908, 'policy_logps/rejected': -268.0626525878906, 'policy_logps/chosen': -473.4463195800781, 'referece_logps/rejected': -194.95126342773438, 'referece_logps/chosen': -443.1297607421875, 'logits/rejected': -0.5799766182899475, 'logits/chosen': -0.9844525456428528, 'epoch': 2.26}

 38%|███▊      | 4040/10740 [20:13:19<28:18:05, 15.21s/it]

 38%|███▊      | 4041/10740 [20:13:33<27:49:20, 14.95s/it]

 38%|███▊      | 4042/10740 [20:13:52<29:39:09, 15.94s/it]


 38%|███▊      | 4044/10740 [20:14:31<32:41:30, 17.58s/it]
[2024-04-02 15:27:48,983] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2917, 'learning_rate': 1.4337090556767813e-06, 'rewards/chosen': -3.3537254333496094, 'rewards/rejected': -3.9077887535095215, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5540633201599121, 'policy_logps/rejected': -432.743896484375, 'policy_logps/chosen': -309.1051025390625, 'referece_logps/rejected': -393.6659851074219, 'referece_logps/chosen': -275.56781005859375, 'logits/rejected': 0.23458093404769897, 'logits/chosen': 0.08278167992830276, 'epoch': 2.26}

 38%|███▊      | 4045/10740 [20:14:46<31:33:52, 16.97s/it]


 38%|███▊      | 4047/10740 [20:15:17<30:01:26, 16.15s/it]
{'loss': 0.2331, 'learning_rate': 1.432893650843469e-06, 'rewards/chosen': -2.5375583171844482, 'rewards/rejected': -5.001782417297363, 'rewards/accuracies': 0.75, 'rewards/margins': 2.464223861694336, 'policy_logps/rejected': -333.9666748046875, 'policy_logps/chosen': -418.3619384765625, 'referece_logps/rejected': -283.9488525390625, 'referece_logps/chosen': -392.986328125, 'logits/rejected': -0.5495955348014832, 'logits/chosen': -0.8063139319419861, 'epoch': 2.26}


 38%|███▊      | 4048/10740 [20:15:30<28:20:36, 15.25s/it]
{'loss': 0.3042, 'learning_rate': 1.4323498507163629e-06, 'rewards/chosen': -2.719721794128418, 'rewards/rejected': -4.8406982421875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.120976448059082, 'policy_logps/rejected': -413.22271728515625, 'policy_logps/chosen': -386.095703125, 'referece_logps/rejected': -364.815673828125, 'referece_logps/chosen': -358.8984680175781, 'logits/rejected': -0.7325292825698853, 'logits/chosen': -0.6183820962905884, 'epoch': 2.26}

 38%|███▊      | 4050/10740 [20:16:09<31:46:05, 17.10s/it]
{'loss': 0.3178, 'learning_rate': 1.4320778916555253e-06, 'rewards/chosen': -4.92557430267334, 'rewards/rejected': -7.088407516479492, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1628329753875732, 'policy_logps/rejected': -432.0523986816406, 'policy_logps/chosen': -400.2528991699219, 'referece_logps/rejected': -361.1683349609375, 'referece_logps/chosen': -350.9971618652344, 'logits/rejected': 0.07644130289554596, 'logits/chosen': -0.037166375666856766, 'epoch': 2.26}

 38%|███▊      | 4051/10740 [20:16:30<34:02:17, 18.32s/it]

 38%|███▊      | 4052/10740 [20:16:50<34:55:48, 18.80s/it]

 38%|███▊      | 4053/10740 [20:17:12<36:51:23, 19.84s/it]

 38%|███▊      | 4054/10740 [20:17:30<35:45:25, 19.25s/it]

 38%|███▊      | 4055/10740 [20:17:43<32:19:36, 17.41s/it]
[2024-04-02 15:31:19,961] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4056/10740 [20:18:02<33:01:54, 17.79s/it]

 38%|███▊      | 4057/10740 [20:18:21<34:03:47, 18.35s/it]

 38%|███▊      | 4058/10740 [20:18:41<34:35:11, 18.63s/it]

 38%|███▊      | 4059/10740 [20:19:03<36:30:14, 19.67s/it]

 38%|███▊      | 4060/10740 [20:19:19<34:23:59, 18.54s/it]

 38%|███▊      | 4061/10740 [20:19:40<36:10:22, 19.50s/it]


 38%|███▊      | 4063/10740 [20:20:19<35:55:14, 19.37s/it]
{'loss': 0.1811, 'learning_rate': 1.4285388567056798e-06, 'rewards/chosen': -3.687603712081909, 'rewards/rejected': -6.1741228103637695, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4865188598632812, 'policy_logps/rejected': -278.87310791015625, 'policy_logps/chosen': -447.31976318359375, 'referece_logps/rejected': -217.1319122314453, 'referece_logps/chosen': -410.4437255859375, 'logits/rejected': -0.09611175954341888, 'logits/chosen': -0.2811287045478821, 'epoch': 2.27}
[2024-04-02 15:33:58,025] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4064/10740 [20:20:40<36:43:09, 19.80s/it]

 38%|███▊      | 4065/10740 [20:20:53<32:58:36, 17.79s/it]

 38%|███▊      | 4066/10740 [20:21:13<34:23:07, 18.55s/it]

 38%|███▊      | 4067/10740 [20:21:25<30:51:00, 16.64s/it]

 38%|███▊      | 4068/10740 [20:21:45<32:45:50, 17.68s/it]

 38%|███▊      | 4069/10740 [20:21:59<30:14:48, 16.32s/it]

 38%|███▊      | 4070/10740 [20:22:18<32:11:29, 17.37s/it]

 38%|███▊      | 4071/10740 [20:22:38<33:27:41, 18.06s/it]

 38%|███▊      | 4072/10740 [20:22:52<31:10:56, 16.84s/it]

 38%|███▊      | 4073/10740 [20:23:12<32:44:30, 17.68s/it]

 38%|███▊      | 4074/10740 [20:23:32<33:59:00, 18.35s/it]

 38%|███▊      | 4075/10740 [20:23:50<34:05:43, 18.42s/it]

 38%|███▊      | 4076/10740 [20:24:10<34:36:55, 18.70s/it]

 38%|███▊      | 4077/10740 [20:24:30<35:17:49, 19.07s/it]

 38%|███▊      | 4078/10740 [20:24:46<33:39:10, 18.19s/it]

 38%|███▊      | 4079/10740 [20:25:05<34:04:36, 18.42s/it]

 38%|███▊      | 4080/10740 [20:25:21<32:58:53, 17.83s/it]


 38%|███▊      | 4082/10740 [20:25:59<34:18:22, 18.55s/it]
{'loss': 0.3005, 'learning_rate': 1.42335458719872e-06, 'rewards/chosen': -2.4604504108428955, 'rewards/rejected': -5.613428592681885, 'rewards/accuracies': 1.0, 'rewards/margins': 3.15297794342041, 'policy_logps/rejected': -369.9884948730469, 'policy_logps/chosen': -409.6475524902344, 'referece_logps/rejected': -313.8542175292969, 'referece_logps/chosen': -385.04302978515625, 'logits/rejected': 0.11333942413330078, 'logits/chosen': 0.06952384859323502, 'epoch': 2.28}

 38%|███▊      | 4083/10740 [20:26:15<32:41:53, 17.68s/it]

 38%|███▊      | 4084/10740 [20:26:35<33:40:43, 18.22s/it]

 38%|███▊      | 4085/10740 [20:26:53<33:43:36, 18.24s/it]

 38%|███▊      | 4086/10740 [20:27:08<31:51:19, 17.23s/it]

 38%|███▊      | 4087/10740 [20:27:27<33:09:51, 17.95s/it]

 38%|███▊      | 4088/10740 [20:27:47<34:05:12, 18.45s/it]

 38%|███▊      | 4089/10740 [20:28:04<33:14:45, 18.00s/it]

 38%|███▊      | 4090/10740 [20:28:23<33:37:17, 18.20s/it]

 38%|███▊      | 4091/10740 [20:28:38<32:14:19, 17.46s/it]

 38%|███▊      | 4092/10740 [20:28:55<31:54:14, 17.28s/it]

 38%|███▊      | 4093/10740 [20:29:13<32:23:16, 17.54s/it]

 38%|███▊      | 4094/10740 [20:29:26<29:48:14, 16.14s/it]

 38%|███▊      | 4095/10740 [20:29:38<27:34:41, 14.94s/it]

 38%|███▊      | 4096/10740 [20:29:58<30:08:09, 16.33s/it]

 38%|███▊      | 4097/10740 [20:30:09<27:14:17, 14.76s/it]

 38%|███▊      | 4098/10740 [20:30:31<31:01:02, 16.81s/it]

 38%|███▊      | 4099/10740 [20:30:51<33:17:47, 18.05s/it]

 38%|███▊      | 4100/10740 [20:31:02<29:19:47, 15.90s/it]

 38%|███▊      | 4101/10740 [20:31:23<31:45:00, 17.22s/it]

 38%|███▊      | 4102/10740 [20:31:43<33:34:35, 18.21s/it]

 38%|███▊      | 4103/10740 [20:32:00<32:55:58, 17.86s/it]

 38%|███▊      | 4104/10740 [20:32:14<30:45:55, 16.69s/it]

 38%|███▊      | 4105/10740 [20:32:34<32:26:28, 17.60s/it]

 38%|███▊      | 4106/10740 [20:32:51<31:59:04, 17.36s/it]

 38%|███▊      | 4107/10740 [20:33:10<33:16:34, 18.06s/it]

 38%|███▊      | 4108/10740 [20:33:30<33:51:06, 18.38s/it]
[2024-04-02 15:47:09,225] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4109/10740 [20:33:51<35:33:17, 19.30s/it]

 38%|███▊      | 4110/10740 [20:34:07<33:53:34, 18.40s/it]

 38%|███▊      | 4111/10740 [20:34:23<32:18:24, 17.54s/it]


 38%|███▊      | 4113/10740 [20:35:02<34:12:01, 18.58s/it]

 38%|███▊      | 4114/10740 [20:35:16<31:35:00, 17.16s/it]
{'loss': 0.2277, 'learning_rate': 1.414591853786376e-06, 'rewards/chosen': -4.181796550750732, 'rewards/rejected': -8.531672477722168, 'rewards/accuracies': 0.875, 'rewards/margins': 4.3498759269714355, 'policy_logps/rejected': -583.6192626953125, 'policy_logps/chosen': -506.1607666015625, 'referece_logps/rejected': -498.302490234375, 'referece_logps/chosen': -464.34283447265625, 'logits/rejected': -0.25016263127326965, 'logits/chosen': -0.1541273295879364, 'epoch': 2.3}

 38%|███▊      | 4115/10740 [20:35:30<30:02:02, 16.32s/it]

 38%|███▊      | 4116/10740 [20:35:49<31:11:57, 16.96s/it]

 38%|███▊      | 4117/10740 [20:36:10<33:42:39, 18.32s/it]

 38%|███▊      | 4118/10740 [20:36:30<34:26:43, 18.73s/it]

 38%|███▊      | 4119/10740 [20:36:47<33:33:51, 18.25s/it]

 38%|███▊      | 4120/10740 [20:37:06<33:46:44, 18.37s/it]
[2024-04-02 15:50:45,507] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4121/10740 [20:37:27<35:34:07, 19.35s/it]
[2024-04-02 15:50:58,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4122/10740 [20:37:40<31:48:05, 17.30s/it]

 38%|███▊      | 4123/10740 [20:38:00<33:17:28, 18.11s/it]
[2024-04-02 15:51:39,718] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 4124/10740 [20:38:21<35:15:06, 19.18s/it]

 38%|███▊      | 4125/10740 [20:38:42<36:01:56, 19.61s/it]

 38%|███▊      | 4126/10740 [20:39:04<37:33:55, 20.45s/it]

 38%|███▊      | 4127/10740 [20:39:16<32:52:02, 17.89s/it]

 38%|███▊      | 4128/10740 [20:39:36<33:52:33, 18.44s/it]

 38%|███▊      | 4129/10740 [20:39:57<34:56:05, 19.02s/it]

 38%|███▊      | 4130/10740 [20:40:16<34:59:37, 19.06s/it]

 38%|███▊      | 4131/10740 [20:40:32<33:16:39, 18.13s/it]

 38%|███▊      | 4132/10740 [20:40:47<31:34:53, 17.21s/it]

 38%|███▊      | 4133/10740 [20:40:59<29:01:53, 15.82s/it]

 38%|███▊      | 4134/10740 [20:41:21<32:32:03, 17.73s/it]

 39%|███▊      | 4135/10740 [20:41:41<33:30:54, 18.27s/it]

 39%|███▊      | 4136/10740 [20:41:55<31:21:27, 17.09s/it]

 39%|███▊      | 4137/10740 [20:42:17<33:42:25, 18.38s/it]

 39%|███▊      | 4138/10740 [20:42:38<35:03:34, 19.12s/it]

 39%|███▊      | 4139/10740 [20:42:58<35:39:30, 19.45s/it]

 39%|███▊      | 4140/10740 [20:43:18<35:54:24, 19.59s/it]

 39%|███▊      | 4141/10740 [20:43:37<35:48:35, 19.54s/it]

 39%|███▊      | 4142/10740 [20:43:57<35:50:25, 19.56s/it]

 39%|███▊      | 4143/10740 [20:44:11<32:44:02, 17.86s/it]

 39%|███▊      | 4144/10740 [20:44:33<34:58:11, 19.09s/it]

 39%|███▊      | 4145/10740 [20:44:52<35:21:47, 19.30s/it]

 39%|███▊      | 4146/10740 [20:45:12<35:29:59, 19.38s/it]

 39%|███▊      | 4147/10740 [20:45:34<36:55:06, 20.16s/it]

 39%|███▊      | 4148/10740 [20:45:50<34:49:04, 19.01s/it]

 39%|███▊      | 4149/10740 [20:46:07<33:38:31, 18.38s/it]

 39%|███▊      | 4150/10740 [20:46:22<31:50:17, 17.39s/it]

 39%|███▊      | 4151/10740 [20:46:40<32:11:35, 17.59s/it]

 39%|███▊      | 4152/10740 [20:47:00<33:18:52, 18.20s/it]

 39%|███▊      | 4153/10740 [20:47:15<31:48:12, 17.38s/it]

 39%|███▊      | 4154/10740 [20:47:32<31:35:24, 17.27s/it]

 39%|███▊      | 4155/10740 [20:47:49<31:04:50, 16.99s/it]

 39%|███▊      | 4156/10740 [20:48:05<30:53:23, 16.89s/it]

 39%|███▊      | 4157/10740 [20:48:27<33:36:42, 18.38s/it]

 39%|███▊      | 4158/10740 [20:48:47<34:23:00, 18.81s/it]


 39%|███▊      | 4160/10740 [20:49:26<34:47:07, 19.03s/it]

 39%|███▊      | 4161/10740 [20:49:45<35:14:30, 19.28s/it]

 39%|███▉      | 4162/10740 [20:50:01<33:08:34, 18.14s/it]

 39%|███▉      | 4163/10740 [20:50:19<32:56:06, 18.03s/it]

 39%|███▉      | 4164/10740 [20:50:40<34:42:27, 19.00s/it]

 39%|███▉      | 4165/10740 [20:51:01<35:50:21, 19.62s/it]

 39%|███▉      | 4166/10740 [20:51:18<34:35:58, 18.95s/it]

 39%|███▉      | 4167/10740 [20:51:36<33:35:34, 18.40s/it]

 39%|███▉      | 4168/10740 [20:51:58<35:36:32, 19.51s/it]

 39%|███▉      | 4169/10740 [20:52:18<36:12:13, 19.83s/it]
[2024-04-02 16:05:36,441] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4170/10740 [20:52:38<36:02:46, 19.75s/it]

 39%|███▉      | 4171/10740 [20:52:57<35:32:02, 19.47s/it]

 39%|███▉      | 4172/10740 [20:53:17<35:56:53, 19.70s/it]

 39%|███▉      | 4173/10740 [20:53:34<34:38:30, 18.99s/it]

 39%|███▉      | 4174/10740 [20:53:55<35:27:37, 19.44s/it]

 39%|███▉      | 4175/10740 [20:54:15<35:43:17, 19.59s/it]
{'loss': 0.2337, 'learning_rate': 1.3977816207374038e-06, 'rewards/chosen': -3.1896538734436035, 'rewards/rejected': -5.625988006591797, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4363338947296143, 'policy_logps/rejected': -366.63397216796875, 'policy_logps/chosen': -378.8486633300781, 'referece_logps/rejected': -310.3740539550781, 'referece_logps/chosen': -346.9521179199219, 'logits/rejected': -0.11244860291481018, 'logits/chosen': -0.14192000031471252, 'epoch': 2.33}

 39%|███▉      | 4176/10740 [20:54:31<34:14:45, 18.78s/it]


 39%|███▉      | 4178/10740 [20:55:14<36:40:22, 20.12s/it]
[2024-04-02 16:08:32,136] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2131, 'learning_rate': 1.3969513681455678e-06, 'rewards/chosen': -3.131683349609375, 'rewards/rejected': -6.6133317947387695, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4816486835479736, 'policy_logps/rejected': -375.5521240234375, 'policy_logps/chosen': -349.51776123046875, 'referece_logps/rejected': -309.4187927246094, 'referece_logps/chosen': -318.200927734375, 'logits/rejected': -0.2667858302593231, 'logits/chosen': -0.22616201639175415, 'epoch': 2.33}
[2024-04-02 16:08:51,904] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 39%|███▉      | 4180/10740 [20:55:54<36:33:03, 20.06s/it]
[2024-04-02 16:09:12,066] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4181/10740 [20:56:10<34:21:06, 18.85s/it]
[2024-04-02 16:09:28,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4182/10740 [20:56:27<33:35:34, 18.44s/it]

 39%|███▉      | 4183/10740 [20:56:38<29:21:19, 16.12s/it]
{'loss': 0.2479, 'learning_rate': 1.3955668920876122e-06, 'rewards/chosen': -2.9187369346618652, 'rewards/rejected': -4.94862699508667, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0298895835876465, 'policy_logps/rejected': -446.5729675292969, 'policy_logps/chosen': -503.19158935546875, 'referece_logps/rejected': -397.0867004394531, 'referece_logps/chosen': -474.00421142578125, 'logits/rejected': 0.7998864650726318, 'logits/chosen': 0.6903416514396667, 'epoch': 2.34}


 39%|███▉      | 4185/10740 [20:57:10<29:53:26, 16.42s/it]

 39%|███▉      | 4186/10740 [20:57:28<30:49:22, 16.93s/it]
{'loss': 0.3006, 'learning_rate': 1.3947357745180105e-06, 'rewards/chosen': -2.4890897274017334, 'rewards/rejected': -5.914412021636963, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4253225326538086, 'policy_logps/rejected': -439.14990234375, 'policy_logps/chosen': -521.226318359375, 'referece_logps/rejected': -380.0057373046875, 'referece_logps/chosen': -496.33544921875, 'logits/rejected': -0.8825957775115967, 'logits/chosen': -0.9547609686851501, 'epoch': 2.34}
[2024-04-02 16:11:03,838] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 39%|███▉      | 4188/10740 [20:58:07<33:35:08, 18.45s/it]

 39%|███▉      | 4189/10740 [20:58:29<35:17:34, 19.39s/it]

 39%|███▉      | 4190/10740 [20:58:43<32:15:28, 17.73s/it]

 39%|███▉      | 4191/10740 [20:59:01<32:29:13, 17.86s/it]

 39%|███▉      | 4192/10740 [20:59:21<33:35:18, 18.47s/it]

 39%|███▉      | 4193/10740 [20:59:42<35:10:24, 19.34s/it]

 39%|███▉      | 4194/10740 [20:59:59<34:07:27, 18.77s/it]

 39%|███▉      | 4195/10740 [21:00:17<33:39:59, 18.52s/it]

 39%|███▉      | 4196/10740 [21:00:35<33:10:51, 18.25s/it]
{'loss': 0.2803, 'learning_rate': 1.3919630528009258e-06, 'rewards/chosen': -2.956186056137085, 'rewards/rejected': -6.114557266235352, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1583714485168457, 'policy_logps/rejected': -410.07537841796875, 'policy_logps/chosen': -345.4827880859375, 'referece_logps/rejected': -348.9298095703125, 'referece_logps/chosen': -315.9209289550781, 'logits/rejected': -0.19774547219276428, 'logits/chosen': -0.18133783340454102, 'epoch': 2.34}


 39%|███▉      | 4198/10740 [21:01:06<31:14:29, 17.19s/it]
{'loss': 0.4427, 'learning_rate': 1.3914080798502528e-06, 'rewards/chosen': -2.8546102046966553, 'rewards/rejected': -4.980987071990967, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1263773441314697, 'policy_logps/rejected': -352.55572509765625, 'policy_logps/chosen': -520.8480224609375, 'referece_logps/rejected': -302.745849609375, 'referece_logps/chosen': -492.30194091796875, 'logits/rejected': 0.38633638620376587, 'logits/chosen': 0.48263776302337646, 'epoch': 2.35}


 39%|███▉      | 4200/10740 [21:01:42<31:59:43, 17.61s/it]

 39%|███▉      | 4201/10740 [21:01:57<30:32:35, 16.82s/it]

 39%|███▉      | 4202/10740 [21:02:09<28:04:39, 15.46s/it]

 39%|███▉      | 4203/10740 [21:02:29<30:43:13, 16.92s/it]
[2024-04-02 16:15:47,493] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4204/10740 [21:02:51<33:31:01, 18.46s/it]

 39%|███▉      | 4205/10740 [21:03:05<31:06:13, 17.13s/it]

 39%|███▉      | 4206/10740 [21:03:25<32:33:00, 17.93s/it]

 39%|███▉      | 4207/10740 [21:03:45<33:36:10, 18.52s/it]

 39%|███▉      | 4208/10740 [21:03:57<29:56:39, 16.50s/it]

 39%|███▉      | 4209/10740 [21:04:07<26:42:06, 14.72s/it]
{'loss': 0.2839, 'learning_rate': 1.3883531886717574e-06, 'rewards/chosen': -4.676977634429932, 'rewards/rejected': -6.513018608093262, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8360410928726196, 'policy_logps/rejected': -369.64678955078125, 'policy_logps/chosen': -264.8623046875, 'referece_logps/rejected': -304.5165710449219, 'referece_logps/chosen': -218.092529296875, 'logits/rejected': -0.663662850856781, 'logits/chosen': -0.6841488480567932, 'epoch': 2.35}

 39%|███▉      | 4210/10740 [21:04:18<24:26:51, 13.48s/it]

 39%|███▉      | 4211/10740 [21:04:40<29:07:47, 16.06s/it]


 39%|███▉      | 4213/10740 [21:05:17<31:43:20, 17.50s/it]
[2024-04-02 16:18:35,187] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 4214/10740 [21:05:35<31:49:03, 17.55s/it]

 39%|███▉      | 4215/10740 [21:05:46<28:17:18, 15.61s/it]

 39%|███▉      | 4216/10740 [21:05:57<25:54:49, 14.30s/it]

 39%|███▉      | 4217/10740 [21:06:17<28:53:44, 15.95s/it]
{'loss': 0.281, 'learning_rate': 1.3861287632664768e-06, 'rewards/chosen': -3.6217312812805176, 'rewards/rejected': -6.456378936767578, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8346476554870605, 'policy_logps/rejected': -346.0164794921875, 'policy_logps/chosen': -367.9528503417969, 'referece_logps/rejected': -281.45269775390625, 'referece_logps/chosen': -331.7355041503906, 'logits/rejected': 0.42248547077178955, 'logits/chosen': 0.2421722710132599, 'epoch': 2.36}
[2024-04-02 16:19:56,388] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 39%|███▉      | 4219/10740 [21:06:55<31:27:17, 17.37s/it]

 39%|███▉      | 4220/10740 [21:07:15<32:58:19, 18.21s/it]

 39%|███▉      | 4221/10740 [21:07:35<33:47:27, 18.66s/it]
{'loss': 0.1774, 'learning_rate': 1.3850157068898683e-06, 'rewards/chosen': -3.1263651847839355, 'rewards/rejected': -6.25055456161499, 'rewards/accuracies': 0.875, 'rewards/margins': 3.124190092086792, 'policy_logps/rejected': -310.1773986816406, 'policy_logps/chosen': -233.4442596435547, 'referece_logps/rejected': -247.67184448242188, 'referece_logps/chosen': -202.1805877685547, 'logits/rejected': -0.5017592310905457, 'logits/chosen': -0.4822518527507782, 'epoch': 2.36}


 39%|███▉      | 4223/10740 [21:08:18<36:42:19, 20.28s/it]

 39%|███▉      | 4224/10740 [21:08:42<38:38:36, 21.35s/it]
[2024-04-02 16:21:59,974] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3371, 'learning_rate': 1.3841805468285534e-06, 'rewards/chosen': -2.8179140090942383, 'rewards/rejected': -5.9820475578308105, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1641342639923096, 'policy_logps/rejected': -288.3631896972656, 'policy_logps/chosen': -260.41168212890625, 'referece_logps/rejected': -228.542724609375, 'referece_logps/chosen': -232.23257446289062, 'logits/rejected': -0.657055139541626, 'logits/chosen': -0.7967274785041809, 'epoch': 2.36}
[2024-04-02 16:22:12,701] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 39%|███▉      | 4226/10740 [21:09:10<31:58:15, 17.67s/it]

 39%|███▉      | 4227/10740 [21:09:27<32:01:52, 17.71s/it]
{'loss': 0.2678, 'learning_rate': 1.383345072287791e-06, 'rewards/chosen': -3.5497820377349854, 'rewards/rejected': -5.936013221740723, 'rewards/accuracies': 0.75, 'rewards/margins': 2.386230707168579, 'policy_logps/rejected': -239.87631225585938, 'policy_logps/chosen': -266.977294921875, 'referece_logps/rejected': -180.51620483398438, 'referece_logps/chosen': -231.47947692871094, 'logits/rejected': -0.5788798332214355, 'logits/chosen': -0.5562272071838379, 'epoch': 2.36}

 39%|███▉      | 4228/10740 [21:09:45<31:47:46, 17.58s/it]

 39%|███▉      | 4229/10740 [21:10:04<33:00:13, 18.25s/it]


 39%|███▉      | 4231/10740 [21:10:40<32:18:18, 17.87s/it]

 39%|███▉      | 4232/10740 [21:10:54<29:57:29, 16.57s/it]

 39%|███▉      | 4233/10740 [21:11:17<33:44:02, 18.66s/it]

 39%|███▉      | 4234/10740 [21:11:40<35:43:17, 19.77s/it]

 39%|███▉      | 4235/10740 [21:11:59<35:45:29, 19.79s/it]

 39%|███▉      | 4236/10740 [21:12:19<35:44:32, 19.78s/it]

 39%|███▉      | 4237/10740 [21:12:33<32:34:00, 18.03s/it]

 39%|███▉      | 4238/10740 [21:12:48<30:59:30, 17.16s/it]

 39%|███▉      | 4239/10740 [21:13:00<28:04:53, 15.55s/it]

 39%|███▉      | 4240/10740 [21:13:17<28:39:19, 15.87s/it]

 39%|███▉      | 4241/10740 [21:13:37<31:04:55, 17.22s/it]

 39%|███▉      | 4242/10740 [21:13:58<33:03:02, 18.31s/it]
[2024-04-02 16:27:16,155] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4243/10740 [21:14:18<34:12:34, 18.96s/it]
[2024-04-02 16:27:36,616] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4244/10740 [21:14:32<31:13:01, 17.30s/it]

 40%|███▉      | 4245/10740 [21:14:47<29:47:44, 16.51s/it]

 40%|███▉      | 4246/10740 [21:15:06<31:27:28, 17.44s/it]
[2024-04-02 16:28:24,331] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2709, 'learning_rate': 1.378046474755596e-06, 'rewards/chosen': -2.347240447998047, 'rewards/rejected': -6.053256034851074, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7060153484344482, 'policy_logps/rejected': -416.684814453125, 'policy_logps/chosen': -400.67578125, 'referece_logps/rejected': -356.15228271484375, 'referece_logps/chosen': -377.2033996582031, 'logits/rejected': 0.04602666199207306, 'logits/chosen': 0.22960089147090912, 'epoch': 2.37}


 40%|███▉      | 4248/10740 [21:15:41<31:23:34, 17.41s/it]
{'loss': 0.1723, 'learning_rate': 1.3774880025575031e-06, 'rewards/chosen': -2.55854868888855, 'rewards/rejected': -4.029435634613037, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4708869457244873, 'policy_logps/rejected': -434.87518310546875, 'policy_logps/chosen': -416.41717529296875, 'referece_logps/rejected': -394.58087158203125, 'referece_logps/chosen': -390.8316650390625, 'logits/rejected': -0.8359803557395935, 'logits/chosen': -0.7824757099151611, 'epoch': 2.37}


 40%|███▉      | 4250/10740 [21:16:06<26:44:52, 14.84s/it]
{'loss': 0.2188, 'learning_rate': 1.376929393025575e-06, 'rewards/chosen': -2.792595863342285, 'rewards/rejected': -6.257297515869141, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4647016525268555, 'policy_logps/rejected': -291.7166748046875, 'policy_logps/chosen': -537.42919921875, 'referece_logps/rejected': -229.1436767578125, 'referece_logps/chosen': -509.50323486328125, 'logits/rejected': 0.46876105666160583, 'logits/chosen': 0.11425533890724182, 'epoch': 2.37}

 40%|███▉      | 4251/10740 [21:16:19<25:49:05, 14.32s/it]


 40%|███▉      | 4253/10740 [21:16:52<27:29:32, 15.26s/it]
{'loss': 0.1843, 'learning_rate': 1.3760912216713156e-06, 'rewards/chosen': -3.1313045024871826, 'rewards/rejected': -7.9591145515441895, 'rewards/accuracies': 1.0, 'rewards/margins': 4.827809810638428, 'policy_logps/rejected': -453.4892272949219, 'policy_logps/chosen': -471.6651611328125, 'referece_logps/rejected': -373.8981018066406, 'referece_logps/chosen': -440.3521423339844, 'logits/rejected': 0.7902023196220398, 'logits/chosen': 0.7741188406944275, 'epoch': 2.38}


 40%|███▉      | 4255/10740 [21:17:31<31:21:19, 17.41s/it]
{'loss': 0.2149, 'learning_rate': 1.3755322696940292e-06, 'rewards/chosen': -2.5535666942596436, 'rewards/rejected': -5.184501647949219, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6309351921081543, 'policy_logps/rejected': -458.4800720214844, 'policy_logps/chosen': -465.7192077636719, 'referece_logps/rejected': -406.6350402832031, 'referece_logps/chosen': -440.18353271484375, 'logits/rejected': -0.46828171610832214, 'logits/chosen': -0.5078954696655273, 'epoch': 2.38}


 40%|███▉      | 4257/10740 [21:18:06<32:14:40, 17.91s/it]

 40%|███▉      | 4258/10740 [21:18:26<33:16:18, 18.48s/it]
{'loss': 0.264, 'learning_rate': 1.3746935856248103e-06, 'rewards/chosen': -2.2217955589294434, 'rewards/rejected': -5.938920497894287, 'rewards/accuracies': 1.0, 'rewards/margins': 3.717125177383423, 'policy_logps/rejected': -277.6973876953125, 'policy_logps/chosen': -301.2764587402344, 'referece_logps/rejected': -218.30819702148438, 'referece_logps/chosen': -279.0585021972656, 'logits/rejected': 0.0881330668926239, 'logits/chosen': 0.08081266283988953, 'epoch': 2.38}

 40%|███▉      | 4259/10740 [21:18:47<34:50:59, 19.36s/it]


 40%|███▉      | 4261/10740 [21:19:23<32:38:23, 18.14s/it]
{'loss': 0.2192, 'learning_rate': 1.373854594841905e-06, 'rewards/chosen': -3.0760905742645264, 'rewards/rejected': -7.767009735107422, 'rewards/accuracies': 1.0, 'rewards/margins': 4.690918922424316, 'policy_logps/rejected': -404.0660705566406, 'policy_logps/chosen': -536.9425048828125, 'referece_logps/rejected': -326.3959655761719, 'referece_logps/chosen': -506.181640625, 'logits/rejected': 0.1316153109073639, 'logits/chosen': 0.23835758864879608, 'epoch': 2.38}


 40%|███▉      | 4263/10740 [21:20:03<34:55:59, 19.42s/it]
[2024-04-02 16:33:21,203] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4264/10740 [21:20:17<32:11:04, 17.89s/it]
[2024-04-02 16:33:35,536] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4265/10740 [21:20:37<32:53:54, 18.29s/it]
{'loss': 0.264, 'learning_rate': 1.3727354645415733e-06, 'rewards/chosen': -4.081790924072266, 'rewards/rejected': -6.64536714553833, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5635759830474854, 'policy_logps/rejected': -379.2234802246094, 'policy_logps/chosen': -284.74981689453125, 'referece_logps/rejected': -312.7698059082031, 'referece_logps/chosen': -243.93191528320312, 'logits/rejected': -0.42956405878067017, 'logits/chosen': -0.48963624238967896, 'epoch': 2.38}

 40%|███▉      | 4266/10740 [21:20:54<32:11:47, 17.90s/it]

 40%|███▉      | 4267/10740 [21:21:12<32:28:51, 18.06s/it]

 40%|███▉      | 4268/10740 [21:21:33<34:12:54, 19.03s/it]

 40%|███▉      | 4269/10740 [21:21:53<34:42:51, 19.31s/it]
[2024-04-02 16:35:11,455] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4270/10740 [21:22:14<35:41:05, 19.86s/it]

 40%|███▉      | 4271/10740 [21:22:34<35:47:17, 19.92s/it]
[2024-04-02 16:35:52,635] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4272/10740 [21:22:59<38:13:03, 21.27s/it]
[2024-04-02 16:36:17,069] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 4273/10740 [21:23:16<35:46:48, 19.92s/it]

 40%|███▉      | 4274/10740 [21:23:33<34:18:01, 19.10s/it]

 40%|███▉      | 4275/10740 [21:23:49<32:50:49, 18.29s/it]
{'loss': 0.1658, 'learning_rate': 1.3699352692712002e-06, 'rewards/chosen': -3.311654567718506, 'rewards/rejected': -6.3207783699035645, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0091238021850586, 'policy_logps/rejected': -358.4140930175781, 'policy_logps/chosen': -489.323974609375, 'referece_logps/rejected': -295.2063293457031, 'referece_logps/chosen': -456.2074279785156, 'logits/rejected': 0.3159671127796173, 'logits/chosen': 0.308857262134552, 'epoch': 2.39}


 40%|███▉      | 4277/10740 [21:24:18<29:49:40, 16.61s/it]
{'loss': 0.2261, 'learning_rate': 1.3693748256436587e-06, 'rewards/chosen': -3.2577202320098877, 'rewards/rejected': -6.282346725463867, 'rewards/accuracies': 1.0, 'rewards/margins': 3.024625778198242, 'policy_logps/rejected': -353.0537414550781, 'policy_logps/chosen': -421.3255920410156, 'referece_logps/rejected': -290.23028564453125, 'referece_logps/chosen': -388.7484130859375, 'logits/rejected': -0.13315075635910034, 'logits/chosen': -0.06707721203565598, 'epoch': 2.39}


 40%|███▉      | 4279/10740 [21:24:51<29:22:40, 16.37s/it]
{'loss': 0.2311, 'learning_rate': 1.368814247633935e-06, 'rewards/chosen': -2.9589884281158447, 'rewards/rejected': -6.818799018859863, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8598110675811768, 'policy_logps/rejected': -375.66485595703125, 'policy_logps/chosen': -281.8429870605469, 'referece_logps/rejected': -307.47686767578125, 'referece_logps/chosen': -252.25311279296875, 'logits/rejected': -0.24040454626083374, 'logits/chosen': -0.10092581063508987, 'epoch': 2.39}

 40%|███▉      | 4280/10740 [21:25:10<30:46:44, 17.15s/it]


 40%|███▉      | 4282/10740 [21:25:50<33:02:34, 18.42s/it]

 40%|███▉      | 4283/10740 [21:26:07<32:34:51, 18.16s/it]

 40%|███▉      | 4284/10740 [21:26:27<33:15:46, 18.55s/it]

 40%|███▉      | 4285/10740 [21:26:45<33:06:11, 18.46s/it]

 40%|███▉      | 4286/10740 [21:26:58<30:04:43, 16.78s/it]

 40%|███▉      | 4287/10740 [21:27:18<31:42:40, 17.69s/it]

 40%|███▉      | 4288/10740 [21:27:33<30:40:03, 17.11s/it]

 40%|███▉      | 4289/10740 [21:27:47<28:49:38, 16.09s/it]

 40%|███▉      | 4290/10740 [21:28:05<29:39:38, 16.55s/it]
{'loss': 0.1976, 'learning_rate': 1.3657286756151416e-06, 'rewards/chosen': -2.133814573287964, 'rewards/rejected': -5.889045715332031, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7552316188812256, 'policy_logps/rejected': -284.5032958984375, 'policy_logps/chosen': -322.5481872558594, 'referece_logps/rejected': -225.61285400390625, 'referece_logps/chosen': -301.2100524902344, 'logits/rejected': -0.5916993021965027, 'logits/chosen': -0.6971495151519775, 'epoch': 2.4}

 40%|███▉      | 4291/10740 [21:28:18<27:55:37, 15.59s/it]


 40%|███▉      | 4293/10740 [21:28:51<28:39:39, 16.00s/it]
{'loss': 0.1922, 'learning_rate': 1.3648864560021934e-06, 'rewards/chosen': -2.992750406265259, 'rewards/rejected': -6.42996883392334, 'rewards/accuracies': 1.0, 'rewards/margins': 3.43721866607666, 'policy_logps/rejected': -553.9981079101562, 'policy_logps/chosen': -336.34466552734375, 'referece_logps/rejected': -489.6983947753906, 'referece_logps/chosen': -306.4171447753906, 'logits/rejected': -0.4903656840324402, 'logits/chosen': -0.4267932176589966, 'epoch': 2.4}


 40%|███▉      | 4295/10740 [21:29:33<33:15:46, 18.58s/it]
{'loss': 0.2597, 'learning_rate': 1.3643248102810811e-06, 'rewards/chosen': -3.0274226665496826, 'rewards/rejected': -6.6556525230407715, 'rewards/accuracies': 0.875, 'rewards/margins': 3.628230333328247, 'policy_logps/rejected': -451.52227783203125, 'policy_logps/chosen': -379.1946105957031, 'referece_logps/rejected': -384.96575927734375, 'referece_logps/chosen': -348.9203796386719, 'logits/rejected': -0.20928829908370972, 'logits/chosen': -0.24374499917030334, 'epoch': 2.4}

 40%|████      | 4296/10740 [21:29:48<31:24:14, 17.54s/it]

 40%|████      | 4297/10740 [21:30:00<28:28:29, 15.91s/it]


 40%|████      | 4299/10740 [21:30:27<26:00:23, 14.54s/it]

 40%|████      | 4300/10740 [21:30:47<28:56:42, 16.18s/it]
{'loss': 0.3162, 'learning_rate': 1.3629201165412969e-06, 'rewards/chosen': -3.551790952682495, 'rewards/rejected': -5.412685871124268, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8608946800231934, 'policy_logps/rejected': -381.8005065917969, 'policy_logps/chosen': -564.9107666015625, 'referece_logps/rejected': -327.67364501953125, 'referece_logps/chosen': -529.392822265625, 'logits/rejected': 0.6150816082954407, 'logits/chosen': 0.4435383081436157, 'epoch': 2.4}

 40%|████      | 4301/10740 [21:31:02<28:23:19, 15.87s/it]

 40%|████      | 4302/10740 [21:31:22<30:36:56, 17.12s/it]

 40%|████      | 4303/10740 [21:31:42<31:50:03, 17.80s/it]


 40%|████      | 4305/10740 [21:32:15<30:37:15, 17.13s/it]
{'loss': 0.3509, 'learning_rate': 1.361514597589797e-06, 'rewards/chosen': -2.7864603996276855, 'rewards/rejected': -5.595383644104004, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8089237213134766, 'policy_logps/rejected': -279.4224548339844, 'policy_logps/chosen': -358.27435302734375, 'referece_logps/rejected': -223.4686279296875, 'referece_logps/chosen': -330.4097595214844, 'logits/rejected': -0.5849969983100891, 'logits/chosen': -0.8330763578414917, 'epoch': 2.41}


 40%|████      | 4307/10740 [21:32:48<29:47:57, 16.68s/it]
{'loss': 0.2293, 'learning_rate': 1.3609521596657323e-06, 'rewards/chosen': -3.2388272285461426, 'rewards/rejected': -7.619183540344238, 'rewards/accuracies': 1.0, 'rewards/margins': 4.380356311798096, 'policy_logps/rejected': -565.75732421875, 'policy_logps/chosen': -358.81121826171875, 'referece_logps/rejected': -489.5654602050781, 'referece_logps/chosen': -326.4229736328125, 'logits/rejected': -0.3003111183643341, 'logits/chosen': -0.18584959208965302, 'epoch': 2.41}

 40%|████      | 4308/10740 [21:33:05<29:55:31, 16.75s/it]

 40%|████      | 4309/10740 [21:33:22<30:16:53, 16.95s/it]

 40%|████      | 4310/10740 [21:33:42<32:00:11, 17.92s/it]


 40%|████      | 4312/10740 [21:34:15<30:54:26, 17.31s/it]
{'loss': 0.3752, 'learning_rate': 1.359545490787335e-06, 'rewards/chosen': -4.810615062713623, 'rewards/rejected': -6.96766996383667, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1570546627044678, 'policy_logps/rejected': -397.10089111328125, 'policy_logps/chosen': -407.8159484863281, 'referece_logps/rejected': -327.4241943359375, 'referece_logps/chosen': -359.70977783203125, 'logits/rejected': -0.17385119199752808, 'logits/chosen': -0.18351849913597107, 'epoch': 2.41}


 40%|████      | 4314/10740 [21:34:56<33:54:37, 19.00s/it]
[2024-04-02 16:48:13,797] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3886, 'learning_rate': 1.3589825941460302e-06, 'rewards/chosen': -3.860323905944824, 'rewards/rejected': -6.603846073150635, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7435219287872314, 'policy_logps/rejected': -388.06219482421875, 'policy_logps/chosen': -411.1981506347656, 'referece_logps/rejected': -322.0237731933594, 'referece_logps/chosen': -372.5948791503906, 'logits/rejected': 0.3470366299152374, 'logits/chosen': 0.2941263020038605, 'epoch': 2.41}
[2024-04-02 16:48:34,349] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 40%|████      | 4316/10740 [21:35:32<32:38:28, 18.29s/it]

 40%|████      | 4317/10740 [21:35:52<33:36:40, 18.84s/it]

 40%|████      | 4318/10740 [21:36:04<29:59:57, 16.82s/it]

 40%|████      | 4319/10740 [21:36:24<31:40:40, 17.76s/it]

 40%|████      | 4320/10740 [21:36:44<32:46:38, 18.38s/it]

 40%|████      | 4321/10740 [21:37:05<34:24:37, 19.30s/it]
{'loss': 0.2291, 'learning_rate': 1.3570114287598917e-06, 'rewards/chosen': -2.2890424728393555, 'rewards/rejected': -5.377498626708984, 'rewards/accuracies': 1.0, 'rewards/margins': 3.088456392288208, 'policy_logps/rejected': -316.3475341796875, 'policy_logps/chosen': -252.05242919921875, 'referece_logps/rejected': -262.5725402832031, 'referece_logps/chosen': -229.16200256347656, 'logits/rejected': -0.6856414675712585, 'logits/chosen': -0.8073441386222839, 'epoch': 2.41}


 40%|████      | 4323/10740 [21:37:41<33:02:25, 18.54s/it]
{'loss': 0.2024, 'learning_rate': 1.356447946025734e-06, 'rewards/chosen': -2.3845326900482178, 'rewards/rejected': -6.3489460945129395, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9644129276275635, 'policy_logps/rejected': -344.87005615234375, 'policy_logps/chosen': -298.6856689453125, 'referece_logps/rejected': -281.380615234375, 'referece_logps/chosen': -274.8403625488281, 'logits/rejected': -0.18033288419246674, 'logits/chosen': -0.2992485463619232, 'epoch': 2.42}


 40%|████      | 4325/10740 [21:38:24<35:21:11, 19.84s/it]

 40%|████      | 4326/10740 [21:38:44<35:18:40, 19.82s/it]

 40%|████      | 4327/10740 [21:38:58<32:16:14, 18.12s/it]

 40%|████      | 4328/10740 [21:39:16<32:18:16, 18.14s/it]
{'loss': 0.2229, 'learning_rate': 1.3550386722921476e-06, 'rewards/chosen': -2.391063690185547, 'rewards/rejected': -5.657994270324707, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2669308185577393, 'policy_logps/rejected': -253.56568908691406, 'policy_logps/chosen': -373.89013671875, 'referece_logps/rejected': -196.98574829101562, 'referece_logps/chosen': -349.9794921875, 'logits/rejected': 0.10628138482570648, 'logits/chosen': -0.01702379435300827, 'epoch': 2.42}

 40%|████      | 4329/10740 [21:39:35<32:38:58, 18.33s/it]

 40%|████      | 4330/10740 [21:39:50<31:04:18, 17.45s/it]


 40%|████      | 4332/10740 [21:40:32<33:47:03, 18.98s/it]
{'loss': 0.1859, 'learning_rate': 1.3539106719020278e-06, 'rewards/chosen': -2.778665781021118, 'rewards/rejected': -6.607113838195801, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8284482955932617, 'policy_logps/rejected': -303.3916015625, 'policy_logps/chosen': -328.061279296875, 'referece_logps/rejected': -237.32046508789062, 'referece_logps/chosen': -300.2746276855469, 'logits/rejected': 0.09085805714130402, 'logits/chosen': 0.08073408901691437, 'epoch': 2.42}


 40%|████      | 4334/10740 [21:41:01<30:14:21, 16.99s/it]

 40%|████      | 4335/10740 [21:41:22<31:55:31, 17.94s/it]
{'loss': 0.1904, 'learning_rate': 1.35306433353472e-06, 'rewards/chosen': -2.3624889850616455, 'rewards/rejected': -5.526349067687988, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1638598442077637, 'policy_logps/rejected': -302.22320556640625, 'policy_logps/chosen': -450.06231689453125, 'referece_logps/rejected': -246.959716796875, 'referece_logps/chosen': -426.4374694824219, 'logits/rejected': -0.2160559594631195, 'logits/chosen': -0.20487861335277557, 'epoch': 2.42}

 40%|████      | 4336/10740 [21:41:35<29:24:18, 16.53s/it]


 40%|████      | 4338/10740 [21:42:04<27:33:12, 15.49s/it]

 40%|████      | 4339/10740 [21:42:24<29:50:34, 16.78s/it]

 40%|████      | 4340/10740 [21:42:44<31:23:21, 17.66s/it]

 40%|████      | 4341/10740 [21:43:02<31:44:08, 17.85s/it]
{'loss': 0.2753, 'learning_rate': 1.3513707904673744e-06, 'rewards/chosen': -3.4607455730438232, 'rewards/rejected': -5.951686382293701, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4909415245056152, 'policy_logps/rejected': -527.5797119140625, 'policy_logps/chosen': -447.8928527832031, 'referece_logps/rejected': -468.06280517578125, 'referece_logps/chosen': -413.285400390625, 'logits/rejected': -0.2953343391418457, 'logits/chosen': -0.09708297252655029, 'epoch': 2.43}

 40%|████      | 4342/10740 [21:43:14<28:54:22, 16.26s/it]

 40%|████      | 4343/10740 [21:43:33<29:59:58, 16.88s/it]

 40%|████      | 4344/10740 [21:43:53<31:56:29, 17.98s/it]

 40%|████      | 4345/10740 [21:44:09<30:45:21, 17.31s/it]


 40%|████      | 4347/10740 [21:44:48<32:27:53, 18.28s/it]

 40%|████      | 4348/10740 [21:45:06<32:16:01, 18.17s/it]

 40%|████      | 4349/10740 [21:45:27<33:44:27, 19.01s/it]

 41%|████      | 4350/10740 [21:45:47<34:16:51, 19.31s/it]
{'loss': 0.2566, 'learning_rate': 1.3488283204334477e-06, 'rewards/chosen': -3.499108076095581, 'rewards/rejected': -7.131131172180176, 'rewards/accuracies': 0.875, 'rewards/margins': 3.632023334503174, 'policy_logps/rejected': -671.563232421875, 'policy_logps/chosen': -404.0853576660156, 'referece_logps/rejected': -600.251953125, 'referece_logps/chosen': -369.0942687988281, 'logits/rejected': -0.07931645214557648, 'logits/chosen': 0.2947627902030945, 'epoch': 2.43}


 41%|████      | 4352/10740 [21:46:24<33:17:13, 18.76s/it]
{'loss': 0.172, 'learning_rate': 1.348262977438239e-06, 'rewards/chosen': -2.6514334678649902, 'rewards/rejected': -6.694424629211426, 'rewards/accuracies': 1.0, 'rewards/margins': 4.0429911613464355, 'policy_logps/rejected': -375.2840881347656, 'policy_logps/chosen': -389.6759338378906, 'referece_logps/rejected': -308.33984375, 'referece_logps/chosen': -363.16156005859375, 'logits/rejected': -0.15326054394245148, 'logits/chosen': -0.1606869250535965, 'epoch': 2.43}

 41%|████      | 4353/10740 [21:46:41<32:30:20, 18.32s/it]

 41%|████      | 4354/10740 [21:46:59<32:17:06, 18.20s/it]

 41%|████      | 4355/10740 [21:47:17<32:07:35, 18.11s/it]

 41%|████      | 4356/10740 [21:47:38<33:41:08, 19.00s/it]

 41%|████      | 4357/10740 [21:47:53<31:25:42, 17.73s/it]


 41%|████      | 4359/10740 [21:48:31<32:32:03, 18.36s/it]

 41%|████      | 4360/10740 [21:48:51<33:29:59, 18.90s/it]

 41%|████      | 4361/10740 [21:49:10<33:55:45, 19.15s/it]
{'loss': 0.194, 'learning_rate': 1.3457173689994747e-06, 'rewards/chosen': -2.7880923748016357, 'rewards/rejected': -6.589256286621094, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8011634349823, 'policy_logps/rejected': -411.4802551269531, 'policy_logps/chosen': -524.037109375, 'referece_logps/rejected': -345.587646484375, 'referece_logps/chosen': -496.1561584472656, 'logits/rejected': -0.4365065097808838, 'logits/chosen': -0.4719633162021637, 'epoch': 2.44}

 41%|████      | 4362/10740 [21:49:24<30:59:34, 17.49s/it]

 41%|████      | 4363/10740 [21:49:42<31:18:35, 17.68s/it]

 41%|████      | 4364/10740 [21:49:59<31:01:33, 17.52s/it]


 41%|████      | 4366/10740 [21:50:39<33:04:29, 18.68s/it]

 41%|████      | 4367/10740 [21:50:51<29:22:52, 16.60s/it]
{'loss': 0.2194, 'learning_rate': 1.3440188805764886e-06, 'rewards/chosen': -2.8847970962524414, 'rewards/rejected': -6.1791863441467285, 'rewards/accuracies': 0.75, 'rewards/margins': 3.294389247894287, 'policy_logps/rejected': -439.1468200683594, 'policy_logps/chosen': -256.3272705078125, 'referece_logps/rejected': -377.3549499511719, 'referece_logps/chosen': -227.47930908203125, 'logits/rejected': -0.7428652048110962, 'logits/chosen': -0.6643904447555542, 'epoch': 2.44}

 41%|████      | 4368/10740 [21:51:10<31:05:29, 17.57s/it]

 41%|████      | 4369/10740 [21:51:30<32:16:51, 18.24s/it]

 41%|████      | 4370/10740 [21:51:41<28:16:16, 15.98s/it]

 41%|████      | 4371/10740 [21:52:00<29:42:31, 16.79s/it]


 41%|████      | 4373/10740 [21:52:41<33:23:05, 18.88s/it]
{'loss': 0.2076, 'learning_rate': 1.3423192657368098e-06, 'rewards/chosen': -3.679121255874634, 'rewards/rejected': -6.020242691040039, 'rewards/accuracies': 0.75, 'rewards/margins': 2.341120958328247, 'policy_logps/rejected': -314.3874816894531, 'policy_logps/chosen': -275.9541931152344, 'referece_logps/rejected': -254.18505859375, 'referece_logps/chosen': -239.16297912597656, 'logits/rejected': -0.8532661199569702, 'logits/chosen': -0.8547271490097046, 'epoch': 2.44}

 41%|████      | 4374/10740 [21:53:02<34:29:54, 19.51s/it]

 41%|████      | 4375/10740 [21:53:19<33:13:49, 18.79s/it]


 41%|████      | 4377/10740 [21:53:59<33:52:41, 19.17s/it]
{'loss': 0.261, 'learning_rate': 1.3411855661379835e-06, 'rewards/chosen': -3.3328306674957275, 'rewards/rejected': -5.912091255187988, 'rewards/accuracies': 0.875, 'rewards/margins': 2.579261064529419, 'policy_logps/rejected': -359.4893798828125, 'policy_logps/chosen': -341.0262145996094, 'referece_logps/rejected': -300.36846923828125, 'referece_logps/chosen': -307.6979064941406, 'logits/rejected': 0.38478636741638184, 'logits/chosen': 0.33025115728378296, 'epoch': 2.45}

 41%|████      | 4378/10740 [21:54:16<32:56:02, 18.64s/it]

 41%|████      | 4379/10740 [21:54:34<32:40:08, 18.49s/it]

 41%|████      | 4380/10740 [21:54:48<30:14:39, 17.12s/it]

 41%|████      | 4381/10740 [21:55:02<28:36:55, 16.20s/it]


 41%|████      | 4383/10740 [21:55:37<30:00:16, 16.99s/it]

 41%|████      | 4384/10740 [21:55:55<30:38:34, 17.36s/it]
{'loss': 0.3506, 'learning_rate': 1.3392003981139774e-06, 'rewards/chosen': -4.38485860824585, 'rewards/rejected': -5.333216667175293, 'rewards/accuracies': 0.5, 'rewards/margins': 0.9483577609062195, 'policy_logps/rejected': -394.45672607421875, 'policy_logps/chosen': -381.4322814941406, 'referece_logps/rejected': -341.1245422363281, 'referece_logps/chosen': -337.58367919921875, 'logits/rejected': 0.28078925609588623, 'logits/chosen': 0.10594287514686584, 'epoch': 2.45}

 41%|████      | 4385/10740 [21:56:10<29:19:49, 16.62s/it]


 41%|████      | 4387/10740 [21:56:37<26:03:55, 14.77s/it]
{'loss': 0.2913, 'learning_rate': 1.3383491485354097e-06, 'rewards/chosen': -3.415757179260254, 'rewards/rejected': -5.459808826446533, 'rewards/accuracies': 0.625, 'rewards/margins': 2.0440518856048584, 'policy_logps/rejected': -417.664306640625, 'policy_logps/chosen': -347.566650390625, 'referece_logps/rejected': -363.06622314453125, 'referece_logps/chosen': -313.4090576171875, 'logits/rejected': 0.06358297169208527, 'logits/chosen': 0.17451974749565125, 'epoch': 2.45}

 41%|████      | 4388/10740 [21:56:48<24:21:05, 13.80s/it]

 41%|████      | 4389/10740 [21:57:02<24:27:36, 13.86s/it]


 41%|████      | 4391/10740 [21:57:43<30:14:40, 17.15s/it]
{'loss': 0.248, 'learning_rate': 1.3372137183862261e-06, 'rewards/chosen': -2.7054147720336914, 'rewards/rejected': -7.823044300079346, 'rewards/accuracies': 1.0, 'rewards/margins': 5.117629051208496, 'policy_logps/rejected': -442.32415771484375, 'policy_logps/chosen': -508.6255187988281, 'referece_logps/rejected': -364.09375, 'referece_logps/chosen': -481.5713806152344, 'logits/rejected': -0.12414170801639557, 'logits/chosen': -0.22345608472824097, 'epoch': 2.45}

 41%|████      | 4392/10740 [21:58:03<31:36:47, 17.93s/it]

 41%|████      | 4393/10740 [21:58:24<33:05:37, 18.77s/it]

 41%|████      | 4394/10740 [21:58:42<32:41:22, 18.54s/it]

 41%|████      | 4395/10740 [21:59:01<33:23:12, 18.94s/it]

 41%|████      | 4396/10740 [21:59:21<33:41:46, 19.12s/it]


 41%|████      | 4398/10740 [21:59:57<32:16:40, 18.32s/it]
{'loss': 0.2666, 'learning_rate': 1.3352255358087023e-06, 'rewards/chosen': -3.035886287689209, 'rewards/rejected': -6.230920314788818, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1950340270996094, 'policy_logps/rejected': -388.34686279296875, 'policy_logps/chosen': -528.9776000976562, 'referece_logps/rejected': -326.03765869140625, 'referece_logps/chosen': -498.6187744140625, 'logits/rejected': -0.40962454676628113, 'logits/chosen': -0.5761907696723938, 'epoch': 2.46}

 41%|████      | 4399/10740 [22:00:19<33:52:42, 19.23s/it]


 41%|████      | 4401/10740 [22:00:52<31:10:47, 17.71s/it]

 41%|████      | 4402/10740 [22:01:09<31:08:31, 17.69s/it]
{'loss': 0.3029, 'learning_rate': 1.3340887601394739e-06, 'rewards/chosen': -2.3417296409606934, 'rewards/rejected': -3.945119857788086, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6033902168273926, 'policy_logps/rejected': -262.53173828125, 'policy_logps/chosen': -414.3216247558594, 'referece_logps/rejected': -223.08055114746094, 'referece_logps/chosen': -390.90435791015625, 'logits/rejected': -0.4437406361103058, 'logits/chosen': -0.5060522556304932, 'epoch': 2.46}


 41%|████      | 4404/10740 [22:01:50<33:17:44, 18.92s/it]
{'loss': 0.2125, 'learning_rate': 1.3335201898843078e-06, 'rewards/chosen': -4.3036112785339355, 'rewards/rejected': -7.17810583114624, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8744940757751465, 'policy_logps/rejected': -482.4568176269531, 'policy_logps/chosen': -361.1351013183594, 'referece_logps/rejected': -410.6757507324219, 'referece_logps/chosen': -318.0989990234375, 'logits/rejected': 0.0917893648147583, 'logits/chosen': 0.20441439747810364, 'epoch': 2.46}

 41%|████      | 4405/10740 [22:02:07<32:19:32, 18.37s/it]

 41%|████      | 4406/10740 [22:02:26<32:39:40, 18.56s/it]


 41%|████      | 4408/10740 [22:02:47<25:48:27, 14.67s/it]
{'loss': 0.1842, 'learning_rate': 1.332382685567129e-06, 'rewards/chosen': -2.8701300621032715, 'rewards/rejected': -7.001124858856201, 'rewards/accuracies': 1.0, 'rewards/margins': 4.13099479675293, 'policy_logps/rejected': -259.0052490234375, 'policy_logps/chosen': -237.92514038085938, 'referece_logps/rejected': -188.99400329589844, 'referece_logps/chosen': -209.22381591796875, 'logits/rejected': -0.27523577213287354, 'logits/chosen': -0.18721485137939453, 'epoch': 2.46}

 41%|████      | 4409/10740 [22:03:05<27:08:57, 15.44s/it]

 41%|████      | 4410/10740 [22:03:19<26:17:36, 14.95s/it]


 41%|████      | 4412/10740 [22:03:48<26:03:38, 14.83s/it]
{'loss': 0.3417, 'learning_rate': 1.3312446975536789e-06, 'rewards/chosen': -2.816612720489502, 'rewards/rejected': -4.286318778991699, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4697062969207764, 'policy_logps/rejected': -271.6797790527344, 'policy_logps/chosen': -508.4367370605469, 'referece_logps/rejected': -228.8166046142578, 'referece_logps/chosen': -480.27056884765625, 'logits/rejected': -0.22157585620880127, 'logits/chosen': -0.5598674416542053, 'epoch': 2.46}

 41%|████      | 4413/10740 [22:04:08<28:57:19, 16.48s/it]

 41%|████      | 4414/10740 [22:04:20<26:49:00, 15.26s/it]

 41%|████      | 4415/10740 [22:04:40<29:17:32, 16.67s/it]

 41%|████      | 4416/10740 [22:04:54<27:55:39, 15.90s/it]

 41%|████      | 4417/10740 [22:05:14<29:57:18, 17.05s/it]

 41%|████      | 4418/10740 [22:05:31<29:38:44, 16.88s/it]

 41%|████      | 4419/10740 [22:05:49<30:16:01, 17.24s/it]


 41%|████      | 4421/10740 [22:06:26<31:19:27, 17.85s/it]
{'loss': 0.1898, 'learning_rate': 1.328682464587853e-06, 'rewards/chosen': -3.315337896347046, 'rewards/rejected': -7.065705299377441, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7503671646118164, 'policy_logps/rejected': -283.64678955078125, 'policy_logps/chosen': -376.03863525390625, 'referece_logps/rejected': -212.98974609375, 'referece_logps/chosen': -342.88525390625, 'logits/rejected': -0.039426982402801514, 'logits/chosen': -0.07228022813796997, 'epoch': 2.47}


 41%|████      | 4423/10740 [22:07:04<32:30:56, 18.53s/it]
{'loss': 0.1276, 'learning_rate': 1.3281127499803481e-06, 'rewards/chosen': -3.895704507827759, 'rewards/rejected': -7.113001346588135, 'rewards/accuracies': 0.875, 'rewards/margins': 3.217297077178955, 'policy_logps/rejected': -447.7867431640625, 'policy_logps/chosen': -434.09881591796875, 'referece_logps/rejected': -376.65673828125, 'referece_logps/chosen': -395.14178466796875, 'logits/rejected': -0.10035423934459686, 'logits/chosen': -0.16651594638824463, 'epoch': 2.47}

 41%|████      | 4424/10740 [22:07:15<28:40:03, 16.34s/it]

 41%|████      | 4425/10740 [22:07:35<30:40:25, 17.49s/it]

 41%|████      | 4426/10740 [22:07:50<29:15:32, 16.68s/it]

 41%|████      | 4427/10740 [22:08:10<31:00:02, 17.68s/it]

 41%|████      | 4428/10740 [22:08:31<32:26:30, 18.50s/it]

 41%|████      | 4429/10740 [22:08:50<33:02:07, 18.84s/it]

 41%|████      | 4430/10740 [22:09:05<30:53:16, 17.62s/it]

 41%|████▏     | 4431/10740 [22:09:25<32:12:42, 18.38s/it]

 41%|████▏     | 4432/10740 [22:09:45<32:55:03, 18.79s/it]

 41%|████▏     | 4433/10740 [22:09:57<29:22:52, 16.77s/it]


 41%|████▏     | 4435/10740 [22:10:34<31:05:35, 17.75s/it]
{'loss': 0.2146, 'learning_rate': 1.3246919628094036e-06, 'rewards/chosen': -3.6800522804260254, 'rewards/rejected': -6.996798038482666, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3167457580566406, 'policy_logps/rejected': -376.3819885253906, 'policy_logps/chosen': -357.49462890625, 'referece_logps/rejected': -306.41400146484375, 'referece_logps/chosen': -320.6941223144531, 'logits/rejected': -0.6595668792724609, 'logits/chosen': -0.8294796943664551, 'epoch': 2.48}

 41%|████▏     | 4436/10740 [22:10:51<30:30:58, 17.43s/it]


 41%|████▏     | 4438/10740 [22:11:20<28:04:40, 16.04s/it]
{'loss': 0.1687, 'learning_rate': 1.3238360998066287e-06, 'rewards/chosen': -3.110262870788574, 'rewards/rejected': -6.9512481689453125, 'rewards/accuracies': 1.0, 'rewards/margins': 3.840985059738159, 'policy_logps/rejected': -367.81024169921875, 'policy_logps/chosen': -335.20489501953125, 'referece_logps/rejected': -298.29779052734375, 'referece_logps/chosen': -304.102294921875, 'logits/rejected': -0.07948917150497437, 'logits/chosen': -0.0065686702728271484, 'epoch': 2.48}

 41%|████▏     | 4439/10740 [22:11:39<29:40:40, 16.96s/it]

 41%|████▏     | 4440/10740 [22:11:51<27:08:16, 15.51s/it]

 41%|████▏     | 4441/10740 [22:12:12<29:39:58, 16.95s/it]


 41%|████▏     | 4443/10740 [22:12:40<27:10:24, 15.54s/it]
{'loss': 0.2713, 'learning_rate': 1.3224090727410241e-06, 'rewards/chosen': -2.3120229244232178, 'rewards/rejected': -7.093646049499512, 'rewards/accuracies': 0.875, 'rewards/margins': 4.781623363494873, 'policy_logps/rejected': -387.31646728515625, 'policy_logps/chosen': -425.17449951171875, 'referece_logps/rejected': -316.3800048828125, 'referece_logps/chosen': -402.0543212890625, 'logits/rejected': -0.7585052251815796, 'logits/chosen': -0.6846728920936584, 'epoch': 2.48}

 41%|████▏     | 4444/10740 [22:12:57<27:37:08, 15.79s/it]

 41%|████▏     | 4445/10740 [22:13:17<30:07:37, 17.23s/it]

 41%|████▏     | 4446/10740 [22:13:37<31:25:46, 17.98s/it]

 41%|████▏     | 4447/10740 [22:13:57<32:31:52, 18.61s/it]


 41%|████▏     | 4449/10740 [22:14:36<33:19:25, 19.07s/it]
{'loss': 0.241, 'learning_rate': 1.3206956728595704e-06, 'rewards/chosen': -3.3191723823547363, 'rewards/rejected': -6.647824764251709, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3286519050598145, 'policy_logps/rejected': -496.38946533203125, 'policy_logps/chosen': -368.0934143066406, 'referece_logps/rejected': -429.9111633300781, 'referece_logps/chosen': -334.9017333984375, 'logits/rejected': 0.8354085683822632, 'logits/chosen': 0.9063122272491455, 'epoch': 2.49}

 41%|████▏     | 4450/10740 [22:14:47<29:00:53, 16.61s/it]

 41%|████▏     | 4451/10740 [22:15:05<29:52:07, 17.10s/it]

 41%|████▏     | 4452/10740 [22:15:25<31:13:13, 17.87s/it]

 41%|████▏     | 4453/10740 [22:15:44<32:08:34, 18.41s/it]


 41%|████▏     | 4455/10740 [22:16:10<27:06:29, 15.53s/it]

 41%|████▏     | 4456/10740 [22:16:30<29:21:05, 16.81s/it]
{'loss': 0.2529, 'learning_rate': 1.3186953795790388e-06, 'rewards/chosen': -3.982180595397949, 'rewards/rejected': -8.26071548461914, 'rewards/accuracies': 0.875, 'rewards/margins': 4.278534889221191, 'policy_logps/rejected': -345.64337158203125, 'policy_logps/chosen': -348.508544921875, 'referece_logps/rejected': -263.0362243652344, 'referece_logps/chosen': -308.68670654296875, 'logits/rejected': 0.06352464854717255, 'logits/chosen': -0.03758290410041809, 'epoch': 2.49}

 41%|████▏     | 4457/10740 [22:16:48<29:48:53, 17.08s/it]

 42%|████▏     | 4458/10740 [22:17:08<31:17:22, 17.93s/it]

 42%|████▏     | 4459/10740 [22:17:24<30:18:59, 17.38s/it]

 42%|████▏     | 4460/10740 [22:17:45<32:20:18, 18.54s/it]

 42%|████▏     | 4461/10740 [22:17:59<29:51:49, 17.12s/it]

 42%|████▏     | 4462/10740 [22:18:16<29:51:59, 17.13s/it]

 42%|████▏     | 4463/10740 [22:18:35<30:28:45, 17.48s/it]

 42%|████▏     | 4464/10740 [22:18:54<31:31:54, 18.09s/it]

 42%|████▏     | 4465/10740 [22:19:06<28:15:23, 16.21s/it]

 42%|████▏     | 4466/10740 [22:19:24<29:08:12, 16.72s/it]

 42%|████▏     | 4467/10740 [22:19:36<26:47:31, 15.38s/it]

 42%|████▏     | 4468/10740 [22:19:55<28:32:31, 16.38s/it]

 42%|████▏     | 4469/10740 [22:20:12<29:10:54, 16.75s/it]

 42%|████▏     | 4470/10740 [22:20:33<31:13:55, 17.93s/it]

 42%|████▏     | 4471/10740 [22:20:51<31:30:31, 18.09s/it]

 42%|████▏     | 4472/10740 [22:21:08<30:47:31, 17.69s/it]

 42%|████▏     | 4473/10740 [22:21:25<30:32:16, 17.54s/it]

 42%|████▏     | 4474/10740 [22:21:44<31:13:53, 17.94s/it]

 42%|████▏     | 4475/10740 [22:22:04<32:12:17, 18.51s/it]

 42%|████▏     | 4476/10740 [22:22:21<31:08:29, 17.90s/it]

 42%|████▏     | 4477/10740 [22:22:33<28:29:56, 16.38s/it]

 42%|████▏     | 4478/10740 [22:22:53<30:13:54, 17.38s/it]

 42%|████▏     | 4479/10740 [22:23:05<27:33:17, 15.84s/it]

 42%|████▏     | 4480/10740 [22:23:24<29:10:03, 16.77s/it]

 42%|████▏     | 4481/10740 [22:23:44<30:41:17, 17.65s/it]

 42%|████▏     | 4482/10740 [22:24:04<31:47:40, 18.29s/it]

 42%|████▏     | 4483/10740 [22:24:25<33:22:59, 19.21s/it]

 42%|████▏     | 4484/10740 [22:24:45<33:35:02, 19.33s/it]

 42%|████▏     | 4485/10740 [22:25:04<33:41:12, 19.39s/it]

 42%|████▏     | 4486/10740 [22:25:27<35:17:55, 20.32s/it]

 42%|████▏     | 4487/10740 [22:25:41<32:08:08, 18.50s/it]

 42%|████▏     | 4488/10740 [22:25:58<31:31:34, 18.15s/it]

 42%|████▏     | 4489/10740 [22:26:10<28:14:49, 16.27s/it]

 42%|████▏     | 4490/10740 [22:26:26<27:41:52, 15.95s/it]

 42%|████▏     | 4491/10740 [22:26:41<27:29:20, 15.84s/it]

 42%|████▏     | 4492/10740 [22:26:56<26:50:35, 15.47s/it]

 42%|████▏     | 4493/10740 [22:27:06<24:15:55, 13.98s/it]

 42%|████▏     | 4494/10740 [22:27:22<25:24:59, 14.65s/it]

 42%|████▏     | 4495/10740 [22:27:35<24:06:05, 13.89s/it]

 42%|████▏     | 4496/10740 [22:27:47<23:36:32, 13.61s/it]

 42%|████▏     | 4497/10740 [22:28:09<27:45:02, 16.00s/it]

 42%|████▏     | 4498/10740 [22:28:30<30:27:52, 17.57s/it]

 42%|████▏     | 4499/10740 [22:28:43<27:41:03, 15.97s/it]

 42%|████▏     | 4500/10740 [22:29:03<29:51:26, 17.23s/it]

 42%|████▏     | 4501/10740 [22:29:38<39:28:52, 22.78s/it]

 42%|████▏     | 4502/10740 [22:29:55<36:21:37, 20.98s/it]

 42%|████▏     | 4503/10740 [22:30:15<35:53:25, 20.72s/it]

 42%|████▏     | 4504/10740 [22:30:31<33:10:51, 19.16s/it]

 42%|████▏     | 4505/10740 [22:30:47<31:41:25, 18.30s/it]

 42%|████▏     | 4506/10740 [22:31:01<29:12:21, 16.87s/it]

 42%|████▏     | 4507/10740 [22:31:21<30:46:07, 17.77s/it]

 42%|████▏     | 4508/10740 [22:31:38<30:26:54, 17.59s/it]

 42%|████▏     | 4509/10740 [22:31:58<31:50:29, 18.40s/it]

 42%|████▏     | 4510/10740 [22:32:15<31:20:01, 18.11s/it]

 42%|████▏     | 4511/10740 [22:32:32<30:42:24, 17.75s/it]

 42%|████▏     | 4512/10740 [22:32:52<31:47:41, 18.38s/it]

 42%|████▏     | 4513/10740 [22:33:04<28:29:10, 16.47s/it]

 42%|████▏     | 4514/10740 [22:33:21<28:33:37, 16.51s/it]

 42%|████▏     | 4515/10740 [22:33:33<26:24:31, 15.27s/it]

 42%|████▏     | 4516/10740 [22:33:52<28:12:38, 16.32s/it]

 42%|████▏     | 4517/10740 [22:34:10<28:56:57, 16.75s/it]

 42%|████▏     | 4518/10740 [22:34:24<27:43:13, 16.04s/it]

 42%|████▏     | 4519/10740 [22:34:41<28:12:29, 16.32s/it]

 42%|████▏     | 4520/10740 [22:34:59<29:14:26, 16.92s/it]

 42%|████▏     | 4521/10740 [22:35:20<30:54:14, 17.89s/it]

 42%|████▏     | 4522/10740 [22:35:36<30:08:21, 17.45s/it]

 42%|████▏     | 4523/10740 [22:35:56<31:23:42, 18.18s/it]

 42%|████▏     | 4524/10740 [22:36:17<33:00:13, 19.11s/it]

 42%|████▏     | 4525/10740 [22:36:34<31:38:38, 18.33s/it]

 42%|████▏     | 4526/10740 [22:36:53<32:15:54, 18.69s/it]

 42%|████▏     | 4527/10740 [22:37:09<30:52:57, 17.89s/it]

 42%|████▏     | 4528/10740 [22:37:20<27:17:49, 15.82s/it]


 42%|████▏     | 4530/10740 [22:37:44<23:54:11, 13.86s/it]

 42%|████▏     | 4531/10740 [22:38:02<26:03:53, 15.11s/it]

 42%|████▏     | 4532/10740 [22:38:20<27:32:35, 15.97s/it]

 42%|████▏     | 4533/10740 [22:38:40<29:41:09, 17.22s/it]

 42%|████▏     | 4534/10740 [22:39:00<30:51:41, 17.90s/it]

 42%|████▏     | 4535/10740 [22:39:20<32:11:43, 18.68s/it]

 42%|████▏     | 4536/10740 [22:39:37<31:19:34, 18.18s/it]

 42%|████▏     | 4537/10740 [22:39:57<32:03:27, 18.61s/it]

 42%|████▏     | 4538/10740 [22:40:13<31:00:00, 17.99s/it]

 42%|████▏     | 4539/10740 [22:40:33<32:03:23, 18.61s/it]

 42%|████▏     | 4540/10740 [22:40:47<29:16:23, 17.00s/it]

 42%|████▏     | 4541/10740 [22:41:05<29:58:38, 17.41s/it]

 42%|████▏     | 4542/10740 [22:41:24<30:35:20, 17.77s/it]

 42%|████▏     | 4543/10740 [22:41:45<32:40:22, 18.98s/it]

 42%|████▏     | 4544/10740 [22:42:08<34:25:43, 20.00s/it]

 42%|████▏     | 4545/10740 [22:42:28<34:27:00, 20.02s/it]

 42%|████▏     | 4546/10740 [22:42:49<35:05:34, 20.40s/it]

 42%|████▏     | 4547/10740 [22:43:07<33:38:55, 19.56s/it]

 42%|████▏     | 4548/10740 [22:43:26<33:38:54, 19.56s/it]

 42%|████▏     | 4549/10740 [22:43:46<33:46:00, 19.64s/it]

 42%|████▏     | 4550/10740 [22:43:59<30:22:16, 17.66s/it]

 42%|████▏     | 4551/10740 [22:44:17<30:33:27, 17.77s/it]

 42%|████▏     | 4552/10740 [22:44:31<28:33:22, 16.61s/it]

 42%|████▏     | 4553/10740 [22:44:46<27:43:10, 16.13s/it]

 42%|████▏     | 4554/10740 [22:45:04<28:46:31, 16.75s/it]

 42%|████▏     | 4555/10740 [22:45:24<30:11:52, 17.58s/it]

 42%|████▏     | 4556/10740 [22:45:42<30:45:20, 17.90s/it]

 42%|████▏     | 4557/10740 [22:45:54<27:42:16, 16.13s/it]

 42%|████▏     | 4558/10740 [22:46:14<29:30:57, 17.19s/it]
[2024-04-02 17:59:32,301] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 4559/10740 [22:46:27<27:21:31, 15.93s/it]

 42%|████▏     | 4560/10740 [22:46:49<30:36:16, 17.83s/it]

 42%|████▏     | 4561/10740 [22:47:09<31:35:40, 18.41s/it]

 42%|████▏     | 4562/10740 [22:47:31<33:20:57, 19.43s/it]

 42%|████▏     | 4563/10740 [22:47:50<33:14:48, 19.38s/it]

 42%|████▏     | 4564/10740 [22:48:12<34:22:12, 20.03s/it]

 43%|████▎     | 4565/10740 [22:48:26<31:37:48, 18.44s/it]

 43%|████▎     | 4566/10740 [22:48:43<30:53:24, 18.01s/it]

 43%|████▎     | 4567/10740 [22:49:03<31:48:49, 18.55s/it]

 43%|████▎     | 4568/10740 [22:49:21<31:14:32, 18.22s/it]
{'loss': 0.1376, 'learning_rate': 1.2865036046011622e-06, 'rewards/chosen': -4.317605018615723, 'rewards/rejected': -8.112923622131348, 'rewards/accuracies': 1.0, 'rewards/margins': 3.795318365097046, 'policy_logps/rejected': -575.005126953125, 'policy_logps/chosen': -424.4251708984375, 'referece_logps/rejected': -493.87591552734375, 'referece_logps/chosen': -381.2491149902344, 'logits/rejected': 0.035856302827596664, 'logits/chosen': 0.11629185825586319, 'epoch': 2.55}


 43%|████▎     | 4570/10740 [22:49:44<25:36:56, 14.95s/it]

 43%|████▎     | 4571/10740 [22:49:55<23:24:18, 13.66s/it]
{'loss': 0.1199, 'learning_rate': 1.2856366655903796e-06, 'rewards/chosen': -4.243471622467041, 'rewards/rejected': -8.091910362243652, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8484387397766113, 'policy_logps/rejected': -256.6406555175781, 'policy_logps/chosen': -468.23846435546875, 'referece_logps/rejected': -175.72154235839844, 'referece_logps/chosen': -425.80377197265625, 'logits/rejected': -0.4015721380710602, 'logits/chosen': -0.12792527675628662, 'epoch': 2.55}


 43%|████▎     | 4573/10740 [22:50:22<24:05:35, 14.06s/it]
{'loss': 0.235, 'learning_rate': 1.2850585763092766e-06, 'rewards/chosen': -3.0534579753875732, 'rewards/rejected': -7.977745056152344, 'rewards/accuracies': 1.0, 'rewards/margins': 4.924287796020508, 'policy_logps/rejected': -389.54132080078125, 'policy_logps/chosen': -346.7232666015625, 'referece_logps/rejected': -309.76385498046875, 'referece_logps/chosen': -316.188720703125, 'logits/rejected': 0.5423367023468018, 'logits/chosen': 0.5919132828712463, 'epoch': 2.55}


 43%|████▎     | 4575/10740 [22:50:55<25:40:07, 14.99s/it]

 43%|████▎     | 4576/10740 [22:51:16<28:34:22, 16.69s/it]

 43%|████▎     | 4577/10740 [22:51:39<32:03:51, 18.73s/it]

 43%|████▎     | 4578/10740 [22:51:58<31:59:38, 18.69s/it]

 43%|████▎     | 4579/10740 [22:52:19<33:07:19, 19.35s/it]

 43%|████▎     | 4580/10740 [22:52:39<33:42:10, 19.70s/it]
[2024-04-02 18:05:57,400] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4581/10740 [22:52:58<33:18:33, 19.47s/it]

 43%|████▎     | 4582/10740 [22:53:18<33:24:19, 19.53s/it]
{'loss': 0.2044, 'learning_rate': 1.282455894206199e-06, 'rewards/chosen': -4.147980690002441, 'rewards/rejected': -6.504288673400879, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3563079833984375, 'policy_logps/rejected': -337.30291748046875, 'policy_logps/chosen': -379.822998046875, 'referece_logps/rejected': -272.2600402832031, 'referece_logps/chosen': -338.34320068359375, 'logits/rejected': -0.4717189073562622, 'logits/chosen': -0.5228018164634705, 'epoch': 2.56}


 43%|████▎     | 4584/10740 [22:53:49<29:27:44, 17.23s/it]
{'loss': 0.1972, 'learning_rate': 1.2818772371396758e-06, 'rewards/chosen': -2.7724995613098145, 'rewards/rejected': -5.9942731857299805, 'rewards/accuracies': 0.875, 'rewards/margins': 3.221773862838745, 'policy_logps/rejected': -298.00537109375, 'policy_logps/chosen': -381.70379638671875, 'referece_logps/rejected': -238.06263732910156, 'referece_logps/chosen': -353.97882080078125, 'logits/rejected': 0.7273168563842773, 'logits/chosen': 0.8206222057342529, 'epoch': 2.56}


 43%|████▎     | 4586/10740 [22:54:25<30:58:29, 18.12s/it]

 43%|████▎     | 4587/10740 [22:54:48<33:12:56, 19.43s/it]
[2024-04-02 18:08:06,075] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2199, 'learning_rate': 1.2810090593249983e-06, 'rewards/chosen': -2.6382946968078613, 'rewards/rejected': -7.037327289581299, 'rewards/accuracies': 1.0, 'rewards/margins': 4.3990325927734375, 'policy_logps/rejected': -481.0565490722656, 'policy_logps/chosen': -541.4464721679688, 'referece_logps/rejected': -410.68328857421875, 'referece_logps/chosen': -515.0634765625, 'logits/rejected': 0.5033291578292847, 'logits/chosen': 0.46199628710746765, 'epoch': 2.56}


 43%|████▎     | 4589/10740 [22:55:27<33:24:09, 19.55s/it]

 43%|████▎     | 4590/10740 [22:55:47<33:25:52, 19.57s/it]
{'loss': 0.2245, 'learning_rate': 1.2801406514841668e-06, 'rewards/chosen': -3.0312998294830322, 'rewards/rejected': -8.046372413635254, 'rewards/accuracies': 1.0, 'rewards/margins': 5.015072345733643, 'policy_logps/rejected': -375.10693359375, 'policy_logps/chosen': -379.03643798828125, 'referece_logps/rejected': -294.64324951171875, 'referece_logps/chosen': -348.7234191894531, 'logits/rejected': -0.06592782586812973, 'logits/chosen': 0.016559451818466187, 'epoch': 2.56}


 43%|████▎     | 4592/10740 [22:56:17<30:16:39, 17.73s/it]

 43%|████▎     | 4593/10740 [22:56:37<31:25:16, 18.40s/it]

 43%|████▎     | 4594/10740 [22:56:52<29:43:19, 17.41s/it]

 43%|████▎     | 4595/10740 [22:57:09<29:19:13, 17.18s/it]

 43%|████▎     | 4596/10740 [22:57:27<29:59:54, 17.58s/it]

 43%|████▎     | 4597/10740 [22:57:39<26:58:51, 15.81s/it]

 43%|████▎     | 4598/10740 [22:57:50<24:37:42, 14.44s/it]

 43%|████▎     | 4599/10740 [22:58:11<27:36:40, 16.19s/it]

 43%|████▎     | 4600/10740 [22:58:32<30:27:30, 17.86s/it]

 43%|████▎     | 4601/10740 [22:58:52<31:28:57, 18.46s/it]

 43%|████▎     | 4602/10740 [22:59:09<30:39:26, 17.98s/it]

 43%|████▎     | 4603/10740 [22:59:23<28:32:24, 16.74s/it]

 43%|████▎     | 4604/10740 [22:59:43<30:10:24, 17.70s/it]

 43%|████▎     | 4605/10740 [22:59:57<28:07:15, 16.50s/it]

 43%|████▎     | 4606/10740 [23:00:14<28:21:11, 16.64s/it]

 43%|████▎     | 4607/10740 [23:00:32<29:26:12, 17.28s/it]

 43%|████▎     | 4608/10740 [23:00:52<30:48:24, 18.09s/it]

 43%|████▎     | 4609/10740 [23:01:12<31:44:41, 18.64s/it]

 43%|████▎     | 4610/10740 [23:01:23<27:56:44, 16.41s/it]

 43%|████▎     | 4611/10740 [23:01:41<28:35:41, 16.80s/it]

 43%|████▎     | 4612/10740 [23:02:02<30:48:30, 18.10s/it]

 43%|████▎     | 4613/10740 [23:02:20<30:51:37, 18.13s/it]

 43%|████▎     | 4614/10740 [23:02:37<29:51:37, 17.55s/it]

 43%|████▎     | 4615/10740 [23:02:56<30:57:57, 18.20s/it]

 43%|████▎     | 4616/10740 [23:03:15<31:05:13, 18.27s/it]

 43%|████▎     | 4617/10740 [23:03:29<29:14:50, 17.20s/it]

 43%|████▎     | 4618/10740 [23:03:48<29:49:49, 17.54s/it]

 43%|████▎     | 4619/10740 [23:04:08<31:02:15, 18.25s/it]

 43%|████▎     | 4620/10740 [23:04:29<32:39:42, 19.21s/it]
[2024-04-02 18:17:47,409] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4621/10740 [23:04:50<33:42:34, 19.83s/it]
[2024-04-02 18:18:08,688] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4622/10740 [23:05:11<34:16:58, 20.17s/it]
[2024-04-02 18:18:29,655] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4623/10740 [23:05:25<31:09:48, 18.34s/it]
[2024-04-02 18:18:43,719] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4624/10740 [23:05:45<31:49:19, 18.73s/it]

 43%|████▎     | 4625/10740 [23:06:05<32:30:36, 19.14s/it]

 43%|████▎     | 4626/10740 [23:06:25<32:48:27, 19.32s/it]

 43%|████▎     | 4627/10740 [23:06:47<33:58:43, 20.01s/it]

 43%|████▎     | 4628/10740 [23:07:07<34:16:19, 20.19s/it]
[2024-04-02 18:20:25,412] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4629/10740 [23:07:26<33:49:19, 19.92s/it]

 43%|████▎     | 4630/10740 [23:07:47<34:04:22, 20.08s/it]

 43%|████▎     | 4631/10740 [23:08:05<33:09:23, 19.54s/it]

 43%|████▎     | 4632/10740 [23:08:26<33:40:55, 19.85s/it]

 43%|████▎     | 4633/10740 [23:08:43<32:07:10, 18.93s/it]

 43%|████▎     | 4634/10740 [23:09:01<31:52:07, 18.79s/it]

 43%|████▎     | 4635/10740 [23:09:23<33:34:29, 19.80s/it]

 43%|████▎     | 4636/10740 [23:09:41<32:26:33, 19.13s/it]

 43%|████▎     | 4637/10740 [23:10:02<33:21:10, 19.67s/it]

 43%|████▎     | 4638/10740 [23:10:21<33:18:42, 19.65s/it]

 43%|████▎     | 4639/10740 [23:10:42<33:45:57, 19.92s/it]
[2024-04-02 18:24:00,098] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 4640/10740 [23:11:06<35:40:06, 21.05s/it]

 43%|████▎     | 4641/10740 [23:11:25<35:00:53, 20.67s/it]

 43%|████▎     | 4642/10740 [23:11:46<34:55:30, 20.62s/it]

 43%|████▎     | 4643/10740 [23:12:03<33:11:28, 19.60s/it]

 43%|████▎     | 4644/10740 [23:12:24<34:05:20, 20.13s/it]
[2024-04-02 18:25:42,647] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2271, 'learning_rate': 1.2644707871512815e-06, 'rewards/chosen': -4.106338977813721, 'rewards/rejected': -7.6233720779418945, 'rewards/accuracies': 0.875, 'rewards/margins': 3.517033338546753, 'policy_logps/rejected': -402.2690124511719, 'policy_logps/chosen': -349.68426513671875, 'referece_logps/rejected': -326.0353088378906, 'referece_logps/chosen': -308.6208801269531, 'logits/rejected': -0.006937013939023018, 'logits/chosen': -0.047848910093307495, 'epoch': 2.59}


 43%|████▎     | 4646/10740 [23:12:58<30:36:20, 18.08s/it]

 43%|████▎     | 4647/10740 [23:13:15<30:19:20, 17.92s/it]

 43%|████▎     | 4648/10740 [23:13:31<29:13:29, 17.27s/it]

 43%|████▎     | 4649/10740 [23:13:51<30:27:45, 18.00s/it]

 43%|████▎     | 4650/10740 [23:14:05<28:37:38, 16.92s/it]

 43%|████▎     | 4651/10740 [23:14:25<30:00:43, 17.74s/it]

 43%|████▎     | 4652/10740 [23:14:42<29:35:44, 17.50s/it]

 43%|████▎     | 4653/10740 [23:15:00<30:06:29, 17.81s/it]

 43%|████▎     | 4654/10740 [23:15:20<31:07:56, 18.42s/it]

 43%|████▎     | 4655/10740 [23:15:40<31:45:25, 18.79s/it]

 43%|████▎     | 4656/10740 [23:16:01<33:06:16, 19.59s/it]

 43%|████▎     | 4657/10740 [23:16:15<30:10:26, 17.86s/it]

 43%|████▎     | 4658/10740 [23:16:37<31:57:36, 18.92s/it]

 43%|████▎     | 4659/10740 [23:16:50<29:08:30, 17.25s/it]

 43%|████▎     | 4660/10740 [23:17:08<29:23:44, 17.41s/it]

 43%|████▎     | 4661/10740 [23:17:26<29:41:45, 17.59s/it]

 43%|████▎     | 4662/10740 [23:17:42<28:51:16, 17.09s/it]
{'loss': 0.1674, 'learning_rate': 1.2592317064473333e-06, 'rewards/chosen': -4.0143280029296875, 'rewards/rejected': -8.204246520996094, 'rewards/accuracies': 1.0, 'rewards/margins': 4.189918518066406, 'policy_logps/rejected': -525.243408203125, 'policy_logps/chosen': -409.39666748046875, 'referece_logps/rejected': -443.2009582519531, 'referece_logps/chosen': -369.2533874511719, 'logits/rejected': -0.11594585329294205, 'logits/chosen': 0.13053180277347565, 'epoch': 2.6}


 43%|████▎     | 4664/10740 [23:18:17<29:33:11, 17.51s/it]

 43%|████▎     | 4665/10740 [23:18:33<28:32:01, 16.91s/it]

 43%|████▎     | 4666/10740 [23:18:49<27:55:21, 16.55s/it]

 43%|████▎     | 4667/10740 [23:19:04<27:08:43, 16.09s/it]
{'loss': 0.2738, 'learning_rate': 1.2577750439323485e-06, 'rewards/chosen': -4.4358134269714355, 'rewards/rejected': -8.16305923461914, 'rewards/accuracies': 0.875, 'rewards/margins': 3.727245807647705, 'policy_logps/rejected': -410.70758056640625, 'policy_logps/chosen': -362.5010681152344, 'referece_logps/rejected': -329.0770263671875, 'referece_logps/chosen': -318.1429138183594, 'logits/rejected': -0.40210264921188354, 'logits/chosen': -0.36772316694259644, 'epoch': 2.61}


 43%|████▎     | 4669/10740 [23:19:42<29:40:38, 17.60s/it]

 43%|████▎     | 4670/10740 [23:20:01<30:43:40, 18.22s/it]
{'loss': 0.2683, 'learning_rate': 1.256900764868105e-06, 'rewards/chosen': -3.4577605724334717, 'rewards/rejected': -7.243956565856934, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7861955165863037, 'policy_logps/rejected': -425.6526794433594, 'policy_logps/chosen': -308.93426513671875, 'referece_logps/rejected': -353.2131042480469, 'referece_logps/chosen': -274.3566589355469, 'logits/rejected': -0.7395166754722595, 'logits/chosen': -0.642004132270813, 'epoch': 2.61}


 44%|████▎     | 4672/10740 [23:20:38<30:26:19, 18.06s/it]

 44%|████▎     | 4673/10740 [23:20:54<29:15:16, 17.36s/it]

 44%|████▎     | 4674/10740 [23:21:10<28:42:05, 17.03s/it]

 44%|████▎     | 4675/10740 [23:21:23<26:43:41, 15.87s/it]

 44%|████▎     | 4676/10740 [23:21:35<24:58:59, 14.83s/it]

 44%|████▎     | 4677/10740 [23:21:53<26:16:55, 15.61s/it]

 44%|████▎     | 4678/10740 [23:22:13<28:21:10, 16.84s/it]

 44%|████▎     | 4679/10740 [23:22:29<28:04:35, 16.68s/it]

 44%|████▎     | 4680/10740 [23:22:47<28:52:43, 17.16s/it]

 44%|████▎     | 4681/10740 [23:23:09<31:07:33, 18.49s/it]

 44%|████▎     | 4682/10740 [23:23:27<30:54:13, 18.36s/it]

 44%|████▎     | 4683/10740 [23:23:45<30:48:58, 18.32s/it]

 44%|████▎     | 4684/10740 [23:24:05<31:35:49, 18.78s/it]

 44%|████▎     | 4685/10740 [23:24:21<30:21:06, 18.05s/it]

 44%|████▎     | 4686/10740 [23:24:35<28:15:08, 16.80s/it]

 44%|████▎     | 4687/10740 [23:24:51<27:43:02, 16.48s/it]
{'loss': 0.2948, 'learning_rate': 1.2519425656970678e-06, 'rewards/chosen': -4.497803211212158, 'rewards/rejected': -5.324208736419678, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8264055252075195, 'policy_logps/rejected': -459.78509521484375, 'policy_logps/chosen': -349.08294677734375, 'referece_logps/rejected': -406.54296875, 'referece_logps/chosen': -304.10498046875, 'logits/rejected': 0.19982179999351501, 'logits/chosen': 0.2587280869483948, 'epoch': 2.62}

 44%|████▎     | 4688/10740 [23:25:12<30:03:28, 17.88s/it]

 44%|████▎     | 4689/10740 [23:25:27<28:48:30, 17.14s/it]

 44%|████▎     | 4690/10740 [23:25:44<28:33:40, 17.00s/it]

 44%|████▎     | 4691/10740 [23:25:57<26:44:18, 15.91s/it]

 44%|████▎     | 4692/10740 [23:26:16<28:06:33, 16.73s/it]

 44%|████▎     | 4693/10740 [23:26:28<25:39:36, 15.28s/it]

 44%|████▎     | 4694/10740 [23:26:48<27:55:38, 16.63s/it]


 44%|████▎     | 4696/10740 [23:27:23<28:27:58, 16.96s/it]
{'loss': 0.1824, 'learning_rate': 1.2493149474156327e-06, 'rewards/chosen': -4.181106090545654, 'rewards/rejected': -8.02784252166748, 'rewards/accuracies': 1.0, 'rewards/margins': 3.846736192703247, 'policy_logps/rejected': -393.205078125, 'policy_logps/chosen': -397.0348205566406, 'referece_logps/rejected': -312.9266662597656, 'referece_logps/chosen': -355.2237548828125, 'logits/rejected': 0.27511829137802124, 'logits/chosen': 0.16554079949855804, 'epoch': 2.62}

 44%|████▎     | 4697/10740 [23:27:42<29:30:55, 17.58s/it]


 44%|████▍     | 4699/10740 [23:28:19<29:57:05, 17.85s/it]

 44%|████▍     | 4700/10740 [23:28:31<26:55:28, 16.05s/it]

 44%|████▍     | 4701/10740 [23:28:50<28:46:29, 17.15s/it]

 44%|████▍     | 4702/10740 [23:29:09<29:28:26, 17.57s/it]
{'loss': 0.1932, 'learning_rate': 1.2475621802885299e-06, 'rewards/chosen': -3.4183895587921143, 'rewards/rejected': -7.979652404785156, 'rewards/accuracies': 1.0, 'rewards/margins': 4.561262607574463, 'policy_logps/rejected': -459.238525390625, 'policy_logps/chosen': -449.16522216796875, 'referece_logps/rejected': -379.4419860839844, 'referece_logps/chosen': -414.9813232421875, 'logits/rejected': 0.031520500779151917, 'logits/chosen': 0.1334504932165146, 'epoch': 2.63}


 44%|████▍     | 4704/10740 [23:29:47<30:50:02, 18.39s/it]

 44%|████▍     | 4705/10740 [23:30:07<31:25:57, 18.75s/it]
{'loss': 0.2413, 'learning_rate': 1.2466854923950337e-06, 'rewards/chosen': -3.4248738288879395, 'rewards/rejected': -6.078083038330078, 'rewards/accuracies': 0.875, 'rewards/margins': 2.653209686279297, 'policy_logps/rejected': -379.4048767089844, 'policy_logps/chosen': -321.9147033691406, 'referece_logps/rejected': -318.6240234375, 'referece_logps/chosen': -287.66595458984375, 'logits/rejected': -0.6454298496246338, 'logits/chosen': -0.5970141887664795, 'epoch': 2.63}

 44%|████▍     | 4706/10740 [23:30:26<31:41:31, 18.91s/it]


 44%|████▍     | 4708/10740 [23:31:09<33:39:00, 20.08s/it]
{'loss': 0.3278, 'learning_rate': 1.2458086025716952e-06, 'rewards/chosen': -3.5820093154907227, 'rewards/rejected': -5.762300491333008, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1802918910980225, 'policy_logps/rejected': -316.19232177734375, 'policy_logps/chosen': -448.9007568359375, 'referece_logps/rejected': -258.5693359375, 'referece_logps/chosen': -413.0806884765625, 'logits/rejected': 0.7702162265777588, 'logits/chosen': 0.6499198079109192, 'epoch': 2.63}

 44%|████▍     | 4709/10740 [23:31:30<34:15:40, 20.45s/it]


 44%|████▍     | 4711/10740 [23:32:10<33:38:04, 20.08s/it]
{'loss': 0.2629, 'learning_rate': 1.2449315115363116e-06, 'rewards/chosen': -3.9404854774475098, 'rewards/rejected': -6.593635559082031, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6531498432159424, 'policy_logps/rejected': -334.9576416015625, 'policy_logps/chosen': -401.3824462890625, 'referece_logps/rejected': -269.02130126953125, 'referece_logps/chosen': -361.9776306152344, 'logits/rejected': -0.07165225595235825, 'logits/chosen': -0.1488247960805893, 'epoch': 2.63}


 44%|████▍     | 4713/10740 [23:32:37<28:39:07, 17.11s/it]

 44%|████▍     | 4714/10740 [23:32:49<25:57:29, 15.51s/it]
{'loss': 0.1735, 'learning_rate': 1.2440542200068455e-06, 'rewards/chosen': -3.961437940597534, 'rewards/rejected': -7.566789150238037, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6053507328033447, 'policy_logps/rejected': -375.0843200683594, 'policy_logps/chosen': -358.874267578125, 'referece_logps/rejected': -299.4164123535156, 'referece_logps/chosen': -319.2598876953125, 'logits/rejected': -0.04071305692195892, 'logits/chosen': -0.02925640344619751, 'epoch': 2.63}

 44%|████▍     | 4715/10740 [23:33:06<26:48:24, 16.02s/it]


 44%|████▍     | 4717/10740 [23:33:42<28:21:45, 16.95s/it]
{'loss': 0.3184, 'learning_rate': 1.2431767287014232e-06, 'rewards/chosen': -2.8137338161468506, 'rewards/rejected': -8.489073753356934, 'rewards/accuracies': 0.875, 'rewards/margins': 5.675339698791504, 'policy_logps/rejected': -330.1157531738281, 'policy_logps/chosen': -413.6517333984375, 'referece_logps/rejected': -245.22500610351562, 'referece_logps/chosen': -385.5144348144531, 'logits/rejected': -0.3883408308029175, 'logits/chosen': -0.25521212816238403, 'epoch': 2.64}

 44%|████▍     | 4718/10740 [23:33:54<26:06:14, 15.61s/it]

 44%|████▍     | 4719/10740 [23:34:12<27:07:34, 16.22s/it]


 44%|████▍     | 4721/10740 [23:34:52<30:10:42, 18.05s/it]

 44%|████▍     | 4722/10740 [23:35:11<30:54:22, 18.49s/it]
{'loss': 0.2316, 'learning_rate': 1.2417138011968e-06, 'rewards/chosen': -4.489991664886475, 'rewards/rejected': -8.424991607666016, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9349982738494873, 'policy_logps/rejected': -319.1201477050781, 'policy_logps/chosen': -407.5880432128906, 'referece_logps/rejected': -234.87026977539062, 'referece_logps/chosen': -362.6881103515625, 'logits/rejected': -0.1936868131160736, 'logits/chosen': -0.27600300312042236, 'epoch': 2.64}


 44%|████▍     | 4724/10740 [23:35:43<28:41:20, 17.17s/it]
{'loss': 0.2117, 'learning_rate': 1.2411284761174166e-06, 'rewards/chosen': -3.225876569747925, 'rewards/rejected': -5.462714672088623, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2368383407592773, 'policy_logps/rejected': -363.397705078125, 'policy_logps/chosen': -405.9096374511719, 'referece_logps/rejected': -308.7705993652344, 'referece_logps/chosen': -373.6509094238281, 'logits/rejected': 1.0938270092010498, 'logits/chosen': 1.1010277271270752, 'epoch': 2.64}


 44%|████▍     | 4726/10740 [23:36:17<28:24:58, 17.01s/it]

 44%|████▍     | 4727/10740 [23:36:28<25:15:54, 15.13s/it]

 44%|████▍     | 4728/10740 [23:36:39<23:27:39, 14.05s/it]
{'loss': 0.2201, 'learning_rate': 1.2399575629969188e-06, 'rewards/chosen': -3.8406286239624023, 'rewards/rejected': -10.10694408416748, 'rewards/accuracies': 1.0, 'rewards/margins': 6.266315460205078, 'policy_logps/rejected': -707.448486328125, 'policy_logps/chosen': -390.69085693359375, 'referece_logps/rejected': -606.3790893554688, 'referece_logps/chosen': -352.2845458984375, 'logits/rejected': -0.5224722027778625, 'logits/chosen': -0.05727863311767578, 'epoch': 2.64}


 44%|████▍     | 4730/10740 [23:37:14<26:24:19, 15.82s/it]
{'loss': 0.2379, 'learning_rate': 1.2393719753817944e-06, 'rewards/chosen': -2.6346969604492188, 'rewards/rejected': -6.541455268859863, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9067583084106445, 'policy_logps/rejected': -345.97918701171875, 'policy_logps/chosen': -360.0389099121094, 'referece_logps/rejected': -280.5646057128906, 'referece_logps/chosen': -333.6919250488281, 'logits/rejected': -0.9985182285308838, 'logits/chosen': -1.125866413116455, 'epoch': 2.64}

 44%|████▍     | 4731/10740 [23:37:32<27:42:28, 16.60s/it]


 44%|████▍     | 4733/10740 [23:38:00<25:36:00, 15.34s/it]
{'loss': 0.2098, 'learning_rate': 1.2384934307396848e-06, 'rewards/chosen': -2.3396501541137695, 'rewards/rejected': -5.028730392456055, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6890807151794434, 'policy_logps/rejected': -307.22662353515625, 'policy_logps/chosen': -290.6729431152344, 'referece_logps/rejected': -256.9393310546875, 'referece_logps/chosen': -267.27642822265625, 'logits/rejected': -0.5488454103469849, 'logits/chosen': -0.6180135011672974, 'epoch': 2.64}


 44%|████▍     | 4735/10740 [23:38:30<25:35:18, 15.34s/it]
{'loss': 0.1683, 'learning_rate': 1.2379076258094074e-06, 'rewards/chosen': -2.8128745555877686, 'rewards/rejected': -7.802320957183838, 'rewards/accuracies': 1.0, 'rewards/margins': 4.98944616317749, 'policy_logps/rejected': -385.6725158691406, 'policy_logps/chosen': -439.54412841796875, 'referece_logps/rejected': -307.6493225097656, 'referece_logps/chosen': -411.4154052734375, 'logits/rejected': 0.023395175114274025, 'logits/chosen': 0.0372261144220829, 'epoch': 2.65}

 44%|████▍     | 4736/10740 [23:38:47<26:21:21, 15.80s/it]


 44%|████▍     | 4738/10740 [23:39:22<28:16:58, 16.96s/it]

 44%|████▍     | 4739/10740 [23:39:39<28:19:56, 17.00s/it]

 44%|████▍     | 4740/10740 [23:39:59<29:49:27, 17.89s/it]

 44%|████▍     | 4741/10740 [23:40:20<31:23:25, 18.84s/it]

 44%|████▍     | 4742/10740 [23:40:40<31:47:17, 19.08s/it]

 44%|████▍     | 4743/10740 [23:41:00<32:07:25, 19.28s/it]

 44%|████▍     | 4744/10740 [23:41:14<29:34:42, 17.76s/it]

 44%|████▍     | 4745/10740 [23:41:36<31:31:35, 18.93s/it]

 44%|████▍     | 4746/10740 [23:41:52<30:19:47, 18.22s/it]

 44%|████▍     | 4747/10740 [23:42:10<30:16:21, 18.18s/it]
{'loss': 0.2645, 'learning_rate': 1.2343909860735507e-06, 'rewards/chosen': -4.199982643127441, 'rewards/rejected': -8.101959228515625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.901975631713867, 'policy_logps/rejected': -404.7375793457031, 'policy_logps/chosen': -558.252197265625, 'referece_logps/rejected': -323.718017578125, 'referece_logps/chosen': -516.2523193359375, 'logits/rejected': -0.4161919951438904, 'logits/chosen': -0.599412202835083, 'epoch': 2.65}

 44%|████▍     | 4748/10740 [23:42:25<28:42:05, 17.24s/it]


 44%|████▍     | 4750/10740 [23:42:54<26:13:27, 15.76s/it]

 44%|████▍     | 4751/10740 [23:43:14<28:09:13, 16.92s/it]

 44%|████▍     | 4752/10740 [23:43:29<27:26:29, 16.50s/it]
{'loss': 0.1624, 'learning_rate': 1.2329248108398595e-06, 'rewards/chosen': -2.5019137859344482, 'rewards/rejected': -5.564824104309082, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0629098415374756, 'policy_logps/rejected': -409.1441650390625, 'policy_logps/chosen': -416.8937072753906, 'referece_logps/rejected': -353.49591064453125, 'referece_logps/chosen': -391.8745422363281, 'logits/rejected': -0.45741894841194153, 'logits/chosen': -0.4343688189983368, 'epoch': 2.65}

 44%|████▍     | 4753/10740 [23:43:49<29:01:17, 17.45s/it]

 44%|████▍     | 4754/10740 [23:44:09<30:12:19, 18.17s/it]

 44%|████▍     | 4755/10740 [23:44:29<30:58:40, 18.63s/it]


 44%|████▍     | 4757/10740 [23:45:08<32:09:48, 19.35s/it]
{'loss': 0.2235, 'learning_rate': 1.2314581059791959e-06, 'rewards/chosen': -3.2360167503356934, 'rewards/rejected': -6.802979946136475, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5669631958007812, 'policy_logps/rejected': -447.54693603515625, 'policy_logps/chosen': -351.5028381347656, 'referece_logps/rejected': -379.5171203613281, 'referece_logps/chosen': -319.1427001953125, 'logits/rejected': -0.4764918088912964, 'logits/chosen': -0.41443708539009094, 'epoch': 2.66}


 44%|████▍     | 4759/10740 [23:45:46<32:10:27, 19.37s/it]
{'loss': 0.1688, 'learning_rate': 1.2308712764863798e-06, 'rewards/chosen': -3.4125537872314453, 'rewards/rejected': -8.339337348937988, 'rewards/accuracies': 1.0, 'rewards/margins': 4.926784038543701, 'policy_logps/rejected': -464.08111572265625, 'policy_logps/chosen': -461.9124450683594, 'referece_logps/rejected': -380.6877136230469, 'referece_logps/chosen': -427.7868957519531, 'logits/rejected': 0.09336519986391068, 'logits/chosen': 0.00486161932349205, 'epoch': 2.66}

 44%|████▍     | 4760/10740 [23:46:01<30:05:46, 18.12s/it]


 44%|████▍     | 4762/10740 [23:46:34<29:06:06, 17.53s/it]

 44%|████▍     | 4763/10740 [23:46:46<26:30:38, 15.97s/it]

 44%|████▍     | 4764/10740 [23:47:06<28:19:32, 17.06s/it]
{'loss': 0.1854, 'learning_rate': 1.2294038357510377e-06, 'rewards/chosen': -2.644256353378296, 'rewards/rejected': -6.306453704833984, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6621973514556885, 'policy_logps/rejected': -299.4947509765625, 'policy_logps/chosen': -320.44189453125, 'referece_logps/rejected': -236.43016052246094, 'referece_logps/chosen': -293.99932861328125, 'logits/rejected': -0.04248283803462982, 'logits/chosen': -0.19929316639900208, 'epoch': 2.66}

 44%|████▍     | 4765/10740 [23:47:23<28:33:03, 17.20s/it]

 44%|████▍     | 4766/10740 [23:47:35<25:51:45, 15.59s/it]


 44%|████▍     | 4768/10740 [23:48:07<26:38:31, 16.06s/it]
{'loss': 0.2101, 'learning_rate': 1.2282295074354856e-06, 'rewards/chosen': -3.9222068786621094, 'rewards/rejected': -7.784589767456055, 'rewards/accuracies': 1.0, 'rewards/margins': 3.862382411956787, 'policy_logps/rejected': -373.6107177734375, 'policy_logps/chosen': -431.8501892089844, 'referece_logps/rejected': -295.7648010253906, 'referece_logps/chosen': -392.62811279296875, 'logits/rejected': 0.8493529558181763, 'logits/chosen': 0.6716169714927673, 'epoch': 2.66}

 44%|████▍     | 4769/10740 [23:48:29<29:36:46, 17.85s/it]


 44%|████▍     | 4771/10740 [23:49:02<28:22:50, 17.12s/it]

 44%|████▍     | 4772/10740 [23:49:22<30:07:04, 18.17s/it]

 44%|████▍     | 4773/10740 [23:49:42<31:02:54, 18.73s/it]

 44%|████▍     | 4774/10740 [23:50:02<31:42:42, 19.14s/it]

 44%|████▍     | 4775/10740 [23:50:25<33:11:30, 20.03s/it]

 44%|████▍     | 4776/10740 [23:50:47<34:19:25, 20.72s/it]
{'loss': 0.2741, 'learning_rate': 1.2258798561283026e-06, 'rewards/chosen': -4.66963005065918, 'rewards/rejected': -6.737454891204834, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0678248405456543, 'policy_logps/rejected': -308.40252685546875, 'policy_logps/chosen': -297.1197814941406, 'referece_logps/rejected': -241.02798461914062, 'referece_logps/chosen': -250.42347717285156, 'logits/rejected': -0.47455546259880066, 'logits/chosen': -0.5298509001731873, 'epoch': 2.67}

 44%|████▍     | 4777/10740 [23:51:03<32:14:04, 19.46s/it]

 44%|████▍     | 4778/10740 [23:51:17<29:28:28, 17.80s/it]

 44%|████▍     | 4779/10740 [23:51:34<28:46:20, 17.38s/it]


 45%|████▍     | 4781/10740 [23:52:07<27:49:27, 16.81s/it]
{'loss': 0.3008, 'learning_rate': 1.2244106555025066e-06, 'rewards/chosen': -4.067294120788574, 'rewards/rejected': -5.666339874267578, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5990455150604248, 'policy_logps/rejected': -328.9538879394531, 'policy_logps/chosen': -280.4347839355469, 'referece_logps/rejected': -272.29046630859375, 'referece_logps/chosen': -239.7618408203125, 'logits/rejected': 0.07891161739826202, 'logits/chosen': 0.0912506952881813, 'epoch': 2.67}

 45%|████▍     | 4782/10740 [23:52:22<26:54:53, 16.26s/it]


 45%|████▍     | 4784/10740 [23:52:58<28:22:17, 17.15s/it]

 45%|████▍     | 4785/10740 [23:53:17<29:04:56, 17.58s/it]

 45%|████▍     | 4786/10740 [23:53:36<30:05:15, 18.19s/it]
{'loss': 0.1648, 'learning_rate': 1.2229409446093168e-06, 'rewards/chosen': -3.311113119125366, 'rewards/rejected': -5.4212517738342285, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1101388931274414, 'policy_logps/rejected': -269.6031494140625, 'policy_logps/chosen': -246.94386291503906, 'referece_logps/rejected': -215.390625, 'referece_logps/chosen': -213.8327178955078, 'logits/rejected': -0.004990716930478811, 'logits/chosen': 0.016656767576932907, 'epoch': 2.67}


 45%|████▍     | 4788/10740 [23:54:09<28:28:14, 17.22s/it]
{'loss': 0.2778, 'learning_rate': 1.2223529181257046e-06, 'rewards/chosen': -2.9340014457702637, 'rewards/rejected': -5.824151039123535, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8901491165161133, 'policy_logps/rejected': -347.63165283203125, 'policy_logps/chosen': -470.0882568359375, 'referece_logps/rejected': -289.39013671875, 'referece_logps/chosen': -440.7482604980469, 'logits/rejected': -0.8440191745758057, 'logits/chosen': -0.8464637994766235, 'epoch': 2.67}

 45%|████▍     | 4789/10740 [23:54:24<27:09:15, 16.43s/it]

 45%|████▍     | 4790/10740 [23:54:34<24:18:58, 14.71s/it]

 45%|████▍     | 4791/10740 [23:54:48<23:52:00, 14.44s/it]

 45%|████▍     | 4792/10740 [23:55:09<27:08:44, 16.43s/it]

 45%|████▍     | 4793/10740 [23:55:22<25:24:21, 15.38s/it]

 45%|████▍     | 4794/10740 [23:55:42<27:33:57, 16.69s/it]


 45%|████▍     | 4796/10740 [23:56:23<30:54:38, 18.72s/it]
{'loss': 0.1623, 'learning_rate': 1.2200000053892865e-06, 'rewards/chosen': -3.2789862155914307, 'rewards/rejected': -6.556824207305908, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2778379917144775, 'policy_logps/rejected': -499.3796081542969, 'policy_logps/chosen': -484.9286804199219, 'referece_logps/rejected': -433.81134033203125, 'referece_logps/chosen': -452.13885498046875, 'logits/rejected': 1.2200053930282593, 'logits/chosen': 1.1645137071609497, 'epoch': 2.68}


 45%|████▍     | 4798/10740 [23:57:05<32:56:15, 19.96s/it]
{'loss': 0.238, 'learning_rate': 1.2194115765747053e-06, 'rewards/chosen': -2.720074415206909, 'rewards/rejected': -6.465811729431152, 'rewards/accuracies': 0.875, 'rewards/margins': 3.745737314224243, 'policy_logps/rejected': -392.628662109375, 'policy_logps/chosen': -336.32415771484375, 'referece_logps/rejected': -327.97052001953125, 'referece_logps/chosen': -309.1233825683594, 'logits/rejected': -0.21940666437149048, 'logits/chosen': -0.31564921140670776, 'epoch': 2.68}


 45%|████▍     | 4800/10740 [23:57:31<26:54:15, 16.31s/it]
{'loss': 0.1846, 'learning_rate': 1.2188230679360415e-06, 'rewards/chosen': -4.245628833770752, 'rewards/rejected': -7.630739212036133, 'rewards/accuracies': 0.875, 'rewards/margins': 3.385110378265381, 'policy_logps/rejected': -392.0149841308594, 'policy_logps/chosen': -353.443603515625, 'referece_logps/rejected': -315.70758056640625, 'referece_logps/chosen': -310.98736572265625, 'logits/rejected': 0.38519394397735596, 'logits/chosen': 0.43439754843711853, 'epoch': 2.68}

 45%|████▍     | 4801/10740 [23:57:44<25:10:09, 15.26s/it]

 45%|████▍     | 4802/10740 [23:57:54<22:53:22, 13.88s/it]

 45%|████▍     | 4803/10740 [23:58:14<25:41:49, 15.58s/it]


 45%|████▍     | 4805/10740 [23:58:47<26:54:23, 16.32s/it]

 45%|████▍     | 4806/10740 [23:59:07<28:41:43, 17.41s/it]

 45%|████▍     | 4807/10740 [23:59:23<28:14:13, 17.13s/it]

 45%|████▍     | 4808/10740 [23:59:39<27:26:51, 16.66s/it]
{'loss': 0.1526, 'learning_rate': 1.2164682394231052e-06, 'rewards/chosen': -3.3980228900909424, 'rewards/rejected': -6.376242637634277, 'rewards/accuracies': 1.0, 'rewards/margins': 2.978219747543335, 'policy_logps/rejected': -383.29010009765625, 'policy_logps/chosen': -369.8658142089844, 'referece_logps/rejected': -319.5276794433594, 'referece_logps/chosen': -335.8856201171875, 'logits/rejected': 0.2292952835559845, 'logits/chosen': 0.11085456609725952, 'epoch': 2.69}


 45%|████▍     | 4810/10740 [24:00:15<28:30:50, 17.31s/it]

 45%|████▍     | 4811/10740 [24:00:37<30:50:26, 18.73s/it]
{'loss': 0.1743, 'learning_rate': 1.2155848531372052e-06, 'rewards/chosen': -2.311844825744629, 'rewards/rejected': -5.6720099449157715, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3601648807525635, 'policy_logps/rejected': -577.019775390625, 'policy_logps/chosen': -431.2537536621094, 'referece_logps/rejected': -520.2996826171875, 'referece_logps/chosen': -408.13531494140625, 'logits/rejected': -0.18939527869224548, 'logits/chosen': 0.003439590334892273, 'epoch': 2.69}

 45%|████▍     | 4812/10740 [24:00:57<31:14:43, 18.98s/it]

 45%|████▍     | 4813/10740 [24:01:16<31:29:32, 19.13s/it]

 45%|████▍     | 4814/10740 [24:01:30<28:52:36, 17.54s/it]

 45%|████▍     | 4815/10740 [24:01:52<31:09:07, 18.93s/it]

 45%|████▍     | 4816/10740 [24:02:10<30:34:58, 18.59s/it]

 45%|████▍     | 4817/10740 [24:02:25<28:43:48, 17.46s/it]

 45%|████▍     | 4818/10740 [24:02:45<29:41:41, 18.05s/it]

 45%|████▍     | 4819/10740 [24:03:04<30:14:56, 18.39s/it]

 45%|████▍     | 4820/10740 [24:03:18<28:25:45, 17.29s/it]

 45%|████▍     | 4821/10740 [24:03:33<26:56:58, 16.39s/it]

 45%|████▍     | 4822/10740 [24:03:53<28:38:37, 17.42s/it]

 45%|████▍     | 4823/10740 [24:04:14<30:50:32, 18.77s/it]


 45%|████▍     | 4825/10740 [24:04:43<27:31:03, 16.75s/it]
{'loss': 0.1812, 'learning_rate': 1.2114600621443689e-06, 'rewards/chosen': -3.499551773071289, 'rewards/rejected': -5.959339618682861, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4597883224487305, 'policy_logps/rejected': -353.5911560058594, 'policy_logps/chosen': -485.2887878417969, 'referece_logps/rejected': -293.9977111816406, 'referece_logps/chosen': -450.29327392578125, 'logits/rejected': 0.07005888223648071, 'logits/chosen': 0.03993967920541763, 'epoch': 2.7}

 45%|████▍     | 4826/10740 [24:04:57<25:45:23, 15.68s/it]

 45%|████▍     | 4827/10740 [24:05:11<24:55:44, 15.18s/it]

 45%|████▍     | 4828/10740 [24:05:31<27:22:49, 16.67s/it]

 45%|████▍     | 4829/10740 [24:05:49<28:09:19, 17.15s/it]

 45%|████▍     | 4830/10740 [24:06:05<27:27:12, 16.72s/it]

 45%|████▍     | 4831/10740 [24:06:27<30:07:11, 18.35s/it]


 45%|████▌     | 4833/10740 [24:07:04<30:34:29, 18.63s/it]
{'loss': 0.2407, 'learning_rate': 1.2091013415159283e-06, 'rewards/chosen': -4.728578567504883, 'rewards/rejected': -6.857419490814209, 'rewards/accuracies': 0.75, 'rewards/margins': 2.128840923309326, 'policy_logps/rejected': -322.6644287109375, 'policy_logps/chosen': -349.19757080078125, 'referece_logps/rejected': -254.09027099609375, 'referece_logps/chosen': -301.9117736816406, 'logits/rejected': 0.1181257963180542, 'logits/chosen': 0.1257355809211731, 'epoch': 2.7}

 45%|████▌     | 4834/10740 [24:07:17<27:53:49, 17.00s/it]

 45%|████▌     | 4835/10740 [24:07:38<30:04:14, 18.33s/it]

 45%|████▌     | 4836/10740 [24:07:58<30:47:53, 18.78s/it]


 45%|████▌     | 4838/10740 [24:08:40<32:22:56, 19.75s/it]
{'loss': 0.2812, 'learning_rate': 1.2076265221575246e-06, 'rewards/chosen': -2.5955841541290283, 'rewards/rejected': -5.634840488433838, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0392563343048096, 'policy_logps/rejected': -364.80487060546875, 'policy_logps/chosen': -329.9862060546875, 'referece_logps/rejected': -308.45648193359375, 'referece_logps/chosen': -304.0303649902344, 'logits/rejected': 0.010209515690803528, 'logits/chosen': -0.022420130670070648, 'epoch': 2.7}


 45%|████▌     | 4840/10740 [24:09:22<33:12:48, 20.27s/it]
{'loss': 0.2162, 'learning_rate': 1.2070364620373679e-06, 'rewards/chosen': -3.066307544708252, 'rewards/rejected': -5.533197402954102, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4668893814086914, 'policy_logps/rejected': -335.55682373046875, 'policy_logps/chosen': -289.2409973144531, 'referece_logps/rejected': -280.224853515625, 'referece_logps/chosen': -258.57794189453125, 'logits/rejected': -0.050625190138816833, 'logits/chosen': -0.030870551243424416, 'epoch': 2.7}

 45%|████▌     | 4841/10740 [24:09:37<30:45:20, 18.77s/it]

 45%|████▌     | 4842/10740 [24:09:55<30:31:03, 18.63s/it]

 45%|████▌     | 4843/10740 [24:10:15<31:02:43, 18.95s/it]

 45%|████▌     | 4844/10740 [24:10:33<30:42:53, 18.75s/it]

 45%|████▌     | 4845/10740 [24:10:49<29:20:45, 17.92s/it]

 45%|████▌     | 4846/10740 [24:11:05<28:05:31, 17.16s/it]

 45%|████▌     | 4847/10740 [24:11:16<24:53:49, 15.21s/it]

 45%|████▌     | 4848/10740 [24:11:31<25:02:12, 15.30s/it]


 45%|████▌     | 4850/10740 [24:11:56<22:55:51, 14.02s/it]

 45%|████▌     | 4851/10740 [24:12:10<22:57:41, 14.04s/it]

 45%|████▌     | 4852/10740 [24:12:30<25:45:55, 15.75s/it]

 45%|████▌     | 4853/10740 [24:12:50<27:56:25, 17.09s/it]
{'loss': 0.2732, 'learning_rate': 1.2031992448808234e-06, 'rewards/chosen': -2.957951545715332, 'rewards/rejected': -4.852103233337402, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8941519260406494, 'policy_logps/rejected': -293.90533447265625, 'policy_logps/chosen': -403.52728271484375, 'referece_logps/rejected': -245.38426208496094, 'referece_logps/chosen': -373.9477233886719, 'logits/rejected': 0.11321713775396347, 'logits/chosen': 0.3178122043609619, 'epoch': 2.71}

 45%|████▌     | 4854/10740 [24:13:11<29:33:40, 18.08s/it]

 45%|████▌     | 4855/10740 [24:13:31<30:29:39, 18.65s/it]

 45%|████▌     | 4856/10740 [24:13:46<28:46:02, 17.60s/it]

 45%|████▌     | 4857/10740 [24:14:04<28:58:23, 17.73s/it]


 45%|████▌     | 4859/10740 [24:14:42<29:58:02, 18.34s/it]

 45%|████▌     | 4860/10740 [24:15:02<30:40:43, 18.78s/it]

 45%|████▌     | 4861/10740 [24:15:22<31:21:39, 19.20s/it]
{'loss': 0.162, 'learning_rate': 1.200836324274808e-06, 'rewards/chosen': -2.6399872303009033, 'rewards/rejected': -7.093111991882324, 'rewards/accuracies': 0.875, 'rewards/margins': 4.453124523162842, 'policy_logps/rejected': -552.9865112304688, 'policy_logps/chosen': -456.402587890625, 'referece_logps/rejected': -482.05535888671875, 'referece_logps/chosen': -430.00274658203125, 'logits/rejected': -0.37805062532424927, 'logits/chosen': -0.21477702260017395, 'epoch': 2.72}

 45%|████▌     | 4862/10740 [24:15:41<30:52:04, 18.91s/it]


 45%|████▌     | 4864/10740 [24:16:02<24:12:21, 14.83s/it]
{'loss': 0.2267, 'learning_rate': 1.1999499269105363e-06, 'rewards/chosen': -3.426534652709961, 'rewards/rejected': -6.195491790771484, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7689571380615234, 'policy_logps/rejected': -482.2066955566406, 'policy_logps/chosen': -324.7119140625, 'referece_logps/rejected': -420.25177001953125, 'referece_logps/chosen': -290.446533203125, 'logits/rejected': -0.42706814408302307, 'logits/chosen': -0.18326306343078613, 'epoch': 2.72}

 45%|████▌     | 4865/10740 [24:16:19<24:56:17, 15.28s/it]


 45%|████▌     | 4867/10740 [24:16:50<25:42:45, 15.76s/it]

 45%|████▌     | 4868/10740 [24:17:12<28:53:33, 17.71s/it]
{'loss': 0.1527, 'learning_rate': 1.1987678092805103e-06, 'rewards/chosen': -2.9911446571350098, 'rewards/rejected': -6.079617500305176, 'rewards/accuracies': 0.875, 'rewards/margins': 3.088472604751587, 'policy_logps/rejected': -311.9645080566406, 'policy_logps/chosen': -451.6579284667969, 'referece_logps/rejected': -251.16830444335938, 'referece_logps/chosen': -421.7464599609375, 'logits/rejected': 0.10206864774227142, 'logits/chosen': -0.08934205770492554, 'epoch': 2.72}

 45%|████▌     | 4869/10740 [24:17:33<30:08:23, 18.48s/it]

 45%|████▌     | 4870/10740 [24:17:46<27:32:37, 16.89s/it]

 45%|████▌     | 4871/10740 [24:18:07<29:38:13, 18.18s/it]

 45%|████▌     | 4872/10740 [24:18:28<30:46:14, 18.88s/it]

 45%|████▌     | 4873/10740 [24:18:45<30:09:09, 18.50s/it]

 45%|████▌     | 4874/10740 [24:19:05<30:44:24, 18.87s/it]

 45%|████▌     | 4875/10740 [24:19:21<29:26:02, 18.07s/it]

 45%|████▌     | 4876/10740 [24:19:42<30:36:58, 18.80s/it]

 45%|████▌     | 4877/10740 [24:20:02<31:21:58, 19.26s/it]

 45%|████▌     | 4878/10740 [24:20:23<32:09:33, 19.75s/it]

 45%|████▌     | 4879/10740 [24:20:44<32:47:30, 20.14s/it]


 45%|████▌     | 4881/10740 [24:21:18<30:01:30, 18.45s/it]
{'loss': 0.1873, 'learning_rate': 1.1949239382310604e-06, 'rewards/chosen': -3.8924214839935303, 'rewards/rejected': -7.686642646789551, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7942211627960205, 'policy_logps/rejected': -294.0066833496094, 'policy_logps/chosen': -354.43939208984375, 'referece_logps/rejected': -217.14027404785156, 'referece_logps/chosen': -315.5151672363281, 'logits/rejected': -0.1510806679725647, 'logits/chosen': -0.2800663113594055, 'epoch': 2.73}

 45%|████▌     | 4882/10740 [24:21:40<31:43:10, 19.49s/it]

 45%|████▌     | 4883/10740 [24:21:57<30:27:56, 18.73s/it]

 45%|████▌     | 4884/10740 [24:22:15<30:07:20, 18.52s/it]

 45%|████▌     | 4885/10740 [24:22:36<31:15:45, 19.22s/it]

 45%|████▌     | 4886/10740 [24:22:51<29:13:10, 17.97s/it]

 46%|████▌     | 4887/10740 [24:23:08<28:44:01, 17.67s/it]

 46%|████▌     | 4888/10740 [24:23:19<25:34:51, 15.74s/it]

 46%|████▌     | 4889/10740 [24:23:34<25:05:46, 15.44s/it]

 46%|████▌     | 4890/10740 [24:23:51<25:58:12, 15.98s/it]

 46%|████▌     | 4891/10740 [24:24:10<27:18:09, 16.80s/it]

 46%|████▌     | 4892/10740 [24:24:23<25:29:29, 15.69s/it]

 46%|████▌     | 4893/10740 [24:24:43<27:37:51, 17.01s/it]

 46%|████▌     | 4894/10740 [24:24:59<27:05:56, 16.69s/it]

 46%|████▌     | 4895/10740 [24:25:19<28:33:33, 17.59s/it]

 46%|████▌     | 4896/10740 [24:25:39<29:30:57, 18.18s/it]

 46%|████▌     | 4897/10740 [24:26:01<31:26:47, 19.37s/it]

 46%|████▌     | 4898/10740 [24:26:14<28:41:46, 17.68s/it]

 46%|████▌     | 4899/10740 [24:26:27<26:12:48, 16.16s/it]

 46%|████▌     | 4900/10740 [24:26:47<28:05:28, 17.32s/it]

 46%|████▌     | 4901/10740 [24:27:07<29:14:55, 18.03s/it]

 46%|████▌     | 4902/10740 [24:27:22<27:39:32, 17.06s/it]

 46%|████▌     | 4903/10740 [24:27:35<25:44:04, 15.87s/it]

 46%|████▌     | 4904/10740 [24:27:50<25:32:05, 15.75s/it]


 46%|████▌     | 4906/10740 [24:28:31<29:35:12, 18.26s/it]

 46%|████▌     | 4907/10740 [24:28:51<30:25:36, 18.78s/it]

 46%|████▌     | 4908/10740 [24:29:10<30:11:09, 18.63s/it]

 46%|████▌     | 4909/10740 [24:29:29<30:38:20, 18.92s/it]

 46%|████▌     | 4910/10740 [24:29:47<30:01:57, 18.54s/it]

 46%|████▌     | 4911/10740 [24:30:06<30:17:20, 18.71s/it]

 46%|████▌     | 4912/10740 [24:30:23<29:30:20, 18.23s/it]

 46%|████▌     | 4913/10740 [24:30:40<28:44:50, 17.76s/it]

 46%|████▌     | 4914/10740 [24:30:53<26:21:41, 16.29s/it]

 46%|████▌     | 4915/10740 [24:31:13<28:04:21, 17.35s/it]

 46%|████▌     | 4916/10740 [24:31:33<29:36:14, 18.30s/it]

 46%|████▌     | 4917/10740 [24:31:46<27:00:02, 16.69s/it]

 46%|████▌     | 4918/10740 [24:32:02<26:38:08, 16.47s/it]

 46%|████▌     | 4919/10740 [24:32:19<27:07:57, 16.78s/it]

 46%|████▌     | 4920/10740 [24:32:36<26:50:30, 16.60s/it]

 46%|████▌     | 4921/10740 [24:32:52<26:52:10, 16.62s/it]

 46%|████▌     | 4922/10740 [24:33:09<26:40:09, 16.50s/it]

 46%|████▌     | 4923/10740 [24:33:28<28:18:21, 17.52s/it]

 46%|████▌     | 4924/10740 [24:33:48<29:30:05, 18.26s/it]

 46%|████▌     | 4925/10740 [24:34:08<30:08:56, 18.66s/it]

 46%|████▌     | 4926/10740 [24:34:21<27:12:55, 16.85s/it]

 46%|████▌     | 4927/10740 [24:34:42<29:23:31, 18.20s/it]

 46%|████▌     | 4928/10740 [24:34:58<28:18:10, 17.53s/it]

 46%|████▌     | 4929/10740 [24:35:13<27:08:05, 16.81s/it]

 46%|████▌     | 4930/10740 [24:35:29<26:54:53, 16.68s/it]

 46%|████▌     | 4931/10740 [24:35:49<28:09:48, 17.45s/it]

 46%|████▌     | 4932/10740 [24:36:08<29:14:04, 18.12s/it]

 46%|████▌     | 4933/10740 [24:36:30<30:52:54, 19.14s/it]

 46%|████▌     | 4934/10740 [24:36:49<30:44:34, 19.06s/it]

 46%|████▌     | 4935/10740 [24:37:01<27:29:42, 17.05s/it]

 46%|████▌     | 4936/10740 [24:37:13<25:03:07, 15.54s/it]

 46%|████▌     | 4937/10740 [24:37:31<26:22:24, 16.36s/it]

 46%|████▌     | 4938/10740 [24:37:47<25:45:40, 15.98s/it]

 46%|████▌     | 4939/10740 [24:38:02<25:34:52, 15.88s/it]

 46%|████▌     | 4940/10740 [24:38:24<28:18:03, 17.57s/it]

 46%|████▌     | 4941/10740 [24:38:43<29:21:14, 18.22s/it]

 46%|████▌     | 4942/10740 [24:39:01<29:16:05, 18.17s/it]

 46%|████▌     | 4943/10740 [24:39:21<30:03:22, 18.67s/it]

 46%|████▌     | 4944/10740 [24:39:43<31:21:37, 19.48s/it]

 46%|████▌     | 4945/10740 [24:40:00<30:09:31, 18.74s/it]

 46%|████▌     | 4946/10740 [24:40:15<28:39:39, 17.81s/it]

 46%|████▌     | 4947/10740 [24:40:27<25:40:50, 15.96s/it]

 46%|████▌     | 4948/10740 [24:40:43<25:50:32, 16.06s/it]

 46%|████▌     | 4949/10740 [24:41:02<27:12:56, 16.92s/it]

 46%|████▌     | 4950/10740 [24:41:17<26:15:31, 16.33s/it]

 46%|████▌     | 4951/10740 [24:41:30<24:34:43, 15.28s/it]

 46%|████▌     | 4952/10740 [24:41:50<26:43:39, 16.62s/it]

 46%|████▌     | 4953/10740 [24:42:10<28:24:00, 17.67s/it]

 46%|████▌     | 4954/10740 [24:42:28<28:35:57, 17.79s/it]

 46%|████▌     | 4955/10740 [24:42:43<27:28:17, 17.10s/it]

 46%|████▌     | 4956/10740 [24:43:01<27:36:51, 17.19s/it]

 46%|████▌     | 4957/10740 [24:43:18<27:48:42, 17.31s/it]

 46%|████▌     | 4958/10740 [24:43:33<26:32:37, 16.53s/it]

 46%|████▌     | 4959/10740 [24:43:49<26:17:04, 16.37s/it]

 46%|████▌     | 4960/10740 [24:44:09<27:58:40, 17.43s/it]

 46%|████▌     | 4961/10740 [24:44:28<28:35:19, 17.81s/it]

 46%|████▌     | 4962/10740 [24:44:48<29:39:16, 18.48s/it]

 46%|████▌     | 4963/10740 [24:45:06<29:41:15, 18.50s/it]

 46%|████▌     | 4964/10740 [24:45:27<30:43:43, 19.15s/it]

 46%|████▌     | 4965/10740 [24:45:45<30:23:08, 18.94s/it]

 46%|████▌     | 4966/10740 [24:46:05<30:45:45, 19.18s/it]

 46%|████▌     | 4967/10740 [24:46:23<29:55:42, 18.66s/it]

 46%|████▋     | 4968/10740 [24:46:42<30:29:29, 19.02s/it]

 46%|████▋     | 4969/10740 [24:47:00<29:58:37, 18.70s/it]

 46%|████▋     | 4970/10740 [24:47:16<28:17:17, 17.65s/it]

 46%|████▋     | 4971/10740 [24:47:34<28:29:49, 17.78s/it]

 46%|████▋     | 4972/10740 [24:47:53<29:24:38, 18.36s/it]

 46%|████▋     | 4973/10740 [24:48:10<28:38:50, 17.88s/it]

 46%|████▋     | 4974/10740 [24:48:25<27:09:48, 16.96s/it]

 46%|████▋     | 4975/10740 [24:48:45<28:46:38, 17.97s/it]

 46%|████▋     | 4976/10740 [24:49:06<30:10:43, 18.85s/it]

 46%|████▋     | 4977/10740 [24:49:28<31:42:22, 19.81s/it]

 46%|████▋     | 4978/10740 [24:49:40<27:38:59, 17.28s/it]

 46%|████▋     | 4979/10740 [24:50:02<29:57:44, 18.72s/it]

 46%|████▋     | 4980/10740 [24:50:21<30:27:19, 19.03s/it]

 46%|████▋     | 4981/10740 [24:50:41<30:46:19, 19.24s/it]

 46%|████▋     | 4982/10740 [24:50:54<27:40:38, 17.30s/it]

 46%|████▋     | 4983/10740 [24:51:14<28:53:56, 18.07s/it]
{'loss': 0.2978, 'learning_rate': 1.164665043291734e-06, 'rewards/chosen': -4.1479620933532715, 'rewards/rejected': -5.780885219573975, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6329233646392822, 'policy_logps/rejected': -441.95440673828125, 'policy_logps/chosen': -488.0611877441406, 'referece_logps/rejected': -384.14556884765625, 'referece_logps/chosen': -446.5815734863281, 'logits/rejected': 0.237013041973114, 'logits/chosen': 0.2605326771736145, 'epoch': 2.78}


 46%|████▋     | 4985/10740 [24:51:50<28:23:45, 17.76s/it]

 46%|████▋     | 4986/10740 [24:52:09<29:09:40, 18.24s/it]

 46%|████▋     | 4987/10740 [24:52:26<28:21:10, 17.74s/it]

 46%|████▋     | 4988/10740 [24:52:43<28:08:57, 17.62s/it]

 46%|████▋     | 4989/10740 [24:53:03<29:16:48, 18.33s/it]

 46%|████▋     | 4990/10740 [24:53:22<29:28:51, 18.46s/it]

 46%|████▋     | 4991/10740 [24:53:39<28:53:04, 18.09s/it]

 46%|████▋     | 4992/10740 [24:54:00<30:13:25, 18.93s/it]

 46%|████▋     | 4993/10740 [24:54:16<28:41:26, 17.97s/it]

 46%|████▋     | 4994/10740 [24:54:33<28:29:36, 17.85s/it]

 47%|████▋     | 4995/10740 [24:54:49<27:24:01, 17.17s/it]

 47%|████▋     | 4996/10740 [24:55:09<28:38:31, 17.95s/it]

 47%|████▋     | 4997/10740 [24:55:21<25:51:07, 16.21s/it]

 47%|████▋     | 4998/10740 [24:55:42<28:14:15, 17.70s/it]

 47%|████▋     | 4999/10740 [24:55:55<26:00:23, 16.31s/it]
{'loss': 0.2094, 'learning_rate': 1.1599036808230906e-06, 'rewards/chosen': -3.370048761367798, 'rewards/rejected': -7.089605331420898, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7195560932159424, 'policy_logps/rejected': -308.6260986328125, 'policy_logps/chosen': -431.87750244140625, 'referece_logps/rejected': -237.73007202148438, 'referece_logps/chosen': -398.1770324707031, 'logits/rejected': -0.29597726464271545, 'logits/chosen': -0.4742160737514496, 'epoch': 2.79}

 47%|████▋     | 5000/10740 [24:56:11<25:58:38, 16.29s/it]


 47%|████▋     | 5002/10740 [24:56:52<28:30:27, 17.89s/it]
{'loss': 0.1774, 'learning_rate': 1.1590105075234095e-06, 'rewards/chosen': -3.49802303314209, 'rewards/rejected': -5.5211381912231445, 'rewards/accuracies': 1.0, 'rewards/margins': 2.023115634918213, 'policy_logps/rejected': -358.7986145019531, 'policy_logps/chosen': -269.39715576171875, 'referece_logps/rejected': -303.58721923828125, 'referece_logps/chosen': -234.41693115234375, 'logits/rejected': 0.45692917704582214, 'logits/chosen': 0.48377183079719543, 'epoch': 2.79}


 47%|████▋     | 5004/10740 [24:57:24<26:15:50, 16.48s/it]

 47%|████▋     | 5005/10740 [24:57:44<27:58:51, 17.56s/it]
{'loss': 0.1227, 'learning_rate': 1.1581172040621757e-06, 'rewards/chosen': -3.4210283756256104, 'rewards/rejected': -8.495440483093262, 'rewards/accuracies': 1.0, 'rewards/margins': 5.074412822723389, 'policy_logps/rejected': -394.7210388183594, 'policy_logps/chosen': -431.6079406738281, 'referece_logps/rejected': -309.7666320800781, 'referece_logps/chosen': -397.39764404296875, 'logits/rejected': 0.08154184371232986, 'logits/chosen': 0.0140174999833107, 'epoch': 2.8}

 47%|████▋     | 5006/10740 [24:58:02<27:52:13, 17.50s/it]

 47%|████▋     | 5007/10740 [24:58:18<27:11:03, 17.07s/it]


 47%|████▋     | 5009/10740 [24:58:55<28:38:07, 17.99s/it]

 47%|████▋     | 5010/10740 [24:59:17<30:31:39, 19.18s/it]

 47%|████▋     | 5011/10740 [24:59:35<29:43:01, 18.67s/it]
{'loss': 0.245, 'learning_rate': 1.1563302095800893e-06, 'rewards/chosen': -2.1456079483032227, 'rewards/rejected': -4.906342029571533, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7607340812683105, 'policy_logps/rejected': -277.39825439453125, 'policy_logps/chosen': -362.30364990234375, 'referece_logps/rejected': -228.33480834960938, 'referece_logps/chosen': -340.84759521484375, 'logits/rejected': -0.25572317838668823, 'logits/chosen': -0.27810001373291016, 'epoch': 2.8}

 47%|████▋     | 5012/10740 [24:59:55<30:46:57, 19.35s/it]


 47%|████▋     | 5014/10740 [25:00:31<29:00:55, 18.24s/it]

 47%|████▋     | 5015/10740 [25:00:51<29:43:34, 18.69s/it]

 47%|████▋     | 5016/10740 [25:01:07<28:32:43, 17.95s/it]
{'loss': 0.2868, 'learning_rate': 1.1548406562514864e-06, 'rewards/chosen': -4.153322219848633, 'rewards/rejected': -6.032402038574219, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8790791034698486, 'policy_logps/rejected': -259.2574462890625, 'policy_logps/chosen': -300.200927734375, 'referece_logps/rejected': -198.93344116210938, 'referece_logps/chosen': -258.6676940917969, 'logits/rejected': -0.7002476453781128, 'logits/chosen': -0.8634582757949829, 'epoch': 2.8}


 47%|████▋     | 5018/10740 [25:01:47<30:24:37, 19.13s/it]
{'loss': 0.2243, 'learning_rate': 1.1542447361484044e-06, 'rewards/chosen': -2.8043746948242188, 'rewards/rejected': -6.454949378967285, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6505744457244873, 'policy_logps/rejected': -420.61578369140625, 'policy_logps/chosen': -292.0485534667969, 'referece_logps/rejected': -356.0663146972656, 'referece_logps/chosen': -264.00482177734375, 'logits/rejected': 0.26620423793792725, 'logits/chosen': 0.5255061984062195, 'epoch': 2.8}


 47%|████▋     | 5020/10740 [25:02:19<27:45:38, 17.47s/it]

 47%|████▋     | 5021/10740 [25:02:39<28:54:41, 18.20s/it]

 47%|████▋     | 5022/10740 [25:02:59<29:41:22, 18.69s/it]
{'loss': 0.2889, 'learning_rate': 1.1530527278118217e-06, 'rewards/chosen': -3.6469883918762207, 'rewards/rejected': -5.013620376586914, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3666319847106934, 'policy_logps/rejected': -248.82981872558594, 'policy_logps/chosen': -374.4228820800781, 'referece_logps/rejected': -198.69361877441406, 'referece_logps/chosen': -337.9530029296875, 'logits/rejected': -0.6159591674804688, 'logits/chosen': -0.8157700300216675, 'epoch': 2.81}


 47%|████▋     | 5024/10740 [25:03:31<26:56:09, 16.96s/it]

 47%|████▋     | 5025/10740 [25:03:50<28:17:03, 17.82s/it]
{'loss': 0.2593, 'learning_rate': 1.1521585752990555e-06, 'rewards/chosen': -4.056603908538818, 'rewards/rejected': -8.197453498840332, 'rewards/accuracies': 1.0, 'rewards/margins': 4.140849590301514, 'policy_logps/rejected': -400.7217712402344, 'policy_logps/chosen': -388.858642578125, 'referece_logps/rejected': -318.74725341796875, 'referece_logps/chosen': -348.2926025390625, 'logits/rejected': -0.5262798070907593, 'logits/chosen': -0.7981408834457397, 'epoch': 2.81}

 47%|████▋     | 5026/10740 [25:04:12<29:53:25, 18.83s/it]


 47%|████▋     | 5028/10740 [25:04:44<27:19:13, 17.22s/it]
{'loss': 0.2357, 'learning_rate': 1.1512642982335363e-06, 'rewards/chosen': -2.985365390777588, 'rewards/rejected': -9.168879508972168, 'rewards/accuracies': 1.0, 'rewards/margins': 6.18351411819458, 'policy_logps/rejected': -488.0013427734375, 'policy_logps/chosen': -371.7394104003906, 'referece_logps/rejected': -396.3125, 'referece_logps/chosen': -341.88580322265625, 'logits/rejected': -0.6269931197166443, 'logits/chosen': -0.5163332223892212, 'epoch': 2.81}

 47%|████▋     | 5029/10740 [25:05:04<28:33:28, 18.00s/it]


 47%|████▋     | 5031/10740 [25:05:44<29:59:39, 18.91s/it]

 47%|████▋     | 5032/10740 [25:06:05<30:56:41, 19.52s/it]

 47%|████▋     | 5033/10740 [25:06:16<27:15:43, 17.20s/it]
{'loss': 0.3088, 'learning_rate': 1.1497735616620523e-06, 'rewards/chosen': -3.335283041000366, 'rewards/rejected': -7.508988380432129, 'rewards/accuracies': 0.875, 'rewards/margins': 4.173705577850342, 'policy_logps/rejected': -285.4484558105469, 'policy_logps/chosen': -377.075439453125, 'referece_logps/rejected': -210.3585662841797, 'referece_logps/chosen': -343.7226257324219, 'logits/rejected': 0.29831627011299133, 'logits/chosen': 0.24760892987251282, 'epoch': 2.81}

 47%|████▋     | 5034/10740 [25:06:36<28:23:41, 17.91s/it]


 47%|████▋     | 5036/10740 [25:07:07<26:32:16, 16.75s/it]

 47%|████▋     | 5037/10740 [25:07:23<26:14:23, 16.56s/it]
{'loss': 0.2161, 'learning_rate': 1.1485807270412706e-06, 'rewards/chosen': -3.517099380493164, 'rewards/rejected': -8.404342651367188, 'rewards/accuracies': 1.0, 'rewards/margins': 4.887243270874023, 'policy_logps/rejected': -488.8074951171875, 'policy_logps/chosen': -480.9206848144531, 'referece_logps/rejected': -404.7640686035156, 'referece_logps/chosen': -445.7496643066406, 'logits/rejected': 0.3012743890285492, 'logits/chosen': 0.36105281114578247, 'epoch': 2.81}

 47%|████▋     | 5038/10740 [25:07:42<27:17:48, 17.23s/it]


 47%|████▋     | 5040/10740 [25:08:21<29:03:42, 18.35s/it]

 47%|████▋     | 5041/10740 [25:08:37<28:19:31, 17.89s/it]

 47%|████▋     | 5042/10740 [25:08:57<29:09:23, 18.42s/it]

 47%|████▋     | 5043/10740 [25:09:19<30:48:02, 19.46s/it]

 47%|████▋     | 5044/10740 [25:09:33<28:02:28, 17.72s/it]

 47%|████▋     | 5045/10740 [25:09:51<28:34:42, 18.07s/it]
{'loss': 0.2221, 'learning_rate': 1.1461944108744435e-06, 'rewards/chosen': -2.292246103286743, 'rewards/rejected': -7.1225481033325195, 'rewards/accuracies': 1.0, 'rewards/margins': 4.830301284790039, 'policy_logps/rejected': -250.956787109375, 'policy_logps/chosen': -349.76080322265625, 'referece_logps/rejected': -179.73130798339844, 'referece_logps/chosen': -326.8382873535156, 'logits/rejected': -0.49260568618774414, 'logits/chosen': -0.6866419315338135, 'epoch': 2.82}


 47%|████▋     | 5047/10740 [25:10:25<27:18:00, 17.26s/it]
{'loss': 0.2156, 'learning_rate': 1.1455976983226859e-06, 'rewards/chosen': -1.8671618700027466, 'rewards/rejected': -4.268229007720947, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4010672569274902, 'policy_logps/rejected': -447.7364196777344, 'policy_logps/chosen': -363.8852233886719, 'referece_logps/rejected': -405.05413818359375, 'referece_logps/chosen': -345.213623046875, 'logits/rejected': -0.25161948800086975, 'logits/chosen': -0.19476255774497986, 'epoch': 2.82}


 47%|████▋     | 5049/10740 [25:10:58<26:43:21, 16.90s/it]
{'loss': 0.2252, 'learning_rate': 1.1450009328010582e-06, 'rewards/chosen': -3.3222908973693848, 'rewards/rejected': -7.604691505432129, 'rewards/accuracies': 1.0, 'rewards/margins': 4.282401084899902, 'policy_logps/rejected': -375.20587158203125, 'policy_logps/chosen': -331.5564270019531, 'referece_logps/rejected': -299.158935546875, 'referece_logps/chosen': -298.3335266113281, 'logits/rejected': -0.5717525482177734, 'logits/chosen': -0.5190225839614868, 'epoch': 2.82}


 47%|████▋     | 5051/10740 [25:11:30<26:25:44, 16.72s/it]

 47%|████▋     | 5052/10740 [25:11:52<28:53:01, 18.28s/it]

 47%|████▋     | 5053/10740 [25:12:06<26:48:42, 16.97s/it]

 47%|████▋     | 5054/10740 [25:12:28<29:10:51, 18.48s/it]

 47%|████▋     | 5055/10740 [25:12:44<27:57:47, 17.71s/it]

 47%|████▋     | 5056/10740 [25:12:56<25:20:38, 16.05s/it]

 47%|████▋     | 5057/10740 [25:13:07<23:02:48, 14.60s/it]
{'loss': 0.2116, 'learning_rate': 1.1426133453583215e-06, 'rewards/chosen': -3.3432304859161377, 'rewards/rejected': -6.370896816253662, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0276660919189453, 'policy_logps/rejected': -341.72442626953125, 'policy_logps/chosen': -375.4657897949219, 'referece_logps/rejected': -278.01544189453125, 'referece_logps/chosen': -342.03350830078125, 'logits/rejected': 0.705785870552063, 'logits/chosen': 0.6577030420303345, 'epoch': 2.83}


 47%|████▋     | 5059/10740 [25:13:41<25:18:37, 16.04s/it]

 47%|████▋     | 5060/10740 [25:14:01<27:14:10, 17.26s/it]

 47%|████▋     | 5061/10740 [25:14:14<24:59:28, 15.84s/it]

 47%|████▋     | 5062/10740 [25:14:33<26:39:55, 16.91s/it]

 47%|████▋     | 5063/10740 [25:14:53<27:58:37, 17.74s/it]
{'loss': 0.2885, 'learning_rate': 1.1408221092328904e-06, 'rewards/chosen': -3.7043352127075195, 'rewards/rejected': -7.363228797912598, 'rewards/accuracies': 1.0, 'rewards/margins': 3.658893585205078, 'policy_logps/rejected': -494.1260070800781, 'policy_logps/chosen': -429.9912109375, 'referece_logps/rejected': -420.49371337890625, 'referece_logps/chosen': -392.9479064941406, 'logits/rejected': 0.10029013454914093, 'logits/chosen': 0.09118703007698059, 'epoch': 2.83}


 47%|████▋     | 5065/10740 [25:15:34<30:11:50, 19.16s/it]

 47%|████▋     | 5066/10740 [25:15:53<30:23:46, 19.29s/it]
{'loss': 0.2121, 'learning_rate': 1.139926317894029e-06, 'rewards/chosen': -3.491832733154297, 'rewards/rejected': -6.235281467437744, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7434489727020264, 'policy_logps/rejected': -335.26177978515625, 'policy_logps/chosen': -346.6576843261719, 'referece_logps/rejected': -272.9089660644531, 'referece_logps/chosen': -311.7393798828125, 'logits/rejected': 0.09563078731298447, 'logits/chosen': 0.034477829933166504, 'epoch': 2.83}


 47%|████▋     | 5068/10740 [25:16:17<24:17:45, 15.42s/it]
{'loss': 0.1579, 'learning_rate': 1.139329059989649e-06, 'rewards/chosen': -4.178459167480469, 'rewards/rejected': -8.097655296325684, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9191958904266357, 'policy_logps/rejected': -310.854248046875, 'policy_logps/chosen': -261.6034851074219, 'referece_logps/rejected': -229.87770080566406, 'referece_logps/chosen': -219.81887817382812, 'logits/rejected': -0.8395673036575317, 'logits/chosen': -0.7821842432022095, 'epoch': 2.83}


 47%|████▋     | 5070/10740 [25:16:49<24:33:37, 15.59s/it]
{'loss': 0.2408, 'learning_rate': 1.1387317513959914e-06, 'rewards/chosen': -3.2012617588043213, 'rewards/rejected': -8.868768692016602, 'rewards/accuracies': 1.0, 'rewards/margins': 5.667507171630859, 'policy_logps/rejected': -447.7977294921875, 'policy_logps/chosen': -325.0325012207031, 'referece_logps/rejected': -359.1100158691406, 'referece_logps/chosen': -293.0198974609375, 'logits/rejected': -0.6480503678321838, 'logits/chosen': -0.6275262236595154, 'epoch': 2.83}


 47%|████▋     | 5072/10740 [25:17:24<26:12:06, 16.64s/it]
{'loss': 0.3676, 'learning_rate': 1.1381343923303627e-06, 'rewards/chosen': -5.315052509307861, 'rewards/rejected': -9.649791717529297, 'rewards/accuracies': 1.0, 'rewards/margins': 4.334738254547119, 'policy_logps/rejected': -459.323974609375, 'policy_logps/chosen': -493.922607421875, 'referece_logps/rejected': -362.82611083984375, 'referece_logps/chosen': -440.7720031738281, 'logits/rejected': -0.09791289269924164, 'logits/chosen': -0.12187327444553375, 'epoch': 2.83}


 47%|████▋     | 5074/10740 [25:17:59<26:38:02, 16.92s/it]
{'loss': 0.2642, 'learning_rate': 1.1375369830100885e-06, 'rewards/chosen': -2.4956886768341064, 'rewards/rejected': -6.881959915161133, 'rewards/accuracies': 1.0, 'rewards/margins': 4.386270999908447, 'policy_logps/rejected': -454.0223083496094, 'policy_logps/chosen': -376.7555236816406, 'referece_logps/rejected': -385.2027282714844, 'referece_logps/chosen': -351.79864501953125, 'logits/rejected': -0.22207848727703094, 'logits/chosen': -0.1528567671775818, 'epoch': 2.83}


 47%|████▋     | 5076/10740 [25:18:28<24:17:57, 15.44s/it]

 47%|████▋     | 5077/10740 [25:18:49<27:03:20, 17.20s/it]

 47%|████▋     | 5078/10740 [25:19:10<28:31:07, 18.13s/it]
{'loss': 0.1996, 'learning_rate': 1.1363420144749938e-06, 'rewards/chosen': -3.2900888919830322, 'rewards/rejected': -5.3904032707214355, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1003148555755615, 'policy_logps/rejected': -339.1379699707031, 'policy_logps/chosen': -306.8929138183594, 'referece_logps/rejected': -285.2339782714844, 'referece_logps/chosen': -273.99200439453125, 'logits/rejected': -0.2546886205673218, 'logits/chosen': -0.2226635217666626, 'epoch': 2.84}

 47%|████▋     | 5079/10740 [25:19:25<27:14:59, 17.33s/it]


 47%|████▋     | 5081/10740 [25:20:00<27:27:52, 17.47s/it]

 47%|████▋     | 5082/10740 [25:20:21<28:42:23, 18.27s/it]

 47%|████▋     | 5083/10740 [25:20:42<30:23:38, 19.34s/it]

 47%|████▋     | 5084/10740 [25:21:02<30:36:51, 19.49s/it]

 47%|████▋     | 5085/10740 [25:21:23<31:01:39, 19.75s/it]

 47%|████▋     | 5086/10740 [25:21:40<30:07:48, 19.18s/it]

 47%|████▋     | 5087/10740 [25:22:00<30:18:57, 19.31s/it]

 47%|████▋     | 5088/10740 [25:22:20<30:32:27, 19.45s/it]

 47%|████▋     | 5089/10740 [25:22:40<30:39:57, 19.54s/it]

 47%|████▋     | 5090/10740 [25:23:00<30:54:17, 19.69s/it]

 47%|████▋     | 5091/10740 [25:23:17<29:38:38, 18.89s/it]

 47%|████▋     | 5092/10740 [25:23:34<29:04:26, 18.53s/it]
{'loss': 0.3512, 'learning_rate': 1.1321580735361824e-06, 'rewards/chosen': -4.319491863250732, 'rewards/rejected': -6.851669788360596, 'rewards/accuracies': 0.875, 'rewards/margins': 2.532177448272705, 'policy_logps/rejected': -395.17144775390625, 'policy_logps/chosen': -504.27691650390625, 'referece_logps/rejected': -326.6546936035156, 'referece_logps/chosen': -461.0820007324219, 'logits/rejected': 0.38286614418029785, 'logits/chosen': 0.24558764696121216, 'epoch': 2.84}


 47%|████▋     | 5094/10740 [25:24:16<30:44:30, 19.60s/it]
{'loss': 0.2386, 'learning_rate': 1.1315601736263122e-06, 'rewards/chosen': -3.4888603687286377, 'rewards/rejected': -8.017637252807617, 'rewards/accuracies': 1.0, 'rewards/margins': 4.528777122497559, 'policy_logps/rejected': -471.11712646484375, 'policy_logps/chosen': -490.20208740234375, 'referece_logps/rejected': -390.94073486328125, 'referece_logps/chosen': -455.3134765625, 'logits/rejected': 0.33137086033821106, 'logits/chosen': 0.5101666450500488, 'epoch': 2.85}


 47%|████▋     | 5096/10740 [25:24:46<27:28:48, 17.53s/it]

 47%|████▋     | 5097/10740 [25:25:00<25:36:09, 16.33s/it]

 47%|████▋     | 5098/10740 [25:25:20<27:14:46, 17.39s/it]

 47%|████▋     | 5099/10740 [25:25:36<26:51:06, 17.14s/it]

 47%|████▋     | 5100/10740 [25:25:59<29:17:28, 18.70s/it]

 47%|████▋     | 5101/10740 [25:26:21<30:55:28, 19.74s/it]

 48%|████▊     | 5102/10740 [25:26:41<31:04:08, 19.84s/it]

 48%|████▊     | 5103/10740 [25:26:58<29:42:00, 18.97s/it]

 48%|████▊     | 5104/10740 [25:27:15<28:53:58, 18.46s/it]
{'loss': 0.229, 'learning_rate': 1.1285699604847967e-06, 'rewards/chosen': -2.299895763397217, 'rewards/rejected': -5.863030433654785, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5631349086761475, 'policy_logps/rejected': -323.6213073730469, 'policy_logps/chosen': -324.4620666503906, 'referece_logps/rejected': -264.9909973144531, 'referece_logps/chosen': -301.46307373046875, 'logits/rejected': -0.33952754735946655, 'logits/chosen': -0.3764473795890808, 'epoch': 2.85}

 48%|████▊     | 5105/10740 [25:27:33<28:45:51, 18.38s/it]


 48%|████▊     | 5107/10740 [25:28:09<28:06:48, 17.97s/it]

 48%|████▊     | 5108/10740 [25:28:23<26:24:24, 16.88s/it]
{'loss': 0.2272, 'learning_rate': 1.1273735462799712e-06, 'rewards/chosen': -2.443779945373535, 'rewards/rejected': -6.241978645324707, 'rewards/accuracies': 1.0, 'rewards/margins': 3.798198938369751, 'policy_logps/rejected': -364.98260498046875, 'policy_logps/chosen': -394.7882080078125, 'referece_logps/rejected': -302.56280517578125, 'referece_logps/chosen': -370.35040283203125, 'logits/rejected': 0.36003828048706055, 'logits/chosen': 0.33098849654197693, 'epoch': 2.85}

 48%|████▊     | 5109/10740 [25:28:37<25:06:16, 16.05s/it]

 48%|████▊     | 5110/10740 [25:28:52<24:37:04, 15.74s/it]

 48%|████▊     | 5111/10740 [25:29:08<24:42:05, 15.80s/it]


 48%|████▊     | 5113/10740 [25:29:47<27:12:59, 17.41s/it]
{'loss': 0.2206, 'learning_rate': 1.1258777680669566e-06, 'rewards/chosen': -3.4734249114990234, 'rewards/rejected': -6.572851181030273, 'rewards/accuracies': 0.875, 'rewards/margins': 3.09942626953125, 'policy_logps/rejected': -341.9759521484375, 'policy_logps/chosen': -306.0086364746094, 'referece_logps/rejected': -276.2474365234375, 'referece_logps/chosen': -271.2744140625, 'logits/rejected': -0.1980496495962143, 'logits/chosen': -0.1600843071937561, 'epoch': 2.86}

 48%|████▊     | 5114/10740 [25:30:03<26:40:00, 17.06s/it]


 48%|████▊     | 5116/10740 [25:30:37<26:33:12, 17.00s/it]
{'loss': 0.1971, 'learning_rate': 1.1249801635347738e-06, 'rewards/chosen': -2.903108835220337, 'rewards/rejected': -5.897697925567627, 'rewards/accuracies': 0.875, 'rewards/margins': 2.994589328765869, 'policy_logps/rejected': -390.14276123046875, 'policy_logps/chosen': -328.8252258300781, 'referece_logps/rejected': -331.165771484375, 'referece_logps/chosen': -299.79412841796875, 'logits/rejected': -0.02313704788684845, 'logits/chosen': 0.05457724630832672, 'epoch': 2.86}

 48%|████▊     | 5117/10740 [25:30:53<26:12:48, 16.78s/it]

 48%|████▊     | 5118/10740 [25:31:16<28:50:19, 18.47s/it]


 48%|████▊     | 5120/10740 [25:31:53<29:23:12, 18.82s/it]

 48%|████▊     | 5121/10740 [25:32:07<27:22:47, 17.54s/it]
{'loss': 0.2405, 'learning_rate': 1.123483928998981e-06, 'rewards/chosen': -3.4886720180511475, 'rewards/rejected': -6.710630416870117, 'rewards/accuracies': 1.0, 'rewards/margins': 3.221958637237549, 'policy_logps/rejected': -424.10760498046875, 'policy_logps/chosen': -317.4126892089844, 'referece_logps/rejected': -357.0013122558594, 'referece_logps/chosen': -282.5259704589844, 'logits/rejected': 0.019594460725784302, 'logits/chosen': 0.22376567125320435, 'epoch': 2.86}

 48%|████▊     | 5122/10740 [25:32:18<24:12:18, 15.51s/it]

 48%|████▊     | 5123/10740 [25:32:34<24:25:09, 15.65s/it]

 48%|████▊     | 5124/10740 [25:32:52<25:35:57, 16.41s/it]

 48%|████▊     | 5125/10740 [25:33:07<24:38:49, 15.80s/it]


 48%|████▊     | 5127/10740 [25:33:37<23:41:15, 15.19s/it]
{'loss': 0.234, 'learning_rate': 1.1216880772270485e-06, 'rewards/chosen': -4.586625099182129, 'rewards/rejected': -7.508852958679199, 'rewards/accuracies': 1.0, 'rewards/margins': 2.922227144241333, 'policy_logps/rejected': -358.0737609863281, 'policy_logps/chosen': -314.2152099609375, 'referece_logps/rejected': -282.9852294921875, 'referece_logps/chosen': -268.34893798828125, 'logits/rejected': -0.6830626726150513, 'logits/chosen': -0.6125437021255493, 'epoch': 2.86}

 48%|████▊     | 5128/10740 [25:33:58<26:14:09, 16.83s/it]

 48%|████▊     | 5129/10740 [25:34:15<26:09:25, 16.78s/it]

 48%|████▊     | 5130/10740 [25:34:29<24:53:04, 15.97s/it]

 48%|████▊     | 5131/10740 [25:34:46<25:26:30, 16.33s/it]

 48%|████▊     | 5132/10740 [25:35:00<24:21:48, 15.64s/it]


 48%|████▊     | 5134/10740 [25:35:33<24:46:41, 15.91s/it]
{'loss': 0.1807, 'learning_rate': 1.1195924136533243e-06, 'rewards/chosen': -2.781158447265625, 'rewards/rejected': -5.216964244842529, 'rewards/accuracies': 0.875, 'rewards/margins': 2.435805320739746, 'policy_logps/rejected': -388.67254638671875, 'policy_logps/chosen': -395.760009765625, 'referece_logps/rejected': -336.5028991699219, 'referece_logps/chosen': -367.94842529296875, 'logits/rejected': 0.140128493309021, 'logits/chosen': 0.014877915382385254, 'epoch': 2.87}

 48%|████▊     | 5135/10740 [25:35:52<25:56:44, 16.66s/it]

 48%|████▊     | 5136/10740 [25:36:12<27:40:22, 17.78s/it]

 48%|████▊     | 5137/10740 [25:36:33<28:57:04, 18.60s/it]

 48%|████▊     | 5138/10740 [25:36:52<29:32:56, 18.99s/it]

 48%|████▊     | 5139/10740 [25:37:13<30:05:25, 19.34s/it]

 48%|████▊     | 5140/10740 [25:37:24<26:28:00, 17.01s/it]

 48%|████▊     | 5141/10740 [25:37:44<27:54:03, 17.94s/it]


 48%|████▊     | 5143/10740 [25:38:18<26:38:58, 17.14s/it]

 48%|████▊     | 5144/10740 [25:38:39<28:39:21, 18.43s/it]
{'loss': 0.1849, 'learning_rate': 1.1165976862993619e-06, 'rewards/chosen': -2.9676334857940674, 'rewards/rejected': -6.22341251373291, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2557790279388428, 'policy_logps/rejected': -435.1366882324219, 'policy_logps/chosen': -406.9187316894531, 'referece_logps/rejected': -372.9025573730469, 'referece_logps/chosen': -377.2423400878906, 'logits/rejected': -0.47106021642684937, 'logits/chosen': -0.5099982023239136, 'epoch': 2.87}

 48%|████▊     | 5145/10740 [25:38:59<29:18:57, 18.86s/it]

 48%|████▊     | 5146/10740 [25:39:16<28:33:08, 18.37s/it]

 48%|████▊     | 5147/10740 [25:39:30<26:40:10, 17.17s/it]


 48%|████▊     | 5149/10740 [25:39:58<23:34:48, 15.18s/it]

 48%|████▊     | 5150/10740 [25:40:19<26:36:28, 17.14s/it]

 48%|████▊     | 5151/10740 [25:40:32<24:26:17, 15.74s/it]
{'loss': 0.2343, 'learning_rate': 1.114500744542717e-06, 'rewards/chosen': -2.415090799331665, 'rewards/rejected': -4.780277729034424, 'rewards/accuracies': 0.875, 'rewards/margins': 2.365186929702759, 'policy_logps/rejected': -340.6955261230469, 'policy_logps/chosen': -410.8777770996094, 'referece_logps/rejected': -292.89276123046875, 'referece_logps/chosen': -386.7268371582031, 'logits/rejected': -1.1742777824401855, 'logits/chosen': -1.3211382627487183, 'epoch': 2.88}

 48%|████▊     | 5152/10740 [25:40:42<22:04:09, 14.22s/it]

 48%|████▊     | 5153/10740 [25:40:55<21:19:35, 13.74s/it]


 48%|████▊     | 5155/10740 [25:41:17<19:15:45, 12.42s/it]
{'loss': 0.263, 'learning_rate': 1.1133022624002082e-06, 'rewards/chosen': -3.511652946472168, 'rewards/rejected': -6.414831638336182, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9031784534454346, 'policy_logps/rejected': -405.15155029296875, 'policy_logps/chosen': -501.6166687011719, 'referece_logps/rejected': -341.0032653808594, 'referece_logps/chosen': -466.5001220703125, 'logits/rejected': 0.47377830743789673, 'logits/chosen': 0.4761804938316345, 'epoch': 2.88}


 48%|████▊     | 5157/10740 [25:41:50<22:41:46, 14.63s/it]

 48%|████▊     | 5158/10740 [25:42:10<25:06:22, 16.19s/it]
{'loss': 0.1902, 'learning_rate': 1.112403292494205e-06, 'rewards/chosen': -2.3942220211029053, 'rewards/rejected': -5.858745098114014, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4645233154296875, 'policy_logps/rejected': -465.2625732421875, 'policy_logps/chosen': -567.7667236328125, 'referece_logps/rejected': -406.67510986328125, 'referece_logps/chosen': -543.824462890625, 'logits/rejected': -0.28618669509887695, 'logits/chosen': -0.16402935981750488, 'epoch': 2.88}

 48%|████▊     | 5159/10740 [25:42:31<27:33:13, 17.77s/it]

 48%|████▊     | 5160/10740 [25:42:53<29:25:44, 18.99s/it]

 48%|████▊     | 5161/10740 [25:43:15<30:47:28, 19.87s/it]

 48%|████▊     | 5162/10740 [25:43:27<27:18:01, 17.62s/it]

 48%|████▊     | 5163/10740 [25:43:40<25:10:47, 16.25s/it]

 48%|████▊     | 5164/10740 [25:44:01<27:15:57, 17.60s/it]


 48%|████▊     | 5166/10740 [25:44:46<31:09:01, 20.12s/it]
{'loss': 0.2236, 'learning_rate': 1.1100055915828343e-06, 'rewards/chosen': -3.207303047180176, 'rewards/rejected': -8.456185340881348, 'rewards/accuracies': 0.875, 'rewards/margins': 5.248883247375488, 'policy_logps/rejected': -663.5399780273438, 'policy_logps/chosen': -458.98272705078125, 'referece_logps/rejected': -578.9781494140625, 'referece_logps/chosen': -426.90972900390625, 'logits/rejected': -0.05357864499092102, 'logits/chosen': 0.1482270509004593, 'epoch': 2.89}

 48%|████▊     | 5167/10740 [25:45:03<29:43:03, 19.20s/it]


 48%|████▊     | 5169/10740 [25:45:34<27:02:36, 17.48s/it]
{'loss': 0.2426, 'learning_rate': 1.1091062879042982e-06, 'rewards/chosen': -4.416623592376709, 'rewards/rejected': -6.742861270904541, 'rewards/accuracies': 1.0, 'rewards/margins': 2.326237440109253, 'policy_logps/rejected': -417.674072265625, 'policy_logps/chosen': -395.1178894042969, 'referece_logps/rejected': -350.2454528808594, 'referece_logps/chosen': -350.95172119140625, 'logits/rejected': 0.5034258365631104, 'logits/chosen': 0.5344805717468262, 'epoch': 2.89}

 48%|████▊     | 5170/10740 [25:45:51<26:52:16, 17.37s/it]

 48%|████▊     | 5171/10740 [25:46:11<28:19:32, 18.31s/it]


 48%|████▊     | 5173/10740 [25:46:50<29:11:47, 18.88s/it]
{'loss': 0.1875, 'learning_rate': 1.1079070775313884e-06, 'rewards/chosen': -3.6180810928344727, 'rewards/rejected': -7.707493782043457, 'rewards/accuracies': 1.0, 'rewards/margins': 4.089413166046143, 'policy_logps/rejected': -336.77471923828125, 'policy_logps/chosen': -328.3402099609375, 'referece_logps/rejected': -259.69976806640625, 'referece_logps/chosen': -292.15936279296875, 'logits/rejected': 0.05421237647533417, 'logits/chosen': 0.002378717064857483, 'epoch': 2.89}

 48%|████▊     | 5174/10740 [25:47:07<28:23:59, 18.37s/it]

 48%|████▊     | 5175/10740 [25:47:27<29:11:17, 18.88s/it]

 48%|████▊     | 5176/10740 [25:47:44<27:52:36, 18.04s/it]

 48%|████▊     | 5177/10740 [25:48:03<28:38:14, 18.53s/it]


 48%|████▊     | 5179/10740 [25:48:42<29:11:19, 18.90s/it]

 48%|████▊     | 5180/10740 [25:48:54<25:56:33, 16.80s/it]
{'loss': 0.2306, 'learning_rate': 1.1058080825738641e-06, 'rewards/chosen': -4.0637617111206055, 'rewards/rejected': -7.549577713012695, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4858174324035645, 'policy_logps/rejected': -300.5420837402344, 'policy_logps/chosen': -497.65313720703125, 'referece_logps/rejected': -225.04629516601562, 'referece_logps/chosen': -457.0155334472656, 'logits/rejected': -0.6789793968200684, 'logits/chosen': -1.040489673614502, 'epoch': 2.89}


 48%|████▊     | 5182/10740 [25:49:26<25:04:52, 16.25s/it]

 48%|████▊     | 5183/10740 [25:49:38<23:14:42, 15.06s/it]
{'loss': 0.1378, 'learning_rate': 1.1049083684083091e-06, 'rewards/chosen': -5.989181995391846, 'rewards/rejected': -10.772049903869629, 'rewards/accuracies': 1.0, 'rewards/margins': 4.782868385314941, 'policy_logps/rejected': -441.9939880371094, 'policy_logps/chosen': -453.220703125, 'referece_logps/rejected': -334.2734680175781, 'referece_logps/chosen': -393.3288879394531, 'logits/rejected': 0.18174825608730316, 'logits/chosen': 0.25796791911125183, 'epoch': 2.9}

 48%|████▊     | 5184/10740 [25:49:53<23:02:25, 14.93s/it]

 48%|████▊     | 5185/10740 [25:50:12<24:55:11, 16.15s/it]

 48%|████▊     | 5186/10740 [25:50:30<25:44:24, 16.68s/it]


 48%|████▊     | 5188/10740 [25:51:10<28:27:48, 18.46s/it]
{'loss': 0.2003, 'learning_rate': 1.1034086543293204e-06, 'rewards/chosen': -3.0915093421936035, 'rewards/rejected': -8.27188491821289, 'rewards/accuracies': 1.0, 'rewards/margins': 5.180376052856445, 'policy_logps/rejected': -497.3114929199219, 'policy_logps/chosen': -446.455810546875, 'referece_logps/rejected': -414.59259033203125, 'referece_logps/chosen': -415.54071044921875, 'logits/rejected': 0.258963406085968, 'logits/chosen': 0.3624821603298187, 'epoch': 2.9}

 48%|████▊     | 5189/10740 [25:51:29<28:37:36, 18.57s/it]

 48%|████▊     | 5190/10740 [25:51:42<25:48:32, 16.74s/it]


 48%|████▊     | 5192/10740 [25:52:16<26:25:35, 17.15s/it]
{'loss': 0.1586, 'learning_rate': 1.1022087136075864e-06, 'rewards/chosen': -3.560728073120117, 'rewards/rejected': -7.127665042877197, 'rewards/accuracies': 1.0, 'rewards/margins': 3.566937208175659, 'policy_logps/rejected': -358.48809814453125, 'policy_logps/chosen': -506.5797119140625, 'referece_logps/rejected': -287.2114562988281, 'referece_logps/chosen': -470.9724426269531, 'logits/rejected': -0.9000213742256165, 'logits/chosen': -1.0621623992919922, 'epoch': 2.9}

 48%|████▊     | 5193/10740 [25:52:36<27:31:59, 17.87s/it]

 48%|████▊     | 5194/10740 [25:52:55<28:26:10, 18.46s/it]


 48%|████▊     | 5196/10740 [25:53:32<28:44:15, 18.66s/it]
{'loss': 0.2602, 'learning_rate': 1.1010086241477385e-06, 'rewards/chosen': -3.4515621662139893, 'rewards/rejected': -5.5451741218566895, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0936121940612793, 'policy_logps/rejected': -295.506103515625, 'policy_logps/chosen': -246.61605834960938, 'referece_logps/rejected': -240.05438232421875, 'referece_logps/chosen': -212.1004180908203, 'logits/rejected': -0.8910407423973083, 'logits/chosen': -0.7585886716842651, 'epoch': 2.9}


 48%|████▊     | 5198/10740 [25:54:08<27:53:49, 18.12s/it]
{'loss': 0.1226, 'learning_rate': 1.1004085241867694e-06, 'rewards/chosen': -3.48964786529541, 'rewards/rejected': -7.142500877380371, 'rewards/accuracies': 1.0, 'rewards/margins': 3.652853012084961, 'policy_logps/rejected': -452.6706237792969, 'policy_logps/chosen': -338.641845703125, 'referece_logps/rejected': -381.2455749511719, 'referece_logps/chosen': -303.7453308105469, 'logits/rejected': -0.28401660919189453, 'logits/chosen': -0.17606553435325623, 'epoch': 2.9}


 48%|████▊     | 5200/10740 [25:54:46<28:38:03, 18.61s/it]

 48%|████▊     | 5201/10740 [25:55:06<29:09:35, 18.95s/it]
{'loss': 0.2312, 'learning_rate': 1.0995083058205343e-06, 'rewards/chosen': -2.8148510456085205, 'rewards/rejected': -5.994204044342041, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1793529987335205, 'policy_logps/rejected': -371.36883544921875, 'policy_logps/chosen': -395.3649597167969, 'referece_logps/rejected': -311.4267883300781, 'referece_logps/chosen': -367.21649169921875, 'logits/rejected': -0.6297732591629028, 'logits/chosen': -0.632986843585968, 'epoch': 2.91}


 48%|████▊     | 5203/10740 [25:55:39<26:28:08, 17.21s/it]
{'loss': 0.1716, 'learning_rate': 1.098908114944935e-06, 'rewards/chosen': -2.943281412124634, 'rewards/rejected': -7.406769752502441, 'rewards/accuracies': 0.875, 'rewards/margins': 4.463488578796387, 'policy_logps/rejected': -515.783935546875, 'policy_logps/chosen': -317.74859619140625, 'referece_logps/rejected': -441.7162170410156, 'referece_logps/chosen': -288.3157653808594, 'logits/rejected': -0.4047103822231293, 'logits/chosen': -0.21097931265830994, 'epoch': 2.91}


 48%|████▊     | 5205/10740 [25:56:16<27:52:26, 18.13s/it]

 48%|████▊     | 5206/10740 [25:56:36<28:38:54, 18.64s/it]
{'loss': 0.2208, 'learning_rate': 1.0980077612302598e-06, 'rewards/chosen': -4.582841396331787, 'rewards/rejected': -8.236024856567383, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6531832218170166, 'policy_logps/rejected': -460.8547058105469, 'policy_logps/chosen': -510.9869384765625, 'referece_logps/rejected': -378.49444580078125, 'referece_logps/chosen': -465.15850830078125, 'logits/rejected': 0.466982364654541, 'logits/chosen': 0.707868754863739, 'epoch': 2.91}


 48%|████▊     | 5208/10740 [25:57:05<25:00:53, 16.28s/it]

 49%|████▊     | 5209/10740 [25:57:24<26:38:44, 17.34s/it]

 49%|████▊     | 5210/10740 [25:57:43<27:00:15, 17.58s/it]

 49%|████▊     | 5211/10740 [25:58:02<28:02:57, 18.26s/it]
{'loss': 0.1653, 'learning_rate': 1.09650699378887e-06, 'rewards/chosen': -3.255798578262329, 'rewards/rejected': -7.708531856536865, 'rewards/accuracies': 1.0, 'rewards/margins': 4.452733039855957, 'policy_logps/rejected': -296.758056640625, 'policy_logps/chosen': -532.0765991210938, 'referece_logps/rejected': -219.67274475097656, 'referece_logps/chosen': -499.5185852050781, 'logits/rejected': 0.18219703435897827, 'logits/chosen': 0.1887677013874054, 'epoch': 2.91}

 49%|████▊     | 5212/10740 [25:58:19<27:29:31, 17.90s/it]

 49%|████▊     | 5213/10740 [25:58:32<24:48:41, 16.16s/it]

 49%|████▊     | 5214/10740 [25:58:48<25:03:20, 16.32s/it]

 49%|████▊     | 5215/10740 [25:59:06<25:47:24, 16.80s/it]

 49%|████▊     | 5216/10740 [25:59:26<27:09:49, 17.70s/it]

 49%|████▊     | 5217/10740 [25:59:43<27:02:10, 17.62s/it]

 49%|████▊     | 5218/10740 [26:00:00<26:43:30, 17.42s/it]


 49%|████▊     | 5220/10740 [26:00:43<29:55:19, 19.51s/it]
{'loss': 0.2261, 'learning_rate': 1.0938050617024525e-06, 'rewards/chosen': -4.775689125061035, 'rewards/rejected': -7.336164951324463, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5604751110076904, 'policy_logps/rejected': -347.3067626953125, 'policy_logps/chosen': -405.0406188964844, 'referece_logps/rejected': -273.94512939453125, 'referece_logps/chosen': -357.28375244140625, 'logits/rejected': -0.07384180277585983, 'logits/chosen': -0.1005915105342865, 'epoch': 2.92}

 49%|████▊     | 5221/10740 [26:01:03<30:17:28, 19.76s/it]

 49%|████▊     | 5222/10740 [26:01:15<26:40:55, 17.41s/it]

 49%|████▊     | 5223/10740 [26:01:31<25:42:53, 16.78s/it]


 49%|████▊     | 5225/10740 [26:02:09<27:37:15, 18.03s/it]
{'loss': 0.242, 'learning_rate': 1.0923036884338216e-06, 'rewards/chosen': -2.9519782066345215, 'rewards/rejected': -5.313962459564209, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3619842529296875, 'policy_logps/rejected': -296.2007751464844, 'policy_logps/chosen': -307.7472839355469, 'referece_logps/rejected': -243.06117248535156, 'referece_logps/chosen': -278.2275085449219, 'logits/rejected': -0.40592843294143677, 'logits/chosen': -0.24507400393486023, 'epoch': 2.92}

 49%|████▊     | 5226/10740 [26:02:27<27:24:05, 17.89s/it]

 49%|████▊     | 5227/10740 [26:02:50<29:37:39, 19.35s/it]

 49%|████▊     | 5228/10740 [26:03:10<29:54:19, 19.53s/it]

 49%|████▊     | 5229/10740 [26:03:30<30:20:25, 19.82s/it]

 49%|████▊     | 5230/10740 [26:03:50<30:36:12, 19.99s/it]

 49%|████▊     | 5231/10740 [26:04:10<30:29:03, 19.92s/it]

 49%|████▊     | 5232/10740 [26:04:30<30:19:36, 19.82s/it]

 49%|████▊     | 5233/10740 [26:04:48<29:43:01, 19.43s/it]

 49%|████▊     | 5234/10740 [26:05:08<30:00:17, 19.62s/it]

 49%|████▊     | 5235/10740 [26:05:25<28:30:32, 18.64s/it]

 49%|████▉     | 5236/10740 [26:05:42<28:00:41, 18.32s/it]

 49%|████▉     | 5237/10740 [26:06:00<27:57:45, 18.29s/it]

 49%|████▉     | 5238/10740 [26:06:21<28:54:52, 18.92s/it]

 49%|████▉     | 5239/10740 [26:06:40<29:11:00, 19.10s/it]

 49%|████▉     | 5240/10740 [26:06:57<28:07:26, 18.41s/it]

 49%|████▉     | 5241/10740 [26:07:17<28:43:23, 18.80s/it]


 49%|████▉     | 5243/10740 [26:07:52<27:46:11, 18.19s/it]
{'loss': 0.3091, 'learning_rate': 1.0868970313534495e-06, 'rewards/chosen': -2.644805669784546, 'rewards/rejected': -7.247305870056152, 'rewards/accuracies': 0.875, 'rewards/margins': 4.602499961853027, 'policy_logps/rejected': -575.7950439453125, 'policy_logps/chosen': -378.03704833984375, 'referece_logps/rejected': -503.3218994140625, 'referece_logps/chosen': -351.5890197753906, 'logits/rejected': -0.4762462079524994, 'logits/chosen': -0.6137012839317322, 'epoch': 2.93}

 49%|████▉     | 5244/10740 [26:08:04<25:03:12, 16.41s/it]

 49%|████▉     | 5245/10740 [26:08:26<27:38:25, 18.11s/it]

 49%|████▉     | 5246/10740 [26:08:41<26:16:51, 17.22s/it]

 49%|████▉     | 5247/10740 [26:08:55<24:44:13, 16.21s/it]

 49%|████▉     | 5248/10740 [26:09:08<23:19:12, 15.29s/it]


 49%|████▉     | 5250/10740 [26:09:42<25:03:46, 16.43s/it]
{'loss': 0.1994, 'learning_rate': 1.0847937421669957e-06, 'rewards/chosen': -3.6776621341705322, 'rewards/rejected': -6.129993438720703, 'rewards/accuracies': 0.875, 'rewards/margins': 2.452331781387329, 'policy_logps/rejected': -313.50341796875, 'policy_logps/chosen': -462.9715270996094, 'referece_logps/rejected': -252.20350646972656, 'referece_logps/chosen': -426.1949157714844, 'logits/rejected': -0.3386073708534241, 'logits/chosen': -0.4091775119304657, 'epoch': 2.93}

 49%|████▉     | 5251/10740 [26:10:03<27:14:57, 17.87s/it]

 49%|████▉     | 5252/10740 [26:10:21<27:13:01, 17.85s/it]

 49%|████▉     | 5253/10740 [26:10:39<27:12:49, 17.85s/it]

 49%|████▉     | 5254/10740 [26:10:54<26:13:17, 17.21s/it]

 49%|████▉     | 5255/10740 [26:11:08<24:48:17, 16.28s/it]

 49%|████▉     | 5256/10740 [26:11:26<25:33:21, 16.78s/it]

 49%|████▉     | 5257/10740 [26:11:45<26:25:01, 17.34s/it]

 49%|████▉     | 5258/10740 [26:11:57<23:46:46, 15.62s/it]

 49%|████▉     | 5259/10740 [26:12:12<23:51:48, 15.67s/it]

 49%|████▉     | 5260/10740 [26:12:25<22:20:34, 14.68s/it]

 49%|████▉     | 5261/10740 [26:12:44<24:18:30, 15.97s/it]

 49%|████▉     | 5262/10740 [26:13:04<26:06:52, 17.16s/it]

 49%|████▉     | 5263/10740 [26:13:26<28:20:44, 18.63s/it]

 49%|████▉     | 5264/10740 [26:13:45<28:47:17, 18.93s/it]


 49%|████▉     | 5266/10740 [26:14:18<26:45:50, 17.60s/it]
{'loss': 0.2198, 'learning_rate': 1.0799848200706784e-06, 'rewards/chosen': -3.4914321899414062, 'rewards/rejected': -6.852210521697998, 'rewards/accuracies': 0.875, 'rewards/margins': 3.360779047012329, 'policy_logps/rejected': -483.0980224609375, 'policy_logps/chosen': -462.14154052734375, 'referece_logps/rejected': -414.575927734375, 'referece_logps/chosen': -427.22723388671875, 'logits/rejected': -0.16436727344989777, 'logits/chosen': -0.06340144574642181, 'epoch': 2.94}

 49%|████▉     | 5267/10740 [26:14:37<27:40:07, 18.20s/it]

 49%|████▉     | 5268/10740 [26:14:57<28:20:01, 18.64s/it]

 49%|████▉     | 5269/10740 [26:15:14<27:42:38, 18.23s/it]

 49%|████▉     | 5270/10740 [26:15:34<28:23:44, 18.69s/it]


 49%|████▉     | 5272/10740 [26:16:12<28:39:29, 18.87s/it]
{'loss': 0.1897, 'learning_rate': 1.0781809881322432e-06, 'rewards/chosen': -3.959398031234741, 'rewards/rejected': -8.940278053283691, 'rewards/accuracies': 0.875, 'rewards/margins': 4.980880260467529, 'policy_logps/rejected': -293.26422119140625, 'policy_logps/chosen': -487.79150390625, 'referece_logps/rejected': -203.86146545410156, 'referece_logps/chosen': -448.197509765625, 'logits/rejected': -0.06375495344400406, 'logits/chosen': -0.42605164647102356, 'epoch': 2.95}

 49%|████▉     | 5273/10740 [26:16:27<26:55:28, 17.73s/it]

 49%|████▉     | 5274/10740 [26:16:45<26:51:15, 17.69s/it]

 49%|████▉     | 5275/10740 [26:17:00<25:39:14, 16.90s/it]

 49%|████▉     | 5276/10740 [26:17:19<26:58:12, 17.77s/it]

 49%|████▉     | 5277/10740 [26:17:33<25:14:31, 16.63s/it]

 49%|████▉     | 5278/10740 [26:17:54<26:48:00, 17.66s/it]

 49%|████▉     | 5279/10740 [26:18:09<25:55:12, 17.09s/it]

 49%|████▉     | 5280/10740 [26:18:26<25:53:56, 17.08s/it]


 49%|████▉     | 5282/10740 [26:19:02<26:32:22, 17.51s/it]

 49%|████▉     | 5283/10740 [26:19:15<24:38:57, 16.26s/it]

 49%|████▉     | 5284/10740 [26:19:27<22:36:01, 14.91s/it]

 49%|████▉     | 5285/10740 [26:19:45<24:01:33, 15.86s/it]

 49%|████▉     | 5286/10740 [26:19:59<23:07:46, 15.27s/it]

 49%|████▉     | 5287/10740 [26:20:18<25:04:41, 16.56s/it]

 49%|████▉     | 5288/10740 [26:20:41<27:38:38, 18.25s/it]

 49%|████▉     | 5289/10740 [26:21:00<28:00:33, 18.50s/it]

 49%|████▉     | 5290/10740 [26:21:15<26:35:28, 17.56s/it]

 49%|████▉     | 5291/10740 [26:21:37<28:31:37, 18.85s/it]

 49%|████▉     | 5292/10740 [26:21:58<29:31:42, 19.51s/it]

 49%|████▉     | 5293/10740 [26:22:20<30:43:22, 20.31s/it]

 49%|████▉     | 5294/10740 [26:22:42<31:31:56, 20.84s/it]

 49%|████▉     | 5295/10740 [26:23:01<30:40:55, 20.29s/it]

 49%|████▉     | 5296/10740 [26:23:14<27:21:38, 18.09s/it]

 49%|████▉     | 5297/10740 [26:23:32<27:09:55, 17.97s/it]

 49%|████▉     | 5298/10740 [26:23:48<26:28:46, 17.52s/it]

 49%|████▉     | 5299/10740 [26:24:06<26:31:46, 17.55s/it]

 49%|████▉     | 5300/10740 [26:24:25<27:14:00, 18.02s/it]

 49%|████▉     | 5301/10740 [26:24:46<28:40:38, 18.98s/it]

 49%|████▉     | 5302/10740 [26:25:07<29:14:00, 19.35s/it]

 49%|████▉     | 5303/10740 [26:25:22<27:15:52, 18.05s/it]

 49%|████▉     | 5304/10740 [26:25:41<28:04:02, 18.59s/it]

 49%|████▉     | 5305/10740 [26:25:58<27:01:39, 17.90s/it]

 49%|████▉     | 5306/10740 [26:26:10<24:36:01, 16.30s/it]

 49%|████▉     | 5307/10740 [26:26:30<26:14:53, 17.39s/it]

 49%|████▉     | 5308/10740 [26:26:43<24:12:16, 16.04s/it]

 49%|████▉     | 5309/10740 [26:26:59<24:17:07, 16.10s/it]

 49%|████▉     | 5310/10740 [26:27:11<22:25:45, 14.87s/it]

 49%|████▉     | 5311/10740 [26:27:23<20:53:55, 13.86s/it]

 49%|████▉     | 5312/10740 [26:27:40<22:14:30, 14.75s/it]

 49%|████▉     | 5313/10740 [26:27:59<24:11:46, 16.05s/it]

 49%|████▉     | 5314/10740 [26:28:19<26:06:35, 17.32s/it]

 49%|████▉     | 5315/10740 [26:28:36<25:43:22, 17.07s/it]
{'loss': 0.2677, 'learning_rate': 1.0652463901264577e-06, 'rewards/chosen': -3.4294192790985107, 'rewards/rejected': -5.711887836456299, 'rewards/accuracies': 0.625, 'rewards/margins': 2.282468795776367, 'policy_logps/rejected': -281.1307067871094, 'policy_logps/chosen': -290.8920593261719, 'referece_logps/rejected': -224.01185607910156, 'referece_logps/chosen': -256.59783935546875, 'logits/rejected': -0.47238296270370483, 'logits/chosen': -0.47643646597862244, 'epoch': 2.97}


 50%|████▉     | 5317/10740 [26:29:10<25:59:07, 17.25s/it]

 50%|████▉     | 5318/10740 [26:29:28<26:27:48, 17.57s/it]
{'loss': 0.2079, 'learning_rate': 1.0643435416680665e-06, 'rewards/chosen': -2.8076157569885254, 'rewards/rejected': -6.844455242156982, 'rewards/accuracies': 1.0, 'rewards/margins': 4.036839962005615, 'policy_logps/rejected': -379.4910583496094, 'policy_logps/chosen': -472.60406494140625, 'referece_logps/rejected': -311.0464782714844, 'referece_logps/chosen': -444.5279541015625, 'logits/rejected': -0.42372605204582214, 'logits/chosen': -0.5195990800857544, 'epoch': 2.97}


 50%|████▉     | 5320/10740 [26:30:02<25:46:32, 17.12s/it]
{'loss': 0.2004, 'learning_rate': 1.0637416133891711e-06, 'rewards/chosen': -2.323381185531616, 'rewards/rejected': -5.347274303436279, 'rewards/accuracies': 0.875, 'rewards/margins': 3.023892879486084, 'policy_logps/rejected': -325.62152099609375, 'policy_logps/chosen': -256.08282470703125, 'referece_logps/rejected': -272.1487731933594, 'referece_logps/chosen': -232.84901428222656, 'logits/rejected': -0.5772677659988403, 'logits/chosen': -0.3762713670730591, 'epoch': 2.97}


 50%|████▉     | 5322/10740 [26:30:35<25:19:57, 16.83s/it]

 50%|████▉     | 5323/10740 [26:30:50<24:34:16, 16.33s/it]
{'loss': 0.1444, 'learning_rate': 1.0628386775583447e-06, 'rewards/chosen': -2.4350826740264893, 'rewards/rejected': -4.781696319580078, 'rewards/accuracies': 0.875, 'rewards/margins': 2.346613883972168, 'policy_logps/rejected': -313.9733581542969, 'policy_logps/chosen': -404.4033508300781, 'referece_logps/rejected': -266.1564025878906, 'referece_logps/chosen': -380.0525207519531, 'logits/rejected': 0.7549463510513306, 'logits/chosen': 0.479666143655777, 'epoch': 2.97}


 50%|████▉     | 5325/10740 [26:31:18<23:03:51, 15.33s/it]

 50%|████▉     | 5326/10740 [26:31:38<25:15:17, 16.79s/it]

 50%|████▉     | 5327/10740 [26:31:51<23:31:33, 15.65s/it]

 50%|████▉     | 5328/10740 [26:32:11<25:28:15, 16.94s/it]

 50%|████▉     | 5329/10740 [26:32:31<26:53:59, 17.90s/it]

 50%|████▉     | 5330/10740 [26:32:53<28:43:24, 19.11s/it]
{'loss': 0.1884, 'learning_rate': 1.0607316285273805e-06, 'rewards/chosen': -3.6474108695983887, 'rewards/rejected': -5.441486835479736, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7940759658813477, 'policy_logps/rejected': -443.552490234375, 'policy_logps/chosen': -318.88592529296875, 'referece_logps/rejected': -389.1376037597656, 'referece_logps/chosen': -282.41180419921875, 'logits/rejected': -0.6822637319564819, 'logits/chosen': -0.46752989292144775, 'epoch': 2.98}


 50%|████▉     | 5332/10740 [26:33:26<26:55:08, 17.92s/it]

 50%|████▉     | 5333/10740 [26:33:45<27:02:50, 18.01s/it]

 50%|████▉     | 5334/10740 [26:33:58<24:57:20, 16.62s/it]
{'loss': 0.2322, 'learning_rate': 1.0595274783861876e-06, 'rewards/chosen': -3.5373106002807617, 'rewards/rejected': -5.678934097290039, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1416239738464355, 'policy_logps/rejected': -411.03009033203125, 'policy_logps/chosen': -357.1036071777344, 'referece_logps/rejected': -354.24078369140625, 'referece_logps/chosen': -321.73046875, 'logits/rejected': -0.9093570709228516, 'logits/chosen': -0.8160001039505005, 'epoch': 2.98}


 50%|████▉     | 5336/10740 [26:34:38<27:29:26, 18.31s/it]

 50%|████▉     | 5337/10740 [26:34:58<28:14:56, 18.82s/it]

 50%|████▉     | 5338/10740 [26:35:14<27:14:42, 18.16s/it]

 50%|████▉     | 5339/10740 [26:35:33<27:17:04, 18.19s/it]

 50%|████▉     | 5340/10740 [26:35:54<28:50:00, 19.22s/it]

 50%|████▉     | 5341/10740 [26:36:08<26:26:48, 17.63s/it]

 50%|████▉     | 5342/10740 [26:36:25<25:53:24, 17.27s/it]

 50%|████▉     | 5343/10740 [26:36:38<24:12:25, 16.15s/it]

 50%|████▉     | 5344/10740 [26:37:00<26:59:38, 18.01s/it]

 50%|████▉     | 5345/10740 [26:37:20<27:36:12, 18.42s/it]

 50%|████▉     | 5346/10740 [26:37:36<26:30:34, 17.69s/it]

 50%|████▉     | 5347/10740 [26:37:52<25:51:06, 17.26s/it]

 50%|████▉     | 5348/10740 [26:38:11<26:33:20, 17.73s/it]
{'loss': 0.2037, 'learning_rate': 1.0553122822074237e-06, 'rewards/chosen': -2.6661558151245117, 'rewards/rejected': -5.825381278991699, 'rewards/accuracies': 0.75, 'rewards/margins': 3.1592249870300293, 'policy_logps/rejected': -451.64141845703125, 'policy_logps/chosen': -337.58526611328125, 'referece_logps/rejected': -393.3876037597656, 'referece_logps/chosen': -310.9236755371094, 'logits/rejected': -0.5293993949890137, 'logits/chosen': -0.5387700796127319, 'epoch': 2.99}


 50%|████▉     | 5350/10740 [26:38:47<27:10:46, 18.15s/it]

 50%|████▉     | 5351/10740 [26:39:06<27:43:44, 18.52s/it]

 50%|████▉     | 5352/10740 [26:39:27<28:51:08, 19.28s/it]

 50%|████▉     | 5353/10740 [26:39:46<28:41:55, 19.18s/it]

 50%|████▉     | 5354/10740 [26:40:04<28:04:12, 18.76s/it]

 50%|████▉     | 5355/10740 [26:40:25<29:15:09, 19.56s/it]

 50%|████▉     | 5356/10740 [26:40:48<30:37:11, 20.47s/it]

 50%|████▉     | 5357/10740 [26:41:05<29:13:09, 19.54s/it]

 50%|████▉     | 5358/10740 [26:41:25<29:25:40, 19.68s/it]

 50%|████▉     | 5359/10740 [26:41:45<29:27:25, 19.71s/it]

 50%|████▉     | 5360/10740 [26:42:05<29:20:49, 19.64s/it]

 50%|████▉     | 5361/10740 [26:42:27<30:23:50, 20.34s/it]
{'loss': 0.325, 'learning_rate': 1.0513972869657936e-06, 'rewards/chosen': -3.235924482345581, 'rewards/rejected': -7.021554470062256, 'rewards/accuracies': 0.75, 'rewards/margins': 3.7856297492980957, 'policy_logps/rejected': -381.1151123046875, 'policy_logps/chosen': -268.4997253417969, 'referece_logps/rejected': -310.8995666503906, 'referece_logps/chosen': -236.1405029296875, 'logits/rejected': -0.249171182513237, 'logits/chosen': -0.13046789169311523, 'epoch': 2.99}


 50%|████▉     | 5363/10740 [26:43:01<27:11:25, 18.20s/it]

 50%|████▉     | 5364/10740 [26:43:21<28:02:34, 18.78s/it]

 50%|████▉     | 5365/10740 [26:43:41<28:49:24, 19.31s/it]

 50%|████▉     | 5366/10740 [26:44:03<30:04:12, 20.14s/it]

 50%|████▉     | 5367/10740 [26:44:21<28:59:12, 19.42s/it]

 50%|████▉     | 5368/10740 [26:44:37<27:36:01, 18.50s/it]

 50%|████▉     | 5369/10740 [26:44:49<24:20:37, 16.32s/it]
{'loss': 0.1715, 'learning_rate': 1.0489876626145073e-06, 'rewards/chosen': -4.04336404800415, 'rewards/rejected': -7.2738847732543945, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2305209636688232, 'policy_logps/rejected': -434.8410949707031, 'policy_logps/chosen': -389.6217346191406, 'referece_logps/rejected': -362.1022644042969, 'referece_logps/chosen': -349.1881103515625, 'logits/rejected': -0.5204417705535889, 'logits/chosen': -0.3968679904937744, 'epoch': 3.0}


 50%|█████     | 5371/10740 [26:45:18<23:05:00, 15.48s/it]

 50%|█████     | 5372/10740 [26:45:37<24:58:35, 16.75s/it]

 50%|█████     | 5373/10740 [26:46:00<27:30:23, 18.45s/it]

 50%|█████     | 5374/10740 [26:46:18<27:13:59, 18.27s/it]
{'loss': 0.246, 'learning_rate': 1.0474815016992986e-06, 'rewards/chosen': -3.1315090656280518, 'rewards/rejected': -5.54348611831665, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4119768142700195, 'policy_logps/rejected': -387.7115173339844, 'policy_logps/chosen': -500.41705322265625, 'referece_logps/rejected': -332.2766418457031, 'referece_logps/chosen': -469.10198974609375, 'logits/rejected': 0.18066315352916718, 'logits/chosen': 0.09771684557199478, 'epoch': 3.0}


 50%|█████     | 5376/10740 [26:46:57<28:23:41, 19.06s/it]

 50%|█████     | 5377/10740 [26:47:10<25:39:45, 17.23s/it]

 50%|█████     | 5378/10740 [26:47:23<23:46:03, 15.96s/it]

 50%|█████     | 5379/10740 [26:47:41<24:54:51, 16.73s/it]

 50%|█████     | 5380/10740 [26:48:01<26:10:32, 17.58s/it]

 50%|█████     | 5381/10740 [26:48:21<27:23:44, 18.40s/it]

 50%|█████     | 5382/10740 [26:48:41<27:58:07, 18.79s/it]

 50%|█████     | 5383/10740 [26:48:56<26:17:24, 17.67s/it]

 50%|█████     | 5384/10740 [26:49:08<23:32:23, 15.82s/it]

 50%|█████     | 5385/10740 [26:49:19<21:33:55, 14.50s/it]

 50%|█████     | 5386/10740 [26:49:35<22:27:09, 15.10s/it]

 50%|█████     | 5387/10740 [26:49:57<25:14:04, 16.97s/it]

 50%|█████     | 5388/10740 [26:50:19<27:25:46, 18.45s/it]

 50%|█████     | 5389/10740 [26:50:36<26:43:55, 17.98s/it]

 50%|█████     | 5390/10740 [26:50:56<27:50:58, 18.74s/it]

 50%|█████     | 5391/10740 [26:51:14<27:19:02, 18.39s/it]

 50%|█████     | 5392/10740 [26:51:31<26:52:47, 18.09s/it]

 50%|█████     | 5393/10740 [26:51:45<25:07:24, 16.92s/it]

 50%|█████     | 5394/10740 [26:51:56<22:21:06, 15.05s/it]

 50%|█████     | 5395/10740 [26:52:16<24:30:19, 16.51s/it]

 50%|█████     | 5396/10740 [26:52:30<23:22:46, 15.75s/it]
{'loss': 0.1468, 'learning_rate': 1.0408531571762892e-06, 'rewards/chosen': -3.584014892578125, 'rewards/rejected': -7.4704790115356445, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8864641189575195, 'policy_logps/rejected': -254.81393432617188, 'policy_logps/chosen': -389.6270751953125, 'referece_logps/rejected': -180.1091766357422, 'referece_logps/chosen': -353.78692626953125, 'logits/rejected': -0.21927258372306824, 'logits/chosen': -0.5931299924850464, 'epoch': 3.01}

 50%|█████     | 5397/10740 [26:52:49<24:48:59, 16.72s/it]


 50%|█████     | 5399/10740 [26:53:27<26:38:55, 17.96s/it]

 50%|█████     | 5400/10740 [26:53:48<27:57:39, 18.85s/it]
{'loss': 0.266, 'learning_rate': 1.0396478018602082e-06, 'rewards/chosen': -3.57114315032959, 'rewards/rejected': -6.557806491851807, 'rewards/accuracies': 0.875, 'rewards/margins': 2.986663341522217, 'policy_logps/rejected': -315.548095703125, 'policy_logps/chosen': -328.6288146972656, 'referece_logps/rejected': -249.96998596191406, 'referece_logps/chosen': -292.9173889160156, 'logits/rejected': -0.34401971101760864, 'logits/chosen': -0.4282546937465668, 'epoch': 3.02}


 50%|█████     | 5402/10740 [26:54:24<27:35:59, 18.61s/it]

 50%|█████     | 5403/10740 [26:54:44<28:10:59, 19.01s/it]

 50%|█████     | 5404/10740 [26:55:05<28:44:12, 19.39s/it]

 50%|█████     | 5405/10740 [26:55:24<28:53:03, 19.49s/it]

 50%|█████     | 5406/10740 [26:55:45<29:35:54, 19.98s/it]
{'loss': 0.3053, 'learning_rate': 1.0378396612523275e-06, 'rewards/chosen': -2.473642110824585, 'rewards/rejected': -5.92297887802124, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4493367671966553, 'policy_logps/rejected': -234.40818786621094, 'policy_logps/chosen': -415.3134460449219, 'referece_logps/rejected': -175.17840576171875, 'referece_logps/chosen': -390.5770263671875, 'logits/rejected': 0.21879395842552185, 'logits/chosen': -0.02418004721403122, 'epoch': 3.02}


 50%|█████     | 5408/10740 [26:56:15<26:07:40, 17.64s/it]

 50%|█████     | 5409/10740 [26:56:32<25:48:43, 17.43s/it]

 50%|█████     | 5410/10740 [26:56:51<26:19:47, 17.78s/it]

 50%|█████     | 5411/10740 [26:57:11<27:32:24, 18.60s/it]

 50%|█████     | 5412/10740 [26:57:26<26:01:25, 17.58s/it]

 50%|█████     | 5413/10740 [26:57:45<26:17:13, 17.76s/it]

 50%|█████     | 5414/10740 [26:58:02<26:03:53, 17.62s/it]

 50%|█████     | 5415/10740 [26:58:24<28:02:54, 18.96s/it]

 50%|█████     | 5416/10740 [26:58:40<26:54:06, 18.19s/it]

 50%|█████     | 5417/10740 [26:59:00<27:43:51, 18.75s/it]

 50%|█████     | 5418/10740 [26:59:12<24:32:54, 16.61s/it]
{'loss': 0.3472, 'learning_rate': 1.0342230142635639e-06, 'rewards/chosen': -2.4881913661956787, 'rewards/rejected': -7.357195854187012, 'rewards/accuracies': 0.875, 'rewards/margins': 4.869004726409912, 'policy_logps/rejected': -412.91644287109375, 'policy_logps/chosen': -305.0443115234375, 'referece_logps/rejected': -339.3445129394531, 'referece_logps/chosen': -280.16241455078125, 'logits/rejected': 0.27962446212768555, 'logits/chosen': 0.2678554356098175, 'epoch': 3.03}

 50%|█████     | 5419/10740 [26:59:23<22:11:21, 15.01s/it]


 50%|█████     | 5421/10740 [26:59:51<20:50:51, 14.11s/it]
{'loss': 0.1422, 'learning_rate': 1.033318780631004e-06, 'rewards/chosen': -3.657219171524048, 'rewards/rejected': -7.278548717498779, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6213297843933105, 'policy_logps/rejected': -311.6819152832031, 'policy_logps/chosen': -384.58172607421875, 'referece_logps/rejected': -238.8964385986328, 'referece_logps/chosen': -348.00958251953125, 'logits/rejected': 0.5628640055656433, 'logits/chosen': 0.40519991517066956, 'epoch': 3.03}

 50%|█████     | 5422/10740 [27:00:02<19:28:31, 13.18s/it]


 51%|█████     | 5424/10740 [27:00:41<24:11:38, 16.38s/it]
{'loss': 0.185, 'learning_rate': 1.0324145197246226e-06, 'rewards/chosen': -3.4437196254730225, 'rewards/rejected': -7.181181907653809, 'rewards/accuracies': 0.875, 'rewards/margins': 3.737462043762207, 'policy_logps/rejected': -434.8230895996094, 'policy_logps/chosen': -547.176025390625, 'referece_logps/rejected': -363.01129150390625, 'referece_logps/chosen': -512.7388916015625, 'logits/rejected': -0.8131608963012695, 'logits/chosen': -1.14198637008667, 'epoch': 3.03}

 51%|█████     | 5425/10740 [27:00:57<24:17:00, 16.45s/it]

 51%|█████     | 5426/10740 [27:01:15<25:00:04, 16.94s/it]


 51%|█████     | 5428/10740 [27:01:53<26:45:35, 18.14s/it]

 51%|█████     | 5429/10740 [27:02:13<27:25:56, 18.59s/it]

 51%|█████     | 5430/10740 [27:02:34<28:42:28, 19.46s/it]
{'loss': 0.1242, 'learning_rate': 1.0306059190512268e-06, 'rewards/chosen': -4.507688522338867, 'rewards/rejected': -8.292289733886719, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7846009731292725, 'policy_logps/rejected': -481.87457275390625, 'policy_logps/chosen': -436.90533447265625, 'referece_logps/rejected': -398.95166015625, 'referece_logps/chosen': -391.8284912109375, 'logits/rejected': 0.5007490515708923, 'logits/chosen': 0.47159865498542786, 'epoch': 3.03}


 51%|█████     | 5432/10740 [27:03:12<28:13:59, 19.15s/it]

 51%|█████     | 5433/10740 [27:03:32<28:44:00, 19.49s/it]

 51%|█████     | 5434/10740 [27:03:52<28:51:20, 19.58s/it]

 51%|█████     | 5435/10740 [27:04:13<29:27:06, 19.99s/it]

 51%|█████     | 5436/10740 [27:04:33<29:21:04, 19.92s/it]

 51%|█████     | 5437/10740 [27:04:48<27:26:50, 18.63s/it]

 51%|█████     | 5438/10740 [27:05:03<25:42:40, 17.46s/it]

 51%|█████     | 5439/10740 [27:05:24<27:15:52, 18.52s/it]
{'loss': 0.1943, 'learning_rate': 1.0278928319932311e-06, 'rewards/chosen': -2.9066901206970215, 'rewards/rejected': -5.767530918121338, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8608410358428955, 'policy_logps/rejected': -349.00177001953125, 'policy_logps/chosen': -604.1531372070312, 'referece_logps/rejected': -291.3264465332031, 'referece_logps/chosen': -575.0862426757812, 'logits/rejected': 0.12825237214565277, 'logits/chosen': 0.14599905908107758, 'epoch': 3.04}


 51%|█████     | 5441/10740 [27:05:59<26:45:36, 18.18s/it]

 51%|█████     | 5442/10740 [27:06:19<27:24:38, 18.63s/it]

 51%|█████     | 5443/10740 [27:06:36<26:54:31, 18.29s/it]

 51%|█████     | 5444/10740 [27:06:55<26:54:48, 18.29s/it]

 51%|█████     | 5445/10740 [27:07:17<28:35:43, 19.44s/it]

 51%|█████     | 5446/10740 [27:07:36<28:40:27, 19.50s/it]
{'loss': 0.2402, 'learning_rate': 1.0257825100791701e-06, 'rewards/chosen': -3.39540433883667, 'rewards/rejected': -5.728009223937988, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3326048851013184, 'policy_logps/rejected': -268.12835693359375, 'policy_logps/chosen': -262.12725830078125, 'referece_logps/rejected': -210.8482666015625, 'referece_logps/chosen': -228.17320251464844, 'logits/rejected': 0.12734664976596832, 'logits/chosen': 0.012417219579219818, 'epoch': 3.04}

 51%|█████     | 5447/10740 [27:07:58<29:27:31, 20.04s/it]


 51%|█████     | 5449/10740 [27:08:35<28:41:26, 19.52s/it]

 51%|█████     | 5450/10740 [27:08:56<29:22:18, 19.99s/it]
{'loss': 0.2417, 'learning_rate': 1.0245765596497089e-06, 'rewards/chosen': -2.963104009628296, 'rewards/rejected': -7.939199447631836, 'rewards/accuracies': 1.0, 'rewards/margins': 4.976095676422119, 'policy_logps/rejected': -509.3656005859375, 'policy_logps/chosen': -339.7684326171875, 'referece_logps/rejected': -429.9736022949219, 'referece_logps/chosen': -310.13739013671875, 'logits/rejected': -0.8389928340911865, 'logits/chosen': -0.8441148996353149, 'epoch': 3.04}

 51%|█████     | 5451/10740 [27:09:10<26:34:25, 18.09s/it]


 51%|█████     | 5453/10740 [27:09:44<25:09:43, 17.13s/it]
{'loss': 0.2028, 'learning_rate': 1.023672073261007e-06, 'rewards/chosen': -3.6383345127105713, 'rewards/rejected': -5.990401744842529, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3520667552948, 'policy_logps/rejected': -354.42437744140625, 'policy_logps/chosen': -411.55987548828125, 'referece_logps/rejected': -294.5203552246094, 'referece_logps/chosen': -375.1765441894531, 'logits/rejected': -0.1426437348127365, 'logits/chosen': -0.32212188839912415, 'epoch': 3.05}


 51%|█████     | 5455/10740 [27:10:17<24:22:24, 16.60s/it]

 51%|█████     | 5456/10740 [27:10:35<25:10:13, 17.15s/it]

 51%|█████     | 5457/10740 [27:10:57<27:03:54, 18.44s/it]

 51%|█████     | 5458/10740 [27:11:19<28:32:33, 19.45s/it]

 51%|█████     | 5459/10740 [27:11:37<27:55:09, 19.03s/it]

 51%|█████     | 5460/10740 [27:11:53<26:44:17, 18.23s/it]

 51%|█████     | 5461/10740 [27:12:07<24:59:39, 17.04s/it]

 51%|█████     | 5462/10740 [27:12:27<26:12:52, 17.88s/it]
{'loss': 0.1761, 'learning_rate': 1.0209585007927482e-06, 'rewards/chosen': -1.7743879556655884, 'rewards/rejected': -5.54487943649292, 'rewards/accuracies': 0.875, 'rewards/margins': 3.770491361618042, 'policy_logps/rejected': -281.671142578125, 'policy_logps/chosen': -285.4256896972656, 'referece_logps/rejected': -226.22238159179688, 'referece_logps/chosen': -267.6817932128906, 'logits/rejected': 0.577260434627533, 'logits/chosen': 0.5096439123153687, 'epoch': 3.05}

 51%|█████     | 5463/10740 [27:12:46<26:36:52, 18.16s/it]


 51%|█████     | 5465/10740 [27:13:26<27:49:28, 18.99s/it]
{'loss': 0.2344, 'learning_rate': 1.020053941337338e-06, 'rewards/chosen': -3.3481383323669434, 'rewards/rejected': -5.86091423034668, 'rewards/accuracies': 0.75, 'rewards/margins': 2.512775421142578, 'policy_logps/rejected': -287.3890686035156, 'policy_logps/chosen': -294.25811767578125, 'referece_logps/rejected': -228.77993774414062, 'referece_logps/chosen': -260.7767639160156, 'logits/rejected': 0.021032702177762985, 'logits/chosen': -0.04157142713665962, 'epoch': 3.05}

 51%|█████     | 5466/10740 [27:13:38<24:52:58, 16.98s/it]

 51%|█████     | 5467/10740 [27:13:52<23:43:14, 16.19s/it]


 51%|█████     | 5469/10740 [27:14:27<24:07:17, 16.47s/it]
{'loss': 0.2003, 'learning_rate': 1.0188478366560679e-06, 'rewards/chosen': -3.338801145553589, 'rewards/rejected': -8.14574146270752, 'rewards/accuracies': 0.875, 'rewards/margins': 4.806940078735352, 'policy_logps/rejected': -371.29974365234375, 'policy_logps/chosen': -392.3426818847656, 'referece_logps/rejected': -289.8423156738281, 'referece_logps/chosen': -358.9547119140625, 'logits/rejected': -0.4983791708946228, 'logits/chosen': -0.5169857740402222, 'epoch': 3.06}


 51%|█████     | 5471/10740 [27:15:02<25:17:38, 17.28s/it]

 51%|█████     | 5472/10740 [27:15:16<23:53:00, 16.32s/it]
{'loss': 0.2123, 'learning_rate': 1.0179432400494315e-06, 'rewards/chosen': -2.877519369125366, 'rewards/rejected': -6.797600269317627, 'rewards/accuracies': 0.875, 'rewards/margins': 3.920081377029419, 'policy_logps/rejected': -455.71380615234375, 'policy_logps/chosen': -424.9328308105469, 'referece_logps/rejected': -387.73779296875, 'referece_logps/chosen': -396.1576232910156, 'logits/rejected': -0.21058979630470276, 'logits/chosen': -0.22353824973106384, 'epoch': 3.06}


 51%|█████     | 5474/10740 [27:15:44<22:09:06, 15.14s/it]

 51%|█████     | 5475/10740 [27:16:04<24:11:50, 16.55s/it]

 51%|█████     | 5476/10740 [27:16:21<24:37:17, 16.84s/it]

 51%|█████     | 5477/10740 [27:16:41<25:53:01, 17.71s/it]
{'loss': 0.2898, 'learning_rate': 1.0164355467644128e-06, 'rewards/chosen': -3.530486822128296, 'rewards/rejected': -5.996479034423828, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4659924507141113, 'policy_logps/rejected': -375.22088623046875, 'policy_logps/chosen': -380.057373046875, 'referece_logps/rejected': -315.256103515625, 'referece_logps/chosen': -344.75250244140625, 'logits/rejected': -0.07474100589752197, 'logits/chosen': -0.17855042219161987, 'epoch': 3.06}


 51%|█████     | 5479/10740 [27:17:16<25:57:04, 17.76s/it]

 51%|█████     | 5480/10740 [27:17:38<27:54:40, 19.10s/it]

 51%|█████     | 5481/10740 [27:17:58<28:14:17, 19.33s/it]
{'loss': 0.2626, 'learning_rate': 1.0152293650644863e-06, 'rewards/chosen': -3.9177963733673096, 'rewards/rejected': -5.658471584320068, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7406750917434692, 'policy_logps/rejected': -311.229736328125, 'policy_logps/chosen': -257.9380187988281, 'referece_logps/rejected': -254.64501953125, 'referece_logps/chosen': -218.76004028320312, 'logits/rejected': -0.1751357614994049, 'logits/chosen': -0.13357926905155182, 'epoch': 3.06}

 51%|█████     | 5482/10740 [27:18:18<28:46:28, 19.70s/it]


 51%|█████     | 5484/10740 [27:18:50<25:36:00, 17.53s/it]

 51%|█████     | 5485/10740 [27:19:09<26:32:07, 18.18s/it]

 51%|█████     | 5486/10740 [27:19:24<24:49:24, 17.01s/it]

 51%|█████     | 5487/10740 [27:19:46<27:10:29, 18.62s/it]
{'loss': 0.1026, 'learning_rate': 1.0134200515086932e-06, 'rewards/chosen': -3.336376667022705, 'rewards/rejected': -6.283752918243408, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9473764896392822, 'policy_logps/rejected': -531.467529296875, 'policy_logps/chosen': -405.79302978515625, 'referece_logps/rejected': -468.62994384765625, 'referece_logps/chosen': -372.42926025390625, 'logits/rejected': 0.43392491340637207, 'logits/chosen': 0.487341046333313, 'epoch': 3.07}

 51%|█████     | 5488/10740 [27:20:06<27:52:52, 19.11s/it]


 51%|█████     | 5490/10740 [27:20:47<28:57:51, 19.86s/it]
{'loss': 0.3034, 'learning_rate': 1.0125153778826132e-06, 'rewards/chosen': -3.4079039096832275, 'rewards/rejected': -8.21310806274414, 'rewards/accuracies': 0.875, 'rewards/margins': 4.80520486831665, 'policy_logps/rejected': -396.7435302734375, 'policy_logps/chosen': -280.87921142578125, 'referece_logps/rejected': -314.6124572753906, 'referece_logps/chosen': -246.8001708984375, 'logits/rejected': -0.37779828906059265, 'logits/chosen': -0.48935002088546753, 'epoch': 3.07}


 51%|█████     | 5492/10740 [27:21:21<26:59:29, 18.52s/it]
{'loss': 0.2549, 'learning_rate': 1.011912256394659e-06, 'rewards/chosen': -2.9086146354675293, 'rewards/rejected': -6.630917072296143, 'rewards/accuracies': 0.75, 'rewards/margins': 3.7223026752471924, 'policy_logps/rejected': -370.1138000488281, 'policy_logps/chosen': -406.2884826660156, 'referece_logps/rejected': -303.8046569824219, 'referece_logps/chosen': -377.20233154296875, 'logits/rejected': 0.13106006383895874, 'logits/chosen': -0.11974628269672394, 'epoch': 3.07}


 51%|█████     | 5494/10740 [27:21:52<24:38:29, 16.91s/it]

 51%|█████     | 5495/10740 [27:22:10<25:01:52, 17.18s/it]

 51%|█████     | 5496/10740 [27:22:30<26:12:28, 17.99s/it]
{'loss': 0.1972, 'learning_rate': 1.010706000636787e-06, 'rewards/chosen': -3.1634302139282227, 'rewards/rejected': -5.583654880523682, 'rewards/accuracies': 0.875, 'rewards/margins': 2.420224189758301, 'policy_logps/rejected': -326.7308654785156, 'policy_logps/chosen': -359.5824890136719, 'referece_logps/rejected': -270.8943176269531, 'referece_logps/chosen': -327.9482421875, 'logits/rejected': -0.9133750200271606, 'logits/chosen': -0.8325541019439697, 'epoch': 3.07}

 51%|█████     | 5497/10740 [27:22:52<28:11:38, 19.36s/it]

 51%|█████     | 5498/10740 [27:23:12<28:30:43, 19.58s/it]


 51%|█████     | 5500/10740 [27:23:52<28:56:21, 19.88s/it]

 51%|█████     | 5501/10740 [27:24:24<33:55:24, 23.31s/it]

 51%|█████     | 5502/10740 [27:24:44<32:35:19, 22.40s/it]
{'loss': 0.2594, 'learning_rate': 1.0088965883364376e-06, 'rewards/chosen': -3.940779685974121, 'rewards/rejected': -7.227043628692627, 'rewards/accuracies': 0.875, 'rewards/margins': 3.286264419555664, 'policy_logps/rejected': -322.77374267578125, 'policy_logps/chosen': -456.8890075683594, 'referece_logps/rejected': -250.50332641601562, 'referece_logps/chosen': -417.481201171875, 'logits/rejected': 0.6482982039451599, 'logits/chosen': 0.6262843608856201, 'epoch': 3.07}


 51%|█████     | 5504/10740 [27:25:24<31:09:58, 21.43s/it]

 51%|█████▏    | 5505/10740 [27:25:40<28:49:20, 19.82s/it]
{'loss': 0.1585, 'learning_rate': 1.00799187089223e-06, 'rewards/chosen': -3.087512254714966, 'rewards/rejected': -7.567149639129639, 'rewards/accuracies': 1.0, 'rewards/margins': 4.47963809967041, 'policy_logps/rejected': -421.4969177246094, 'policy_logps/chosen': -290.8381042480469, 'referece_logps/rejected': -345.8254089355469, 'referece_logps/chosen': -259.9629821777344, 'logits/rejected': 0.3104202151298523, 'logits/chosen': 0.5824301838874817, 'epoch': 3.08}

 51%|█████▏    | 5506/10740 [27:25:59<28:25:14, 19.55s/it]

 51%|█████▏    | 5507/10740 [27:26:21<29:16:44, 20.14s/it]


 51%|█████▏    | 5509/10740 [27:26:59<28:08:54, 19.37s/it]

 51%|█████▏    | 5510/10740 [27:27:18<28:19:56, 19.50s/it]
{'loss': 0.1857, 'learning_rate': 1.0064839943133327e-06, 'rewards/chosen': -2.969956398010254, 'rewards/rejected': -5.973576545715332, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0036203861236572, 'policy_logps/rejected': -259.29827880859375, 'policy_logps/chosen': -371.28955078125, 'referece_logps/rejected': -199.5625457763672, 'referece_logps/chosen': -341.5899658203125, 'logits/rejected': -0.02757355570793152, 'logits/chosen': -0.123525470495224, 'epoch': 3.08}

 51%|█████▏    | 5511/10740 [27:27:35<27:16:38, 18.78s/it]


 51%|█████▏    | 5513/10740 [27:28:04<24:00:12, 16.53s/it]
{'loss': 0.3116, 'learning_rate': 1.0055792610697398e-06, 'rewards/chosen': -3.392798900604248, 'rewards/rejected': -5.09446382522583, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7016651630401611, 'policy_logps/rejected': -388.0557556152344, 'policy_logps/chosen': -340.3116455078125, 'referece_logps/rejected': -337.11114501953125, 'referece_logps/chosen': -306.3836364746094, 'logits/rejected': 0.28105437755584717, 'logits/chosen': 0.44812753796577454, 'epoch': 3.08}


 51%|█████▏    | 5515/10740 [27:28:42<25:41:01, 17.70s/it]
{'loss': 0.2703, 'learning_rate': 1.0049761029910584e-06, 'rewards/chosen': -4.191972255706787, 'rewards/rejected': -8.669008255004883, 'rewards/accuracies': 0.75, 'rewards/margins': 4.477034568786621, 'policy_logps/rejected': -601.345458984375, 'policy_logps/chosen': -416.7156982421875, 'referece_logps/rejected': -514.6553955078125, 'referece_logps/chosen': -374.79595947265625, 'logits/rejected': 0.11047421395778656, 'logits/chosen': 0.2302509993314743, 'epoch': 3.08}

 51%|█████▏    | 5516/10740 [27:29:01<26:14:51, 18.09s/it]

 51%|█████▏    | 5517/10740 [27:29:18<25:24:46, 17.52s/it]


 51%|█████▏    | 5519/10740 [27:30:02<29:09:46, 20.11s/it]
{'loss': 0.2184, 'learning_rate': 1.003769781622066e-06, 'rewards/chosen': -3.259321928024292, 'rewards/rejected': -8.38223934173584, 'rewards/accuracies': 1.0, 'rewards/margins': 5.1229166984558105, 'policy_logps/rejected': -414.1954040527344, 'policy_logps/chosen': -400.8876953125, 'referece_logps/rejected': -330.3730163574219, 'referece_logps/chosen': -368.2944641113281, 'logits/rejected': 0.162685826420784, 'logits/chosen': 0.16465309262275696, 'epoch': 3.08}

 51%|█████▏    | 5520/10740 [27:30:23<29:20:45, 20.24s/it]

 51%|█████▏    | 5521/10740 [27:30:41<28:24:10, 19.59s/it]


 51%|█████▏    | 5523/10740 [27:31:23<29:01:46, 20.03s/it]
{'loss': 0.2107, 'learning_rate': 1.00256345476714e-06, 'rewards/chosen': -3.2059836387634277, 'rewards/rejected': -5.0669379234313965, 'rewards/accuracies': 0.625, 'rewards/margins': 1.860954761505127, 'policy_logps/rejected': -457.3929138183594, 'policy_logps/chosen': -355.16326904296875, 'referece_logps/rejected': -406.7235107421875, 'referece_logps/chosen': -323.1034240722656, 'logits/rejected': 0.4444368779659271, 'logits/chosen': 0.4850555658340454, 'epoch': 3.09}

 51%|█████▏    | 5524/10740 [27:31:42<28:52:07, 19.92s/it]

 51%|█████▏    | 5525/10740 [27:31:58<27:09:30, 18.75s/it]

 51%|█████▏    | 5526/10740 [27:32:15<26:26:59, 18.26s/it]

 51%|█████▏    | 5527/10740 [27:32:32<25:49:36, 17.84s/it]

 51%|█████▏    | 5528/10740 [27:32:50<25:41:35, 17.75s/it]


 51%|█████▏    | 5530/10740 [27:33:15<21:45:41, 15.04s/it]

 51%|█████▏    | 5531/10740 [27:33:35<23:53:44, 16.51s/it]
{'loss': 0.208, 'learning_rate': 1.0001507916214688e-06, 'rewards/chosen': -4.922477722167969, 'rewards/rejected': -8.686952590942383, 'rewards/accuracies': 0.75, 'rewards/margins': 3.764474868774414, 'policy_logps/rejected': -443.70928955078125, 'policy_logps/chosen': -422.7065734863281, 'referece_logps/rejected': -356.8397521972656, 'referece_logps/chosen': -373.4818115234375, 'logits/rejected': -0.0053250789642333984, 'logits/chosen': -0.11957994103431702, 'epoch': 3.09}

 52%|█████▏    | 5532/10740 [27:33:54<25:13:09, 17.43s/it]

 52%|█████▏    | 5533/10740 [27:34:06<22:59:08, 15.89s/it]


 52%|█████▏    | 5535/10740 [27:34:31<20:20:02, 14.06s/it]
{'loss': 0.3502, 'learning_rate': 9.989444588417252e-07, 'rewards/chosen': -3.601975440979004, 'rewards/rejected': -8.535916328430176, 'rewards/accuracies': 1.0, 'rewards/margins': 4.933940887451172, 'policy_logps/rejected': -385.9516296386719, 'policy_logps/chosen': -479.9439697265625, 'referece_logps/rejected': -300.59246826171875, 'referece_logps/chosen': -443.9242248535156, 'logits/rejected': 0.42317527532577515, 'logits/chosen': 0.2938861846923828, 'epoch': 3.09}

 52%|█████▏    | 5536/10740 [27:34:48<21:53:53, 15.15s/it]


 52%|█████▏    | 5538/10740 [27:35:27<24:46:03, 17.14s/it]
{'loss': 0.2138, 'learning_rate': 9.980397101689565e-07, 'rewards/chosen': -3.453054428100586, 'rewards/rejected': -7.400824069976807, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9477698802948, 'policy_logps/rejected': -388.1085205078125, 'policy_logps/chosen': -270.08148193359375, 'referece_logps/rejected': -314.10028076171875, 'referece_logps/chosen': -235.55091857910156, 'logits/rejected': 0.009685203433036804, 'logits/chosen': 0.10246910899877548, 'epoch': 3.09}

 52%|█████▏    | 5539/10740 [27:35:38<22:23:38, 15.50s/it]

 52%|█████▏    | 5540/10740 [27:35:52<21:39:53, 15.00s/it]


 52%|█████▏    | 5542/10740 [27:36:25<22:09:03, 15.34s/it]

 52%|█████▏    | 5543/10740 [27:36:37<20:40:54, 14.33s/it]

 52%|█████▏    | 5544/10740 [27:36:55<22:25:02, 15.53s/it]
{'loss': 0.1113, 'learning_rate': 9.962302183779338e-07, 'rewards/chosen': -2.7241954803466797, 'rewards/rejected': -7.24820613861084, 'rewards/accuracies': 1.0, 'rewards/margins': 4.524010181427002, 'policy_logps/rejected': -460.2655029296875, 'policy_logps/chosen': -284.6547546386719, 'referece_logps/rejected': -387.783447265625, 'referece_logps/chosen': -257.41278076171875, 'logits/rejected': -0.270577073097229, 'logits/chosen': -0.10304480046033859, 'epoch': 3.1}

 52%|█████▏    | 5545/10740 [27:37:14<23:37:01, 16.37s/it]


 52%|█████▏    | 5547/10740 [27:37:53<26:12:23, 18.17s/it]
{'loss': 0.148, 'learning_rate': 9.953254767408801e-07, 'rewards/chosen': -5.099447727203369, 'rewards/rejected': -8.89876937866211, 'rewards/accuracies': 1.0, 'rewards/margins': 3.799321413040161, 'policy_logps/rejected': -498.244384765625, 'policy_logps/chosen': -465.9007263183594, 'referece_logps/rejected': -409.2566833496094, 'referece_logps/chosen': -414.9062805175781, 'logits/rejected': 0.03475705534219742, 'logits/chosen': 0.20323485136032104, 'epoch': 3.1}

 52%|█████▏    | 5548/10740 [27:38:11<26:10:39, 18.15s/it]

 52%|█████▏    | 5549/10740 [27:38:28<25:42:35, 17.83s/it]

 52%|█████▏    | 5550/10740 [27:38:47<25:48:25, 17.90s/it]

 52%|█████▏    | 5551/10740 [27:39:04<25:41:55, 17.83s/it]

 52%|█████▏    | 5552/10740 [27:39:24<26:30:29, 18.39s/it]


 52%|█████▏    | 5554/10740 [27:39:57<24:36:32, 17.08s/it]
{'loss': 0.2416, 'learning_rate': 9.932144290817012e-07, 'rewards/chosen': -3.0754828453063965, 'rewards/rejected': -7.0745849609375, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9991021156311035, 'policy_logps/rejected': -435.9486999511719, 'policy_logps/chosen': -349.6173095703125, 'referece_logps/rejected': -365.202880859375, 'referece_logps/chosen': -318.86248779296875, 'logits/rejected': 0.07675197720527649, 'logits/chosen': 0.12942315638065338, 'epoch': 3.1}

 52%|█████▏    | 5555/10740 [27:40:18<26:21:50, 18.30s/it]

 52%|█████▏    | 5556/10740 [27:40:38<26:58:25, 18.73s/it]

 52%|█████▏    | 5557/10740 [27:40:58<27:17:39, 18.96s/it]

 52%|█████▏    | 5558/10740 [27:41:14<26:00:32, 18.07s/it]


 52%|█████▏    | 5560/10740 [27:41:39<22:24:33, 15.57s/it]
{'loss': 0.169, 'learning_rate': 9.914049833723698e-07, 'rewards/chosen': -4.020442008972168, 'rewards/rejected': -6.86503267288208, 'rewards/accuracies': 0.75, 'rewards/margins': 2.844590902328491, 'policy_logps/rejected': -498.6732177734375, 'policy_logps/chosen': -534.9769897460938, 'referece_logps/rejected': -430.02288818359375, 'referece_logps/chosen': -494.7725830078125, 'logits/rejected': -0.4721122980117798, 'logits/chosen': -0.7095105648040771, 'epoch': 3.11}

 52%|█████▏    | 5561/10740 [27:41:56<22:58:41, 15.97s/it]

 52%|█████▏    | 5562/10740 [27:42:16<24:47:54, 17.24s/it]

 52%|█████▏    | 5563/10740 [27:42:28<22:28:27, 15.63s/it]


 52%|█████▏    | 5565/10740 [27:43:03<23:30:01, 16.35s/it]
{'loss': 0.2022, 'learning_rate': 9.898971331942834e-07, 'rewards/chosen': -3.027362108230591, 'rewards/rejected': -4.086674213409424, 'rewards/accuracies': 0.75, 'rewards/margins': 1.059311866760254, 'policy_logps/rejected': -233.6456298828125, 'policy_logps/chosen': -185.2125244140625, 'referece_logps/rejected': -192.77890014648438, 'referece_logps/chosen': -154.93890380859375, 'logits/rejected': -0.3234664797782898, 'logits/chosen': -0.3055293560028076, 'epoch': 3.11}

 52%|█████▏    | 5566/10740 [27:43:16<22:02:28, 15.34s/it]

 52%|█████▏    | 5567/10740 [27:43:29<20:45:27, 14.45s/it]

 52%|█████▏    | 5568/10740 [27:43:46<21:53:44, 15.24s/it]

 52%|█████▏    | 5569/10740 [27:43:59<20:58:49, 14.61s/it]

 52%|█████▏    | 5570/10740 [27:44:13<20:41:22, 14.41s/it]

 52%|█████▏    | 5571/10740 [27:44:31<22:19:29, 15.55s/it]

 52%|█████▏    | 5572/10740 [27:44:44<21:21:08, 14.87s/it]

 52%|█████▏    | 5573/10740 [27:44:56<20:08:27, 14.03s/it]

 52%|█████▏    | 5574/10740 [27:45:17<22:57:37, 16.00s/it]

 52%|█████▏    | 5575/10740 [27:45:37<24:39:58, 17.19s/it]

 52%|█████▏    | 5576/10740 [27:45:57<25:42:55, 17.93s/it]

 52%|█████▏    | 5577/10740 [27:46:14<25:30:05, 17.78s/it]

 52%|█████▏    | 5578/10740 [27:46:27<23:20:31, 16.28s/it]

 52%|█████▏    | 5579/10740 [27:46:45<24:01:35, 16.76s/it]


 52%|█████▏    | 5581/10740 [27:47:18<24:18:19, 16.96s/it]

 52%|█████▏    | 5582/10740 [27:47:37<25:30:42, 17.81s/it]

 52%|█████▏    | 5583/10740 [27:47:56<25:43:59, 17.96s/it]
{'loss': 0.3245, 'learning_rate': 9.844690873642437e-07, 'rewards/chosen': -3.80191707611084, 'rewards/rejected': -7.182209014892578, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3802924156188965, 'policy_logps/rejected': -479.3731384277344, 'policy_logps/chosen': -439.89056396484375, 'referece_logps/rejected': -407.551025390625, 'referece_logps/chosen': -401.87139892578125, 'logits/rejected': -0.1535097360610962, 'logits/chosen': -0.15943831205368042, 'epoch': 3.12}

 52%|█████▏    | 5584/10740 [27:48:15<26:23:30, 18.43s/it]

 52%|█████▏    | 5585/10740 [27:48:34<26:33:21, 18.55s/it]

 52%|█████▏    | 5586/10740 [27:48:53<26:44:08, 18.67s/it]

 52%|█████▏    | 5587/10740 [27:49:11<26:30:58, 18.52s/it]


 52%|█████▏    | 5589/10740 [27:49:44<24:50:29, 17.36s/it]
{'loss': 0.1312, 'learning_rate': 9.826598325606097e-07, 'rewards/chosen': -3.937551736831665, 'rewards/rejected': -8.544540405273438, 'rewards/accuracies': 1.0, 'rewards/margins': 4.606988430023193, 'policy_logps/rejected': -488.48907470703125, 'policy_logps/chosen': -334.0546875, 'referece_logps/rejected': -403.04364013671875, 'referece_logps/chosen': -294.6791687011719, 'logits/rejected': -0.15526092052459717, 'logits/chosen': -0.16120536625385284, 'epoch': 3.12}


 52%|█████▏    | 5591/10740 [27:50:12<22:36:36, 15.81s/it]
{'loss': 0.2369, 'learning_rate': 9.820567599505688e-07, 'rewards/chosen': -4.112569808959961, 'rewards/rejected': -7.987442493438721, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8748724460601807, 'policy_logps/rejected': -313.3853454589844, 'policy_logps/chosen': -363.4767761230469, 'referece_logps/rejected': -233.51092529296875, 'referece_logps/chosen': -322.35107421875, 'logits/rejected': 0.3467215597629547, 'logits/chosen': 0.5406562089920044, 'epoch': 3.12}


 52%|█████▏    | 5593/10740 [27:50:46<23:29:31, 16.43s/it]
{'loss': 0.1289, 'learning_rate': 9.814536938684543e-07, 'rewards/chosen': -2.077646017074585, 'rewards/rejected': -6.680030345916748, 'rewards/accuracies': 1.0, 'rewards/margins': 4.602384567260742, 'policy_logps/rejected': -253.79409790039062, 'policy_logps/chosen': -395.4353332519531, 'referece_logps/rejected': -186.99380493164062, 'referece_logps/chosen': -374.6589050292969, 'logits/rejected': -0.08075845241546631, 'logits/chosen': -0.3714825212955475, 'epoch': 3.12}

 52%|█████▏    | 5594/10740 [27:51:06<24:52:45, 17.40s/it]

 52%|█████▏    | 5595/10740 [27:51:24<25:09:37, 17.61s/it]

 52%|█████▏    | 5596/10740 [27:51:34<22:11:46, 15.53s/it]

 52%|█████▏    | 5597/10740 [27:51:55<24:23:15, 17.07s/it]

 52%|█████▏    | 5598/10740 [27:52:15<25:35:30, 17.92s/it]

 52%|█████▏    | 5599/10740 [27:52:36<27:10:54, 19.03s/it]


 52%|█████▏    | 5601/10740 [27:53:16<27:18:19, 19.13s/it]
{'loss': 0.235, 'learning_rate': 9.790414992072518e-07, 'rewards/chosen': -3.3944199085235596, 'rewards/rejected': -7.580873489379883, 'rewards/accuracies': 1.0, 'rewards/margins': 4.186453342437744, 'policy_logps/rejected': -417.303466796875, 'policy_logps/chosen': -395.9544677734375, 'referece_logps/rejected': -341.4947204589844, 'referece_logps/chosen': -362.01025390625, 'logits/rejected': 0.30400824546813965, 'logits/chosen': 0.221547469496727, 'epoch': 3.13}


 52%|█████▏    | 5603/10740 [27:53:58<28:49:52, 20.20s/it]
{'loss': 0.2564, 'learning_rate': 9.784384690557404e-07, 'rewards/chosen': -1.9695744514465332, 'rewards/rejected': -6.267431735992432, 'rewards/accuracies': 1.0, 'rewards/margins': 4.297857761383057, 'policy_logps/rejected': -319.0357360839844, 'policy_logps/chosen': -296.734619140625, 'referece_logps/rejected': -256.3614501953125, 'referece_logps/chosen': -277.03887939453125, 'logits/rejected': -0.19750767946243286, 'logits/chosen': -0.38489776849746704, 'epoch': 3.13}

 52%|█████▏    | 5604/10740 [27:54:15<27:32:27, 19.30s/it]

 52%|█████▏    | 5605/10740 [27:54:34<27:33:58, 19.33s/it]

 52%|█████▏    | 5606/10740 [27:54:53<27:23:52, 19.21s/it]


 52%|█████▏    | 5608/10740 [27:55:36<29:01:58, 20.37s/it]
{'loss': 0.2613, 'learning_rate': 9.769309284756624e-07, 'rewards/chosen': -2.6045498847961426, 'rewards/rejected': -7.293193817138672, 'rewards/accuracies': 1.0, 'rewards/margins': 4.688643455505371, 'policy_logps/rejected': -381.97076416015625, 'policy_logps/chosen': -381.4039001464844, 'referece_logps/rejected': -309.0388488769531, 'referece_logps/chosen': -355.3583984375, 'logits/rejected': -0.08931602537631989, 'logits/chosen': -0.3743845224380493, 'epoch': 3.13}

 52%|█████▏    | 5609/10740 [27:55:52<27:03:28, 18.98s/it]

 52%|█████▏    | 5610/10740 [27:56:08<26:05:05, 18.31s/it]

 52%|█████▏    | 5611/10740 [27:56:29<27:01:25, 18.97s/it]


 52%|█████▏    | 5613/10740 [27:57:04<26:30:02, 18.61s/it]
{'loss': 0.2455, 'learning_rate': 9.75423440350291e-07, 'rewards/chosen': -2.7627973556518555, 'rewards/rejected': -6.609521865844727, 'rewards/accuracies': 1.0, 'rewards/margins': 3.846724510192871, 'policy_logps/rejected': -322.8914794921875, 'policy_logps/chosen': -362.2626647949219, 'referece_logps/rejected': -256.7962341308594, 'referece_logps/chosen': -334.6347351074219, 'logits/rejected': 0.4003192186355591, 'logits/chosen': 0.3250279724597931, 'epoch': 3.14}

 52%|█████▏    | 5614/10740 [27:57:23<26:30:54, 18.62s/it]

 52%|█████▏    | 5615/10740 [27:57:43<27:07:01, 19.05s/it]

 52%|█████▏    | 5616/10740 [27:57:59<25:41:55, 18.06s/it]

 52%|█████▏    | 5617/10740 [27:58:21<27:36:57, 19.41s/it]

 52%|█████▏    | 5618/10740 [27:58:39<26:41:07, 18.76s/it]

 52%|█████▏    | 5619/10740 [27:58:58<26:47:27, 18.83s/it]

 52%|█████▏    | 5620/10740 [27:59:15<26:00:32, 18.29s/it]

 52%|█████▏    | 5621/10740 [27:59:35<26:46:01, 18.82s/it]

 52%|█████▏    | 5622/10740 [27:59:54<26:51:02, 18.89s/it]

 52%|█████▏    | 5623/10740 [28:00:13<27:06:05, 19.07s/it]

 52%|█████▏    | 5624/10740 [28:00:29<25:30:30, 17.95s/it]

 52%|█████▏    | 5625/10740 [28:00:48<26:16:35, 18.49s/it]

 52%|█████▏    | 5626/10740 [28:01:08<26:51:11, 18.90s/it]

 52%|█████▏    | 5627/10740 [28:01:28<27:21:29, 19.26s/it]

 52%|█████▏    | 5628/10740 [28:01:50<28:17:41, 19.93s/it]

 52%|█████▏    | 5629/10740 [28:02:09<28:08:49, 19.83s/it]

 52%|█████▏    | 5630/10740 [28:02:31<28:45:53, 20.26s/it]

 52%|█████▏    | 5631/10740 [28:02:47<26:52:49, 18.94s/it]

 52%|█████▏    | 5632/10740 [28:03:00<24:31:58, 17.29s/it]

 52%|█████▏    | 5633/10740 [28:03:12<22:11:31, 15.64s/it]

 52%|█████▏    | 5634/10740 [28:03:29<22:46:05, 16.05s/it]

 52%|█████▏    | 5635/10740 [28:03:43<21:56:29, 15.47s/it]

 52%|█████▏    | 5636/10740 [28:03:57<21:22:09, 15.07s/it]

 52%|█████▏    | 5637/10740 [28:04:09<20:08:39, 14.21s/it]

 52%|█████▏    | 5638/10740 [28:04:33<24:09:35, 17.05s/it]

 53%|█████▎    | 5639/10740 [28:04:53<25:37:49, 18.09s/it]

 53%|█████▎    | 5640/10740 [28:05:11<25:22:59, 17.92s/it]

 53%|█████▎    | 5641/10740 [28:05:31<26:23:47, 18.64s/it]

 53%|█████▎    | 5642/10740 [28:05:49<26:03:31, 18.40s/it]

 53%|█████▎    | 5643/10740 [28:06:09<26:44:23, 18.89s/it]

 53%|█████▎    | 5644/10740 [28:06:25<25:33:20, 18.05s/it]

 53%|█████▎    | 5645/10740 [28:06:46<26:52:40, 18.99s/it]

 53%|█████▎    | 5646/10740 [28:07:00<24:33:43, 17.36s/it]

 53%|█████▎    | 5647/10740 [28:07:14<23:04:14, 16.31s/it]

 53%|█████▎    | 5648/10740 [28:07:31<23:25:13, 16.56s/it]

 53%|█████▎    | 5649/10740 [28:07:46<22:45:23, 16.09s/it]

 53%|█████▎    | 5650/10740 [28:08:03<22:59:40, 16.26s/it]

 53%|█████▎    | 5651/10740 [28:08:18<22:29:29, 15.91s/it]


 53%|█████▎    | 5653/10740 [28:08:57<24:54:54, 17.63s/it]

 53%|█████▎    | 5654/10740 [28:09:19<27:02:26, 19.14s/it]

 53%|█████▎    | 5655/10740 [28:09:38<27:05:47, 19.18s/it]

 53%|█████▎    | 5656/10740 [28:09:59<27:28:05, 19.45s/it]

 53%|█████▎    | 5657/10740 [28:10:18<27:30:43, 19.49s/it]

 53%|█████▎    | 5658/10740 [28:10:41<29:05:59, 20.61s/it]
{'loss': 0.1967, 'learning_rate': 9.618589732157778e-07, 'rewards/chosen': -3.9915568828582764, 'rewards/rejected': -6.538740158081055, 'rewards/accuracies': 0.875, 'rewards/margins': 2.547183036804199, 'policy_logps/rejected': -586.2864990234375, 'policy_logps/chosen': -601.5711669921875, 'referece_logps/rejected': -520.8991088867188, 'referece_logps/chosen': -561.6555786132812, 'logits/rejected': 0.606900155544281, 'logits/chosen': 0.5014035701751709, 'epoch': 3.16}


 53%|█████▎    | 5660/10740 [28:11:18<27:20:50, 19.38s/it]

 53%|█████▎    | 5661/10740 [28:11:37<26:53:50, 19.06s/it]

 53%|█████▎    | 5662/10740 [28:11:56<27:12:56, 19.29s/it]

 53%|█████▎    | 5663/10740 [28:12:14<26:25:47, 18.74s/it]

 53%|█████▎    | 5664/10740 [28:12:27<23:59:37, 17.02s/it]

 53%|█████▎    | 5665/10740 [28:12:45<24:25:19, 17.32s/it]

 53%|█████▎    | 5666/10740 [28:13:02<24:24:48, 17.32s/it]

 53%|█████▎    | 5667/10740 [28:13:18<23:48:16, 16.89s/it]

 53%|█████▎    | 5668/10740 [28:13:36<24:07:03, 17.12s/it]

 53%|█████▎    | 5669/10740 [28:13:52<23:51:05, 16.93s/it]

 53%|█████▎    | 5670/10740 [28:14:04<21:31:24, 15.28s/it]

 53%|█████▎    | 5671/10740 [28:14:17<20:30:19, 14.56s/it]

 53%|█████▎    | 5672/10740 [28:14:36<22:44:29, 16.15s/it]

 53%|█████▎    | 5673/10740 [28:14:59<25:30:58, 18.13s/it]

 53%|█████▎    | 5674/10740 [28:15:16<24:55:49, 17.72s/it]

 53%|█████▎    | 5675/10740 [28:15:31<23:50:44, 16.95s/it]

 53%|█████▎    | 5676/10740 [28:15:51<25:02:30, 17.80s/it]

 53%|█████▎    | 5677/10740 [28:16:10<25:47:38, 18.34s/it]

 53%|█████▎    | 5678/10740 [28:16:31<26:32:25, 18.88s/it]

 53%|█████▎    | 5679/10740 [28:16:51<27:01:54, 19.23s/it]

 53%|█████▎    | 5680/10740 [28:17:02<23:46:27, 16.91s/it]

 53%|█████▎    | 5681/10740 [28:17:20<24:18:32, 17.30s/it]

 53%|█████▎    | 5682/10740 [28:17:41<25:31:01, 18.16s/it]

 53%|█████▎    | 5683/10740 [28:17:55<24:00:37, 17.09s/it]

 53%|█████▎    | 5684/10740 [28:18:18<26:13:17, 18.67s/it]

 53%|█████▎    | 5685/10740 [28:18:32<24:33:13, 17.49s/it]
{'loss': 0.224, 'learning_rate': 9.537235049313119e-07, 'rewards/chosen': -4.051104545593262, 'rewards/rejected': -8.234701156616211, 'rewards/accuracies': 0.875, 'rewards/margins': 4.183596134185791, 'policy_logps/rejected': -418.93560791015625, 'policy_logps/chosen': -584.3387451171875, 'referece_logps/rejected': -336.5885925292969, 'referece_logps/chosen': -543.82763671875, 'logits/rejected': 0.07882152497768402, 'logits/chosen': -0.07120833545923233, 'epoch': 3.18}


 53%|█████▎    | 5687/10740 [28:19:09<24:50:58, 17.70s/it]

 53%|█████▎    | 5688/10740 [28:19:21<22:31:34, 16.05s/it]

 53%|█████▎    | 5689/10740 [28:19:32<20:23:45, 14.54s/it]

 53%|█████▎    | 5690/10740 [28:19:47<20:51:02, 14.86s/it]

 53%|█████▎    | 5691/10740 [28:20:05<21:50:00, 15.57s/it]

 53%|█████▎    | 5692/10740 [28:20:26<24:28:03, 17.45s/it]

 53%|█████▎    | 5693/10740 [28:20:45<25:00:30, 17.84s/it]
{'loss': 0.198, 'learning_rate': 9.513135607670278e-07, 'rewards/chosen': -4.274049758911133, 'rewards/rejected': -7.516650199890137, 'rewards/accuracies': 0.875, 'rewards/margins': 3.242600440979004, 'policy_logps/rejected': -417.5006408691406, 'policy_logps/chosen': -431.7713928222656, 'referece_logps/rejected': -342.33416748046875, 'referece_logps/chosen': -389.0308532714844, 'logits/rejected': 0.3064432740211487, 'logits/chosen': 0.08064255118370056, 'epoch': 3.18}


 53%|█████▎    | 5695/10740 [28:21:24<26:22:25, 18.82s/it]

 53%|█████▎    | 5696/10740 [28:21:41<25:51:31, 18.46s/it]

 53%|█████▎    | 5697/10740 [28:21:56<24:04:38, 17.19s/it]

 53%|█████▎    | 5698/10740 [28:22:12<23:49:34, 17.01s/it]

 53%|█████▎    | 5699/10740 [28:22:29<23:48:05, 17.00s/it]

 53%|█████▎    | 5700/10740 [28:22:46<23:43:15, 16.94s/it]

 53%|█████▎    | 5701/10740 [28:23:05<24:32:36, 17.53s/it]

 53%|█████▎    | 5702/10740 [28:23:25<25:29:34, 18.22s/it]

 53%|█████▎    | 5703/10740 [28:23:36<22:39:10, 16.19s/it]

 53%|█████▎    | 5704/10740 [28:23:51<21:57:01, 15.69s/it]

 53%|█████▎    | 5705/10740 [28:24:12<24:17:00, 17.36s/it]

 53%|█████▎    | 5706/10740 [28:24:31<25:07:29, 17.97s/it]

 53%|█████▎    | 5707/10740 [28:24:48<24:42:48, 17.68s/it]

 53%|█████▎    | 5708/10740 [28:25:08<25:34:18, 18.29s/it]

 53%|█████▎    | 5709/10740 [28:25:27<25:54:59, 18.54s/it]

 53%|█████▎    | 5710/10740 [28:25:41<24:01:47, 17.20s/it]

 53%|█████▎    | 5711/10740 [28:25:59<24:23:38, 17.46s/it]

 53%|█████▎    | 5712/10740 [28:26:19<25:21:24, 18.16s/it]

 53%|█████▎    | 5713/10740 [28:26:39<26:03:49, 18.67s/it]

 53%|█████▎    | 5714/10740 [28:26:59<26:32:19, 19.01s/it]

 53%|█████▎    | 5715/10740 [28:27:14<24:43:15, 17.71s/it]

 53%|█████▎    | 5716/10740 [28:27:25<21:56:40, 15.72s/it]

 53%|█████▎    | 5717/10740 [28:27:36<19:55:49, 14.28s/it]

 53%|█████▎    | 5718/10740 [28:27:52<20:59:30, 15.05s/it]

 53%|█████▎    | 5719/10740 [28:28:09<21:40:12, 15.54s/it]
{'loss': 0.1511, 'learning_rate': 9.43483272125138e-07, 'rewards/chosen': -3.9905612468719482, 'rewards/rejected': -9.919828414916992, 'rewards/accuracies': 1.0, 'rewards/margins': 5.929267406463623, 'policy_logps/rejected': -437.6622009277344, 'policy_logps/chosen': -602.507568359375, 'referece_logps/rejected': -338.4639587402344, 'referece_logps/chosen': -562.6019287109375, 'logits/rejected': -0.1111670732498169, 'logits/chosen': -0.14354604482650757, 'epoch': 3.19}


 53%|█████▎    | 5721/10740 [28:28:44<23:15:06, 16.68s/it]

 53%|█████▎    | 5722/10740 [28:29:06<25:36:54, 18.38s/it]
{'loss': 0.2276, 'learning_rate': 9.425799917522218e-07, 'rewards/chosen': -4.080196380615234, 'rewards/rejected': -6.631896495819092, 'rewards/accuracies': 0.75, 'rewards/margins': 2.551699638366699, 'policy_logps/rejected': -412.9979248046875, 'policy_logps/chosen': -377.4975280761719, 'referece_logps/rejected': -346.678955078125, 'referece_logps/chosen': -336.695556640625, 'logits/rejected': -0.3610979914665222, 'logits/chosen': -0.3757604956626892, 'epoch': 3.2}


 53%|█████▎    | 5724/10740 [28:29:46<26:40:41, 19.15s/it]

 53%|█████▎    | 5725/10740 [28:30:06<26:53:01, 19.30s/it]

 53%|█████▎    | 5726/10740 [28:30:24<26:25:05, 18.97s/it]

 53%|█████▎    | 5727/10740 [28:30:42<25:54:41, 18.61s/it]

 53%|█████▎    | 5728/10740 [28:31:00<25:47:35, 18.53s/it]

 53%|█████▎    | 5729/10740 [28:31:20<26:14:37, 18.85s/it]
{'loss': 0.2273, 'learning_rate': 9.404725216138123e-07, 'rewards/chosen': -2.2863051891326904, 'rewards/rejected': -5.86583948135376, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5795340538024902, 'policy_logps/rejected': -314.4478759765625, 'policy_logps/chosen': -327.7821044921875, 'referece_logps/rejected': -255.78948974609375, 'referece_logps/chosen': -304.9190979003906, 'logits/rejected': 0.1088801696896553, 'logits/chosen': 0.0842922180891037, 'epoch': 3.2}


 53%|█████▎    | 5731/10740 [28:31:58<26:30:23, 19.05s/it]

 53%|█████▎    | 5732/10740 [28:32:18<26:38:46, 19.15s/it]

 53%|█████▎    | 5733/10740 [28:32:37<26:54:06, 19.34s/it]

 53%|█████▎    | 5734/10740 [28:32:56<26:25:00, 19.00s/it]

 53%|█████▎    | 5735/10740 [28:33:14<26:07:33, 18.79s/it]
{'loss': 0.185, 'learning_rate': 9.38666329434647e-07, 'rewards/chosen': -4.335109233856201, 'rewards/rejected': -8.154102325439453, 'rewards/accuracies': 0.875, 'rewards/margins': 3.81899356842041, 'policy_logps/rejected': -312.1715087890625, 'policy_logps/chosen': -390.9847412109375, 'referece_logps/rejected': -230.63047790527344, 'referece_logps/chosen': -347.6336364746094, 'logits/rejected': -0.2428639531135559, 'logits/chosen': -0.43946948647499084, 'epoch': 3.2}


 53%|█████▎    | 5737/10740 [28:33:46<24:02:33, 17.30s/it]

 53%|█████▎    | 5738/10740 [28:34:06<24:59:24, 17.99s/it]

 53%|█████▎    | 5739/10740 [28:34:19<22:54:39, 16.49s/it]
{'loss': 0.1892, 'learning_rate': 9.374623125190948e-07, 'rewards/chosen': -2.7004055976867676, 'rewards/rejected': -7.516525745391846, 'rewards/accuracies': 1.0, 'rewards/margins': 4.816120147705078, 'policy_logps/rejected': -314.431640625, 'policy_logps/chosen': -318.9468994140625, 'referece_logps/rejected': -239.26641845703125, 'referece_logps/chosen': -291.94281005859375, 'logits/rejected': -0.22569583356380463, 'logits/chosen': -0.17800326645374298, 'epoch': 3.21}


 53%|█████▎    | 5741/10740 [28:34:54<23:39:46, 17.04s/it]

 53%|█████▎    | 5742/10740 [28:35:14<24:42:31, 17.80s/it]

 53%|█████▎    | 5743/10740 [28:35:30<24:11:43, 17.43s/it]

 53%|█████▎    | 5744/10740 [28:35:47<23:40:53, 17.06s/it]

 53%|█████▎    | 5745/10740 [28:36:05<24:09:02, 17.41s/it]
{'loss': 0.2425, 'learning_rate': 9.356564583319334e-07, 'rewards/chosen': -3.0761911869049072, 'rewards/rejected': -7.623377323150635, 'rewards/accuracies': 0.875, 'rewards/margins': 4.547185897827148, 'policy_logps/rejected': -372.020751953125, 'policy_logps/chosen': -444.2262268066406, 'referece_logps/rejected': -295.7870178222656, 'referece_logps/chosen': -413.4643249511719, 'logits/rejected': 0.09532296657562256, 'logits/chosen': 0.024905793368816376, 'epoch': 3.21}


 54%|█████▎    | 5747/10740 [28:36:34<22:20:09, 16.10s/it]

 54%|█████▎    | 5748/10740 [28:36:50<22:11:25, 16.00s/it]
{'loss': 0.2299, 'learning_rate': 9.347536098735424e-07, 'rewards/chosen': -3.825716018676758, 'rewards/rejected': -7.040591239929199, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2148754596710205, 'policy_logps/rejected': -424.8651123046875, 'policy_logps/chosen': -632.7742309570312, 'referece_logps/rejected': -354.4591369628906, 'referece_logps/chosen': -594.51708984375, 'logits/rejected': -0.026224903762340546, 'logits/chosen': -0.2903437316417694, 'epoch': 3.21}

 54%|█████▎    | 5749/10740 [28:37:12<24:28:40, 17.66s/it]


 54%|█████▎    | 5751/10740 [28:37:50<25:28:14, 18.38s/it]

 54%|█████▎    | 5752/10740 [28:38:10<26:07:32, 18.86s/it]
{'loss': 0.2279, 'learning_rate': 9.335498951372149e-07, 'rewards/chosen': -3.0474746227264404, 'rewards/rejected': -6.748257637023926, 'rewards/accuracies': 0.875, 'rewards/margins': 3.700782299041748, 'policy_logps/rejected': -303.230712890625, 'policy_logps/chosen': -307.0433654785156, 'referece_logps/rejected': -235.74815368652344, 'referece_logps/chosen': -276.5686340332031, 'logits/rejected': 0.3765352964401245, 'logits/chosen': 0.49422508478164673, 'epoch': 3.21}


 54%|█████▎    | 5754/10740 [28:38:51<27:15:45, 19.68s/it]

 54%|█████▎    | 5755/10740 [28:39:12<28:05:27, 20.29s/it]

 54%|█████▎    | 5756/10740 [28:39:33<28:12:21, 20.37s/it]

 54%|█████▎    | 5757/10740 [28:39:49<26:23:38, 19.07s/it]

 54%|█████▎    | 5758/10740 [28:40:11<27:34:03, 19.92s/it]
{'loss': 0.2768, 'learning_rate': 9.317445048940685e-07, 'rewards/chosen': -3.134840488433838, 'rewards/rejected': -8.154690742492676, 'rewards/accuracies': 0.875, 'rewards/margins': 5.019850254058838, 'policy_logps/rejected': -374.129638671875, 'policy_logps/chosen': -403.35101318359375, 'referece_logps/rejected': -292.582763671875, 'referece_logps/chosen': -372.00262451171875, 'logits/rejected': -0.2808709740638733, 'logits/chosen': -0.6485530734062195, 'epoch': 3.22}


 54%|█████▎    | 5760/10740 [28:40:49<26:52:44, 19.43s/it]
{'loss': 0.1958, 'learning_rate': 9.311427575184787e-07, 'rewards/chosen': -3.2165567874908447, 'rewards/rejected': -5.420709609985352, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2041525840759277, 'policy_logps/rejected': -292.0320129394531, 'policy_logps/chosen': -369.5962219238281, 'referece_logps/rejected': -237.82492065429688, 'referece_logps/chosen': -337.4306640625, 'logits/rejected': -0.7566312551498413, 'logits/chosen': -0.8660979270935059, 'epoch': 3.22}


 54%|█████▎    | 5762/10740 [28:41:20<24:56:37, 18.04s/it]

 54%|█████▎    | 5763/10740 [28:41:39<25:06:54, 18.17s/it]
{'loss': 0.1805, 'learning_rate': 9.302401834940168e-07, 'rewards/chosen': -3.8544869422912598, 'rewards/rejected': -7.225439071655273, 'rewards/accuracies': 1.0, 'rewards/margins': 3.37095308303833, 'policy_logps/rejected': -304.6796875, 'policy_logps/chosen': -321.5723876953125, 'referece_logps/rejected': -232.42527770996094, 'referece_logps/chosen': -283.02752685546875, 'logits/rejected': 0.6402438879013062, 'logits/chosen': 0.5986554026603699, 'epoch': 3.22}

 54%|█████▎    | 5764/10740 [28:41:59<26:06:05, 18.88s/it]


 54%|█████▎    | 5766/10740 [28:42:38<26:12:21, 18.97s/it]

 54%|█████▎    | 5767/10740 [28:42:53<24:14:53, 17.55s/it]

 54%|█████▎    | 5768/10740 [28:43:13<25:20:54, 18.35s/it]
{'loss': 0.2196, 'learning_rate': 9.287360207145961e-07, 'rewards/chosen': -3.5790863037109375, 'rewards/rejected': -6.23354959487915, 'rewards/accuracies': 1.0, 'rewards/margins': 2.654463768005371, 'policy_logps/rejected': -609.5511474609375, 'policy_logps/chosen': -360.29901123046875, 'referece_logps/rejected': -547.2156982421875, 'referece_logps/chosen': -324.50811767578125, 'logits/rejected': -0.10808148980140686, 'logits/chosen': 0.11253921687602997, 'epoch': 3.22}


 54%|█████▎    | 5770/10740 [28:43:49<25:05:18, 18.17s/it]

 54%|█████▎    | 5771/10740 [28:44:11<26:51:17, 19.46s/it]

 54%|█████▎    | 5772/10740 [28:44:30<26:48:57, 19.43s/it]

 54%|█████▍    | 5773/10740 [28:44:51<27:20:04, 19.81s/it]

 54%|█████▍    | 5774/10740 [28:45:10<27:04:52, 19.63s/it]
{'loss': 0.2709, 'learning_rate': 9.269312395741138e-07, 'rewards/chosen': -2.8779072761535645, 'rewards/rejected': -5.147605895996094, 'rewards/accuracies': 0.625, 'rewards/margins': 2.2696990966796875, 'policy_logps/rejected': -303.3533935546875, 'policy_logps/chosen': -323.97247314453125, 'referece_logps/rejected': -251.87734985351562, 'referece_logps/chosen': -295.19342041015625, 'logits/rejected': 0.24439737200737, 'logits/chosen': 0.19235682487487793, 'epoch': 3.23}


 54%|█████▍    | 5776/10740 [28:45:51<27:48:22, 20.17s/it]
{'loss': 0.1916, 'learning_rate': 9.263296987350709e-07, 'rewards/chosen': -2.5484542846679688, 'rewards/rejected': -5.738908767700195, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1904547214508057, 'policy_logps/rejected': -319.6799621582031, 'policy_logps/chosen': -300.8824157714844, 'referece_logps/rejected': -262.2908935546875, 'referece_logps/chosen': -275.3978576660156, 'logits/rejected': -0.09045740216970444, 'logits/chosen': -0.23376917839050293, 'epoch': 3.23}


 54%|█████▍    | 5778/10740 [28:46:30<27:17:49, 19.80s/it]

 54%|█████▍    | 5779/10740 [28:46:51<27:46:31, 20.16s/it]

 54%|█████▍    | 5780/10740 [28:47:06<25:36:20, 18.58s/it]

 54%|█████▍    | 5781/10740 [28:47:27<26:19:33, 19.11s/it]
{'loss': 0.3736, 'learning_rate': 9.248259643748186e-07, 'rewards/chosen': -2.315852165222168, 'rewards/rejected': -4.569064617156982, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2532119750976562, 'policy_logps/rejected': -214.90652465820312, 'policy_logps/chosen': -250.25498962402344, 'referece_logps/rejected': -169.21588134765625, 'referece_logps/chosen': -227.09646606445312, 'logits/rejected': 0.09668505191802979, 'logits/chosen': -0.06407621502876282, 'epoch': 3.23}


 54%|█████▍    | 5783/10740 [28:48:07<27:17:35, 19.82s/it]
{'loss': 0.122, 'learning_rate': 9.242245183000864e-07, 'rewards/chosen': -2.915731430053711, 'rewards/rejected': -9.16335391998291, 'rewards/accuracies': 1.0, 'rewards/margins': 6.247623443603516, 'policy_logps/rejected': -489.20404052734375, 'policy_logps/chosen': -479.42059326171875, 'referece_logps/rejected': -397.57049560546875, 'referece_logps/chosen': -450.26324462890625, 'logits/rejected': 0.4117567241191864, 'logits/chosen': 0.38307952880859375, 'epoch': 3.23}

 54%|█████▍    | 5784/10740 [28:48:30<28:34:43, 20.76s/it]


 54%|█████▍    | 5786/10740 [28:49:11<28:12:29, 20.50s/it]

 54%|█████▍    | 5787/10740 [28:49:27<26:19:49, 19.14s/it]
{'loss': 0.2252, 'learning_rate': 9.230217090730136e-07, 'rewards/chosen': -4.173416614532471, 'rewards/rejected': -6.523136615753174, 'rewards/accuracies': 0.875, 'rewards/margins': 2.349720001220703, 'policy_logps/rejected': -327.5137939453125, 'policy_logps/chosen': -313.92694091796875, 'referece_logps/rejected': -262.282470703125, 'referece_logps/chosen': -272.19281005859375, 'logits/rejected': -0.2709306478500366, 'logits/chosen': -0.1335010826587677, 'epoch': 3.23}

 54%|█████▍    | 5788/10740 [28:49:38<23:14:53, 16.90s/it]

 54%|█████▍    | 5789/10740 [28:49:54<22:48:40, 16.59s/it]


 54%|█████▍    | 5791/10740 [28:50:33<24:33:55, 17.87s/it]

 54%|█████▍    | 5792/10740 [28:50:50<23:58:54, 17.45s/it]
{'loss': 0.1596, 'learning_rate': 9.21518355274958e-07, 'rewards/chosen': -3.441183090209961, 'rewards/rejected': -6.398929595947266, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9577465057373047, 'policy_logps/rejected': -397.363037109375, 'policy_logps/chosen': -368.2499694824219, 'referece_logps/rejected': -333.373779296875, 'referece_logps/chosen': -333.8381042480469, 'logits/rejected': -0.14247649908065796, 'logits/chosen': -0.1983024775981903, 'epoch': 3.24}


 54%|█████▍    | 5794/10740 [28:51:28<25:11:47, 18.34s/it]

 54%|█████▍    | 5795/10740 [28:51:46<24:56:16, 18.15s/it]
{'loss': 0.1854, 'learning_rate': 9.206164284345291e-07, 'rewards/chosen': -3.858837366104126, 'rewards/rejected': -6.846063613891602, 'rewards/accuracies': 0.875, 'rewards/margins': 2.987226963043213, 'policy_logps/rejected': -528.5563354492188, 'policy_logps/chosen': -484.2745666503906, 'referece_logps/rejected': -460.095703125, 'referece_logps/chosen': -445.68621826171875, 'logits/rejected': -0.3141883611679077, 'logits/chosen': -0.16818411648273468, 'epoch': 3.24}


 54%|█████▍    | 5797/10740 [28:52:21<24:40:14, 17.97s/it]
{'loss': 0.2729, 'learning_rate': 9.200151799293217e-07, 'rewards/chosen': -3.278871536254883, 'rewards/rejected': -4.780092239379883, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5012205839157104, 'policy_logps/rejected': -374.0304260253906, 'policy_logps/chosen': -477.416259765625, 'referece_logps/rejected': -326.22955322265625, 'referece_logps/chosen': -444.6275634765625, 'logits/rejected': -0.09269110858440399, 'logits/chosen': -0.26070401072502136, 'epoch': 3.24}


 54%|█████▍    | 5799/10740 [28:53:00<25:26:25, 18.54s/it]
{'loss': 0.1918, 'learning_rate': 9.19413960523376e-07, 'rewards/chosen': -3.0074849128723145, 'rewards/rejected': -5.698605537414551, 'rewards/accuracies': 1.0, 'rewards/margins': 2.691120147705078, 'policy_logps/rejected': -285.20947265625, 'policy_logps/chosen': -419.970703125, 'referece_logps/rejected': -228.2234344482422, 'referece_logps/chosen': -389.8958740234375, 'logits/rejected': 0.33834561705589294, 'logits/chosen': 0.21639853715896606, 'epoch': 3.24}


 54%|█████▍    | 5801/10740 [28:53:24<20:49:44, 15.18s/it]

 54%|█████▍    | 5802/10740 [28:53:45<23:15:17, 16.95s/it]

 54%|█████▍    | 5803/10740 [28:54:08<25:38:11, 18.69s/it]

 54%|█████▍    | 5804/10740 [28:54:25<25:16:56, 18.44s/it]

 54%|█████▍    | 5805/10740 [28:54:41<24:13:49, 17.68s/it]
{'loss': 0.2953, 'learning_rate': 9.176104790883503e-07, 'rewards/chosen': -4.4398274421691895, 'rewards/rejected': -8.846277236938477, 'rewards/accuracies': 0.875, 'rewards/margins': 4.406449317932129, 'policy_logps/rejected': -453.2484436035156, 'policy_logps/chosen': -452.7142333984375, 'referece_logps/rejected': -364.7856750488281, 'referece_logps/chosen': -408.3159484863281, 'logits/rejected': -0.5333587527275085, 'logits/chosen': -0.6215177774429321, 'epoch': 3.24}

 54%|█████▍    | 5806/10740 [28:55:01<24:52:22, 18.15s/it]


 54%|█████▍    | 5808/10740 [28:55:43<27:19:41, 19.95s/it]
{'loss': 0.1967, 'learning_rate': 9.167088391644159e-07, 'rewards/chosen': -2.6750736236572266, 'rewards/rejected': -6.982958793640137, 'rewards/accuracies': 0.75, 'rewards/margins': 4.307885646820068, 'policy_logps/rejected': -290.41217041015625, 'policy_logps/chosen': -481.1341247558594, 'referece_logps/rejected': -220.58258056640625, 'referece_logps/chosen': -454.3834228515625, 'logits/rejected': -0.19928616285324097, 'logits/chosen': -0.5532634258270264, 'epoch': 3.24}

 54%|█████▍    | 5809/10740 [28:55:56<24:29:17, 17.88s/it]


 54%|█████▍    | 5811/10740 [28:56:34<25:19:18, 18.49s/it]

 54%|█████▍    | 5812/10740 [28:56:47<23:02:12, 16.83s/it]

 54%|█████▍    | 5813/10740 [28:57:02<22:17:42, 16.29s/it]

 54%|█████▍    | 5814/10740 [28:57:14<20:18:40, 14.84s/it]
{'loss': 0.2038, 'learning_rate': 9.14905764593991e-07, 'rewards/chosen': -3.692378282546997, 'rewards/rejected': -5.888462543487549, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1960840225219727, 'policy_logps/rejected': -270.0337829589844, 'policy_logps/chosen': -316.7502136230469, 'referece_logps/rejected': -211.149169921875, 'referece_logps/chosen': -279.8263854980469, 'logits/rejected': -0.08128800988197327, 'logits/chosen': -0.24747620522975922, 'epoch': 3.25}


 54%|█████▍    | 5816/10740 [28:57:37<18:09:44, 13.28s/it]
{'loss': 0.2241, 'learning_rate': 9.143048013618827e-07, 'rewards/chosen': -3.5123841762542725, 'rewards/rejected': -7.8924479484558105, 'rewards/accuracies': 0.875, 'rewards/margins': 4.380063533782959, 'policy_logps/rejected': -426.49530029296875, 'policy_logps/chosen': -419.8722229003906, 'referece_logps/rejected': -347.57080078125, 'referece_logps/chosen': -384.7483825683594, 'logits/rejected': 0.8088533878326416, 'logits/chosen': 0.8278655409812927, 'epoch': 3.25}


 54%|█████▍    | 5818/10740 [28:58:14<22:13:48, 16.26s/it]

 54%|█████▍    | 5819/10740 [28:58:31<22:32:28, 16.49s/it]

 54%|█████▍    | 5820/10740 [28:58:48<22:40:00, 16.59s/it]

 54%|█████▍    | 5821/10740 [28:59:10<24:44:24, 18.11s/it]

 54%|█████▍    | 5822/10740 [28:59:30<25:42:59, 18.82s/it]

 54%|█████▍    | 5823/10740 [28:59:49<25:51:44, 18.94s/it]

 54%|█████▍    | 5824/10740 [29:00:06<24:46:08, 18.14s/it]
{'loss': 0.1849, 'learning_rate': 9.119012623871735e-07, 'rewards/chosen': -2.7039079666137695, 'rewards/rejected': -6.396536827087402, 'rewards/accuracies': 1.0, 'rewards/margins': 3.692628860473633, 'policy_logps/rejected': -515.6755981445312, 'policy_logps/chosen': -545.9952392578125, 'referece_logps/rejected': -451.71026611328125, 'referece_logps/chosen': -518.9561157226562, 'logits/rejected': -0.28815191984176636, 'logits/chosen': -0.3290688097476959, 'epoch': 3.25}


 54%|█████▍    | 5826/10740 [29:00:42<25:06:21, 18.39s/it]

 54%|█████▍    | 5827/10740 [29:01:02<25:43:25, 18.85s/it]
{'loss': 0.2036, 'learning_rate': 9.11000066731363e-07, 'rewards/chosen': -2.7498891353607178, 'rewards/rejected': -4.747872829437256, 'rewards/accuracies': 0.875, 'rewards/margins': 1.997983694076538, 'policy_logps/rejected': -350.165771484375, 'policy_logps/chosen': -329.837646484375, 'referece_logps/rejected': -302.68701171875, 'referece_logps/chosen': -302.3387451171875, 'logits/rejected': 0.43262577056884766, 'logits/chosen': 0.23463237285614014, 'epoch': 3.26}

 54%|█████▍    | 5828/10740 [29:01:13<22:30:41, 16.50s/it]


 54%|█████▍    | 5830/10740 [29:01:44<21:58:51, 16.12s/it]

 54%|█████▍    | 5831/10740 [29:02:06<24:22:23, 17.87s/it]

 54%|█████▍    | 5832/10740 [29:02:26<25:05:04, 18.40s/it]

 54%|█████▍    | 5833/10740 [29:02:46<25:51:56, 18.98s/it]
{'loss': 0.2472, 'learning_rate': 9.091978947159418e-07, 'rewards/chosen': -3.588656425476074, 'rewards/rejected': -5.471667766571045, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8830113410949707, 'policy_logps/rejected': -385.1134033203125, 'policy_logps/chosen': -422.7652282714844, 'referece_logps/rejected': -330.3967590332031, 'referece_logps/chosen': -386.878662109375, 'logits/rejected': -0.23355618119239807, 'logits/chosen': -0.30856436491012573, 'epoch': 3.26}


 54%|█████▍    | 5835/10740 [29:03:22<24:39:46, 18.10s/it]
{'loss': 0.1489, 'learning_rate': 9.085972364888061e-07, 'rewards/chosen': -3.3698763847351074, 'rewards/rejected': -5.696805477142334, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3269288539886475, 'policy_logps/rejected': -477.5526428222656, 'policy_logps/chosen': -427.3331298828125, 'referece_logps/rejected': -420.5845642089844, 'referece_logps/chosen': -393.6343994140625, 'logits/rejected': -0.4532228112220764, 'logits/chosen': -0.4616658091545105, 'epoch': 3.26}

 54%|█████▍    | 5836/10740 [29:03:43<25:58:06, 19.06s/it]

 54%|█████▍    | 5837/10740 [29:04:00<24:56:52, 18.32s/it]

 54%|█████▍    | 5838/10740 [29:04:16<23:57:18, 17.59s/it]

 54%|█████▍    | 5839/10740 [29:04:38<25:39:25, 18.85s/it]


 54%|█████▍    | 5841/10740 [29:05:05<21:51:56, 16.07s/it]
{'loss': 0.2408, 'learning_rate': 9.067954622007675e-07, 'rewards/chosen': -3.758495330810547, 'rewards/rejected': -6.682162761688232, 'rewards/accuracies': 0.875, 'rewards/margins': 2.923668146133423, 'policy_logps/rejected': -425.8387145996094, 'policy_logps/chosen': -259.6461181640625, 'referece_logps/rejected': -359.01708984375, 'referece_logps/chosen': -222.06117248535156, 'logits/rejected': -0.6833072304725647, 'logits/chosen': -0.44810327887535095, 'epoch': 3.26}

 54%|█████▍    | 5842/10740 [29:05:24<23:03:23, 16.95s/it]

 54%|█████▍    | 5843/10740 [29:05:44<24:19:22, 17.88s/it]

 54%|█████▍    | 5844/10740 [29:06:04<25:16:19, 18.58s/it]

 54%|█████▍    | 5845/10740 [29:06:24<25:46:17, 18.95s/it]

 54%|█████▍    | 5846/10740 [29:06:43<26:05:27, 19.19s/it]

 54%|█████▍    | 5847/10740 [29:07:01<25:22:39, 18.67s/it]


 54%|█████▍    | 5849/10740 [29:07:37<24:49:47, 18.28s/it]

 54%|█████▍    | 5850/10740 [29:07:56<25:22:31, 18.68s/it]
{'loss': 0.1786, 'learning_rate': 9.040933748216237e-07, 'rewards/chosen': -3.2198803424835205, 'rewards/rejected': -8.755264282226562, 'rewards/accuracies': 1.0, 'rewards/margins': 5.535383701324463, 'policy_logps/rejected': -318.3524169921875, 'policy_logps/chosen': -467.31298828125, 'referece_logps/rejected': -230.7998046875, 'referece_logps/chosen': -435.1141662597656, 'logits/rejected': -0.44193127751350403, 'logits/chosen': -0.7810523509979248, 'epoch': 3.27}

 54%|█████▍    | 5851/10740 [29:08:19<26:58:15, 19.86s/it]

 54%|█████▍    | 5852/10740 [29:08:39<27:03:17, 19.93s/it]

 54%|█████▍    | 5853/10740 [29:08:50<23:17:02, 17.15s/it]

 55%|█████▍    | 5854/10740 [29:09:09<24:15:06, 17.87s/it]


 55%|█████▍    | 5856/10740 [29:09:49<25:18:48, 18.66s/it]
{'loss': 0.2206, 'learning_rate': 9.022923745391204e-07, 'rewards/chosen': -4.3828020095825195, 'rewards/rejected': -6.715816497802734, 'rewards/accuracies': 1.0, 'rewards/margins': 2.333014965057373, 'policy_logps/rejected': -393.3360290527344, 'policy_logps/chosen': -462.2734069824219, 'referece_logps/rejected': -326.1778869628906, 'referece_logps/chosen': -418.44537353515625, 'logits/rejected': 0.45814383029937744, 'logits/chosen': 0.45653197169303894, 'epoch': 3.27}


 55%|█████▍    | 5858/10740 [29:10:20<23:51:46, 17.60s/it]

 55%|█████▍    | 5859/10740 [29:10:36<23:15:13, 17.15s/it]
{'loss': 0.2137, 'learning_rate': 9.013919940004156e-07, 'rewards/chosen': -2.591169834136963, 'rewards/rejected': -6.207558631896973, 'rewards/accuracies': 0.875, 'rewards/margins': 3.616389036178589, 'policy_logps/rejected': -290.36572265625, 'policy_logps/chosen': -271.7221984863281, 'referece_logps/rejected': -228.29014587402344, 'referece_logps/chosen': -245.81051635742188, 'logits/rejected': 0.0676778107881546, 'logits/chosen': -0.11213475465774536, 'epoch': 3.27}

 55%|█████▍    | 5860/10740 [29:10:55<23:55:43, 17.65s/it]

 55%|█████▍    | 5861/10740 [29:11:09<22:30:46, 16.61s/it]

 55%|█████▍    | 5862/10740 [29:11:21<20:34:17, 15.18s/it]

 55%|█████▍    | 5863/10740 [29:11:41<22:34:15, 16.66s/it]

 55%|█████▍    | 5864/10740 [29:11:58<22:22:55, 16.52s/it]


 55%|█████▍    | 5866/10740 [29:12:39<25:16:31, 18.67s/it]
{'loss': 0.1871, 'learning_rate': 8.992914212529e-07, 'rewards/chosen': -3.0709290504455566, 'rewards/rejected': -6.626086235046387, 'rewards/accuracies': 0.875, 'rewards/margins': 3.555156707763672, 'policy_logps/rejected': -352.6824951171875, 'policy_logps/chosen': -289.26812744140625, 'referece_logps/rejected': -286.42169189453125, 'referece_logps/chosen': -258.5588684082031, 'logits/rejected': -0.30650603771209717, 'logits/chosen': -0.3824261724948883, 'epoch': 3.28}


 55%|█████▍    | 5868/10740 [29:13:13<24:01:31, 17.75s/it]
{'loss': 0.1709, 'learning_rate': 8.986913396386048e-07, 'rewards/chosen': -3.011998176574707, 'rewards/rejected': -6.316210746765137, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3042125701904297, 'policy_logps/rejected': -263.2265625, 'policy_logps/chosen': -198.312744140625, 'referece_logps/rejected': -200.064453125, 'referece_logps/chosen': -168.19277954101562, 'logits/rejected': -0.4516306519508362, 'logits/chosen': -0.3202489912509918, 'epoch': 3.28}


 55%|█████▍    | 5870/10740 [29:13:39<20:43:27, 15.32s/it]

 55%|█████▍    | 5871/10740 [29:14:01<23:24:58, 17.31s/it]

 55%|█████▍    | 5872/10740 [29:14:19<23:52:05, 17.65s/it]

 55%|█████▍    | 5873/10740 [29:14:37<23:47:28, 17.60s/it]
{'loss': 0.1362, 'learning_rate': 8.971912973301398e-07, 'rewards/chosen': -3.7026634216308594, 'rewards/rejected': -8.689157485961914, 'rewards/accuracies': 0.875, 'rewards/margins': 4.9864935874938965, 'policy_logps/rejected': -556.6009521484375, 'policy_logps/chosen': -325.77972412109375, 'referece_logps/rejected': -469.70941162109375, 'referece_logps/chosen': -288.7531433105469, 'logits/rejected': -0.9878539443016052, 'logits/chosen': -0.5624078512191772, 'epoch': 3.28}

 55%|█████▍    | 5874/10740 [29:14:59<25:26:45, 18.83s/it]

 55%|█████▍    | 5875/10740 [29:15:12<23:26:22, 17.34s/it]


 55%|█████▍    | 5877/10740 [29:15:45<22:39:45, 16.78s/it]
{'loss': 0.1714, 'learning_rate': 8.959914316323014e-07, 'rewards/chosen': -3.67486572265625, 'rewards/rejected': -9.219706535339355, 'rewards/accuracies': 1.0, 'rewards/margins': 5.5448408126831055, 'policy_logps/rejected': -377.9312438964844, 'policy_logps/chosen': -388.0992736816406, 'referece_logps/rejected': -285.73419189453125, 'referece_logps/chosen': -351.35064697265625, 'logits/rejected': -0.5723463296890259, 'logits/chosen': -0.5285510420799255, 'epoch': 3.28}


 55%|█████▍    | 5879/10740 [29:16:17<21:37:13, 16.01s/it]

 55%|█████▍    | 5880/10740 [29:16:37<23:07:39, 17.13s/it]
{'loss': 0.2219, 'learning_rate': 8.950916315916912e-07, 'rewards/chosen': -4.0805487632751465, 'rewards/rejected': -7.738083362579346, 'rewards/accuracies': 1.0, 'rewards/margins': 3.657534599304199, 'policy_logps/rejected': -422.7310791015625, 'policy_logps/chosen': -346.984375, 'referece_logps/rejected': -345.35028076171875, 'referece_logps/chosen': -306.1789245605469, 'logits/rejected': -0.8031467199325562, 'logits/chosen': -0.7200646996498108, 'epoch': 3.28}


 55%|█████▍    | 5882/10740 [29:17:11<22:42:43, 16.83s/it]
{'loss': 0.1828, 'learning_rate': 8.944918125608519e-07, 'rewards/chosen': -3.857395648956299, 'rewards/rejected': -6.403952121734619, 'rewards/accuracies': 0.875, 'rewards/margins': 2.546556234359741, 'policy_logps/rejected': -371.6351013183594, 'policy_logps/chosen': -375.6839294433594, 'referece_logps/rejected': -307.5955810546875, 'referece_logps/chosen': -337.1099853515625, 'logits/rejected': 0.20623795688152313, 'logits/chosen': 0.14639604091644287, 'epoch': 3.29}

 55%|█████▍    | 5883/10740 [29:17:32<24:14:23, 17.97s/it]

 55%|█████▍    | 5884/10740 [29:17:50<24:23:49, 18.09s/it]


 55%|█████▍    | 5886/10740 [29:18:21<22:00:30, 16.32s/it]

 55%|█████▍    | 5887/10740 [29:18:43<24:17:07, 18.02s/it]
{'loss': 0.3814, 'learning_rate': 8.929924333950685e-07, 'rewards/chosen': -3.815272092819214, 'rewards/rejected': -6.9358110427856445, 'rewards/accuracies': 0.75, 'rewards/margins': 3.1205384731292725, 'policy_logps/rejected': -324.38818359375, 'policy_logps/chosen': -311.2696533203125, 'referece_logps/rejected': -255.0300750732422, 'referece_logps/chosen': -273.1169128417969, 'logits/rejected': 0.1610296666622162, 'logits/chosen': 0.04131069779396057, 'epoch': 3.29}

 55%|█████▍    | 5888/10740 [29:19:00<24:02:16, 17.84s/it]

 55%|█████▍    | 5889/10740 [29:19:21<25:01:45, 18.57s/it]


 55%|█████▍    | 5891/10740 [29:19:53<23:54:28, 17.75s/it]
{'loss': 0.2356, 'learning_rate': 8.917931050855925e-07, 'rewards/chosen': -4.091907024383545, 'rewards/rejected': -8.669685363769531, 'rewards/accuracies': 1.0, 'rewards/margins': 4.577777862548828, 'policy_logps/rejected': -505.6124572753906, 'policy_logps/chosen': -361.1602783203125, 'referece_logps/rejected': -418.9156799316406, 'referece_logps/chosen': -320.24114990234375, 'logits/rejected': -0.10754060745239258, 'logits/chosen': -0.19257564842700958, 'epoch': 3.29}


 55%|█████▍    | 5893/10740 [29:20:33<25:27:01, 18.90s/it]
{'loss': 0.2232, 'learning_rate': 8.911934998718698e-07, 'rewards/chosen': -3.9250376224517822, 'rewards/rejected': -7.45249080657959, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5274529457092285, 'policy_logps/rejected': -414.5130920410156, 'policy_logps/chosen': -404.124267578125, 'referece_logps/rejected': -339.9881591796875, 'referece_logps/chosen': -364.8738708496094, 'logits/rejected': -1.0118963718414307, 'logits/chosen': -1.1713329553604126, 'epoch': 3.29}


 55%|█████▍    | 5895/10740 [29:21:06<23:54:30, 17.76s/it]
{'loss': 0.2256, 'learning_rate': 8.905939342430183e-07, 'rewards/chosen': -3.4121904373168945, 'rewards/rejected': -5.78256368637085, 'rewards/accuracies': 0.875, 'rewards/margins': 2.370373487472534, 'policy_logps/rejected': -348.2509460449219, 'policy_logps/chosen': -394.719970703125, 'referece_logps/rejected': -290.4253234863281, 'referece_logps/chosen': -360.59808349609375, 'logits/rejected': -0.7418033480644226, 'logits/chosen': -0.8182399272918701, 'epoch': 3.29}

 55%|█████▍    | 5896/10740 [29:21:26<24:54:42, 18.51s/it]


 55%|█████▍    | 5898/10740 [29:22:02<24:32:01, 18.24s/it]

 55%|█████▍    | 5899/10740 [29:22:21<25:06:21, 18.67s/it]
{'loss': 0.1811, 'learning_rate': 8.893949226124257e-07, 'rewards/chosen': -3.278005838394165, 'rewards/rejected': -7.438600540161133, 'rewards/accuracies': 0.875, 'rewards/margins': 4.160594463348389, 'policy_logps/rejected': -390.7283935546875, 'policy_logps/chosen': -372.4411926269531, 'referece_logps/rejected': -316.3423767089844, 'referece_logps/chosen': -339.6611328125, 'logits/rejected': 0.498262494802475, 'logits/chosen': 0.3693193793296814, 'epoch': 3.3}

 55%|█████▍    | 5900/10740 [29:22:41<25:36:31, 19.05s/it]

 55%|█████▍    | 5901/10740 [29:22:53<22:32:20, 16.77s/it]

 55%|█████▍    | 5902/10740 [29:23:10<22:50:31, 17.00s/it]

 55%|█████▍    | 5903/10740 [29:23:26<22:22:38, 16.65s/it]

 55%|█████▍    | 5904/10740 [29:23:39<20:42:31, 15.42s/it]

 55%|█████▍    | 5905/10740 [29:23:57<21:44:57, 16.19s/it]

 55%|█████▍    | 5906/10740 [29:24:10<20:38:23, 15.37s/it]

 55%|█████▌    | 5907/10740 [29:24:33<23:33:22, 17.55s/it]

 55%|█████▌    | 5908/10740 [29:24:52<24:28:10, 18.23s/it]

 55%|█████▌    | 5909/10740 [29:25:13<25:26:59, 18.96s/it]

 55%|█████▌    | 5910/10740 [29:25:33<25:41:23, 19.15s/it]

 55%|█████▌    | 5911/10740 [29:25:49<24:37:23, 18.36s/it]

 55%|█████▌    | 5912/10740 [29:26:09<25:08:27, 18.75s/it]

 55%|█████▌    | 5913/10740 [29:26:29<25:42:05, 19.17s/it]

 55%|█████▌    | 5914/10740 [29:26:42<23:17:14, 17.37s/it]

 55%|█████▌    | 5915/10740 [29:27:01<23:43:53, 17.71s/it]

 55%|█████▌    | 5916/10740 [29:27:16<22:57:18, 17.13s/it]

 55%|█████▌    | 5917/10740 [29:27:37<24:27:09, 18.25s/it]


 55%|█████▌    | 5919/10740 [29:28:14<24:17:30, 18.14s/it]
{'loss': 0.1936, 'learning_rate': 8.834023137006381e-07, 'rewards/chosen': -3.537914276123047, 'rewards/rejected': -6.331429481506348, 'rewards/accuracies': 0.875, 'rewards/margins': 2.793515682220459, 'policy_logps/rejected': -269.5625915527344, 'policy_logps/chosen': -433.9830322265625, 'referece_logps/rejected': -206.24830627441406, 'referece_logps/chosen': -398.6039123535156, 'logits/rejected': 0.2727556526660919, 'logits/chosen': 0.11376383900642395, 'epoch': 3.31}


 55%|█████▌    | 5921/10740 [29:28:56<26:03:15, 19.46s/it]

 55%|█████▌    | 5922/10740 [29:29:16<26:10:13, 19.55s/it]
{'loss': 0.204, 'learning_rate': 8.825037829041481e-07, 'rewards/chosen': -2.9246997833251953, 'rewards/rejected': -4.807223796844482, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8825243711471558, 'policy_logps/rejected': -254.1361541748047, 'policy_logps/chosen': -241.42909240722656, 'referece_logps/rejected': -206.06390380859375, 'referece_logps/chosen': -212.1820831298828, 'logits/rejected': -0.7780982851982117, 'logits/chosen': -0.7856529951095581, 'epoch': 3.31}


 55%|█████▌    | 5924/10740 [29:29:52<24:50:59, 18.58s/it]
{'loss': 0.1808, 'learning_rate': 8.819048157605984e-07, 'rewards/chosen': -2.952364683151245, 'rewards/rejected': -6.925931930541992, 'rewards/accuracies': 0.875, 'rewards/margins': 3.973567247390747, 'policy_logps/rejected': -335.6757507324219, 'policy_logps/chosen': -335.5063171386719, 'referece_logps/rejected': -266.4164733886719, 'referece_logps/chosen': -305.9826965332031, 'logits/rejected': -0.15411385893821716, 'logits/chosen': -0.2610737979412079, 'epoch': 3.31}

 55%|█████▌    | 5925/10740 [29:30:08<23:59:32, 17.94s/it]


 55%|█████▌    | 5927/10740 [29:30:42<23:00:32, 17.21s/it]

 55%|█████▌    | 5928/10740 [29:31:02<24:04:53, 18.02s/it]
{'loss': 0.1802, 'learning_rate': 8.807070105839502e-07, 'rewards/chosen': -3.165785312652588, 'rewards/rejected': -5.863602638244629, 'rewards/accuracies': 0.875, 'rewards/margins': 2.697817325592041, 'policy_logps/rejected': -420.169677734375, 'policy_logps/chosen': -347.5130310058594, 'referece_logps/rejected': -361.53363037109375, 'referece_logps/chosen': -315.85516357421875, 'logits/rejected': 0.32276058197021484, 'logits/chosen': 0.4068465232849121, 'epoch': 3.31}

 55%|█████▌    | 5929/10740 [29:31:21<24:16:03, 18.16s/it]

 55%|█████▌    | 5930/10740 [29:31:34<22:34:43, 16.90s/it]


 55%|█████▌    | 5932/10740 [29:32:06<21:55:28, 16.42s/it]
{'loss': 0.1505, 'learning_rate': 8.795093790071212e-07, 'rewards/chosen': -5.218057155609131, 'rewards/rejected': -9.491715431213379, 'rewards/accuracies': 1.0, 'rewards/margins': 4.27365779876709, 'policy_logps/rejected': -521.9153442382812, 'policy_logps/chosen': -396.27911376953125, 'referece_logps/rejected': -426.9981994628906, 'referece_logps/chosen': -344.0985412597656, 'logits/rejected': -0.21561509370803833, 'logits/chosen': -0.34737512469291687, 'epoch': 3.31}

 55%|█████▌    | 5933/10740 [29:32:26<23:11:12, 17.36s/it]

 55%|█████▌    | 5934/10740 [29:32:41<22:16:50, 16.69s/it]

 55%|█████▌    | 5935/10740 [29:33:01<23:31:34, 17.63s/it]


 55%|█████▌    | 5937/10740 [29:33:38<24:13:53, 18.16s/it]
{'loss': 0.2123, 'learning_rate': 8.780125863159149e-07, 'rewards/chosen': -5.169593811035156, 'rewards/rejected': -9.544178009033203, 'rewards/accuracies': 0.875, 'rewards/margins': 4.3745832443237305, 'policy_logps/rejected': -340.3813171386719, 'policy_logps/chosen': -384.47222900390625, 'referece_logps/rejected': -244.9395751953125, 'referece_logps/chosen': -332.77630615234375, 'logits/rejected': -0.30362439155578613, 'logits/chosen': -0.24606622755527496, 'epoch': 3.32}

 55%|█████▌    | 5938/10740 [29:33:51<21:53:46, 16.42s/it]

 55%|█████▌    | 5939/10740 [29:34:12<23:47:25, 17.84s/it]

 55%|█████▌    | 5940/10740 [29:34:26<22:10:30, 16.63s/it]

 55%|█████▌    | 5941/10740 [29:34:43<22:27:23, 16.85s/it]


 55%|█████▌    | 5943/10740 [29:35:23<24:27:15, 18.35s/it]

 55%|█████▌    | 5944/10740 [29:35:36<22:39:37, 17.01s/it]
{'loss': 0.2509, 'learning_rate': 8.759175433026743e-07, 'rewards/chosen': -3.086495876312256, 'rewards/rejected': -7.312828063964844, 'rewards/accuracies': 1.0, 'rewards/margins': 4.226332664489746, 'policy_logps/rejected': -333.4354248046875, 'policy_logps/chosen': -345.9468994140625, 'referece_logps/rejected': -260.3071594238281, 'referece_logps/chosen': -315.0819091796875, 'logits/rejected': -0.4963398277759552, 'logits/chosen': -0.5224649906158447, 'epoch': 3.32}

 55%|█████▌    | 5945/10740 [29:35:58<24:35:19, 18.46s/it]

 55%|█████▌    | 5946/10740 [29:36:19<25:37:22, 19.24s/it]

 55%|█████▌    | 5947/10740 [29:36:39<25:54:24, 19.46s/it]

 55%|█████▌    | 5948/10740 [29:36:59<26:00:41, 19.54s/it]

 55%|█████▌    | 5949/10740 [29:37:20<26:23:11, 19.83s/it]

 55%|█████▌    | 5950/10740 [29:37:41<27:09:19, 20.41s/it]

 55%|█████▌    | 5951/10740 [29:37:57<25:20:21, 19.05s/it]

 55%|█████▌    | 5952/10740 [29:38:18<26:01:49, 19.57s/it]

 55%|█████▌    | 5953/10740 [29:38:38<26:01:54, 19.58s/it]

 55%|█████▌    | 5954/10740 [29:38:55<24:57:05, 18.77s/it]

 55%|█████▌    | 5955/10740 [29:39:10<23:36:43, 17.76s/it]

 55%|█████▌    | 5956/10740 [29:39:30<24:21:02, 18.32s/it]

 55%|█████▌    | 5957/10740 [29:39:50<25:13:00, 18.98s/it]

 55%|█████▌    | 5958/10740 [29:40:09<25:05:14, 18.89s/it]

 55%|█████▌    | 5959/10740 [29:40:29<25:30:20, 19.21s/it]

 55%|█████▌    | 5960/10740 [29:40:48<25:43:55, 19.38s/it]

 56%|█████▌    | 5961/10740 [29:41:10<26:29:43, 19.96s/it]

 56%|█████▌    | 5962/10740 [29:41:29<26:13:44, 19.76s/it]

 56%|█████▌    | 5963/10740 [29:41:49<26:20:31, 19.85s/it]

 56%|█████▌    | 5964/10740 [29:42:03<24:08:19, 18.20s/it]

 56%|█████▌    | 5965/10740 [29:42:21<24:01:40, 18.12s/it]

 56%|█████▌    | 5966/10740 [29:42:40<24:18:43, 18.33s/it]

 56%|█████▌    | 5967/10740 [29:43:00<25:02:05, 18.88s/it]

 56%|█████▌    | 5968/10740 [29:43:23<26:23:13, 19.91s/it]


 56%|█████▌    | 5970/10740 [29:43:59<25:28:50, 19.23s/it]
{'loss': 0.152, 'learning_rate': 8.681408704222972e-07, 'rewards/chosen': -3.435086250305176, 'rewards/rejected': -6.391109943389893, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9560232162475586, 'policy_logps/rejected': -333.0748291015625, 'policy_logps/chosen': -366.6064453125, 'referece_logps/rejected': -269.1636962890625, 'referece_logps/chosen': -332.255615234375, 'logits/rejected': -0.6921234130859375, 'logits/chosen': -0.8982689380645752, 'epoch': 3.34}

 56%|█████▌    | 5971/10740 [29:44:15<24:10:00, 18.24s/it]

 56%|█████▌    | 5972/10740 [29:44:35<24:45:51, 18.70s/it]

 56%|█████▌    | 5973/10740 [29:44:51<23:54:09, 18.05s/it]

 56%|█████▌    | 5974/10740 [29:45:10<24:04:20, 18.18s/it]

 56%|█████▌    | 5975/10740 [29:45:32<25:38:08, 19.37s/it]

 56%|█████▌    | 5976/10740 [29:45:54<26:30:38, 20.03s/it]

 56%|█████▌    | 5977/10740 [29:46:10<25:04:50, 18.96s/it]

 56%|█████▌    | 5978/10740 [29:46:28<24:47:08, 18.74s/it]

 56%|█████▌    | 5979/10740 [29:46:45<23:49:02, 18.01s/it]

 56%|█████▌    | 5980/10740 [29:47:02<23:39:32, 17.89s/it]

 56%|█████▌    | 5981/10740 [29:47:22<24:19:55, 18.41s/it]

 56%|█████▌    | 5982/10740 [29:47:40<24:24:14, 18.46s/it]

 56%|█████▌    | 5983/10740 [29:48:01<25:03:55, 18.97s/it]

 56%|█████▌    | 5984/10740 [29:48:20<25:23:02, 19.21s/it]

 56%|█████▌    | 5985/10740 [29:48:40<25:34:56, 19.37s/it]

 56%|█████▌    | 5986/10740 [29:48:54<23:21:32, 17.69s/it]

 56%|█████▌    | 5987/10740 [29:49:14<24:13:49, 18.35s/it]

 56%|█████▌    | 5988/10740 [29:49:36<25:47:04, 19.53s/it]

 56%|█████▌    | 5989/10740 [29:49:51<23:49:45, 18.06s/it]

 56%|█████▌    | 5990/10740 [29:50:04<22:04:51, 16.74s/it]

 56%|█████▌    | 5991/10740 [29:50:22<22:32:13, 17.08s/it]

 56%|█████▌    | 5992/10740 [29:50:35<20:46:05, 15.75s/it]

 56%|█████▌    | 5993/10740 [29:50:55<22:19:42, 16.93s/it]

 56%|█████▌    | 5994/10740 [29:51:13<22:54:06, 17.37s/it]

 56%|█████▌    | 5995/10740 [29:51:31<23:12:00, 17.60s/it]

 56%|█████▌    | 5996/10740 [29:51:46<22:19:09, 16.94s/it]

 56%|█████▌    | 5997/10740 [29:52:02<21:46:09, 16.52s/it]

 56%|█████▌    | 5998/10740 [29:52:17<21:16:25, 16.15s/it]

 56%|█████▌    | 5999/10740 [29:52:34<21:24:07, 16.25s/it]

 56%|█████▌    | 6000/10740 [29:52:51<21:52:55, 16.62s/it]

 56%|█████▌    | 6001/10740 [29:53:17<25:31:09, 19.39s/it]

 56%|█████▌    | 6002/10740 [29:53:33<24:06:24, 18.32s/it]

 56%|█████▌    | 6003/10740 [29:53:48<22:50:05, 17.35s/it]

 56%|█████▌    | 6004/10740 [29:54:02<21:31:42, 16.36s/it]

 56%|█████▌    | 6005/10740 [29:54:16<20:24:29, 15.52s/it]

 56%|█████▌    | 6006/10740 [29:54:35<22:01:55, 16.75s/it]

 56%|█████▌    | 6007/10740 [29:54:55<23:10:11, 17.62s/it]

 56%|█████▌    | 6008/10740 [29:55:13<23:32:07, 17.91s/it]

 56%|█████▌    | 6009/10740 [29:55:33<24:14:03, 18.44s/it]

 56%|█████▌    | 6010/10740 [29:55:48<22:51:49, 17.40s/it]

 56%|█████▌    | 6011/10740 [29:56:07<23:23:35, 17.81s/it]

 56%|█████▌    | 6012/10740 [29:56:27<24:19:11, 18.52s/it]

 56%|█████▌    | 6013/10740 [29:56:46<24:38:05, 18.76s/it]

 56%|█████▌    | 6014/10740 [29:57:06<25:09:20, 19.16s/it]

 56%|█████▌    | 6015/10740 [29:57:25<24:43:26, 18.84s/it]

 56%|█████▌    | 6016/10740 [29:57:44<25:00:19, 19.06s/it]

 56%|█████▌    | 6017/10740 [29:58:05<25:53:19, 19.73s/it]


 56%|█████▌    | 6019/10740 [29:58:44<25:43:01, 19.61s/it]

 56%|█████▌    | 6020/10740 [29:59:05<26:13:22, 20.00s/it]

 56%|█████▌    | 6021/10740 [29:59:19<23:45:03, 18.12s/it]

 56%|█████▌    | 6022/10740 [29:59:40<24:54:10, 19.00s/it]

 56%|█████▌    | 6023/10740 [30:00:00<25:16:28, 19.29s/it]

 56%|█████▌    | 6024/10740 [30:00:20<25:43:03, 19.63s/it]

 56%|█████▌    | 6025/10740 [30:00:32<22:46:24, 17.39s/it]

 56%|█████▌    | 6026/10740 [30:00:50<23:00:49, 17.58s/it]

 56%|█████▌    | 6027/10740 [30:01:13<24:53:40, 19.02s/it]

 56%|█████▌    | 6028/10740 [30:01:33<25:34:29, 19.54s/it]

 56%|█████▌    | 6029/10740 [30:01:51<24:49:42, 18.97s/it]

 56%|█████▌    | 6030/10740 [30:02:13<25:57:41, 19.84s/it]

 56%|█████▌    | 6031/10740 [30:02:26<23:09:13, 17.70s/it]

 56%|█████▌    | 6032/10740 [30:02:45<23:44:53, 18.16s/it]

 56%|█████▌    | 6033/10740 [30:03:00<22:24:17, 17.14s/it]

 56%|█████▌    | 6034/10740 [30:03:17<22:40:31, 17.35s/it]

 56%|█████▌    | 6035/10740 [30:03:34<22:32:15, 17.24s/it]

 56%|█████▌    | 6036/10740 [30:03:53<23:11:35, 17.75s/it]

 56%|█████▌    | 6037/10740 [30:04:13<23:54:05, 18.30s/it]

 56%|█████▌    | 6038/10740 [30:04:34<24:48:19, 18.99s/it]
{'loss': 0.2984, 'learning_rate': 8.478414247009445e-07, 'rewards/chosen': -4.234269142150879, 'rewards/rejected': -6.762383937835693, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5281147956848145, 'policy_logps/rejected': -434.8551330566406, 'policy_logps/chosen': -493.08074951171875, 'referece_logps/rejected': -367.2313232421875, 'referece_logps/chosen': -450.738037109375, 'logits/rejected': -0.12278063595294952, 'logits/chosen': -0.10749907791614532, 'epoch': 3.37}


 56%|█████▌    | 6040/10740 [30:05:08<23:24:21, 17.93s/it]

 56%|█████▌    | 6041/10740 [30:05:20<21:12:31, 16.25s/it]

 56%|█████▋    | 6042/10740 [30:05:34<20:05:57, 15.40s/it]

 56%|█████▋    | 6043/10740 [30:05:56<22:45:23, 17.44s/it]
{'loss': 0.1428, 'learning_rate': 8.463512400704244e-07, 'rewards/chosen': -2.989473342895508, 'rewards/rejected': -6.146695137023926, 'rewards/accuracies': 0.875, 'rewards/margins': 3.157222032546997, 'policy_logps/rejected': -276.0408020019531, 'policy_logps/chosen': -341.408203125, 'referece_logps/rejected': -214.57386779785156, 'referece_logps/chosen': -311.5134582519531, 'logits/rejected': -1.0710054636001587, 'logits/chosen': -1.304851770401001, 'epoch': 3.38}


 56%|█████▋    | 6045/10740 [30:06:34<23:47:46, 18.25s/it]

 56%|█████▋    | 6046/10740 [30:06:51<23:23:05, 17.93s/it]

 56%|█████▋    | 6047/10740 [30:07:11<24:08:53, 18.52s/it]

 56%|█████▋    | 6048/10740 [30:07:31<24:46:47, 19.01s/it]

 56%|█████▋    | 6049/10740 [30:07:48<23:51:48, 18.31s/it]

 56%|█████▋    | 6050/10740 [30:08:07<24:00:37, 18.43s/it]

 56%|█████▋    | 6051/10740 [30:08:25<24:09:58, 18.55s/it]

 56%|█████▋    | 6052/10740 [30:08:41<22:48:18, 17.51s/it]

 56%|█████▋    | 6053/10740 [30:08:58<22:47:35, 17.51s/it]

 56%|█████▋    | 6054/10740 [30:09:18<23:33:31, 18.10s/it]
{'loss': 0.3141, 'learning_rate': 8.430740684291202e-07, 'rewards/chosen': -3.5142767429351807, 'rewards/rejected': -7.290645122528076, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7763683795928955, 'policy_logps/rejected': -423.5335998535156, 'policy_logps/chosen': -476.0236511230469, 'referece_logps/rejected': -350.6272277832031, 'referece_logps/chosen': -440.8808898925781, 'logits/rejected': 0.2640092968940735, 'logits/chosen': 0.3278259038925171, 'epoch': 3.38}


 56%|█████▋    | 6056/10740 [30:09:52<22:53:52, 17.60s/it]

 56%|█████▋    | 6057/10740 [30:10:11<23:08:23, 17.79s/it]

 56%|█████▋    | 6058/10740 [30:10:28<23:01:34, 17.70s/it]
{'loss': 0.269, 'learning_rate': 8.418827959378241e-07, 'rewards/chosen': -4.234477519989014, 'rewards/rejected': -9.946869850158691, 'rewards/accuracies': 0.875, 'rewards/margins': 5.712392807006836, 'policy_logps/rejected': -603.255126953125, 'policy_logps/chosen': -620.3370971679688, 'referece_logps/rejected': -503.7864990234375, 'referece_logps/chosen': -577.9923095703125, 'logits/rejected': 0.7519558668136597, 'logits/chosen': 0.5987558364868164, 'epoch': 3.38}


 56%|█████▋    | 6060/10740 [30:11:05<24:02:55, 18.50s/it]

 56%|█████▋    | 6061/10740 [30:11:27<25:12:20, 19.39s/it]
{'loss': 0.1538, 'learning_rate': 8.409894924765906e-07, 'rewards/chosen': -4.665691375732422, 'rewards/rejected': -8.145771026611328, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4800796508789062, 'policy_logps/rejected': -581.012451171875, 'policy_logps/chosen': -545.19189453125, 'referece_logps/rejected': -499.5547180175781, 'referece_logps/chosen': -498.5350036621094, 'logits/rejected': 0.4265606999397278, 'logits/chosen': 0.576835036277771, 'epoch': 3.39}


 56%|█████▋    | 6063/10740 [30:12:04<24:18:57, 18.72s/it]

 56%|█████▋    | 6064/10740 [30:12:25<25:15:32, 19.45s/it]

 56%|█████▋    | 6065/10740 [30:12:42<24:22:04, 18.76s/it]

 56%|█████▋    | 6066/10740 [30:12:54<21:41:45, 16.71s/it]

 56%|█████▋    | 6067/10740 [30:13:15<23:24:16, 18.03s/it]

 56%|█████▋    | 6068/10740 [30:13:34<23:43:10, 18.28s/it]

 57%|█████▋    | 6069/10740 [30:13:57<25:26:06, 19.60s/it]
{'loss': 0.2606, 'learning_rate': 8.386079882443969e-07, 'rewards/chosen': -3.7259671688079834, 'rewards/rejected': -8.627435684204102, 'rewards/accuracies': 1.0, 'rewards/margins': 4.901468276977539, 'policy_logps/rejected': -383.83978271484375, 'policy_logps/chosen': -422.1783447265625, 'referece_logps/rejected': -297.5653991699219, 'referece_logps/chosen': -384.918701171875, 'logits/rejected': 0.6393235325813293, 'logits/chosen': 0.6314422488212585, 'epoch': 3.39}


 57%|█████▋    | 6071/10740 [30:14:37<25:45:05, 19.86s/it]

 57%|█████▋    | 6072/10740 [30:14:53<24:14:06, 18.69s/it]
{'loss': 0.2899, 'learning_rate': 8.377151656162958e-07, 'rewards/chosen': -3.335322141647339, 'rewards/rejected': -6.194480895996094, 'rewards/accuracies': 0.75, 'rewards/margins': 2.859158515930176, 'policy_logps/rejected': -250.9309539794922, 'policy_logps/chosen': -269.2573547363281, 'referece_logps/rejected': -188.98614501953125, 'referece_logps/chosen': -235.90414428710938, 'logits/rejected': -0.7532286047935486, 'logits/chosen': -0.6151156425476074, 'epoch': 3.39}


 57%|█████▋    | 6074/10740 [30:15:27<23:38:45, 18.24s/it]

 57%|█████▋    | 6075/10740 [30:15:42<22:28:16, 17.34s/it]
{'loss': 0.2132, 'learning_rate': 8.368224758300195e-07, 'rewards/chosen': -4.178656578063965, 'rewards/rejected': -6.334280967712402, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1556246280670166, 'policy_logps/rejected': -664.7021484375, 'policy_logps/chosen': -564.6444091796875, 'referece_logps/rejected': -601.3594360351562, 'referece_logps/chosen': -522.85791015625, 'logits/rejected': -0.07488182187080383, 'logits/chosen': 0.021540284156799316, 'epoch': 3.39}


 57%|█████▋    | 6077/10740 [30:16:17<22:40:34, 17.51s/it]

 57%|█████▋    | 6078/10740 [30:16:39<24:30:29, 18.93s/it]

 57%|█████▋    | 6079/10740 [30:16:52<22:11:34, 17.14s/it]
{'loss': 0.2666, 'learning_rate': 8.356324306874658e-07, 'rewards/chosen': -4.67938756942749, 'rewards/rejected': -7.719217300415039, 'rewards/accuracies': 1.0, 'rewards/margins': 3.039829730987549, 'policy_logps/rejected': -469.2608337402344, 'policy_logps/chosen': -366.84912109375, 'referece_logps/rejected': -392.0686950683594, 'referece_logps/chosen': -320.05523681640625, 'logits/rejected': 0.21031920611858368, 'logits/chosen': 0.2800378203392029, 'epoch': 3.4}


 57%|█████▋    | 6081/10740 [30:17:30<23:18:23, 18.01s/it]

 57%|█████▋    | 6082/10740 [30:17:48<23:16:25, 17.99s/it]

 57%|█████▋    | 6083/10740 [30:18:06<23:20:01, 18.04s/it]

 57%|█████▋    | 6084/10740 [30:18:25<23:37:54, 18.27s/it]

 57%|█████▋    | 6085/10740 [30:18:39<21:58:25, 16.99s/it]
{'loss': 0.2684, 'learning_rate': 8.338478120036824e-07, 'rewards/chosen': -3.6966981887817383, 'rewards/rejected': -6.543099403381348, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8464012145996094, 'policy_logps/rejected': -361.3832702636719, 'policy_logps/chosen': -347.7508239746094, 'referece_logps/rejected': -295.9522705078125, 'referece_logps/chosen': -310.78387451171875, 'logits/rejected': -0.14666703343391418, 'logits/chosen': -0.09876644611358643, 'epoch': 3.4}


 57%|█████▋    | 6087/10740 [30:19:10<20:33:58, 15.91s/it]

 57%|█████▋    | 6088/10740 [30:19:27<20:52:59, 16.16s/it]

 57%|█████▋    | 6089/10740 [30:19:47<22:28:48, 17.40s/it]

 57%|█████▋    | 6090/10740 [30:20:04<22:15:45, 17.24s/it]

 57%|█████▋    | 6091/10740 [30:20:17<20:39:06, 15.99s/it]

 57%|█████▋    | 6092/10740 [30:20:34<21:03:26, 16.31s/it]

 57%|█████▋    | 6093/10740 [30:20:55<22:35:06, 17.50s/it]
{'loss': 0.1269, 'learning_rate': 8.314691677039298e-07, 'rewards/chosen': -3.082010269165039, 'rewards/rejected': -5.773520469665527, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6915106773376465, 'policy_logps/rejected': -350.572998046875, 'policy_logps/chosen': -317.349609375, 'referece_logps/rejected': -292.8377990722656, 'referece_logps/chosen': -286.5295104980469, 'logits/rejected': 0.06286494433879852, 'logits/chosen': 0.030962156131863594, 'epoch': 3.4}


 57%|█████▋    | 6095/10740 [30:21:34<23:57:36, 18.57s/it]

 57%|█████▋    | 6096/10740 [30:21:52<23:42:49, 18.38s/it]

 57%|█████▋    | 6097/10740 [30:22:09<23:11:54, 17.99s/it]

 57%|█████▋    | 6098/10740 [30:22:27<23:12:51, 18.00s/it]

 57%|█████▋    | 6099/10740 [30:22:47<23:59:42, 18.61s/it]

 57%|█████▋    | 6100/10740 [30:23:00<21:33:12, 16.72s/it]

 57%|█████▋    | 6101/10740 [30:23:15<20:53:47, 16.22s/it]

 57%|█████▋    | 6102/10740 [30:23:33<21:42:30, 16.85s/it]

 57%|█████▋    | 6103/10740 [30:23:53<22:49:15, 17.72s/it]

 57%|█████▋    | 6104/10740 [30:24:06<20:54:08, 16.23s/it]

 57%|█████▋    | 6105/10740 [30:24:25<22:11:14, 17.23s/it]

 57%|█████▋    | 6106/10740 [30:24:45<23:18:34, 18.11s/it]

 57%|█████▋    | 6107/10740 [30:25:06<24:31:23, 19.06s/it]
{'loss': 0.2291, 'learning_rate': 8.273089090612844e-07, 'rewards/chosen': -4.43262243270874, 'rewards/rejected': -7.770129680633545, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3375070095062256, 'policy_logps/rejected': -453.107666015625, 'policy_logps/chosen': -392.3704833984375, 'referece_logps/rejected': -375.4063415527344, 'referece_logps/chosen': -348.0442199707031, 'logits/rejected': -0.25228843092918396, 'logits/chosen': -0.4199868142604828, 'epoch': 3.41}


 57%|█████▋    | 6109/10740 [30:25:45<24:43:51, 19.23s/it]

 57%|█████▋    | 6110/10740 [30:26:03<24:16:17, 18.87s/it]

 57%|█████▋    | 6111/10740 [30:26:23<24:32:59, 19.09s/it]
{'loss': 0.2094, 'learning_rate': 8.261208259324536e-07, 'rewards/chosen': -3.8931374549865723, 'rewards/rejected': -8.709633827209473, 'rewards/accuracies': 0.875, 'rewards/margins': 4.816496849060059, 'policy_logps/rejected': -303.010986328125, 'policy_logps/chosen': -233.8972930908203, 'referece_logps/rejected': -215.91461181640625, 'referece_logps/chosen': -194.9658966064453, 'logits/rejected': -0.5197720527648926, 'logits/chosen': -0.34387075901031494, 'epoch': 3.41}

 57%|█████▋    | 6112/10740 [30:26:44<25:14:57, 19.64s/it]

 57%|█████▋    | 6113/10740 [30:27:04<25:24:37, 19.77s/it]


 57%|█████▋    | 6115/10740 [30:27:43<24:56:31, 19.41s/it]

 57%|█████▋    | 6116/10740 [30:28:02<24:48:42, 19.32s/it]

 57%|█████▋    | 6117/10740 [30:28:15<22:29:13, 17.51s/it]

 57%|█████▋    | 6118/10740 [30:28:34<23:03:26, 17.96s/it]

 57%|█████▋    | 6119/10740 [30:28:51<22:42:10, 17.69s/it]

 57%|█████▋    | 6120/10740 [30:29:11<23:27:19, 18.28s/it]

 57%|█████▋    | 6121/10740 [30:29:33<24:58:52, 19.47s/it]
{'loss': 0.2456, 'learning_rate': 8.231517289230227e-07, 'rewards/chosen': -4.33560848236084, 'rewards/rejected': -6.765942573547363, 'rewards/accuracies': 0.75, 'rewards/margins': 2.430333137512207, 'policy_logps/rejected': -451.7734680175781, 'policy_logps/chosen': -347.0911560058594, 'referece_logps/rejected': -384.1140441894531, 'referece_logps/chosen': -303.7350769042969, 'logits/rejected': -0.13958728313446045, 'logits/chosen': 0.09478339552879333, 'epoch': 3.42}


 57%|█████▋    | 6123/10740 [30:29:59<20:28:37, 15.97s/it]

 57%|█████▋    | 6124/10740 [30:30:15<20:35:10, 16.06s/it]

 57%|█████▋    | 6125/10740 [30:30:28<19:24:05, 15.13s/it]

 57%|█████▋    | 6126/10740 [30:30:41<18:39:46, 14.56s/it]

 57%|█████▋    | 6127/10740 [30:31:01<20:34:54, 16.06s/it]
{'loss': 0.1317, 'learning_rate': 8.213710410587233e-07, 'rewards/chosen': -2.475421905517578, 'rewards/rejected': -9.262534141540527, 'rewards/accuracies': 1.0, 'rewards/margins': 6.787112712860107, 'policy_logps/rejected': -377.01348876953125, 'policy_logps/chosen': -344.8140869140625, 'referece_logps/rejected': -284.3881530761719, 'referece_logps/chosen': -320.0598449707031, 'logits/rejected': -0.2869614362716675, 'logits/chosen': -0.38603246212005615, 'epoch': 3.42}


 57%|█████▋    | 6129/10740 [30:31:32<20:08:24, 15.72s/it]

 57%|█████▋    | 6130/10740 [30:31:51<21:35:36, 16.86s/it]

 57%|█████▋    | 6131/10740 [30:32:12<22:54:15, 17.89s/it]

 57%|█████▋    | 6132/10740 [30:32:29<22:37:17, 17.67s/it]

 57%|█████▋    | 6133/10740 [30:32:45<21:59:51, 17.19s/it]

 57%|█████▋    | 6134/10740 [30:32:58<20:32:26, 16.05s/it]

 57%|█████▋    | 6135/10740 [30:33:18<21:54:36, 17.13s/it]

 57%|█████▋    | 6136/10740 [30:33:39<23:24:01, 18.30s/it]

 57%|█████▋    | 6137/10740 [30:34:00<24:18:20, 19.01s/it]

 57%|█████▋    | 6138/10740 [30:34:20<24:49:04, 19.41s/it]

 57%|█████▋    | 6139/10740 [30:34:40<24:54:39, 19.49s/it]

 57%|█████▋    | 6140/10740 [30:34:58<24:21:15, 19.06s/it]
{'loss': 0.159, 'learning_rate': 8.17514898267314e-07, 'rewards/chosen': -4.027007579803467, 'rewards/rejected': -8.77457046508789, 'rewards/accuracies': 1.0, 'rewards/margins': 4.747562408447266, 'policy_logps/rejected': -484.9090576171875, 'policy_logps/chosen': -381.728759765625, 'referece_logps/rejected': -397.163330078125, 'referece_logps/chosen': -341.45867919921875, 'logits/rejected': -0.5726103782653809, 'logits/chosen': -0.40259575843811035, 'epoch': 3.43}


 57%|█████▋    | 6142/10740 [30:35:40<25:49:32, 20.22s/it]

 57%|█████▋    | 6143/10740 [30:36:00<25:37:05, 20.06s/it]
{'loss': 0.1591, 'learning_rate': 8.16625415342878e-07, 'rewards/chosen': -3.6079790592193604, 'rewards/rejected': -7.749953269958496, 'rewards/accuracies': 0.875, 'rewards/margins': 4.141973495483398, 'policy_logps/rejected': -414.68975830078125, 'policy_logps/chosen': -393.319091796875, 'referece_logps/rejected': -337.19024658203125, 'referece_logps/chosen': -357.2393493652344, 'logits/rejected': -0.3400273323059082, 'logits/chosen': -0.21104279160499573, 'epoch': 3.43}


 57%|█████▋    | 6145/10740 [30:36:42<26:22:54, 20.67s/it]
{'loss': 0.1847, 'learning_rate': 8.160325100734834e-07, 'rewards/chosen': -4.073690414428711, 'rewards/rejected': -8.240580558776855, 'rewards/accuracies': 0.875, 'rewards/margins': 4.1668901443481445, 'policy_logps/rejected': -471.7381896972656, 'policy_logps/chosen': -363.3862609863281, 'referece_logps/rejected': -389.3323669433594, 'referece_logps/chosen': -322.64935302734375, 'logits/rejected': -0.31941014528274536, 'logits/chosen': -0.189128577709198, 'epoch': 3.43}

 57%|█████▋    | 6146/10740 [30:36:59<25:15:31, 19.79s/it]

 57%|█████▋    | 6147/10740 [30:37:19<25:10:24, 19.73s/it]

 57%|█████▋    | 6148/10740 [30:37:35<23:58:05, 18.79s/it]


 57%|█████▋    | 6150/10740 [30:38:15<24:14:06, 19.01s/it]
{'loss': 0.2018, 'learning_rate': 8.145505401869359e-07, 'rewards/chosen': -3.0071144104003906, 'rewards/rejected': -5.232847690582275, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2257328033447266, 'policy_logps/rejected': -309.0247802734375, 'policy_logps/chosen': -364.4308166503906, 'referece_logps/rejected': -256.6963195800781, 'referece_logps/chosen': -334.35968017578125, 'logits/rejected': -0.5279657244682312, 'logits/chosen': -0.6299217939376831, 'epoch': 3.44}

 57%|█████▋    | 6151/10740 [30:38:31<23:14:48, 18.24s/it]


 57%|█████▋    | 6153/10740 [30:39:12<24:39:54, 19.36s/it]

 57%|█████▋    | 6154/10740 [30:39:30<24:17:45, 19.07s/it]

 57%|█████▋    | 6155/10740 [30:39:42<21:33:10, 16.92s/it]

 57%|█████▋    | 6156/10740 [30:40:00<21:56:42, 17.23s/it]

 57%|█████▋    | 6157/10740 [30:40:18<22:15:04, 17.48s/it]

 57%|█████▋    | 6158/10740 [30:40:30<20:00:02, 15.71s/it]
{'loss': 0.3041, 'learning_rate': 8.121802668580695e-07, 'rewards/chosen': -3.088378667831421, 'rewards/rejected': -6.0949811935424805, 'rewards/accuracies': 0.75, 'rewards/margins': 3.006603240966797, 'policy_logps/rejected': -372.9774169921875, 'policy_logps/chosen': -338.28448486328125, 'referece_logps/rejected': -312.027587890625, 'referece_logps/chosen': -307.4007263183594, 'logits/rejected': -0.21396921575069427, 'logits/chosen': -0.145328551530838, 'epoch': 3.44}

 57%|█████▋    | 6159/10740 [30:40:51<22:01:38, 17.31s/it]

 57%|█████▋    | 6160/10740 [30:41:07<21:42:54, 17.07s/it]


 57%|█████▋    | 6162/10740 [30:41:40<21:05:30, 16.59s/it]

 57%|█████▋    | 6163/10740 [30:41:56<20:51:01, 16.40s/it]
{'loss': 0.1636, 'learning_rate': 8.106994003383487e-07, 'rewards/chosen': -3.6909103393554688, 'rewards/rejected': -5.992326736450195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3014166355133057, 'policy_logps/rejected': -282.91510009765625, 'policy_logps/chosen': -353.44610595703125, 'referece_logps/rejected': -222.9918212890625, 'referece_logps/chosen': -316.5369873046875, 'logits/rejected': -0.6484708189964294, 'logits/chosen': -0.45849186182022095, 'epoch': 3.44}

 57%|█████▋    | 6164/10740 [30:42:13<21:15:01, 16.72s/it]


 57%|█████▋    | 6166/10740 [30:42:53<23:01:45, 18.13s/it]
{'loss': 0.2144, 'learning_rate': 8.098110868192632e-07, 'rewards/chosen': -3.90917706489563, 'rewards/rejected': -8.606880187988281, 'rewards/accuracies': 1.0, 'rewards/margins': 4.697703838348389, 'policy_logps/rejected': -433.79559326171875, 'policy_logps/chosen': -406.1562805175781, 'referece_logps/rejected': -347.72674560546875, 'referece_logps/chosen': -367.0644836425781, 'logits/rejected': -0.44912534952163696, 'logits/chosen': -0.38893449306488037, 'epoch': 3.44}


 57%|█████▋    | 6168/10740 [30:43:32<23:54:18, 18.82s/it]

 57%|█████▋    | 6169/10740 [30:43:50<23:33:31, 18.55s/it]
{'loss': 0.1742, 'learning_rate': 8.089229289835009e-07, 'rewards/chosen': -4.3031487464904785, 'rewards/rejected': -7.825730800628662, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5225822925567627, 'policy_logps/rejected': -269.1293029785156, 'policy_logps/chosen': -393.5675964355469, 'referece_logps/rejected': -190.8719940185547, 'referece_logps/chosen': -350.5361022949219, 'logits/rejected': -0.4141671657562256, 'logits/chosen': -0.46556031703948975, 'epoch': 3.45}

 57%|█████▋    | 6170/10740 [30:44:04<21:42:50, 17.11s/it]

 57%|█████▋    | 6171/10740 [30:44:17<20:26:59, 16.11s/it]

 57%|█████▋    | 6172/10740 [30:44:37<21:50:01, 17.21s/it]


 57%|█████▋    | 6174/10740 [30:45:16<23:22:38, 18.43s/it]
{'loss': 0.198, 'learning_rate': 8.07443013861405e-07, 'rewards/chosen': -4.037843227386475, 'rewards/rejected': -7.279311180114746, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2414684295654297, 'policy_logps/rejected': -508.5443420410156, 'policy_logps/chosen': -413.4183654785156, 'referece_logps/rejected': -435.751220703125, 'referece_logps/chosen': -373.0398864746094, 'logits/rejected': -0.3312894105911255, 'logits/chosen': -0.3109625577926636, 'epoch': 3.45}

 57%|█████▋    | 6175/10740 [30:45:34<23:07:20, 18.23s/it]

 58%|█████▊    | 6176/10740 [30:45:50<22:13:31, 17.53s/it]


 58%|█████▊    | 6178/10740 [30:46:28<23:14:26, 18.34s/it]

 58%|█████▊    | 6179/10740 [30:46:49<23:59:18, 18.93s/it]
{'loss': 0.2695, 'learning_rate': 8.059635365774826e-07, 'rewards/chosen': -3.709522247314453, 'rewards/rejected': -8.159221649169922, 'rewards/accuracies': 0.875, 'rewards/margins': 4.449699878692627, 'policy_logps/rejected': -427.25482177734375, 'policy_logps/chosen': -355.56072998046875, 'referece_logps/rejected': -345.66259765625, 'referece_logps/chosen': -318.46551513671875, 'logits/rejected': -0.32571932673454285, 'logits/chosen': -0.2322709560394287, 'epoch': 3.45}

 58%|█████▊    | 6180/10740 [30:47:03<22:25:52, 17.71s/it]


 58%|█████▊    | 6182/10740 [30:47:35<21:01:25, 16.60s/it]

 58%|█████▊    | 6183/10740 [30:47:53<21:31:42, 17.01s/it]

 58%|█████▊    | 6184/10740 [30:48:13<22:37:20, 17.88s/it]
{'loss': 0.1999, 'learning_rate': 8.044845004957852e-07, 'rewards/chosen': -4.050914764404297, 'rewards/rejected': -7.471299171447754, 'rewards/accuracies': 0.75, 'rewards/margins': 3.420384645462036, 'policy_logps/rejected': -412.8971862792969, 'policy_logps/chosen': -337.1683349609375, 'referece_logps/rejected': -338.1841735839844, 'referece_logps/chosen': -296.6591796875, 'logits/rejected': 0.3154069185256958, 'logits/chosen': 0.30163243412971497, 'epoch': 3.45}


 58%|█████▊    | 6186/10740 [30:48:45<21:19:26, 16.86s/it]
{'loss': 0.3439, 'learning_rate': 8.03893010353086e-07, 'rewards/chosen': -4.38621187210083, 'rewards/rejected': -6.485848903656006, 'rewards/accuracies': 0.875, 'rewards/margins': 2.099637031555176, 'policy_logps/rejected': -307.1800842285156, 'policy_logps/chosen': -405.95379638671875, 'referece_logps/rejected': -242.3215789794922, 'referece_logps/chosen': -362.0917053222656, 'logits/rejected': -0.5383819937705994, 'logits/chosen': -0.6136371493339539, 'epoch': 3.46}

 58%|█████▊    | 6187/10740 [30:49:04<22:00:16, 17.40s/it]

 58%|█████▊    | 6188/10740 [30:49:21<22:06:44, 17.49s/it]

 58%|█████▊    | 6189/10740 [30:49:41<23:02:45, 18.23s/it]


 58%|█████▊    | 6191/10740 [30:50:21<23:58:15, 18.97s/it]
{'loss': 0.2512, 'learning_rate': 8.024145976041983e-07, 'rewards/chosen': -3.3641748428344727, 'rewards/rejected': -5.758137226104736, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3939623832702637, 'policy_logps/rejected': -389.816162109375, 'policy_logps/chosen': -508.7987060546875, 'referece_logps/rejected': -332.2347717285156, 'referece_logps/chosen': -475.1568908691406, 'logits/rejected': -0.6071216464042664, 'logits/chosen': -0.8216779232025146, 'epoch': 3.46}

 58%|█████▊    | 6192/10740 [30:50:40<23:59:04, 18.99s/it]


 58%|█████▊    | 6194/10740 [30:51:13<21:55:36, 17.36s/it]
{'loss': 0.226, 'learning_rate': 8.015277653902475e-07, 'rewards/chosen': -3.807281970977783, 'rewards/rejected': -7.026344299316406, 'rewards/accuracies': 1.0, 'rewards/margins': 3.219062328338623, 'policy_logps/rejected': -316.79180908203125, 'policy_logps/chosen': -293.2802734375, 'referece_logps/rejected': -246.5283966064453, 'referece_logps/chosen': -255.2074432373047, 'logits/rejected': -0.4989420175552368, 'logits/chosen': -0.4919878840446472, 'epoch': 3.46}

 58%|█████▊    | 6195/10740 [30:51:32<22:47:19, 18.05s/it]


 58%|█████▊    | 6197/10740 [30:52:09<22:43:42, 18.01s/it]
{'loss': 0.1543, 'learning_rate': 8.006410956401152e-07, 'rewards/chosen': -3.5280730724334717, 'rewards/rejected': -10.267008781433105, 'rewards/accuracies': 1.0, 'rewards/margins': 6.738936424255371, 'policy_logps/rejected': -555.5384521484375, 'policy_logps/chosen': -566.3648681640625, 'referece_logps/rejected': -452.868408203125, 'referece_logps/chosen': -531.0841674804688, 'logits/rejected': -0.919933557510376, 'logits/chosen': -1.0216240882873535, 'epoch': 3.46}


 58%|█████▊    | 6199/10740 [30:52:41<21:11:08, 16.80s/it]

 58%|█████▊    | 6200/10740 [30:53:01<22:15:27, 17.65s/it]
{'loss': 0.1406, 'learning_rate': 7.997545890796043e-07, 'rewards/chosen': -3.902815341949463, 'rewards/rejected': -9.032282829284668, 'rewards/accuracies': 1.0, 'rewards/margins': 5.1294660568237305, 'policy_logps/rejected': -484.1608581542969, 'policy_logps/chosen': -352.8031311035156, 'referece_logps/rejected': -393.8380432128906, 'referece_logps/chosen': -313.77496337890625, 'logits/rejected': -0.20380519330501556, 'logits/chosen': 0.06862671673297882, 'epoch': 3.46}


 58%|█████▊    | 6202/10740 [30:53:31<20:06:29, 15.95s/it]
{'loss': 0.2133, 'learning_rate': 7.991636757251919e-07, 'rewards/chosen': -4.173713207244873, 'rewards/rejected': -8.132551193237305, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9588382244110107, 'policy_logps/rejected': -451.82623291015625, 'policy_logps/chosen': -419.6636962890625, 'referece_logps/rejected': -370.5007019042969, 'referece_logps/chosen': -377.926513671875, 'logits/rejected': 0.30293646454811096, 'logits/chosen': 0.14900512993335724, 'epoch': 3.46}

 58%|█████▊    | 6203/10740 [30:53:46<19:44:43, 15.67s/it]

 58%|█████▊    | 6204/10740 [30:54:06<21:32:04, 17.09s/it]

 58%|█████▊    | 6205/10740 [30:54:24<21:50:45, 17.34s/it]


 58%|█████▊    | 6207/10740 [30:54:55<21:10:07, 16.81s/it]
{'loss': 0.2431, 'learning_rate': 7.976867124740914e-07, 'rewards/chosen': -4.382328987121582, 'rewards/rejected': -9.963821411132812, 'rewards/accuracies': 0.875, 'rewards/margins': 5.581491470336914, 'policy_logps/rejected': -492.4134216308594, 'policy_logps/chosen': -321.580322265625, 'referece_logps/rejected': -392.7752380371094, 'referece_logps/chosen': -277.75701904296875, 'logits/rejected': -1.0123924016952515, 'logits/chosen': -0.5951675176620483, 'epoch': 3.47}


 58%|█████▊    | 6209/10740 [30:55:35<23:01:46, 18.30s/it]

 58%|█████▊    | 6210/10740 [30:55:49<21:27:18, 17.05s/it]

 58%|█████▊    | 6211/10740 [30:56:01<19:31:36, 15.52s/it]
{'loss': 0.2254, 'learning_rate': 7.965054729279995e-07, 'rewards/chosen': -3.753349542617798, 'rewards/rejected': -6.448889255523682, 'rewards/accuracies': 0.875, 'rewards/margins': 2.695539712905884, 'policy_logps/rejected': -424.00335693359375, 'policy_logps/chosen': -383.6410217285156, 'referece_logps/rejected': -359.51446533203125, 'referece_logps/chosen': -346.10748291015625, 'logits/rejected': 0.37130406498908997, 'logits/chosen': 0.4277898371219635, 'epoch': 3.47}

 58%|█████▊    | 6212/10740 [30:56:16<19:19:14, 15.36s/it]


 58%|█████▊    | 6214/10740 [30:56:43<17:39:48, 14.05s/it]
{'loss': 0.1457, 'learning_rate': 7.956197375118439e-07, 'rewards/chosen': -4.036442279815674, 'rewards/rejected': -9.025218963623047, 'rewards/accuracies': 1.0, 'rewards/margins': 4.988777160644531, 'policy_logps/rejected': -354.40032958984375, 'policy_logps/chosen': -308.5225830078125, 'referece_logps/rejected': -264.148193359375, 'referece_logps/chosen': -268.15814208984375, 'logits/rejected': -1.0388996601104736, 'logits/chosen': -1.0089335441589355, 'epoch': 3.47}

 58%|█████▊    | 6215/10740 [30:56:58<18:14:31, 14.51s/it]


 58%|█████▊    | 6217/10740 [30:57:40<22:14:57, 17.71s/it]
{'loss': 0.3055, 'learning_rate': 7.947341693956524e-07, 'rewards/chosen': -4.241456985473633, 'rewards/rejected': -8.910902976989746, 'rewards/accuracies': 1.0, 'rewards/margins': 4.669445037841797, 'policy_logps/rejected': -457.0767822265625, 'policy_logps/chosen': -345.567138671875, 'referece_logps/rejected': -367.96783447265625, 'referece_logps/chosen': -303.1525573730469, 'logits/rejected': 0.1386863887310028, 'logits/chosen': 0.17076638340950012, 'epoch': 3.47}

 58%|█████▊    | 6218/10740 [30:57:52<20:09:58, 16.05s/it]

 58%|█████▊    | 6219/10740 [30:58:08<20:13:28, 16.10s/it]

 58%|█████▊    | 6220/10740 [30:58:31<22:33:39, 17.97s/it]


 58%|█████▊    | 6222/10740 [30:59:11<24:05:02, 19.19s/it]
{'loss': 0.1473, 'learning_rate': 7.932585962818391e-07, 'rewards/chosen': -3.3303701877593994, 'rewards/rejected': -7.411966323852539, 'rewards/accuracies': 1.0, 'rewards/margins': 4.081595420837402, 'policy_logps/rejected': -360.4549255371094, 'policy_logps/chosen': -326.8596496582031, 'referece_logps/rejected': -286.3352355957031, 'referece_logps/chosen': -293.55596923828125, 'logits/rejected': -0.36054909229278564, 'logits/chosen': -0.6202142238616943, 'epoch': 3.48}

 58%|█████▊    | 6223/10740 [30:59:31<24:16:33, 19.35s/it]

 58%|█████▊    | 6224/10740 [30:59:51<24:31:33, 19.55s/it]


 58%|█████▊    | 6226/10740 [31:00:26<22:54:11, 18.27s/it]

 58%|█████▊    | 6227/10740 [31:00:44<22:48:36, 18.20s/it]

 58%|█████▊    | 6228/10740 [31:01:03<23:24:27, 18.68s/it]

 58%|█████▊    | 6229/10740 [31:01:16<20:54:59, 16.69s/it]

 58%|█████▊    | 6230/10740 [31:01:36<22:13:06, 17.74s/it]

 58%|█████▊    | 6231/10740 [31:01:56<23:06:47, 18.45s/it]

 58%|█████▊    | 6232/10740 [31:02:11<21:57:26, 17.53s/it]
{'loss': 0.294, 'learning_rate': 7.903088636808873e-07, 'rewards/chosen': -3.4280054569244385, 'rewards/rejected': -7.398257255554199, 'rewards/accuracies': 0.875, 'rewards/margins': 3.970252275466919, 'policy_logps/rejected': -421.9140319824219, 'policy_logps/chosen': -369.2851257324219, 'referece_logps/rejected': -347.93145751953125, 'referece_logps/chosen': -335.00506591796875, 'logits/rejected': -0.181052565574646, 'logits/chosen': -0.18140238523483276, 'epoch': 3.48}

 58%|█████▊    | 6233/10740 [31:02:24<20:17:36, 16.21s/it]

 58%|█████▊    | 6234/10740 [31:02:45<21:46:38, 17.40s/it]


 58%|█████▊    | 6236/10740 [31:03:20<21:36:43, 17.27s/it]
{'loss': 0.1361, 'learning_rate': 7.891295031521356e-07, 'rewards/chosen': -3.431489944458008, 'rewards/rejected': -9.151521682739258, 'rewards/accuracies': 1.0, 'rewards/margins': 5.720032215118408, 'policy_logps/rejected': -291.9974365234375, 'policy_logps/chosen': -453.279541015625, 'referece_logps/rejected': -200.4822235107422, 'referece_logps/chosen': -418.9646911621094, 'logits/rejected': 0.15325818955898285, 'logits/chosen': -0.06268484890460968, 'epoch': 3.48}

 58%|█████▊    | 6237/10740 [31:03:41<22:59:01, 18.37s/it]


 58%|█████▊    | 6239/10740 [31:04:14<21:46:59, 17.42s/it]

 58%|█████▊    | 6240/10740 [31:04:33<22:36:59, 18.09s/it]
{'loss': 0.1835, 'learning_rate': 7.879504494903702e-07, 'rewards/chosen': -3.129152774810791, 'rewards/rejected': -6.116512298583984, 'rewards/accuracies': 0.75, 'rewards/margins': 2.9873597621917725, 'policy_logps/rejected': -312.33868408203125, 'policy_logps/chosen': -283.05145263671875, 'referece_logps/rejected': -251.1735382080078, 'referece_logps/chosen': -251.7599334716797, 'logits/rejected': -0.9642159342765808, 'logits/chosen': -0.9470155835151672, 'epoch': 3.49}


 58%|█████▊    | 6242/10740 [31:05:14<24:08:34, 19.32s/it]

 58%|█████▊    | 6243/10740 [31:05:28<22:08:49, 17.73s/it]
{'loss': 0.3116, 'learning_rate': 7.870663616576869e-07, 'rewards/chosen': -3.208484172821045, 'rewards/rejected': -8.305112838745117, 'rewards/accuracies': 1.0, 'rewards/margins': 5.096628665924072, 'policy_logps/rejected': -403.66766357421875, 'policy_logps/chosen': -497.01007080078125, 'referece_logps/rejected': -320.6165466308594, 'referece_logps/chosen': -464.9252014160156, 'logits/rejected': 0.4915694296360016, 'logits/chosen': 0.2652864456176758, 'epoch': 3.49}

 58%|█████▊    | 6244/10740 [31:05:44<21:17:19, 17.05s/it]

 58%|█████▊    | 6245/10740 [31:06:03<22:13:34, 17.80s/it]


 58%|█████▊    | 6247/10740 [31:06:40<22:25:08, 17.96s/it]

 58%|█████▊    | 6248/10740 [31:07:00<23:13:48, 18.62s/it]

 58%|█████▊    | 6249/10740 [31:07:20<23:43:15, 19.01s/it]
{'loss': 0.2763, 'learning_rate': 7.85298709620424e-07, 'rewards/chosen': -2.9031827449798584, 'rewards/rejected': -6.307559490203857, 'rewards/accuracies': 0.875, 'rewards/margins': 3.404376745223999, 'policy_logps/rejected': -288.5814514160156, 'policy_logps/chosen': -327.3196716308594, 'referece_logps/rejected': -225.505859375, 'referece_logps/chosen': -298.287841796875, 'logits/rejected': -0.18813680112361908, 'logits/chosen': -0.2909752428531647, 'epoch': 3.49}

 58%|█████▊    | 6250/10740 [31:07:37<23:02:29, 18.47s/it]

 58%|█████▊    | 6251/10740 [31:07:51<21:19:39, 17.10s/it]

 58%|█████▊    | 6252/10740 [31:08:07<21:00:48, 16.86s/it]


 58%|█████▊    | 6254/10740 [31:08:34<18:46:00, 15.06s/it]
{'loss': 0.1563, 'learning_rate': 7.838262030195851e-07, 'rewards/chosen': -4.103344440460205, 'rewards/rejected': -7.500268459320068, 'rewards/accuracies': 1.0, 'rewards/margins': 3.396923542022705, 'policy_logps/rejected': -283.27886962890625, 'policy_logps/chosen': -347.8163757324219, 'referece_logps/rejected': -208.27618408203125, 'referece_logps/chosen': -306.7829284667969, 'logits/rejected': -0.20829197764396667, 'logits/chosen': -0.34341344237327576, 'epoch': 3.49}

 58%|█████▊    | 6255/10740 [31:08:55<20:49:11, 16.71s/it]


 58%|█████▊    | 6257/10740 [31:09:26<20:24:20, 16.39s/it]
{'loss': 0.203, 'learning_rate': 7.829429347832443e-07, 'rewards/chosen': -3.6232893466949463, 'rewards/rejected': -7.772195816040039, 'rewards/accuracies': 0.875, 'rewards/margins': 4.148906707763672, 'policy_logps/rejected': -383.10552978515625, 'policy_logps/chosen': -247.00592041015625, 'referece_logps/rejected': -305.3835754394531, 'referece_logps/chosen': -210.77304077148438, 'logits/rejected': -0.32780662178993225, 'logits/chosen': -0.2773588001728058, 'epoch': 3.5}

 58%|█████▊    | 6258/10740 [31:09:40<19:16:59, 15.49s/it]

 58%|█████▊    | 6259/10740 [31:10:00<20:53:10, 16.78s/it]

 58%|█████▊    | 6260/10740 [31:10:11<19:01:37, 15.29s/it]


 58%|█████▊    | 6262/10740 [31:10:42<19:14:19, 15.47s/it]
{'loss': 0.2112, 'learning_rate': 7.814712162504142e-07, 'rewards/chosen': -3.9846110343933105, 'rewards/rejected': -6.762896537780762, 'rewards/accuracies': 0.75, 'rewards/margins': 2.778285026550293, 'policy_logps/rejected': -321.45062255859375, 'policy_logps/chosen': -372.29852294921875, 'referece_logps/rejected': -253.8216552734375, 'referece_logps/chosen': -332.4524230957031, 'logits/rejected': 0.12963776290416718, 'logits/chosen': -0.1032877117395401, 'epoch': 3.5}

 58%|█████▊    | 6263/10740 [31:10:54<17:51:23, 14.36s/it]

 58%|█████▊    | 6264/10740 [31:11:14<19:48:51, 15.94s/it]

 58%|█████▊    | 6265/10740 [31:11:32<20:34:42, 16.55s/it]

 58%|█████▊    | 6266/10740 [31:11:50<21:11:43, 17.05s/it]

 58%|█████▊    | 6267/10740 [31:12:02<19:08:42, 15.41s/it]

 58%|█████▊    | 6268/10740 [31:12:21<20:44:09, 16.69s/it]

 58%|█████▊    | 6269/10740 [31:12:37<20:26:41, 16.46s/it]

 58%|█████▊    | 6270/10740 [31:12:50<19:09:21, 15.43s/it]

 58%|█████▊    | 6271/10740 [31:13:07<19:47:28, 15.94s/it]

 58%|█████▊    | 6272/10740 [31:13:26<20:44:00, 16.71s/it]

 58%|█████▊    | 6273/10740 [31:13:42<20:31:44, 16.54s/it]

 58%|█████▊    | 6274/10740 [31:14:02<21:45:02, 17.53s/it]

 58%|█████▊    | 6275/10740 [31:14:20<21:51:08, 17.62s/it]

 58%|█████▊    | 6276/10740 [31:14:38<22:13:07, 17.92s/it]

 58%|█████▊    | 6277/10740 [31:14:55<21:54:26, 17.67s/it]

 58%|█████▊    | 6278/10740 [31:15:07<19:46:49, 15.96s/it]

 58%|█████▊    | 6279/10740 [31:15:28<21:29:41, 17.35s/it]


 58%|█████▊    | 6281/10740 [31:16:01<20:14:27, 16.34s/it]
{'loss': 0.1785, 'learning_rate': 7.758832459616622e-07, 'rewards/chosen': -3.735442638397217, 'rewards/rejected': -8.51504898071289, 'rewards/accuracies': 1.0, 'rewards/margins': 4.779605865478516, 'policy_logps/rejected': -490.0625305175781, 'policy_logps/chosen': -489.1651611328125, 'referece_logps/rejected': -404.91204833984375, 'referece_logps/chosen': -451.8106994628906, 'logits/rejected': -0.03454938530921936, 'logits/chosen': 0.029561519622802734, 'epoch': 3.51}

 58%|█████▊    | 6282/10740 [31:16:19<21:02:10, 16.99s/it]

 59%|█████▊    | 6283/10740 [31:16:31<18:57:17, 15.31s/it]

 59%|█████▊    | 6284/10740 [31:16:47<19:13:21, 15.53s/it]

 59%|█████▊    | 6285/10740 [31:17:02<19:06:05, 15.44s/it]

 59%|█████▊    | 6286/10740 [31:17:22<20:49:58, 16.84s/it]

 59%|█████▊    | 6287/10740 [31:17:37<20:16:03, 16.39s/it]


 59%|█████▊    | 6289/10740 [31:18:19<23:01:04, 18.62s/it]
{'loss': 0.2502, 'learning_rate': 7.73532607244602e-07, 'rewards/chosen': -4.535133361816406, 'rewards/rejected': -6.025856018066406, 'rewards/accuracies': 0.75, 'rewards/margins': 1.490722894668579, 'policy_logps/rejected': -403.0240783691406, 'policy_logps/chosen': -557.9567260742188, 'referece_logps/rejected': -342.7655029296875, 'referece_logps/chosen': -512.6054077148438, 'logits/rejected': 0.5387157201766968, 'logits/chosen': 0.46442490816116333, 'epoch': 3.51}


 59%|█████▊    | 6291/10740 [31:19:03<25:12:20, 20.40s/it]
{'loss': 0.2188, 'learning_rate': 7.729451530085633e-07, 'rewards/chosen': -3.978940486907959, 'rewards/rejected': -7.840932369232178, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8619914054870605, 'policy_logps/rejected': -362.1568298339844, 'policy_logps/chosen': -418.610595703125, 'referece_logps/rejected': -283.74749755859375, 'referece_logps/chosen': -378.82122802734375, 'logits/rejected': 0.10365644842386246, 'logits/chosen': 0.06316883862018585, 'epoch': 3.51}

 59%|█████▊    | 6292/10740 [31:19:18<23:18:16, 18.86s/it]

 59%|█████▊    | 6293/10740 [31:19:31<21:08:05, 17.11s/it]


 59%|█████▊    | 6295/10740 [31:20:09<22:06:58, 17.91s/it]
{'loss': 0.1828, 'learning_rate': 7.717704925645144e-07, 'rewards/chosen': -3.1514499187469482, 'rewards/rejected': -6.7606282234191895, 'rewards/accuracies': 1.0, 'rewards/margins': 3.609178304672241, 'policy_logps/rejected': -341.7449951171875, 'policy_logps/chosen': -347.96240234375, 'referece_logps/rejected': -274.13873291015625, 'referece_logps/chosen': -316.4479064941406, 'logits/rejected': -1.0419740676879883, 'logits/chosen': -1.1137160062789917, 'epoch': 3.52}

 59%|█████▊    | 6296/10740 [31:20:29<22:44:52, 18.43s/it]


 59%|█████▊    | 6298/10740 [31:21:05<22:40:21, 18.37s/it]
{'loss': 0.2197, 'learning_rate': 7.708897150973386e-07, 'rewards/chosen': -2.916414737701416, 'rewards/rejected': -5.961909294128418, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0454940795898438, 'policy_logps/rejected': -367.1863098144531, 'policy_logps/chosen': -375.364013671875, 'referece_logps/rejected': -307.56719970703125, 'referece_logps/chosen': -346.1998291015625, 'logits/rejected': -0.4042000472545624, 'logits/chosen': -0.549085795879364, 'epoch': 3.52}

 59%|█████▊    | 6299/10740 [31:21:26<23:26:22, 19.00s/it]

 59%|█████▊    | 6300/10740 [31:21:42<22:36:17, 18.33s/it]

 59%|█████▊    | 6301/10740 [31:21:55<20:26:36, 16.58s/it]


 59%|█████▊    | 6303/10740 [31:22:25<19:11:52, 15.58s/it]
{'loss': 0.2756, 'learning_rate': 7.694221697708358e-07, 'rewards/chosen': -3.6122689247131348, 'rewards/rejected': -9.147292137145996, 'rewards/accuracies': 1.0, 'rewards/margins': 5.535022735595703, 'policy_logps/rejected': -430.76898193359375, 'policy_logps/chosen': -563.7656860351562, 'referece_logps/rejected': -339.2960510253906, 'referece_logps/chosen': -527.6429443359375, 'logits/rejected': -0.7732198238372803, 'logits/chosen': -1.0483530759811401, 'epoch': 3.52}


 59%|█████▊    | 6305/10740 [31:23:05<21:34:32, 17.51s/it]
{'loss': 0.1216, 'learning_rate': 7.68835298254714e-07, 'rewards/chosen': -4.491316318511963, 'rewards/rejected': -8.843132972717285, 'rewards/accuracies': 0.875, 'rewards/margins': 4.3518171310424805, 'policy_logps/rejected': -430.954345703125, 'policy_logps/chosen': -465.15557861328125, 'referece_logps/rejected': -342.52301025390625, 'referece_logps/chosen': -420.242431640625, 'logits/rejected': 0.30821964144706726, 'logits/chosen': 0.6550437211990356, 'epoch': 3.52}


 59%|█████▊    | 6307/10740 [31:23:44<22:38:29, 18.39s/it]

 59%|█████▊    | 6308/10740 [31:24:03<23:07:12, 18.78s/it]
{'loss': 0.2769, 'learning_rate': 7.679551487347156e-07, 'rewards/chosen': -5.183297157287598, 'rewards/rejected': -7.786430835723877, 'rewards/accuracies': 0.75, 'rewards/margins': 2.603133201599121, 'policy_logps/rejected': -430.9405212402344, 'policy_logps/chosen': -367.4599914550781, 'referece_logps/rejected': -353.0762023925781, 'referece_logps/chosen': -315.62701416015625, 'logits/rejected': 0.5181044340133667, 'logits/chosen': 0.46673357486724854, 'epoch': 3.52}

 59%|█████▊    | 6309/10740 [31:24:19<21:55:28, 17.81s/it]


 59%|█████▉    | 6311/10740 [31:24:55<22:21:53, 18.18s/it]
{'loss': 0.2051, 'learning_rate': 7.670751891601404e-07, 'rewards/chosen': -2.812981367111206, 'rewards/rejected': -4.33144474029541, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5184632539749146, 'policy_logps/rejected': -250.92185974121094, 'policy_logps/chosen': -189.877685546875, 'referece_logps/rejected': -207.607421875, 'referece_logps/chosen': -161.7478790283203, 'logits/rejected': -0.8439236283302307, 'logits/chosen': -0.7785560488700867, 'epoch': 3.53}

 59%|█████▉    | 6312/10740 [31:25:15<22:43:16, 18.47s/it]


 59%|█████▉    | 6314/10740 [31:25:49<22:24:32, 18.23s/it]
{'loss': 0.1729, 'learning_rate': 7.661954202512986e-07, 'rewards/chosen': -3.4492619037628174, 'rewards/rejected': -6.584341049194336, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1350793838500977, 'policy_logps/rejected': -512.1412353515625, 'policy_logps/chosen': -381.0846252441406, 'referece_logps/rejected': -446.2978210449219, 'referece_logps/chosen': -346.5920104980469, 'logits/rejected': -0.2854447662830353, 'logits/chosen': -0.1727854609489441, 'epoch': 3.53}


 59%|█████▉    | 6316/10740 [31:26:30<23:39:26, 19.25s/it]
{'loss': 0.3143, 'learning_rate': 7.656090139264495e-07, 'rewards/chosen': -4.507750034332275, 'rewards/rejected': -7.092094421386719, 'rewards/accuracies': 0.875, 'rewards/margins': 2.584343910217285, 'policy_logps/rejected': -402.0994567871094, 'policy_logps/chosen': -439.11370849609375, 'referece_logps/rejected': -331.1785583496094, 'referece_logps/chosen': -394.03619384765625, 'logits/rejected': 0.0024821385741233826, 'logits/chosen': 0.11541302502155304, 'epoch': 3.53}

 59%|█████▉    | 6317/10740 [31:26:50<24:02:37, 19.57s/it]

 59%|█████▉    | 6318/10740 [31:27:06<22:52:52, 18.63s/it]

 59%|█████▉    | 6319/10740 [31:27:26<23:15:55, 18.95s/it]

 59%|█████▉    | 6320/10740 [31:27:46<23:34:38, 19.20s/it]

 59%|█████▉    | 6321/10740 [31:27:58<21:05:19, 17.18s/it]

 59%|█████▉    | 6322/10740 [31:28:18<21:59:15, 17.92s/it]

 59%|█████▉    | 6323/10740 [31:28:37<22:13:13, 18.11s/it]


 59%|█████▉    | 6325/10740 [31:29:16<23:16:09, 18.97s/it]

 59%|█████▉    | 6326/10740 [31:29:36<23:38:09, 19.28s/it]
{'loss': 0.1883, 'learning_rate': 7.626782656739957e-07, 'rewards/chosen': -2.9831807613372803, 'rewards/rejected': -6.147101402282715, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1639204025268555, 'policy_logps/rejected': -393.9898681640625, 'policy_logps/chosen': -238.93222045898438, 'referece_logps/rejected': -332.51885986328125, 'referece_logps/chosen': -209.10040283203125, 'logits/rejected': -0.41075220704078674, 'logits/chosen': -0.40539300441741943, 'epoch': 3.53}

 59%|█████▉    | 6327/10740 [31:29:50<21:54:31, 17.87s/it]

 59%|█████▉    | 6328/10740 [31:30:03<19:49:09, 16.17s/it]

 59%|█████▉    | 6329/10740 [31:30:20<20:20:35, 16.60s/it]

 59%|█████▉    | 6330/10740 [31:30:37<20:30:30, 16.74s/it]


 59%|█████▉    | 6332/10740 [31:31:14<21:33:07, 17.60s/it]
{'loss': 0.2054, 'learning_rate': 7.609208510962857e-07, 'rewards/chosen': -3.8402576446533203, 'rewards/rejected': -6.564800262451172, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7245428562164307, 'policy_logps/rejected': -305.4314270019531, 'policy_logps/chosen': -227.8324737548828, 'referece_logps/rejected': -239.7834014892578, 'referece_logps/chosen': -189.4298858642578, 'logits/rejected': -0.640844464302063, 'logits/chosen': -0.5484535694122314, 'epoch': 3.54}

 59%|█████▉    | 6333/10740 [31:31:30<20:50:57, 17.03s/it]

 59%|█████▉    | 6334/10740 [31:31:49<21:34:33, 17.63s/it]

 59%|█████▉    | 6335/10740 [31:32:05<21:06:00, 17.24s/it]

 59%|█████▉    | 6336/10740 [31:32:18<19:43:43, 16.13s/it]


 59%|█████▉    | 6338/10740 [31:32:56<20:57:52, 17.15s/it]
{'loss': 0.2262, 'learning_rate': 7.591642193324246e-07, 'rewards/chosen': -3.8302040100097656, 'rewards/rejected': -8.284839630126953, 'rewards/accuracies': 1.0, 'rewards/margins': 4.4546356201171875, 'policy_logps/rejected': -403.748046875, 'policy_logps/chosen': -460.9544372558594, 'referece_logps/rejected': -320.8996276855469, 'referece_logps/chosen': -422.65240478515625, 'logits/rejected': 0.06141355261206627, 'logits/chosen': -0.13955610990524292, 'epoch': 3.54}

 59%|█████▉    | 6339/10740 [31:33:17<22:28:16, 18.38s/it]

 59%|█████▉    | 6340/10740 [31:33:34<22:01:57, 18.03s/it]

 59%|█████▉    | 6341/10740 [31:33:54<22:43:33, 18.60s/it]

 59%|█████▉    | 6342/10740 [31:34:14<23:05:41, 18.90s/it]

 59%|█████▉    | 6343/10740 [31:34:31<22:23:27, 18.33s/it]

 59%|█████▉    | 6344/10740 [31:34:50<22:43:19, 18.61s/it]

 59%|█████▉    | 6345/10740 [31:35:02<20:18:50, 16.64s/it]

 59%|█████▉    | 6346/10740 [31:35:24<22:15:40, 18.24s/it]

 59%|█████▉    | 6347/10740 [31:35:43<22:21:28, 18.32s/it]

 59%|█████▉    | 6348/10740 [31:36:03<23:05:44, 18.93s/it]

 59%|█████▉    | 6349/10740 [31:36:23<23:30:39, 19.28s/it]

 59%|█████▉    | 6350/10740 [31:36:43<23:35:11, 19.34s/it]

 59%|█████▉    | 6351/10740 [31:37:00<22:55:33, 18.80s/it]

 59%|█████▉    | 6352/10740 [31:37:20<23:18:34, 19.12s/it]

 59%|█████▉    | 6353/10740 [31:37:38<22:41:45, 18.62s/it]

 59%|█████▉    | 6354/10740 [31:38:00<24:09:09, 19.82s/it]

 59%|█████▉    | 6355/10740 [31:38:11<20:47:15, 17.07s/it]

 59%|█████▉    | 6356/10740 [31:38:30<21:43:57, 17.85s/it]

 59%|█████▉    | 6357/10740 [31:38:49<22:04:37, 18.13s/it]

 59%|█████▉    | 6358/10740 [31:39:09<22:41:17, 18.64s/it]

 59%|█████▉    | 6359/10740 [31:39:20<19:59:29, 16.43s/it]

 59%|█████▉    | 6360/10740 [31:39:34<18:59:39, 15.61s/it]

 59%|█████▉    | 6361/10740 [31:39:54<20:26:07, 16.80s/it]

 59%|█████▉    | 6362/10740 [31:40:04<18:09:23, 14.93s/it]

 59%|█████▉    | 6363/10740 [31:40:25<20:06:44, 16.54s/it]

 59%|█████▉    | 6364/10740 [31:40:35<17:57:42, 14.78s/it]

 59%|█████▉    | 6365/10740 [31:40:49<17:37:22, 14.50s/it]

 59%|█████▉    | 6366/10740 [31:41:00<16:10:33, 13.31s/it]

 59%|█████▉    | 6367/10740 [31:41:17<17:35:28, 14.48s/it]

 59%|█████▉    | 6368/10740 [31:41:38<20:05:55, 16.55s/it]

 59%|█████▉    | 6369/10740 [31:41:58<21:12:54, 17.47s/it]

 59%|█████▉    | 6370/10740 [31:42:12<20:07:04, 16.57s/it]

 59%|█████▉    | 6371/10740 [31:42:29<20:17:57, 16.73s/it]

 59%|█████▉    | 6372/10740 [31:42:49<21:19:39, 17.58s/it]

 59%|█████▉    | 6373/10740 [31:43:09<22:07:31, 18.24s/it]

 59%|█████▉    | 6374/10740 [31:43:24<21:07:54, 17.42s/it]

 59%|█████▉    | 6375/10740 [31:43:42<21:20:02, 17.60s/it]

 59%|█████▉    | 6376/10740 [31:43:57<20:17:47, 16.74s/it]

 59%|█████▉    | 6377/10740 [31:44:08<18:24:10, 15.18s/it]

 59%|█████▉    | 6378/10740 [31:44:22<17:48:09, 14.69s/it]

 59%|█████▉    | 6379/10740 [31:44:38<18:06:33, 14.95s/it]

 59%|█████▉    | 6380/10740 [31:44:53<18:06:09, 14.95s/it]

 59%|█████▉    | 6381/10740 [31:45:08<18:26:45, 15.23s/it]

 59%|█████▉    | 6382/10740 [31:45:27<19:32:14, 16.14s/it]

 59%|█████▉    | 6383/10740 [31:45:46<20:44:29, 17.14s/it]


 59%|█████▉    | 6385/10740 [31:46:25<22:13:36, 18.37s/it]

 59%|█████▉    | 6386/10740 [31:46:44<22:20:16, 18.47s/it]

 59%|█████▉    | 6387/10740 [31:47:00<21:25:03, 17.71s/it]

 59%|█████▉    | 6388/10740 [31:47:15<20:21:36, 16.84s/it]
{'loss': 0.3325, 'learning_rate': 7.445568338672072e-07, 'rewards/chosen': -3.71539044380188, 'rewards/rejected': -5.997103214263916, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2817134857177734, 'policy_logps/rejected': -443.20751953125, 'policy_logps/chosen': -297.62445068359375, 'referece_logps/rejected': -383.2364807128906, 'referece_logps/chosen': -260.4705810546875, 'logits/rejected': 0.21727700531482697, 'logits/chosen': 0.3779839277267456, 'epoch': 3.57}


 59%|█████▉    | 6390/10740 [31:47:46<19:25:44, 16.08s/it]

 60%|█████▉    | 6391/10740 [31:48:01<18:48:50, 15.57s/it]

 60%|█████▉    | 6392/10740 [31:48:20<20:11:54, 16.72s/it]

 60%|█████▉    | 6393/10740 [31:48:41<21:35:21, 17.88s/it]

 60%|█████▉    | 6394/10740 [31:49:01<22:36:25, 18.73s/it]
{'loss': 0.2249, 'learning_rate': 7.428077853760491e-07, 'rewards/chosen': -4.154755592346191, 'rewards/rejected': -8.231797218322754, 'rewards/accuracies': 0.75, 'rewards/margins': 4.077042102813721, 'policy_logps/rejected': -379.78753662109375, 'policy_logps/chosen': -386.3224792480469, 'referece_logps/rejected': -297.4696044921875, 'referece_logps/chosen': -344.7749328613281, 'logits/rejected': -0.5886232852935791, 'logits/chosen': -0.5620489716529846, 'epoch': 3.57}


 60%|█████▉    | 6396/10740 [31:49:33<20:30:48, 17.00s/it]

 60%|█████▉    | 6397/10740 [31:49:49<20:07:27, 16.68s/it]

 60%|█████▉    | 6398/10740 [31:50:06<20:07:36, 16.69s/it]

 60%|█████▉    | 6399/10740 [31:50:24<20:44:56, 17.21s/it]

 60%|█████▉    | 6400/10740 [31:50:41<20:28:24, 16.98s/it]

 60%|█████▉    | 6401/10740 [31:51:01<21:25:18, 17.77s/it]

 60%|█████▉    | 6402/10740 [31:51:16<20:29:41, 17.01s/it]

 60%|█████▉    | 6403/10740 [31:51:34<21:03:54, 17.49s/it]

 60%|█████▉    | 6404/10740 [31:51:53<21:25:47, 17.79s/it]

 60%|█████▉    | 6405/10740 [31:52:12<21:55:26, 18.21s/it]

 60%|█████▉    | 6406/10740 [31:52:28<21:16:41, 17.67s/it]

 60%|█████▉    | 6407/10740 [31:52:50<22:46:53, 18.93s/it]

 60%|█████▉    | 6408/10740 [31:53:10<23:12:09, 19.28s/it]

 60%|█████▉    | 6409/10740 [31:53:31<23:45:04, 19.74s/it]

 60%|█████▉    | 6410/10740 [31:53:51<23:51:45, 19.84s/it]

 60%|█████▉    | 6411/10740 [31:54:08<22:44:32, 18.91s/it]

 60%|█████▉    | 6412/10740 [31:54:22<20:49:29, 17.32s/it]

 60%|█████▉    | 6413/10740 [31:54:43<22:16:44, 18.54s/it]

 60%|█████▉    | 6414/10740 [31:55:01<22:03:16, 18.35s/it]

 60%|█████▉    | 6415/10740 [31:55:21<22:29:41, 18.72s/it]

 60%|█████▉    | 6416/10740 [31:55:40<22:51:32, 19.03s/it]

 60%|█████▉    | 6417/10740 [31:56:02<23:50:22, 19.85s/it]

 60%|█████▉    | 6418/10740 [31:56:22<23:49:52, 19.85s/it]
{'loss': 0.2548, 'learning_rate': 7.358200698511443e-07, 'rewards/chosen': -4.161768913269043, 'rewards/rejected': -7.807894706726074, 'rewards/accuracies': 0.875, 'rewards/margins': 3.646125078201294, 'policy_logps/rejected': -381.8430480957031, 'policy_logps/chosen': -416.41839599609375, 'referece_logps/rejected': -303.7640686035156, 'referece_logps/chosen': -374.8007507324219, 'logits/rejected': -0.34866875410079956, 'logits/chosen': -0.3683658242225647, 'epoch': 3.59}


 60%|█████▉    | 6420/10740 [31:56:54<21:15:07, 17.71s/it]

 60%|█████▉    | 6421/10740 [31:57:16<22:43:27, 18.94s/it]

 60%|█████▉    | 6422/10740 [31:57:40<24:34:52, 20.49s/it]

 60%|█████▉    | 6423/10740 [31:58:00<24:05:06, 20.08s/it]

 60%|█████▉    | 6424/10740 [31:58:15<22:24:47, 18.69s/it]

 60%|█████▉    | 6425/10740 [31:58:35<22:59:18, 19.18s/it]

 60%|█████▉    | 6426/10740 [31:58:55<23:10:47, 19.34s/it]

 60%|█████▉    | 6427/10740 [31:59:17<24:02:52, 20.07s/it]

 60%|█████▉    | 6428/10740 [31:59:32<22:21:51, 18.67s/it]

 60%|█████▉    | 6429/10740 [31:59:55<23:49:56, 19.90s/it]

 60%|█████▉    | 6430/10740 [32:00:17<24:29:50, 20.46s/it]

 60%|█████▉    | 6431/10740 [32:00:36<24:10:08, 20.19s/it]

 60%|█████▉    | 6432/10740 [32:00:56<23:50:18, 19.92s/it]

 60%|█████▉    | 6433/10740 [32:01:10<21:47:50, 18.22s/it]

 60%|█████▉    | 6434/10740 [32:01:31<22:54:02, 19.15s/it]

 60%|█████▉    | 6435/10740 [32:01:53<23:52:01, 19.96s/it]

 60%|█████▉    | 6436/10740 [32:02:10<22:44:22, 19.02s/it]

 60%|█████▉    | 6437/10740 [32:02:30<23:05:21, 19.32s/it]

 60%|█████▉    | 6438/10740 [32:02:50<23:12:54, 19.43s/it]

 60%|█████▉    | 6439/10740 [32:03:08<22:49:39, 19.11s/it]
{'loss': 0.1006, 'learning_rate': 7.297171593641664e-07, 'rewards/chosen': -3.2601912021636963, 'rewards/rejected': -8.42837142944336, 'rewards/accuracies': 1.0, 'rewards/margins': 5.1681809425354, 'policy_logps/rejected': -349.6878662109375, 'policy_logps/chosen': -270.850341796875, 'referece_logps/rejected': -265.40411376953125, 'referece_logps/chosen': -238.2484588623047, 'logits/rejected': -0.17094749212265015, 'logits/chosen': -0.1485321819782257, 'epoch': 3.6}


 60%|█████▉    | 6441/10740 [32:03:44<22:19:11, 18.69s/it]

 60%|█████▉    | 6442/10740 [32:04:03<22:28:14, 18.82s/it]

 60%|█████▉    | 6443/10740 [32:04:21<22:06:00, 18.52s/it]

 60%|██████    | 6444/10740 [32:04:37<21:10:29, 17.74s/it]
{'loss': 0.1773, 'learning_rate': 7.282656741669626e-07, 'rewards/chosen': -3.4745707511901855, 'rewards/rejected': -8.529525756835938, 'rewards/accuracies': 1.0, 'rewards/margins': 5.054955005645752, 'policy_logps/rejected': -475.3993835449219, 'policy_logps/chosen': -355.23663330078125, 'referece_logps/rejected': -390.1041564941406, 'referece_logps/chosen': -320.49090576171875, 'logits/rejected': -0.1808394193649292, 'logits/chosen': -0.17775726318359375, 'epoch': 3.6}


 60%|██████    | 6446/10740 [32:05:11<20:47:06, 17.43s/it]
{'loss': 0.2036, 'learning_rate': 7.276852529075978e-07, 'rewards/chosen': -2.9924137592315674, 'rewards/rejected': -6.805718421936035, 'rewards/accuracies': 1.0, 'rewards/margins': 3.813305139541626, 'policy_logps/rejected': -370.7803955078125, 'policy_logps/chosen': -359.8595886230469, 'referece_logps/rejected': -302.72320556640625, 'referece_logps/chosen': -329.9354553222656, 'logits/rejected': -0.2997799217700958, 'logits/chosen': -0.3283543586730957, 'epoch': 3.6}


 60%|██████    | 6448/10740 [32:05:42<19:35:08, 16.43s/it]
{'loss': 0.1836, 'learning_rate': 7.271049307190069e-07, 'rewards/chosen': -4.404369831085205, 'rewards/rejected': -8.33549690246582, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9311270713806152, 'policy_logps/rejected': -389.68902587890625, 'policy_logps/chosen': -350.620361328125, 'referece_logps/rejected': -306.3340759277344, 'referece_logps/chosen': -306.57672119140625, 'logits/rejected': -0.7893542647361755, 'logits/chosen': -0.8601072430610657, 'epoch': 3.6}


 60%|██████    | 6450/10740 [32:06:17<20:16:21, 17.01s/it]

 60%|██████    | 6451/10740 [32:06:40<22:09:58, 18.61s/it]
{'loss': 0.16, 'learning_rate': 7.26234633655652e-07, 'rewards/chosen': -3.041865348815918, 'rewards/rejected': -6.339700222015381, 'rewards/accuracies': 1.0, 'rewards/margins': 3.297835111618042, 'policy_logps/rejected': -312.8920593261719, 'policy_logps/chosen': -401.58502197265625, 'referece_logps/rejected': -249.4950714111328, 'referece_logps/chosen': -371.1663818359375, 'logits/rejected': 0.4883474111557007, 'logits/chosen': 0.4480130672454834, 'epoch': 3.6}

 60%|██████    | 6452/10740 [32:06:56<21:27:38, 18.02s/it]


 60%|██████    | 6454/10740 [32:07:21<17:49:45, 14.98s/it]
{'loss': 0.2497, 'learning_rate': 7.253645606889668e-07, 'rewards/chosen': -3.1442806720733643, 'rewards/rejected': -5.713610649108887, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5693306922912598, 'policy_logps/rejected': -428.88409423828125, 'policy_logps/chosen': -387.56610107421875, 'referece_logps/rejected': -371.7479553222656, 'referece_logps/chosen': -356.123291015625, 'logits/rejected': -0.6578730940818787, 'logits/chosen': -0.6165094375610352, 'epoch': 3.61}


 60%|██████    | 6456/10740 [32:07:54<18:12:15, 15.30s/it]

 60%|██████    | 6457/10740 [32:08:10<18:25:10, 15.48s/it]

 60%|██████    | 6458/10740 [32:08:28<19:17:49, 16.22s/it]

 60%|██████    | 6459/10740 [32:08:48<20:44:49, 17.45s/it]
{'loss': 0.2293, 'learning_rate': 7.239149390047582e-07, 'rewards/chosen': -3.3789472579956055, 'rewards/rejected': -7.241957187652588, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8630099296569824, 'policy_logps/rejected': -583.2565307617188, 'policy_logps/chosen': -486.84765625, 'referece_logps/rejected': -510.8369445800781, 'referece_logps/chosen': -453.0581359863281, 'logits/rejected': -0.3475351631641388, 'logits/chosen': -0.2695547044277191, 'epoch': 3.61}

 60%|██████    | 6460/10740 [32:09:07<21:04:43, 17.73s/it]


 60%|██████    | 6462/10740 [32:09:37<19:43:15, 16.60s/it]
{'loss': 0.173, 'learning_rate': 7.230454671106434e-07, 'rewards/chosen': -3.6470448970794678, 'rewards/rejected': -7.289844989776611, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6427998542785645, 'policy_logps/rejected': -367.7958984375, 'policy_logps/chosen': -423.2247619628906, 'referece_logps/rejected': -294.8974609375, 'referece_logps/chosen': -386.7543029785156, 'logits/rejected': -0.6227283477783203, 'logits/chosen': -0.6001064777374268, 'epoch': 3.61}

 60%|██████    | 6463/10740 [32:09:55<20:11:05, 16.99s/it]


 60%|██████    | 6465/10740 [32:10:36<22:28:49, 18.93s/it]

 60%|██████    | 6466/10740 [32:10:48<19:52:35, 16.74s/it]

 60%|██████    | 6467/10740 [32:11:08<21:07:23, 17.80s/it]

 60%|██████    | 6468/10740 [32:11:29<22:14:40, 18.75s/it]

 60%|██████    | 6469/10740 [32:11:50<22:58:35, 19.37s/it]

 60%|██████    | 6470/10740 [32:12:10<23:07:07, 19.49s/it]

 60%|██████    | 6471/10740 [32:12:28<22:45:55, 19.20s/it]

 60%|██████    | 6472/10740 [32:12:42<20:56:26, 17.66s/it]

 60%|██████    | 6473/10740 [32:13:04<22:15:45, 18.78s/it]

 60%|██████    | 6474/10740 [32:13:26<23:23:27, 19.74s/it]

 60%|██████    | 6475/10740 [32:13:46<23:24:00, 19.75s/it]

 60%|██████    | 6476/10740 [32:14:02<22:11:29, 18.74s/it]
{'loss': 0.1726, 'learning_rate': 7.189909406750017e-07, 'rewards/chosen': -2.4028050899505615, 'rewards/rejected': -7.275712490081787, 'rewards/accuracies': 1.0, 'rewards/margins': 4.8729071617126465, 'policy_logps/rejected': -325.0361633300781, 'policy_logps/chosen': -409.374267578125, 'referece_logps/rejected': -252.27903747558594, 'referece_logps/chosen': -385.34625244140625, 'logits/rejected': 0.08549124002456665, 'logits/chosen': -0.030384454876184464, 'epoch': 3.62}


 60%|██████    | 6478/10740 [32:14:32<19:41:30, 16.63s/it]

 60%|██████    | 6479/10740 [32:14:51<20:27:30, 17.28s/it]
{'loss': 0.3134, 'learning_rate': 7.181227628603242e-07, 'rewards/chosen': -4.127915859222412, 'rewards/rejected': -7.732680797576904, 'rewards/accuracies': 0.75, 'rewards/margins': 3.604764938354492, 'policy_logps/rejected': -406.5711669921875, 'policy_logps/chosen': -489.863037109375, 'referece_logps/rejected': -329.244384765625, 'referece_logps/chosen': -448.5838623046875, 'logits/rejected': 0.574524998664856, 'logits/chosen': 0.422906756401062, 'epoch': 3.62}


 60%|██████    | 6481/10740 [32:15:15<17:24:17, 14.71s/it]

 60%|██████    | 6482/10740 [32:15:34<19:07:25, 16.17s/it]
{'loss': 0.2014, 'learning_rate': 7.172548157824668e-07, 'rewards/chosen': -3.1668829917907715, 'rewards/rejected': -6.759613990783691, 'rewards/accuracies': 0.75, 'rewards/margins': 3.592731475830078, 'policy_logps/rejected': -306.45135498046875, 'policy_logps/chosen': -388.2462158203125, 'referece_logps/rejected': -238.85520935058594, 'referece_logps/chosen': -356.5774230957031, 'logits/rejected': -0.6808373332023621, 'logits/chosen': -0.8127585649490356, 'epoch': 3.62}


 60%|██████    | 6484/10740 [32:16:09<19:32:45, 16.53s/it]
{'loss': 0.1805, 'learning_rate': 7.166763129352115e-07, 'rewards/chosen': -5.109792232513428, 'rewards/rejected': -9.160154342651367, 'rewards/accuracies': 1.0, 'rewards/margins': 4.050361633300781, 'policy_logps/rejected': -541.5360107421875, 'policy_logps/chosen': -466.1143493652344, 'referece_logps/rejected': -449.9344787597656, 'referece_logps/chosen': -415.0164489746094, 'logits/rejected': -0.9456883072853088, 'logits/chosen': -0.7703974843025208, 'epoch': 3.62}


 60%|██████    | 6486/10740 [32:16:46<20:44:06, 17.55s/it]
{'loss': 0.2071, 'learning_rate': 7.160979131638905e-07, 'rewards/chosen': -3.231788158416748, 'rewards/rejected': -7.576474666595459, 'rewards/accuracies': 1.0, 'rewards/margins': 4.344686031341553, 'policy_logps/rejected': -323.82574462890625, 'policy_logps/chosen': -314.5608825683594, 'referece_logps/rejected': -248.0609893798828, 'referece_logps/chosen': -282.24298095703125, 'logits/rejected': 0.695218563079834, 'logits/chosen': 0.622535765171051, 'epoch': 3.62}


 60%|██████    | 6488/10740 [32:17:12<18:13:24, 15.43s/it]

 60%|██████    | 6489/10740 [32:17:30<19:08:01, 16.20s/it]
{'loss': 0.2765, 'learning_rate': 7.152305072345856e-07, 'rewards/chosen': -3.5363829135894775, 'rewards/rejected': -7.035924911499023, 'rewards/accuracies': 1.0, 'rewards/margins': 3.499542474746704, 'policy_logps/rejected': -386.82196044921875, 'policy_logps/chosen': -334.66253662109375, 'referece_logps/rejected': -316.4627380371094, 'referece_logps/chosen': -299.2987060546875, 'logits/rejected': -0.1400979459285736, 'logits/chosen': -0.18118922412395477, 'epoch': 3.63}


 60%|██████    | 6491/10740 [32:18:01<18:55:33, 16.04s/it]

 60%|██████    | 6492/10740 [32:18:14<18:01:21, 15.27s/it]

 60%|██████    | 6493/10740 [32:18:32<18:55:37, 16.04s/it]

 60%|██████    | 6494/10740 [32:18:52<20:13:53, 17.15s/it]
{'loss': 0.3144, 'learning_rate': 7.137853490459354e-07, 'rewards/chosen': -3.110053300857544, 'rewards/rejected': -7.490912914276123, 'rewards/accuracies': 0.75, 'rewards/margins': 4.380859375, 'policy_logps/rejected': -405.7132873535156, 'policy_logps/chosen': -423.50372314453125, 'referece_logps/rejected': -330.80413818359375, 'referece_logps/chosen': -392.4031677246094, 'logits/rejected': 0.5102008581161499, 'logits/chosen': 0.5271304845809937, 'epoch': 3.63}

 60%|██████    | 6495/10740 [32:19:12<21:08:34, 17.93s/it]


 60%|██████    | 6497/10740 [32:19:51<22:07:54, 18.78s/it]

 61%|██████    | 6498/10740 [32:20:12<22:58:49, 19.50s/it]

 61%|██████    | 6499/10740 [32:20:24<20:22:07, 17.29s/it]
{'loss': 0.1885, 'learning_rate': 7.123408416552777e-07, 'rewards/chosen': -3.564763069152832, 'rewards/rejected': -8.192410469055176, 'rewards/accuracies': 0.875, 'rewards/margins': 4.6276469230651855, 'policy_logps/rejected': -236.78265380859375, 'policy_logps/chosen': -342.85107421875, 'referece_logps/rejected': -154.85855102539062, 'referece_logps/chosen': -307.2034606933594, 'logits/rejected': -0.2641793191432953, 'logits/chosen': -0.32678699493408203, 'epoch': 3.63}

 61%|██████    | 6500/10740 [32:20:45<21:45:10, 18.47s/it]

 61%|██████    | 6501/10740 [32:21:15<25:44:57, 21.87s/it]

 61%|██████    | 6502/10740 [32:21:35<25:05:49, 21.32s/it]


 61%|██████    | 6504/10740 [32:22:13<23:43:13, 20.16s/it]
{'loss': 0.1638, 'learning_rate': 7.108969883471484e-07, 'rewards/chosen': -3.314180612564087, 'rewards/rejected': -7.078553676605225, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7643730640411377, 'policy_logps/rejected': -254.27557373046875, 'policy_logps/chosen': -297.57958984375, 'referece_logps/rejected': -183.49005126953125, 'referece_logps/chosen': -264.43780517578125, 'logits/rejected': -0.8828906416893005, 'logits/chosen': -1.0691654682159424, 'epoch': 3.63}


 61%|██████    | 6506/10740 [32:22:45<21:38:53, 18.41s/it]

 61%|██████    | 6507/10740 [32:23:05<22:08:41, 18.83s/it]

 61%|██████    | 6508/10740 [32:23:20<21:06:41, 17.96s/it]

 61%|██████    | 6509/10740 [32:23:38<21:06:34, 17.96s/it]

 61%|██████    | 6510/10740 [32:23:56<21:00:50, 17.88s/it]

 61%|██████    | 6511/10740 [32:24:19<22:35:01, 19.22s/it]

 61%|██████    | 6512/10740 [32:24:35<21:33:58, 18.36s/it]

 61%|██████    | 6513/10740 [32:24:53<21:26:25, 18.26s/it]
{'loss': 0.1596, 'learning_rate': 7.082997111590285e-07, 'rewards/chosen': -3.511892318725586, 'rewards/rejected': -7.688914775848389, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1770219802856445, 'policy_logps/rejected': -432.2396240234375, 'policy_logps/chosen': -506.13555908203125, 'referece_logps/rejected': -355.3504638671875, 'referece_logps/chosen': -471.0166320800781, 'logits/rejected': -0.14057950675487518, 'logits/chosen': -0.2982112765312195, 'epoch': 3.64}


 61%|██████    | 6515/10740 [32:25:33<22:34:07, 19.23s/it]
{'loss': 0.2883, 'learning_rate': 7.077228296164231e-07, 'rewards/chosen': -3.6204142570495605, 'rewards/rejected': -6.358972549438477, 'rewards/accuracies': 1.0, 'rewards/margins': 2.738558292388916, 'policy_logps/rejected': -397.3309326171875, 'policy_logps/chosen': -422.4410705566406, 'referece_logps/rejected': -333.7412109375, 'referece_logps/chosen': -386.2369384765625, 'logits/rejected': -0.5436274409294128, 'logits/chosen': -0.536754310131073, 'epoch': 3.64}

 61%|██████    | 6516/10740 [32:25:46<20:04:16, 17.11s/it]


 61%|██████    | 6518/10740 [32:26:19<19:46:33, 16.86s/it]

 61%|██████    | 6519/10740 [32:26:39<20:50:52, 17.78s/it]

 61%|██████    | 6520/10740 [32:26:59<21:34:10, 18.40s/it]

 61%|██████    | 6521/10740 [32:27:14<20:27:26, 17.46s/it]

 61%|██████    | 6522/10740 [32:27:31<20:04:49, 17.14s/it]

 61%|██████    | 6523/10740 [32:27:50<20:57:55, 17.90s/it]

 61%|██████    | 6524/10740 [32:28:11<21:47:23, 18.61s/it]
{'loss': 0.1686, 'learning_rate': 7.051281815784427e-07, 'rewards/chosen': -3.6216530799865723, 'rewards/rejected': -6.299819469451904, 'rewards/accuracies': 1.0, 'rewards/margins': 2.678166151046753, 'policy_logps/rejected': -308.33538818359375, 'policy_logps/chosen': -339.7048645019531, 'referece_logps/rejected': -245.33721923828125, 'referece_logps/chosen': -303.4883117675781, 'logits/rejected': -0.45628052949905396, 'logits/chosen': -0.6303436756134033, 'epoch': 3.64}

 61%|██████    | 6525/10740 [32:28:30<22:10:26, 18.94s/it]

 61%|██████    | 6526/10740 [32:28:50<22:28:51, 19.21s/it]

 61%|██████    | 6527/10740 [32:29:04<20:37:31, 17.62s/it]


 61%|██████    | 6529/10740 [32:29:38<19:41:47, 16.84s/it]

 61%|██████    | 6530/10740 [32:29:55<20:01:22, 17.12s/it]

 61%|██████    | 6531/10740 [32:30:13<20:12:31, 17.28s/it]
{'loss': 0.2458, 'learning_rate': 7.031116228957266e-07, 'rewards/chosen': -3.8551831245422363, 'rewards/rejected': -6.80068302154541, 'rewards/accuracies': 0.875, 'rewards/margins': 2.945499897003174, 'policy_logps/rejected': -414.3526611328125, 'policy_logps/chosen': -533.139892578125, 'referece_logps/rejected': -346.3457946777344, 'referece_logps/chosen': -494.58807373046875, 'logits/rejected': -0.86744624376297, 'logits/chosen': -0.8047817945480347, 'epoch': 3.65}


 61%|██████    | 6533/10740 [32:30:41<18:10:52, 15.56s/it]

 61%|██████    | 6534/10740 [32:30:57<18:23:03, 15.74s/it]

 61%|██████    | 6535/10740 [32:31:17<19:39:06, 16.82s/it]

 61%|██████    | 6536/10740 [32:31:33<19:34:47, 16.77s/it]

 61%|██████    | 6537/10740 [32:31:52<20:07:22, 17.24s/it]

 61%|██████    | 6538/10740 [32:32:11<20:45:50, 17.79s/it]

 61%|██████    | 6539/10740 [32:32:27<20:07:54, 17.25s/it]
{'loss': 0.1541, 'learning_rate': 7.008086050879258e-07, 'rewards/chosen': -3.1008143424987793, 'rewards/rejected': -7.674769401550293, 'rewards/accuracies': 0.875, 'rewards/margins': 4.57395601272583, 'policy_logps/rejected': -244.64389038085938, 'policy_logps/chosen': -339.9718322753906, 'referece_logps/rejected': -167.8961639404297, 'referece_logps/chosen': -308.96368408203125, 'logits/rejected': -0.08043593168258667, 'logits/chosen': -0.14175699651241302, 'epoch': 3.65}

 61%|██████    | 6540/10740 [32:32:44<20:13:57, 17.34s/it]


 61%|██████    | 6542/10740 [32:33:23<21:28:33, 18.42s/it]

 61%|██████    | 6543/10740 [32:33:37<19:46:32, 16.96s/it]
{'loss': 0.2042, 'learning_rate': 6.996577484388348e-07, 'rewards/chosen': -3.390904664993286, 'rewards/rejected': -7.5585761070251465, 'rewards/accuracies': 0.875, 'rewards/margins': 4.167670726776123, 'policy_logps/rejected': -411.67822265625, 'policy_logps/chosen': -324.9022216796875, 'referece_logps/rejected': -336.09246826171875, 'referece_logps/chosen': -290.9931945800781, 'logits/rejected': 0.15624591708183289, 'logits/chosen': 0.21594318747520447, 'epoch': 3.66}

 61%|██████    | 6544/10740 [32:33:58<21:16:58, 18.26s/it]

 61%|██████    | 6545/10740 [32:34:10<19:07:25, 16.41s/it]

 61%|██████    | 6546/10740 [32:34:31<20:28:47, 17.58s/it]


 61%|██████    | 6548/10740 [32:35:04<19:28:23, 16.72s/it]

 61%|██████    | 6549/10740 [32:35:24<20:44:44, 17.82s/it]

 61%|██████    | 6550/10740 [32:35:40<20:00:00, 17.18s/it]

 61%|██████    | 6551/10740 [32:35:50<17:29:41, 15.03s/it]

 61%|██████    | 6552/10740 [32:36:02<16:22:57, 14.08s/it]
{'loss': 0.249, 'learning_rate': 6.970699215651435e-07, 'rewards/chosen': -4.3611836433410645, 'rewards/rejected': -9.732751846313477, 'rewards/accuracies': 0.75, 'rewards/margins': 5.371567726135254, 'policy_logps/rejected': -457.9872741699219, 'policy_logps/chosen': -424.0604553222656, 'referece_logps/rejected': -360.6597595214844, 'referece_logps/chosen': -380.44866943359375, 'logits/rejected': -0.6527813673019409, 'logits/chosen': -0.6673190593719482, 'epoch': 3.66}

 61%|██████    | 6553/10740 [32:36:23<18:47:15, 16.15s/it]

 61%|██████    | 6554/10740 [32:36:39<18:57:54, 16.31s/it]

 61%|██████    | 6555/10740 [32:36:59<20:03:09, 17.25s/it]


 61%|██████    | 6557/10740 [32:37:38<21:36:34, 18.60s/it]
{'loss': 0.1128, 'learning_rate': 6.95633203076476e-07, 'rewards/chosen': -3.380244493484497, 'rewards/rejected': -6.047922611236572, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6676788330078125, 'policy_logps/rejected': -442.8600769042969, 'policy_logps/chosen': -462.3855895996094, 'referece_logps/rejected': -382.38079833984375, 'referece_logps/chosen': -428.58319091796875, 'logits/rejected': -0.049772944301366806, 'logits/chosen': 0.060301851481199265, 'epoch': 3.66}

 61%|██████    | 6558/10740 [32:37:57<21:59:55, 18.94s/it]


 61%|██████    | 6560/10740 [32:38:34<21:23:49, 18.43s/it]
{'loss': 0.2055, 'learning_rate': 6.947715039690831e-07, 'rewards/chosen': -3.0922131538391113, 'rewards/rejected': -5.902649402618408, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8104360103607178, 'policy_logps/rejected': -255.68331909179688, 'policy_logps/chosen': -375.8816833496094, 'referece_logps/rejected': -196.65682983398438, 'referece_logps/chosen': -344.9595642089844, 'logits/rejected': -0.6251415014266968, 'logits/chosen': -1.01758873462677, 'epoch': 3.66}

 61%|██████    | 6561/10740 [32:38:51<20:50:59, 17.96s/it]

 61%|██████    | 6562/10740 [32:39:07<20:08:26, 17.35s/it]


 61%|██████    | 6564/10740 [32:39:48<22:00:43, 18.98s/it]

 61%|██████    | 6565/10740 [32:40:10<23:10:47, 19.99s/it]
{'loss': 0.2474, 'learning_rate': 6.933358943639013e-07, 'rewards/chosen': -3.6763205528259277, 'rewards/rejected': -6.994144916534424, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3178250789642334, 'policy_logps/rejected': -379.7030029296875, 'policy_logps/chosen': -333.83441162109375, 'referece_logps/rejected': -309.7615661621094, 'referece_logps/chosen': -297.07122802734375, 'logits/rejected': 0.20449507236480713, 'logits/chosen': 0.22563201189041138, 'epoch': 3.67}

 61%|██████    | 6566/10740 [32:40:27<22:06:19, 19.07s/it]


 61%|██████    | 6568/10740 [32:40:56<19:23:47, 16.74s/it]
{'loss': 0.1623, 'learning_rate': 6.924748630941113e-07, 'rewards/chosen': -2.7643320560455322, 'rewards/rejected': -6.910419464111328, 'rewards/accuracies': 1.0, 'rewards/margins': 4.146087169647217, 'policy_logps/rejected': -201.55020141601562, 'policy_logps/chosen': -307.97760009765625, 'referece_logps/rejected': -132.4459991455078, 'referece_logps/chosen': -280.334228515625, 'logits/rejected': 0.2858603000640869, 'logits/chosen': 0.22704842686653137, 'epoch': 3.67}

 61%|██████    | 6569/10740 [32:41:15<20:04:14, 17.32s/it]

 61%|██████    | 6570/10740 [32:41:33<20:25:01, 17.63s/it]


 61%|██████    | 6572/10740 [32:42:08<20:21:02, 17.58s/it]
{'loss': 0.2212, 'learning_rate': 6.913272131051401e-07, 'rewards/chosen': -3.1456401348114014, 'rewards/rejected': -6.363503932952881, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2178637981414795, 'policy_logps/rejected': -347.215576171875, 'policy_logps/chosen': -303.05999755859375, 'referece_logps/rejected': -283.5805358886719, 'referece_logps/chosen': -271.6036071777344, 'logits/rejected': 0.24804145097732544, 'logits/chosen': 0.23481687903404236, 'epoch': 3.67}

 61%|██████    | 6573/10740 [32:42:20<18:09:19, 15.69s/it]

 61%|██████    | 6574/10740 [32:42:34<17:32:15, 15.15s/it]

 61%|██████    | 6575/10740 [32:42:49<17:31:38, 15.15s/it]

 61%|██████    | 6576/10740 [32:43:09<19:27:06, 16.82s/it]

 61%|██████    | 6577/10740 [32:43:29<20:30:12, 17.73s/it]

 61%|██████    | 6578/10740 [32:43:45<19:56:47, 17.25s/it]

 61%|██████▏   | 6579/10740 [32:44:07<21:29:11, 18.59s/it]

 61%|██████▏   | 6580/10740 [32:44:26<21:24:25, 18.53s/it]

 61%|██████▏   | 6581/10740 [32:44:46<21:58:04, 19.02s/it]

 61%|██████▏   | 6582/10740 [32:44:58<19:31:41, 16.91s/it]

 61%|██████▏   | 6583/10740 [32:45:17<20:19:04, 17.60s/it]


 61%|██████▏   | 6585/10740 [32:45:49<19:16:12, 16.70s/it]
{'loss': 0.1571, 'learning_rate': 6.876004615242448e-07, 'rewards/chosen': -3.055069923400879, 'rewards/rejected': -6.896420478820801, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8413503170013428, 'policy_logps/rejected': -323.4637451171875, 'policy_logps/chosen': -248.4690704345703, 'referece_logps/rejected': -254.4995574951172, 'referece_logps/chosen': -217.91839599609375, 'logits/rejected': -0.3053356409072876, 'logits/chosen': -0.28712067008018494, 'epoch': 3.68}

 61%|██████▏   | 6586/10740 [32:46:05<19:13:42, 16.66s/it]

 61%|██████▏   | 6587/10740 [32:46:20<18:31:36, 16.06s/it]


 61%|██████▏   | 6589/10740 [32:46:50<17:35:50, 15.26s/it]

 61%|██████▏   | 6590/10740 [32:47:08<18:31:09, 16.07s/it]
{'loss': 0.3287, 'learning_rate': 6.861683710112145e-07, 'rewards/chosen': -3.866971492767334, 'rewards/rejected': -7.336410045623779, 'rewards/accuracies': 0.875, 'rewards/margins': 3.469438314437866, 'policy_logps/rejected': -419.0522155761719, 'policy_logps/chosen': -411.6507873535156, 'referece_logps/rejected': -345.6881103515625, 'referece_logps/chosen': -372.9810485839844, 'logits/rejected': 0.3902658224105835, 'logits/chosen': 0.2787191867828369, 'epoch': 3.68}

 61%|██████▏   | 6591/10740 [32:47:19<16:44:08, 14.52s/it]

 61%|██████▏   | 6592/10740 [32:47:34<16:40:59, 14.48s/it]

 61%|██████▏   | 6593/10740 [32:47:52<17:59:11, 15.61s/it]

 61%|██████▏   | 6594/10740 [32:48:08<18:08:43, 15.76s/it]

 61%|██████▏   | 6595/10740 [32:48:29<20:04:03, 17.43s/it]

 61%|██████▏   | 6596/10740 [32:48:49<21:00:23, 18.25s/it]

 61%|██████▏   | 6597/10740 [32:49:03<19:22:04, 16.83s/it]

 61%|██████▏   | 6598/10740 [32:49:18<18:35:28, 16.16s/it]

 61%|██████▏   | 6599/10740 [32:49:31<17:41:44, 15.38s/it]


 61%|██████▏   | 6601/10740 [32:50:08<19:22:59, 16.86s/it]
{'loss': 0.1624, 'learning_rate': 6.830202883151206e-07, 'rewards/chosen': -3.193899631500244, 'rewards/rejected': -6.648970127105713, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4550697803497314, 'policy_logps/rejected': -486.5426940917969, 'policy_logps/chosen': -501.69329833984375, 'referece_logps/rejected': -420.052978515625, 'referece_logps/chosen': -469.7543029785156, 'logits/rejected': 0.15526534616947174, 'logits/chosen': 0.2018267959356308, 'epoch': 3.69}

 61%|██████▏   | 6602/10740 [32:50:28<20:24:54, 17.76s/it]

 61%|██████▏   | 6603/10740 [32:50:47<20:48:50, 18.11s/it]

 61%|██████▏   | 6604/10740 [32:51:07<21:28:29, 18.69s/it]

 61%|██████▏   | 6605/10740 [32:51:26<21:20:05, 18.57s/it]


 62%|██████▏   | 6607/10740 [32:52:03<21:39:32, 18.87s/it]
{'loss': 0.1649, 'learning_rate': 6.813046204209614e-07, 'rewards/chosen': -3.0207557678222656, 'rewards/rejected': -4.857656478881836, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8369008302688599, 'policy_logps/rejected': -261.9579772949219, 'policy_logps/chosen': -310.7919921875, 'referece_logps/rejected': -213.3813934326172, 'referece_logps/chosen': -280.58441162109375, 'logits/rejected': -0.3131762444972992, 'logits/chosen': -0.41950350999832153, 'epoch': 3.69}

 62%|██████▏   | 6608/10740 [32:52:18<20:22:52, 17.76s/it]

 62%|██████▏   | 6609/10740 [32:52:37<20:51:37, 18.18s/it]

 62%|██████▏   | 6610/10740 [32:52:58<21:50:50, 19.04s/it]

 62%|██████▏   | 6611/10740 [32:53:17<21:54:32, 19.10s/it]


 62%|██████▏   | 6613/10740 [32:53:47<19:19:25, 16.86s/it]

 62%|██████▏   | 6614/10740 [32:53:59<17:46:39, 15.51s/it]

 62%|██████▏   | 6615/10740 [32:54:15<17:56:07, 15.65s/it]
{'loss': 0.217, 'learning_rate': 6.790186874218437e-07, 'rewards/chosen': -4.3362321853637695, 'rewards/rejected': -11.20412826538086, 'rewards/accuracies': 1.0, 'rewards/margins': 6.867895603179932, 'policy_logps/rejected': -389.8513488769531, 'policy_logps/chosen': -351.8175048828125, 'referece_logps/rejected': -277.81005859375, 'referece_logps/chosen': -308.4552001953125, 'logits/rejected': -0.036033205687999725, 'logits/chosen': 0.0959421694278717, 'epoch': 3.7}

 62%|██████▏   | 6616/10740 [32:54:36<19:36:01, 17.11s/it]

 62%|██████▏   | 6617/10740 [32:54:52<19:18:52, 16.86s/it]

 62%|██████▏   | 6618/10740 [32:55:06<18:13:34, 15.92s/it]


 62%|██████▏   | 6620/10740 [32:55:39<18:40:27, 16.32s/it]
{'loss': 0.2116, 'learning_rate': 6.77590927258976e-07, 'rewards/chosen': -3.4330668449401855, 'rewards/rejected': -8.925463676452637, 'rewards/accuracies': 1.0, 'rewards/margins': 5.492396354675293, 'policy_logps/rejected': -329.4955139160156, 'policy_logps/chosen': -318.99798583984375, 'referece_logps/rejected': -240.2408447265625, 'referece_logps/chosen': -284.66729736328125, 'logits/rejected': 0.1378498673439026, 'logits/chosen': 0.12365618348121643, 'epoch': 3.7}

 62%|██████▏   | 6621/10740 [32:55:58<19:42:32, 17.23s/it]


 62%|██████▏   | 6623/10740 [32:56:41<22:06:41, 19.33s/it]
{'loss': 0.2293, 'learning_rate': 6.767346228402377e-07, 'rewards/chosen': -3.880565881729126, 'rewards/rejected': -7.72138786315918, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8408217430114746, 'policy_logps/rejected': -293.92901611328125, 'policy_logps/chosen': -239.615234375, 'referece_logps/rejected': -216.71511840820312, 'referece_logps/chosen': -200.80958557128906, 'logits/rejected': 0.0973740741610527, 'logits/chosen': 0.09010153263807297, 'epoch': 3.7}


 62%|██████▏   | 6625/10740 [32:57:19<22:14:51, 19.46s/it]
{'loss': 0.1836, 'learning_rate': 6.761639001933713e-07, 'rewards/chosen': -3.7219817638397217, 'rewards/rejected': -6.234472274780273, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5124900341033936, 'policy_logps/rejected': -470.89923095703125, 'policy_logps/chosen': -336.412841796875, 'referece_logps/rejected': -408.55450439453125, 'referece_logps/chosen': -299.1930236816406, 'logits/rejected': 0.19221888482570648, 'logits/chosen': 0.4957205355167389, 'epoch': 3.7}

 62%|██████▏   | 6626/10740 [32:57:38<22:06:49, 19.35s/it]

 62%|██████▏   | 6627/10740 [32:57:58<22:11:45, 19.43s/it]

 62%|██████▏   | 6628/10740 [32:58:14<21:02:21, 18.42s/it]

 62%|██████▏   | 6629/10740 [32:58:36<22:12:43, 19.45s/it]

 62%|██████▏   | 6630/10740 [32:58:52<21:03:51, 18.45s/it]

 62%|██████▏   | 6631/10740 [32:59:09<20:34:40, 18.03s/it]

 62%|██████▏   | 6632/10740 [32:59:29<21:11:48, 18.58s/it]

 62%|██████▏   | 6633/10740 [32:59:48<21:21:16, 18.72s/it]

 62%|██████▏   | 6634/10740 [33:00:03<20:06:44, 17.63s/it]

 62%|██████▏   | 6635/10740 [33:00:20<19:59:20, 17.53s/it]


 62%|██████▏   | 6637/10740 [33:01:01<21:52:19, 19.19s/it]
{'loss': 0.2611, 'learning_rate': 6.727420456860658e-07, 'rewards/chosen': -3.1811890602111816, 'rewards/rejected': -7.572854042053223, 'rewards/accuracies': 0.875, 'rewards/margins': 4.391665458679199, 'policy_logps/rejected': -268.0415954589844, 'policy_logps/chosen': -338.0858459472656, 'referece_logps/rejected': -192.31307983398438, 'referece_logps/chosen': -306.2739562988281, 'logits/rejected': -0.0064280033111572266, 'logits/chosen': -0.35350796580314636, 'epoch': 3.71}

 62%|██████▏   | 6638/10740 [33:01:16<20:30:22, 18.00s/it]


 62%|██████▏   | 6640/10740 [33:01:54<20:46:38, 18.24s/it]

 62%|██████▏   | 6641/10740 [33:02:10<20:03:01, 17.61s/it]

 62%|██████▏   | 6642/10740 [33:02:30<20:50:25, 18.31s/it]

 62%|██████▏   | 6643/10740 [33:02:45<19:57:41, 17.54s/it]

 62%|██████▏   | 6644/10740 [33:03:01<19:26:54, 17.09s/it]
{'loss': 0.2422, 'learning_rate': 6.707479403825442e-07, 'rewards/chosen': -3.536579132080078, 'rewards/rejected': -5.335111141204834, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7985315322875977, 'policy_logps/rejected': -375.6485900878906, 'policy_logps/chosen': -303.97918701171875, 'referece_logps/rejected': -322.2974853515625, 'referece_logps/chosen': -268.6134033203125, 'logits/rejected': -0.11853346973657608, 'logits/chosen': -0.23625557124614716, 'epoch': 3.71}

 62%|██████▏   | 6645/10740 [33:03:20<19:52:58, 17.48s/it]

 62%|██████▏   | 6646/10740 [33:03:38<20:08:39, 17.71s/it]

 62%|██████▏   | 6647/10740 [33:03:58<20:53:00, 18.37s/it]

 62%|██████▏   | 6648/10740 [33:04:20<22:13:51, 19.56s/it]


 62%|██████▏   | 6650/10740 [33:04:53<20:40:30, 18.20s/it]
{'loss': 0.1371, 'learning_rate': 6.690398748331738e-07, 'rewards/chosen': -3.2940986156463623, 'rewards/rejected': -8.364449501037598, 'rewards/accuracies': 1.0, 'rewards/margins': 5.0703511238098145, 'policy_logps/rejected': -726.4174194335938, 'policy_logps/chosen': -455.91241455078125, 'referece_logps/rejected': -642.7728881835938, 'referece_logps/chosen': -422.971435546875, 'logits/rejected': -0.07829459756612778, 'logits/chosen': 0.13737191259860992, 'epoch': 3.72}


 62%|██████▏   | 6652/10740 [33:05:20<17:54:39, 15.77s/it]
{'loss': 0.1772, 'learning_rate': 6.684707601869861e-07, 'rewards/chosen': -3.1713151931762695, 'rewards/rejected': -7.504119396209717, 'rewards/accuracies': 1.0, 'rewards/margins': 4.332803726196289, 'policy_logps/rejected': -381.475341796875, 'policy_logps/chosen': -356.548828125, 'referece_logps/rejected': -306.4341735839844, 'referece_logps/chosen': -324.835693359375, 'logits/rejected': 0.14209391176700592, 'logits/chosen': 0.08814779669046402, 'epoch': 3.72}

 62%|██████▏   | 6653/10740 [33:05:30<16:14:51, 14.31s/it]

 62%|██████▏   | 6654/10740 [33:05:48<17:23:29, 15.32s/it]

 62%|██████▏   | 6655/10740 [33:06:03<17:11:25, 15.15s/it]


 62%|██████▏   | 6657/10740 [33:06:33<16:57:14, 14.95s/it]
{'loss': 0.2327, 'learning_rate': 6.670485017087726e-07, 'rewards/chosen': -3.9725027084350586, 'rewards/rejected': -8.076559066772461, 'rewards/accuracies': 0.875, 'rewards/margins': 4.104056358337402, 'policy_logps/rejected': -437.587646484375, 'policy_logps/chosen': -427.7899475097656, 'referece_logps/rejected': -356.8220520019531, 'referece_logps/chosen': -388.0649108886719, 'logits/rejected': -0.8049936890602112, 'logits/chosen': -0.46192026138305664, 'epoch': 3.72}

 62%|██████▏   | 6658/10740 [33:06:50<17:30:30, 15.44s/it]

 62%|██████▏   | 6659/10740 [33:07:07<18:03:18, 15.93s/it]

 62%|██████▏   | 6660/10740 [33:07:27<19:17:22, 17.02s/it]


 62%|██████▏   | 6662/10740 [33:08:00<18:51:22, 16.65s/it]
{'loss': 0.2191, 'learning_rate': 6.656270002993039e-07, 'rewards/chosen': -3.8871583938598633, 'rewards/rejected': -7.742094993591309, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8549375534057617, 'policy_logps/rejected': -429.6614074707031, 'policy_logps/chosen': -637.5845947265625, 'referece_logps/rejected': -352.2404479980469, 'referece_logps/chosen': -598.7130126953125, 'logits/rejected': -0.04646708071231842, 'logits/chosen': -0.14261497557163239, 'epoch': 3.72}


 62%|██████▏   | 6664/10740 [33:08:36<19:46:04, 17.46s/it]
{'loss': 0.2537, 'learning_rate': 6.650586124388413e-07, 'rewards/chosen': -4.24073600769043, 'rewards/rejected': -7.51991605758667, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2791788578033447, 'policy_logps/rejected': -423.9341125488281, 'policy_logps/chosen': -331.9245300292969, 'referece_logps/rejected': -348.7349548339844, 'referece_logps/chosen': -289.51715087890625, 'logits/rejected': -0.07078510522842407, 'logits/chosen': 0.017082272097468376, 'epoch': 3.72}

 62%|██████▏   | 6665/10740 [33:08:56<20:36:00, 18.20s/it]

 62%|██████▏   | 6666/10740 [33:09:16<21:18:31, 18.83s/it]

 62%|██████▏   | 6667/10740 [33:09:29<19:14:33, 17.01s/it]

 62%|██████▏   | 6668/10740 [33:09:45<18:47:46, 16.62s/it]

 62%|██████▏   | 6669/10740 [33:10:05<19:58:39, 17.67s/it]

 62%|██████▏   | 6670/10740 [33:10:27<21:32:28, 19.05s/it]

 62%|██████▏   | 6671/10740 [33:10:48<22:07:29, 19.57s/it]

 62%|██████▏   | 6672/10740 [33:11:05<21:19:59, 18.88s/it]

 62%|██████▏   | 6673/10740 [33:11:25<21:34:04, 19.09s/it]

 62%|██████▏   | 6674/10740 [33:11:45<22:00:47, 19.49s/it]

 62%|██████▏   | 6675/10740 [33:12:07<22:51:53, 20.25s/it]

 62%|██████▏   | 6676/10740 [33:12:24<21:30:40, 19.06s/it]

 62%|██████▏   | 6677/10740 [33:12:43<21:43:16, 19.25s/it]

 62%|██████▏   | 6678/10740 [33:13:02<21:36:51, 19.16s/it]

 62%|██████▏   | 6679/10740 [33:13:19<20:41:39, 18.35s/it]

 62%|██████▏   | 6680/10740 [33:13:32<18:50:44, 16.71s/it]

 62%|██████▏   | 6681/10740 [33:13:42<16:48:51, 14.91s/it]

 62%|██████▏   | 6682/10740 [33:13:55<16:12:17, 14.38s/it]

 62%|██████▏   | 6683/10740 [33:14:13<17:14:06, 15.29s/it]

 62%|██████▏   | 6684/10740 [33:14:25<16:18:29, 14.47s/it]

 62%|██████▏   | 6685/10740 [33:14:46<18:16:24, 16.22s/it]

 62%|██████▏   | 6686/10740 [33:15:04<18:53:50, 16.78s/it]

 62%|██████▏   | 6687/10740 [33:15:24<20:05:47, 17.85s/it]

 62%|██████▏   | 6688/10740 [33:15:39<19:08:13, 17.00s/it]

 62%|██████▏   | 6689/10740 [33:15:59<20:03:30, 17.83s/it]

 62%|██████▏   | 6690/10740 [33:16:13<18:49:41, 16.74s/it]

 62%|██████▏   | 6691/10740 [33:16:34<20:10:42, 17.94s/it]

 62%|██████▏   | 6692/10740 [33:16:53<20:28:36, 18.21s/it]

 62%|██████▏   | 6693/10740 [33:17:12<20:45:26, 18.46s/it]


 62%|██████▏   | 6695/10740 [33:17:43<19:09:33, 17.05s/it]
{'loss': 0.2031, 'learning_rate': 6.562643104903209e-07, 'rewards/chosen': -3.833254337310791, 'rewards/rejected': -7.80490255355835, 'rewards/accuracies': 1.0, 'rewards/margins': 3.971649169921875, 'policy_logps/rejected': -397.888916015625, 'policy_logps/chosen': -352.6767272949219, 'referece_logps/rejected': -319.83990478515625, 'referece_logps/chosen': -314.34417724609375, 'logits/rejected': 0.029558837413787842, 'logits/chosen': -0.1421949565410614, 'epoch': 3.74}

 62%|██████▏   | 6696/10740 [33:18:01<19:31:26, 17.38s/it]

 62%|██████▏   | 6697/10740 [33:18:16<18:44:38, 16.69s/it]

 62%|██████▏   | 6698/10740 [33:18:32<18:24:40, 16.40s/it]

 62%|██████▏   | 6699/10740 [33:18:44<17:06:04, 15.23s/it]

 62%|██████▏   | 6700/10740 [33:19:01<17:45:23, 15.82s/it]

 62%|██████▏   | 6701/10740 [33:19:13<16:29:42, 14.70s/it]

 62%|██████▏   | 6702/10740 [33:19:25<15:24:09, 13.73s/it]

 62%|██████▏   | 6703/10740 [33:19:42<16:38:37, 14.84s/it]

 62%|██████▏   | 6704/10740 [33:19:59<17:13:10, 15.36s/it]

 62%|██████▏   | 6705/10740 [33:20:20<19:02:29, 16.99s/it]

 62%|██████▏   | 6706/10740 [33:20:35<18:36:25, 16.61s/it]

 62%|██████▏   | 6707/10740 [33:20:49<17:39:54, 15.77s/it]

 62%|██████▏   | 6708/10740 [33:21:09<19:03:51, 17.02s/it]

 62%|██████▏   | 6709/10740 [33:21:26<19:01:08, 16.99s/it]

 62%|██████▏   | 6710/10740 [33:21:42<18:44:10, 16.74s/it]

 62%|██████▏   | 6711/10740 [33:21:59<18:41:38, 16.70s/it]

 62%|██████▏   | 6712/10740 [33:22:19<19:41:23, 17.60s/it]

 63%|██████▎   | 6713/10740 [33:22:36<19:32:27, 17.47s/it]

 63%|██████▎   | 6714/10740 [33:22:56<20:29:40, 18.33s/it]

 63%|██████▎   | 6715/10740 [33:23:17<21:13:39, 18.99s/it]

 63%|██████▎   | 6716/10740 [33:23:30<19:26:52, 17.40s/it]

 63%|██████▎   | 6717/10740 [33:23:50<20:15:00, 18.12s/it]

 63%|██████▎   | 6718/10740 [33:24:05<19:04:33, 17.07s/it]

 63%|██████▎   | 6719/10740 [33:24:22<19:04:56, 17.08s/it]

 63%|██████▎   | 6720/10740 [33:24:38<18:50:13, 16.87s/it]

 63%|██████▎   | 6721/10740 [33:24:58<19:47:01, 17.72s/it]

 63%|██████▎   | 6722/10740 [33:25:15<19:36:58, 17.58s/it]

 63%|██████▎   | 6723/10740 [33:25:34<20:06:27, 18.02s/it]

 63%|██████▎   | 6724/10740 [33:25:55<21:10:47, 18.99s/it]

 63%|██████▎   | 6725/10740 [33:26:07<18:40:14, 16.74s/it]

 63%|██████▎   | 6726/10740 [33:26:26<19:18:27, 17.32s/it]

 63%|██████▎   | 6727/10740 [33:26:44<19:38:07, 17.61s/it]

 63%|██████▎   | 6728/10740 [33:27:06<21:02:17, 18.88s/it]

 63%|██████▎   | 6729/10740 [33:27:22<20:13:57, 18.16s/it]

 63%|██████▎   | 6730/10740 [33:27:42<20:46:53, 18.66s/it]

 63%|██████▎   | 6731/10740 [33:28:02<21:13:38, 19.06s/it]

 63%|██████▎   | 6732/10740 [33:28:19<20:22:48, 18.31s/it]

 63%|██████▎   | 6733/10740 [33:28:39<21:00:37, 18.88s/it]

 63%|██████▎   | 6734/10740 [33:28:58<21:05:33, 18.95s/it]

 63%|██████▎   | 6735/10740 [33:29:18<21:37:05, 19.43s/it]

 63%|██████▎   | 6736/10740 [33:29:36<20:59:29, 18.87s/it]

 63%|██████▎   | 6737/10740 [33:29:52<19:58:46, 17.97s/it]

 63%|██████▎   | 6738/10740 [33:30:12<20:35:06, 18.52s/it]

 63%|██████▎   | 6739/10740 [33:30:31<20:57:47, 18.86s/it]

 63%|██████▎   | 6740/10740 [33:30:50<20:50:57, 18.76s/it]

 63%|██████▎   | 6741/10740 [33:31:08<20:47:28, 18.72s/it]

 63%|██████▎   | 6742/10740 [33:31:28<21:06:23, 19.01s/it]

 63%|██████▎   | 6743/10740 [33:31:45<20:21:00, 18.33s/it]


 63%|██████▎   | 6745/10740 [33:32:22<20:38:59, 18.61s/it]
{'loss': 0.141, 'learning_rate': 6.421435907359034e-07, 'rewards/chosen': -2.7251393795013428, 'rewards/rejected': -6.707085132598877, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9819459915161133, 'policy_logps/rejected': -220.8292236328125, 'policy_logps/chosen': -274.75201416015625, 'referece_logps/rejected': -153.75836181640625, 'referece_logps/chosen': -247.50057983398438, 'logits/rejected': -0.03752349317073822, 'logits/chosen': -0.09260867536067963, 'epoch': 3.77}

 63%|██████▎   | 6746/10740 [33:32:42<21:15:58, 19.17s/it]

 63%|██████▎   | 6747/10740 [33:33:01<21:20:02, 19.23s/it]

 63%|██████▎   | 6748/10740 [33:33:21<21:27:43, 19.35s/it]

 63%|██████▎   | 6749/10740 [33:33:41<21:34:40, 19.46s/it]

 63%|██████▎   | 6750/10740 [33:33:59<21:14:33, 19.17s/it]

 63%|██████▎   | 6751/10740 [33:34:17<20:39:29, 18.64s/it]

 63%|██████▎   | 6752/10740 [33:34:37<21:04:04, 19.02s/it]

 63%|██████▎   | 6753/10740 [33:34:59<22:07:35, 19.98s/it]

 63%|██████▎   | 6754/10740 [33:35:21<22:43:54, 20.53s/it]

 63%|██████▎   | 6755/10740 [33:35:40<22:25:50, 20.26s/it]


 63%|██████▎   | 6757/10740 [33:36:15<21:02:19, 19.02s/it]

 63%|██████▎   | 6758/10740 [33:36:35<21:17:35, 19.25s/it]

 63%|██████▎   | 6759/10740 [33:36:47<19:03:11, 17.23s/it]

 63%|██████▎   | 6760/10740 [33:37:07<19:49:48, 17.94s/it]

 63%|██████▎   | 6761/10740 [33:37:26<20:07:33, 18.21s/it]

 63%|██████▎   | 6762/10740 [33:37:42<19:17:51, 17.46s/it]

 63%|██████▎   | 6763/10740 [33:38:03<20:41:45, 18.73s/it]

 63%|██████▎   | 6764/10740 [33:38:26<21:50:45, 19.78s/it]

 63%|██████▎   | 6765/10740 [33:38:46<21:56:28, 19.87s/it]

 63%|██████▎   | 6766/10740 [33:39:07<22:27:07, 20.34s/it]
{'loss': 0.2335, 'learning_rate': 6.362369679849681e-07, 'rewards/chosen': -3.586306095123291, 'rewards/rejected': -6.856951713562012, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2706453800201416, 'policy_logps/rejected': -332.47235107421875, 'policy_logps/chosen': -430.6492004394531, 'referece_logps/rejected': -263.9028015136719, 'referece_logps/chosen': -394.7861633300781, 'logits/rejected': 0.7591094374656677, 'logits/chosen': 0.6570560932159424, 'epoch': 3.78}


 63%|██████▎   | 6768/10740 [33:39:44<21:14:48, 19.26s/it]

 63%|██████▎   | 6769/10740 [33:39:56<18:56:49, 17.18s/it]

 63%|██████▎   | 6770/10740 [33:40:10<17:44:20, 16.09s/it]

 63%|██████▎   | 6771/10740 [33:40:29<18:49:26, 17.07s/it]

 63%|██████▎   | 6772/10740 [33:40:49<19:41:21, 17.86s/it]
{'loss': 0.1699, 'learning_rate': 6.345520310259633e-07, 'rewards/chosen': -3.7891690731048584, 'rewards/rejected': -8.32406234741211, 'rewards/accuracies': 0.875, 'rewards/margins': 4.534893989562988, 'policy_logps/rejected': -615.2468872070312, 'policy_logps/chosen': -577.7327880859375, 'referece_logps/rejected': -532.0062866210938, 'referece_logps/chosen': -539.841064453125, 'logits/rejected': 0.2961649000644684, 'logits/chosen': 0.2711530029773712, 'epoch': 3.78}


 63%|██████▎   | 6774/10740 [33:41:23<19:11:50, 17.43s/it]

 63%|██████▎   | 6775/10740 [33:41:43<20:07:54, 18.28s/it]

 63%|██████▎   | 6776/10740 [33:42:01<20:02:46, 18.21s/it]
{'loss': 0.1695, 'learning_rate': 6.334294041471717e-07, 'rewards/chosen': -2.9082210063934326, 'rewards/rejected': -6.912270545959473, 'rewards/accuracies': 1.0, 'rewards/margins': 4.004049777984619, 'policy_logps/rejected': -331.28076171875, 'policy_logps/chosen': -307.8505859375, 'referece_logps/rejected': -262.1580810546875, 'referece_logps/chosen': -278.76837158203125, 'logits/rejected': -0.4079657793045044, 'logits/chosen': -0.46242988109588623, 'epoch': 3.79}


 63%|██████▎   | 6778/10740 [33:42:37<20:06:38, 18.27s/it]

 63%|██████▎   | 6779/10740 [33:42:55<20:10:52, 18.34s/it]
{'loss': 0.3441, 'learning_rate': 6.325877839739189e-07, 'rewards/chosen': -3.443315029144287, 'rewards/rejected': -9.390510559082031, 'rewards/accuracies': 0.875, 'rewards/margins': 5.947195529937744, 'policy_logps/rejected': -460.84295654296875, 'policy_logps/chosen': -293.0981750488281, 'referece_logps/rejected': -366.9378356933594, 'referece_logps/chosen': -258.66497802734375, 'logits/rejected': -0.14921128749847412, 'logits/chosen': 0.01708526723086834, 'epoch': 3.79}


 63%|██████▎   | 6781/10740 [33:43:38<21:48:49, 19.84s/it]
{'loss': 0.1717, 'learning_rate': 6.320268709011011e-07, 'rewards/chosen': -4.389233112335205, 'rewards/rejected': -8.88602352142334, 'rewards/accuracies': 1.0, 'rewards/margins': 4.496790885925293, 'policy_logps/rejected': -413.06005859375, 'policy_logps/chosen': -377.0220947265625, 'referece_logps/rejected': -324.1998291015625, 'referece_logps/chosen': -333.1297607421875, 'logits/rejected': -0.09854714572429657, 'logits/chosen': -0.1749178171157837, 'epoch': 3.79}


 63%|██████▎   | 6783/10740 [33:44:13<20:36:23, 18.75s/it]

 63%|██████▎   | 6784/10740 [33:44:30<20:01:24, 18.22s/it]
{'loss': 0.3335, 'learning_rate': 6.311857523660651e-07, 'rewards/chosen': -4.676842212677002, 'rewards/rejected': -7.269527435302734, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5926852226257324, 'policy_logps/rejected': -322.1085510253906, 'policy_logps/chosen': -441.0523376464844, 'referece_logps/rejected': -249.4132537841797, 'referece_logps/chosen': -394.283935546875, 'logits/rejected': 0.12457133084535599, 'logits/chosen': -0.07706950604915619, 'epoch': 3.79}


 63%|██████▎   | 6786/10740 [33:45:08<20:37:16, 18.77s/it]

 63%|██████▎   | 6787/10740 [33:45:26<20:18:31, 18.50s/it]

 63%|██████▎   | 6788/10740 [33:45:42<19:38:55, 17.90s/it]

 63%|██████▎   | 6789/10740 [33:45:58<18:52:35, 17.20s/it]

 63%|██████▎   | 6790/10740 [33:46:15<18:57:50, 17.28s/it]
{'loss': 0.1673, 'learning_rate': 6.295044216873351e-07, 'rewards/chosen': -3.952846050262451, 'rewards/rejected': -6.366526126861572, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4136805534362793, 'policy_logps/rejected': -473.4493103027344, 'policy_logps/chosen': -414.6426696777344, 'referece_logps/rejected': -409.7840576171875, 'referece_logps/chosen': -375.11419677734375, 'logits/rejected': -0.4502566158771515, 'logits/chosen': -0.2882630228996277, 'epoch': 3.79}


 63%|██████▎   | 6792/10740 [33:46:48<18:33:32, 16.92s/it]
{'loss': 0.1453, 'learning_rate': 6.289442474357929e-07, 'rewards/chosen': -4.471350193023682, 'rewards/rejected': -9.887152671813965, 'rewards/accuracies': 1.0, 'rewards/margins': 5.415802955627441, 'policy_logps/rejected': -469.5492248535156, 'policy_logps/chosen': -471.6685791015625, 'referece_logps/rejected': -370.6777038574219, 'referece_logps/chosen': -426.9551086425781, 'logits/rejected': -0.11648039519786835, 'logits/chosen': -0.14458100497722626, 'epoch': 3.79}


 63%|██████▎   | 6794/10740 [33:47:22<18:30:02, 16.88s/it]
{'loss': 0.2021, 'learning_rate': 6.283842081779706e-07, 'rewards/chosen': -2.2153401374816895, 'rewards/rejected': -6.728715419769287, 'rewards/accuracies': 1.0, 'rewards/margins': 4.513375759124756, 'policy_logps/rejected': -272.7920227050781, 'policy_logps/chosen': -377.61083984375, 'referece_logps/rejected': -205.50485229492188, 'referece_logps/chosen': -355.45745849609375, 'logits/rejected': 0.033143300563097, 'logits/chosen': 0.04785536229610443, 'epoch': 3.8}


 63%|██████▎   | 6796/10740 [33:47:51<17:01:14, 15.54s/it]
{'loss': 0.1061, 'learning_rate': 6.278243041176162e-07, 'rewards/chosen': -4.859023094177246, 'rewards/rejected': -9.26759147644043, 'rewards/accuracies': 0.875, 'rewards/margins': 4.408568382263184, 'policy_logps/rejected': -322.4233703613281, 'policy_logps/chosen': -431.4908752441406, 'referece_logps/rejected': -229.7474365234375, 'referece_logps/chosen': -382.900634765625, 'logits/rejected': 0.06768439710140228, 'logits/chosen': -0.11047078669071198, 'epoch': 3.8}


 63%|██████▎   | 6798/10740 [33:48:24<17:05:34, 15.61s/it]

 63%|██████▎   | 6799/10740 [33:48:39<17:04:06, 15.59s/it]

 63%|██████▎   | 6800/10740 [33:48:55<16:57:18, 15.49s/it]

 63%|██████▎   | 6801/10740 [33:49:15<18:26:58, 16.86s/it]

 63%|██████▎   | 6802/10740 [33:49:29<17:29:42, 15.99s/it]

 63%|██████▎   | 6803/10740 [33:49:49<18:46:51, 17.17s/it]

 63%|██████▎   | 6804/10740 [33:50:09<19:46:48, 18.09s/it]

 63%|██████▎   | 6805/10740 [33:50:28<20:18:44, 18.58s/it]

 63%|██████▎   | 6806/10740 [33:50:50<21:24:46, 19.59s/it]
{'loss': 0.2202, 'learning_rate': 6.250268189055772e-07, 'rewards/chosen': -3.40578556060791, 'rewards/rejected': -8.83344841003418, 'rewards/accuracies': 0.875, 'rewards/margins': 5.4276628494262695, 'policy_logps/rejected': -522.3176879882812, 'policy_logps/chosen': -529.166748046875, 'referece_logps/rejected': -433.98321533203125, 'referece_logps/chosen': -495.1089172363281, 'logits/rejected': 0.614827036857605, 'logits/chosen': 0.6239173412322998, 'epoch': 3.8}

 63%|██████▎   | 6807/10740 [33:51:09<21:08:01, 19.34s/it]


 63%|██████▎   | 6809/10740 [33:51:43<20:01:24, 18.34s/it]

 63%|██████▎   | 6810/10740 [33:52:04<21:01:48, 19.26s/it]
{'loss': 0.1632, 'learning_rate': 6.239087783286842e-07, 'rewards/chosen': -3.129821300506592, 'rewards/rejected': -5.9591498374938965, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8293282985687256, 'policy_logps/rejected': -365.011474609375, 'policy_logps/chosen': -372.6349182128906, 'referece_logps/rejected': -305.41998291015625, 'referece_logps/chosen': -341.33673095703125, 'logits/rejected': 0.16540014743804932, 'logits/chosen': 0.10833138972520828, 'epoch': 3.8}

 63%|██████▎   | 6811/10740 [33:52:19<19:31:22, 17.89s/it]


 63%|██████▎   | 6813/10740 [33:52:55<19:36:04, 17.97s/it]

 63%|██████▎   | 6814/10740 [33:53:15<20:16:29, 18.59s/it]
{'loss': 0.1585, 'learning_rate': 6.227912850544322e-07, 'rewards/chosen': -4.99615478515625, 'rewards/rejected': -6.93527364730835, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9391183853149414, 'policy_logps/rejected': -298.420654296875, 'policy_logps/chosen': -374.42816162109375, 'referece_logps/rejected': -229.06790161132812, 'referece_logps/chosen': -324.46661376953125, 'logits/rejected': -0.5462824106216431, 'logits/chosen': -0.7851673364639282, 'epoch': 3.81}


 63%|██████▎   | 6816/10740 [33:53:51<20:07:03, 18.46s/it]
{'loss': 0.1481, 'learning_rate': 6.222327441640214e-07, 'rewards/chosen': -3.3630638122558594, 'rewards/rejected': -7.164167881011963, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8011038303375244, 'policy_logps/rejected': -269.5762939453125, 'policy_logps/chosen': -337.6361999511719, 'referece_logps/rejected': -197.93463134765625, 'referece_logps/chosen': -304.00555419921875, 'logits/rejected': 0.13278208673000336, 'logits/chosen': -0.07628707587718964, 'epoch': 3.81}


 63%|██████▎   | 6818/10740 [33:54:24<19:10:38, 17.60s/it]

 63%|██████▎   | 6819/10740 [33:54:44<20:01:40, 18.39s/it]

 64%|██████▎   | 6820/10740 [33:55:04<20:28:09, 18.80s/it]

 64%|██████▎   | 6821/10740 [33:55:18<19:06:49, 17.56s/it]

 64%|██████▎   | 6822/10740 [33:55:36<19:03:16, 17.51s/it]

 64%|██████▎   | 6823/10740 [33:55:52<18:41:06, 17.17s/it]
{'loss': 0.1573, 'learning_rate': 6.20278934684663e-07, 'rewards/chosen': -4.092781066894531, 'rewards/rejected': -8.482232093811035, 'rewards/accuracies': 1.0, 'rewards/margins': 4.389451503753662, 'policy_logps/rejected': -251.65716552734375, 'policy_logps/chosen': -376.2215576171875, 'referece_logps/rejected': -166.83482360839844, 'referece_logps/chosen': -335.29376220703125, 'logits/rejected': -0.32224172353744507, 'logits/chosen': -0.13244716823101044, 'epoch': 3.81}

 64%|██████▎   | 6824/10740 [33:56:03<16:47:28, 15.44s/it]


 64%|██████▎   | 6826/10740 [33:56:37<17:08:11, 15.76s/it]

 64%|██████▎   | 6827/10740 [33:56:49<15:53:12, 14.62s/it]

 64%|██████▎   | 6828/10740 [33:57:08<17:33:28, 16.16s/it]

 64%|██████▎   | 6829/10740 [33:57:31<19:31:31, 17.97s/it]
{'loss': 0.205, 'learning_rate': 6.186055874408287e-07, 'rewards/chosen': -3.6092166900634766, 'rewards/rejected': -8.630942344665527, 'rewards/accuracies': 0.875, 'rewards/margins': 5.021726131439209, 'policy_logps/rejected': -313.12689208984375, 'policy_logps/chosen': -411.02880859375, 'referece_logps/rejected': -226.81747436523438, 'referece_logps/chosen': -374.9366455078125, 'logits/rejected': -0.27831578254699707, 'logits/chosen': -0.44587528705596924, 'epoch': 3.82}


 64%|██████▎   | 6831/10740 [33:58:09<20:30:25, 18.89s/it]

 64%|██████▎   | 6832/10740 [33:58:23<18:50:06, 17.35s/it]

 64%|██████▎   | 6833/10740 [33:58:43<19:46:15, 18.22s/it]
{'loss': 0.1433, 'learning_rate': 6.174907160485219e-07, 'rewards/chosen': -3.6322004795074463, 'rewards/rejected': -8.33186149597168, 'rewards/accuracies': 1.0, 'rewards/margins': 4.699660778045654, 'policy_logps/rejected': -458.5859375, 'policy_logps/chosen': -361.4476623535156, 'referece_logps/rejected': -375.267333984375, 'referece_logps/chosen': -325.12567138671875, 'logits/rejected': -0.3705463409423828, 'logits/chosen': -0.37663429975509644, 'epoch': 3.82}


 64%|██████▎   | 6835/10740 [33:59:13<17:33:58, 16.19s/it]

 64%|██████▎   | 6836/10740 [33:59:26<16:38:17, 15.34s/it]

 64%|██████▎   | 6837/10740 [33:59:40<16:06:13, 14.85s/it]

 64%|██████▎   | 6838/10740 [33:59:57<16:43:55, 15.44s/it]

 64%|██████▎   | 6839/10740 [34:00:10<16:00:18, 14.77s/it]

 64%|██████▎   | 6840/10740 [34:00:25<16:06:24, 14.87s/it]

 64%|██████▎   | 6841/10740 [34:00:42<16:51:50, 15.57s/it]

 64%|██████▎   | 6842/10740 [34:01:03<18:22:55, 16.98s/it]

 64%|██████▎   | 6843/10740 [34:01:25<20:08:23, 18.61s/it]

 64%|██████▎   | 6844/10740 [34:01:43<19:56:47, 18.43s/it]

 64%|██████▎   | 6845/10740 [34:02:01<19:45:10, 18.26s/it]

 64%|██████▎   | 6846/10740 [34:02:14<18:06:30, 16.74s/it]
{'loss': 0.1548, 'learning_rate': 6.138712367335235e-07, 'rewards/chosen': -3.3675150871276855, 'rewards/rejected': -5.903696537017822, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5361812114715576, 'policy_logps/rejected': -394.4251708984375, 'policy_logps/chosen': -268.880615234375, 'referece_logps/rejected': -335.38818359375, 'referece_logps/chosen': -235.2054443359375, 'logits/rejected': -0.5809354782104492, 'logits/chosen': -0.3875375986099243, 'epoch': 3.82}

 64%|██████▍   | 6847/10740 [34:02:34<19:00:06, 17.57s/it]

 64%|██████▍   | 6848/10740 [34:02:54<19:55:14, 18.43s/it]


 64%|██████▍   | 6850/10740 [34:03:33<20:33:03, 19.02s/it]

 64%|██████▍   | 6851/10740 [34:03:55<21:27:31, 19.86s/it]

 64%|██████▍   | 6852/10740 [34:04:10<19:58:46, 18.50s/it]

 64%|██████▍   | 6853/10740 [34:04:29<19:53:08, 18.42s/it]
{'loss': 0.2305, 'learning_rate': 6.119247411435142e-07, 'rewards/chosen': -4.245898246765137, 'rewards/rejected': -8.461740493774414, 'rewards/accuracies': 0.75, 'rewards/margins': 4.215843200683594, 'policy_logps/rejected': -397.37640380859375, 'policy_logps/chosen': -413.7598876953125, 'referece_logps/rejected': -312.75897216796875, 'referece_logps/chosen': -371.30084228515625, 'logits/rejected': 0.5332773327827454, 'logits/chosen': 0.5828437805175781, 'epoch': 3.83}


 64%|██████▍   | 6855/10740 [34:05:03<19:02:23, 17.64s/it]

 64%|██████▍   | 6856/10740 [34:05:23<19:58:09, 18.51s/it]

 64%|██████▍   | 6857/10740 [34:05:40<19:20:49, 17.94s/it]

 64%|██████▍   | 6858/10740 [34:05:59<19:55:04, 18.47s/it]

 64%|██████▍   | 6859/10740 [34:06:20<20:25:48, 18.95s/it]

 64%|██████▍   | 6860/10740 [34:06:33<18:35:06, 17.24s/it]
{'loss': 0.2318, 'learning_rate': 6.09979975076305e-07, 'rewards/chosen': -4.684357643127441, 'rewards/rejected': -6.835225582122803, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1508679389953613, 'policy_logps/rejected': -422.97021484375, 'policy_logps/chosen': -343.307861328125, 'referece_logps/rejected': -354.6179504394531, 'referece_logps/chosen': -296.4642639160156, 'logits/rejected': 0.4491612911224365, 'logits/chosen': 0.5189778208732605, 'epoch': 3.83}

 64%|██████▍   | 6861/10740 [34:06:54<19:56:20, 18.50s/it]

 64%|██████▍   | 6862/10740 [34:07:06<17:45:36, 16.49s/it]


 64%|██████▍   | 6864/10740 [34:07:33<16:15:01, 15.09s/it]
{'loss': 0.1899, 'learning_rate': 6.088694600369907e-07, 'rewards/chosen': -4.066542148590088, 'rewards/rejected': -6.783045291900635, 'rewards/accuracies': 0.75, 'rewards/margins': 2.716503620147705, 'policy_logps/rejected': -631.9906005859375, 'policy_logps/chosen': -416.574951171875, 'referece_logps/rejected': -564.1600952148438, 'referece_logps/chosen': -375.9095153808594, 'logits/rejected': -0.4722054600715637, 'logits/chosen': -0.2620198726654053, 'epoch': 3.83}

 64%|██████▍   | 6865/10740 [34:07:49<16:22:15, 15.21s/it]


 64%|██████▍   | 6867/10740 [34:08:28<18:44:18, 17.42s/it]

 64%|██████▍   | 6868/10740 [34:08:47<19:31:08, 18.15s/it]
{'loss': 0.2621, 'learning_rate': 6.077595141861208e-07, 'rewards/chosen': -3.6671502590179443, 'rewards/rejected': -4.662647247314453, 'rewards/accuracies': 0.75, 'rewards/margins': 0.995497465133667, 'policy_logps/rejected': -324.8075866699219, 'policy_logps/chosen': -375.0543518066406, 'referece_logps/rejected': -278.1811218261719, 'referece_logps/chosen': -338.38287353515625, 'logits/rejected': 0.35823768377304077, 'logits/chosen': 0.43925541639328003, 'epoch': 3.84}


 64%|██████▍   | 6870/10740 [34:09:21<18:20:58, 17.07s/it]
{'loss': 0.2387, 'learning_rate': 6.072047552111461e-07, 'rewards/chosen': -3.1917378902435303, 'rewards/rejected': -8.193807601928711, 'rewards/accuracies': 0.875, 'rewards/margins': 5.00206995010376, 'policy_logps/rejected': -507.0353698730469, 'policy_logps/chosen': -523.2056274414062, 'referece_logps/rejected': -425.0972900390625, 'referece_logps/chosen': -491.2882080078125, 'logits/rejected': -0.2674762010574341, 'logits/chosen': -0.19194258749485016, 'epoch': 3.84}

 64%|██████▍   | 6871/10740 [34:09:43<19:53:46, 18.51s/it]


 64%|██████▍   | 6873/10740 [34:10:15<18:33:30, 17.28s/it]
{'loss': 0.1911, 'learning_rate': 6.063728847544163e-07, 'rewards/chosen': -5.000219345092773, 'rewards/rejected': -8.660029411315918, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6598105430603027, 'policy_logps/rejected': -490.85040283203125, 'policy_logps/chosen': -424.51324462890625, 'referece_logps/rejected': -404.2501220703125, 'referece_logps/chosen': -374.51104736328125, 'logits/rejected': -0.055070772767066956, 'logits/chosen': 0.06501803547143936, 'epoch': 3.84}

 64%|██████▍   | 6874/10740 [34:10:35<19:20:12, 18.01s/it]

 64%|██████▍   | 6875/10740 [34:10:56<20:27:45, 19.06s/it]


 64%|██████▍   | 6877/10740 [34:11:38<21:30:11, 20.04s/it]
{'loss': 0.2291, 'learning_rate': 6.052642254819895e-07, 'rewards/chosen': -3.2689671516418457, 'rewards/rejected': -6.784814834594727, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5158467292785645, 'policy_logps/rejected': -404.13177490234375, 'policy_logps/chosen': -407.2099609375, 'referece_logps/rejected': -336.2836608886719, 'referece_logps/chosen': -374.520263671875, 'logits/rejected': -0.33797526359558105, 'logits/chosen': -0.43768250942230225, 'epoch': 3.84}

 64%|██████▍   | 6878/10740 [34:11:59<21:47:05, 20.31s/it]

 64%|██████▍   | 6879/10740 [34:12:11<19:05:22, 17.80s/it]

 64%|██████▍   | 6880/10740 [34:12:28<18:57:05, 17.68s/it]

 64%|██████▍   | 6881/10740 [34:12:49<19:48:32, 18.48s/it]

 64%|██████▍   | 6882/10740 [34:13:05<19:10:33, 17.89s/it]

 64%|██████▍   | 6883/10740 [34:13:26<20:16:25, 18.92s/it]

 64%|██████▍   | 6884/10740 [34:13:39<18:08:49, 16.94s/it]


 64%|██████▍   | 6886/10740 [34:14:09<17:30:49, 16.36s/it]

 64%|██████▍   | 6887/10740 [34:14:24<17:02:23, 15.92s/it]
{'loss': 0.141, 'learning_rate': 6.024950939808859e-07, 'rewards/chosen': -3.489342212677002, 'rewards/rejected': -7.657458305358887, 'rewards/accuracies': 1.0, 'rewards/margins': 4.168116092681885, 'policy_logps/rejected': -413.25714111328125, 'policy_logps/chosen': -401.21380615234375, 'referece_logps/rejected': -336.68255615234375, 'referece_logps/chosen': -366.3203430175781, 'logits/rejected': -0.4010789394378662, 'logits/chosen': -0.5272955894470215, 'epoch': 3.85}


 64%|██████▍   | 6889/10740 [34:14:53<16:05:38, 15.04s/it]
{'loss': 0.2486, 'learning_rate': 6.019417007235205e-07, 'rewards/chosen': -4.37080717086792, 'rewards/rejected': -7.372580051422119, 'rewards/accuracies': 0.875, 'rewards/margins': 3.00177264213562, 'policy_logps/rejected': -307.0980224609375, 'policy_logps/chosen': -388.6425476074219, 'referece_logps/rejected': -233.3722381591797, 'referece_logps/chosen': -344.9344482421875, 'logits/rejected': -0.1192733570933342, 'logits/chosen': -0.3254238963127136, 'epoch': 3.85}

 64%|██████▍   | 6890/10740 [34:15:09<16:14:16, 15.18s/it]


 64%|██████▍   | 6892/10740 [34:15:42<17:06:09, 16.00s/it]

 64%|██████▍   | 6893/10740 [34:16:02<18:29:26, 17.30s/it]

 64%|██████▍   | 6894/10740 [34:16:18<17:59:23, 16.84s/it]
{'loss': 0.1767, 'learning_rate': 6.005588515969957e-07, 'rewards/chosen': -3.875443458557129, 'rewards/rejected': -7.196361064910889, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3209173679351807, 'policy_logps/rejected': -316.20452880859375, 'policy_logps/chosen': -357.3717346191406, 'referece_logps/rejected': -244.24090576171875, 'referece_logps/chosen': -318.6173095703125, 'logits/rejected': 0.29651519656181335, 'logits/chosen': 0.10277216136455536, 'epoch': 3.85}


 64%|██████▍   | 6896/10740 [34:16:54<18:32:24, 17.36s/it]

 64%|██████▍   | 6897/10740 [34:17:14<19:19:43, 18.11s/it]
{'loss': 0.1548, 'learning_rate': 5.997295778816746e-07, 'rewards/chosen': -4.2291154861450195, 'rewards/rejected': -8.574601173400879, 'rewards/accuracies': 1.0, 'rewards/margins': 4.345485210418701, 'policy_logps/rejected': -380.8831787109375, 'policy_logps/chosen': -303.95208740234375, 'referece_logps/rejected': -295.1371765136719, 'referece_logps/chosen': -261.66094970703125, 'logits/rejected': -0.2945428490638733, 'logits/chosen': -0.006462700664997101, 'epoch': 3.85}


 64%|██████▍   | 6899/10740 [34:17:50<19:24:17, 18.19s/it]
{'loss': 0.2576, 'learning_rate': 5.991769107241072e-07, 'rewards/chosen': -5.092210292816162, 'rewards/rejected': -6.087602615356445, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9953917860984802, 'policy_logps/rejected': -444.71875, 'policy_logps/chosen': -454.91436767578125, 'referece_logps/rejected': -383.8427429199219, 'referece_logps/chosen': -403.9923095703125, 'logits/rejected': -0.9298622608184814, 'logits/chosen': -0.9534097909927368, 'epoch': 3.85}

 64%|██████▍   | 6900/10740 [34:18:05<18:28:01, 17.31s/it]

 64%|██████▍   | 6901/10740 [34:18:23<18:41:20, 17.53s/it]

 64%|██████▍   | 6902/10740 [34:18:35<16:54:05, 15.85s/it]


 64%|██████▍   | 6904/10740 [34:19:09<17:38:38, 16.56s/it]
{'loss': 0.1584, 'learning_rate': 5.977958812471268e-07, 'rewards/chosen': -3.1818783283233643, 'rewards/rejected': -9.177278518676758, 'rewards/accuracies': 1.0, 'rewards/margins': 5.995399475097656, 'policy_logps/rejected': -514.3912353515625, 'policy_logps/chosen': -357.5381164550781, 'referece_logps/rejected': -422.61846923828125, 'referece_logps/chosen': -325.7193603515625, 'logits/rejected': -0.13067761063575745, 'logits/chosen': -0.21712692081928253, 'epoch': 3.86}

 64%|██████▍   | 6905/10740 [34:19:30<19:04:51, 17.91s/it]


 64%|██████▍   | 6907/10740 [34:20:04<18:43:22, 17.58s/it]
{'loss': 0.1871, 'learning_rate': 5.969677023373841e-07, 'rewards/chosen': -3.174626111984253, 'rewards/rejected': -6.085947513580322, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9113214015960693, 'policy_logps/rejected': -188.93124389648438, 'policy_logps/chosen': -350.0777282714844, 'referece_logps/rejected': -128.07177734375, 'referece_logps/chosen': -318.33148193359375, 'logits/rejected': -0.2512485980987549, 'logits/chosen': -0.1810077726840973, 'epoch': 3.86}


 64%|██████▍   | 6909/10740 [34:20:41<19:05:09, 17.94s/it]

 64%|██████▍   | 6910/10740 [34:21:02<20:09:03, 18.94s/it]

 64%|██████▍   | 6911/10740 [34:21:18<19:12:17, 18.06s/it]
{'loss': 0.2077, 'learning_rate': 5.958639771030245e-07, 'rewards/chosen': -4.227813243865967, 'rewards/rejected': -9.196666717529297, 'rewards/accuracies': 1.0, 'rewards/margins': 4.968853950500488, 'policy_logps/rejected': -480.73876953125, 'policy_logps/chosen': -457.268310546875, 'referece_logps/rejected': -388.7721252441406, 'referece_logps/chosen': -414.9901428222656, 'logits/rejected': 0.4339396357536316, 'logits/chosen': 0.38101983070373535, 'epoch': 3.86}

 64%|██████▍   | 6912/10740 [34:21:33<18:26:24, 17.34s/it]

 64%|██████▍   | 6913/10740 [34:21:53<19:12:35, 18.07s/it]


 64%|██████▍   | 6915/10740 [34:22:20<16:26:06, 15.47s/it]
{'loss': 0.1405, 'learning_rate': 5.947608399831967e-07, 'rewards/chosen': -2.455472469329834, 'rewards/rejected': -5.606082439422607, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1506099700927734, 'policy_logps/rejected': -311.9163513183594, 'policy_logps/chosen': -317.198974609375, 'referece_logps/rejected': -255.8555450439453, 'referece_logps/chosen': -292.64422607421875, 'logits/rejected': 0.17445379495620728, 'logits/chosen': 0.08610689640045166, 'epoch': 3.86}

 64%|██████▍   | 6916/10740 [34:22:40<17:49:09, 16.78s/it]

 64%|██████▍   | 6917/10740 [34:22:59<18:33:29, 17.48s/it]


 64%|██████▍   | 6919/10740 [34:23:36<19:21:41, 18.24s/it]
{'loss': 0.1477, 'learning_rate': 5.936582925832282e-07, 'rewards/chosen': -4.164716720581055, 'rewards/rejected': -6.289778232574463, 'rewards/accuracies': 0.875, 'rewards/margins': 2.125061273574829, 'policy_logps/rejected': -358.6924743652344, 'policy_logps/chosen': -296.68634033203125, 'referece_logps/rejected': -295.7947082519531, 'referece_logps/chosen': -255.0391387939453, 'logits/rejected': -0.45885103940963745, 'logits/chosen': -0.35405945777893066, 'epoch': 3.87}


 64%|██████▍   | 6921/10740 [34:24:13<19:14:18, 18.14s/it]

 64%|██████▍   | 6922/10740 [34:24:31<19:07:59, 18.04s/it]
{'loss': 0.1989, 'learning_rate': 5.928317700021314e-07, 'rewards/chosen': -3.16737699508667, 'rewards/rejected': -6.699194431304932, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5318171977996826, 'policy_logps/rejected': -296.2237548828125, 'policy_logps/chosen': -354.95623779296875, 'referece_logps/rejected': -229.23182678222656, 'referece_logps/chosen': -323.2825012207031, 'logits/rejected': -0.5342899560928345, 'logits/chosen': -0.6606350541114807, 'epoch': 3.87}


 64%|██████▍   | 6924/10740 [34:25:05<18:17:40, 17.26s/it]
{'loss': 0.1891, 'learning_rate': 5.922809400710485e-07, 'rewards/chosen': -4.548752307891846, 'rewards/rejected': -7.6957292556762695, 'rewards/accuracies': 0.875, 'rewards/margins': 3.146976947784424, 'policy_logps/rejected': -424.48565673828125, 'policy_logps/chosen': -499.15716552734375, 'referece_logps/rejected': -347.5283203125, 'referece_logps/chosen': -453.6696472167969, 'logits/rejected': -0.23411810398101807, 'logits/chosen': -0.2939331531524658, 'epoch': 3.87}

 64%|██████▍   | 6925/10740 [34:25:22<18:11:34, 17.17s/it]


 64%|██████▍   | 6927/10740 [34:25:52<16:42:02, 15.77s/it]

 65%|██████▍   | 6928/10740 [34:26:06<16:07:50, 15.23s/it]
{'loss': 0.2046, 'learning_rate': 5.911797254057971e-07, 'rewards/chosen': -3.456444501876831, 'rewards/rejected': -5.5393757820129395, 'rewards/accuracies': 0.75, 'rewards/margins': 2.08293080329895, 'policy_logps/rejected': -280.0169372558594, 'policy_logps/chosen': -354.5570983886719, 'referece_logps/rejected': -224.62319946289062, 'referece_logps/chosen': -319.99267578125, 'logits/rejected': -0.2884954810142517, 'logits/chosen': -0.3636154234409332, 'epoch': 3.87}

 65%|██████▍   | 6929/10740 [34:26:27<18:03:35, 17.06s/it]

 65%|██████▍   | 6930/10740 [34:26:44<17:47:27, 16.81s/it]


 65%|██████▍   | 6932/10740 [34:27:23<19:14:01, 18.18s/it]
{'loss': 0.3539, 'learning_rate': 5.900791056717833e-07, 'rewards/chosen': -3.602508306503296, 'rewards/rejected': -6.668177604675293, 'rewards/accuracies': 0.75, 'rewards/margins': 3.0656697750091553, 'policy_logps/rejected': -311.2152099609375, 'policy_logps/chosen': -269.9291076660156, 'referece_logps/rejected': -244.53341674804688, 'referece_logps/chosen': -233.90406799316406, 'logits/rejected': -0.7141615748405457, 'logits/chosen': -0.7480593919754028, 'epoch': 3.87}


 65%|██████▍   | 6934/10740 [34:28:00<19:25:12, 18.37s/it]

 65%|██████▍   | 6935/10740 [34:28:23<20:36:02, 19.49s/it]

 65%|██████▍   | 6936/10740 [34:28:35<18:13:07, 17.24s/it]

 65%|██████▍   | 6937/10740 [34:28:55<19:06:06, 18.08s/it]

 65%|██████▍   | 6938/10740 [34:29:15<19:53:57, 18.84s/it]
{'loss': 0.1688, 'learning_rate': 5.884292950702372e-07, 'rewards/chosen': -4.007417678833008, 'rewards/rejected': -6.001451015472412, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9940333366394043, 'policy_logps/rejected': -327.8262023925781, 'policy_logps/chosen': -323.70721435546875, 'referece_logps/rejected': -267.81170654296875, 'referece_logps/chosen': -283.633056640625, 'logits/rejected': 0.5654527544975281, 'logits/chosen': 0.41168707609176636, 'epoch': 3.88}


 65%|██████▍   | 6940/10740 [34:29:49<18:55:47, 17.93s/it]
{'loss': 0.2328, 'learning_rate': 5.878796574032592e-07, 'rewards/chosen': -3.96112322807312, 'rewards/rejected': -8.006926536560059, 'rewards/accuracies': 0.875, 'rewards/margins': 4.045803070068359, 'policy_logps/rejected': -506.4659118652344, 'policy_logps/chosen': -555.0366821289062, 'referece_logps/rejected': -426.39666748046875, 'referece_logps/chosen': -515.4254760742188, 'logits/rejected': 0.5060390830039978, 'logits/chosen': 0.5788012742996216, 'epoch': 3.88}

 65%|██████▍   | 6941/10740 [34:30:04<17:57:04, 17.01s/it]

 65%|██████▍   | 6942/10740 [34:30:22<18:25:49, 17.47s/it]

 65%|██████▍   | 6943/10740 [34:30:42<19:13:59, 18.24s/it]

 65%|██████▍   | 6944/10740 [34:31:00<19:12:26, 18.22s/it]

 65%|██████▍   | 6945/10740 [34:31:20<19:40:15, 18.66s/it]

 65%|██████▍   | 6946/10740 [34:31:36<18:40:53, 17.73s/it]

 65%|██████▍   | 6947/10740 [34:31:54<19:03:05, 18.08s/it]


 65%|██████▍   | 6949/10740 [34:32:29<18:19:29, 17.40s/it]

 65%|██████▍   | 6950/10740 [34:32:43<17:12:57, 16.35s/it]

 65%|██████▍   | 6951/10740 [34:33:03<18:31:03, 17.59s/it]

 65%|██████▍   | 6952/10740 [34:33:25<19:42:27, 18.73s/it]
{'loss': 0.2177, 'learning_rate': 5.845849869981136e-07, 'rewards/chosen': -3.127607822418213, 'rewards/rejected': -7.014753818511963, 'rewards/accuracies': 0.875, 'rewards/margins': 3.887145757675171, 'policy_logps/rejected': -244.43978881835938, 'policy_logps/chosen': -338.98956298828125, 'referece_logps/rejected': -174.29225158691406, 'referece_logps/chosen': -307.71343994140625, 'logits/rejected': 0.4712183475494385, 'logits/chosen': 0.18870308995246887, 'epoch': 3.88}

 65%|██████▍   | 6953/10740 [34:33:44<19:58:21, 18.99s/it]

 65%|██████▍   | 6954/10740 [34:34:00<18:50:34, 17.92s/it]


 65%|██████▍   | 6956/10740 [34:34:41<20:20:33, 19.35s/it]

 65%|██████▍   | 6957/10740 [34:34:57<19:25:49, 18.49s/it]

 65%|██████▍   | 6958/10740 [34:35:13<18:31:07, 17.63s/it]
{'loss': 0.2928, 'learning_rate': 5.829396893819512e-07, 'rewards/chosen': -4.514358043670654, 'rewards/rejected': -8.440814018249512, 'rewards/accuracies': 0.875, 'rewards/margins': 3.926455497741699, 'policy_logps/rejected': -370.5013732910156, 'policy_logps/chosen': -496.03369140625, 'referece_logps/rejected': -286.09320068359375, 'referece_logps/chosen': -450.89013671875, 'logits/rejected': 0.5335603356361389, 'logits/chosen': 0.42777329683303833, 'epoch': 3.89}


 65%|██████▍   | 6960/10740 [34:35:53<19:56:18, 18.99s/it]
{'loss': 0.1568, 'learning_rate': 5.823915600385094e-07, 'rewards/chosen': -3.1302807331085205, 'rewards/rejected': -7.910022258758545, 'rewards/accuracies': 1.0, 'rewards/margins': 4.779742240905762, 'policy_logps/rejected': -565.599365234375, 'policy_logps/chosen': -428.3022766113281, 'referece_logps/rejected': -486.4991760253906, 'referece_logps/chosen': -396.9994812011719, 'logits/rejected': 0.5448278188705444, 'logits/chosen': 0.5005167126655579, 'epoch': 3.89}

 65%|██████▍   | 6961/10740 [34:36:12<20:02:57, 19.10s/it]


 65%|██████▍   | 6963/10740 [34:36:48<19:38:09, 18.72s/it]
{'loss': 0.0985, 'learning_rate': 5.815696509544809e-07, 'rewards/chosen': -3.811244487762451, 'rewards/rejected': -7.82574987411499, 'rewards/accuracies': 1.0, 'rewards/margins': 4.014505386352539, 'policy_logps/rejected': -404.0684814453125, 'policy_logps/chosen': -604.35595703125, 'referece_logps/rejected': -325.81097412109375, 'referece_logps/chosen': -566.2435302734375, 'logits/rejected': -0.07720097154378891, 'logits/chosen': -0.23884552717208862, 'epoch': 3.89}

 65%|██████▍   | 6964/10740 [34:37:05<19:14:07, 18.34s/it]


 65%|██████▍   | 6966/10740 [34:37:32<16:21:54, 15.61s/it]
{'loss': 0.1808, 'learning_rate': 5.807480843858293e-07, 'rewards/chosen': -4.044904708862305, 'rewards/rejected': -6.554899215698242, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5099949836730957, 'policy_logps/rejected': -312.9988098144531, 'policy_logps/chosen': -332.49688720703125, 'referece_logps/rejected': -247.4498291015625, 'referece_logps/chosen': -292.0478210449219, 'logits/rejected': 0.17586961388587952, 'logits/chosen': 0.24367132782936096, 'epoch': 3.89}

 65%|██████▍   | 6967/10740 [34:37:46<16:04:46, 15.34s/it]


 65%|██████▍   | 6969/10740 [34:38:10<14:03:58, 13.43s/it]
{'loss': 0.2063, 'learning_rate': 5.799268610050657e-07, 'rewards/chosen': -3.648341655731201, 'rewards/rejected': -6.137179851531982, 'rewards/accuracies': 0.875, 'rewards/margins': 2.488837957382202, 'policy_logps/rejected': -357.05859375, 'policy_logps/chosen': -396.9826965332031, 'referece_logps/rejected': -295.686767578125, 'referece_logps/chosen': -360.4992980957031, 'logits/rejected': -0.009301543235778809, 'logits/chosen': -0.047350987792015076, 'epoch': 3.89}

 65%|██████▍   | 6970/10740 [34:38:32<16:49:01, 16.06s/it]

 65%|██████▍   | 6971/10740 [34:38:51<17:45:22, 16.96s/it]

 65%|██████▍   | 6972/10740 [34:39:11<18:38:06, 17.80s/it]


 65%|██████▍   | 6974/10740 [34:39:48<19:10:11, 18.32s/it]
{'loss': 0.2501, 'learning_rate': 5.785589198358959e-07, 'rewards/chosen': -3.9823153018951416, 'rewards/rejected': -7.422816276550293, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4405009746551514, 'policy_logps/rejected': -348.1389465332031, 'policy_logps/chosen': -389.5329284667969, 'referece_logps/rejected': -273.9107971191406, 'referece_logps/chosen': -349.70977783203125, 'logits/rejected': -0.6430925130844116, 'logits/chosen': -0.7752326726913452, 'epoch': 3.9}

 65%|██████▍   | 6975/10740 [34:40:05<18:43:50, 17.91s/it]

 65%|██████▍   | 6976/10740 [34:40:24<19:17:42, 18.45s/it]


 65%|██████▍   | 6978/10740 [34:40:56<17:58:18, 17.20s/it]
{'loss': 0.11, 'learning_rate': 5.774652567110024e-07, 'rewards/chosen': -2.9244298934936523, 'rewards/rejected': -10.088141441345215, 'rewards/accuracies': 1.0, 'rewards/margins': 7.1637115478515625, 'policy_logps/rejected': -367.0032958984375, 'policy_logps/chosen': -358.0719909667969, 'referece_logps/rejected': -266.1219177246094, 'referece_logps/chosen': -328.8276672363281, 'logits/rejected': -0.08206085115671158, 'logits/chosen': -0.1800604909658432, 'epoch': 3.9}


 65%|██████▍   | 6980/10740 [34:41:30<17:45:22, 17.00s/it]
{'loss': 0.212, 'learning_rate': 5.769186556325205e-07, 'rewards/chosen': -2.932576894760132, 'rewards/rejected': -6.49818754196167, 'rewards/accuracies': 0.875, 'rewards/margins': 3.565609931945801, 'policy_logps/rejected': -283.67999267578125, 'policy_logps/chosen': -270.4974365234375, 'referece_logps/rejected': -218.69815063476562, 'referece_logps/chosen': -241.17166137695312, 'logits/rejected': -0.587374746799469, 'logits/chosen': -0.676985502243042, 'epoch': 3.9}

 65%|██████▌   | 6981/10740 [34:41:50<18:51:07, 18.05s/it]

 65%|██████▌   | 6982/10740 [34:42:11<19:41:49, 18.87s/it]


 65%|██████▌   | 6984/10740 [34:42:44<18:49:04, 18.04s/it]
{'loss': 0.1186, 'learning_rate': 5.758259154377778e-07, 'rewards/chosen': -3.0566327571868896, 'rewards/rejected': -8.045713424682617, 'rewards/accuracies': 1.0, 'rewards/margins': 4.989080429077148, 'policy_logps/rejected': -514.728759765625, 'policy_logps/chosen': -364.7557373046875, 'referece_logps/rejected': -434.2716369628906, 'referece_logps/chosen': -334.189453125, 'logits/rejected': -0.03814133256673813, 'logits/chosen': -0.06138673424720764, 'epoch': 3.9}


 65%|██████▌   | 6986/10740 [34:43:12<16:47:58, 16.11s/it]
{'loss': 0.2202, 'learning_rate': 5.752797767190678e-07, 'rewards/chosen': -2.9107329845428467, 'rewards/rejected': -7.322230815887451, 'rewards/accuracies': 1.0, 'rewards/margins': 4.411497592926025, 'policy_logps/rejected': -359.31683349609375, 'policy_logps/chosen': -365.5045471191406, 'referece_logps/rejected': -286.0945129394531, 'referece_logps/chosen': -336.3972473144531, 'logits/rejected': 0.13696499168872833, 'logits/chosen': -0.1824629008769989, 'epoch': 3.9}

 65%|██████▌   | 6987/10740 [34:43:31<17:37:23, 16.90s/it]

 65%|██████▌   | 6988/10740 [34:43:47<17:28:51, 16.77s/it]

 65%|██████▌   | 6989/10740 [34:44:03<17:11:53, 16.51s/it]

 65%|██████▌   | 6990/10740 [34:44:25<18:44:10, 17.99s/it]

 65%|██████▌   | 6991/10740 [34:44:47<20:02:50, 19.25s/it]

 65%|██████▌   | 6992/10740 [34:45:03<19:10:00, 18.41s/it]


 65%|██████▌   | 6994/10740 [34:45:42<19:36:11, 18.84s/it]

 65%|██████▌   | 6995/10740 [34:45:54<17:26:10, 16.76s/it]
{'loss': 0.3066, 'learning_rate': 5.728240675047259e-07, 'rewards/chosen': -3.867699146270752, 'rewards/rejected': -8.269729614257812, 'rewards/accuracies': 0.75, 'rewards/margins': 4.402029514312744, 'policy_logps/rejected': -299.6802673339844, 'policy_logps/chosen': -367.50115966796875, 'referece_logps/rejected': -216.98297119140625, 'referece_logps/chosen': -328.8241882324219, 'logits/rejected': -0.6945189237594604, 'logits/chosen': -0.7994658946990967, 'epoch': 3.91}


 65%|██████▌   | 6997/10740 [34:46:28<17:42:03, 17.02s/it]
{'loss': 0.1733, 'learning_rate': 5.722787810888933e-07, 'rewards/chosen': -3.886758804321289, 'rewards/rejected': -7.0316009521484375, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1448423862457275, 'policy_logps/rejected': -300.3954772949219, 'policy_logps/chosen': -335.509033203125, 'referece_logps/rejected': -230.07948303222656, 'referece_logps/chosen': -296.6414794921875, 'logits/rejected': 0.08538378030061722, 'logits/chosen': 0.1074865460395813, 'epoch': 3.91}


 65%|██████▌   | 6999/10740 [34:47:02<17:15:44, 16.61s/it]
{'loss': 0.3052, 'learning_rate': 5.717336502822327e-07, 'rewards/chosen': -3.3641393184661865, 'rewards/rejected': -4.786654949188232, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4225157499313354, 'policy_logps/rejected': -415.5447692871094, 'policy_logps/chosen': -408.19488525390625, 'referece_logps/rejected': -367.6782531738281, 'referece_logps/chosen': -374.553466796875, 'logits/rejected': -0.3191719055175781, 'logits/chosen': -0.3065282702445984, 'epoch': 3.91}

 65%|██████▌   | 7000/10740 [34:47:23<18:42:27, 18.01s/it]

 65%|██████▌   | 7001/10740 [34:47:52<21:52:58, 21.07s/it]


 65%|██████▌   | 7003/10740 [34:48:30<21:05:38, 20.32s/it]
{'loss': 0.1056, 'learning_rate': 5.706438562896652e-07, 'rewards/chosen': -3.8393919467926025, 'rewards/rejected': -8.418185234069824, 'rewards/accuracies': 1.0, 'rewards/margins': 4.578793525695801, 'policy_logps/rejected': -359.11749267578125, 'policy_logps/chosen': -373.9432373046875, 'referece_logps/rejected': -274.9356384277344, 'referece_logps/chosen': -335.54931640625, 'logits/rejected': -0.4250643849372864, 'logits/chosen': -0.2015380561351776, 'epoch': 3.91}

 65%|██████▌   | 7004/10740 [34:48:43<18:52:49, 18.19s/it]

 65%|██████▌   | 7005/10740 [34:48:59<18:11:12, 17.53s/it]

 65%|██████▌   | 7006/10740 [34:49:18<18:22:30, 17.72s/it]


 65%|██████▌   | 7008/10740 [34:49:58<19:51:14, 19.15s/it]
{'loss': 0.2661, 'learning_rate': 5.692824926319893e-07, 'rewards/chosen': -2.8486337661743164, 'rewards/rejected': -9.123262405395508, 'rewards/accuracies': 1.0, 'rewards/margins': 6.274628162384033, 'policy_logps/rejected': -439.1813659667969, 'policy_logps/chosen': -404.0843505859375, 'referece_logps/rejected': -347.9487609863281, 'referece_logps/chosen': -375.5979919433594, 'logits/rejected': 0.4326942563056946, 'logits/chosen': 0.47023218870162964, 'epoch': 3.92}

 65%|██████▌   | 7009/10740 [34:50:17<19:42:41, 19.02s/it]


 65%|██████▌   | 7011/10740 [34:50:48<17:31:40, 16.92s/it]

 65%|██████▌   | 7012/10740 [34:51:00<16:00:26, 15.46s/it]

 65%|██████▌   | 7013/10740 [34:51:20<17:25:20, 16.83s/it]
{'loss': 0.1992, 'learning_rate': 5.679221083444745e-07, 'rewards/chosen': -3.500946283340454, 'rewards/rejected': -7.020777225494385, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5198307037353516, 'policy_logps/rejected': -362.34918212890625, 'policy_logps/chosen': -345.6729736328125, 'referece_logps/rejected': -292.1413879394531, 'referece_logps/chosen': -310.6635437011719, 'logits/rejected': 0.8302481770515442, 'logits/chosen': 0.853119969367981, 'epoch': 3.92}

 65%|██████▌   | 7014/10740 [34:51:33<16:08:39, 15.60s/it]

 65%|██████▌   | 7015/10740 [34:51:53<17:32:58, 16.96s/it]

 65%|██████▌   | 7016/10740 [34:52:14<18:47:05, 18.16s/it]


 65%|██████▌   | 7018/10740 [34:52:55<20:03:36, 19.40s/it]
{'loss': 0.1587, 'learning_rate': 5.665627065203782e-07, 'rewards/chosen': -5.041707992553711, 'rewards/rejected': -9.19958782196045, 'rewards/accuracies': 0.875, 'rewards/margins': 4.1578803062438965, 'policy_logps/rejected': -458.9086608886719, 'policy_logps/chosen': -425.8972473144531, 'referece_logps/rejected': -366.91278076171875, 'referece_logps/chosen': -375.48016357421875, 'logits/rejected': -0.10757153481245041, 'logits/chosen': -0.23645031452178955, 'epoch': 3.92}

 65%|██████▌   | 7019/10740 [34:53:10<18:47:57, 18.19s/it]

 65%|██████▌   | 7020/10740 [34:53:31<19:38:39, 19.01s/it]

 65%|██████▌   | 7021/10740 [34:53:49<19:27:04, 18.83s/it]

 65%|██████▌   | 7022/10740 [34:54:08<19:14:20, 18.63s/it]

 65%|██████▌   | 7023/10740 [34:54:28<19:41:49, 19.08s/it]

 65%|██████▌   | 7024/10740 [34:54:49<20:29:49, 19.86s/it]

 65%|██████▌   | 7025/10740 [34:55:09<20:30:37, 19.88s/it]

 65%|██████▌   | 7026/10740 [34:55:29<20:30:34, 19.88s/it]

 65%|██████▌   | 7027/10740 [34:55:48<20:03:56, 19.45s/it]

 65%|██████▌   | 7028/10740 [34:56:08<20:19:51, 19.72s/it]

 65%|██████▌   | 7029/10740 [34:56:25<19:29:11, 18.90s/it]

 65%|██████▌   | 7030/10740 [34:56:45<19:40:31, 19.09s/it]

 65%|██████▌   | 7031/10740 [34:56:58<17:49:36, 17.30s/it]

 65%|██████▌   | 7032/10740 [34:57:16<18:10:23, 17.64s/it]

 65%|██████▌   | 7033/10740 [34:57:38<19:20:19, 18.78s/it]

 65%|██████▌   | 7034/10740 [34:57:57<19:36:44, 19.05s/it]

 66%|██████▌   | 7035/10740 [34:58:14<18:47:18, 18.26s/it]


 66%|██████▌   | 7037/10740 [34:58:53<19:17:50, 18.76s/it]
{'loss': 0.2009, 'learning_rate': 5.614059941284102e-07, 'rewards/chosen': -4.401102066040039, 'rewards/rejected': -8.306920051574707, 'rewards/accuracies': 0.875, 'rewards/margins': 3.905817747116089, 'policy_logps/rejected': -513.5100708007812, 'policy_logps/chosen': -463.52996826171875, 'referece_logps/rejected': -430.44085693359375, 'referece_logps/chosen': -419.5190124511719, 'logits/rejected': -0.042032793164253235, 'logits/chosen': -0.009308084845542908, 'epoch': 3.93}

 66%|██████▌   | 7038/10740 [34:59:14<20:08:54, 19.59s/it]


 66%|██████▌   | 7040/10740 [34:59:49<19:17:38, 18.77s/it]
{'loss': 0.2006, 'learning_rate': 5.605930884502954e-07, 'rewards/chosen': -3.36077880859375, 'rewards/rejected': -8.58297061920166, 'rewards/accuracies': 0.875, 'rewards/margins': 5.222191333770752, 'policy_logps/rejected': -328.3915710449219, 'policy_logps/chosen': -367.4461364746094, 'referece_logps/rejected': -242.56187438964844, 'referece_logps/chosen': -333.83831787109375, 'logits/rejected': -0.15561649203300476, 'logits/chosen': -0.3182779848575592, 'epoch': 3.93}

 66%|██████▌   | 7041/10740 [35:00:09<19:37:37, 19.10s/it]

 66%|██████▌   | 7042/10740 [35:00:26<19:01:20, 18.52s/it]

 66%|██████▌   | 7043/10740 [35:00:43<18:26:33, 17.96s/it]


 66%|██████▌   | 7045/10740 [35:01:15<17:17:04, 16.84s/it]
{'loss': 0.2011, 'learning_rate': 5.592390452845843e-07, 'rewards/chosen': -3.9003453254699707, 'rewards/rejected': -6.121243476867676, 'rewards/accuracies': 0.75, 'rewards/margins': 2.220897912979126, 'policy_logps/rejected': -416.78271484375, 'policy_logps/chosen': -399.3734436035156, 'referece_logps/rejected': -355.5703125, 'referece_logps/chosen': -360.3699645996094, 'logits/rejected': 0.5621873736381531, 'logits/chosen': 0.6204910278320312, 'epoch': 3.94}

 66%|██████▌   | 7046/10740 [35:01:32<17:17:14, 16.85s/it]


 66%|██████▌   | 7048/10740 [35:02:09<18:09:03, 17.70s/it]

 66%|██████▌   | 7049/10740 [35:02:21<16:18:12, 15.90s/it]
{'loss': 0.2053, 'learning_rate': 5.58156532193392e-07, 'rewards/chosen': -4.078726291656494, 'rewards/rejected': -7.719592571258545, 'rewards/accuracies': 0.875, 'rewards/margins': 3.64086651802063, 'policy_logps/rejected': -434.6240234375, 'policy_logps/chosen': -351.20953369140625, 'referece_logps/rejected': -357.42816162109375, 'referece_logps/chosen': -310.4222717285156, 'logits/rejected': -0.6684833765029907, 'logits/chosen': -0.4966140389442444, 'epoch': 3.94}


 66%|██████▌   | 7051/10740 [35:02:59<18:12:41, 17.77s/it]
{'loss': 0.1603, 'learning_rate': 5.576155166698294e-07, 'rewards/chosen': -4.654041290283203, 'rewards/rejected': -6.420854091644287, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7668131589889526, 'policy_logps/rejected': -488.3289794921875, 'policy_logps/chosen': -442.47369384765625, 'referece_logps/rejected': -424.1204528808594, 'referece_logps/chosen': -395.93328857421875, 'logits/rejected': 0.2971247434616089, 'logits/chosen': 0.3732764720916748, 'epoch': 3.94}

 66%|██████▌   | 7052/10740 [35:03:16<17:55:40, 17.50s/it]

 66%|██████▌   | 7053/10740 [35:03:36<18:44:15, 18.30s/it]

 66%|██████▌   | 7054/10740 [35:03:57<19:26:25, 18.99s/it]

 66%|██████▌   | 7055/10740 [35:04:12<18:08:45, 17.73s/it]

 66%|██████▌   | 7056/10740 [35:04:27<17:17:13, 16.89s/it]

 66%|██████▌   | 7057/10740 [35:04:46<18:06:29, 17.70s/it]


 66%|██████▌   | 7059/10740 [35:05:19<17:31:35, 17.14s/it]
{'loss': 0.1408, 'learning_rate': 5.55453065981079e-07, 'rewards/chosen': -4.158362865447998, 'rewards/rejected': -9.516622543334961, 'rewards/accuracies': 0.875, 'rewards/margins': 5.358259201049805, 'policy_logps/rejected': -418.8399963378906, 'policy_logps/chosen': -280.1832275390625, 'referece_logps/rejected': -323.6737976074219, 'referece_logps/chosen': -238.599609375, 'logits/rejected': -0.4423201084136963, 'logits/chosen': -0.34610986709594727, 'epoch': 3.94}

 66%|██████▌   | 7060/10740 [35:05:39<18:09:01, 17.76s/it]

 66%|██████▌   | 7061/10740 [35:05:57<18:11:27, 17.80s/it]

 66%|██████▌   | 7062/10740 [35:06:18<19:26:42, 19.03s/it]

 66%|██████▌   | 7063/10740 [35:06:38<19:30:29, 19.10s/it]

 66%|██████▌   | 7064/10740 [35:06:52<17:54:57, 17.55s/it]

 66%|██████▌   | 7065/10740 [35:07:09<17:43:08, 17.36s/it]

 66%|██████▌   | 7066/10740 [35:07:24<17:03:55, 16.72s/it]

 66%|██████▌   | 7067/10740 [35:07:43<17:42:54, 17.36s/it]

 66%|██████▌   | 7068/10740 [35:07:56<16:38:30, 16.32s/it]

 66%|██████▌   | 7069/10740 [35:08:14<17:07:54, 16.80s/it]

 66%|██████▌   | 7070/10740 [35:08:34<18:02:57, 17.71s/it]

 66%|██████▌   | 7071/10740 [35:08:55<18:51:03, 18.50s/it]

 66%|██████▌   | 7072/10740 [35:09:11<18:10:20, 17.84s/it]

 66%|██████▌   | 7073/10740 [35:09:32<19:11:49, 18.85s/it]

 66%|██████▌   | 7074/10740 [35:09:46<17:37:56, 17.32s/it]

 66%|██████▌   | 7075/10740 [35:10:05<18:04:27, 17.75s/it]

 66%|██████▌   | 7076/10740 [35:10:18<16:48:20, 16.51s/it]

 66%|██████▌   | 7077/10740 [35:10:38<17:55:17, 17.61s/it]

 66%|██████▌   | 7078/10740 [35:10:57<18:06:02, 17.79s/it]

 66%|██████▌   | 7079/10740 [35:11:14<17:54:29, 17.61s/it]

 66%|██████▌   | 7080/10740 [35:11:32<18:03:37, 17.76s/it]

 66%|██████▌   | 7081/10740 [35:11:48<17:28:03, 17.19s/it]

 66%|██████▌   | 7082/10740 [35:12:08<18:19:40, 18.04s/it]

 66%|██████▌   | 7083/10740 [35:12:20<16:25:51, 16.17s/it]

 66%|██████▌   | 7084/10740 [35:12:40<17:34:05, 17.30s/it]

 66%|██████▌   | 7085/10740 [35:12:59<18:19:22, 18.05s/it]

 66%|██████▌   | 7086/10740 [35:13:19<18:48:10, 18.53s/it]

 66%|██████▌   | 7087/10740 [35:13:36<18:23:07, 18.12s/it]

 66%|██████▌   | 7088/10740 [35:13:49<16:49:51, 16.59s/it]

 66%|██████▌   | 7089/10740 [35:14:07<17:14:13, 17.00s/it]

 66%|██████▌   | 7090/10740 [35:14:24<17:13:00, 16.98s/it]

 66%|██████▌   | 7091/10740 [35:14:40<16:53:30, 16.67s/it]

 66%|██████▌   | 7092/10740 [35:15:00<17:46:12, 17.54s/it]

 66%|██████▌   | 7093/10740 [35:15:18<17:59:21, 17.76s/it]

 66%|██████▌   | 7094/10740 [35:15:36<18:01:33, 17.80s/it]

 66%|██████▌   | 7095/10740 [35:15:50<16:55:05, 16.71s/it]

 66%|██████▌   | 7096/10740 [35:16:09<17:39:25, 17.44s/it]

 66%|██████▌   | 7097/10740 [35:16:21<16:04:05, 15.88s/it]

 66%|██████▌   | 7098/10740 [35:16:42<17:28:42, 17.28s/it]

 66%|██████▌   | 7099/10740 [35:17:04<18:50:15, 18.63s/it]

 66%|██████▌   | 7100/10740 [35:17:22<18:38:36, 18.44s/it]

 66%|██████▌   | 7101/10740 [35:17:40<18:35:42, 18.40s/it]

 66%|██████▌   | 7102/10740 [35:18:00<18:59:18, 18.79s/it]

 66%|██████▌   | 7103/10740 [35:18:17<18:40:59, 18.49s/it]

 66%|██████▌   | 7104/10740 [35:18:33<17:46:36, 17.60s/it]

 66%|██████▌   | 7105/10740 [35:18:45<16:08:07, 15.98s/it]

 66%|██████▌   | 7106/10740 [35:19:00<15:53:44, 15.75s/it]

 66%|██████▌   | 7107/10740 [35:19:20<17:07:03, 16.96s/it]

 66%|██████▌   | 7108/10740 [35:19:33<16:01:19, 15.88s/it]

 66%|██████▌   | 7109/10740 [35:19:53<17:13:43, 17.08s/it]

 66%|██████▌   | 7110/10740 [35:20:06<15:54:46, 15.78s/it]


 66%|██████▌   | 7112/10740 [35:20:33<14:38:29, 14.53s/it]
{'loss': 0.292, 'learning_rate': 5.411927840695604e-07, 'rewards/chosen': -4.3310723304748535, 'rewards/rejected': -6.148613929748535, 'rewards/accuracies': 0.875, 'rewards/margins': 1.817541480064392, 'policy_logps/rejected': -320.12939453125, 'policy_logps/chosen': -544.633056640625, 'referece_logps/rejected': -258.6432189941406, 'referece_logps/chosen': -501.32232666015625, 'logits/rejected': -0.41408130526542664, 'logits/chosen': -0.5129799842834473, 'epoch': 3.97}


 66%|██████▌   | 7114/10740 [35:21:04<15:03:36, 14.95s/it]

 66%|██████▌   | 7115/10740 [35:21:22<16:05:51, 15.99s/it]

 66%|██████▋   | 7116/10740 [35:21:41<16:57:49, 16.85s/it]

 66%|██████▋   | 7117/10740 [35:21:55<16:06:19, 16.00s/it]
{'loss': 0.1427, 'learning_rate': 5.398534685924946e-07, 'rewards/chosen': -4.429102420806885, 'rewards/rejected': -9.681386947631836, 'rewards/accuracies': 1.0, 'rewards/margins': 5.252285003662109, 'policy_logps/rejected': -411.76153564453125, 'policy_logps/chosen': -435.1613464355469, 'referece_logps/rejected': -314.9476623535156, 'referece_logps/chosen': -390.87030029296875, 'logits/rejected': -0.01728498935699463, 'logits/chosen': -0.19702962040901184, 'epoch': 3.98}


 66%|██████▋   | 7119/10740 [35:22:35<18:06:42, 18.01s/it]

 66%|██████▋   | 7120/10740 [35:22:56<18:58:39, 18.87s/it]

 66%|██████▋   | 7121/10740 [35:23:15<19:11:15, 19.09s/it]

 66%|██████▋   | 7122/10740 [35:23:27<16:59:10, 16.90s/it]

 66%|██████▋   | 7123/10740 [35:23:42<16:22:14, 16.29s/it]

 66%|██████▋   | 7124/10740 [35:23:57<15:52:34, 15.81s/it]

 66%|██████▋   | 7125/10740 [35:24:16<17:01:30, 16.95s/it]

 66%|██████▋   | 7126/10740 [35:24:36<17:58:53, 17.91s/it]

 66%|██████▋   | 7127/10740 [35:24:50<16:36:40, 16.55s/it]

 66%|██████▋   | 7128/10740 [35:25:02<15:20:31, 15.29s/it]

 66%|██████▋   | 7129/10740 [35:25:18<15:23:12, 15.34s/it]

 66%|██████▋   | 7130/10740 [35:25:32<15:11:19, 15.15s/it]

 66%|██████▋   | 7131/10740 [35:25:52<16:36:32, 16.57s/it]

 66%|██████▋   | 7132/10740 [35:26:14<18:08:53, 18.11s/it]

 66%|██████▋   | 7133/10740 [35:26:34<18:50:29, 18.80s/it]
{'loss': 0.1172, 'learning_rate': 5.355747051013635e-07, 'rewards/chosen': -3.6118953227996826, 'rewards/rejected': -7.050732135772705, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4388368129730225, 'policy_logps/rejected': -266.88616943359375, 'policy_logps/chosen': -390.9715270996094, 'referece_logps/rejected': -196.3788299560547, 'referece_logps/chosen': -354.8525695800781, 'logits/rejected': 0.012127101421356201, 'logits/chosen': -0.07337015867233276, 'epoch': 3.98}


 66%|██████▋   | 7135/10740 [35:27:10<18:35:36, 18.57s/it]

 66%|██████▋   | 7136/10740 [35:27:30<18:56:03, 18.91s/it]

 66%|██████▋   | 7137/10740 [35:27:49<19:11:45, 19.18s/it]

 66%|██████▋   | 7138/10740 [35:28:09<19:22:48, 19.37s/it]

 66%|██████▋   | 7139/10740 [35:28:28<19:20:01, 19.33s/it]

 66%|██████▋   | 7140/10740 [35:28:46<18:43:20, 18.72s/it]

 66%|██████▋   | 7141/10740 [35:29:01<17:31:06, 17.52s/it]

 66%|██████▋   | 7142/10740 [35:29:15<16:27:17, 16.46s/it]

 67%|██████▋   | 7143/10740 [35:29:35<17:31:20, 17.54s/it]

 67%|██████▋   | 7144/10740 [35:29:52<17:24:33, 17.43s/it]

 67%|██████▋   | 7145/10740 [35:30:14<18:45:01, 18.78s/it]

 67%|██████▋   | 7146/10740 [35:30:32<18:29:11, 18.52s/it]

 67%|██████▋   | 7147/10740 [35:30:52<19:01:42, 19.07s/it]

 67%|██████▋   | 7148/10740 [35:31:13<19:36:26, 19.65s/it]

 67%|██████▋   | 7149/10740 [35:31:33<19:36:47, 19.66s/it]

 67%|██████▋   | 7150/10740 [35:31:49<18:33:07, 18.60s/it]

 67%|██████▋   | 7151/10740 [35:32:07<18:27:23, 18.51s/it]

 67%|██████▋   | 7152/10740 [35:32:27<18:48:07, 18.86s/it]

 67%|██████▋   | 7153/10740 [35:32:45<18:33:42, 18.63s/it]

 67%|██████▋   | 7154/10740 [35:33:04<18:39:19, 18.73s/it]

 67%|██████▋   | 7155/10740 [35:33:22<18:38:16, 18.72s/it]

 67%|██████▋   | 7156/10740 [35:33:42<18:54:13, 18.99s/it]
{'loss': 0.1761, 'learning_rate': 5.294429528625949e-07, 'rewards/chosen': -4.9786553382873535, 'rewards/rejected': -8.212325096130371, 'rewards/accuracies': 1.0, 'rewards/margins': 3.233670473098755, 'policy_logps/rejected': -419.437255859375, 'policy_logps/chosen': -393.14013671875, 'referece_logps/rejected': -337.31402587890625, 'referece_logps/chosen': -343.35357666015625, 'logits/rejected': -0.22264373302459717, 'logits/chosen': -0.43577829003334045, 'epoch': 4.0}


 67%|██████▋   | 7158/10740 [35:34:18<18:01:29, 18.12s/it]
{'loss': 0.1631, 'learning_rate': 5.289108227341969e-07, 'rewards/chosen': -4.146560192108154, 'rewards/rejected': -8.38178539276123, 'rewards/accuracies': 1.0, 'rewards/margins': 4.235225200653076, 'policy_logps/rejected': -387.8431396484375, 'policy_logps/chosen': -399.1728210449219, 'referece_logps/rejected': -304.0252990722656, 'referece_logps/chosen': -357.70721435546875, 'logits/rejected': -0.3571174144744873, 'logits/chosen': -0.5019984245300293, 'epoch': 4.0}


 67%|██████▋   | 7160/10740 [35:34:56<18:13:41, 18.33s/it]

 67%|██████▋   | 7161/10740 [35:35:12<17:41:58, 17.80s/it]

 67%|██████▋   | 7162/10740 [35:35:35<19:04:52, 19.20s/it]

 67%|██████▋   | 7163/10740 [35:35:46<16:41:25, 16.80s/it]

 67%|██████▋   | 7164/10740 [35:36:00<15:50:27, 15.95s/it]
{'loss': 0.1923, 'learning_rate': 5.273154614442277e-07, 'rewards/chosen': -3.769146680831909, 'rewards/rejected': -6.859078884124756, 'rewards/accuracies': 0.875, 'rewards/margins': 3.089932918548584, 'policy_logps/rejected': -371.404052734375, 'policy_logps/chosen': -295.3339538574219, 'referece_logps/rejected': -302.8132629394531, 'referece_logps/chosen': -257.64251708984375, 'logits/rejected': -0.7175527811050415, 'logits/chosen': -0.5532848834991455, 'epoch': 4.0}


 67%|██████▋   | 7166/10740 [35:36:35<16:18:56, 16.43s/it]

 67%|██████▋   | 7167/10740 [35:36:54<17:15:43, 17.39s/it]

 67%|██████▋   | 7168/10740 [35:37:10<16:49:46, 16.96s/it]

 67%|██████▋   | 7169/10740 [35:37:27<16:52:13, 17.01s/it]

 67%|██████▋   | 7170/10740 [35:37:45<16:52:48, 17.02s/it]

 67%|██████▋   | 7171/10740 [35:37:59<16:03:48, 16.20s/it]

 67%|██████▋   | 7172/10740 [35:38:15<16:06:49, 16.26s/it]

 67%|██████▋   | 7173/10740 [35:38:31<16:00:37, 16.16s/it]

 67%|██████▋   | 7174/10740 [35:38:43<14:40:21, 14.81s/it]
{'loss': 0.1388, 'learning_rate': 5.246599678827701e-07, 'rewards/chosen': -2.7253570556640625, 'rewards/rejected': -6.397225379943848, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6718685626983643, 'policy_logps/rejected': -302.0754089355469, 'policy_logps/chosen': -320.64971923828125, 'referece_logps/rejected': -238.1031494140625, 'referece_logps/chosen': -293.3961486816406, 'logits/rejected': 0.7011652588844299, 'logits/chosen': 0.5856589078903198, 'epoch': 4.01}

 67%|██████▋   | 7175/10740 [35:39:00<15:20:19, 15.49s/it]


 67%|██████▋   | 7177/10740 [35:39:31<15:49:44, 15.99s/it]

 67%|██████▋   | 7178/10740 [35:39:50<16:27:54, 16.64s/it]

 67%|██████▋   | 7179/10740 [35:40:09<17:12:10, 17.39s/it]
{'loss': 0.1967, 'learning_rate': 5.233338408431487e-07, 'rewards/chosen': -4.147130966186523, 'rewards/rejected': -7.551078796386719, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4039480686187744, 'policy_logps/rejected': -364.6784362792969, 'policy_logps/chosen': -367.21282958984375, 'referece_logps/rejected': -289.16766357421875, 'referece_logps/chosen': -325.7415771484375, 'logits/rejected': 0.03851482272148132, 'logits/chosen': 0.15844164788722992, 'epoch': 4.01}

 67%|██████▋   | 7180/10740 [35:40:28<17:44:50, 17.95s/it]

 67%|██████▋   | 7181/10740 [35:40:44<17:09:04, 17.35s/it]


 67%|██████▋   | 7183/10740 [35:41:13<15:24:00, 15.59s/it]

 67%|██████▋   | 7184/10740 [35:41:25<14:26:04, 14.61s/it]

 67%|██████▋   | 7185/10740 [35:41:39<14:13:27, 14.40s/it]

 67%|██████▋   | 7186/10740 [35:41:56<14:46:07, 14.96s/it]

 67%|██████▋   | 7187/10740 [35:42:16<16:14:52, 16.46s/it]

 67%|██████▋   | 7188/10740 [35:42:32<16:08:20, 16.36s/it]

 67%|██████▋   | 7189/10740 [35:42:51<17:05:01, 17.32s/it]

 67%|██████▋   | 7190/10740 [35:43:11<17:53:45, 18.15s/it]
{'loss': 0.3009, 'learning_rate': 5.204201807454161e-07, 'rewards/chosen': -3.580038547515869, 'rewards/rejected': -7.972112655639648, 'rewards/accuracies': 0.875, 'rewards/margins': 4.392075061798096, 'policy_logps/rejected': -303.9982604980469, 'policy_logps/chosen': -341.6773376464844, 'referece_logps/rejected': -224.27711486816406, 'referece_logps/chosen': -305.876953125, 'logits/rejected': -0.3079846501350403, 'logits/chosen': -0.5345901846885681, 'epoch': 4.02}


 67%|██████▋   | 7192/10740 [35:43:47<17:44:40, 18.00s/it]
{'loss': 0.2275, 'learning_rate': 5.198909904713489e-07, 'rewards/chosen': -3.192894220352173, 'rewards/rejected': -4.947722911834717, 'rewards/accuracies': 0.75, 'rewards/margins': 1.754828691482544, 'policy_logps/rejected': -315.39312744140625, 'policy_logps/chosen': -421.86505126953125, 'referece_logps/rejected': -265.9158630371094, 'referece_logps/chosen': -389.9361267089844, 'logits/rejected': 0.08083975315093994, 'logits/chosen': 0.02657371759414673, 'epoch': 4.02}


 67%|██████▋   | 7194/10740 [35:44:26<18:23:56, 18.68s/it]

 67%|██████▋   | 7195/10740 [35:44:47<19:10:37, 19.47s/it]

 67%|██████▋   | 7196/10740 [35:45:07<19:21:54, 19.67s/it]

 67%|██████▋   | 7197/10740 [35:45:26<19:01:13, 19.33s/it]

 67%|██████▋   | 7198/10740 [35:45:45<19:07:47, 19.44s/it]
{'loss': 0.2079, 'learning_rate': 5.183044684291089e-07, 'rewards/chosen': -3.0661377906799316, 'rewards/rejected': -8.520149230957031, 'rewards/accuracies': 1.0, 'rewards/margins': 5.454011917114258, 'policy_logps/rejected': -480.2774658203125, 'policy_logps/chosen': -424.7465515136719, 'referece_logps/rejected': -395.07598876953125, 'referece_logps/chosen': -394.0851745605469, 'logits/rejected': -0.2305913269519806, 'logits/chosen': -0.09154117852449417, 'epoch': 4.02}


 67%|██████▋   | 7200/10740 [35:46:19<18:08:36, 18.45s/it]
{'loss': 0.148, 'learning_rate': 5.177759779830071e-07, 'rewards/chosen': -3.7890172004699707, 'rewards/rejected': -6.990278720855713, 'rewards/accuracies': 0.875, 'rewards/margins': 3.201261520385742, 'policy_logps/rejected': -363.5003967285156, 'policy_logps/chosen': -489.69873046875, 'referece_logps/rejected': -293.59765625, 'referece_logps/chosen': -451.80859375, 'logits/rejected': 0.03604316711425781, 'logits/chosen': -0.05220448970794678, 'epoch': 4.02}


 67%|██████▋   | 7202/10740 [35:46:58<18:39:48, 18.99s/it]

 67%|██████▋   | 7203/10740 [35:47:13<17:33:42, 17.87s/it]
{'loss': 0.1526, 'learning_rate': 5.169835713198445e-07, 'rewards/chosen': -4.185693740844727, 'rewards/rejected': -7.626422882080078, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4407289028167725, 'policy_logps/rejected': -471.129150390625, 'policy_logps/chosen': -338.32989501953125, 'referece_logps/rejected': -394.86492919921875, 'referece_logps/chosen': -296.4729919433594, 'logits/rejected': -0.2264532744884491, 'logits/chosen': -0.07631659507751465, 'epoch': 4.02}


 67%|██████▋   | 7205/10740 [35:47:48<17:28:21, 17.79s/it]

 67%|██████▋   | 7206/10740 [35:48:08<18:03:52, 18.40s/it]

 67%|██████▋   | 7207/10740 [35:48:22<16:49:14, 17.14s/it]

 67%|██████▋   | 7208/10740 [35:48:43<18:03:51, 18.41s/it]

 67%|██████▋   | 7209/10740 [35:49:02<18:03:49, 18.42s/it]

 67%|██████▋   | 7210/10740 [35:49:14<16:10:18, 16.49s/it]

 67%|██████▋   | 7211/10740 [35:49:29<15:50:18, 16.16s/it]

 67%|██████▋   | 7212/10740 [35:49:40<14:18:23, 14.60s/it]
{'loss': 0.206, 'learning_rate': 5.146087262257154e-07, 'rewards/chosen': -5.544782638549805, 'rewards/rejected': -8.132835388183594, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5880537033081055, 'policy_logps/rejected': -449.1077880859375, 'policy_logps/chosen': -434.4288635253906, 'referece_logps/rejected': -367.77935791015625, 'referece_logps/chosen': -378.98101806640625, 'logits/rejected': -0.5339375734329224, 'logits/chosen': -0.46106988191604614, 'epoch': 4.03}

 67%|██████▋   | 7213/10740 [35:49:59<15:35:58, 15.92s/it]


 67%|██████▋   | 7215/10740 [35:50:34<16:27:31, 16.81s/it]

 67%|██████▋   | 7216/10740 [35:50:46<14:59:41, 15.32s/it]

 67%|██████▋   | 7217/10740 [35:51:07<16:44:57, 17.12s/it]
{'loss': 0.1437, 'learning_rate': 5.132909118836737e-07, 'rewards/chosen': -3.19860577583313, 'rewards/rejected': -5.876639366149902, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6780335903167725, 'policy_logps/rejected': -329.7122802734375, 'policy_logps/chosen': -339.0271911621094, 'referece_logps/rejected': -270.9459228515625, 'referece_logps/chosen': -307.0411071777344, 'logits/rejected': -0.09791184216737747, 'logits/chosen': 0.1463923454284668, 'epoch': 4.03}


 67%|██████▋   | 7219/10740 [35:51:44<17:27:37, 17.85s/it]

 67%|██████▋   | 7220/10740 [35:52:04<18:01:59, 18.44s/it]

 67%|██████▋   | 7221/10740 [35:52:24<18:37:15, 19.05s/it]

 67%|██████▋   | 7222/10740 [35:52:44<18:46:04, 19.21s/it]
{'loss': 0.1361, 'learning_rate': 5.119742042260602e-07, 'rewards/chosen': -4.569633960723877, 'rewards/rejected': -7.744478702545166, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1748452186584473, 'policy_logps/rejected': -282.8642883300781, 'policy_logps/chosen': -231.85934448242188, 'referece_logps/rejected': -205.4195098876953, 'referece_logps/chosen': -186.1630096435547, 'logits/rejected': -0.2231166660785675, 'logits/chosen': -0.20105162262916565, 'epoch': 4.03}

 67%|██████▋   | 7223/10740 [35:52:59<17:34:35, 17.99s/it]


 67%|██████▋   | 7225/10740 [35:53:37<18:06:23, 18.54s/it]

 67%|██████▋   | 7226/10740 [35:53:48<16:03:04, 16.44s/it]

 67%|██████▋   | 7227/10740 [35:54:08<16:57:52, 17.38s/it]

 67%|██████▋   | 7228/10740 [35:54:21<15:53:05, 16.28s/it]
{'loss': 0.1424, 'learning_rate': 5.103956200756649e-07, 'rewards/chosen': -2.8945116996765137, 'rewards/rejected': -5.874391078948975, 'rewards/accuracies': 0.875, 'rewards/margins': 2.979879856109619, 'policy_logps/rejected': -299.91424560546875, 'policy_logps/chosen': -251.17910766601562, 'referece_logps/rejected': -241.1703338623047, 'referece_logps/chosen': -222.23399353027344, 'logits/rejected': -0.10161439329385757, 'logits/chosen': -0.051380328834056854, 'epoch': 4.04}


 67%|██████▋   | 7230/10740 [35:54:58<17:08:22, 17.58s/it]

 67%|██████▋   | 7231/10740 [35:55:18<17:56:04, 18.40s/it]
{'loss': 0.139, 'learning_rate': 5.096069288421743e-07, 'rewards/chosen': -3.0512070655822754, 'rewards/rejected': -8.14554500579834, 'rewards/accuracies': 1.0, 'rewards/margins': 5.094336986541748, 'policy_logps/rejected': -333.824462890625, 'policy_logps/chosen': -411.5514831542969, 'referece_logps/rejected': -252.36904907226562, 'referece_logps/chosen': -381.0394287109375, 'logits/rejected': -0.19012324512004852, 'logits/chosen': -0.2937069535255432, 'epoch': 4.04}

 67%|██████▋   | 7232/10740 [35:55:41<19:14:28, 19.75s/it]


 67%|██████▋   | 7234/10740 [35:56:18<18:37:09, 19.12s/it]
{'loss': 0.2279, 'learning_rate': 5.088186390307314e-07, 'rewards/chosen': -2.0987942218780518, 'rewards/rejected': -7.4974799156188965, 'rewards/accuracies': 1.0, 'rewards/margins': 5.398685455322266, 'policy_logps/rejected': -381.45770263671875, 'policy_logps/chosen': -303.9751892089844, 'referece_logps/rejected': -306.48291015625, 'referece_logps/chosen': -282.98724365234375, 'logits/rejected': 0.12151780724525452, 'logits/chosen': 0.13624297082424164, 'epoch': 4.04}

 67%|██████▋   | 7235/10740 [35:56:37<18:45:52, 19.27s/it]

 67%|██████▋   | 7236/10740 [35:56:53<17:51:36, 18.35s/it]


 67%|██████▋   | 7238/10740 [35:57:34<18:48:26, 19.33s/it]

 67%|██████▋   | 7239/10740 [35:57:54<18:57:15, 19.49s/it]

 67%|██████▋   | 7240/10740 [35:58:10<17:58:15, 18.48s/it]

 67%|██████▋   | 7241/10740 [35:58:30<18:28:02, 19.00s/it]
{'loss': 0.2553, 'learning_rate': 5.069808608471661e-07, 'rewards/chosen': -3.635420083999634, 'rewards/rejected': -6.769505977630615, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1340858936309814, 'policy_logps/rejected': -506.347900390625, 'policy_logps/chosen': -453.4676513671875, 'referece_logps/rejected': -438.65283203125, 'referece_logps/chosen': -417.1134033203125, 'logits/rejected': 0.03143833577632904, 'logits/chosen': -0.06150170415639877, 'epoch': 4.05}


 67%|██████▋   | 7243/10740 [35:59:08<18:23:17, 18.93s/it]

 67%|██████▋   | 7244/10740 [35:59:26<18:13:36, 18.77s/it]

 67%|██████▋   | 7245/10740 [35:59:46<18:29:40, 19.05s/it]

 67%|██████▋   | 7246/10740 [36:00:05<18:23:05, 18.94s/it]

 67%|██████▋   | 7247/10740 [36:00:23<18:03:57, 18.62s/it]

 67%|██████▋   | 7248/10740 [36:00:41<17:52:43, 18.43s/it]
{'loss': 0.1331, 'learning_rate': 5.051452798864916e-07, 'rewards/chosen': -3.1524288654327393, 'rewards/rejected': -7.801162242889404, 'rewards/accuracies': 1.0, 'rewards/margins': 4.648733615875244, 'policy_logps/rejected': -265.7784729003906, 'policy_logps/chosen': -328.1685485839844, 'referece_logps/rejected': -187.766845703125, 'referece_logps/chosen': -296.644287109375, 'logits/rejected': -0.023290976881980896, 'logits/chosen': -0.11463211476802826, 'epoch': 4.05}

 67%|██████▋   | 7249/10740 [36:01:01<18:35:06, 19.17s/it]


 68%|██████▊   | 7251/10740 [36:01:37<18:09:56, 18.74s/it]

 68%|██████▊   | 7252/10740 [36:01:54<17:53:08, 18.46s/it]

 68%|██████▊   | 7253/10740 [36:02:11<17:20:25, 17.90s/it]

 68%|██████▊   | 7254/10740 [36:02:31<17:52:27, 18.46s/it]

 68%|██████▊   | 7255/10740 [36:02:50<18:09:31, 18.76s/it]

 68%|██████▊   | 7256/10740 [36:03:05<16:51:20, 17.42s/it]

 68%|██████▊   | 7257/10740 [36:03:22<16:53:42, 17.46s/it]
{'loss': 0.1496, 'learning_rate': 5.027884889592902e-07, 'rewards/chosen': -4.355690002441406, 'rewards/rejected': -8.479576110839844, 'rewards/accuracies': 1.0, 'rewards/margins': 4.123886585235596, 'policy_logps/rejected': -318.2234802246094, 'policy_logps/chosen': -480.3138122558594, 'referece_logps/rejected': -233.427734375, 'referece_logps/chosen': -436.75689697265625, 'logits/rejected': -0.5044422149658203, 'logits/chosen': -0.7418282628059387, 'epoch': 4.05}

 68%|██████▊   | 7258/10740 [36:03:40<16:52:27, 17.45s/it]

 68%|██████▊   | 7259/10740 [36:03:53<15:47:33, 16.33s/it]


 68%|██████▊   | 7261/10740 [36:04:20<14:14:36, 14.74s/it]

 68%|██████▊   | 7262/10740 [36:04:37<14:51:04, 15.37s/it]

 68%|██████▊   | 7263/10740 [36:04:53<15:00:05, 15.53s/it]

 68%|██████▊   | 7264/10740 [36:05:09<15:04:23, 15.61s/it]
{'loss': 0.1831, 'learning_rate': 5.009579609254411e-07, 'rewards/chosen': -3.4915709495544434, 'rewards/rejected': -6.834323406219482, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3427529335021973, 'policy_logps/rejected': -331.79840087890625, 'policy_logps/chosen': -344.2203063964844, 'referece_logps/rejected': -263.45513916015625, 'referece_logps/chosen': -309.3045959472656, 'logits/rejected': -0.09209305047988892, 'logits/chosen': -0.2286698818206787, 'epoch': 4.06}

 68%|██████▊   | 7265/10740 [36:05:24<15:05:54, 15.64s/it]

 68%|██████▊   | 7266/10740 [36:05:43<15:50:14, 16.41s/it]

 68%|██████▊   | 7267/10740 [36:06:03<16:52:56, 17.50s/it]

 68%|██████▊   | 7268/10740 [36:06:26<18:28:46, 19.16s/it]

 68%|██████▊   | 7269/10740 [36:06:38<16:29:39, 17.11s/it]


 68%|██████▊   | 7271/10740 [36:07:11<16:04:19, 16.68s/it]
{'loss': 0.1296, 'learning_rate': 4.991296569565514e-07, 'rewards/chosen': -4.116044521331787, 'rewards/rejected': -10.043764114379883, 'rewards/accuracies': 0.875, 'rewards/margins': 5.927720069885254, 'policy_logps/rejected': -476.8251953125, 'policy_logps/chosen': -426.69415283203125, 'referece_logps/rejected': -376.38751220703125, 'referece_logps/chosen': -385.53369140625, 'logits/rejected': -0.6351609826087952, 'logits/chosen': -0.7635489702224731, 'epoch': 4.06}

 68%|██████▊   | 7272/10740 [36:07:27<16:03:11, 16.66s/it]


 68%|██████▊   | 7274/10740 [36:07:59<15:45:30, 16.37s/it]

 68%|██████▊   | 7275/10740 [36:08:19<16:44:38, 17.40s/it]
{'loss': 0.1452, 'learning_rate': 4.980859135273877e-07, 'rewards/chosen': -4.224570274353027, 'rewards/rejected': -7.8884077072143555, 'rewards/accuracies': 0.75, 'rewards/margins': 3.6638379096984863, 'policy_logps/rejected': -350.78375244140625, 'policy_logps/chosen': -415.38104248046875, 'referece_logps/rejected': -271.899658203125, 'referece_logps/chosen': -373.1353759765625, 'logits/rejected': 0.5564502477645874, 'logits/chosen': 0.41891375184059143, 'epoch': 4.06}


 68%|██████▊   | 7277/10740 [36:08:53<16:29:23, 17.14s/it]
{'loss': 0.2044, 'learning_rate': 4.975643156197879e-07, 'rewards/chosen': -2.7112700939178467, 'rewards/rejected': -7.851313591003418, 'rewards/accuracies': 1.0, 'rewards/margins': 5.140042781829834, 'policy_logps/rejected': -348.9741516113281, 'policy_logps/chosen': -216.6741943359375, 'referece_logps/rejected': -270.46099853515625, 'referece_logps/chosen': -189.56149291992188, 'logits/rejected': -0.8630629777908325, 'logits/chosen': -0.6187281012535095, 'epoch': 4.07}


 68%|██████▊   | 7279/10740 [36:09:33<17:52:08, 18.59s/it]
{'loss': 0.2217, 'learning_rate': 4.970429005032144e-07, 'rewards/chosen': -3.9415695667266846, 'rewards/rejected': -8.285892486572266, 'rewards/accuracies': 0.625, 'rewards/margins': 4.34432315826416, 'policy_logps/rejected': -381.28924560546875, 'policy_logps/chosen': -349.01885986328125, 'referece_logps/rejected': -298.4302978515625, 'referece_logps/chosen': -309.6031188964844, 'logits/rejected': -0.34640398621559143, 'logits/chosen': -0.3235914409160614, 'epoch': 4.07}

 68%|██████▊   | 7280/10740 [36:09:54<18:37:23, 19.38s/it]


 68%|██████▊   | 7282/10740 [36:10:29<17:24:35, 18.12s/it]
{'loss': 0.3179, 'learning_rate': 4.96261120976471e-07, 'rewards/chosen': -2.9821512699127197, 'rewards/rejected': -8.437206268310547, 'rewards/accuracies': 1.0, 'rewards/margins': 5.45505428314209, 'policy_logps/rejected': -327.66046142578125, 'policy_logps/chosen': -338.43194580078125, 'referece_logps/rejected': -243.28839111328125, 'referece_logps/chosen': -308.6104431152344, 'logits/rejected': -0.011188486590981483, 'logits/chosen': -0.19270731508731842, 'epoch': 4.07}

 68%|██████▊   | 7283/10740 [36:10:50<18:08:25, 18.89s/it]

 68%|██████▊   | 7284/10740 [36:11:07<17:32:09, 18.27s/it]


 68%|██████▊   | 7286/10740 [36:11:48<18:40:08, 19.46s/it]
{'loss': 0.2775, 'learning_rate': 4.952193898126761e-07, 'rewards/chosen': -3.9349722862243652, 'rewards/rejected': -7.444319248199463, 'rewards/accuracies': 0.75, 'rewards/margins': 3.5093462467193604, 'policy_logps/rejected': -431.52392578125, 'policy_logps/chosen': -354.0748291015625, 'referece_logps/rejected': -357.0807189941406, 'referece_logps/chosen': -314.72509765625, 'logits/rejected': -0.1845208704471588, 'logits/chosen': -0.162191703915596, 'epoch': 4.07}

 68%|██████▊   | 7287/10740 [36:12:00<16:47:10, 17.50s/it]

 68%|██████▊   | 7288/10740 [36:12:15<15:50:48, 16.53s/it]


 68%|██████▊   | 7290/10740 [36:12:48<16:15:01, 16.96s/it]
{'loss': 0.1089, 'learning_rate': 4.941783932253486e-07, 'rewards/chosen': -3.726595640182495, 'rewards/rejected': -8.74623966217041, 'rewards/accuracies': 0.875, 'rewards/margins': 5.019644260406494, 'policy_logps/rejected': -414.0955810546875, 'policy_logps/chosen': -416.3240051269531, 'referece_logps/rejected': -326.6332092285156, 'referece_logps/chosen': -379.05804443359375, 'logits/rejected': -0.45577049255371094, 'logits/chosen': -0.532284677028656, 'epoch': 4.07}

 68%|██████▊   | 7291/10740 [36:13:01<15:08:05, 15.80s/it]


 68%|██████▊   | 7293/10740 [36:13:41<17:16:32, 18.04s/it]
{'loss': 0.165, 'learning_rate': 4.933981287620065e-07, 'rewards/chosen': -3.691354990005493, 'rewards/rejected': -7.730432033538818, 'rewards/accuracies': 0.875, 'rewards/margins': 4.03907585144043, 'policy_logps/rejected': -404.5595397949219, 'policy_logps/chosen': -456.5592956542969, 'referece_logps/rejected': -327.2552795410156, 'referece_logps/chosen': -419.645751953125, 'logits/rejected': -0.876502513885498, 'logits/chosen': -0.8404698371887207, 'epoch': 4.07}


 68%|██████▊   | 7295/10740 [36:14:18<17:10:57, 17.96s/it]
{'loss': 0.1804, 'learning_rate': 4.928781827970983e-07, 'rewards/chosen': -3.762275457382202, 'rewards/rejected': -8.214082717895508, 'rewards/accuracies': 1.0, 'rewards/margins': 4.451807498931885, 'policy_logps/rejected': -512.6612548828125, 'policy_logps/chosen': -368.5953063964844, 'referece_logps/rejected': -430.52044677734375, 'referece_logps/chosen': -330.9725341796875, 'logits/rejected': -0.8025091290473938, 'logits/chosen': -0.5785461068153381, 'epoch': 4.08}

 68%|██████▊   | 7296/10740 [36:14:36<17:25:20, 18.21s/it]


 68%|██████▊   | 7298/10740 [36:15:11<16:51:55, 17.64s/it]
{'loss': 0.2833, 'learning_rate': 4.920986098386203e-07, 'rewards/chosen': -3.5769495964050293, 'rewards/rejected': -9.605621337890625, 'rewards/accuracies': 1.0, 'rewards/margins': 6.028670787811279, 'policy_logps/rejected': -446.2550048828125, 'policy_logps/chosen': -410.8441162109375, 'referece_logps/rejected': -350.19879150390625, 'referece_logps/chosen': -375.07464599609375, 'logits/rejected': 0.04935051500797272, 'logits/chosen': -0.033471137285232544, 'epoch': 4.08}


 68%|██████▊   | 7300/10740 [36:15:50<17:50:00, 18.66s/it]

 68%|██████▊   | 7301/10740 [36:16:09<18:05:54, 18.95s/it]

 68%|██████▊   | 7302/10740 [36:16:32<19:00:32, 19.90s/it]
{'loss': 0.134, 'learning_rate': 4.910598260658014e-07, 'rewards/chosen': -4.517114639282227, 'rewards/rejected': -7.944457054138184, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4273414611816406, 'policy_logps/rejected': -380.5279541015625, 'policy_logps/chosen': -361.74169921875, 'referece_logps/rejected': -301.0833740234375, 'referece_logps/chosen': -316.570556640625, 'logits/rejected': -0.08364024758338928, 'logits/chosen': -0.11631519347429276, 'epoch': 4.08}

 68%|██████▊   | 7303/10740 [36:16:47<17:40:29, 18.51s/it]


 68%|██████▊   | 7305/10740 [36:17:23<17:16:42, 18.11s/it]
{'loss': 0.1725, 'learning_rate': 4.902812241917506e-07, 'rewards/chosen': -5.330965995788574, 'rewards/rejected': -7.721841335296631, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3908755779266357, 'policy_logps/rejected': -295.26898193359375, 'policy_logps/chosen': -321.1903381347656, 'referece_logps/rejected': -218.05059814453125, 'referece_logps/chosen': -267.88067626953125, 'logits/rejected': -0.3838508427143097, 'logits/chosen': -0.36306872963905334, 'epoch': 4.08}


 68%|██████▊   | 7307/10740 [36:18:00<17:18:03, 18.14s/it]

 68%|██████▊   | 7308/10740 [36:18:20<17:51:07, 18.73s/it]
{'loss': 0.1825, 'learning_rate': 4.895030395592287e-07, 'rewards/chosen': -4.807149887084961, 'rewards/rejected': -8.986923217773438, 'rewards/accuracies': 1.0, 'rewards/margins': 4.179771900177002, 'policy_logps/rejected': -457.9486389160156, 'policy_logps/chosen': -457.2002868652344, 'referece_logps/rejected': -368.079345703125, 'referece_logps/chosen': -409.1287536621094, 'logits/rejected': 0.5669286251068115, 'logits/chosen': 0.5216964483261108, 'epoch': 4.08}

 68%|██████▊   | 7309/10740 [36:18:39<17:56:54, 18.83s/it]

 68%|██████▊   | 7310/10740 [36:18:59<18:13:41, 19.13s/it]


 68%|██████▊   | 7312/10740 [36:19:28<16:09:14, 16.96s/it]

 68%|██████▊   | 7313/10740 [36:19:46<16:30:24, 17.34s/it]
{'loss': 0.1886, 'learning_rate': 4.882069941050593e-07, 'rewards/chosen': -3.5166513919830322, 'rewards/rejected': -6.271505832672119, 'rewards/accuracies': 0.875, 'rewards/margins': 2.754854917526245, 'policy_logps/rejected': -286.0914001464844, 'policy_logps/chosen': -278.1526184082031, 'referece_logps/rejected': -223.37635803222656, 'referece_logps/chosen': -242.98614501953125, 'logits/rejected': -0.16134947538375854, 'logits/chosen': -0.20765919983386993, 'epoch': 4.09}

 68%|██████▊   | 7314/10740 [36:20:03<16:26:55, 17.28s/it]

 68%|██████▊   | 7315/10740 [36:20:23<17:08:14, 18.01s/it]

 68%|██████▊   | 7316/10740 [36:20:37<16:07:08, 16.95s/it]


 68%|██████▊   | 7318/10740 [36:21:16<17:19:38, 18.23s/it]

 68%|██████▊   | 7319/10740 [36:21:38<18:22:36, 19.34s/it]

 68%|██████▊   | 7320/10740 [36:21:56<17:54:21, 18.85s/it]

 68%|██████▊   | 7321/10740 [36:22:12<17:12:32, 18.12s/it]
{'loss': 0.1202, 'learning_rate': 4.861357431420032e-07, 'rewards/chosen': -4.650774955749512, 'rewards/rejected': -8.408026695251465, 'rewards/accuracies': 1.0, 'rewards/margins': 3.757251739501953, 'policy_logps/rejected': -585.7319946289062, 'policy_logps/chosen': -504.2646484375, 'referece_logps/rejected': -501.6517028808594, 'referece_logps/chosen': -457.75689697265625, 'logits/rejected': 0.3730056881904602, 'logits/chosen': 0.32631808519363403, 'epoch': 4.09}

 68%|██████▊   | 7322/10740 [36:22:29<16:44:25, 17.63s/it]


 68%|██████▊   | 7324/10740 [36:22:58<15:34:09, 16.41s/it]
{'loss': 0.2695, 'learning_rate': 4.853597945475059e-07, 'rewards/chosen': -3.289438009262085, 'rewards/rejected': -6.144964218139648, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8555262088775635, 'policy_logps/rejected': -380.283203125, 'policy_logps/chosen': -436.8464660644531, 'referece_logps/rejected': -318.83355712890625, 'referece_logps/chosen': -403.95208740234375, 'logits/rejected': 0.6254069209098816, 'logits/chosen': 0.654796838760376, 'epoch': 4.09}


 68%|██████▊   | 7326/10740 [36:23:36<16:53:47, 17.82s/it]
{'loss': 0.1639, 'learning_rate': 4.848427294842436e-07, 'rewards/chosen': -3.0916285514831543, 'rewards/rejected': -8.497255325317383, 'rewards/accuracies': 1.0, 'rewards/margins': 5.40562629699707, 'policy_logps/rejected': -378.5353088378906, 'policy_logps/chosen': -408.6397399902344, 'referece_logps/rejected': -293.562744140625, 'referece_logps/chosen': -377.7234802246094, 'logits/rejected': -0.21859662234783173, 'logits/chosen': 0.015385761857032776, 'epoch': 4.09}

 68%|██████▊   | 7327/10740 [36:23:51<15:52:44, 16.75s/it]


 68%|██████▊   | 7329/10740 [36:24:28<16:48:09, 17.73s/it]
{'loss': 0.1243, 'learning_rate': 4.840674833592379e-07, 'rewards/chosen': -3.2431302070617676, 'rewards/rejected': -9.479412078857422, 'rewards/accuracies': 1.0, 'rewards/margins': 6.2362823486328125, 'policy_logps/rejected': -392.92608642578125, 'policy_logps/chosen': -386.42535400390625, 'referece_logps/rejected': -298.1319580078125, 'referece_logps/chosen': -353.9941101074219, 'logits/rejected': -0.3346576690673828, 'logits/chosen': -0.34033241868019104, 'epoch': 4.09}

 68%|██████▊   | 7330/10740 [36:24:48<17:20:14, 18.30s/it]


 68%|██████▊   | 7332/10740 [36:25:24<17:08:54, 18.11s/it]

 68%|██████▊   | 7333/10740 [36:25:45<17:45:35, 18.77s/it]

 68%|██████▊   | 7334/10740 [36:26:04<18:04:42, 19.11s/it]
{'loss': 0.1623, 'learning_rate': 4.827763453039485e-07, 'rewards/chosen': -3.1217925548553467, 'rewards/rejected': -9.359418869018555, 'rewards/accuracies': 0.875, 'rewards/margins': 6.237627029418945, 'policy_logps/rejected': -575.5621948242188, 'policy_logps/chosen': -321.0987854003906, 'referece_logps/rejected': -481.96795654296875, 'referece_logps/chosen': -289.8808288574219, 'logits/rejected': 0.007128030061721802, 'logits/chosen': 0.03533308580517769, 'epoch': 4.1}

 68%|██████▊   | 7335/10740 [36:26:23<17:57:12, 18.98s/it]


 68%|██████▊   | 7337/10740 [36:27:01<17:59:06, 19.03s/it]
{'loss': 0.1538, 'learning_rate': 4.820022267960101e-07, 'rewards/chosen': -2.570378303527832, 'rewards/rejected': -6.686157703399658, 'rewards/accuracies': 1.0, 'rewards/margins': 4.115779399871826, 'policy_logps/rejected': -362.47088623046875, 'policy_logps/chosen': -464.98187255859375, 'referece_logps/rejected': -295.60931396484375, 'referece_logps/chosen': -439.2781066894531, 'logits/rejected': 0.3387882709503174, 'logits/chosen': 0.09308190643787384, 'epoch': 4.1}

 68%|██████▊   | 7338/10740 [36:27:19<17:55:29, 18.97s/it]

 68%|██████▊   | 7339/10740 [36:27:39<18:09:21, 19.22s/it]

 68%|██████▊   | 7340/10740 [36:28:01<19:00:27, 20.13s/it]


 68%|██████▊   | 7342/10740 [36:28:37<17:42:45, 18.77s/it]

 68%|██████▊   | 7343/10740 [36:28:56<17:57:27, 19.03s/it]
{'loss': 0.1808, 'learning_rate': 4.804552624689121e-07, 'rewards/chosen': -2.5467429161071777, 'rewards/rejected': -8.274338722229004, 'rewards/accuracies': 1.0, 'rewards/margins': 5.727595806121826, 'policy_logps/rejected': -466.8623962402344, 'policy_logps/chosen': -396.759033203125, 'referece_logps/rejected': -384.1190185546875, 'referece_logps/chosen': -371.2916259765625, 'logits/rejected': -0.05174112319946289, 'logits/chosen': -0.04497554898262024, 'epoch': 4.1}

 68%|██████▊   | 7344/10740 [36:29:14<17:28:42, 18.53s/it]

 68%|██████▊   | 7345/10740 [36:29:33<17:50:53, 18.93s/it]

 68%|██████▊   | 7346/10740 [36:29:54<18:12:12, 19.31s/it]

 68%|██████▊   | 7347/10740 [36:30:13<18:17:05, 19.40s/it]

 68%|██████▊   | 7348/10740 [36:30:30<17:27:07, 18.52s/it]

 68%|██████▊   | 7349/10740 [36:30:42<15:33:31, 16.52s/it]


 68%|██████▊   | 7351/10740 [36:31:12<14:57:42, 15.89s/it]
{'loss': 0.1418, 'learning_rate': 4.78395290456687e-07, 'rewards/chosen': -3.3544421195983887, 'rewards/rejected': -8.303945541381836, 'rewards/accuracies': 0.875, 'rewards/margins': 4.949503421783447, 'policy_logps/rejected': -318.3064270019531, 'policy_logps/chosen': -421.337890625, 'referece_logps/rejected': -235.26695251464844, 'referece_logps/chosen': -387.79345703125, 'logits/rejected': 0.9705557823181152, 'logits/chosen': 0.8710302114486694, 'epoch': 4.11}

 68%|██████▊   | 7352/10740 [36:31:23<13:29:41, 14.34s/it]

 68%|██████▊   | 7353/10740 [36:31:35<12:52:56, 13.69s/it]


 68%|██████▊   | 7355/10740 [36:31:59<11:55:27, 12.68s/it]
{'loss': 0.1835, 'learning_rate': 4.773664422907439e-07, 'rewards/chosen': -3.5337073802948, 'rewards/rejected': -7.731372833251953, 'rewards/accuracies': 1.0, 'rewards/margins': 4.197665214538574, 'policy_logps/rejected': -365.0360107421875, 'policy_logps/chosen': -362.6007080078125, 'referece_logps/rejected': -287.7222900390625, 'referece_logps/chosen': -327.26361083984375, 'logits/rejected': -0.4217911958694458, 'logits/chosen': -0.49860331416130066, 'epoch': 4.11}

 68%|██████▊   | 7356/10740 [36:32:22<14:51:07, 15.80s/it]

 69%|██████▊   | 7357/10740 [36:32:37<14:38:48, 15.59s/it]

 69%|██████▊   | 7358/10740 [36:32:56<15:29:21, 16.49s/it]

 69%|██████▊   | 7359/10740 [36:33:09<14:30:43, 15.45s/it]

 69%|██████▊   | 7360/10740 [36:33:29<15:44:54, 16.77s/it]


 69%|██████▊   | 7362/10740 [36:34:03<16:04:18, 17.13s/it]
{'loss': 0.2083, 'learning_rate': 4.755677889901277e-07, 'rewards/chosen': -3.0988290309906006, 'rewards/rejected': -8.382756233215332, 'rewards/accuracies': 1.0, 'rewards/margins': 5.283926486968994, 'policy_logps/rejected': -563.83447265625, 'policy_logps/chosen': -449.4107360839844, 'referece_logps/rejected': -480.0069274902344, 'referece_logps/chosen': -418.4224548339844, 'logits/rejected': 0.2702697217464447, 'logits/chosen': 0.3170369267463684, 'epoch': 4.11}

 69%|██████▊   | 7363/10740 [36:34:20<15:53:44, 16.95s/it]

 69%|██████▊   | 7364/10740 [36:34:34<15:15:44, 16.28s/it]

 69%|██████▊   | 7365/10740 [36:34:57<16:59:32, 18.13s/it]

 69%|██████▊   | 7366/10740 [36:35:09<15:20:21, 16.37s/it]

 69%|██████▊   | 7367/10740 [36:35:20<13:58:30, 14.92s/it]

 69%|██████▊   | 7368/10740 [36:35:38<14:47:45, 15.80s/it]

 69%|██████▊   | 7369/10740 [36:35:54<14:48:53, 15.82s/it]

 69%|██████▊   | 7370/10740 [36:36:09<14:31:40, 15.52s/it]

 69%|██████▊   | 7371/10740 [36:36:29<15:52:08, 16.96s/it]

 69%|██████▊   | 7372/10740 [36:36:49<16:36:45, 17.76s/it]

 69%|██████▊   | 7373/10740 [36:37:06<16:16:52, 17.41s/it]

 69%|██████▊   | 7374/10740 [36:37:18<15:00:50, 16.06s/it]

 69%|██████▊   | 7375/10740 [36:37:34<14:51:44, 15.90s/it]

 69%|██████▊   | 7376/10740 [36:37:54<15:52:42, 16.99s/it]

 69%|██████▊   | 7377/10740 [36:38:04<14:05:25, 15.08s/it]

 69%|██████▊   | 7378/10740 [36:38:24<15:24:19, 16.50s/it]

 69%|██████▊   | 7379/10740 [36:38:36<14:13:42, 15.24s/it]

 69%|██████▊   | 7380/10740 [36:38:50<13:44:45, 14.73s/it]

 69%|██████▊   | 7381/10740 [36:39:09<15:03:50, 16.14s/it]

 69%|██████▊   | 7382/10740 [36:39:27<15:28:03, 16.58s/it]

 69%|██████▊   | 7383/10740 [36:39:43<15:15:04, 16.36s/it]

 69%|██████▉   | 7384/10740 [36:40:02<16:09:54, 17.34s/it]


 69%|██████▉   | 7386/10740 [36:40:40<16:44:41, 17.97s/it]
{'loss': 0.1622, 'learning_rate': 4.6941877064804216e-07, 'rewards/chosen': -4.845758438110352, 'rewards/rejected': -7.539086818695068, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6933279037475586, 'policy_logps/rejected': -381.83917236328125, 'policy_logps/chosen': -441.1372985839844, 'referece_logps/rejected': -306.44830322265625, 'referece_logps/chosen': -392.6797180175781, 'logits/rejected': -0.46861404180526733, 'logits/chosen': -0.49535664916038513, 'epoch': 4.13}

 69%|██████▉   | 7387/10740 [36:40:58<16:47:12, 18.02s/it]

 69%|██████▉   | 7388/10740 [36:41:10<15:10:28, 16.30s/it]

 69%|██████▉   | 7389/10740 [36:41:31<16:36:45, 17.85s/it]

 69%|██████▉   | 7390/10740 [36:41:51<16:58:06, 18.23s/it]

 69%|██████▉   | 7391/10740 [36:42:09<16:54:15, 18.17s/it]

 69%|██████▉   | 7392/10740 [36:42:29<17:29:29, 18.81s/it]

 69%|██████▉   | 7393/10740 [36:42:47<17:15:22, 18.56s/it]

 69%|██████▉   | 7394/10740 [36:43:01<15:55:26, 17.13s/it]

 69%|██████▉   | 7395/10740 [36:43:22<17:04:54, 18.38s/it]

 69%|██████▉   | 7396/10740 [36:43:40<17:06:45, 18.42s/it]

 69%|██████▉   | 7397/10740 [36:44:02<17:55:06, 19.30s/it]

 69%|██████▉   | 7398/10740 [36:44:22<18:07:12, 19.52s/it]

 69%|██████▉   | 7399/10740 [36:44:41<17:52:29, 19.26s/it]


 69%|██████▉   | 7401/10740 [36:45:18<17:36:05, 18.98s/it]
{'loss': 0.2134, 'learning_rate': 4.6558972910175666e-07, 'rewards/chosen': -3.3985795974731445, 'rewards/rejected': -5.403881072998047, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0053014755249023, 'policy_logps/rejected': -697.621337890625, 'policy_logps/chosen': -495.59893798828125, 'referece_logps/rejected': -643.58251953125, 'referece_logps/chosen': -461.6131896972656, 'logits/rejected': -0.4423463046550751, 'logits/chosen': -0.35664141178131104, 'epoch': 4.13}

 69%|██████▉   | 7402/10740 [36:45:38<17:50:13, 19.24s/it]


 69%|██████▉   | 7404/10740 [36:46:10<15:56:40, 17.21s/it]
{'loss': 0.3563, 'learning_rate': 4.6482523064669533e-07, 'rewards/chosen': -2.6392791271209717, 'rewards/rejected': -6.835605144500732, 'rewards/accuracies': 0.875, 'rewards/margins': 4.19632625579834, 'policy_logps/rejected': -436.4842529296875, 'policy_logps/chosen': -448.3208923339844, 'referece_logps/rejected': -368.1282043457031, 'referece_logps/chosen': -421.9281005859375, 'logits/rejected': 0.09275215864181519, 'logits/chosen': -0.17431087791919708, 'epoch': 4.14}

 69%|██████▉   | 7405/10740 [36:46:30<16:38:28, 17.96s/it]

 69%|██████▉   | 7406/10740 [36:46:49<17:04:40, 18.44s/it]

 69%|██████▉   | 7407/10740 [36:47:07<17:03:13, 18.42s/it]

 69%|██████▉   | 7408/10740 [36:47:23<16:14:54, 17.56s/it]

 69%|██████▉   | 7409/10740 [36:47:43<16:57:23, 18.33s/it]

 69%|██████▉   | 7410/10740 [36:48:04<17:33:05, 18.97s/it]

 69%|██████▉   | 7411/10740 [36:48:25<18:07:54, 19.61s/it]

 69%|██████▉   | 7412/10740 [36:48:39<16:36:24, 17.96s/it]


 69%|██████▉   | 7414/10740 [36:49:14<16:26:42, 17.80s/it]
{'loss': 0.1886, 'learning_rate': 4.622800698803122e-07, 'rewards/chosen': -3.152792453765869, 'rewards/rejected': -5.533470153808594, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3806777000427246, 'policy_logps/rejected': -240.1759033203125, 'policy_logps/chosen': -248.88958740234375, 'referece_logps/rejected': -184.8411865234375, 'referece_logps/chosen': -217.3616485595703, 'logits/rejected': -0.4077466130256653, 'logits/chosen': -0.47998490929603577, 'epoch': 4.14}

 69%|██████▉   | 7415/10740 [36:49:31<16:14:42, 17.59s/it]

 69%|██████▉   | 7416/10740 [36:49:51<16:47:44, 18.19s/it]

 69%|██████▉   | 7417/10740 [36:50:10<17:04:03, 18.49s/it]

 69%|██████▉   | 7418/10740 [36:50:30<17:26:14, 18.90s/it]

 69%|██████▉   | 7419/10740 [36:50:50<17:40:18, 19.16s/it]

 69%|██████▉   | 7420/10740 [36:51:10<17:55:55, 19.44s/it]

 69%|██████▉   | 7421/10740 [36:51:28<17:32:52, 19.03s/it]

 69%|██████▉   | 7422/10740 [36:51:48<17:45:02, 19.26s/it]

 69%|██████▉   | 7423/10740 [36:52:07<17:54:31, 19.44s/it]

 69%|██████▉   | 7424/10740 [36:52:23<16:42:25, 18.14s/it]

 69%|██████▉   | 7425/10740 [36:52:44<17:36:02, 19.11s/it]

 69%|██████▉   | 7426/10740 [36:53:04<17:45:30, 19.29s/it]

 69%|██████▉   | 7427/10740 [36:53:16<15:43:17, 17.08s/it]

 69%|██████▉   | 7428/10740 [36:53:31<15:21:32, 16.69s/it]

 69%|██████▉   | 7429/10740 [36:53:53<16:45:38, 18.22s/it]

 69%|██████▉   | 7430/10740 [36:54:06<15:11:58, 16.53s/it]

 69%|██████▉   | 7431/10740 [36:54:22<15:15:19, 16.60s/it]

 69%|██████▉   | 7432/10740 [36:54:40<15:23:01, 16.74s/it]

 69%|██████▉   | 7433/10740 [36:54:55<15:01:43, 16.36s/it]


 69%|██████▉   | 7435/10740 [36:55:30<15:35:01, 16.97s/it]
{'loss': 0.2223, 'learning_rate': 4.569511790437254e-07, 'rewards/chosen': -3.6239194869995117, 'rewards/rejected': -8.055816650390625, 'rewards/accuracies': 1.0, 'rewards/margins': 4.431896686553955, 'policy_logps/rejected': -373.8615417480469, 'policy_logps/chosen': -320.6262512207031, 'referece_logps/rejected': -293.3033447265625, 'referece_logps/chosen': -284.3870849609375, 'logits/rejected': -0.37729668617248535, 'logits/chosen': -0.3597151041030884, 'epoch': 4.15}

 69%|██████▉   | 7436/10740 [36:55:51<16:26:59, 17.92s/it]

 69%|██████▉   | 7437/10740 [36:56:10<16:54:34, 18.43s/it]

 69%|██████▉   | 7438/10740 [36:56:25<15:54:26, 17.34s/it]

 69%|██████▉   | 7439/10740 [36:56:45<16:45:04, 18.27s/it]

 69%|██████▉   | 7440/10740 [36:57:05<17:10:14, 18.73s/it]

 69%|██████▉   | 7441/10740 [36:57:25<17:28:38, 19.07s/it]

 69%|██████▉   | 7442/10740 [36:57:45<17:39:12, 19.27s/it]

 69%|██████▉   | 7443/10740 [36:58:04<17:36:59, 19.24s/it]

 69%|██████▉   | 7444/10740 [36:58:21<16:55:57, 18.49s/it]

 69%|██████▉   | 7445/10740 [36:58:38<16:30:06, 18.03s/it]

 69%|██████▉   | 7446/10740 [36:59:00<17:38:57, 19.29s/it]

 69%|██████▉   | 7447/10740 [36:59:18<17:22:05, 18.99s/it]

 69%|██████▉   | 7448/10740 [36:59:39<17:47:39, 19.46s/it]

 69%|██████▉   | 7449/10740 [36:59:59<18:03:27, 19.75s/it]

 69%|██████▉   | 7450/10740 [37:00:19<18:03:01, 19.75s/it]

 69%|██████▉   | 7451/10740 [37:00:39<18:14:42, 19.97s/it]

 69%|██████▉   | 7452/10740 [37:00:59<18:13:09, 19.95s/it]

 69%|██████▉   | 7453/10740 [37:01:12<16:09:36, 17.70s/it]

 69%|██████▉   | 7454/10740 [37:01:24<14:44:52, 16.16s/it]

 69%|██████▉   | 7455/10740 [37:01:44<15:42:02, 17.21s/it]

 69%|██████▉   | 7456/10740 [37:02:04<16:29:44, 18.08s/it]

 69%|██████▉   | 7457/10740 [37:02:24<16:58:17, 18.61s/it]

 69%|██████▉   | 7458/10740 [37:02:41<16:36:34, 18.22s/it]

 69%|██████▉   | 7459/10740 [37:02:54<15:04:43, 16.54s/it]

 69%|██████▉   | 7460/10740 [37:03:16<16:40:17, 18.30s/it]


 69%|██████▉   | 7462/10740 [37:03:59<17:54:58, 19.68s/it]

 69%|██████▉   | 7463/10740 [37:04:19<17:56:06, 19.70s/it]

 69%|██████▉   | 7464/10740 [37:04:38<17:54:45, 19.68s/it]

 70%|██████▉   | 7465/10740 [37:04:51<15:55:31, 17.51s/it]

 70%|██████▉   | 7466/10740 [37:05:06<15:20:56, 16.88s/it]

 70%|██████▉   | 7467/10740 [37:05:26<16:15:04, 17.87s/it]

 70%|██████▉   | 7468/10740 [37:05:42<15:46:45, 17.36s/it]

 70%|██████▉   | 7469/10740 [37:06:00<15:43:29, 17.31s/it]

 70%|██████▉   | 7470/10740 [37:06:18<16:08:44, 17.78s/it]

 70%|██████▉   | 7471/10740 [37:06:39<17:00:50, 18.74s/it]

 70%|██████▉   | 7472/10740 [37:06:54<15:59:41, 17.62s/it]

 70%|██████▉   | 7473/10740 [37:07:15<16:47:15, 18.50s/it]

 70%|██████▉   | 7474/10740 [37:07:37<17:41:49, 19.51s/it]

 70%|██████▉   | 7475/10740 [37:07:53<16:43:36, 18.44s/it]

 70%|██████▉   | 7476/10740 [37:08:12<16:48:54, 18.55s/it]

 70%|██████▉   | 7477/10740 [37:08:32<17:12:04, 18.98s/it]

 70%|██████▉   | 7478/10740 [37:08:46<15:54:18, 17.55s/it]

 70%|██████▉   | 7479/10740 [37:09:00<15:04:58, 16.65s/it]

 70%|██████▉   | 7480/10740 [37:09:14<14:21:45, 15.86s/it]

 70%|██████▉   | 7481/10740 [37:09:34<15:18:48, 16.92s/it]

 70%|██████▉   | 7482/10740 [37:09:54<16:18:37, 18.02s/it]

 70%|██████▉   | 7483/10740 [37:10:13<16:34:17, 18.32s/it]

 70%|██████▉   | 7484/10740 [37:10:30<16:07:52, 17.84s/it]

 70%|██████▉   | 7485/10740 [37:10:50<16:36:23, 18.37s/it]

 70%|██████▉   | 7486/10740 [37:11:06<15:56:05, 17.63s/it]

 70%|██████▉   | 7487/10740 [37:11:21<15:14:52, 16.87s/it]

 70%|██████▉   | 7488/10740 [37:11:40<16:02:10, 17.75s/it]

 70%|██████▉   | 7489/10740 [37:12:00<16:27:03, 18.22s/it]

 70%|██████▉   | 7490/10740 [37:12:22<17:35:21, 19.48s/it]

 70%|██████▉   | 7491/10740 [37:12:38<16:40:31, 18.48s/it]

 70%|██████▉   | 7492/10740 [37:13:00<17:23:59, 19.29s/it]

 70%|██████▉   | 7493/10740 [37:13:15<16:22:44, 18.16s/it]

 70%|██████▉   | 7494/10740 [37:13:36<17:10:52, 19.05s/it]

 70%|██████▉   | 7495/10740 [37:13:50<15:43:19, 17.44s/it]

 70%|██████▉   | 7496/10740 [37:14:02<14:09:45, 15.72s/it]

 70%|██████▉   | 7497/10740 [37:14:19<14:30:27, 16.10s/it]

 70%|██████▉   | 7498/10740 [37:14:34<14:20:22, 15.92s/it]
{'loss': 0.2251, 'learning_rate': 4.4109604691830326e-07, 'rewards/chosen': -4.583270072937012, 'rewards/rejected': -7.226154327392578, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6428842544555664, 'policy_logps/rejected': -331.9571838378906, 'policy_logps/chosen': -459.1533508300781, 'referece_logps/rejected': -259.69561767578125, 'referece_logps/chosen': -413.3206787109375, 'logits/rejected': 0.25642529129981995, 'logits/chosen': 0.2650149166584015, 'epoch': 4.19}


 70%|██████▉   | 7500/10740 [37:15:07<14:11:53, 15.78s/it]
{'loss': 0.2249, 'learning_rate': 4.40595983422141e-07, 'rewards/chosen': -3.6188411712646484, 'rewards/rejected': -6.626162052154541, 'rewards/accuracies': 1.0, 'rewards/margins': 3.007321357727051, 'policy_logps/rejected': -409.3209533691406, 'policy_logps/chosen': -420.4856262207031, 'referece_logps/rejected': -343.059326171875, 'referece_logps/chosen': -384.29718017578125, 'logits/rejected': 0.07260145246982574, 'logits/chosen': -0.023200809955596924, 'epoch': 4.19}

 70%|██████▉   | 7501/10740 [37:15:38<18:20:14, 20.38s/it]


 70%|██████▉   | 7503/10740 [37:16:21<18:44:05, 20.84s/it]

 70%|██████▉   | 7504/10740 [37:16:37<17:35:19, 19.57s/it]

 70%|██████▉   | 7505/10740 [37:16:52<16:19:57, 18.18s/it]

 70%|██████▉   | 7506/10740 [37:17:09<15:57:05, 17.76s/it]

 70%|██████▉   | 7507/10740 [37:17:24<15:05:58, 16.81s/it]

 70%|██████▉   | 7508/10740 [37:17:35<13:44:03, 15.30s/it]

 70%|██████▉   | 7509/10740 [37:17:51<13:55:32, 15.52s/it]

 70%|██████▉   | 7510/10740 [37:18:13<15:31:34, 17.30s/it]

 70%|██████▉   | 7511/10740 [37:18:31<15:49:27, 17.64s/it]

 70%|██████▉   | 7512/10740 [37:18:54<17:05:36, 19.06s/it]

 70%|██████▉   | 7513/10740 [37:19:12<16:51:57, 18.82s/it]

 70%|██████▉   | 7514/10740 [37:19:28<16:10:26, 18.05s/it]

 70%|██████▉   | 7515/10740 [37:19:45<15:55:41, 17.78s/it]

 70%|██████▉   | 7516/10740 [37:20:07<17:02:12, 19.02s/it]

 70%|██████▉   | 7517/10740 [37:20:28<17:37:07, 19.68s/it]

 70%|███████   | 7518/10740 [37:20:46<17:11:12, 19.20s/it]

 70%|███████   | 7519/10740 [37:21:04<16:50:51, 18.83s/it]

 70%|███████   | 7520/10740 [37:21:24<17:02:49, 19.06s/it]

 70%|███████   | 7521/10740 [37:21:38<15:41:18, 17.55s/it]

 70%|███████   | 7522/10740 [37:21:52<14:47:45, 16.55s/it]

 70%|███████   | 7523/10740 [37:22:11<15:21:45, 17.19s/it]

 70%|███████   | 7524/10740 [37:22:29<15:39:02, 17.52s/it]

 70%|███████   | 7525/10740 [37:22:51<16:50:33, 18.86s/it]

 70%|███████   | 7526/10740 [37:23:10<16:44:32, 18.75s/it]

 70%|███████   | 7527/10740 [37:23:26<16:00:38, 17.94s/it]

 70%|███████   | 7528/10740 [37:23:46<16:30:03, 18.49s/it]

 70%|███████   | 7529/10740 [37:23:58<14:51:55, 16.67s/it]

 70%|███████   | 7530/10740 [37:24:18<15:52:19, 17.80s/it]

 70%|███████   | 7531/10740 [37:24:40<16:57:40, 19.03s/it]

 70%|███████   | 7532/10740 [37:25:00<17:10:16, 19.27s/it]

 70%|███████   | 7533/10740 [37:25:20<17:14:30, 19.35s/it]

 70%|███████   | 7534/10740 [37:25:35<16:02:29, 18.01s/it]

 70%|███████   | 7535/10740 [37:25:55<16:40:30, 18.73s/it]

 70%|███████   | 7536/10740 [37:26:15<17:08:21, 19.26s/it]

 70%|███████   | 7537/10740 [37:26:36<17:35:54, 19.78s/it]

 70%|███████   | 7538/10740 [37:26:55<17:18:58, 19.47s/it]

 70%|███████   | 7539/10740 [37:27:11<16:26:37, 18.49s/it]

 70%|███████   | 7540/10740 [37:27:27<15:37:27, 17.58s/it]

 70%|███████   | 7541/10740 [37:27:47<16:17:33, 18.34s/it]

 70%|███████   | 7542/10740 [37:28:07<16:37:55, 18.72s/it]

 70%|███████   | 7543/10740 [37:28:25<16:38:48, 18.75s/it]

 70%|███████   | 7544/10740 [37:28:43<16:19:53, 18.40s/it]

 70%|███████   | 7545/10740 [37:29:03<16:44:29, 18.86s/it]

 70%|███████   | 7546/10740 [37:29:19<15:55:22, 17.95s/it]

 70%|███████   | 7547/10740 [37:29:32<14:35:05, 16.44s/it]

 70%|███████   | 7548/10740 [37:29:52<15:29:54, 17.48s/it]

 70%|███████   | 7549/10740 [37:30:04<14:13:15, 16.04s/it]

 70%|███████   | 7550/10740 [37:30:22<14:35:54, 16.47s/it]

 70%|███████   | 7551/10740 [37:30:41<15:16:30, 17.24s/it]

 70%|███████   | 7552/10740 [37:30:59<15:30:54, 17.52s/it]

 70%|███████   | 7553/10740 [37:31:16<15:17:41, 17.28s/it]

 70%|███████   | 7554/10740 [37:31:32<15:02:36, 17.00s/it]

 70%|███████   | 7555/10740 [37:31:51<15:33:32, 17.59s/it]

 70%|███████   | 7556/10740 [37:32:12<16:24:16, 18.55s/it]

 70%|███████   | 7557/10740 [37:32:32<16:47:05, 18.98s/it]

 70%|███████   | 7558/10740 [37:32:52<17:09:20, 19.41s/it]

 70%|███████   | 7559/10740 [37:33:06<15:40:50, 17.75s/it]

 70%|███████   | 7560/10740 [37:33:19<14:21:20, 16.25s/it]

 70%|███████   | 7561/10740 [37:33:36<14:38:55, 16.59s/it]
{'loss': 0.1425, 'learning_rate': 4.2544266762162707e-07, 'rewards/chosen': -3.5046865940093994, 'rewards/rejected': -7.450421333312988, 'rewards/accuracies': 1.0, 'rewards/margins': 3.945734739303589, 'policy_logps/rejected': -347.1609802246094, 'policy_logps/chosen': -422.63714599609375, 'referece_logps/rejected': -272.6567687988281, 'referece_logps/chosen': -387.59027099609375, 'logits/rejected': 0.24158287048339844, 'logits/chosen': 0.15842537581920624, 'epoch': 4.22}


 70%|███████   | 7563/10740 [37:34:11<14:57:33, 16.95s/it]
{'loss': 0.2581, 'learning_rate': 4.249491019062008e-07, 'rewards/chosen': -4.165175914764404, 'rewards/rejected': -8.145609855651855, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9804341793060303, 'policy_logps/rejected': -504.2759704589844, 'policy_logps/chosen': -499.8559875488281, 'referece_logps/rejected': -422.81988525390625, 'referece_logps/chosen': -458.2042236328125, 'logits/rejected': 0.3556520342826843, 'logits/chosen': 0.330689013004303, 'epoch': 4.23}


 70%|███████   | 7565/10740 [37:34:47<15:32:36, 17.62s/it]

 70%|███████   | 7566/10740 [37:35:07<16:06:35, 18.27s/it]

 70%|███████   | 7567/10740 [37:35:25<15:59:46, 18.15s/it]

 70%|███████   | 7568/10740 [37:35:43<16:04:27, 18.24s/it]

 70%|███████   | 7569/10740 [37:36:02<16:15:59, 18.47s/it]
{'loss': 0.1944, 'learning_rate': 4.234696607327165e-07, 'rewards/chosen': -2.967785596847534, 'rewards/rejected': -6.085424900054932, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1176395416259766, 'policy_logps/rejected': -259.935546875, 'policy_logps/chosen': -287.9696960449219, 'referece_logps/rejected': -199.081298828125, 'referece_logps/chosen': -258.2918395996094, 'logits/rejected': -0.371154248714447, 'logits/chosen': -0.6493405103683472, 'epoch': 4.23}


 70%|███████   | 7571/10740 [37:36:43<17:09:23, 19.49s/it]
{'loss': 0.1898, 'learning_rate': 4.229769329305261e-07, 'rewards/chosen': -4.1215009689331055, 'rewards/rejected': -8.151777267456055, 'rewards/accuracies': 1.0, 'rewards/margins': 4.030276298522949, 'policy_logps/rejected': -433.021484375, 'policy_logps/chosen': -491.5762939453125, 'referece_logps/rejected': -351.50372314453125, 'referece_logps/chosen': -450.36126708984375, 'logits/rejected': 0.2892535626888275, 'logits/chosen': 0.3809468746185303, 'epoch': 4.23}


 71%|███████   | 7573/10740 [37:37:17<15:45:20, 17.91s/it]

 71%|███████   | 7574/10740 [37:37:33<15:08:45, 17.22s/it]

 71%|███████   | 7575/10740 [37:37:48<14:32:47, 16.55s/it]

 71%|███████   | 7576/10740 [37:38:09<15:46:46, 17.95s/it]

 71%|███████   | 7577/10740 [37:38:23<14:44:54, 16.79s/it]
{'loss': 0.2538, 'learning_rate': 4.215000098004923e-07, 'rewards/chosen': -4.3819708824157715, 'rewards/rejected': -7.380505084991455, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9985339641571045, 'policy_logps/rejected': -557.4231567382812, 'policy_logps/chosen': -409.5437927246094, 'referece_logps/rejected': -483.61810302734375, 'referece_logps/chosen': -365.72406005859375, 'logits/rejected': 0.031768910586833954, 'logits/chosen': 0.18016961216926575, 'epoch': 4.23}


 71%|███████   | 7579/10740 [37:38:53<14:24:59, 16.42s/it]
{'loss': 0.1767, 'learning_rate': 4.21008122779682e-07, 'rewards/chosen': -4.389642715454102, 'rewards/rejected': -8.051605224609375, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6619620323181152, 'policy_logps/rejected': -399.96710205078125, 'policy_logps/chosen': -451.76312255859375, 'referece_logps/rejected': -319.4510498046875, 'referece_logps/chosen': -407.86669921875, 'logits/rejected': 0.34373101592063904, 'logits/chosen': 0.314712256193161, 'epoch': 4.23}


 71%|███████   | 7581/10740 [37:39:23<13:57:13, 15.90s/it]

 71%|███████   | 7582/10740 [37:39:45<15:20:09, 17.48s/it]

 71%|███████   | 7583/10740 [37:40:02<15:14:58, 17.39s/it]

 71%|███████   | 7584/10740 [37:40:17<14:40:46, 16.74s/it]
{'loss': 0.2386, 'learning_rate': 4.197793271817033e-07, 'rewards/chosen': -4.208617210388184, 'rewards/rejected': -5.4583234786987305, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2497060298919678, 'policy_logps/rejected': -358.1606140136719, 'policy_logps/chosen': -329.27752685546875, 'referece_logps/rejected': -303.577392578125, 'referece_logps/chosen': -287.1913757324219, 'logits/rejected': -0.29152804613113403, 'logits/chosen': -0.3575957715511322, 'epoch': 4.24}


 71%|███████   | 7586/10740 [37:40:58<16:21:26, 18.67s/it]
{'loss': 0.1478, 'learning_rate': 4.1928817819353025e-07, 'rewards/chosen': -5.045820236206055, 'rewards/rejected': -8.515569686889648, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4697489738464355, 'policy_logps/rejected': -423.10552978515625, 'policy_logps/chosen': -471.94189453125, 'referece_logps/rejected': -337.9498596191406, 'referece_logps/chosen': -421.48370361328125, 'logits/rejected': 0.419139564037323, 'logits/chosen': 0.29932311177253723, 'epoch': 4.24}


 71%|███████   | 7588/10740 [37:41:27<14:21:17, 16.40s/it]

 71%|███████   | 7589/10740 [37:41:43<14:23:23, 16.44s/it]

 71%|███████   | 7590/10740 [37:42:03<15:16:34, 17.46s/it]
{'loss': 0.2904, 'learning_rate': 4.183065142017502e-07, 'rewards/chosen': -4.883112907409668, 'rewards/rejected': -9.537201881408691, 'rewards/accuracies': 0.75, 'rewards/margins': 4.654088973999023, 'policy_logps/rejected': -374.14837646484375, 'policy_logps/chosen': -302.89471435546875, 'referece_logps/rejected': -278.7763977050781, 'referece_logps/chosen': -254.06353759765625, 'logits/rejected': -0.24219200015068054, 'logits/chosen': -0.20454874634742737, 'epoch': 4.24}

 71%|███████   | 7591/10740 [37:42:18<14:38:11, 16.73s/it]

 71%|███████   | 7592/10740 [37:42:40<16:02:41, 18.35s/it]

 71%|███████   | 7593/10740 [37:42:58<15:56:32, 18.24s/it]

 71%|███████   | 7594/10740 [37:43:14<15:20:23, 17.55s/it]


 71%|███████   | 7596/10740 [37:43:54<16:20:21, 18.71s/it]

 71%|███████   | 7597/10740 [37:44:14<16:40:37, 19.10s/it]

 71%|███████   | 7598/10740 [37:44:34<16:48:47, 19.26s/it]
{'loss': 0.2535, 'learning_rate': 4.1634572715478167e-07, 'rewards/chosen': -4.08435583114624, 'rewards/rejected': -8.187004089355469, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1026482582092285, 'policy_logps/rejected': -344.70526123046875, 'policy_logps/chosen': -344.49200439453125, 'referece_logps/rejected': -262.835205078125, 'referece_logps/chosen': -303.6484680175781, 'logits/rejected': -0.27261993288993835, 'logits/chosen': -0.001958440989255905, 'epoch': 4.24}

 71%|███████   | 7599/10740 [37:44:51<16:13:34, 18.60s/it]


 71%|███████   | 7601/10740 [37:45:22<14:43:22, 16.89s/it]

 71%|███████   | 7602/10740 [37:45:36<13:53:28, 15.94s/it]

 71%|███████   | 7603/10740 [37:45:47<12:38:14, 14.50s/it]

 71%|███████   | 7604/10740 [37:45:57<11:38:57, 13.37s/it]

 71%|███████   | 7605/10740 [37:46:18<13:28:40, 15.48s/it]

 71%|███████   | 7606/10740 [37:46:29<12:26:58, 14.30s/it]

 71%|███████   | 7607/10740 [37:46:40<11:30:21, 13.22s/it]
{'loss': 0.1845, 'learning_rate': 4.141439032789296e-07, 'rewards/chosen': -3.2354278564453125, 'rewards/rejected': -8.178836822509766, 'rewards/accuracies': 1.0, 'rewards/margins': 4.943408012390137, 'policy_logps/rejected': -424.34002685546875, 'policy_logps/chosen': -432.552490234375, 'referece_logps/rejected': -342.5516662597656, 'referece_logps/chosen': -400.1982116699219, 'logits/rejected': 0.24580280482769012, 'logits/chosen': 0.21064110100269318, 'epoch': 4.25}

 71%|███████   | 7608/10740 [37:46:51<10:50:21, 12.46s/it]


 71%|███████   | 7610/10740 [37:47:31<14:15:48, 16.41s/it]

 71%|███████   | 7611/10740 [37:47:51<15:15:55, 17.56s/it]

 71%|███████   | 7612/10740 [37:48:08<14:59:06, 17.25s/it]

 71%|███████   | 7613/10740 [37:48:28<15:44:28, 18.12s/it]

 71%|███████   | 7614/10740 [37:48:46<15:34:44, 17.94s/it]

 71%|███████   | 7615/10740 [37:49:01<14:55:40, 17.20s/it]
{'loss': 0.2409, 'learning_rate': 4.1219034937786524e-07, 'rewards/chosen': -2.5951614379882812, 'rewards/rejected': -6.377979755401611, 'rewards/accuracies': 0.875, 'rewards/margins': 3.78281831741333, 'policy_logps/rejected': -399.63018798828125, 'policy_logps/chosen': -297.38238525390625, 'referece_logps/rejected': -335.8504333496094, 'referece_logps/chosen': -271.4307861328125, 'logits/rejected': -0.6213529109954834, 'logits/chosen': -0.42593756318092346, 'epoch': 4.25}


 71%|███████   | 7617/10740 [37:49:38<15:18:22, 17.64s/it]

 71%|███████   | 7618/10740 [37:49:58<15:52:56, 18.31s/it]
{'loss': 0.0981, 'learning_rate': 4.114586481894361e-07, 'rewards/chosen': -4.703780174255371, 'rewards/rejected': -8.94930648803711, 'rewards/accuracies': 1.0, 'rewards/margins': 4.245526313781738, 'policy_logps/rejected': -297.236083984375, 'policy_logps/chosen': -354.51141357421875, 'referece_logps/rejected': -207.74301147460938, 'referece_logps/chosen': -307.4736328125, 'logits/rejected': 0.4646154046058655, 'logits/chosen': 0.43913742899894714, 'epoch': 4.26}

 71%|███████   | 7619/10740 [37:50:11<14:34:03, 16.80s/it]

 71%|███████   | 7620/10740 [37:50:27<14:21:27, 16.57s/it]

 71%|███████   | 7621/10740 [37:50:47<15:17:01, 17.64s/it]

 71%|███████   | 7622/10740 [37:51:07<15:48:50, 18.26s/it]

 71%|███████   | 7623/10740 [37:51:25<15:42:14, 18.14s/it]


 71%|███████   | 7625/10740 [37:52:01<15:58:05, 18.45s/it]
{'loss': 0.1359, 'learning_rate': 4.097532199755801e-07, 'rewards/chosen': -3.6304218769073486, 'rewards/rejected': -7.514876365661621, 'rewards/accuracies': 1.0, 'rewards/margins': 3.884453773498535, 'policy_logps/rejected': -404.5153503417969, 'policy_logps/chosen': -406.4632568359375, 'referece_logps/rejected': -329.3665771484375, 'referece_logps/chosen': -370.1590576171875, 'logits/rejected': 0.058400511741638184, 'logits/chosen': -0.06424526125192642, 'epoch': 4.26}


 71%|███████   | 7627/10740 [37:52:41<16:41:49, 19.31s/it]

 71%|███████   | 7628/10740 [37:52:54<14:48:55, 17.14s/it]

 71%|███████   | 7629/10740 [37:53:14<15:34:58, 18.03s/it]
{'loss': 0.2203, 'learning_rate': 4.087798701368463e-07, 'rewards/chosen': -3.2407758235931396, 'rewards/rejected': -8.23013687133789, 'rewards/accuracies': 1.0, 'rewards/margins': 4.98936128616333, 'policy_logps/rejected': -423.31011962890625, 'policy_logps/chosen': -311.183349609375, 'referece_logps/rejected': -341.0087585449219, 'referece_logps/chosen': -278.7756042480469, 'logits/rejected': -0.010784877464175224, 'logits/chosen': 0.07353251427412033, 'epoch': 4.26}

 71%|███████   | 7630/10740 [37:53:31<15:22:29, 17.80s/it]

 71%|███████   | 7631/10740 [37:53:49<15:22:23, 17.80s/it]

 71%|███████   | 7632/10740 [37:54:05<14:55:17, 17.28s/it]


 71%|███████   | 7634/10740 [37:54:41<15:40:13, 18.16s/it]

 71%|███████   | 7635/10740 [37:54:58<15:11:08, 17.61s/it]

 71%|███████   | 7636/10740 [37:55:14<14:54:53, 17.30s/it]

 71%|███████   | 7637/10740 [37:55:26<13:31:57, 15.70s/it]

 71%|███████   | 7638/10740 [37:55:46<14:32:25, 16.87s/it]

 71%|███████   | 7639/10740 [37:56:05<15:02:12, 17.46s/it]
{'loss': 0.3074, 'learning_rate': 4.063502627394527e-07, 'rewards/chosen': -3.3467512130737305, 'rewards/rejected': -6.697081089019775, 'rewards/accuracies': 1.0, 'rewards/margins': 3.350330352783203, 'policy_logps/rejected': -351.4117431640625, 'policy_logps/chosen': -454.0410461425781, 'referece_logps/rejected': -284.44091796875, 'referece_logps/chosen': -420.57354736328125, 'logits/rejected': -0.6510492563247681, 'logits/chosen': -0.6423927545547485, 'epoch': 4.27}

 71%|███████   | 7640/10740 [37:56:27<16:17:33, 18.92s/it]


 71%|███████   | 7642/10740 [37:56:57<14:43:43, 17.12s/it]

 71%|███████   | 7643/10740 [37:57:19<15:58:30, 18.57s/it]
{'loss': 0.1917, 'learning_rate': 4.0537993037271414e-07, 'rewards/chosen': -4.787100315093994, 'rewards/rejected': -8.115583419799805, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3284828662872314, 'policy_logps/rejected': -541.9598388671875, 'policy_logps/chosen': -436.80316162109375, 'referece_logps/rejected': -460.8040466308594, 'referece_logps/chosen': -388.9321594238281, 'logits/rejected': -0.43004441261291504, 'logits/chosen': -0.4324986934661865, 'epoch': 4.27}

 71%|███████   | 7644/10740 [37:57:35<15:26:47, 17.96s/it]


 71%|███████   | 7646/10740 [37:58:13<15:42:04, 18.27s/it]

 71%|███████   | 7647/10740 [37:58:32<15:54:49, 18.52s/it]

 71%|███████   | 7648/10740 [37:58:46<14:42:06, 17.12s/it]

 71%|███████   | 7649/10740 [37:58:59<13:36:43, 15.85s/it]
{'loss': 0.2085, 'learning_rate': 4.039260547279555e-07, 'rewards/chosen': -3.9995272159576416, 'rewards/rejected': -6.5141754150390625, 'rewards/accuracies': 0.75, 'rewards/margins': 2.514648675918579, 'policy_logps/rejected': -356.2037353515625, 'policy_logps/chosen': -335.4408264160156, 'referece_logps/rejected': -291.0619812011719, 'referece_logps/chosen': -295.44561767578125, 'logits/rejected': -0.40931642055511475, 'logits/chosen': -0.44586610794067383, 'epoch': 4.27}


 71%|███████   | 7651/10740 [37:59:37<15:04:43, 17.57s/it]

 71%|███████   | 7652/10740 [37:59:50<14:00:16, 16.33s/it]

 71%|███████▏  | 7653/10740 [38:00:11<15:03:31, 17.56s/it]

 71%|███████▏  | 7654/10740 [38:00:30<15:36:32, 18.21s/it]

 71%|███████▏  | 7655/10740 [38:00:49<15:43:01, 18.34s/it]

 71%|███████▏  | 7656/10740 [38:01:08<15:50:27, 18.49s/it]

 71%|███████▏  | 7657/10740 [38:01:27<15:57:20, 18.63s/it]

 71%|███████▏  | 7658/10740 [38:01:47<16:19:04, 19.06s/it]
{'loss': 0.1926, 'learning_rate': 4.0174890221685954e-07, 'rewards/chosen': -3.700641393661499, 'rewards/rejected': -6.975543022155762, 'rewards/accuracies': 1.0, 'rewards/margins': 3.274902105331421, 'policy_logps/rejected': -386.5615539550781, 'policy_logps/chosen': -382.268798828125, 'referece_logps/rejected': -316.8061218261719, 'referece_logps/chosen': -345.2624206542969, 'logits/rejected': -0.6134847402572632, 'logits/chosen': -0.7081100344657898, 'epoch': 4.28}


 71%|███████▏  | 7660/10740 [38:02:31<17:36:30, 20.58s/it]

 71%|███████▏  | 7661/10740 [38:02:51<17:21:02, 20.29s/it]

 71%|███████▏  | 7662/10740 [38:03:10<17:12:40, 20.13s/it]

 71%|███████▏  | 7663/10740 [38:03:27<16:17:09, 19.05s/it]

 71%|███████▏  | 7664/10740 [38:03:48<16:46:52, 19.64s/it]

 71%|███████▏  | 7665/10740 [38:04:06<16:25:10, 19.22s/it]

 71%|███████▏  | 7666/10740 [38:04:23<15:48:35, 18.52s/it]

 71%|███████▏  | 7667/10740 [38:04:43<16:06:34, 18.87s/it]

 71%|███████▏  | 7668/10740 [38:05:01<15:49:59, 18.55s/it]

 71%|███████▏  | 7669/10740 [38:05:19<15:42:32, 18.41s/it]
{'loss': 0.2969, 'learning_rate': 3.990939250083417e-07, 'rewards/chosen': -5.424495697021484, 'rewards/rejected': -7.070347785949707, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6458511352539062, 'policy_logps/rejected': -479.05975341796875, 'policy_logps/chosen': -353.8161315917969, 'referece_logps/rejected': -408.35626220703125, 'referece_logps/chosen': -299.5711669921875, 'logits/rejected': -0.1274707019329071, 'logits/chosen': -0.09547106176614761, 'epoch': 4.28}


 71%|███████▏  | 7671/10740 [38:05:55<15:44:55, 18.47s/it]
{'loss': 0.2778, 'learning_rate': 3.986119115243476e-07, 'rewards/chosen': -4.912293910980225, 'rewards/rejected': -8.166973114013672, 'rewards/accuracies': 0.75, 'rewards/margins': 3.254678249359131, 'policy_logps/rejected': -551.4998779296875, 'policy_logps/chosen': -482.4977111816406, 'referece_logps/rejected': -469.83013916015625, 'referece_logps/chosen': -433.374755859375, 'logits/rejected': -0.03672827035188675, 'logits/chosen': -0.16444724798202515, 'epoch': 4.29}


 71%|███████▏  | 7673/10740 [38:06:29<14:57:34, 17.56s/it]

 71%|███████▏  | 7674/10740 [38:06:49<15:30:05, 18.20s/it]
{'loss': 0.1896, 'learning_rate': 3.978893015860361e-07, 'rewards/chosen': -2.8314504623413086, 'rewards/rejected': -7.021241188049316, 'rewards/accuracies': 1.0, 'rewards/margins': 4.189790725708008, 'policy_logps/rejected': -236.15728759765625, 'policy_logps/chosen': -311.9906311035156, 'referece_logps/rejected': -165.94488525390625, 'referece_logps/chosen': -283.6761474609375, 'logits/rejected': 0.17417308688163757, 'logits/chosen': -0.009625628590583801, 'epoch': 4.29}


 71%|███████▏  | 7676/10740 [38:07:29<16:25:50, 19.31s/it]

 71%|███████▏  | 7677/10740 [38:07:45<15:30:28, 18.23s/it]
{'loss': 0.1612, 'learning_rate': 3.971671845186968e-07, 'rewards/chosen': -4.679108142852783, 'rewards/rejected': -9.615955352783203, 'rewards/accuracies': 1.0, 'rewards/margins': 4.936848163604736, 'policy_logps/rejected': -489.6199951171875, 'policy_logps/chosen': -439.13702392578125, 'referece_logps/rejected': -393.4604797363281, 'referece_logps/chosen': -392.345947265625, 'logits/rejected': 0.1604032665491104, 'logits/chosen': 0.1868293583393097, 'epoch': 4.29}


 71%|███████▏  | 7679/10740 [38:08:21<15:22:51, 18.09s/it]
{'loss': 0.1214, 'learning_rate': 3.966860472495973e-07, 'rewards/chosen': -2.9606144428253174, 'rewards/rejected': -7.343220233917236, 'rewards/accuracies': 1.0, 'rewards/margins': 4.38260555267334, 'policy_logps/rejected': -343.0649108886719, 'policy_logps/chosen': -393.5273742675781, 'referece_logps/rejected': -269.6327209472656, 'referece_logps/chosen': -363.92120361328125, 'logits/rejected': -0.19239962100982666, 'logits/chosen': -0.23121997714042664, 'epoch': 4.29}


 72%|███████▏  | 7681/10740 [38:08:55<15:12:00, 17.89s/it]
{'loss': 0.1986, 'learning_rate': 3.9620512947202746e-07, 'rewards/chosen': -3.6725966930389404, 'rewards/rejected': -6.412432670593262, 'rewards/accuracies': 0.875, 'rewards/margins': 2.739835739135742, 'policy_logps/rejected': -294.0517883300781, 'policy_logps/chosen': -350.954833984375, 'referece_logps/rejected': -229.9274444580078, 'referece_logps/chosen': -314.2289123535156, 'logits/rejected': -0.010659866034984589, 'logits/chosen': -0.13028667867183685, 'epoch': 4.29}


 72%|███████▏  | 7683/10740 [38:09:27<14:30:04, 17.08s/it]
{'loss': 0.1705, 'learning_rate': 3.9572443136094925e-07, 'rewards/chosen': -4.116711616516113, 'rewards/rejected': -8.642146110534668, 'rewards/accuracies': 1.0, 'rewards/margins': 4.525434494018555, 'policy_logps/rejected': -381.195556640625, 'policy_logps/chosen': -438.21539306640625, 'referece_logps/rejected': -294.77410888671875, 'referece_logps/chosen': -397.0483093261719, 'logits/rejected': -0.5259843468666077, 'logits/chosen': -0.5908541083335876, 'epoch': 4.29}

 72%|███████▏  | 7684/10740 [38:09:40<13:24:17, 15.79s/it]

 72%|███████▏  | 7685/10740 [38:09:54<12:57:43, 15.27s/it]

 72%|███████▏  | 7686/10740 [38:10:12<13:35:47, 16.03s/it]


 72%|███████▏  | 7688/10740 [38:10:51<15:05:27, 17.80s/it]

 72%|███████▏  | 7689/10740 [38:11:11<15:35:26, 18.40s/it]

 72%|███████▏  | 7690/10740 [38:11:29<15:34:53, 18.39s/it]
{'loss': 0.1802, 'learning_rate': 3.9404372036994693e-07, 'rewards/chosen': -3.975904703140259, 'rewards/rejected': -8.145401000976562, 'rewards/accuracies': 1.0, 'rewards/margins': 4.169496536254883, 'policy_logps/rejected': -254.06614685058594, 'policy_logps/chosen': -415.67205810546875, 'referece_logps/rejected': -172.61212158203125, 'referece_logps/chosen': -375.9130554199219, 'logits/rejected': -0.40160638093948364, 'logits/chosen': -0.38345229625701904, 'epoch': 4.3}


 72%|███████▏  | 7692/10740 [38:11:57<13:40:44, 16.16s/it]
{'loss': 0.1962, 'learning_rate': 3.935640129211004e-07, 'rewards/chosen': -4.176604270935059, 'rewards/rejected': -9.050119400024414, 'rewards/accuracies': 0.875, 'rewards/margins': 4.8735151290893555, 'policy_logps/rejected': -480.8426818847656, 'policy_logps/chosen': -406.71588134765625, 'referece_logps/rejected': -390.3415222167969, 'referece_logps/chosen': -364.9498596191406, 'logits/rejected': -0.02604876458644867, 'logits/chosen': -0.09727340936660767, 'epoch': 4.3}

 72%|███████▏  | 7693/10740 [38:12:11<12:58:43, 15.33s/it]

 72%|███████▏  | 7694/10740 [38:12:30<14:03:53, 16.62s/it]


 72%|███████▏  | 7696/10740 [38:13:06<14:46:46, 17.48s/it]

 72%|███████▏  | 7697/10740 [38:13:19<13:52:36, 16.42s/it]

 72%|███████▏  | 7698/10740 [38:13:41<15:14:34, 18.04s/it]
{'loss': 0.1448, 'learning_rate': 3.921262150363852e-07, 'rewards/chosen': -4.351243495941162, 'rewards/rejected': -6.994810104370117, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6435675621032715, 'policy_logps/rejected': -452.24810791015625, 'policy_logps/chosen': -386.317138671875, 'referece_logps/rejected': -382.29998779296875, 'referece_logps/chosen': -342.80474853515625, 'logits/rejected': -0.0722278282046318, 'logits/chosen': -0.08822977542877197, 'epoch': 4.3}


 72%|███████▏  | 7700/10740 [38:14:19<15:46:30, 18.68s/it]

 72%|███████▏  | 7701/10740 [38:14:33<14:33:56, 17.25s/it]
{'loss': 0.1062, 'learning_rate': 3.914080621826037e-07, 'rewards/chosen': -3.3172619342803955, 'rewards/rejected': -6.328218460083008, 'rewards/accuracies': 1.0, 'rewards/margins': 3.010956287384033, 'policy_logps/rejected': -333.50592041015625, 'policy_logps/chosen': -498.6018981933594, 'referece_logps/rejected': -270.22369384765625, 'referece_logps/chosen': -465.4293212890625, 'logits/rejected': -0.5904296040534973, 'logits/chosen': -0.7727047801017761, 'epoch': 4.3}

 72%|███████▏  | 7702/10740 [38:14:51<14:32:42, 17.24s/it]

 72%|███████▏  | 7703/10740 [38:15:08<14:42:12, 17.43s/it]

 72%|███████▏  | 7704/10740 [38:15:26<14:49:26, 17.58s/it]

 72%|███████▏  | 7705/10740 [38:15:46<15:27:25, 18.33s/it]

 72%|███████▏  | 7706/10740 [38:16:04<15:20:27, 18.20s/it]

 72%|███████▏  | 7707/10740 [38:16:16<13:43:44, 16.30s/it]

 72%|███████▏  | 7708/10740 [38:16:31<13:24:20, 15.92s/it]

 72%|███████▏  | 7709/10740 [38:16:52<14:42:28, 17.47s/it]

 72%|███████▏  | 7710/10740 [38:17:09<14:35:52, 17.34s/it]

 72%|███████▏  | 7711/10740 [38:17:29<15:11:37, 18.06s/it]

 72%|███████▏  | 7712/10740 [38:17:45<14:34:47, 17.33s/it]


 72%|███████▏  | 7714/10740 [38:18:24<15:15:36, 18.15s/it]
{'loss': 0.1349, 'learning_rate': 3.8830183072671554e-07, 'rewards/chosen': -4.317090034484863, 'rewards/rejected': -11.066485404968262, 'rewards/accuracies': 1.0, 'rewards/margins': 6.749395847320557, 'policy_logps/rejected': -454.757080078125, 'policy_logps/chosen': -356.6150207519531, 'referece_logps/rejected': -344.09222412109375, 'referece_logps/chosen': -313.4441223144531, 'logits/rejected': -0.5078532695770264, 'logits/chosen': -0.3504214286804199, 'epoch': 4.31}

 72%|███████▏  | 7715/10740 [38:18:45<16:01:49, 19.08s/it]


 72%|███████▏  | 7717/10740 [38:19:18<14:41:19, 17.49s/it]
{'loss': 0.2286, 'learning_rate': 3.8758634159646684e-07, 'rewards/chosen': -5.565003395080566, 'rewards/rejected': -9.797616958618164, 'rewards/accuracies': 0.875, 'rewards/margins': 4.2326130867004395, 'policy_logps/rejected': -429.017578125, 'policy_logps/chosen': -349.4855041503906, 'referece_logps/rejected': -331.04144287109375, 'referece_logps/chosen': -293.83544921875, 'logits/rejected': -0.08952348679304123, 'logits/chosen': -0.2465520203113556, 'epoch': 4.31}

 72%|███████▏  | 7718/10740 [38:19:37<15:04:07, 17.95s/it]


 72%|███████▏  | 7720/10740 [38:20:12<14:39:37, 17.48s/it]

 72%|███████▏  | 7721/10740 [38:20:30<14:51:27, 17.72s/it]

 72%|███████▏  | 7722/10740 [38:20:46<14:24:21, 17.18s/it]

 72%|███████▏  | 7723/10740 [38:20:58<13:02:01, 15.55s/it]
{'loss': 0.1712, 'learning_rate': 3.8615686783529956e-07, 'rewards/chosen': -4.965704441070557, 'rewards/rejected': -8.363353729248047, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3976497650146484, 'policy_logps/rejected': -322.4179382324219, 'policy_logps/chosen': -482.4778747558594, 'referece_logps/rejected': -238.78439331054688, 'referece_logps/chosen': -432.8207702636719, 'logits/rejected': 0.1033681333065033, 'logits/chosen': -0.013998553156852722, 'epoch': 4.31}


 72%|███████▏  | 7725/10740 [38:21:38<15:06:53, 18.05s/it]
{'loss': 0.2041, 'learning_rate': 3.8568082299481787e-07, 'rewards/chosen': -4.250372886657715, 'rewards/rejected': -7.653154373168945, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4027817249298096, 'policy_logps/rejected': -521.6321411132812, 'policy_logps/chosen': -372.452392578125, 'referece_logps/rejected': -445.10064697265625, 'referece_logps/chosen': -329.94866943359375, 'logits/rejected': -0.25911474227905273, 'logits/chosen': -0.17467865347862244, 'epoch': 4.32}

 72%|███████▏  | 7726/10740 [38:21:53<14:21:32, 17.15s/it]

 72%|███████▏  | 7727/10740 [38:22:09<14:01:11, 16.75s/it]

 72%|███████▏  | 7728/10740 [38:22:23<13:18:57, 15.92s/it]

 72%|███████▏  | 7729/10740 [38:22:37<12:40:41, 15.16s/it]

 72%|███████▏  | 7730/10740 [38:22:53<13:04:17, 15.63s/it]

 72%|███████▏  | 7731/10740 [38:23:10<13:13:23, 15.82s/it]


 72%|███████▏  | 7733/10740 [38:23:46<14:24:20, 17.25s/it]
{'loss': 0.1036, 'learning_rate': 3.8377888031694726e-07, 'rewards/chosen': -4.353434085845947, 'rewards/rejected': -7.242037296295166, 'rewards/accuracies': 1.0, 'rewards/margins': 2.888603448867798, 'policy_logps/rejected': -572.8670043945312, 'policy_logps/chosen': -588.3765258789062, 'referece_logps/rejected': -500.4466247558594, 'referece_logps/chosen': -544.84228515625, 'logits/rejected': -0.16328857839107513, 'logits/chosen': -0.2773314118385315, 'epoch': 4.32}

 72%|███████▏  | 7734/10740 [38:24:03<14:20:27, 17.17s/it]


 72%|███████▏  | 7736/10740 [38:24:42<15:15:37, 18.29s/it]

 72%|███████▏  | 7737/10740 [38:24:56<14:13:56, 17.06s/it]

 72%|███████▏  | 7738/10740 [38:25:17<15:01:16, 18.01s/it]

 72%|███████▏  | 7739/10740 [38:25:36<15:27:36, 18.55s/it]
{'loss': 0.2265, 'learning_rate': 3.8235477666927985e-07, 'rewards/chosen': -4.260054588317871, 'rewards/rejected': -7.2642083168029785, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0041542053222656, 'policy_logps/rejected': -390.24237060546875, 'policy_logps/chosen': -333.7955017089844, 'referece_logps/rejected': -317.6003112792969, 'referece_logps/chosen': -291.1949462890625, 'logits/rejected': -0.8562883138656616, 'logits/chosen': -0.7744807004928589, 'epoch': 4.32}

 72%|███████▏  | 7740/10740 [38:25:56<15:40:48, 18.82s/it]

 72%|███████▏  | 7741/10740 [38:26:16<15:54:43, 19.10s/it]


 72%|███████▏  | 7743/10740 [38:26:52<15:18:44, 18.39s/it]
{'loss': 0.2627, 'learning_rate': 3.814064974765966e-07, 'rewards/chosen': -3.8441474437713623, 'rewards/rejected': -7.37857723236084, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5344293117523193, 'policy_logps/rejected': -423.5018310546875, 'policy_logps/chosen': -464.9194641113281, 'referece_logps/rejected': -349.7160339355469, 'referece_logps/chosen': -426.47796630859375, 'logits/rejected': 0.18501964211463928, 'logits/chosen': 0.05518593639135361, 'epoch': 4.33}

 72%|███████▏  | 7744/10740 [38:27:13<15:48:22, 18.99s/it]


 72%|███████▏  | 7746/10740 [38:27:51<15:39:40, 18.83s/it]

 72%|███████▏  | 7747/10740 [38:28:10<15:50:59, 19.06s/it]

 72%|███████▏  | 7748/10740 [38:28:32<16:32:24, 19.90s/it]
{'loss': 0.1815, 'learning_rate': 3.802224145556019e-07, 'rewards/chosen': -3.8293890953063965, 'rewards/rejected': -7.4823102951049805, 'rewards/accuracies': 1.0, 'rewards/margins': 3.652920722961426, 'policy_logps/rejected': -416.29852294921875, 'policy_logps/chosen': -385.178955078125, 'referece_logps/rejected': -341.47540283203125, 'referece_logps/chosen': -346.8850402832031, 'logits/rejected': -0.04471026360988617, 'logits/chosen': -0.06976450979709625, 'epoch': 4.33}


 72%|███████▏  | 7750/10740 [38:29:12<16:37:57, 20.03s/it]
{'loss': 0.1852, 'learning_rate': 3.7974917582850587e-07, 'rewards/chosen': -3.8389341831207275, 'rewards/rejected': -8.224077224731445, 'rewards/accuracies': 0.875, 'rewards/margins': 4.3851423263549805, 'policy_logps/rejected': -409.7342529296875, 'policy_logps/chosen': -316.0806884765625, 'referece_logps/rejected': -327.49346923828125, 'referece_logps/chosen': -277.6913146972656, 'logits/rejected': -0.04628715664148331, 'logits/chosen': -0.04118764400482178, 'epoch': 4.33}

 72%|███████▏  | 7751/10740 [38:29:30<16:00:15, 19.28s/it]


 72%|███████▏  | 7753/10740 [38:30:09<16:09:48, 19.48s/it]
{'loss': 0.1971, 'learning_rate': 3.7903974089163457e-07, 'rewards/chosen': -4.474979400634766, 'rewards/rejected': -9.353309631347656, 'rewards/accuracies': 0.875, 'rewards/margins': 4.878330230712891, 'policy_logps/rejected': -456.121337890625, 'policy_logps/chosen': -352.6119384765625, 'referece_logps/rejected': -362.5882873535156, 'referece_logps/chosen': -307.8621826171875, 'logits/rejected': 0.11191807687282562, 'logits/chosen': 0.09897109866142273, 'epoch': 4.33}


 72%|███████▏  | 7755/10740 [38:30:42<15:04:36, 18.18s/it]
{'loss': 0.1623, 'learning_rate': 3.785670666204989e-07, 'rewards/chosen': -3.443962335586548, 'rewards/rejected': -7.2386884689331055, 'rewards/accuracies': 0.875, 'rewards/margins': 3.794726848602295, 'policy_logps/rejected': -436.3394470214844, 'policy_logps/chosen': -383.9508056640625, 'referece_logps/rejected': -363.95257568359375, 'referece_logps/chosen': -349.51116943359375, 'logits/rejected': 0.28086814284324646, 'logits/chosen': 0.21547052264213562, 'epoch': 4.33}

 72%|███████▏  | 7756/10740 [38:31:02<15:22:58, 18.56s/it]


 72%|███████▏  | 7758/10740 [38:31:37<14:42:37, 17.76s/it]
{'loss': 0.2013, 'learning_rate': 3.7785847917387094e-07, 'rewards/chosen': -4.250472545623779, 'rewards/rejected': -9.927578926086426, 'rewards/accuracies': 1.0, 'rewards/margins': 5.6771063804626465, 'policy_logps/rejected': -379.38531494140625, 'policy_logps/chosen': -328.3956298828125, 'referece_logps/rejected': -280.10955810546875, 'referece_logps/chosen': -285.89093017578125, 'logits/rejected': 0.08139123022556305, 'logits/chosen': 0.05510510504245758, 'epoch': 4.33}

 72%|███████▏  | 7759/10740 [38:31:51<13:57:07, 16.85s/it]


 72%|███████▏  | 7761/10740 [38:32:30<15:00:42, 18.14s/it]
{'loss': 0.163, 'learning_rate': 3.7715040099488613e-07, 'rewards/chosen': -4.021237850189209, 'rewards/rejected': -7.1348090171813965, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1135709285736084, 'policy_logps/rejected': -386.96673583984375, 'policy_logps/chosen': -429.04150390625, 'referece_logps/rejected': -315.6186828613281, 'referece_logps/chosen': -388.8291015625, 'logits/rejected': -0.2719261050224304, 'logits/chosen': -0.6038538217544556, 'epoch': 4.34}

 72%|███████▏  | 7762/10740 [38:32:43<13:38:53, 16.50s/it]


 72%|███████▏  | 7764/10740 [38:33:25<15:30:53, 18.77s/it]
{'loss': 0.2222, 'learning_rate': 3.7644283266315724e-07, 'rewards/chosen': -3.604931592941284, 'rewards/rejected': -7.116572856903076, 'rewards/accuracies': 0.875, 'rewards/margins': 3.51164174079895, 'policy_logps/rejected': -336.59576416015625, 'policy_logps/chosen': -399.1744384765625, 'referece_logps/rejected': -265.4300537109375, 'referece_logps/chosen': -363.1251220703125, 'logits/rejected': -0.006965039297938347, 'logits/chosen': 0.056921228766441345, 'epoch': 4.34}

 72%|███████▏  | 7765/10740 [38:33:42<14:58:50, 18.13s/it]


 72%|███████▏  | 7767/10740 [38:34:13<13:44:22, 16.64s/it]
{'loss': 0.1956, 'learning_rate': 3.757357747578793e-07, 'rewards/chosen': -3.840766429901123, 'rewards/rejected': -8.390543937683105, 'rewards/accuracies': 0.875, 'rewards/margins': 4.549776554107666, 'policy_logps/rejected': -249.15982055664062, 'policy_logps/chosen': -274.3349609375, 'referece_logps/rejected': -165.25436401367188, 'referece_logps/chosen': -235.9272918701172, 'logits/rejected': -0.10234929621219635, 'logits/chosen': -0.20705311000347137, 'epoch': 4.34}

 72%|███████▏  | 7768/10740 [38:34:30<13:54:55, 16.86s/it]

 72%|███████▏  | 7769/10740 [38:34:50<14:33:29, 17.64s/it]

 72%|███████▏  | 7770/10740 [38:35:06<14:11:53, 17.21s/it]

 72%|███████▏  | 7771/10740 [38:35:25<14:46:36, 17.92s/it]

 72%|███████▏  | 7772/10740 [38:35:46<15:24:10, 18.68s/it]

 72%|███████▏  | 7773/10740 [38:36:05<15:32:10, 18.85s/it]

 72%|███████▏  | 7774/10740 [38:36:24<15:26:30, 18.74s/it]

 72%|███████▏  | 7775/10740 [38:36:37<14:15:08, 17.30s/it]

 72%|███████▏  | 7776/10740 [38:36:56<14:27:05, 17.55s/it]

 72%|███████▏  | 7777/10740 [38:37:13<14:26:21, 17.54s/it]

 72%|███████▏  | 7778/10740 [38:37:29<14:01:47, 17.05s/it]

 72%|███████▏  | 7779/10740 [38:37:45<13:52:04, 16.86s/it]

 72%|███████▏  | 7780/10740 [38:38:02<13:51:55, 16.86s/it]

 72%|███████▏  | 7781/10740 [38:38:18<13:39:52, 16.62s/it]

 72%|███████▏  | 7782/10740 [38:38:35<13:36:54, 16.57s/it]

 72%|███████▏  | 7783/10740 [38:38:54<14:19:28, 17.44s/it]

 72%|███████▏  | 7784/10740 [38:39:11<14:05:10, 17.16s/it]

 72%|███████▏  | 7785/10740 [38:39:23<12:49:39, 15.63s/it]

 72%|███████▏  | 7786/10740 [38:39:36<12:15:29, 14.94s/it]

 73%|███████▎  | 7787/10740 [38:39:52<12:21:45, 15.07s/it]

 73%|███████▎  | 7788/10740 [38:40:11<13:30:34, 16.48s/it]

 73%|███████▎  | 7789/10740 [38:40:31<14:16:02, 17.40s/it]

 73%|███████▎  | 7790/10740 [38:40:43<12:51:19, 15.69s/it]

 73%|███████▎  | 7791/10740 [38:40:59<12:58:40, 15.84s/it]


 73%|███████▎  | 7793/10740 [38:41:28<12:34:02, 15.35s/it]
{'loss': 0.2896, 'learning_rate': 3.696294068582941e-07, 'rewards/chosen': -3.111687660217285, 'rewards/rejected': -6.15570068359375, 'rewards/accuracies': 0.625, 'rewards/margins': 3.044013023376465, 'policy_logps/rejected': -358.33221435546875, 'policy_logps/chosen': -691.9093627929688, 'referece_logps/rejected': -296.77520751953125, 'referece_logps/chosen': -660.7924194335938, 'logits/rejected': 0.8470518589019775, 'logits/chosen': 0.49855825304985046, 'epoch': 4.35}

 73%|███████▎  | 7794/10740 [38:41:47<13:38:04, 16.66s/it]


 73%|███████▎  | 7796/10740 [38:42:20<13:30:19, 16.51s/it]
{'loss': 0.2663, 'learning_rate': 3.6892731285215697e-07, 'rewards/chosen': -3.6808695793151855, 'rewards/rejected': -7.119201183319092, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4383316040039062, 'policy_logps/rejected': -427.46051025390625, 'policy_logps/chosen': -401.9593811035156, 'referece_logps/rejected': -356.2685546875, 'referece_logps/chosen': -365.15069580078125, 'logits/rejected': -0.04159608483314514, 'logits/chosen': -0.09504992514848709, 'epoch': 4.36}


 73%|███████▎  | 7798/10740 [38:42:56<14:08:59, 17.31s/it]
{'loss': 0.1316, 'learning_rate': 3.6845953713396516e-07, 'rewards/chosen': -4.574503421783447, 'rewards/rejected': -7.592777252197266, 'rewards/accuracies': 0.75, 'rewards/margins': 3.0182735919952393, 'policy_logps/rejected': -435.2518310546875, 'policy_logps/chosen': -405.65802001953125, 'referece_logps/rejected': -359.3240661621094, 'referece_logps/chosen': -359.9129333496094, 'logits/rejected': -0.2773732841014862, 'logits/chosen': -0.24332799017429352, 'epoch': 4.36}

 73%|███████▎  | 7799/10740 [38:43:09<13:17:09, 16.26s/it]

 73%|███████▎  | 7800/10740 [38:43:29<13:59:55, 17.14s/it]

 73%|███████▎  | 7801/10740 [38:43:46<14:05:12, 17.26s/it]

 73%|███████▎  | 7802/10740 [38:44:05<14:30:30, 17.78s/it]


 73%|███████▎  | 7804/10740 [38:44:46<15:30:34, 19.02s/it]
{'loss': 0.2199, 'learning_rate': 3.6705758922335994e-07, 'rewards/chosen': -4.747664451599121, 'rewards/rejected': -8.03589153289795, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2882277965545654, 'policy_logps/rejected': -370.9627380371094, 'policy_logps/chosen': -326.3802490234375, 'referece_logps/rejected': -290.60382080078125, 'referece_logps/chosen': -278.90362548828125, 'logits/rejected': 0.5018491148948669, 'logits/chosen': 0.38540923595428467, 'epoch': 4.36}

 73%|███████▎  | 7805/10740 [38:45:03<15:05:01, 18.50s/it]


 73%|███████▎  | 7807/10740 [38:45:40<15:04:20, 18.50s/it]
{'loss': 0.1939, 'learning_rate': 3.663573921446882e-07, 'rewards/chosen': -3.0289876461029053, 'rewards/rejected': -5.872506618499756, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8435189723968506, 'policy_logps/rejected': -274.4324951171875, 'policy_logps/chosen': -333.35919189453125, 'referece_logps/rejected': -215.7073974609375, 'referece_logps/chosen': -303.0693359375, 'logits/rejected': -0.14075753092765808, 'logits/chosen': -0.22879815101623535, 'epoch': 4.36}

 73%|███████▎  | 7808/10740 [38:45:57<14:38:49, 17.98s/it]

 73%|███████▎  | 7809/10740 [38:46:15<14:46:37, 18.15s/it]

 73%|███████▎  | 7810/10740 [38:46:35<15:10:18, 18.64s/it]

 73%|███████▎  | 7811/10740 [38:46:52<14:55:32, 18.34s/it]

 73%|███████▎  | 7812/10740 [38:47:08<14:07:00, 17.36s/it]

 73%|███████▎  | 7813/10740 [38:47:19<12:45:22, 15.69s/it]

 73%|███████▎  | 7814/10740 [38:47:39<13:45:47, 16.93s/it]

 73%|███████▎  | 7815/10740 [38:47:53<13:06:56, 16.14s/it]

 73%|███████▎  | 7816/10740 [38:48:14<14:04:12, 17.32s/it]


 73%|███████▎  | 7818/10740 [38:48:54<15:16:53, 18.83s/it]

 73%|███████▎  | 7819/10740 [38:49:13<15:19:45, 18.89s/it]

 73%|███████▎  | 7820/10740 [38:49:30<14:53:23, 18.36s/it]

 73%|███████▎  | 7821/10740 [38:49:50<15:18:19, 18.88s/it]

 73%|███████▎  | 7822/10740 [38:50:07<14:43:08, 18.16s/it]

 73%|███████▎  | 7823/10740 [38:50:25<14:46:19, 18.23s/it]

 73%|███████▎  | 7824/10740 [38:50:45<15:05:36, 18.63s/it]

 73%|███████▎  | 7825/10740 [38:51:05<15:32:52, 19.20s/it]

 73%|███████▎  | 7826/10740 [38:51:23<15:12:30, 18.79s/it]

 73%|███████▎  | 7827/10740 [38:51:40<14:37:26, 18.07s/it]

 73%|███████▎  | 7828/10740 [38:51:55<14:06:34, 17.44s/it]

 73%|███████▎  | 7829/10740 [38:52:13<14:14:20, 17.61s/it]

 73%|███████▎  | 7830/10740 [38:52:32<14:26:09, 17.86s/it]

 73%|███████▎  | 7831/10740 [38:52:48<13:58:14, 17.29s/it]

 73%|███████▎  | 7832/10740 [38:53:04<13:47:26, 17.07s/it]

 73%|███████▎  | 7833/10740 [38:53:25<14:33:53, 18.04s/it]

 73%|███████▎  | 7834/10740 [38:53:43<14:42:46, 18.23s/it]

 73%|███████▎  | 7835/10740 [38:53:59<14:07:43, 17.51s/it]

 73%|███████▎  | 7836/10740 [38:54:13<13:13:24, 16.39s/it]

 73%|███████▎  | 7837/10740 [38:54:33<14:02:47, 17.42s/it]

 73%|███████▎  | 7838/10740 [38:54:47<13:12:04, 16.38s/it]

 73%|███████▎  | 7839/10740 [38:55:08<14:29:03, 17.97s/it]

 73%|███████▎  | 7840/10740 [38:55:25<14:04:19, 17.47s/it]

 73%|███████▎  | 7841/10740 [38:55:36<12:27:15, 15.47s/it]

 73%|███████▎  | 7842/10740 [38:55:47<11:30:35, 14.30s/it]

 73%|███████▎  | 7843/10740 [38:55:58<10:44:47, 13.35s/it]

 73%|███████▎  | 7844/10740 [38:56:13<10:58:05, 13.63s/it]

 73%|███████▎  | 7845/10740 [38:56:25<10:37:44, 13.22s/it]

 73%|███████▎  | 7846/10740 [38:56:43<11:47:26, 14.67s/it]

 73%|███████▎  | 7847/10740 [38:57:02<12:46:41, 15.90s/it]

 73%|███████▎  | 7848/10740 [38:57:18<12:57:35, 16.13s/it]

 73%|███████▎  | 7849/10740 [38:57:38<13:51:54, 17.27s/it]

 73%|███████▎  | 7850/10740 [38:57:56<13:52:46, 17.29s/it]

 73%|███████▎  | 7851/10740 [38:58:12<13:41:38, 17.06s/it]

 73%|███████▎  | 7852/10740 [38:58:33<14:36:41, 18.21s/it]

 73%|███████▎  | 7853/10740 [38:58:52<14:47:05, 18.44s/it]

 73%|███████▎  | 7854/10740 [38:59:07<14:04:32, 17.56s/it]

 73%|███████▎  | 7855/10740 [38:59:22<13:21:14, 16.66s/it]

 73%|███████▎  | 7856/10740 [38:59:34<12:06:30, 15.11s/it]

 73%|███████▎  | 7857/10740 [38:59:51<12:44:32, 15.91s/it]

 73%|███████▎  | 7858/10740 [39:00:08<12:56:11, 16.16s/it]

 73%|███████▎  | 7859/10740 [39:00:26<13:22:24, 16.71s/it]

 73%|███████▎  | 7860/10740 [39:00:45<13:52:05, 17.34s/it]

 73%|███████▎  | 7861/10740 [39:01:05<14:36:30, 18.27s/it]

 73%|███████▎  | 7862/10740 [39:01:26<15:17:25, 19.13s/it]

 73%|███████▎  | 7863/10740 [39:01:44<14:49:33, 18.55s/it]

 73%|███████▎  | 7864/10740 [39:02:01<14:29:21, 18.14s/it]

 73%|███████▎  | 7865/10740 [39:02:21<14:52:15, 18.62s/it]

 73%|███████▎  | 7866/10740 [39:02:35<13:56:00, 17.45s/it]

 73%|███████▎  | 7867/10740 [39:02:56<14:41:55, 18.42s/it]

 73%|███████▎  | 7868/10740 [39:03:16<15:01:30, 18.83s/it]

 73%|███████▎  | 7869/10740 [39:03:38<15:46:44, 19.79s/it]

 73%|███████▎  | 7870/10740 [39:03:51<14:09:42, 17.76s/it]

 73%|███████▎  | 7871/10740 [39:04:08<14:06:20, 17.70s/it]

 73%|███████▎  | 7872/10740 [39:04:30<15:07:11, 18.98s/it]

 73%|███████▎  | 7873/10740 [39:04:50<15:21:39, 19.29s/it]

 73%|███████▎  | 7874/10740 [39:05:03<13:48:17, 17.34s/it]
{'loss': 0.2026, 'learning_rate': 3.5085586006289127e-07, 'rewards/chosen': -3.422182559967041, 'rewards/rejected': -7.280955791473389, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8587729930877686, 'policy_logps/rejected': -360.8919677734375, 'policy_logps/chosen': -249.16165161132812, 'referece_logps/rejected': -288.0824279785156, 'referece_logps/chosen': -214.93983459472656, 'logits/rejected': -0.17462071776390076, 'logits/chosen': -0.0562489777803421, 'epoch': 4.4}


 73%|███████▎  | 7876/10740 [39:05:38<13:34:36, 17.07s/it]

 73%|███████▎  | 7877/10740 [39:05:54<13:08:55, 16.53s/it]

 73%|███████▎  | 7878/10740 [39:06:14<14:04:58, 17.71s/it]

 73%|███████▎  | 7879/10740 [39:06:35<14:51:36, 18.70s/it]

 73%|███████▎  | 7880/10740 [39:06:51<14:03:39, 17.70s/it]

 73%|███████▎  | 7881/10740 [39:07:03<12:51:28, 16.19s/it]

 73%|███████▎  | 7882/10740 [39:07:22<13:26:37, 16.93s/it]

 73%|███████▎  | 7883/10740 [39:07:39<13:31:32, 17.04s/it]

 73%|███████▎  | 7884/10740 [39:07:59<14:09:46, 17.85s/it]

 73%|███████▎  | 7885/10740 [39:08:19<14:37:33, 18.44s/it]

 73%|███████▎  | 7886/10740 [39:08:35<14:10:02, 17.87s/it]

 73%|███████▎  | 7887/10740 [39:08:51<13:40:17, 17.25s/it]

 73%|███████▎  | 7888/10740 [39:09:10<14:03:36, 17.75s/it]

 73%|███████▎  | 7889/10740 [39:09:27<13:52:54, 17.53s/it]

 73%|███████▎  | 7890/10740 [39:09:47<14:26:28, 18.24s/it]

 73%|███████▎  | 7891/10740 [39:10:06<14:37:39, 18.48s/it]

 73%|███████▎  | 7892/10740 [39:10:23<14:22:54, 18.18s/it]

 73%|███████▎  | 7893/10740 [39:10:37<13:23:27, 16.93s/it]

 74%|███████▎  | 7894/10740 [39:10:52<12:53:26, 16.31s/it]

 74%|███████▎  | 7895/10740 [39:11:10<13:12:45, 16.72s/it]

 74%|███████▎  | 7896/10740 [39:11:27<13:23:13, 16.95s/it]

 74%|███████▎  | 7897/10740 [39:11:42<12:48:11, 16.21s/it]
{'loss': 0.2486, 'learning_rate': 3.455952279661448e-07, 'rewards/chosen': -3.238217830657959, 'rewards/rejected': -6.2451324462890625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0069141387939453, 'policy_logps/rejected': -224.8662872314453, 'policy_logps/chosen': -382.571044921875, 'referece_logps/rejected': -162.41497802734375, 'referece_logps/chosen': -350.1888732910156, 'logits/rejected': 0.41200733184814453, 'logits/chosen': 0.6161770820617676, 'epoch': 4.41}

 74%|███████▎  | 7898/10740 [39:12:02<13:40:05, 17.31s/it]


 74%|███████▎  | 7900/10740 [39:12:41<14:39:03, 18.57s/it]

 74%|███████▎  | 7901/10740 [39:12:55<13:28:14, 17.08s/it]

 74%|███████▎  | 7902/10740 [39:13:11<13:13:02, 16.77s/it]
{'loss': 0.1419, 'learning_rate': 3.444557699482984e-07, 'rewards/chosen': -4.394721508026123, 'rewards/rejected': -6.847944736480713, 'rewards/accuracies': 1.0, 'rewards/margins': 2.45322322845459, 'policy_logps/rejected': -275.4140625, 'policy_logps/chosen': -272.0543212890625, 'referece_logps/rejected': -206.93460083007812, 'referece_logps/chosen': -228.1071014404297, 'logits/rejected': 0.0972447469830513, 'logits/chosen': 0.06100474298000336, 'epoch': 4.41}


 74%|███████▎  | 7904/10740 [39:13:45<12:57:45, 16.45s/it]

 74%|███████▎  | 7905/10740 [39:14:01<12:55:53, 16.42s/it]

 74%|███████▎  | 7906/10740 [39:14:19<13:22:21, 16.99s/it]

 74%|███████▎  | 7907/10740 [39:14:39<14:00:15, 17.80s/it]
{'loss': 0.181, 'learning_rate': 3.433178025140656e-07, 'rewards/chosen': -3.283736228942871, 'rewards/rejected': -7.1062331199646, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8224968910217285, 'policy_logps/rejected': -278.98486328125, 'policy_logps/chosen': -324.1505126953125, 'referece_logps/rejected': -207.92251586914062, 'referece_logps/chosen': -291.31317138671875, 'logits/rejected': -0.15396474301815033, 'logits/chosen': -0.1869000792503357, 'epoch': 4.42}


 74%|███████▎  | 7909/10740 [39:15:15<13:54:20, 17.68s/it]

 74%|███████▎  | 7910/10740 [39:15:35<14:30:15, 18.45s/it]

 74%|███████▎  | 7911/10740 [39:15:47<13:02:03, 16.59s/it]

 74%|███████▎  | 7912/10740 [39:16:04<13:09:19, 16.75s/it]

 74%|███████▎  | 7913/10740 [39:16:23<13:33:01, 17.26s/it]

 74%|███████▎  | 7914/10740 [39:16:39<13:14:34, 16.87s/it]

 74%|███████▎  | 7915/10740 [39:16:57<13:39:44, 17.41s/it]

 74%|███████▎  | 7916/10740 [39:17:12<13:06:36, 16.71s/it]
{'loss': 0.1769, 'learning_rate': 3.4127322566035857e-07, 'rewards/chosen': -4.513792991638184, 'rewards/rejected': -10.878406524658203, 'rewards/accuracies': 1.0, 'rewards/margins': 6.3646135330200195, 'policy_logps/rejected': -465.6312255859375, 'policy_logps/chosen': -507.4238586425781, 'referece_logps/rejected': -356.84716796875, 'referece_logps/chosen': -462.28594970703125, 'logits/rejected': -0.05205756798386574, 'logits/chosen': -0.09537407755851746, 'epoch': 4.42}


 74%|███████▎  | 7918/10740 [39:17:52<14:15:13, 18.18s/it]

 74%|███████▎  | 7919/10740 [39:18:09<13:58:46, 17.84s/it]

 74%|███████▎  | 7920/10740 [39:18:28<14:11:54, 18.13s/it]

 74%|███████▍  | 7921/10740 [39:18:43<13:28:41, 17.21s/it]

 74%|███████▍  | 7922/10740 [39:18:56<12:29:46, 15.96s/it]

 74%|███████▍  | 7923/10740 [39:19:11<12:20:26, 15.77s/it]
{'loss': 0.2254, 'learning_rate': 3.3968635356411023e-07, 'rewards/chosen': -4.404382705688477, 'rewards/rejected': -8.235727310180664, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8313441276550293, 'policy_logps/rejected': -343.93206787109375, 'policy_logps/chosen': -378.6131896972656, 'referece_logps/rejected': -261.5748291015625, 'referece_logps/chosen': -334.5693359375, 'logits/rejected': 0.09243687987327576, 'logits/chosen': 0.0538896881043911, 'epoch': 4.43}


 74%|███████▍  | 7925/10740 [39:19:48<13:32:01, 17.31s/it]

 74%|███████▍  | 7926/10740 [39:20:03<13:11:43, 16.88s/it]
{'loss': 0.2328, 'learning_rate': 3.390071659678085e-07, 'rewards/chosen': -2.8372905254364014, 'rewards/rejected': -7.745885848999023, 'rewards/accuracies': 1.0, 'rewards/margins': 4.908594608306885, 'policy_logps/rejected': -302.432373046875, 'policy_logps/chosen': -325.5834045410156, 'referece_logps/rejected': -224.97352600097656, 'referece_logps/chosen': -297.21044921875, 'logits/rejected': -0.7422472834587097, 'logits/chosen': -0.7937290072441101, 'epoch': 4.43}


 74%|███████▍  | 7928/10740 [39:20:39<13:25:22, 17.18s/it]

 74%|███████▍  | 7929/10740 [39:20:55<13:08:24, 16.83s/it]

 74%|███████▍  | 7930/10740 [39:21:12<13:06:54, 16.80s/it]

 74%|███████▍  | 7931/10740 [39:21:31<13:50:22, 17.74s/it]

 74%|███████▍  | 7932/10740 [39:21:45<12:48:06, 16.41s/it]

 74%|███████▍  | 7933/10740 [39:22:05<13:42:58, 17.59s/it]

 74%|███████▍  | 7934/10740 [39:22:25<14:12:14, 18.22s/it]

 74%|███████▍  | 7935/10740 [39:22:45<14:42:29, 18.88s/it]

 74%|███████▍  | 7936/10740 [39:23:04<14:43:20, 18.90s/it]

 74%|███████▍  | 7937/10740 [39:23:21<14:19:29, 18.40s/it]

 74%|███████▍  | 7938/10740 [39:23:40<14:16:10, 18.33s/it]

 74%|███████▍  | 7939/10740 [39:23:51<12:39:43, 16.27s/it]
{'loss': 0.1207, 'learning_rate': 3.3607027921369034e-07, 'rewards/chosen': -4.154231071472168, 'rewards/rejected': -7.103852272033691, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9496207237243652, 'policy_logps/rejected': -393.5174865722656, 'policy_logps/chosen': -497.3769836425781, 'referece_logps/rejected': -322.4789733886719, 'referece_logps/chosen': -455.8346862792969, 'logits/rejected': -0.037264853715896606, 'logits/chosen': -0.0065292418003082275, 'epoch': 4.44}


 74%|███████▍  | 7941/10740 [39:24:35<14:58:19, 19.26s/it]
{'loss': 0.1583, 'learning_rate': 3.35619355143681e-07, 'rewards/chosen': -3.0637729167938232, 'rewards/rejected': -7.256213665008545, 'rewards/accuracies': 0.875, 'rewards/margins': 4.192440986633301, 'policy_logps/rejected': -375.5694580078125, 'policy_logps/chosen': -423.7865295410156, 'referece_logps/rejected': -303.0072937011719, 'referece_logps/chosen': -393.1488037109375, 'logits/rejected': -0.08704723417758942, 'logits/chosen': -0.2523484528064728, 'epoch': 4.44}


 74%|███████▍  | 7943/10740 [39:25:12<14:28:39, 18.63s/it]

 74%|███████▍  | 7944/10740 [39:25:30<14:25:05, 18.56s/it]

 74%|███████▍  | 7945/10740 [39:25:43<13:04:18, 16.84s/it]
{'loss': 0.2002, 'learning_rate': 3.3471823229219663e-07, 'rewards/chosen': -3.2695729732513428, 'rewards/rejected': -6.028732776641846, 'rewards/accuracies': 0.875, 'rewards/margins': 2.759159564971924, 'policy_logps/rejected': -454.4014587402344, 'policy_logps/chosen': -580.5200805664062, 'referece_logps/rejected': -394.1141357421875, 'referece_logps/chosen': -547.8243408203125, 'logits/rejected': -0.21263150870800018, 'logits/chosen': -0.3154522776603699, 'epoch': 4.44}


 74%|███████▍  | 7947/10740 [39:26:16<13:04:43, 16.86s/it]

 74%|███████▍  | 7948/10740 [39:26:32<12:58:16, 16.73s/it]

 74%|███████▍  | 7949/10740 [39:26:52<13:42:39, 17.69s/it]

 74%|███████▍  | 7950/10740 [39:27:15<14:56:17, 19.27s/it]

 74%|███████▍  | 7951/10740 [39:27:37<15:39:27, 20.21s/it]

 74%|███████▍  | 7952/10740 [39:27:57<15:34:35, 20.11s/it]

 74%|███████▍  | 7953/10740 [39:28:17<15:31:35, 20.06s/it]

 74%|███████▍  | 7954/10740 [39:28:39<15:48:13, 20.42s/it]

 74%|███████▍  | 7955/10740 [39:29:00<16:08:06, 20.86s/it]

 74%|███████▍  | 7956/10740 [39:29:17<15:13:41, 19.69s/it]

 74%|███████▍  | 7957/10740 [39:29:31<13:52:41, 17.95s/it]

 74%|███████▍  | 7958/10740 [39:29:45<13:00:37, 16.84s/it]

 74%|███████▍  | 7959/10740 [39:30:03<13:16:21, 17.18s/it]

 74%|███████▍  | 7960/10740 [39:30:24<14:04:43, 18.23s/it]

 74%|███████▍  | 7961/10740 [39:30:45<14:47:40, 19.17s/it]

 74%|███████▍  | 7962/10740 [39:31:02<14:11:18, 18.39s/it]
{'loss': 0.175, 'learning_rate': 3.308992768524085e-07, 'rewards/chosen': -6.28524112701416, 'rewards/rejected': -8.6903715133667, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4051313400268555, 'policy_logps/rejected': -476.6393127441406, 'policy_logps/chosen': -398.71710205078125, 'referece_logps/rejected': -389.735595703125, 'referece_logps/chosen': -335.8646545410156, 'logits/rejected': 0.41679608821868896, 'logits/chosen': 0.44440963864326477, 'epoch': 4.45}

 74%|███████▍  | 7963/10740 [39:31:23<14:46:12, 19.15s/it]


 74%|███████▍  | 7965/10740 [39:31:50<12:28:23, 16.18s/it]

 74%|███████▍  | 7966/10740 [39:32:07<12:38:57, 16.42s/it]

 74%|███████▍  | 7967/10740 [39:32:20<11:40:41, 15.16s/it]

 74%|███████▍  | 7968/10740 [39:32:38<12:18:27, 15.98s/it]

 74%|███████▍  | 7969/10740 [39:32:49<11:12:05, 14.55s/it]

 74%|███████▍  | 7970/10740 [39:33:08<12:12:32, 15.87s/it]
{'loss': 0.1982, 'learning_rate': 3.2910820086104583e-07, 'rewards/chosen': -3.5664737224578857, 'rewards/rejected': -6.717523097991943, 'rewards/accuracies': 1.0, 'rewards/margins': 3.151049852371216, 'policy_logps/rejected': -393.3581848144531, 'policy_logps/chosen': -402.37640380859375, 'referece_logps/rejected': -326.1829528808594, 'referece_logps/chosen': -366.711669921875, 'logits/rejected': -0.8153911232948303, 'logits/chosen': -0.9236571788787842, 'epoch': 4.45}


 74%|███████▍  | 7972/10740 [39:33:44<12:56:58, 16.84s/it]

 74%|███████▍  | 7973/10740 [39:34:01<13:07:58, 17.09s/it]

 74%|███████▍  | 7974/10740 [39:34:20<13:31:29, 17.60s/it]

 74%|███████▍  | 7975/10740 [39:34:40<13:59:41, 18.22s/it]
{'loss': 0.1724, 'learning_rate': 3.27990760830421e-07, 'rewards/chosen': -3.8576722145080566, 'rewards/rejected': -7.205273628234863, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3476016521453857, 'policy_logps/rejected': -374.3181457519531, 'policy_logps/chosen': -345.4515380859375, 'referece_logps/rejected': -302.2654113769531, 'referece_logps/chosen': -306.87481689453125, 'logits/rejected': 0.06312388181686401, 'logits/chosen': 0.04933308809995651, 'epoch': 4.46}


 74%|███████▍  | 7977/10740 [39:35:12<13:25:34, 17.49s/it]

 74%|███████▍  | 7978/10740 [39:35:32<13:57:00, 18.18s/it]
{'loss': 0.1763, 'learning_rate': 3.273210301001027e-07, 'rewards/chosen': -4.150365829467773, 'rewards/rejected': -7.107689380645752, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9573233127593994, 'policy_logps/rejected': -306.3685302734375, 'policy_logps/chosen': -633.5908203125, 'referece_logps/rejected': -235.29164123535156, 'referece_logps/chosen': -592.087158203125, 'logits/rejected': -0.059239715337753296, 'logits/chosen': -0.5906878113746643, 'epoch': 4.46}


 74%|███████▍  | 7980/10740 [39:36:12<14:34:42, 19.02s/it]
{'loss': 0.196, 'learning_rate': 3.2687484882172665e-07, 'rewards/chosen': -5.862269401550293, 'rewards/rejected': -8.700183868408203, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8379151821136475, 'policy_logps/rejected': -453.1238098144531, 'policy_logps/chosen': -374.97308349609375, 'referece_logps/rejected': -366.12200927734375, 'referece_logps/chosen': -316.3503723144531, 'logits/rejected': -0.37258997559547424, 'logits/chosen': -0.6296297311782837, 'epoch': 4.46}

 74%|███████▍  | 7981/10740 [39:36:34<15:06:34, 19.72s/it]


 74%|███████▍  | 7983/10740 [39:37:06<13:38:28, 17.81s/it]

 74%|███████▍  | 7984/10740 [39:37:23<13:20:22, 17.42s/it]
{'loss': 0.1764, 'learning_rate': 3.259832210957929e-07, 'rewards/chosen': -4.3676438331604, 'rewards/rejected': -8.86209487915039, 'rewards/accuracies': 1.0, 'rewards/margins': 4.494451522827148, 'policy_logps/rejected': -414.3152770996094, 'policy_logps/chosen': -561.654541015625, 'referece_logps/rejected': -325.6943359375, 'referece_logps/chosen': -517.9780883789062, 'logits/rejected': -0.25286006927490234, 'logits/chosen': -0.48037561774253845, 'epoch': 4.46}


 74%|███████▍  | 7986/10740 [39:37:57<13:19:20, 17.42s/it]

 74%|███████▍  | 7987/10740 [39:38:15<13:27:16, 17.59s/it]

 74%|███████▍  | 7988/10740 [39:38:33<13:38:54, 17.85s/it]

 74%|███████▍  | 7989/10740 [39:38:53<14:02:13, 18.37s/it]
{'loss': 0.079, 'learning_rate': 3.2487006591841527e-07, 'rewards/chosen': -4.8243513107299805, 'rewards/rejected': -8.75639533996582, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9320437908172607, 'policy_logps/rejected': -460.2541809082031, 'policy_logps/chosen': -385.3576354980469, 'referece_logps/rejected': -372.69024658203125, 'referece_logps/chosen': -337.1141052246094, 'logits/rejected': 0.15889087319374084, 'logits/chosen': 0.12824328243732452, 'epoch': 4.46}


 74%|███████▍  | 7991/10740 [39:39:27<13:37:23, 17.84s/it]
{'loss': 0.2308, 'learning_rate': 3.2442523353882367e-07, 'rewards/chosen': -4.176814079284668, 'rewards/rejected': -8.228673934936523, 'rewards/accuracies': 0.875, 'rewards/margins': 4.0518598556518555, 'policy_logps/rejected': -408.0875244140625, 'policy_logps/chosen': -418.68280029296875, 'referece_logps/rejected': -325.80084228515625, 'referece_logps/chosen': -376.9146423339844, 'logits/rejected': -0.08421492576599121, 'logits/chosen': -0.19699665904045105, 'epoch': 4.46}


 74%|███████▍  | 7993/10740 [39:40:01<13:03:52, 17.12s/it]
{'loss': 0.2441, 'learning_rate': 3.239806469399541e-07, 'rewards/chosen': -3.0203773975372314, 'rewards/rejected': -9.690506935119629, 'rewards/accuracies': 1.0, 'rewards/margins': 6.67012882232666, 'policy_logps/rejected': -308.61627197265625, 'policy_logps/chosen': -283.9088134765625, 'referece_logps/rejected': -211.71124267578125, 'referece_logps/chosen': -253.7050018310547, 'logits/rejected': -0.3790714144706726, 'logits/chosen': -0.3779424726963043, 'epoch': 4.47}


 74%|███████▍  | 7995/10740 [39:40:43<14:24:36, 18.90s/it]

 74%|███████▍  | 7996/10740 [39:41:01<14:20:38, 18.82s/it]

 74%|███████▍  | 7997/10740 [39:41:23<15:01:51, 19.73s/it]

 74%|███████▍  | 7998/10740 [39:41:43<15:00:11, 19.70s/it]

 74%|███████▍  | 7999/10740 [39:41:55<13:09:06, 17.27s/it]

 74%|███████▍  | 8000/10740 [39:42:13<13:26:21, 17.66s/it]

 74%|███████▍  | 8001/10740 [39:42:39<15:21:02, 20.18s/it]

 75%|███████▍  | 8002/10740 [39:42:53<13:54:20, 18.28s/it]

 75%|███████▍  | 8003/10740 [39:43:13<14:19:51, 18.85s/it]
{'loss': 0.1016, 'learning_rate': 3.217614063143702e-07, 'rewards/chosen': -3.978566884994507, 'rewards/rejected': -9.220328330993652, 'rewards/accuracies': 1.0, 'rewards/margins': 5.241761207580566, 'policy_logps/rejected': -172.0194091796875, 'policy_logps/chosen': -549.8782958984375, 'referece_logps/rejected': -79.81612396240234, 'referece_logps/chosen': -510.0926208496094, 'logits/rejected': 0.24556735157966614, 'logits/chosen': -0.2463107407093048, 'epoch': 4.47}

 75%|███████▍  | 8004/10740 [39:43:34<14:52:42, 19.58s/it]

 75%|███████▍  | 8005/10740 [39:43:54<14:58:22, 19.71s/it]

 75%|███████▍  | 8006/10740 [39:44:14<14:59:08, 19.73s/it]


 75%|███████▍  | 8008/10740 [39:44:52<14:20:03, 18.89s/it]
{'loss': 0.1818, 'learning_rate': 3.206540980203776e-07, 'rewards/chosen': -2.1600635051727295, 'rewards/rejected': -6.374885559082031, 'rewards/accuracies': 1.0, 'rewards/margins': 4.214822769165039, 'policy_logps/rejected': -395.8240051269531, 'policy_logps/chosen': -304.9808349609375, 'referece_logps/rejected': -332.0751647949219, 'referece_logps/chosen': -283.3801574707031, 'logits/rejected': 0.13353773951530457, 'logits/chosen': 0.29782575368881226, 'epoch': 4.47}


 75%|███████▍  | 8010/10740 [39:45:29<14:03:41, 18.54s/it]

 75%|███████▍  | 8011/10740 [39:45:49<14:27:09, 19.07s/it]

 75%|███████▍  | 8012/10740 [39:46:09<14:42:48, 19.42s/it]

 75%|███████▍  | 8013/10740 [39:46:21<12:59:49, 17.16s/it]
{'loss': 0.153, 'learning_rate': 3.195483344304998e-07, 'rewards/chosen': -2.894134998321533, 'rewards/rejected': -6.62205171585083, 'rewards/accuracies': 1.0, 'rewards/margins': 3.727917432785034, 'policy_logps/rejected': -393.5981140136719, 'policy_logps/chosen': -394.2986145019531, 'referece_logps/rejected': -327.3775939941406, 'referece_logps/chosen': -365.35723876953125, 'logits/rejected': 0.6293856501579285, 'logits/chosen': 0.3774133622646332, 'epoch': 4.48}

 75%|███████▍  | 8014/10740 [39:46:34<12:04:39, 15.95s/it]


 75%|███████▍  | 8016/10740 [39:47:09<12:25:02, 16.41s/it]
{'loss': 0.175, 'learning_rate': 3.1888561878064224e-07, 'rewards/chosen': -2.8036866188049316, 'rewards/rejected': -6.558045864105225, 'rewards/accuracies': 0.875, 'rewards/margins': 3.754359245300293, 'policy_logps/rejected': -489.55517578125, 'policy_logps/chosen': -332.6471252441406, 'referece_logps/rejected': -423.9747314453125, 'referece_logps/chosen': -304.6102600097656, 'logits/rejected': -0.8968076109886169, 'logits/chosen': -0.7849851250648499, 'epoch': 4.48}

 75%|███████▍  | 8017/10740 [39:47:31<13:30:04, 17.85s/it]

 75%|███████▍  | 8018/10740 [39:47:51<14:00:00, 18.52s/it]

 75%|███████▍  | 8019/10740 [39:48:10<14:15:36, 18.87s/it]

 75%|███████▍  | 8020/10740 [39:48:23<12:42:43, 16.82s/it]


 75%|███████▍  | 8022/10740 [39:49:03<14:02:17, 18.59s/it]
{'loss': 0.236, 'learning_rate': 3.175618606464802e-07, 'rewards/chosen': -3.733525514602661, 'rewards/rejected': -6.849972724914551, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1164469718933105, 'policy_logps/rejected': -437.09808349609375, 'policy_logps/chosen': -415.54156494140625, 'referece_logps/rejected': -368.598388671875, 'referece_logps/chosen': -378.20623779296875, 'logits/rejected': 0.23118029534816742, 'logits/chosen': 0.17335212230682373, 'epoch': 4.48}

 75%|███████▍  | 8023/10740 [39:49:23<14:20:54, 19.01s/it]

 75%|███████▍  | 8024/10740 [39:49:36<13:02:49, 17.29s/it]

 75%|███████▍  | 8025/10740 [39:49:50<12:21:12, 16.38s/it]

 75%|███████▍  | 8026/10740 [39:50:06<12:12:45, 16.20s/it]

 75%|███████▍  | 8027/10740 [39:50:24<12:38:50, 16.78s/it]


 75%|███████▍  | 8029/10740 [39:50:58<12:37:47, 16.77s/it]
{'loss': 0.1721, 'learning_rate': 3.160203006185296e-07, 'rewards/chosen': -3.3087217807769775, 'rewards/rejected': -7.037586212158203, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7288646697998047, 'policy_logps/rejected': -530.3258056640625, 'policy_logps/chosen': -314.0611572265625, 'referece_logps/rejected': -459.949951171875, 'referece_logps/chosen': -280.97393798828125, 'logits/rejected': -0.9190231561660767, 'logits/chosen': -0.6086441874504089, 'epoch': 4.49}

 75%|███████▍  | 8030/10740 [39:51:14<12:31:24, 16.64s/it]


 75%|███████▍  | 8032/10740 [39:51:52<13:10:37, 17.52s/it]
{'loss': 0.1351, 'learning_rate': 3.153605647791333e-07, 'rewards/chosen': -3.2539143562316895, 'rewards/rejected': -8.232861518859863, 'rewards/accuracies': 1.0, 'rewards/margins': 4.97894811630249, 'policy_logps/rejected': -408.49090576171875, 'policy_logps/chosen': -473.78662109375, 'referece_logps/rejected': -326.1623229980469, 'referece_logps/chosen': -441.24749755859375, 'logits/rejected': -0.5405033826828003, 'logits/chosen': -0.4842959940433502, 'epoch': 4.49}

 75%|███████▍  | 8033/10740 [39:52:09<13:08:17, 17.47s/it]

 75%|███████▍  | 8034/10740 [39:52:20<11:47:35, 15.69s/it]

 75%|███████▍  | 8035/10740 [39:52:35<11:27:14, 15.24s/it]

 75%|███████▍  | 8036/10740 [39:52:55<12:29:07, 16.62s/it]

 75%|███████▍  | 8037/10740 [39:53:13<12:52:51, 17.16s/it]

 75%|███████▍  | 8038/10740 [39:53:32<13:24:53, 17.87s/it]


 75%|███████▍  | 8040/10740 [39:54:12<14:04:14, 18.76s/it]
{'loss': 0.1641, 'learning_rate': 3.136040105365377e-07, 'rewards/chosen': -3.7884457111358643, 'rewards/rejected': -8.142936706542969, 'rewards/accuracies': 1.0, 'rewards/margins': 4.354491233825684, 'policy_logps/rejected': -249.64309692382812, 'policy_logps/chosen': -232.64212036132812, 'referece_logps/rejected': -168.2137451171875, 'referece_logps/chosen': -194.7576904296875, 'logits/rejected': 0.061937861144542694, 'logits/chosen': 0.0019047893583774567, 'epoch': 4.49}


 75%|███████▍  | 8042/10740 [39:54:43<12:59:12, 17.33s/it]

 75%|███████▍  | 8043/10740 [39:55:00<12:46:41, 17.06s/it]

 75%|███████▍  | 8044/10740 [39:55:17<12:54:17, 17.23s/it]

 75%|███████▍  | 8045/10740 [39:55:35<13:03:55, 17.45s/it]
{'loss': 0.1279, 'learning_rate': 3.1250819244133174e-07, 'rewards/chosen': -3.8428759574890137, 'rewards/rejected': -9.04051399230957, 'rewards/accuracies': 1.0, 'rewards/margins': 5.19763708114624, 'policy_logps/rejected': -328.8392639160156, 'policy_logps/chosen': -377.2906494140625, 'referece_logps/rejected': -238.4341583251953, 'referece_logps/chosen': -338.86187744140625, 'logits/rejected': -0.4600605070590973, 'logits/chosen': -0.6338861584663391, 'epoch': 4.49}

 75%|███████▍  | 8046/10740 [39:55:51<12:36:52, 16.86s/it]

 75%|███████▍  | 8047/10740 [39:56:03<11:35:39, 15.50s/it]

 75%|███████▍  | 8048/10740 [39:56:23<12:32:39, 16.78s/it]

 75%|███████▍  | 8049/10740 [39:56:36<11:48:39, 15.80s/it]

 75%|███████▍  | 8050/10740 [39:56:52<11:51:58, 15.88s/it]

 75%|███████▍  | 8051/10740 [39:57:09<12:02:26, 16.12s/it]

 75%|███████▍  | 8052/10740 [39:57:23<11:34:55, 15.51s/it]

 75%|███████▍  | 8053/10740 [39:57:43<12:31:21, 16.78s/it]

 75%|███████▍  | 8054/10740 [39:58:03<13:15:09, 17.76s/it]

 75%|███████▌  | 8055/10740 [39:58:15<11:57:09, 16.03s/it]

 75%|███████▌  | 8056/10740 [39:58:35<12:46:25, 17.13s/it]


 75%|███████▌  | 8058/10740 [39:59:10<12:56:45, 17.38s/it]
{'loss': 0.2676, 'learning_rate': 3.096663875022143e-07, 'rewards/chosen': -4.578214168548584, 'rewards/rejected': -7.587339401245117, 'rewards/accuracies': 0.75, 'rewards/margins': 3.009124994277954, 'policy_logps/rejected': -392.3347473144531, 'policy_logps/chosen': -625.9222412109375, 'referece_logps/rejected': -316.46136474609375, 'referece_logps/chosen': -580.1400756835938, 'logits/rejected': 0.20364952087402344, 'logits/chosen': 0.1476740837097168, 'epoch': 4.5}


 75%|███████▌  | 8060/10740 [39:59:39<11:40:22, 15.68s/it]
{'loss': 0.2119, 'learning_rate': 3.0923012746285015e-07, 'rewards/chosen': -4.474088191986084, 'rewards/rejected': -6.848121166229248, 'rewards/accuracies': 0.75, 'rewards/margins': 2.374032974243164, 'policy_logps/rejected': -397.586181640625, 'policy_logps/chosen': -561.4661865234375, 'referece_logps/rejected': -329.10498046875, 'referece_logps/chosen': -516.725341796875, 'logits/rejected': 0.46139147877693176, 'logits/chosen': 0.22647452354431152, 'epoch': 4.5}

 75%|███████▌  | 8061/10740 [39:59:59<12:45:00, 17.13s/it]


 75%|███████▌  | 8063/10740 [40:00:32<12:27:07, 16.75s/it]

 75%|███████▌  | 8064/10740 [40:00:52<13:05:24, 17.61s/it]

 75%|███████▌  | 8065/10740 [40:01:06<12:18:38, 16.57s/it]

 75%|███████▌  | 8066/10740 [40:01:26<13:02:24, 17.56s/it]
{'loss': 0.1886, 'learning_rate': 3.079228558322633e-07, 'rewards/chosen': -4.453299045562744, 'rewards/rejected': -8.287384986877441, 'rewards/accuracies': 1.0, 'rewards/margins': 3.834085464477539, 'policy_logps/rejected': -368.9178161621094, 'policy_logps/chosen': -402.5571594238281, 'referece_logps/rejected': -286.0439758300781, 'referece_logps/chosen': -358.024169921875, 'logits/rejected': 0.19052881002426147, 'logits/chosen': 0.11360400915145874, 'epoch': 4.51}


 75%|███████▌  | 8068/10740 [40:02:07<14:02:29, 18.92s/it]
{'loss': 0.1885, 'learning_rate': 3.0748760197967914e-07, 'rewards/chosen': -5.1944122314453125, 'rewards/rejected': -6.802647590637207, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6082350015640259, 'policy_logps/rejected': -390.96258544921875, 'policy_logps/chosen': -535.1898803710938, 'referece_logps/rejected': -322.9361267089844, 'referece_logps/chosen': -483.2457580566406, 'logits/rejected': -0.40794336795806885, 'logits/chosen': -0.6651591062545776, 'epoch': 4.51}


 75%|███████▌  | 8070/10740 [40:02:39<12:35:38, 16.98s/it]

 75%|███████▌  | 8071/10740 [40:02:57<12:48:54, 17.29s/it]
{'loss': 0.1739, 'learning_rate': 3.06835193643008e-07, 'rewards/chosen': -4.884149074554443, 'rewards/rejected': -11.012508392333984, 'rewards/accuracies': 1.0, 'rewards/margins': 6.128358840942383, 'policy_logps/rejected': -412.1629638671875, 'policy_logps/chosen': -425.3590087890625, 'referece_logps/rejected': -302.0378723144531, 'referece_logps/chosen': -376.5175476074219, 'logits/rejected': -0.8123260140419006, 'logits/chosen': -0.9110930562019348, 'epoch': 4.51}

 75%|███████▌  | 8072/10740 [40:03:10<11:52:18, 16.02s/it]


 75%|███████▌  | 8074/10740 [40:03:38<10:57:10, 14.79s/it]
{'loss': 0.1507, 'learning_rate': 3.061833527116544e-07, 'rewards/chosen': -4.6416544914245605, 'rewards/rejected': -8.464120864868164, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8224661350250244, 'policy_logps/rejected': -590.8137817382812, 'policy_logps/chosen': -451.0628967285156, 'referece_logps/rejected': -506.17254638671875, 'referece_logps/chosen': -404.6463317871094, 'logits/rejected': 0.21088877320289612, 'logits/chosen': 0.25908035039901733, 'epoch': 4.51}


 75%|███████▌  | 8076/10740 [40:04:06<10:50:02, 14.64s/it]

 75%|███████▌  | 8077/10740 [40:04:23<11:13:01, 15.16s/it]
{'loss': 0.1804, 'learning_rate': 3.055320797191967e-07, 'rewards/chosen': -4.169616222381592, 'rewards/rejected': -6.523122310638428, 'rewards/accuracies': 1.0, 'rewards/margins': 2.353506088256836, 'policy_logps/rejected': -318.1945495605469, 'policy_logps/chosen': -319.78509521484375, 'referece_logps/rejected': -252.96328735351562, 'referece_logps/chosen': -278.08892822265625, 'logits/rejected': -0.16275706887245178, 'logits/chosen': -0.14514927566051483, 'epoch': 4.51}

 75%|███████▌  | 8078/10740 [40:04:43<12:29:30, 16.89s/it]

 75%|███████▌  | 8079/10740 [40:05:02<12:45:06, 17.25s/it]


 75%|███████▌  | 8081/10740 [40:05:44<14:19:46, 19.40s/it]

 75%|███████▌  | 8082/10740 [40:06:05<14:34:30, 19.74s/it]

 75%|███████▌  | 8083/10740 [40:06:25<14:35:33, 19.77s/it]
{'loss': 0.133, 'learning_rate': 3.04231239682959e-07, 'rewards/chosen': -2.3447136878967285, 'rewards/rejected': -5.938261985778809, 'rewards/accuracies': 1.0, 'rewards/margins': 3.593548059463501, 'policy_logps/rejected': -213.96275329589844, 'policy_logps/chosen': -271.2029113769531, 'referece_logps/rejected': -154.58013916015625, 'referece_logps/chosen': -247.7557830810547, 'logits/rejected': 0.4162123501300812, 'logits/chosen': 0.3412111699581146, 'epoch': 4.52}


 75%|███████▌  | 8085/10740 [40:07:03<14:20:27, 19.45s/it]
{'loss': 0.1884, 'learning_rate': 3.037981323822907e-07, 'rewards/chosen': -3.161569833755493, 'rewards/rejected': -8.090862274169922, 'rewards/accuracies': 1.0, 'rewards/margins': 4.929292678833008, 'policy_logps/rejected': -362.58221435546875, 'policy_logps/chosen': -348.2587890625, 'referece_logps/rejected': -281.6736145019531, 'referece_logps/chosen': -316.6430969238281, 'logits/rejected': 0.5910746455192566, 'logits/chosen': 0.443947970867157, 'epoch': 4.52}

 75%|███████▌  | 8086/10740 [40:07:19<13:44:25, 18.64s/it]


 75%|███████▌  | 8088/10740 [40:07:59<14:00:54, 19.03s/it]

 75%|███████▌  | 8089/10740 [40:08:11<12:25:33, 16.87s/it]

 75%|███████▌  | 8090/10740 [40:08:29<12:39:21, 17.19s/it]

 75%|███████▌  | 8091/10740 [40:08:49<13:20:07, 18.12s/it]
{'loss': 0.1929, 'learning_rate': 3.025003308204782e-07, 'rewards/chosen': -2.77703857421875, 'rewards/rejected': -8.59778118133545, 'rewards/accuracies': 1.0, 'rewards/margins': 5.820741653442383, 'policy_logps/rejected': -575.1493530273438, 'policy_logps/chosen': -521.4608764648438, 'referece_logps/rejected': -489.1715087890625, 'referece_logps/chosen': -493.69049072265625, 'logits/rejected': -0.03984019160270691, 'logits/chosen': -0.1882345974445343, 'epoch': 4.52}

 75%|███████▌  | 8092/10740 [40:09:03<12:17:11, 16.70s/it]

 75%|███████▌  | 8093/10740 [40:09:15<11:24:06, 15.51s/it]


 75%|███████▌  | 8095/10740 [40:09:49<12:06:12, 16.47s/it]
{'loss': 0.2659, 'learning_rate': 3.0163639830304335e-07, 'rewards/chosen': -4.180599212646484, 'rewards/rejected': -10.355057716369629, 'rewards/accuracies': 1.0, 'rewards/margins': 6.174457550048828, 'policy_logps/rejected': -487.1483154296875, 'policy_logps/chosen': -539.08642578125, 'referece_logps/rejected': -383.59771728515625, 'referece_logps/chosen': -497.28045654296875, 'logits/rejected': 0.3541344106197357, 'logits/chosen': 0.34954342246055603, 'epoch': 4.52}

 75%|███████▌  | 8096/10740 [40:10:09<12:47:43, 17.42s/it]

 75%|███████▌  | 8097/10740 [40:10:20<11:24:05, 15.53s/it]

 75%|███████▌  | 8098/10740 [40:10:38<11:54:00, 16.22s/it]

 75%|███████▌  | 8099/10740 [40:10:56<12:23:24, 16.89s/it]


 75%|███████▌  | 8101/10740 [40:11:37<13:48:53, 18.85s/it]
{'loss': 0.1368, 'learning_rate': 3.003424054556214e-07, 'rewards/chosen': -3.3505454063415527, 'rewards/rejected': -10.308072090148926, 'rewards/accuracies': 1.0, 'rewards/margins': 6.957527160644531, 'policy_logps/rejected': -402.30377197265625, 'policy_logps/chosen': -291.8468017578125, 'referece_logps/rejected': -299.2230529785156, 'referece_logps/chosen': -258.34136962890625, 'logits/rejected': 0.5455928444862366, 'logits/chosen': 0.4499324858188629, 'epoch': 4.53}

 75%|███████▌  | 8102/10740 [40:11:51<12:38:57, 17.26s/it]

 75%|███████▌  | 8103/10740 [40:12:05<11:52:57, 16.22s/it]


 75%|███████▌  | 8105/10740 [40:12:43<13:10:37, 18.00s/it]
{'loss': 0.19, 'learning_rate': 2.994810160072556e-07, 'rewards/chosen': -4.071987628936768, 'rewards/rejected': -7.0985107421875, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0265231132507324, 'policy_logps/rejected': -476.68634033203125, 'policy_logps/chosen': -381.98260498046875, 'referece_logps/rejected': -405.70123291015625, 'referece_logps/chosen': -341.2627258300781, 'logits/rejected': 0.10472308844327927, 'logits/chosen': 0.09733326733112335, 'epoch': 4.53}


 75%|███████▌  | 8107/10740 [40:13:19<13:06:00, 17.91s/it]
{'loss': 0.1597, 'learning_rate': 2.990507034882589e-07, 'rewards/chosen': -3.5587127208709717, 'rewards/rejected': -7.478628635406494, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9199156761169434, 'policy_logps/rejected': -377.6408386230469, 'policy_logps/chosen': -342.63775634765625, 'referece_logps/rejected': -302.85455322265625, 'referece_logps/chosen': -307.0506286621094, 'logits/rejected': -0.7385649085044861, 'logits/chosen': -0.7262157797813416, 'epoch': 4.53}

 75%|███████▌  | 8108/10740 [40:13:40<13:40:45, 18.71s/it]

 76%|███████▌  | 8109/10740 [40:14:02<14:23:17, 19.69s/it]

 76%|███████▌  | 8110/10740 [40:14:24<14:52:47, 20.37s/it]

 76%|███████▌  | 8111/10740 [40:14:39<13:39:36, 18.71s/it]


 76%|███████▌  | 8113/10740 [40:15:15<13:37:23, 18.67s/it]
{'loss': 0.2569, 'learning_rate': 2.97761296630359e-07, 'rewards/chosen': -3.5716888904571533, 'rewards/rejected': -7.422098636627197, 'rewards/accuracies': 0.875, 'rewards/margins': 3.850409746170044, 'policy_logps/rejected': -387.9679870605469, 'policy_logps/chosen': -342.6356506347656, 'referece_logps/rejected': -313.7469787597656, 'referece_logps/chosen': -306.9187927246094, 'logits/rejected': -0.38701701164245605, 'logits/chosen': -0.4021126925945282, 'epoch': 4.53}

 76%|███████▌  | 8114/10740 [40:15:30<12:49:27, 17.58s/it]

 76%|███████▌  | 8115/10740 [40:15:44<11:57:31, 16.40s/it]


 76%|███████▌  | 8117/10740 [40:16:21<12:44:08, 17.48s/it]
{'loss': 0.2504, 'learning_rate': 2.969029692044308e-07, 'rewards/chosen': -2.842036008834839, 'rewards/rejected': -5.6105852127075195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7685492038726807, 'policy_logps/rejected': -502.056884765625, 'policy_logps/chosen': -385.6954345703125, 'referece_logps/rejected': -445.95098876953125, 'referece_logps/chosen': -357.27508544921875, 'logits/rejected': 0.5089186429977417, 'logits/chosen': 0.5247842669487, 'epoch': 4.53}

 76%|███████▌  | 8118/10740 [40:16:35<11:52:31, 16.31s/it]

 76%|███████▌  | 8119/10740 [40:16:56<12:52:01, 17.67s/it]

 76%|███████▌  | 8120/10740 [40:17:08<11:42:46, 16.09s/it]

 76%|███████▌  | 8121/10740 [40:17:29<12:37:17, 17.35s/it]

 76%|███████▌  | 8122/10740 [40:17:42<11:45:57, 16.18s/it]

 76%|███████▌  | 8123/10740 [40:17:59<11:56:57, 16.44s/it]

 76%|███████▌  | 8124/10740 [40:18:19<12:44:39, 17.54s/it]


 76%|███████▌  | 8126/10740 [40:18:54<12:29:19, 17.20s/it]
{'loss': 0.1531, 'learning_rate': 2.949754753774957e-07, 'rewards/chosen': -4.278775215148926, 'rewards/rejected': -8.118062973022461, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8392884731292725, 'policy_logps/rejected': -498.3098449707031, 'policy_logps/chosen': -419.46722412109375, 'referece_logps/rejected': -417.12921142578125, 'referece_logps/chosen': -376.6794128417969, 'logits/rejected': -0.20301005244255066, 'logits/chosen': -0.18987584114074707, 'epoch': 4.54}

 76%|███████▌  | 8127/10740 [40:19:15<13:29:04, 18.58s/it]

 76%|███████▌  | 8128/10740 [40:19:36<14:00:35, 19.31s/it]

 76%|███████▌  | 8129/10740 [40:19:55<13:51:27, 19.11s/it]

 76%|███████▌  | 8130/10740 [40:20:11<13:07:42, 18.11s/it]

 76%|███████▌  | 8131/10740 [40:20:32<13:52:46, 19.15s/it]

 76%|███████▌  | 8132/10740 [40:20:52<13:57:23, 19.27s/it]

 76%|███████▌  | 8133/10740 [40:21:11<13:59:10, 19.31s/it]

 76%|███████▌  | 8134/10740 [40:21:22<12:06:42, 16.73s/it]

 76%|███████▌  | 8135/10740 [40:21:41<12:34:47, 17.39s/it]


 76%|███████▌  | 8137/10740 [40:22:16<12:44:12, 17.62s/it]
{'loss': 0.2288, 'learning_rate': 2.9262670461814485e-07, 'rewards/chosen': -4.885807037353516, 'rewards/rejected': -9.244206428527832, 'rewards/accuracies': 1.0, 'rewards/margins': 4.358399868011475, 'policy_logps/rejected': -359.123779296875, 'policy_logps/chosen': -383.1376953125, 'referece_logps/rejected': -266.6817626953125, 'referece_logps/chosen': -334.27960205078125, 'logits/rejected': -0.5455792546272278, 'logits/chosen': -0.7568404674530029, 'epoch': 4.55}

 76%|███████▌  | 8138/10740 [40:22:35<13:07:53, 18.17s/it]

 76%|███████▌  | 8139/10740 [40:22:49<12:07:36, 16.78s/it]

 76%|███████▌  | 8140/10740 [40:23:09<12:51:02, 17.79s/it]

 76%|███████▌  | 8141/10740 [40:23:25<12:30:37, 17.33s/it]

 76%|███████▌  | 8142/10740 [40:23:45<13:01:54, 18.06s/it]

 76%|███████▌  | 8143/10740 [40:24:04<13:16:08, 18.39s/it]

 76%|███████▌  | 8144/10740 [40:24:25<13:43:17, 19.03s/it]

 76%|███████▌  | 8145/10740 [40:24:41<13:09:51, 18.26s/it]

 76%|███████▌  | 8146/10740 [40:25:00<13:09:16, 18.26s/it]

 76%|███████▌  | 8147/10740 [40:25:20<13:33:00, 18.81s/it]

 76%|███████▌  | 8148/10740 [40:25:33<12:21:32, 17.17s/it]

 76%|███████▌  | 8149/10740 [40:25:44<10:57:10, 15.22s/it]


 76%|███████▌  | 8151/10740 [40:26:12<10:49:13, 15.05s/it]
{'loss': 0.1102, 'learning_rate': 2.896486224282806e-07, 'rewards/chosen': -3.7180335521698, 'rewards/rejected': -10.071979522705078, 'rewards/accuracies': 1.0, 'rewards/margins': 6.353945732116699, 'policy_logps/rejected': -554.0890502929688, 'policy_logps/chosen': -489.657470703125, 'referece_logps/rejected': -453.3692932128906, 'referece_logps/chosen': -452.47711181640625, 'logits/rejected': -0.2750789225101471, 'logits/chosen': -0.13061010837554932, 'epoch': 4.55}

 76%|███████▌  | 8152/10740 [40:26:28<10:55:32, 15.20s/it]


 76%|███████▌  | 8154/10740 [40:27:02<11:43:08, 16.31s/it]
{'loss': 0.2097, 'learning_rate': 2.890121076609564e-07, 'rewards/chosen': -3.8680105209350586, 'rewards/rejected': -8.255072593688965, 'rewards/accuracies': 1.0, 'rewards/margins': 4.387062072753906, 'policy_logps/rejected': -473.5612487792969, 'policy_logps/chosen': -398.0165710449219, 'referece_logps/rejected': -391.010498046875, 'referece_logps/chosen': -359.3364562988281, 'logits/rejected': 1.3257102966308594, 'logits/chosen': 1.402614951133728, 'epoch': 4.56}

 76%|███████▌  | 8155/10740 [40:27:23<12:36:29, 17.56s/it]

 76%|███████▌  | 8156/10740 [40:27:42<12:57:57, 18.06s/it]

 76%|███████▌  | 8157/10740 [40:28:02<13:18:39, 18.55s/it]

 76%|███████▌  | 8158/10740 [40:28:22<13:40:59, 19.08s/it]

 76%|███████▌  | 8159/10740 [40:28:39<13:13:15, 18.44s/it]


 76%|███████▌  | 8161/10740 [40:29:18<13:40:23, 19.09s/it]
{'loss': 0.234, 'learning_rate': 2.875291707499575e-07, 'rewards/chosen': -2.688227891921997, 'rewards/rejected': -5.428692817687988, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7404651641845703, 'policy_logps/rejected': -468.1382751464844, 'policy_logps/chosen': -420.1786804199219, 'referece_logps/rejected': -413.85137939453125, 'referece_logps/chosen': -393.29638671875, 'logits/rejected': 0.1250201016664505, 'logits/chosen': 0.1642436683177948, 'epoch': 4.56}


 76%|███████▌  | 8163/10740 [40:29:54<13:13:29, 18.47s/it]
{'loss': 0.1459, 'learning_rate': 2.871060574095148e-07, 'rewards/chosen': -4.518662929534912, 'rewards/rejected': -9.354735374450684, 'rewards/accuracies': 1.0, 'rewards/margins': 4.8360724449157715, 'policy_logps/rejected': -423.0451354980469, 'policy_logps/chosen': -394.926025390625, 'referece_logps/rejected': -329.497802734375, 'referece_logps/chosen': -349.7393493652344, 'logits/rejected': 0.5523289442062378, 'logits/chosen': 0.4674206078052521, 'epoch': 4.56}

 76%|███████▌  | 8164/10740 [40:30:15<13:36:44, 19.02s/it]

 76%|███████▌  | 8165/10740 [40:30:34<13:37:19, 19.04s/it]

 76%|███████▌  | 8166/10740 [40:30:52<13:26:20, 18.80s/it]

 76%|███████▌  | 8167/10740 [40:31:09<13:04:27, 18.29s/it]

 76%|███████▌  | 8168/10740 [40:31:29<13:23:02, 18.73s/it]

 76%|███████▌  | 8169/10740 [40:31:47<13:11:01, 18.46s/it]

 76%|███████▌  | 8170/10740 [40:32:05<13:04:00, 18.30s/it]

 76%|███████▌  | 8171/10740 [40:32:21<12:43:22, 17.83s/it]

 76%|███████▌  | 8172/10740 [40:32:42<13:20:40, 18.71s/it]

 76%|███████▌  | 8173/10740 [40:33:04<13:55:39, 19.53s/it]

 76%|███████▌  | 8174/10740 [40:33:23<13:56:38, 19.56s/it]


 76%|███████▌  | 8176/10740 [40:33:57<12:53:39, 18.10s/it]
{'loss': 0.1812, 'learning_rate': 2.8436214941300617e-07, 'rewards/chosen': -4.007483005523682, 'rewards/rejected': -6.239433765411377, 'rewards/accuracies': 0.875, 'rewards/margins': 2.231950521469116, 'policy_logps/rejected': -466.6444396972656, 'policy_logps/chosen': -354.454345703125, 'referece_logps/rejected': -404.2501220703125, 'referece_logps/chosen': -314.3795471191406, 'logits/rejected': 0.3756271004676819, 'logits/chosen': 0.37416574358940125, 'epoch': 4.57}

 76%|███████▌  | 8177/10740 [40:34:13<12:31:46, 17.60s/it]

 76%|███████▌  | 8178/10740 [40:34:30<12:27:08, 17.50s/it]

 76%|███████▌  | 8179/10740 [40:34:50<12:55:09, 18.16s/it]

 76%|███████▌  | 8180/10740 [40:35:04<11:53:19, 16.72s/it]

 76%|███████▌  | 8181/10740 [40:35:22<12:13:13, 17.19s/it]

 76%|███████▌  | 8182/10740 [40:35:40<12:23:35, 17.44s/it]


 76%|███████▌  | 8184/10740 [40:36:21<13:32:25, 19.07s/it]

 76%|███████▌  | 8185/10740 [40:36:39<13:13:42, 18.64s/it]

 76%|███████▌  | 8186/10740 [40:36:54<12:28:11, 17.58s/it]

 76%|███████▌  | 8187/10740 [40:37:15<13:05:55, 18.47s/it]

 76%|███████▌  | 8188/10740 [40:37:30<12:20:57, 17.42s/it]

 76%|███████▌  | 8189/10740 [40:37:40<10:55:58, 15.43s/it]

 76%|███████▋  | 8190/10740 [40:38:00<11:51:19, 16.74s/it]

 76%|███████▋  | 8191/10740 [40:38:20<12:27:34, 17.60s/it]

 76%|███████▋  | 8192/10740 [40:38:38<12:39:37, 17.89s/it]

 76%|███████▋  | 8193/10740 [40:38:55<12:17:25, 17.37s/it]

 76%|███████▋  | 8194/10740 [40:39:14<12:40:15, 17.92s/it]

 76%|███████▋  | 8195/10740 [40:39:27<11:34:32, 16.37s/it]

 76%|███████▋  | 8196/10740 [40:39:41<11:08:28, 15.77s/it]

 76%|███████▋  | 8197/10740 [40:39:58<11:30:22, 16.29s/it]

 76%|███████▋  | 8198/10740 [40:40:18<12:14:58, 17.35s/it]

 76%|███████▋  | 8199/10740 [40:40:36<12:20:11, 17.48s/it]

 76%|███████▋  | 8200/10740 [40:40:56<12:55:30, 18.32s/it]

 76%|███████▋  | 8201/10740 [40:41:09<11:43:02, 16.61s/it]

 76%|███████▋  | 8202/10740 [40:41:21<10:51:30, 15.40s/it]

 76%|███████▋  | 8203/10740 [40:41:41<11:42:35, 16.62s/it]

 76%|███████▋  | 8204/10740 [40:41:59<11:55:06, 16.92s/it]

 76%|███████▋  | 8205/10740 [40:42:19<12:42:24, 18.05s/it]

 76%|███████▋  | 8206/10740 [40:42:39<13:02:08, 18.52s/it]

 76%|███████▋  | 8207/10740 [40:42:57<12:57:44, 18.42s/it]

 76%|███████▋  | 8208/10740 [40:43:15<12:53:38, 18.33s/it]

 76%|███████▋  | 8209/10740 [40:43:35<13:09:16, 18.71s/it]
{'loss': 0.1822, 'learning_rate': 2.774463436965083e-07, 'rewards/chosen': -4.114871025085449, 'rewards/rejected': -7.988274574279785, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8734030723571777, 'policy_logps/rejected': -386.9913635253906, 'policy_logps/chosen': -283.54583740234375, 'referece_logps/rejected': -307.10858154296875, 'referece_logps/chosen': -242.3971405029297, 'logits/rejected': -0.5403960347175598, 'logits/chosen': -0.3848397731781006, 'epoch': 4.59}


 76%|███████▋  | 8211/10740 [40:44:10<12:53:48, 18.36s/it]

 76%|███████▋  | 8212/10740 [40:44:28<12:52:30, 18.33s/it]

 76%|███████▋  | 8213/10740 [40:44:48<13:09:16, 18.74s/it]

 76%|███████▋  | 8214/10740 [40:45:08<13:32:35, 19.30s/it]

 76%|███████▋  | 8215/10740 [40:45:24<12:43:32, 18.14s/it]

 76%|███████▋  | 8216/10740 [40:45:40<12:16:13, 17.50s/it]

 77%|███████▋  | 8217/10740 [40:45:58<12:27:44, 17.78s/it]

 77%|███████▋  | 8218/10740 [40:46:16<12:26:23, 17.76s/it]
{'loss': 0.2113, 'learning_rate': 2.755726068184832e-07, 'rewards/chosen': -2.7571585178375244, 'rewards/rejected': -5.8197784423828125, 'rewards/accuracies': 0.875, 'rewards/margins': 3.062619924545288, 'policy_logps/rejected': -305.2520446777344, 'policy_logps/chosen': -403.9790344238281, 'referece_logps/rejected': -247.05426025390625, 'referece_logps/chosen': -376.4074401855469, 'logits/rejected': -0.9242381453514099, 'logits/chosen': -1.1946327686309814, 'epoch': 4.59}


 77%|███████▋  | 8220/10740 [40:46:54<12:52:00, 18.38s/it]

 77%|███████▋  | 8221/10740 [40:47:14<13:04:38, 18.69s/it]

 77%|███████▋  | 8222/10740 [40:47:35<13:33:26, 19.38s/it]
{'loss': 0.147, 'learning_rate': 2.74741547152186e-07, 'rewards/chosen': -2.837775230407715, 'rewards/rejected': -7.338915824890137, 'rewards/accuracies': 1.0, 'rewards/margins': 4.50114107131958, 'policy_logps/rejected': -380.1648864746094, 'policy_logps/chosen': -541.329345703125, 'referece_logps/rejected': -306.7757263183594, 'referece_logps/chosen': -512.9515991210938, 'logits/rejected': 0.6979770064353943, 'logits/chosen': 0.647236168384552, 'epoch': 4.59}

 77%|███████▋  | 8223/10740 [40:47:50<12:39:23, 18.10s/it]

 77%|███████▋  | 8224/10740 [40:48:12<13:29:31, 19.31s/it]


 77%|███████▋  | 8226/10740 [40:48:45<12:39:28, 18.13s/it]

 77%|███████▋  | 8227/10740 [40:49:05<13:00:35, 18.64s/it]

 77%|███████▋  | 8228/10740 [40:49:22<12:37:13, 18.09s/it]

 77%|███████▋  | 8229/10740 [40:49:39<12:20:48, 17.70s/it]

 77%|███████▋  | 8230/10740 [40:49:53<11:41:18, 16.76s/it]

 77%|███████▋  | 8231/10740 [40:50:14<12:29:23, 17.92s/it]

 77%|███████▋  | 8232/10740 [40:50:33<12:37:12, 18.12s/it]

 77%|███████▋  | 8233/10740 [40:50:52<12:53:21, 18.51s/it]

 77%|███████▋  | 8234/10740 [40:51:08<12:15:40, 17.61s/it]

 77%|███████▋  | 8235/10740 [40:51:29<13:01:18, 18.71s/it]

 77%|███████▋  | 8236/10740 [40:51:49<13:16:20, 19.08s/it]

 77%|███████▋  | 8237/10740 [40:52:05<12:35:45, 18.12s/it]

 77%|███████▋  | 8238/10740 [40:52:17<11:18:29, 16.27s/it]

 77%|███████▋  | 8239/10740 [40:52:35<11:49:55, 17.03s/it]

 77%|███████▋  | 8240/10740 [40:52:48<10:59:02, 15.82s/it]

 77%|███████▋  | 8241/10740 [40:53:08<11:50:51, 17.07s/it]

 77%|███████▋  | 8242/10740 [40:53:31<12:57:26, 18.67s/it]

 77%|███████▋  | 8243/10740 [40:53:49<12:54:13, 18.60s/it]

 77%|███████▋  | 8244/10740 [40:54:07<12:39:36, 18.26s/it]

 77%|███████▋  | 8245/10740 [40:54:24<12:20:24, 17.81s/it]

 77%|███████▋  | 8246/10740 [40:54:40<12:09:14, 17.54s/it]

 77%|███████▋  | 8247/10740 [40:55:00<12:28:09, 18.01s/it]

 77%|███████▋  | 8248/10740 [40:55:12<11:14:20, 16.24s/it]

 77%|███████▋  | 8249/10740 [40:55:25<10:35:06, 15.30s/it]

 77%|███████▋  | 8250/10740 [40:55:44<11:29:34, 16.62s/it]

 77%|███████▋  | 8251/10740 [40:55:58<10:55:59, 15.81s/it]

 77%|███████▋  | 8252/10740 [40:56:14<10:58:37, 15.88s/it]

 77%|███████▋  | 8253/10740 [40:56:31<11:05:39, 16.06s/it]

 77%|███████▋  | 8254/10740 [40:56:44<10:26:04, 15.11s/it]
{'loss': 0.1537, 'learning_rate': 2.681311663674275e-07, 'rewards/chosen': -3.9337692260742188, 'rewards/rejected': -7.563718318939209, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6299498081207275, 'policy_logps/rejected': -489.7266540527344, 'policy_logps/chosen': -514.1543579101562, 'referece_logps/rejected': -414.0894775390625, 'referece_logps/chosen': -474.8166198730469, 'logits/rejected': 0.05806891992688179, 'logits/chosen': 0.09465399384498596, 'epoch': 4.61}


 77%|███████▋  | 8256/10740 [40:57:14<10:14:46, 14.85s/it]

 77%|███████▋  | 8257/10740 [40:57:33<10:59:16, 15.93s/it]

 77%|███████▋  | 8258/10740 [40:57:53<11:54:23, 17.27s/it]

 77%|███████▋  | 8259/10740 [40:58:10<11:47:31, 17.11s/it]

 77%|███████▋  | 8260/10740 [40:58:31<12:32:24, 18.20s/it]

 77%|███████▋  | 8261/10740 [40:58:47<12:12:59, 17.74s/it]
{'loss': 0.1716, 'learning_rate': 2.666942104091723e-07, 'rewards/chosen': -3.3001341819763184, 'rewards/rejected': -7.704737186431885, 'rewards/accuracies': 1.0, 'rewards/margins': 4.404602527618408, 'policy_logps/rejected': -411.73468017578125, 'policy_logps/chosen': -396.3296813964844, 'referece_logps/rejected': -334.6872863769531, 'referece_logps/chosen': -363.3283386230469, 'logits/rejected': -0.32820427417755127, 'logits/chosen': -0.17009133100509644, 'epoch': 4.62}


 77%|███████▋  | 8263/10740 [40:59:26<12:36:56, 18.34s/it]

 77%|███████▋  | 8264/10740 [40:59:39<11:32:58, 16.79s/it]

 77%|███████▋  | 8265/10740 [40:59:50<10:21:46, 15.07s/it]

 77%|███████▋  | 8266/10740 [41:00:08<10:53:56, 15.86s/it]

 77%|███████▋  | 8267/10740 [41:00:25<11:17:55, 16.45s/it]

 77%|███████▋  | 8268/10740 [41:00:39<10:46:34, 15.69s/it]

 77%|███████▋  | 8269/10740 [41:00:58<11:18:43, 16.48s/it]

 77%|███████▋  | 8270/10740 [41:01:17<11:57:23, 17.43s/it]

 77%|███████▋  | 8271/10740 [41:01:30<10:54:41, 15.91s/it]

 77%|███████▋  | 8272/10740 [41:01:46<11:02:22, 16.10s/it]

 77%|███████▋  | 8273/10740 [41:02:06<11:46:56, 17.19s/it]

 77%|███████▋  | 8274/10740 [41:02:19<10:54:01, 15.91s/it]

 77%|███████▋  | 8275/10740 [41:02:38<11:35:42, 16.93s/it]

 77%|███████▋  | 8276/10740 [41:02:58<12:06:01, 17.68s/it]

 77%|███████▋  | 8277/10740 [41:03:12<11:30:06, 16.81s/it]
{'loss': 0.0846, 'learning_rate': 2.6342202198069727e-07, 'rewards/chosen': -4.002647399902344, 'rewards/rejected': -7.934462547302246, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9318149089813232, 'policy_logps/rejected': -388.8369140625, 'policy_logps/chosen': -429.260498046875, 'referece_logps/rejected': -309.4922790527344, 'referece_logps/chosen': -389.2340087890625, 'logits/rejected': -0.37306755781173706, 'logits/chosen': -0.6256128549575806, 'epoch': 4.62}


 77%|███████▋  | 8279/10740 [41:03:51<12:26:41, 18.20s/it]

 77%|███████▋  | 8280/10740 [41:04:10<12:43:18, 18.62s/it]

 77%|███████▋  | 8281/10740 [41:04:30<12:54:29, 18.90s/it]

 77%|███████▋  | 8282/10740 [41:04:44<11:59:17, 17.56s/it]

 77%|███████▋  | 8283/10740 [41:05:00<11:36:34, 17.01s/it]

 77%|███████▋  | 8284/10740 [41:05:11<10:20:24, 15.16s/it]

 77%|███████▋  | 8285/10740 [41:05:33<11:42:17, 17.16s/it]

 77%|███████▋  | 8286/10740 [41:05:46<10:54:30, 16.00s/it]

 77%|███████▋  | 8287/10740 [41:06:06<11:41:01, 17.15s/it]

 77%|███████▋  | 8288/10740 [41:06:19<10:51:36, 15.94s/it]

 77%|███████▋  | 8289/10740 [41:06:32<10:16:21, 15.09s/it]

 77%|███████▋  | 8290/10740 [41:06:48<10:34:15, 15.53s/it]

 77%|███████▋  | 8291/10740 [41:07:02<10:04:50, 14.82s/it]

 77%|███████▋  | 8292/10740 [41:07:21<11:03:29, 16.26s/it]
{'loss': 0.1132, 'learning_rate': 2.603699198687167e-07, 'rewards/chosen': -3.8454678058624268, 'rewards/rejected': -8.520135879516602, 'rewards/accuracies': 1.0, 'rewards/margins': 4.674668312072754, 'policy_logps/rejected': -348.466796875, 'policy_logps/chosen': -408.0492858886719, 'referece_logps/rejected': -263.26544189453125, 'referece_logps/chosen': -369.5946350097656, 'logits/rejected': 0.11478988826274872, 'logits/chosen': 0.0018086731433868408, 'epoch': 4.63}


 77%|███████▋  | 8294/10740 [41:07:56<11:21:58, 16.73s/it]
{'loss': 0.2275, 'learning_rate': 2.5996411516869265e-07, 'rewards/chosen': -4.856131076812744, 'rewards/rejected': -9.073522567749023, 'rewards/accuracies': 0.875, 'rewards/margins': 4.2173919677734375, 'policy_logps/rejected': -318.72955322265625, 'policy_logps/chosen': -480.5823059082031, 'referece_logps/rejected': -227.99429321289062, 'referece_logps/chosen': -432.0209655761719, 'logits/rejected': -0.14494451880455017, 'logits/chosen': -0.32247447967529297, 'epoch': 4.63}


 77%|███████▋  | 8296/10740 [41:08:23<10:14:39, 15.09s/it]
{'loss': 0.1587, 'learning_rate': 2.595585797009765e-07, 'rewards/chosen': -3.7763960361480713, 'rewards/rejected': -7.710973739624023, 'rewards/accuracies': 0.875, 'rewards/margins': 3.934577703475952, 'policy_logps/rejected': -281.93975830078125, 'policy_logps/chosen': -490.435546875, 'referece_logps/rejected': -204.83001708984375, 'referece_logps/chosen': -452.67156982421875, 'logits/rejected': -0.0013782232999801636, 'logits/chosen': -0.5742819309234619, 'epoch': 4.63}


 77%|███████▋  | 8298/10740 [41:08:56<10:56:04, 16.12s/it]

 77%|███████▋  | 8299/10740 [41:09:16<11:39:31, 17.19s/it]

 77%|███████▋  | 8300/10740 [41:09:34<11:54:56, 17.58s/it]
{'loss': 0.2021, 'learning_rate': 2.5874831705252287e-07, 'rewards/chosen': -3.4345831871032715, 'rewards/rejected': -6.988012313842773, 'rewards/accuracies': 1.0, 'rewards/margins': 3.553429126739502, 'policy_logps/rejected': -361.9741516113281, 'policy_logps/chosen': -358.1944580078125, 'referece_logps/rejected': -292.09405517578125, 'referece_logps/chosen': -323.8486328125, 'logits/rejected': -0.2656688690185547, 'logits/chosen': -0.08178489655256271, 'epoch': 4.64}


 77%|███████▋  | 8302/10740 [41:10:07<11:34:06, 17.08s/it]

 77%|███████▋  | 8303/10740 [41:10:26<12:03:02, 17.80s/it]

 77%|███████▋  | 8304/10740 [41:10:42<11:42:18, 17.30s/it]
{'loss': 0.1674, 'learning_rate': 2.5793913310248074e-07, 'rewards/chosen': -5.394041538238525, 'rewards/rejected': -7.763432025909424, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3693907260894775, 'policy_logps/rejected': -285.25958251953125, 'policy_logps/chosen': -309.57122802734375, 'referece_logps/rejected': -207.6252899169922, 'referece_logps/chosen': -255.63079833984375, 'logits/rejected': -0.35579872131347656, 'logits/chosen': -0.4172458350658417, 'epoch': 4.64}


 77%|███████▋  | 8306/10740 [41:11:19<12:05:29, 17.88s/it]
{'loss': 0.1534, 'learning_rate': 2.575349460074122e-07, 'rewards/chosen': -2.9235613346099854, 'rewards/rejected': -8.824491500854492, 'rewards/accuracies': 1.0, 'rewards/margins': 5.900929927825928, 'policy_logps/rejected': -315.4639587402344, 'policy_logps/chosen': -355.7580261230469, 'referece_logps/rejected': -227.2190704345703, 'referece_logps/chosen': -326.5224304199219, 'logits/rejected': -0.02472667396068573, 'logits/chosen': -0.16296671330928802, 'epoch': 4.64}

 77%|███████▋  | 8307/10740 [41:11:35<11:47:39, 17.45s/it]

 77%|███████▋  | 8308/10740 [41:11:55<12:18:14, 18.21s/it]

 77%|███████▋  | 8309/10740 [41:12:12<11:56:56, 17.69s/it]


 77%|███████▋  | 8311/10740 [41:12:51<12:39:17, 18.76s/it]

 77%|███████▋  | 8312/10740 [41:13:03<11:16:05, 16.71s/it]

 77%|███████▋  | 8313/10740 [41:13:23<11:54:49, 17.67s/it]

 77%|███████▋  | 8314/10740 [41:13:41<11:57:29, 17.74s/it]

 77%|███████▋  | 8315/10740 [41:13:57<11:37:19, 17.25s/it]

 77%|███████▋  | 8316/10740 [41:14:19<12:28:18, 18.52s/it]

 77%|███████▋  | 8317/10740 [41:14:39<12:48:02, 19.02s/it]
{'loss': 0.1849, 'learning_rate': 2.5531674924693705e-07, 'rewards/chosen': -3.291644334793091, 'rewards/rejected': -6.699394702911377, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4077510833740234, 'policy_logps/rejected': -371.68316650390625, 'policy_logps/chosen': -307.1614990234375, 'referece_logps/rejected': -304.6892395019531, 'referece_logps/chosen': -274.2450866699219, 'logits/rejected': 0.21673807501792908, 'logits/chosen': 0.23617199063301086, 'epoch': 4.65}

 77%|███████▋  | 8318/10740 [41:14:57<12:45:07, 18.95s/it]


 77%|███████▋  | 8320/10740 [41:15:32<12:16:40, 18.26s/it]
{'loss': 0.1948, 'learning_rate': 2.5471320781397486e-07, 'rewards/chosen': -4.388584136962891, 'rewards/rejected': -7.870002269744873, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4814183712005615, 'policy_logps/rejected': -332.19964599609375, 'policy_logps/chosen': -623.6773681640625, 'referece_logps/rejected': -253.49964904785156, 'referece_logps/chosen': -579.79150390625, 'logits/rejected': -0.5859494209289551, 'logits/chosen': -0.9314519166946411, 'epoch': 4.65}

 77%|███████▋  | 8321/10740 [41:15:52<12:34:15, 18.71s/it]


 77%|███████▋  | 8323/10740 [41:16:33<13:01:32, 19.40s/it]
{'loss': 0.2693, 'learning_rate': 2.541102764519293e-07, 'rewards/chosen': -4.056682586669922, 'rewards/rejected': -6.642611980438232, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5859298706054688, 'policy_logps/rejected': -341.8628845214844, 'policy_logps/chosen': -466.93414306640625, 'referece_logps/rejected': -275.436767578125, 'referece_logps/chosen': -426.3673400878906, 'logits/rejected': 0.051832303404808044, 'logits/chosen': -0.014958560466766357, 'epoch': 4.65}


 78%|███████▊  | 8325/10740 [41:17:05<12:08:15, 18.09s/it]

 78%|███████▊  | 8326/10740 [41:17:21<11:40:40, 17.42s/it]

 78%|███████▊  | 8327/10740 [41:17:36<11:15:25, 16.79s/it]

 78%|███████▊  | 8328/10740 [41:17:49<10:25:02, 15.55s/it]

 78%|███████▊  | 8329/10740 [41:18:11<11:38:54, 17.39s/it]
{'loss': 0.1426, 'learning_rate': 2.529062459142588e-07, 'rewards/chosen': -3.2462360858917236, 'rewards/rejected': -7.054627418518066, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8083908557891846, 'policy_logps/rejected': -244.08859252929688, 'policy_logps/chosen': -207.5325927734375, 'referece_logps/rejected': -173.54232788085938, 'referece_logps/chosen': -175.07025146484375, 'logits/rejected': -0.44823896884918213, 'logits/chosen': -0.4894799590110779, 'epoch': 4.65}

 78%|███████▊  | 8330/10740 [41:18:30<11:58:35, 17.89s/it]

 78%|███████▊  | 8331/10740 [41:18:50<12:23:29, 18.52s/it]


 78%|███████▊  | 8333/10740 [41:19:31<13:04:22, 19.55s/it]
{'loss': 0.2068, 'learning_rate': 2.5210491764594223e-07, 'rewards/chosen': -3.8153607845306396, 'rewards/rejected': -8.500950813293457, 'rewards/accuracies': 0.875, 'rewards/margins': 4.68558931350708, 'policy_logps/rejected': -373.978271484375, 'policy_logps/chosen': -320.13031005859375, 'referece_logps/rejected': -288.96875, 'referece_logps/chosen': -281.9766845703125, 'logits/rejected': 0.08182165026664734, 'logits/chosen': 0.07424163073301315, 'epoch': 4.66}

 78%|███████▊  | 8334/10740 [41:19:50<12:50:43, 19.22s/it]


 78%|███████▊  | 8336/10740 [41:20:29<12:57:50, 19.41s/it]
{'loss': 0.2233, 'learning_rate': 2.5150463562127556e-07, 'rewards/chosen': -4.453073501586914, 'rewards/rejected': -7.874770164489746, 'rewards/accuracies': 0.75, 'rewards/margins': 3.421696662902832, 'policy_logps/rejected': -467.861083984375, 'policy_logps/chosen': -348.0245666503906, 'referece_logps/rejected': -389.1133728027344, 'referece_logps/chosen': -303.4938049316406, 'logits/rejected': 0.5699776411056519, 'logits/chosen': 0.542810320854187, 'epoch': 4.66}


 78%|███████▊  | 8338/10740 [41:21:07<12:42:52, 19.06s/it]
{'loss': 0.241, 'learning_rate': 2.511047879619446e-07, 'rewards/chosen': -3.8181238174438477, 'rewards/rejected': -7.280527114868164, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4624035358428955, 'policy_logps/rejected': -359.9893798828125, 'policy_logps/chosen': -511.65960693359375, 'referece_logps/rejected': -287.18408203125, 'referece_logps/chosen': -473.47833251953125, 'logits/rejected': -0.04115290194749832, 'logits/chosen': -0.29730868339538574, 'epoch': 4.66}


 78%|███████▊  | 8340/10740 [41:21:39<11:59:16, 17.98s/it]
{'loss': 0.1318, 'learning_rate': 2.507052127580317e-07, 'rewards/chosen': -3.1882684230804443, 'rewards/rejected': -8.098028182983398, 'rewards/accuracies': 1.0, 'rewards/margins': 4.909759521484375, 'policy_logps/rejected': -498.6108093261719, 'policy_logps/chosen': -560.9597778320312, 'referece_logps/rejected': -417.6304931640625, 'referece_logps/chosen': -529.0770874023438, 'logits/rejected': -0.08090294897556305, 'logits/chosen': -0.43531012535095215, 'epoch': 4.66}


 78%|███████▊  | 8342/10740 [41:22:16<12:05:45, 18.16s/it]

 78%|███████▊  | 8343/10740 [41:22:29<11:11:55, 16.82s/it]

 78%|███████▊  | 8344/10740 [41:22:48<11:29:58, 17.28s/it]

 78%|███████▊  | 8345/10740 [41:23:02<10:51:02, 16.31s/it]
{'loss': 0.1472, 'learning_rate': 2.4970746769444764e-07, 'rewards/chosen': -2.642601251602173, 'rewards/rejected': -7.283123970031738, 'rewards/accuracies': 1.0, 'rewards/margins': 4.6405229568481445, 'policy_logps/rejected': -286.4659423828125, 'policy_logps/chosen': -434.69720458984375, 'referece_logps/rejected': -213.63470458984375, 'referece_logps/chosen': -408.27117919921875, 'logits/rejected': -0.21469570696353912, 'logits/chosen': -0.5292773842811584, 'epoch': 4.66}

 78%|███████▊  | 8346/10740 [41:23:22<11:41:35, 17.58s/it]

 78%|███████▊  | 8347/10740 [41:23:40<11:44:56, 17.67s/it]


 78%|███████▊  | 8349/10740 [41:24:21<12:36:24, 18.98s/it]
{'loss': 0.1355, 'learning_rate': 2.4891049987161573e-07, 'rewards/chosen': -3.302971839904785, 'rewards/rejected': -8.649383544921875, 'rewards/accuracies': 1.0, 'rewards/margins': 5.34641170501709, 'policy_logps/rejected': -481.73126220703125, 'policy_logps/chosen': -376.7004699707031, 'referece_logps/rejected': -395.2373962402344, 'referece_logps/chosen': -343.6707763671875, 'logits/rejected': -0.5106428861618042, 'logits/chosen': -0.42724886536598206, 'epoch': 4.66}

 78%|███████▊  | 8350/10740 [41:24:39<12:18:48, 18.55s/it]

 78%|███████▊  | 8351/10740 [41:24:58<12:30:05, 18.84s/it]

 78%|███████▊  | 8352/10740 [41:25:14<11:55:12, 17.97s/it]

 78%|███████▊  | 8353/10740 [41:25:36<12:42:49, 19.17s/it]

 78%|███████▊  | 8354/10740 [41:25:51<11:48:07, 17.81s/it]


 78%|███████▊  | 8356/10740 [41:26:23<10:59:34, 16.60s/it]

 78%|███████▊  | 8357/10740 [41:26:43<11:40:33, 17.64s/it]
{'loss': 0.1923, 'learning_rate': 2.4731984442850426e-07, 'rewards/chosen': -3.6921074390411377, 'rewards/rejected': -8.156294822692871, 'rewards/accuracies': 0.875, 'rewards/margins': 4.4641876220703125, 'policy_logps/rejected': -456.36102294921875, 'policy_logps/chosen': -377.50225830078125, 'referece_logps/rejected': -374.798095703125, 'referece_logps/chosen': -340.58111572265625, 'logits/rejected': -0.21820659935474396, 'logits/chosen': -0.18910814821720123, 'epoch': 4.67}

 78%|███████▊  | 8358/10740 [41:27:03<12:04:14, 18.24s/it]

 78%|███████▊  | 8359/10740 [41:27:22<12:19:41, 18.64s/it]


 78%|███████▊  | 8361/10740 [41:27:52<11:00:59, 16.67s/it]
{'loss': 0.1584, 'learning_rate': 2.465261591230088e-07, 'rewards/chosen': -2.0366926193237305, 'rewards/rejected': -6.306209564208984, 'rewards/accuracies': 1.0, 'rewards/margins': 4.269517421722412, 'policy_logps/rejected': -360.49493408203125, 'policy_logps/chosen': -334.7519836425781, 'referece_logps/rejected': -297.4328308105469, 'referece_logps/chosen': -314.3850402832031, 'logits/rejected': -0.5631049871444702, 'logits/chosen': -0.5992259383201599, 'epoch': 4.67}

 78%|███████▊  | 8362/10740 [41:28:11<11:29:47, 17.40s/it]

 78%|███████▊  | 8363/10740 [41:28:32<12:17:42, 18.62s/it]


 78%|███████▊  | 8365/10740 [41:29:11<12:43:46, 19.30s/it]

 78%|███████▊  | 8366/10740 [41:29:27<12:06:41, 18.37s/it]

 78%|███████▊  | 8367/10740 [41:29:44<11:49:04, 17.93s/it]

 78%|███████▊  | 8368/10740 [41:30:05<12:25:26, 18.86s/it]
{'loss': 0.165, 'learning_rate': 2.4513984894830374e-07, 'rewards/chosen': -4.867812156677246, 'rewards/rejected': -8.239418983459473, 'rewards/accuracies': 1.0, 'rewards/margins': 3.371607542037964, 'policy_logps/rejected': -387.56689453125, 'policy_logps/chosen': -416.9127197265625, 'referece_logps/rejected': -305.1727294921875, 'referece_logps/chosen': -368.2345886230469, 'logits/rejected': 0.2960319519042969, 'logits/chosen': 0.20417553186416626, 'epoch': 4.67}

 78%|███████▊  | 8369/10740 [41:30:24<12:29:32, 18.97s/it]

 78%|███████▊  | 8370/10740 [41:30:35<10:50:35, 16.47s/it]


 78%|███████▊  | 8372/10740 [41:31:06<10:43:55, 16.32s/it]
{'loss': 0.2734, 'learning_rate': 2.4434918174980943e-07, 'rewards/chosen': -3.47585129737854, 'rewards/rejected': -5.719605445861816, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2437543869018555, 'policy_logps/rejected': -281.3141174316406, 'policy_logps/chosen': -474.3719787597656, 'referece_logps/rejected': -224.11805725097656, 'referece_logps/chosen': -439.61346435546875, 'logits/rejected': -0.14475996792316437, 'logits/chosen': -0.2868120074272156, 'epoch': 4.68}


 78%|███████▊  | 8374/10740 [41:31:46<11:52:01, 18.06s/it]

 78%|███████▊  | 8375/10740 [41:32:02<11:34:34, 17.62s/it]
{'loss': 0.2069, 'learning_rate': 2.4375690293509144e-07, 'rewards/chosen': -4.186776161193848, 'rewards/rejected': -7.318490982055664, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1317145824432373, 'policy_logps/rejected': -487.2466735839844, 'policy_logps/chosen': -441.5934753417969, 'referece_logps/rejected': -414.0617980957031, 'referece_logps/chosen': -399.72576904296875, 'logits/rejected': -0.19223341345787048, 'logits/chosen': -0.22436317801475525, 'epoch': 4.68}

 78%|███████▊  | 8376/10740 [41:32:17<10:59:35, 16.74s/it]

 78%|███████▊  | 8377/10740 [41:32:37<11:34:25, 17.63s/it]


 78%|███████▊  | 8379/10740 [41:33:10<11:20:41, 17.30s/it]

 78%|███████▊  | 8380/10740 [41:33:30<11:57:23, 18.24s/it]
{'loss': 0.1909, 'learning_rate': 2.427711474596199e-07, 'rewards/chosen': -3.750708818435669, 'rewards/rejected': -6.357041358947754, 'rewards/accuracies': 0.625, 'rewards/margins': 2.606332302093506, 'policy_logps/rejected': -390.77740478515625, 'policy_logps/chosen': -414.94317626953125, 'referece_logps/rejected': -327.20697021484375, 'referece_logps/chosen': -377.43609619140625, 'logits/rejected': 0.37643295526504517, 'logits/chosen': 0.38196122646331787, 'epoch': 4.68}

 78%|███████▊  | 8381/10740 [41:33:47<11:38:27, 17.76s/it]


 78%|███████▊  | 8383/10740 [41:34:20<11:06:47, 16.97s/it]
{'loss': 0.1784, 'learning_rate': 2.421805204927525e-07, 'rewards/chosen': -3.5749661922454834, 'rewards/rejected': -9.470135688781738, 'rewards/accuracies': 1.0, 'rewards/margins': 5.895169734954834, 'policy_logps/rejected': -485.0726013183594, 'policy_logps/chosen': -381.9615478515625, 'referece_logps/rejected': -390.3712463378906, 'referece_logps/chosen': -346.21185302734375, 'logits/rejected': -0.45211899280548096, 'logits/chosen': -0.49067020416259766, 'epoch': 4.68}


 78%|███████▊  | 8385/10740 [41:34:55<11:09:10, 17.05s/it]
{'loss': 0.2616, 'learning_rate': 2.4178711377936476e-07, 'rewards/chosen': -4.348453998565674, 'rewards/rejected': -8.830665588378906, 'rewards/accuracies': 0.875, 'rewards/margins': 4.482212066650391, 'policy_logps/rejected': -394.2670593261719, 'policy_logps/chosen': -384.996337890625, 'referece_logps/rejected': -305.96044921875, 'referece_logps/chosen': -341.5118103027344, 'logits/rejected': -0.4130834639072418, 'logits/chosen': -0.4481150507926941, 'epoch': 4.68}


 78%|███████▊  | 8387/10740 [41:35:26<10:33:37, 16.16s/it]
{'loss': 0.1861, 'learning_rate': 2.413939829112562e-07, 'rewards/chosen': -4.990767478942871, 'rewards/rejected': -7.005911350250244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.015143871307373, 'policy_logps/rejected': -430.9211730957031, 'policy_logps/chosen': -509.8304748535156, 'referece_logps/rejected': -360.862060546875, 'referece_logps/chosen': -459.92279052734375, 'logits/rejected': -0.5229973793029785, 'logits/chosen': -0.5802914500236511, 'epoch': 4.69}

 78%|███████▊  | 8388/10740 [41:35:47<11:36:23, 17.76s/it]

 78%|███████▊  | 8389/10740 [41:36:07<12:00:18, 18.38s/it]


 78%|███████▊  | 8391/10740 [41:36:42<11:41:32, 17.92s/it]

 78%|███████▊  | 8392/10740 [41:37:02<12:02:10, 18.45s/it]

 78%|███████▊  | 8393/10740 [41:37:20<11:59:40, 18.40s/it]

 78%|███████▊  | 8394/10740 [41:37:32<10:40:08, 16.37s/it]

 78%|███████▊  | 8395/10740 [41:37:52<11:22:10, 17.45s/it]

 78%|███████▊  | 8396/10740 [41:38:12<12:00:55, 18.45s/it]

 78%|███████▊  | 8397/10740 [41:38:26<11:03:53, 17.00s/it]

 78%|███████▊  | 8398/10740 [41:38:49<12:10:00, 18.70s/it]

 78%|███████▊  | 8399/10740 [41:39:09<12:22:42, 19.04s/it]

 78%|███████▊  | 8400/10740 [41:39:26<12:05:06, 18.59s/it]

 78%|███████▊  | 8401/10740 [41:39:44<11:58:38, 18.43s/it]

 78%|███████▊  | 8402/10740 [41:40:05<12:20:38, 19.01s/it]

 78%|███████▊  | 8403/10740 [41:40:25<12:31:27, 19.29s/it]
{'loss': 0.1749, 'learning_rate': 2.382588835383388e-07, 'rewards/chosen': -2.8801259994506836, 'rewards/rejected': -6.240203380584717, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3600780963897705, 'policy_logps/rejected': -228.73095703125, 'policy_logps/chosen': -405.2908630371094, 'referece_logps/rejected': -166.32891845703125, 'referece_logps/chosen': -376.4895935058594, 'logits/rejected': -0.40588414669036865, 'logits/chosen': -0.6675108671188354, 'epoch': 4.69}

 78%|███████▊  | 8404/10740 [41:40:42<12:10:36, 18.77s/it]


 78%|███████▊  | 8406/10740 [41:41:21<12:30:25, 19.29s/it]
{'loss': 0.176, 'learning_rate': 2.376730247512868e-07, 'rewards/chosen': -4.0634446144104, 'rewards/rejected': -7.787330150604248, 'rewards/accuracies': 0.875, 'rewards/margins': 3.723886251449585, 'policy_logps/rejected': -341.0373840332031, 'policy_logps/chosen': -333.80255126953125, 'referece_logps/rejected': -263.1640625, 'referece_logps/chosen': -293.1680908203125, 'logits/rejected': 0.050042055547237396, 'logits/chosen': 0.13919518887996674, 'epoch': 4.7}


 78%|███████▊  | 8408/10740 [41:42:01<12:42:21, 19.61s/it]
{'loss': 0.2144, 'learning_rate': 2.372827988745222e-07, 'rewards/chosen': -3.1272006034851074, 'rewards/rejected': -7.734392166137695, 'rewards/accuracies': 1.0, 'rewards/margins': 4.60719108581543, 'policy_logps/rejected': -421.046142578125, 'policy_logps/chosen': -235.8883514404297, 'referece_logps/rejected': -343.70220947265625, 'referece_logps/chosen': -204.61636352539062, 'logits/rejected': -0.414559543132782, 'logits/chosen': -0.45309126377105713, 'epoch': 4.7}

 78%|███████▊  | 8409/10740 [41:42:18<12:11:59, 18.84s/it]

 78%|███████▊  | 8410/10740 [41:42:35<11:55:35, 18.43s/it]

 78%|███████▊  | 8411/10740 [41:42:54<11:57:44, 18.49s/it]

 78%|███████▊  | 8412/10740 [41:43:15<12:32:57, 19.41s/it]


 78%|███████▊  | 8414/10740 [41:43:53<12:04:46, 18.70s/it]
{'loss': 0.1198, 'learning_rate': 2.3611378671555682e-07, 'rewards/chosen': -4.747902870178223, 'rewards/rejected': -9.268778800964355, 'rewards/accuracies': 1.0, 'rewards/margins': 4.520875930786133, 'policy_logps/rejected': -573.4693603515625, 'policy_logps/chosen': -356.5246276855469, 'referece_logps/rejected': -480.781494140625, 'referece_logps/chosen': -309.04559326171875, 'logits/rejected': 0.3728097081184387, 'logits/chosen': 0.38632121682167053, 'epoch': 4.7}

 78%|███████▊  | 8415/10740 [41:44:12<12:09:53, 18.84s/it]


 78%|███████▊  | 8417/10740 [41:44:49<12:12:56, 18.93s/it]
{'loss': 0.188, 'learning_rate': 2.355302183407949e-07, 'rewards/chosen': -4.457761764526367, 'rewards/rejected': -7.7690887451171875, 'rewards/accuracies': 0.875, 'rewards/margins': 3.311326503753662, 'policy_logps/rejected': -401.00579833984375, 'policy_logps/chosen': -352.0464782714844, 'referece_logps/rejected': -323.31494140625, 'referece_logps/chosen': -307.4688720703125, 'logits/rejected': -0.26957985758781433, 'logits/chosen': -0.5015000104904175, 'epoch': 4.7}

 78%|███████▊  | 8418/10740 [41:45:08<12:20:58, 19.15s/it]

 78%|███████▊  | 8419/10740 [41:45:25<11:46:11, 18.26s/it]

 78%|███████▊  | 8420/10740 [41:45:42<11:34:50, 17.97s/it]


 78%|███████▊  | 8422/10740 [41:46:09<10:01:24, 15.57s/it]

 78%|███████▊  | 8423/10740 [41:46:23<9:41:48, 15.07s/it]
{'loss': 0.2129, 'learning_rate': 2.3436495938917744e-07, 'rewards/chosen': -2.7267513275146484, 'rewards/rejected': -6.886599540710449, 'rewards/accuracies': 1.0, 'rewards/margins': 4.159848213195801, 'policy_logps/rejected': -460.705322265625, 'policy_logps/chosen': -494.16632080078125, 'referece_logps/rejected': -391.83935546875, 'referece_logps/chosen': -466.8988037109375, 'logits/rejected': 0.009315602481365204, 'logits/chosen': 0.06963199377059937, 'epoch': 4.71}

 78%|███████▊  | 8424/10740 [41:46:40<10:04:00, 15.65s/it]

 78%|███████▊  | 8425/10740 [41:46:56<10:09:01, 15.78s/it]


 78%|███████▊  | 8427/10740 [41:47:25<9:30:37, 14.80s/it]
{'loss': 0.1419, 'learning_rate': 2.3358951258024128e-07, 'rewards/chosen': -3.68109393119812, 'rewards/rejected': -8.939640045166016, 'rewards/accuracies': 1.0, 'rewards/margins': 5.258545875549316, 'policy_logps/rejected': -388.2820129394531, 'policy_logps/chosen': -440.6662902832031, 'referece_logps/rejected': -298.8855895996094, 'referece_logps/chosen': -403.8553466796875, 'logits/rejected': -0.8253075480461121, 'logits/chosen': -0.7295196652412415, 'epoch': 4.71}


 78%|███████▊  | 8429/10740 [41:48:03<11:04:38, 17.26s/it]

 78%|███████▊  | 8430/10740 [41:48:15<10:03:36, 15.68s/it]
{'loss': 0.2567, 'learning_rate': 2.3300865933439495e-07, 'rewards/chosen': -4.575200080871582, 'rewards/rejected': -7.455199241638184, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8799984455108643, 'policy_logps/rejected': -412.70721435546875, 'policy_logps/chosen': -427.37738037109375, 'referece_logps/rejected': -338.1551818847656, 'referece_logps/chosen': -381.6253967285156, 'logits/rejected': 0.4031189978122711, 'logits/chosen': 0.36534565687179565, 'epoch': 4.71}


 79%|███████▊  | 8432/10740 [41:48:51<10:52:29, 16.96s/it]

 79%|███████▊  | 8433/10740 [41:49:11<11:25:40, 17.83s/it]
{'loss': 0.1768, 'learning_rate': 2.3242843392620094e-07, 'rewards/chosen': -4.021769046783447, 'rewards/rejected': -8.640336990356445, 'rewards/accuracies': 1.0, 'rewards/margins': 4.618567943572998, 'policy_logps/rejected': -500.48809814453125, 'policy_logps/chosen': -439.7088317871094, 'referece_logps/rejected': -414.0847473144531, 'referece_logps/chosen': -399.49114990234375, 'logits/rejected': 0.07019893079996109, 'logits/chosen': 0.0822291225194931, 'epoch': 4.71}

 79%|███████▊  | 8434/10740 [41:49:28<11:10:14, 17.44s/it]

 79%|███████▊  | 8435/10740 [41:49:48<11:40:29, 18.23s/it]


 79%|███████▊  | 8437/10740 [41:50:18<10:29:25, 16.40s/it]

 79%|███████▊  | 8438/10740 [41:50:37<11:07:35, 17.40s/it]
{'loss': 0.1381, 'learning_rate': 2.314627880637673e-07, 'rewards/chosen': -3.1757261753082275, 'rewards/rejected': -8.997305870056152, 'rewards/accuracies': 1.0, 'rewards/margins': 5.821579933166504, 'policy_logps/rejected': -382.9429016113281, 'policy_logps/chosen': -375.59039306640625, 'referece_logps/rejected': -292.9698181152344, 'referece_logps/chosen': -343.8331298828125, 'logits/rejected': -0.1852191984653473, 'logits/chosen': -0.10816387832164764, 'epoch': 4.71}


 79%|███████▊  | 8440/10740 [41:51:18<12:06:54, 18.96s/it]
{'loss': 0.1505, 'learning_rate': 2.3107701889828856e-07, 'rewards/chosen': -4.506722927093506, 'rewards/rejected': -7.900538444519043, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3938159942626953, 'policy_logps/rejected': -402.1650695800781, 'policy_logps/chosen': -503.7388610839844, 'referece_logps/rejected': -323.15966796875, 'referece_logps/chosen': -458.6716613769531, 'logits/rejected': -0.2686726748943329, 'logits/chosen': -0.28954190015792847, 'epoch': 4.72}

 79%|███████▊  | 8441/10740 [41:51:33<11:23:46, 17.85s/it]

 79%|███████▊  | 8442/10740 [41:51:44<10:06:48, 15.84s/it]

 79%|███████▊  | 8443/10740 [41:52:02<10:33:11, 16.54s/it]

 79%|███████▊  | 8444/10740 [41:52:13<9:26:17, 14.80s/it]

 79%|███████▊  | 8445/10740 [41:52:35<10:46:58, 16.91s/it]

 79%|███████▊  | 8446/10740 [41:52:54<11:18:34, 17.75s/it]

 79%|███████▊  | 8447/10740 [41:53:14<11:44:37, 18.44s/it]

 79%|███████▊  | 8448/10740 [41:53:27<10:31:39, 16.54s/it]

 79%|███████▊  | 8449/10740 [41:53:39<9:38:52, 15.16s/it]

 79%|███████▊  | 8450/10740 [41:53:58<10:31:47, 16.55s/it]

 79%|███████▊  | 8451/10740 [41:54:14<10:21:10, 16.28s/it]

 79%|███████▊  | 8452/10740 [41:54:31<10:31:38, 16.56s/it]


 79%|███████▊  | 8454/10740 [41:55:02<10:12:18, 16.07s/it]

 79%|███████▊  | 8455/10740 [41:55:13<9:23:04, 14.79s/it]

 79%|███████▊  | 8456/10740 [41:55:30<9:41:03, 15.26s/it]
{'loss': 0.2324, 'learning_rate': 2.2800094804400283e-07, 'rewards/chosen': -2.955636739730835, 'rewards/rejected': -8.062283515930176, 'rewards/accuracies': 0.875, 'rewards/margins': 5.106646537780762, 'policy_logps/rejected': -407.7389221191406, 'policy_logps/chosen': -348.0654602050781, 'referece_logps/rejected': -327.1160583496094, 'referece_logps/chosen': -318.50909423828125, 'logits/rejected': -0.2798815965652466, 'logits/chosen': -0.31782880425453186, 'epoch': 4.72}

 79%|███████▊  | 8457/10740 [41:55:49<10:23:00, 16.37s/it]


 79%|███████▉  | 8459/10740 [41:56:22<10:23:13, 16.39s/it]
{'loss': 0.1548, 'learning_rate': 2.274261837357303e-07, 'rewards/chosen': -5.53193473815918, 'rewards/rejected': -8.230245590209961, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6983115673065186, 'policy_logps/rejected': -452.067138671875, 'policy_logps/chosen': -509.5066223144531, 'referece_logps/rejected': -369.76470947265625, 'referece_logps/chosen': -454.187255859375, 'logits/rejected': -0.2525444030761719, 'logits/chosen': -0.10073389112949371, 'epoch': 4.73}

 79%|███████▉  | 8460/10740 [41:56:38<10:23:55, 16.42s/it]

 79%|███████▉  | 8461/10740 [41:56:58<11:00:48, 17.40s/it]


 79%|███████▉  | 8463/10740 [41:57:32<10:57:31, 17.33s/it]
{'loss': 0.2382, 'learning_rate': 2.2666081515065017e-07, 'rewards/chosen': -3.5468339920043945, 'rewards/rejected': -6.2109150886535645, 'rewards/accuracies': 1.0, 'rewards/margins': 2.664081335067749, 'policy_logps/rejected': -436.4397277832031, 'policy_logps/chosen': -327.9752197265625, 'referece_logps/rejected': -374.33062744140625, 'referece_logps/chosen': -292.50689697265625, 'logits/rejected': 0.48264026641845703, 'logits/chosen': 0.5615651607513428, 'epoch': 4.73}

 79%|███████▉  | 8464/10740 [41:57:49<10:48:04, 17.08s/it]


 79%|███████▉  | 8466/10740 [41:58:24<10:49:42, 17.14s/it]

 79%|███████▉  | 8467/10740 [41:58:40<10:38:09, 16.85s/it]
{'loss': 0.2385, 'learning_rate': 2.258965719589685e-07, 'rewards/chosen': -3.9697396755218506, 'rewards/rejected': -7.344315528869629, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3745760917663574, 'policy_logps/rejected': -471.84295654296875, 'policy_logps/chosen': -272.79571533203125, 'referece_logps/rejected': -398.3997497558594, 'referece_logps/chosen': -233.09829711914062, 'logits/rejected': -0.588546633720398, 'logits/chosen': -0.4768311381340027, 'epoch': 4.73}

 79%|███████▉  | 8468/10740 [41:58:55<10:22:24, 16.44s/it]

 79%|███████▉  | 8469/10740 [41:59:15<11:03:15, 17.52s/it]

 79%|███████▉  | 8470/10740 [41:59:35<11:25:51, 18.13s/it]

 79%|███████▉  | 8471/10740 [41:59:55<11:46:43, 18.69s/it]

 79%|███████▉  | 8472/10740 [42:00:08<10:46:36, 17.11s/it]

 79%|███████▉  | 8473/10740 [42:00:25<10:41:39, 16.98s/it]

 79%|███████▉  | 8474/10740 [42:00:43<10:57:33, 17.41s/it]

 79%|███████▉  | 8475/10740 [42:00:59<10:34:02, 16.80s/it]

 79%|███████▉  | 8476/10740 [42:01:13<10:04:44, 16.03s/it]


 79%|███████▉  | 8478/10740 [42:01:56<11:48:09, 18.78s/it]
{'loss': 0.1423, 'learning_rate': 2.238007150654161e-07, 'rewards/chosen': -4.973087310791016, 'rewards/rejected': -8.654674530029297, 'rewards/accuracies': 1.0, 'rewards/margins': 3.681586742401123, 'policy_logps/rejected': -373.0654602050781, 'policy_logps/chosen': -263.7960510253906, 'referece_logps/rejected': -286.51873779296875, 'referece_logps/chosen': -214.06520080566406, 'logits/rejected': -0.21788904070854187, 'logits/chosen': -0.11465223133563995, 'epoch': 4.74}

 79%|███████▉  | 8479/10740 [42:02:15<11:47:01, 18.76s/it]


 79%|███████▉  | 8481/10740 [42:02:48<10:55:19, 17.41s/it]
{'loss': 0.248, 'learning_rate': 2.2323059930306187e-07, 'rewards/chosen': -4.6623430252075195, 'rewards/rejected': -6.7496819496154785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.087339162826538, 'policy_logps/rejected': -279.5735778808594, 'policy_logps/chosen': -397.34527587890625, 'referece_logps/rejected': -212.0767822265625, 'referece_logps/chosen': -350.72186279296875, 'logits/rejected': 0.2521313726902008, 'logits/chosen': 0.05775235593318939, 'epoch': 4.74}


 79%|███████▉  | 8483/10740 [42:03:26<11:23:43, 18.18s/it]
{'loss': 0.3164, 'learning_rate': 2.2285087534476121e-07, 'rewards/chosen': -4.231532573699951, 'rewards/rejected': -5.199965476989746, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9684326648712158, 'policy_logps/rejected': -433.1007080078125, 'policy_logps/chosen': -359.9466552734375, 'referece_logps/rejected': -381.10107421875, 'referece_logps/chosen': -317.63134765625, 'logits/rejected': 0.304781049489975, 'logits/chosen': 0.3290943503379822, 'epoch': 4.74}

 79%|███████▉  | 8484/10740 [42:03:46<11:41:07, 18.65s/it]

 79%|███████▉  | 8485/10740 [42:04:02<11:12:28, 17.89s/it]

 79%|███████▉  | 8486/10740 [42:04:21<11:32:09, 18.42s/it]


 79%|███████▉  | 8488/10740 [42:04:56<11:09:56, 17.85s/it]

 79%|███████▉  | 8489/10740 [42:05:08<10:03:15, 16.08s/it]
{'loss': 0.1276, 'learning_rate': 2.2171340042874676e-07, 'rewards/chosen': -3.9556920528411865, 'rewards/rejected': -5.764095306396484, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8084031343460083, 'policy_logps/rejected': -260.2978210449219, 'policy_logps/chosen': -323.2705078125, 'referece_logps/rejected': -202.65687561035156, 'referece_logps/chosen': -283.713623046875, 'logits/rejected': 0.08148805797100067, 'logits/chosen': -0.10372603684663773, 'epoch': 4.74}

 79%|███████▉  | 8490/10740 [42:05:19<9:04:30, 14.52s/it]

 79%|███████▉  | 8491/10740 [42:05:41<10:26:40, 16.72s/it]

 79%|███████▉  | 8492/10740 [42:06:00<10:48:43, 17.31s/it]

 79%|███████▉  | 8493/10740 [42:06:19<11:14:04, 18.00s/it]

 79%|███████▉  | 8494/10740 [42:06:36<10:56:50, 17.55s/it]


 79%|███████▉  | 8496/10740 [42:07:01<9:12:09, 14.76s/it]
{'loss': 0.1659, 'learning_rate': 2.2038956743216753e-07, 'rewards/chosen': -3.1103832721710205, 'rewards/rejected': -8.814834594726562, 'rewards/accuracies': 1.0, 'rewards/margins': 5.704451560974121, 'policy_logps/rejected': -434.2290344238281, 'policy_logps/chosen': -391.94781494140625, 'referece_logps/rejected': -346.0806579589844, 'referece_logps/chosen': -360.8439636230469, 'logits/rejected': -0.5713127255439758, 'logits/chosen': -0.5945655107498169, 'epoch': 4.75}

 79%|███████▉  | 8497/10740 [42:07:15<9:12:52, 14.79s/it]

 79%|███████▉  | 8498/10740 [42:07:36<10:17:11, 16.52s/it]


 79%|███████▉  | 8500/10740 [42:08:16<11:28:40, 18.45s/it]
{'loss': 0.1447, 'learning_rate': 2.1963465102035216e-07, 'rewards/chosen': -4.018192291259766, 'rewards/rejected': -8.295626640319824, 'rewards/accuracies': 1.0, 'rewards/margins': 4.277434349060059, 'policy_logps/rejected': -435.1478271484375, 'policy_logps/chosen': -371.3315734863281, 'referece_logps/rejected': -352.19158935546875, 'referece_logps/chosen': -331.149658203125, 'logits/rejected': 0.25277844071388245, 'logits/chosen': 0.277955025434494, 'epoch': 4.75}

 79%|███████▉  | 8501/10740 [42:08:47<13:49:10, 22.22s/it]


 79%|███████▉  | 8503/10740 [42:09:28<13:18:59, 21.43s/it]
{'loss': 0.166, 'learning_rate': 2.1906920890088254e-07, 'rewards/chosen': -2.5990185737609863, 'rewards/rejected': -7.596358776092529, 'rewards/accuracies': 0.875, 'rewards/margins': 4.997340202331543, 'policy_logps/rejected': -333.7961120605469, 'policy_logps/chosen': -305.99932861328125, 'referece_logps/rejected': -257.83251953125, 'referece_logps/chosen': -280.0091552734375, 'logits/rejected': 0.2654198110103607, 'logits/chosen': 0.2040148675441742, 'epoch': 4.75}

 79%|███████▉  | 8504/10740 [42:09:48<12:56:20, 20.83s/it]

 79%|███████▉  | 8505/10740 [42:10:08<12:45:13, 20.54s/it]

 79%|███████▉  | 8506/10740 [42:10:29<12:56:48, 20.86s/it]

 79%|███████▉  | 8507/10740 [42:10:47<12:25:18, 20.03s/it]

 79%|███████▉  | 8508/10740 [42:11:08<12:31:25, 20.20s/it]

 79%|███████▉  | 8509/10740 [42:11:22<11:20:45, 18.31s/it]

 79%|███████▉  | 8510/10740 [42:11:35<10:26:26, 16.85s/it]

 79%|███████▉  | 8511/10740 [42:11:57<11:22:58, 18.38s/it]

 79%|███████▉  | 8512/10740 [42:12:14<11:05:42, 17.93s/it]

 79%|███████▉  | 8513/10740 [42:12:34<11:23:22, 18.41s/it]

 79%|███████▉  | 8514/10740 [42:12:45<10:06:23, 16.34s/it]

 79%|███████▉  | 8515/10740 [42:12:56<9:06:35, 14.74s/it]

 79%|███████▉  | 8516/10740 [42:13:08<8:32:30, 13.83s/it]

 79%|███████▉  | 8517/10740 [42:13:29<9:53:14, 16.01s/it]

 79%|███████▉  | 8518/10740 [42:13:43<9:31:42, 15.44s/it]

 79%|███████▉  | 8519/10740 [42:14:05<10:38:38, 17.25s/it]

 79%|███████▉  | 8520/10740 [42:14:24<11:06:30, 18.01s/it]

 79%|███████▉  | 8521/10740 [42:14:43<11:13:14, 18.20s/it]

 79%|███████▉  | 8522/10740 [42:15:00<11:00:19, 17.86s/it]

 79%|███████▉  | 8523/10740 [42:15:18<11:01:32, 17.90s/it]

 79%|███████▉  | 8524/10740 [42:15:39<11:31:06, 18.71s/it]

 79%|███████▉  | 8525/10740 [42:15:57<11:22:14, 18.48s/it]

 79%|███████▉  | 8526/10740 [42:16:15<11:16:13, 18.33s/it]

 79%|███████▉  | 8527/10740 [42:16:34<11:26:44, 18.62s/it]

 79%|███████▉  | 8528/10740 [42:16:52<11:15:15, 18.32s/it]

 79%|███████▉  | 8529/10740 [42:17:06<10:33:45, 17.20s/it]

 79%|███████▉  | 8530/10740 [42:17:26<11:02:00, 17.97s/it]

 79%|███████▉  | 8531/10740 [42:17:48<11:42:09, 19.07s/it]

 79%|███████▉  | 8532/10740 [42:18:05<11:20:24, 18.49s/it]

 79%|███████▉  | 8533/10740 [42:18:20<10:42:40, 17.47s/it]

 79%|███████▉  | 8534/10740 [42:18:39<11:03:21, 18.04s/it]

 79%|███████▉  | 8535/10740 [42:19:00<11:32:41, 18.85s/it]

 79%|███████▉  | 8536/10740 [42:19:21<11:51:49, 19.38s/it]

 79%|███████▉  | 8537/10740 [42:19:39<11:45:43, 19.22s/it]

 79%|███████▉  | 8538/10740 [42:19:55<11:07:26, 18.19s/it]

 80%|███████▉  | 8539/10740 [42:20:15<11:26:52, 18.72s/it]

 80%|███████▉  | 8540/10740 [42:20:31<10:55:06, 17.87s/it]

 80%|███████▉  | 8541/10740 [42:20:51<11:14:50, 18.41s/it]


 80%|███████▉  | 8543/10740 [42:21:30<11:32:35, 18.91s/it]

 80%|███████▉  | 8544/10740 [42:21:50<11:40:51, 19.15s/it]

 80%|███████▉  | 8545/10740 [42:22:10<11:47:44, 19.35s/it]

 80%|███████▉  | 8546/10740 [42:22:30<11:50:40, 19.44s/it]

 80%|███████▉  | 8547/10740 [42:22:50<11:59:27, 19.68s/it]

 80%|███████▉  | 8548/10740 [42:23:10<12:02:52, 19.79s/it]

 80%|███████▉  | 8549/10740 [42:23:30<12:08:42, 19.96s/it]

 80%|███████▉  | 8550/10740 [42:23:49<11:51:19, 19.49s/it]

 80%|███████▉  | 8551/10740 [42:24:07<11:41:11, 19.22s/it]

 80%|███████▉  | 8552/10740 [42:24:23<11:02:41, 18.17s/it]

 80%|███████▉  | 8553/10740 [42:24:44<11:37:12, 19.13s/it]

 80%|███████▉  | 8554/10740 [42:25:06<12:02:47, 19.84s/it]
{'loss': 0.173, 'learning_rate': 2.095548734858944e-07, 'rewards/chosen': -3.3647470474243164, 'rewards/rejected': -7.67200231552124, 'rewards/accuracies': 0.875, 'rewards/margins': 4.307255744934082, 'policy_logps/rejected': -361.50933837890625, 'policy_logps/chosen': -303.0408020019531, 'referece_logps/rejected': -284.789306640625, 'referece_logps/chosen': -269.393310546875, 'logits/rejected': -0.8647979497909546, 'logits/chosen': -0.8824024796485901, 'epoch': 4.78}


 80%|███████▉  | 8556/10740 [42:25:43<11:44:00, 19.34s/it]

 80%|███████▉  | 8557/10740 [42:26:00<11:14:45, 18.55s/it]

 80%|███████▉  | 8558/10740 [42:26:19<11:20:39, 18.72s/it]

 80%|███████▉  | 8559/10740 [42:26:39<11:37:04, 19.18s/it]

 80%|███████▉  | 8560/10740 [42:26:59<11:43:35, 19.36s/it]

 80%|███████▉  | 8561/10740 [42:27:19<11:48:47, 19.52s/it]

 80%|███████▉  | 8562/10740 [42:27:36<11:24:00, 18.84s/it]

 80%|███████▉  | 8563/10740 [42:27:51<10:36:26, 17.54s/it]

 80%|███████▉  | 8564/10740 [42:28:09<10:45:35, 17.80s/it]

 80%|███████▉  | 8565/10740 [42:28:29<11:10:53, 18.51s/it]

 80%|███████▉  | 8566/10740 [42:28:47<11:04:13, 18.33s/it]

 80%|███████▉  | 8567/10740 [42:29:08<11:25:54, 18.94s/it]

 80%|███████▉  | 8568/10740 [42:29:22<10:33:46, 17.51s/it]

 80%|███████▉  | 8569/10740 [42:29:43<11:14:19, 18.64s/it]

 80%|███████▉  | 8570/10740 [42:30:00<10:59:34, 18.24s/it]

 80%|███████▉  | 8571/10740 [42:30:15<10:20:42, 17.17s/it]

 80%|███████▉  | 8572/10740 [42:30:35<10:46:04, 17.88s/it]

 80%|███████▉  | 8573/10740 [42:30:54<11:05:03, 18.41s/it]

 80%|███████▉  | 8574/10740 [42:31:05<9:46:28, 16.25s/it]

 80%|███████▉  | 8575/10740 [42:31:22<9:48:23, 16.31s/it]

 80%|███████▉  | 8576/10740 [42:31:38<9:42:57, 16.16s/it]

 80%|███████▉  | 8577/10740 [42:31:55<9:51:01, 16.39s/it]

 80%|███████▉  | 8578/10740 [42:32:12<10:04:46, 16.78s/it]

 80%|███████▉  | 8579/10740 [42:32:29<9:59:39, 16.65s/it]

 80%|███████▉  | 8580/10740 [42:32:42<9:25:57, 15.72s/it]

 80%|███████▉  | 8581/10740 [42:32:58<9:23:12, 15.65s/it]

 80%|███████▉  | 8582/10740 [42:33:19<10:25:12, 17.38s/it]

 80%|███████▉  | 8583/10740 [42:33:36<10:19:18, 17.23s/it]

 80%|███████▉  | 8584/10740 [42:33:56<10:46:48, 18.00s/it]

 80%|███████▉  | 8585/10740 [42:34:16<11:06:08, 18.55s/it]

 80%|███████▉  | 8586/10740 [42:34:27<9:44:04, 16.27s/it]

 80%|███████▉  | 8587/10740 [42:34:38<8:53:34, 14.87s/it]

 80%|███████▉  | 8588/10740 [42:34:50<8:25:45, 14.10s/it]

 80%|███████▉  | 8589/10740 [42:35:13<9:54:47, 16.59s/it]

 80%|███████▉  | 8590/10740 [42:35:32<10:25:56, 17.47s/it]

 80%|███████▉  | 8591/10740 [42:35:45<9:32:46, 15.99s/it]

 80%|████████  | 8592/10740 [42:36:05<10:11:26, 17.08s/it]

 80%|████████  | 8593/10740 [42:36:23<10:23:48, 17.43s/it]

 80%|████████  | 8594/10740 [42:36:37<9:51:36, 16.54s/it]

 80%|████████  | 8595/10740 [42:36:55<10:03:44, 16.89s/it]

 80%|████████  | 8596/10740 [42:37:16<10:48:26, 18.15s/it]

 80%|████████  | 8597/10740 [42:37:36<11:04:56, 18.62s/it]

 80%|████████  | 8598/10740 [42:37:55<11:13:56, 18.88s/it]

 80%|████████  | 8599/10740 [42:38:11<10:44:12, 18.05s/it]

 80%|████████  | 8600/10740 [42:38:29<10:37:21, 17.87s/it]

 80%|████████  | 8601/10740 [42:38:49<11:03:29, 18.61s/it]

 80%|████████  | 8602/10740 [42:39:09<11:11:52, 18.86s/it]

 80%|████████  | 8603/10740 [42:39:22<10:13:25, 17.22s/it]

 80%|████████  | 8604/10740 [42:39:42<10:41:26, 18.02s/it]

 80%|████████  | 8605/10740 [42:40:02<11:05:43, 18.71s/it]

 80%|████████  | 8606/10740 [42:40:22<11:16:27, 19.02s/it]

 80%|████████  | 8607/10740 [42:40:40<11:07:07, 18.77s/it]

 80%|████████  | 8608/10740 [42:41:00<11:20:32, 19.15s/it]
{'loss': 0.1079, 'learning_rate': 1.996847310824944e-07, 'rewards/chosen': -3.1400558948516846, 'rewards/rejected': -9.997885704040527, 'rewards/accuracies': 1.0, 'rewards/margins': 6.857830047607422, 'policy_logps/rejected': -571.6192626953125, 'policy_logps/chosen': -528.804931640625, 'referece_logps/rejected': -471.640380859375, 'referece_logps/chosen': -497.4043884277344, 'logits/rejected': -0.03540446236729622, 'logits/chosen': -0.01983746513724327, 'epoch': 4.81}

 80%|████████  | 8609/10740 [42:41:21<11:40:33, 19.72s/it]


 80%|████████  | 8611/10740 [42:42:03<11:51:51, 20.06s/it]

 80%|████████  | 8612/10740 [42:42:23<11:50:52, 20.04s/it]
{'loss': 0.3553, 'learning_rate': 1.9896202116728922e-07, 'rewards/chosen': -3.284106969833374, 'rewards/rejected': -7.48650598526001, 'rewards/accuracies': 0.875, 'rewards/margins': 4.202398777008057, 'policy_logps/rejected': -323.05340576171875, 'policy_logps/chosen': -360.85150146484375, 'referece_logps/rejected': -248.18836975097656, 'referece_logps/chosen': -328.01043701171875, 'logits/rejected': -0.11225992441177368, 'logits/chosen': -0.10221423208713531, 'epoch': 4.81}

 80%|████████  | 8613/10740 [42:42:43<11:56:13, 20.20s/it]

 80%|████████  | 8614/10740 [42:42:55<10:31:24, 17.82s/it]


 80%|████████  | 8616/10740 [42:43:35<11:02:28, 18.71s/it]

 80%|████████  | 8617/10740 [42:43:54<11:14:19, 19.06s/it]

 80%|████████  | 8618/10740 [42:44:16<11:38:09, 19.74s/it]

 80%|████████  | 8619/10740 [42:44:36<11:46:02, 19.97s/it]

 80%|████████  | 8620/10740 [42:44:50<10:41:44, 18.16s/it]

 80%|████████  | 8621/10740 [42:45:06<10:17:10, 17.48s/it]
{'loss': 0.1304, 'learning_rate': 1.9734018755463644e-07, 'rewards/chosen': -4.656231880187988, 'rewards/rejected': -8.321794509887695, 'rewards/accuracies': 1.0, 'rewards/margins': 3.665562152862549, 'policy_logps/rejected': -568.2159423828125, 'policy_logps/chosen': -479.37628173828125, 'referece_logps/rejected': -484.99798583984375, 'referece_logps/chosen': -432.81396484375, 'logits/rejected': 0.5140055418014526, 'logits/chosen': 0.5148115158081055, 'epoch': 4.82}

 80%|████████  | 8622/10740 [42:45:25<10:37:18, 18.05s/it]


 80%|████████  | 8624/10740 [42:45:59<10:14:27, 17.42s/it]

 80%|████████  | 8625/10740 [42:46:12<9:30:17, 16.18s/it]

 80%|████████  | 8626/10740 [42:46:25<8:52:24, 15.11s/it]

 80%|████████  | 8627/10740 [42:46:44<9:35:09, 16.33s/it]
{'loss': 0.1928, 'learning_rate': 1.962622495829811e-07, 'rewards/chosen': -3.0870842933654785, 'rewards/rejected': -7.814406871795654, 'rewards/accuracies': 1.0, 'rewards/margins': 4.727322578430176, 'policy_logps/rejected': -331.5542297363281, 'policy_logps/chosen': -402.24951171875, 'referece_logps/rejected': -253.41012573242188, 'referece_logps/chosen': -371.3786315917969, 'logits/rejected': 0.31044843792915344, 'logits/chosen': 0.11442258954048157, 'epoch': 4.82}

 80%|████████  | 8628/10740 [42:46:56<8:46:28, 14.96s/it]

 80%|████████  | 8629/10740 [42:47:15<9:35:31, 16.36s/it]


 80%|████████  | 8631/10740 [42:47:53<10:27:13, 17.84s/it]

 80%|████████  | 8632/10740 [42:48:10<10:19:56, 17.65s/it]
{'loss': 0.1658, 'learning_rate': 1.953659780926621e-07, 'rewards/chosen': -3.1509149074554443, 'rewards/rejected': -7.657057762145996, 'rewards/accuracies': 0.875, 'rewards/margins': 4.506142616271973, 'policy_logps/rejected': -272.1951904296875, 'policy_logps/chosen': -500.34637451171875, 'referece_logps/rejected': -195.62460327148438, 'referece_logps/chosen': -468.8372802734375, 'logits/rejected': -0.13373598456382751, 'logits/chosen': -0.36613771319389343, 'epoch': 4.82}


 80%|████████  | 8634/10740 [42:48:42<10:03:39, 17.20s/it]
{'loss': 0.2196, 'learning_rate': 1.9500798166651866e-07, 'rewards/chosen': -4.098980903625488, 'rewards/rejected': -7.144364356994629, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0453834533691406, 'policy_logps/rejected': -785.5841064453125, 'policy_logps/chosen': -424.2972412109375, 'referece_logps/rejected': -714.1404418945312, 'referece_logps/chosen': -383.3074035644531, 'logits/rejected': 0.04216271638870239, 'logits/chosen': 0.38816940784454346, 'epoch': 4.82}


 80%|████████  | 8636/10740 [42:49:13<9:22:45, 16.05s/it]

 80%|████████  | 8637/10740 [42:49:35<10:22:10, 17.75s/it]

 80%|████████  | 8638/10740 [42:49:48<9:40:34, 16.57s/it]
{'loss': 0.0798, 'learning_rate': 1.9429286753632713e-07, 'rewards/chosen': -3.552351951599121, 'rewards/rejected': -9.057640075683594, 'rewards/accuracies': 1.0, 'rewards/margins': 5.505288124084473, 'policy_logps/rejected': -436.0211181640625, 'policy_logps/chosen': -538.1566162109375, 'referece_logps/rejected': -345.4447021484375, 'referece_logps/chosen': -502.6330871582031, 'logits/rejected': 0.0536530464887619, 'logits/chosen': -0.06645867973566055, 'epoch': 4.83}


 80%|████████  | 8640/10740 [42:50:25<10:13:59, 17.54s/it]

 80%|████████  | 8641/10740 [42:50:45<10:35:36, 18.17s/it]

 80%|████████  | 8642/10740 [42:51:05<10:59:44, 18.87s/it]
{'loss': 0.2928, 'learning_rate': 1.9357892590263646e-07, 'rewards/chosen': -3.907468318939209, 'rewards/rejected': -9.43313217163086, 'rewards/accuracies': 1.0, 'rewards/margins': 5.525664329528809, 'policy_logps/rejected': -535.4535522460938, 'policy_logps/chosen': -620.5958251953125, 'referece_logps/rejected': -441.1221923828125, 'referece_logps/chosen': -581.5211181640625, 'logits/rejected': 0.46337273716926575, 'logits/chosen': 0.25374430418014526, 'epoch': 4.83}


 80%|████████  | 8644/10740 [42:51:38<10:28:24, 17.99s/it]

 80%|████████  | 8645/10740 [42:51:52<9:47:09, 16.82s/it]

 81%|████████  | 8646/10740 [42:52:08<9:40:08, 16.62s/it]

 81%|████████  | 8647/10740 [42:52:27<10:01:18, 17.24s/it]

 81%|████████  | 8648/10740 [42:52:45<10:05:45, 17.37s/it]

 81%|████████  | 8649/10740 [42:52:59<9:30:15, 16.36s/it]

 81%|████████  | 8650/10740 [42:53:18<10:05:40, 17.39s/it]
{'loss': 0.2263, 'learning_rate': 1.9215456427887254e-07, 'rewards/chosen': -3.9784650802612305, 'rewards/rejected': -8.642032623291016, 'rewards/accuracies': 0.875, 'rewards/margins': 4.663567543029785, 'policy_logps/rejected': -371.7337341308594, 'policy_logps/chosen': -481.97503662109375, 'referece_logps/rejected': -285.31341552734375, 'referece_logps/chosen': -442.1903991699219, 'logits/rejected': 0.2087203860282898, 'logits/chosen': 0.10575668513774872, 'epoch': 4.83}


 81%|████████  | 8652/10740 [42:53:51<9:52:18, 17.02s/it]

 81%|████████  | 8653/10740 [42:54:07<9:46:19, 16.86s/it]
{'loss': 0.2022, 'learning_rate': 1.91621640571068e-07, 'rewards/chosen': -3.8185038566589355, 'rewards/rejected': -8.18887996673584, 'rewards/accuracies': 1.0, 'rewards/margins': 4.370376110076904, 'policy_logps/rejected': -437.53594970703125, 'policy_logps/chosen': -351.28460693359375, 'referece_logps/rejected': -355.64715576171875, 'referece_logps/chosen': -313.09954833984375, 'logits/rejected': -0.5716996192932129, 'logits/chosen': -0.50694340467453, 'epoch': 4.83}


 81%|████████  | 8655/10740 [42:54:40<9:25:15, 16.27s/it]

 81%|████████  | 8656/10740 [42:55:01<10:20:13, 17.86s/it]

 81%|████████  | 8657/10740 [42:55:20<10:33:36, 18.25s/it]

 81%|████████  | 8658/10740 [42:55:40<10:44:13, 18.57s/it]
{'loss': 0.2798, 'learning_rate': 1.907349050863707e-07, 'rewards/chosen': -3.9304563999176025, 'rewards/rejected': -9.246373176574707, 'rewards/accuracies': 1.0, 'rewards/margins': 5.315917015075684, 'policy_logps/rejected': -456.2489013671875, 'policy_logps/chosen': -513.0145263671875, 'referece_logps/rejected': -363.7851257324219, 'referece_logps/chosen': -473.71002197265625, 'logits/rejected': 0.7841253876686096, 'logits/chosen': 0.6702826619148254, 'epoch': 4.84}


 81%|████████  | 8660/10740 [42:56:07<9:11:37, 15.91s/it]

 81%|████████  | 8661/10740 [42:56:28<10:12:28, 17.68s/it]
{'loss': 0.1867, 'learning_rate': 1.9020374692226494e-07, 'rewards/chosen': -4.924594402313232, 'rewards/rejected': -8.407687187194824, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4830920696258545, 'policy_logps/rejected': -411.8793029785156, 'policy_logps/chosen': -402.92767333984375, 'referece_logps/rejected': -327.8023986816406, 'referece_logps/chosen': -353.6817321777344, 'logits/rejected': 0.502376139163971, 'logits/chosen': 0.5232117772102356, 'epoch': 4.84}


 81%|████████  | 8663/10740 [42:57:04<10:12:50, 17.70s/it]
{'loss': 0.2057, 'learning_rate': 1.8985000971746478e-07, 'rewards/chosen': -4.617773056030273, 'rewards/rejected': -9.425990104675293, 'rewards/accuracies': 1.0, 'rewards/margins': 4.808217525482178, 'policy_logps/rejected': -404.11627197265625, 'policy_logps/chosen': -452.9223327636719, 'referece_logps/rejected': -309.8563537597656, 'referece_logps/chosen': -406.7445983886719, 'logits/rejected': -0.025706268846988678, 'logits/chosen': -0.21382352709770203, 'epoch': 4.84}


 81%|████████  | 8665/10740 [42:57:39<10:16:23, 17.82s/it]

 81%|████████  | 8666/10740 [42:58:00<10:44:33, 18.65s/it]

 81%|████████  | 8667/10740 [42:58:20<10:57:38, 19.03s/it]

 81%|████████  | 8668/10740 [42:58:39<11:04:11, 19.23s/it]
{'loss': 0.2705, 'learning_rate': 1.8896695647643467e-07, 'rewards/chosen': -3.2099099159240723, 'rewards/rejected': -7.755653381347656, 'rewards/accuracies': 1.0, 'rewards/margins': 4.545742988586426, 'policy_logps/rejected': -359.68475341796875, 'policy_logps/chosen': -348.8318786621094, 'referece_logps/rejected': -282.12823486328125, 'referece_logps/chosen': -316.7327880859375, 'logits/rejected': -0.2033586949110031, 'logits/chosen': -0.2651965022087097, 'epoch': 4.84}

 81%|████████  | 8669/10740 [42:58:58<11:00:37, 19.14s/it]

 81%|████████  | 8670/10740 [42:59:20<11:29:28, 19.98s/it]


 81%|████████  | 8672/10740 [42:59:57<11:01:11, 19.18s/it]

 81%|████████  | 8673/10740 [43:00:10<9:48:41, 17.09s/it]
{'loss': 0.0981, 'learning_rate': 1.8808574737117676e-07, 'rewards/chosen': -3.055460214614868, 'rewards/rejected': -8.117655754089355, 'rewards/accuracies': 0.875, 'rewards/margins': 5.062195777893066, 'policy_logps/rejected': -337.7842712402344, 'policy_logps/chosen': -345.3155822753906, 'referece_logps/rejected': -256.60772705078125, 'referece_logps/chosen': -314.7609558105469, 'logits/rejected': -0.393741637468338, 'logits/chosen': -0.7239001393318176, 'epoch': 4.85}

 81%|████████  | 8674/10740 [43:00:26<9:43:20, 16.94s/it]

 81%|████████  | 8675/10740 [43:00:40<9:12:21, 16.05s/it]

 81%|████████  | 8676/10740 [43:00:52<8:32:43, 14.90s/it]

 81%|████████  | 8677/10740 [43:01:08<8:44:03, 15.24s/it]

 81%|████████  | 8678/10740 [43:01:27<9:13:20, 16.10s/it]


 81%|████████  | 8680/10740 [43:02:02<9:46:20, 17.08s/it]
{'loss': 0.1683, 'learning_rate': 1.8685515658616403e-07, 'rewards/chosen': -4.371708393096924, 'rewards/rejected': -8.969995498657227, 'rewards/accuracies': 1.0, 'rewards/margins': 4.598287105560303, 'policy_logps/rejected': -639.9402465820312, 'policy_logps/chosen': -487.15521240234375, 'referece_logps/rejected': -550.2402954101562, 'referece_logps/chosen': -443.4381103515625, 'logits/rejected': -0.8601006269454956, 'logits/chosen': -0.646458625793457, 'epoch': 4.85}

 81%|████████  | 8681/10740 [43:02:16<9:18:56, 16.29s/it]


 81%|████████  | 8683/10740 [43:02:58<10:30:37, 18.39s/it]

 81%|████████  | 8684/10740 [43:03:16<10:28:50, 18.35s/it]

 81%|████████  | 8685/10740 [43:03:31<9:56:31, 17.42s/it]

 81%|████████  | 8686/10740 [43:03:50<10:08:26, 17.77s/it]
{'loss': 0.2384, 'learning_rate': 1.8580324861944053e-07, 'rewards/chosen': -4.282240867614746, 'rewards/rejected': -6.68869686126709, 'rewards/accuracies': 0.75, 'rewards/margins': 2.406456232070923, 'policy_logps/rejected': -407.0594177246094, 'policy_logps/chosen': -384.408203125, 'referece_logps/rejected': -340.17242431640625, 'referece_logps/chosen': -341.5858154296875, 'logits/rejected': -0.3870997428894043, 'logits/chosen': -0.5780289173126221, 'epoch': 4.85}


 81%|████████  | 8688/10740 [43:04:25<10:11:43, 17.89s/it]

 81%|████████  | 8689/10740 [43:04:45<10:28:25, 18.38s/it]

 81%|████████  | 8690/10740 [43:05:06<10:54:14, 19.15s/it]

 81%|████████  | 8691/10740 [43:05:26<11:04:44, 19.47s/it]

 81%|████████  | 8692/10740 [43:05:46<11:07:30, 19.56s/it]

 81%|████████  | 8693/10740 [43:06:04<10:55:00, 19.20s/it]

 81%|████████  | 8694/10740 [43:06:24<10:58:55, 19.32s/it]
{'loss': 0.1385, 'learning_rate': 1.8440485223546686e-07, 'rewards/chosen': -4.590152740478516, 'rewards/rejected': -8.454283714294434, 'rewards/accuracies': 0.875, 'rewards/margins': 3.864130973815918, 'policy_logps/rejected': -265.9141540527344, 'policy_logps/chosen': -351.4464111328125, 'referece_logps/rejected': -181.3712921142578, 'referece_logps/chosen': -305.54486083984375, 'logits/rejected': 0.559026300907135, 'logits/chosen': 0.4166427254676819, 'epoch': 4.86}


 81%|████████  | 8696/10740 [43:06:54<9:36:22, 16.92s/it]

 81%|████████  | 8697/10740 [43:07:16<10:24:56, 18.35s/it]

 81%|████████  | 8698/10740 [43:07:33<10:14:42, 18.06s/it]

 81%|████████  | 8699/10740 [43:07:47<9:31:11, 16.79s/it]
{'loss': 0.108, 'learning_rate': 1.8353326484109677e-07, 'rewards/chosen': -3.5762362480163574, 'rewards/rejected': -7.585901737213135, 'rewards/accuracies': 0.875, 'rewards/margins': 4.009665012359619, 'policy_logps/rejected': -384.98834228515625, 'policy_logps/chosen': -416.23236083984375, 'referece_logps/rejected': -309.1293029785156, 'referece_logps/chosen': -380.47003173828125, 'logits/rejected': -0.182822585105896, 'logits/chosen': -0.014192190021276474, 'epoch': 4.86}


 81%|████████  | 8701/10740 [43:08:22<9:34:11, 16.90s/it]

 81%|████████  | 8702/10740 [43:08:44<10:28:43, 18.51s/it]

 81%|████████  | 8703/10740 [43:08:55<9:14:21, 16.33s/it]

 81%|████████  | 8704/10740 [43:09:14<9:37:36, 17.02s/it]
{'loss': 0.1243, 'learning_rate': 1.826635339376863e-07, 'rewards/chosen': -4.966775417327881, 'rewards/rejected': -8.906108856201172, 'rewards/accuracies': 0.75, 'rewards/margins': 3.93933367729187, 'policy_logps/rejected': -520.992431640625, 'policy_logps/chosen': -441.99615478515625, 'referece_logps/rejected': -431.93133544921875, 'referece_logps/chosen': -392.3283996582031, 'logits/rejected': -0.52219557762146, 'logits/chosen': -0.659099817276001, 'epoch': 4.86}


 81%|████████  | 8706/10740 [43:09:45<9:19:00, 16.49s/it]
{'loss': 0.183, 'learning_rate': 1.8231616183691567e-07, 'rewards/chosen': -3.84039306640625, 'rewards/rejected': -8.708916664123535, 'rewards/accuracies': 1.0, 'rewards/margins': 4.868523120880127, 'policy_logps/rejected': -372.498779296875, 'policy_logps/chosen': -391.4645690917969, 'referece_logps/rejected': -285.40960693359375, 'referece_logps/chosen': -353.0606689453125, 'logits/rejected': 0.23160208761692047, 'logits/chosen': 0.041750311851501465, 'epoch': 4.86}


 81%|████████  | 8708/10740 [43:10:26<10:33:54, 18.72s/it]

 81%|████████  | 8709/10740 [43:10:46<10:43:38, 19.01s/it]
{'loss': 0.1155, 'learning_rate': 1.8179566150283852e-07, 'rewards/chosen': -3.159660577774048, 'rewards/rejected': -8.99582290649414, 'rewards/accuracies': 1.0, 'rewards/margins': 5.836162567138672, 'policy_logps/rejected': -742.3334350585938, 'policy_logps/chosen': -410.4693603515625, 'referece_logps/rejected': -652.375244140625, 'referece_logps/chosen': -378.87274169921875, 'logits/rejected': 0.2634695768356323, 'logits/chosen': 0.3644876182079315, 'epoch': 4.87}


 81%|████████  | 8711/10740 [43:11:18<10:04:36, 17.88s/it]
{'loss': 0.2849, 'learning_rate': 1.8144903334227568e-07, 'rewards/chosen': -2.6305155754089355, 'rewards/rejected': -7.6622819900512695, 'rewards/accuracies': 0.75, 'rewards/margins': 5.031766414642334, 'policy_logps/rejected': -380.1044616699219, 'policy_logps/chosen': -303.4153747558594, 'referece_logps/rejected': -303.48162841796875, 'referece_logps/chosen': -277.1101989746094, 'logits/rejected': 0.08558855950832367, 'logits/chosen': 0.1819857507944107, 'epoch': 4.87}

 81%|████████  | 8712/10740 [43:11:36<9:57:27, 17.68s/it]


 81%|████████  | 8714/10740 [43:12:14<10:21:34, 18.41s/it]

 81%|████████  | 8715/10740 [43:12:34<10:35:59, 18.84s/it]

 81%|████████  | 8716/10740 [43:12:47<9:33:37, 17.00s/it]
{'loss': 0.2081, 'learning_rate': 1.8058376607774362e-07, 'rewards/chosen': -4.87505578994751, 'rewards/rejected': -9.30416488647461, 'rewards/accuracies': 1.0, 'rewards/margins': 4.429108619689941, 'policy_logps/rejected': -428.9571228027344, 'policy_logps/chosen': -446.57916259765625, 'referece_logps/rejected': -335.91552734375, 'referece_logps/chosen': -397.8285827636719, 'logits/rejected': -0.566763162612915, 'logits/chosen': -0.6827174425125122, 'epoch': 4.87}


 81%|████████  | 8718/10740 [43:13:14<8:29:46, 15.13s/it]

 81%|████████  | 8719/10740 [43:13:33<9:04:17, 16.16s/it]
{'loss': 0.2028, 'learning_rate': 1.8006549992811282e-07, 'rewards/chosen': -3.7765541076660156, 'rewards/rejected': -8.230303764343262, 'rewards/accuracies': 1.0, 'rewards/margins': 4.453749179840088, 'policy_logps/rejected': -398.35791015625, 'policy_logps/chosen': -445.70111083984375, 'referece_logps/rejected': -316.0549011230469, 'referece_logps/chosen': -407.935546875, 'logits/rejected': -0.5057907700538635, 'logits/chosen': -0.6591511964797974, 'epoch': 4.87}

 81%|████████  | 8720/10740 [43:13:53<9:49:51, 17.52s/it]

 81%|████████  | 8721/10740 [43:14:08<9:14:56, 16.49s/it]


 81%|████████  | 8723/10740 [43:14:44<9:49:34, 17.54s/it]
{'loss': 0.2086, 'learning_rate': 1.793755225192053e-07, 'rewards/chosen': -3.370838165283203, 'rewards/rejected': -7.73279333114624, 'rewards/accuracies': 1.0, 'rewards/margins': 4.361954212188721, 'policy_logps/rejected': -497.9754333496094, 'policy_logps/chosen': -443.2904052734375, 'referece_logps/rejected': -420.6474609375, 'referece_logps/chosen': -409.5820007324219, 'logits/rejected': -0.4167362451553345, 'logits/chosen': -0.3526555299758911, 'epoch': 4.87}


 81%|████████  | 8725/10740 [43:15:27<10:52:00, 19.41s/it]

 81%|████████  | 8726/10740 [43:15:46<10:53:25, 19.47s/it]

 81%|████████▏ | 8727/10740 [43:16:06<10:58:45, 19.64s/it]
{'loss': 0.218, 'learning_rate': 1.786867393151018e-07, 'rewards/chosen': -4.7521138191223145, 'rewards/rejected': -8.75516128540039, 'rewards/accuracies': 0.875, 'rewards/margins': 4.00304651260376, 'policy_logps/rejected': -440.82220458984375, 'policy_logps/chosen': -368.3072509765625, 'referece_logps/rejected': -353.2706298828125, 'referece_logps/chosen': -320.7861022949219, 'logits/rejected': 0.3102492094039917, 'logits/chosen': 0.2781596779823303, 'epoch': 4.88}


 81%|████████▏ | 8729/10740 [43:16:44<10:44:55, 19.24s/it]

 81%|████████▏ | 8730/10740 [43:16:57<9:34:52, 17.16s/it]
{'loss': 0.1752, 'learning_rate': 1.7817093621196499e-07, 'rewards/chosen': -2.3940484523773193, 'rewards/rejected': -5.938013553619385, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5439653396606445, 'policy_logps/rejected': -392.26605224609375, 'policy_logps/chosen': -460.1282043457031, 'referece_logps/rejected': -332.8858642578125, 'referece_logps/chosen': -436.1877746582031, 'logits/rejected': -0.5371268391609192, 'logits/chosen': -0.6203396320343018, 'epoch': 4.88}


 81%|████████▏ | 8732/10740 [43:17:31<9:18:10, 16.68s/it]

 81%|████████▏ | 8733/10740 [43:17:50<9:48:21, 17.59s/it]

 81%|████████▏ | 8734/10740 [43:18:02<8:51:34, 15.90s/it]
{'loss': 0.1007, 'learning_rate': 1.7748424527711336e-07, 'rewards/chosen': -3.3616385459899902, 'rewards/rejected': -9.161038398742676, 'rewards/accuracies': 1.0, 'rewards/margins': 5.799399375915527, 'policy_logps/rejected': -428.4498291015625, 'policy_logps/chosen': -401.2562255859375, 'referece_logps/rejected': -336.8394775390625, 'referece_logps/chosen': -367.6398620605469, 'logits/rejected': 0.02677575871348381, 'logits/chosen': -0.18970930576324463, 'epoch': 4.88}

 81%|████████▏ | 8735/10740 [43:18:22<9:29:46, 17.05s/it]

 81%|████████▏ | 8736/10740 [43:18:39<9:31:25, 17.11s/it]

 81%|████████▏ | 8737/10740 [43:19:00<10:02:17, 18.04s/it]


 81%|████████▏ | 8739/10740 [43:19:39<10:36:09, 19.08s/it]
{'loss': 0.1586, 'learning_rate': 1.7662756494636044e-07, 'rewards/chosen': -3.6021571159362793, 'rewards/rejected': -8.824822425842285, 'rewards/accuracies': 1.0, 'rewards/margins': 5.222665309906006, 'policy_logps/rejected': -509.31158447265625, 'policy_logps/chosen': -539.8948974609375, 'referece_logps/rejected': -421.06341552734375, 'referece_logps/chosen': -503.8733825683594, 'logits/rejected': 0.610235333442688, 'logits/chosen': 0.683603823184967, 'epoch': 4.88}


 81%|████████▏ | 8741/10740 [43:20:17<10:43:12, 19.31s/it]
{'loss': 0.1876, 'learning_rate': 1.762854169192186e-07, 'rewards/chosen': -4.195655345916748, 'rewards/rejected': -7.310223579406738, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1145687103271484, 'policy_logps/rejected': -445.11474609375, 'policy_logps/chosen': -367.5973815917969, 'referece_logps/rejected': -372.0125427246094, 'referece_logps/chosen': -325.64080810546875, 'logits/rejected': 0.3070845603942871, 'logits/chosen': 0.30336955189704895, 'epoch': 4.88}

 81%|████████▏ | 8742/10740 [43:20:36<10:34:56, 19.07s/it]

 81%|████████▏ | 8743/10740 [43:20:50<9:50:31, 17.74s/it]

 81%|████████▏ | 8744/10740 [43:21:10<10:12:16, 18.41s/it]

 81%|████████▏ | 8745/10740 [43:21:26<9:45:41, 17.61s/it]

 81%|████████▏ | 8746/10740 [43:21:48<10:28:18, 18.91s/it]

 81%|████████▏ | 8747/10740 [43:22:08<10:36:56, 19.18s/it]

 81%|████████▏ | 8748/10740 [43:22:28<10:46:44, 19.48s/it]

 81%|████████▏ | 8749/10740 [43:22:50<11:08:34, 20.15s/it]

 81%|████████▏ | 8750/10740 [43:23:08<10:49:55, 19.60s/it]

 81%|████████▏ | 8751/10740 [43:23:30<11:17:01, 20.42s/it]


 81%|████████▏ | 8753/10740 [43:24:05<10:25:17, 18.88s/it]
{'loss': 0.1513, 'learning_rate': 1.7423882628963715e-07, 'rewards/chosen': -3.1320672035217285, 'rewards/rejected': -7.3492045402526855, 'rewards/accuracies': 0.875, 'rewards/margins': 4.217137336730957, 'policy_logps/rejected': -494.4269714355469, 'policy_logps/chosen': -495.0606689453125, 'referece_logps/rejected': -420.93487548828125, 'referece_logps/chosen': -463.74005126953125, 'logits/rejected': -0.023511290550231934, 'logits/chosen': 0.022358253598213196, 'epoch': 4.89}

 82%|████████▏ | 8754/10740 [43:24:24<10:28:29, 18.99s/it]

 82%|████████▏ | 8755/10740 [43:24:46<10:53:46, 19.76s/it]

 82%|████████▏ | 8756/10740 [43:25:00<10:01:04, 18.18s/it]

 82%|████████▏ | 8757/10740 [43:25:20<10:16:08, 18.64s/it]

 82%|████████▏ | 8758/10740 [43:25:36<9:43:52, 17.68s/it]


 82%|████████▏ | 8760/10740 [43:26:15<10:13:48, 18.60s/it]

 82%|████████▏ | 8761/10740 [43:26:35<10:27:02, 19.01s/it]

 82%|████████▏ | 8762/10740 [43:26:53<10:19:55, 18.80s/it]

 82%|████████▏ | 8763/10740 [43:27:09<9:49:44, 17.90s/it]
{'loss': 0.2161, 'learning_rate': 1.7254159450650984e-07, 'rewards/chosen': -5.062567710876465, 'rewards/rejected': -8.408222198486328, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3456552028656006, 'policy_logps/rejected': -570.8961791992188, 'policy_logps/chosen': -478.4503479003906, 'referece_logps/rejected': -486.8138732910156, 'referece_logps/chosen': -427.82464599609375, 'logits/rejected': 0.4851848781108856, 'logits/chosen': 0.6035534739494324, 'epoch': 4.9}

 82%|████████▏ | 8764/10740 [43:27:24<9:21:41, 17.06s/it]

 82%|████████▏ | 8765/10740 [43:27:39<8:55:16, 16.26s/it]

 82%|████████▏ | 8766/10740 [43:27:59<9:31:39, 17.38s/it]


 82%|████████▏ | 8768/10740 [43:28:36<9:46:43, 17.85s/it]

 82%|████████▏ | 8769/10740 [43:28:55<10:06:14, 18.45s/it]
{'loss': 0.2648, 'learning_rate': 1.7152686689863826e-07, 'rewards/chosen': -4.395846366882324, 'rewards/rejected': -7.249163627624512, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8533177375793457, 'policy_logps/rejected': -331.7456970214844, 'policy_logps/chosen': -352.2206115722656, 'referece_logps/rejected': -259.2540588378906, 'referece_logps/chosen': -308.26214599609375, 'logits/rejected': -0.33067023754119873, 'logits/chosen': -0.2145177125930786, 'epoch': 4.9}


 82%|████████▏ | 8771/10740 [43:29:26<9:18:04, 17.01s/it]

 82%|████████▏ | 8772/10740 [43:29:47<10:02:01, 18.35s/it]
{'loss': 0.1851, 'learning_rate': 1.7102052013457624e-07, 'rewards/chosen': -2.799470901489258, 'rewards/rejected': -8.397457122802734, 'rewards/accuracies': 1.0, 'rewards/margins': 5.597985744476318, 'policy_logps/rejected': -332.12799072265625, 'policy_logps/chosen': -441.3979797363281, 'referece_logps/rejected': -248.15342712402344, 'referece_logps/chosen': -413.40325927734375, 'logits/rejected': 0.22001871466636658, 'logits/chosen': 0.16947227716445923, 'epoch': 4.9}

 82%|████████▏ | 8773/10740 [43:30:09<10:30:50, 19.24s/it]

 82%|████████▏ | 8774/10740 [43:30:28<10:35:05, 19.38s/it]

 82%|████████▏ | 8775/10740 [43:30:41<9:25:24, 17.26s/it]

 82%|████████▏ | 8776/10740 [43:31:00<9:47:51, 17.96s/it]

 82%|████████▏ | 8777/10740 [43:31:19<9:51:32, 18.08s/it]

 82%|████████▏ | 8778/10740 [43:31:35<9:34:57, 17.58s/it]

 82%|████████▏ | 8779/10740 [43:31:55<9:54:47, 18.20s/it]

 82%|████████▏ | 8780/10740 [43:32:15<10:15:05, 18.83s/it]


 82%|████████▏ | 8782/10740 [43:32:46<9:27:08, 17.38s/it]

 82%|████████▏ | 8783/10740 [43:33:06<9:49:25, 18.07s/it]
{'loss': 0.2057, 'learning_rate': 1.6916972410296782e-07, 'rewards/chosen': -3.572164297103882, 'rewards/rejected': -6.739394664764404, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1672308444976807, 'policy_logps/rejected': -412.6915588378906, 'policy_logps/chosen': -350.1394958496094, 'referece_logps/rejected': -345.2976379394531, 'referece_logps/chosen': -314.4178771972656, 'logits/rejected': -0.37109148502349854, 'logits/chosen': -0.3412957191467285, 'epoch': 4.91}

 82%|████████▏ | 8784/10740 [43:33:25<9:58:24, 18.36s/it]

 82%|████████▏ | 8785/10740 [43:33:45<10:13:25, 18.83s/it]


 82%|████████▏ | 8787/10740 [43:34:16<9:04:32, 16.73s/it]
{'loss': 0.2457, 'learning_rate': 1.6849897327558017e-07, 'rewards/chosen': -3.5415892601013184, 'rewards/rejected': -8.652168273925781, 'rewards/accuracies': 1.0, 'rewards/margins': 5.110578536987305, 'policy_logps/rejected': -373.5458068847656, 'policy_logps/chosen': -367.1842956542969, 'referece_logps/rejected': -287.0241394042969, 'referece_logps/chosen': -331.7684020996094, 'logits/rejected': -0.40167444944381714, 'logits/chosen': -0.43116244673728943, 'epoch': 4.91}


 82%|████████▏ | 8789/10740 [43:34:48<8:59:05, 16.58s/it]

 82%|████████▏ | 8790/10740 [43:35:00<8:11:04, 15.11s/it]

 82%|████████▏ | 8791/10740 [43:35:14<8:02:51, 14.86s/it]

 82%|████████▏ | 8792/10740 [43:35:28<7:56:33, 14.68s/it]

 82%|████████▏ | 8793/10740 [43:35:48<8:43:59, 16.15s/it]
{'loss': 0.1026, 'learning_rate': 1.674951161505187e-07, 'rewards/chosen': -3.930107355117798, 'rewards/rejected': -7.899709701538086, 'rewards/accuracies': 1.0, 'rewards/margins': 3.969602346420288, 'policy_logps/rejected': -332.0303955078125, 'policy_logps/chosen': -389.63128662109375, 'referece_logps/rejected': -253.03329467773438, 'referece_logps/chosen': -350.3302001953125, 'logits/rejected': -0.5682710409164429, 'logits/chosen': -0.6860871315002441, 'epoch': 4.91}

 82%|████████▏ | 8794/10740 [43:36:03<8:31:09, 15.76s/it]


 82%|████████▏ | 8796/10740 [43:36:42<9:44:27, 18.04s/it]
{'loss': 0.2228, 'learning_rate': 1.6699420958050414e-07, 'rewards/chosen': -2.5992236137390137, 'rewards/rejected': -7.996379375457764, 'rewards/accuracies': 1.0, 'rewards/margins': 5.397154808044434, 'policy_logps/rejected': -494.09381103515625, 'policy_logps/chosen': -810.6200561523438, 'referece_logps/rejected': -414.1299743652344, 'referece_logps/chosen': -784.6278686523438, 'logits/rejected': 0.07851304113864899, 'logits/chosen': -0.2288411259651184, 'epoch': 4.91}


 82%|████████▏ | 8798/10740 [43:37:22<10:13:02, 18.94s/it]
{'loss': 0.3016, 'learning_rate': 1.6666065066144873e-07, 'rewards/chosen': -4.533968448638916, 'rewards/rejected': -10.083929061889648, 'rewards/accuracies': 1.0, 'rewards/margins': 5.549960136413574, 'policy_logps/rejected': -467.6051025390625, 'policy_logps/chosen': -596.8963012695312, 'referece_logps/rejected': -366.76580810546875, 'referece_logps/chosen': -551.556640625, 'logits/rejected': 0.5100009441375732, 'logits/chosen': 0.6026813983917236, 'epoch': 4.92}


 82%|████████▏ | 8800/10740 [43:37:52<9:07:55, 16.95s/it]

 82%|████████▏ | 8801/10740 [43:38:12<9:35:28, 17.81s/it]
{'loss': 0.1671, 'learning_rate': 1.6616088077767576e-07, 'rewards/chosen': -4.834592342376709, 'rewards/rejected': -8.143527030944824, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3089351654052734, 'policy_logps/rejected': -407.74224853515625, 'policy_logps/chosen': -387.06719970703125, 'referece_logps/rejected': -326.30694580078125, 'referece_logps/chosen': -338.7212829589844, 'logits/rejected': 0.15997351706027985, 'logits/chosen': 0.13442477583885193, 'epoch': 4.92}

 82%|████████▏ | 8802/10740 [43:38:31<9:51:56, 18.33s/it]

 82%|████████▏ | 8803/10740 [43:38:45<9:09:52, 17.03s/it]

 82%|████████▏ | 8804/10740 [43:39:03<9:19:07, 17.33s/it]

 82%|████████▏ | 8805/10740 [43:39:23<9:42:31, 18.06s/it]

 82%|████████▏ | 8806/10740 [43:39:43<9:57:44, 18.54s/it]

 82%|████████▏ | 8807/10740 [43:39:56<9:04:14, 16.89s/it]

 82%|████████▏ | 8808/10740 [43:40:10<8:35:17, 16.00s/it]

 82%|████████▏ | 8809/10740 [43:40:27<8:49:10, 16.44s/it]

 82%|████████▏ | 8810/10740 [43:40:47<9:19:56, 17.41s/it]

 82%|████████▏ | 8811/10740 [43:41:09<10:03:31, 18.77s/it]

 82%|████████▏ | 8812/10740 [43:41:27<10:02:38, 18.75s/it]

 82%|████████▏ | 8813/10740 [43:41:48<10:15:42, 19.17s/it]

 82%|████████▏ | 8814/10740 [43:42:07<10:20:00, 19.32s/it]


 82%|████████▏ | 8816/10740 [43:42:46<10:23:28, 19.44s/it]
{'loss': 0.1319, 'learning_rate': 1.6367227788181226e-07, 'rewards/chosen': -4.443396091461182, 'rewards/rejected': -7.835682392120361, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3922860622406006, 'policy_logps/rejected': -491.8443603515625, 'policy_logps/chosen': -432.35888671875, 'referece_logps/rejected': -413.4875793457031, 'referece_logps/chosen': -387.9248962402344, 'logits/rejected': 0.00023178011178970337, 'logits/chosen': -0.012398652732372284, 'epoch': 4.93}

 82%|████████▏ | 8817/10740 [43:43:07<10:37:02, 19.88s/it]

 82%|████████▏ | 8818/10740 [43:43:23<9:59:41, 18.72s/it]

 82%|████████▏ | 8819/10740 [43:43:43<10:07:03, 18.96s/it]

 82%|████████▏ | 8820/10740 [43:44:03<10:16:07, 19.25s/it]

 82%|████████▏ | 8821/10740 [43:44:23<10:25:11, 19.55s/it]

 82%|████████▏ | 8822/10740 [43:44:39<9:51:26, 18.50s/it]

 82%|████████▏ | 8823/10740 [43:45:01<10:24:43, 19.55s/it]

 82%|████████▏ | 8824/10740 [43:45:12<9:02:00, 16.97s/it]

 82%|████████▏ | 8825/10740 [43:45:33<9:38:35, 18.13s/it]

 82%|████████▏ | 8826/10740 [43:45:52<9:48:18, 18.44s/it]

 82%|████████▏ | 8827/10740 [43:46:12<10:07:13, 19.05s/it]

 82%|████████▏ | 8828/10740 [43:46:32<10:13:46, 19.26s/it]

 82%|████████▏ | 8829/10740 [43:46:54<10:34:00, 19.91s/it]

 82%|████████▏ | 8830/10740 [43:47:12<10:22:56, 19.57s/it]

 82%|████████▏ | 8831/10740 [43:47:31<10:09:43, 19.16s/it]

 82%|████████▏ | 8832/10740 [43:47:48<9:56:13, 18.75s/it]

 82%|████████▏ | 8833/10740 [43:48:06<9:45:36, 18.42s/it]

 82%|████████▏ | 8834/10740 [43:48:22<9:21:57, 17.69s/it]


 82%|████████▏ | 8836/10740 [43:48:49<8:08:29, 15.39s/it]

 82%|████████▏ | 8837/10740 [43:49:09<8:54:01, 16.84s/it]
{'loss': 0.1672, 'learning_rate': 1.6021699808954936e-07, 'rewards/chosen': -4.579948425292969, 'rewards/rejected': -7.278909206390381, 'rewards/accuracies': 0.75, 'rewards/margins': 2.698960781097412, 'policy_logps/rejected': -332.77679443359375, 'policy_logps/chosen': -361.92462158203125, 'referece_logps/rejected': -259.9876708984375, 'referece_logps/chosen': -316.1251525878906, 'logits/rejected': 0.3809579908847809, 'logits/chosen': 0.4101226329803467, 'epoch': 4.94}


 82%|████████▏ | 8839/10740 [43:49:49<9:37:29, 18.23s/it]

 82%|████████▏ | 8840/10740 [43:50:01<8:37:46, 16.35s/it]
{'loss': 0.1786, 'learning_rate': 1.597261331575458e-07, 'rewards/chosen': -4.030384063720703, 'rewards/rejected': -6.581407070159912, 'rewards/accuracies': 0.875, 'rewards/margins': 2.551023006439209, 'policy_logps/rejected': -278.1058349609375, 'policy_logps/chosen': -328.4149475097656, 'referece_logps/rejected': -212.29176330566406, 'referece_logps/chosen': -288.111083984375, 'logits/rejected': -0.95692378282547, 'logits/chosen': -0.9722240567207336, 'epoch': 4.94}

 82%|████████▏ | 8841/10740 [43:50:18<8:46:48, 16.65s/it]

 82%|████████▏ | 8842/10740 [43:50:36<8:56:00, 16.94s/it]

 82%|████████▏ | 8843/10740 [43:50:54<9:06:27, 17.28s/it]

 82%|████████▏ | 8844/10740 [43:51:09<8:40:31, 16.47s/it]


 82%|████████▏ | 8846/10740 [43:51:43<8:59:58, 17.11s/it]
{'loss': 0.1468, 'learning_rate': 1.5874646716881868e-07, 'rewards/chosen': -3.86950945854187, 'rewards/rejected': -9.745450019836426, 'rewards/accuracies': 1.0, 'rewards/margins': 5.875940322875977, 'policy_logps/rejected': -377.1968994140625, 'policy_logps/chosen': -287.6594543457031, 'referece_logps/rejected': -279.7424011230469, 'referece_logps/chosen': -248.96434020996094, 'logits/rejected': -0.5736626982688904, 'logits/chosen': -0.5391966700553894, 'epoch': 4.94}

 82%|████████▏ | 8847/10740 [43:51:57<8:27:43, 16.09s/it]

 82%|████████▏ | 8848/10740 [43:52:12<8:24:26, 16.00s/it]

 82%|████████▏ | 8849/10740 [43:52:29<8:33:40, 16.30s/it]

 82%|████████▏ | 8850/10740 [43:52:48<8:58:55, 17.11s/it]

 82%|████████▏ | 8851/10740 [43:52:59<7:59:41, 15.24s/it]

 82%|████████▏ | 8852/10740 [43:53:15<8:07:14, 15.48s/it]

 82%|████████▏ | 8853/10740 [43:53:34<8:34:27, 16.36s/it]


 82%|████████▏ | 8855/10740 [43:54:07<8:48:29, 16.82s/it]
{'loss': 0.1396, 'learning_rate': 1.5728213388415578e-07, 'rewards/chosen': -2.841714382171631, 'rewards/rejected': -7.692379474639893, 'rewards/accuracies': 1.0, 'rewards/margins': 4.850665092468262, 'policy_logps/rejected': -374.61737060546875, 'policy_logps/chosen': -372.33416748046875, 'referece_logps/rejected': -297.693603515625, 'referece_logps/chosen': -343.9169921875, 'logits/rejected': 0.09762228280305862, 'logits/chosen': 0.060181692242622375, 'epoch': 4.95}

 82%|████████▏ | 8856/10740 [43:54:23<8:36:53, 16.46s/it]

 82%|████████▏ | 8857/10740 [43:54:43<9:06:33, 17.42s/it]

 82%|████████▏ | 8858/10740 [43:55:02<9:27:39, 18.10s/it]

 82%|████████▏ | 8859/10740 [43:55:24<9:59:55, 19.14s/it]

 82%|████████▏ | 8860/10740 [43:55:38<9:09:10, 17.53s/it]

 83%|████████▎ | 8861/10740 [43:55:56<9:17:13, 17.79s/it]

 83%|████████▎ | 8862/10740 [43:56:09<8:34:16, 16.43s/it]

 83%|████████▎ | 8863/10740 [43:56:31<9:19:34, 17.89s/it]

 83%|████████▎ | 8864/10740 [43:56:47<9:05:12, 17.44s/it]

 83%|████████▎ | 8865/10740 [43:56:59<8:12:26, 15.76s/it]

 83%|████████▎ | 8866/10740 [43:57:18<8:48:43, 16.93s/it]

 83%|████████▎ | 8867/10740 [43:57:41<9:37:23, 18.50s/it]

 83%|████████▎ | 8868/10740 [43:58:00<9:47:28, 18.83s/it]

 83%|████████▎ | 8869/10740 [43:58:17<9:30:38, 18.30s/it]

 83%|████████▎ | 8870/10740 [43:58:37<9:46:47, 18.83s/it]

 83%|████████▎ | 8871/10740 [43:58:58<10:04:16, 19.40s/it]

 83%|████████▎ | 8872/10740 [43:59:17<10:02:15, 19.34s/it]

 83%|████████▎ | 8873/10740 [43:59:35<9:45:58, 18.83s/it]

 83%|████████▎ | 8874/10740 [43:59:56<10:10:11, 19.62s/it]

 83%|████████▎ | 8875/10740 [44:00:16<10:11:11, 19.66s/it]

 83%|████████▎ | 8876/10740 [44:00:35<10:06:54, 19.54s/it]


 83%|████████▎ | 8878/10740 [44:01:08<9:21:38, 18.10s/it]
{'loss': 0.083, 'learning_rate': 1.5356818028754705e-07, 'rewards/chosen': -5.0685553550720215, 'rewards/rejected': -9.332144737243652, 'rewards/accuracies': 0.875, 'rewards/margins': 4.263589382171631, 'policy_logps/rejected': -436.4316711425781, 'policy_logps/chosen': -654.52197265625, 'referece_logps/rejected': -343.1102294921875, 'referece_logps/chosen': -603.83642578125, 'logits/rejected': 0.02707161009311676, 'logits/chosen': -0.3231339454650879, 'epoch': 4.96}

 83%|████████▎ | 8879/10740 [44:01:25<9:10:59, 17.76s/it]

 83%|████████▎ | 8880/10740 [44:01:42<9:07:09, 17.65s/it]


 83%|████████▎ | 8882/10740 [44:02:16<8:47:58, 17.05s/it]
{'loss': 0.1639, 'learning_rate': 1.5292642749734486e-07, 'rewards/chosen': -3.5821239948272705, 'rewards/rejected': -8.456039428710938, 'rewards/accuracies': 1.0, 'rewards/margins': 4.873915195465088, 'policy_logps/rejected': -477.2458801269531, 'policy_logps/chosen': -349.857177734375, 'referece_logps/rejected': -392.68548583984375, 'referece_logps/chosen': -314.03594970703125, 'logits/rejected': -0.5295689105987549, 'logits/chosen': -0.547806978225708, 'epoch': 4.96}

 83%|████████▎ | 8883/10740 [44:02:36<9:20:36, 18.11s/it]

 83%|████████▎ | 8884/10740 [44:02:55<9:29:44, 18.42s/it]

 83%|████████▎ | 8885/10740 [44:03:09<8:46:54, 17.04s/it]

 83%|████████▎ | 8886/10740 [44:03:28<9:05:20, 17.65s/it]

 83%|████████▎ | 8887/10740 [44:03:48<9:26:16, 18.34s/it]

 83%|████████▎ | 8888/10740 [44:04:11<10:05:20, 19.61s/it]

 83%|████████▎ | 8889/10740 [44:04:32<10:21:35, 20.15s/it]

 83%|████████▎ | 8890/10740 [44:04:52<10:20:37, 20.13s/it]

 83%|████████▎ | 8891/10740 [44:05:09<9:48:10, 19.09s/it]

 83%|████████▎ | 8892/10740 [44:05:22<8:55:23, 17.38s/it]

 83%|████████▎ | 8893/10740 [44:05:39<8:47:29, 17.14s/it]

 83%|████████▎ | 8894/10740 [44:05:59<9:11:48, 17.94s/it]

 83%|████████▎ | 8895/10740 [44:06:19<9:28:29, 18.49s/it]

 83%|████████▎ | 8896/10740 [44:06:38<9:36:10, 18.75s/it]


 83%|████████▎ | 8898/10740 [44:07:14<9:29:39, 18.56s/it]

 83%|████████▎ | 8899/10740 [44:07:31<9:14:06, 18.06s/it]

 83%|████████▎ | 8900/10740 [44:07:50<9:25:14, 18.43s/it]

 83%|████████▎ | 8901/10740 [44:08:02<8:21:44, 16.37s/it]
{'loss': 0.212, 'learning_rate': 1.4989495161931009e-07, 'rewards/chosen': -4.866338729858398, 'rewards/rejected': -6.59742546081543, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7310863733291626, 'policy_logps/rejected': -225.20046997070312, 'policy_logps/chosen': -495.65484619140625, 'referece_logps/rejected': -159.22622680664062, 'referece_logps/chosen': -446.9914245605469, 'logits/rejected': -0.026690110564231873, 'logits/chosen': -0.29737985134124756, 'epoch': 4.97}


 83%|████████▎ | 8903/10740 [44:08:31<7:49:39, 15.34s/it]

 83%|████████▎ | 8904/10740 [44:08:43<7:15:27, 14.23s/it]
{'loss': 0.229, 'learning_rate': 1.494188465165658e-07, 'rewards/chosen': -3.2932889461517334, 'rewards/rejected': -7.804441452026367, 'rewards/accuracies': 0.875, 'rewards/margins': 4.5111517906188965, 'policy_logps/rejected': -453.9068298339844, 'policy_logps/chosen': -545.408447265625, 'referece_logps/rejected': -375.8624572753906, 'referece_logps/chosen': -512.4755249023438, 'logits/rejected': -0.20446130633354187, 'logits/chosen': -0.43249770998954773, 'epoch': 4.97}


 83%|████████▎ | 8906/10740 [44:09:17<7:51:06, 15.41s/it]

 83%|████████▎ | 8907/10740 [44:09:38<8:40:19, 17.03s/it]

 83%|████████▎ | 8908/10740 [44:09:57<9:03:13, 17.79s/it]

 83%|████████▎ | 8909/10740 [44:10:17<9:18:32, 18.30s/it]

 83%|████████▎ | 8910/10740 [44:10:37<9:33:07, 18.79s/it]

 83%|████████▎ | 8911/10740 [44:10:48<8:21:38, 16.46s/it]

 83%|████████▎ | 8912/10740 [44:11:07<8:48:44, 17.35s/it]

 83%|████████▎ | 8913/10740 [44:11:19<8:00:26, 15.78s/it]

 83%|████████▎ | 8914/10740 [44:11:36<8:05:46, 15.96s/it]

 83%|████████▎ | 8915/10740 [44:11:56<8:43:52, 17.22s/it]

 83%|████████▎ | 8916/10740 [44:12:12<8:35:37, 16.96s/it]

 83%|████████▎ | 8917/10740 [44:12:30<8:43:28, 17.23s/it]

 83%|████████▎ | 8918/10740 [44:12:44<8:09:07, 16.11s/it]

 83%|████████▎ | 8919/10740 [44:13:03<8:42:01, 17.20s/it]

 83%|████████▎ | 8920/10740 [44:13:22<8:52:21, 17.55s/it]

 83%|████████▎ | 8921/10740 [44:13:41<9:09:21, 18.12s/it]

 83%|████████▎ | 8922/10740 [44:13:53<8:13:16, 16.28s/it]

 83%|████████▎ | 8923/10740 [44:14:11<8:23:32, 16.63s/it]

 83%|████████▎ | 8924/10740 [44:14:30<8:45:59, 17.38s/it]

 83%|████████▎ | 8925/10740 [44:14:46<8:38:46, 17.15s/it]

 83%|████████▎ | 8926/10740 [44:15:00<8:06:17, 16.08s/it]

 83%|████████▎ | 8927/10740 [44:15:20<8:40:06, 17.21s/it]

 83%|████████▎ | 8928/10740 [44:15:37<8:36:41, 17.11s/it]

 83%|████████▎ | 8929/10740 [44:15:53<8:25:56, 16.76s/it]

 83%|████████▎ | 8930/10740 [44:16:12<8:46:15, 17.44s/it]
{'loss': 0.1801, 'learning_rate': 1.4532180948874874e-07, 'rewards/chosen': -4.535408973693848, 'rewards/rejected': -7.821423053741455, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2860143184661865, 'policy_logps/rejected': -412.1312255859375, 'policy_logps/chosen': -367.6910400390625, 'referece_logps/rejected': -333.9169921875, 'referece_logps/chosen': -322.33697509765625, 'logits/rejected': -0.1846114695072174, 'logits/chosen': -0.2318689227104187, 'epoch': 4.99}


 83%|████████▎ | 8932/10740 [44:16:46<8:43:17, 17.37s/it]
{'loss': 0.207, 'learning_rate': 1.450088261762933e-07, 'rewards/chosen': -4.708892822265625, 'rewards/rejected': -6.4580793380737305, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7491865158081055, 'policy_logps/rejected': -265.6294860839844, 'policy_logps/chosen': -335.2908630371094, 'referece_logps/rejected': -201.04869079589844, 'referece_logps/chosen': -288.2019348144531, 'logits/rejected': -0.1618366837501526, 'logits/chosen': -0.39553120732307434, 'epoch': 4.99}


 83%|████████▎ | 8934/10740 [44:17:20<8:46:06, 17.48s/it]

 83%|████████▎ | 8935/10740 [44:17:38<8:47:01, 17.52s/it]
{'loss': 0.281, 'learning_rate': 1.4453993446972924e-07, 'rewards/chosen': -4.078559398651123, 'rewards/rejected': -6.409208297729492, 'rewards/accuracies': 0.75, 'rewards/margins': 2.330648422241211, 'policy_logps/rejected': -447.5699462890625, 'policy_logps/chosen': -358.9914245605469, 'referece_logps/rejected': -383.4779052734375, 'referece_logps/chosen': -318.2058410644531, 'logits/rejected': -0.0442577600479126, 'logits/chosen': 0.09256067872047424, 'epoch': 4.99}


 83%|████████▎ | 8937/10740 [44:18:09<8:21:37, 16.69s/it]
{'loss': 0.172, 'learning_rate': 1.4422772900594838e-07, 'rewards/chosen': -3.7924766540527344, 'rewards/rejected': -6.864090919494629, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0716147422790527, 'policy_logps/rejected': -296.3651123046875, 'policy_logps/chosen': -314.088623046875, 'referece_logps/rejected': -227.72422790527344, 'referece_logps/chosen': -276.1638488769531, 'logits/rejected': 0.036768823862075806, 'logits/chosen': 0.1360243856906891, 'epoch': 4.99}


 83%|████████▎ | 8939/10740 [44:18:48<9:06:24, 18.20s/it]
{'loss': 0.1834, 'learning_rate': 1.4391583488050762e-07, 'rewards/chosen': -4.709380149841309, 'rewards/rejected': -8.67566967010498, 'rewards/accuracies': 1.0, 'rewards/margins': 3.966289520263672, 'policy_logps/rejected': -411.7809753417969, 'policy_logps/chosen': -470.16064453125, 'referece_logps/rejected': -325.0242614746094, 'referece_logps/chosen': -423.0668640136719, 'logits/rejected': 0.7561439275741577, 'logits/chosen': 0.47276920080184937, 'epoch': 4.99}

 83%|████████▎ | 8940/10740 [44:19:05<8:55:06, 17.84s/it]

 83%|████████▎ | 8941/10740 [44:19:25<9:14:53, 18.51s/it]


 83%|████████▎ | 8943/10740 [44:20:05<9:33:54, 19.16s/it]

 83%|████████▎ | 8944/10740 [44:20:25<9:39:04, 19.35s/it]

 83%|████████▎ | 8945/10740 [44:20:44<9:41:54, 19.45s/it]

 83%|████████▎ | 8946/10740 [44:21:03<9:31:05, 19.10s/it]

 83%|████████▎ | 8947/10740 [44:21:16<8:36:21, 17.28s/it]

 83%|████████▎ | 8948/10740 [44:21:29<7:57:44, 16.00s/it]

 83%|████████▎ | 8949/10740 [44:21:43<7:43:36, 15.53s/it]

 83%|████████▎ | 8950/10740 [44:22:04<8:35:16, 17.27s/it]
{'loss': 0.1597, 'learning_rate': 1.4220598742840918e-07, 'rewards/chosen': -4.051352024078369, 'rewards/rejected': -9.717374801635742, 'rewards/accuracies': 1.0, 'rewards/margins': 5.666023254394531, 'policy_logps/rejected': -371.8550720214844, 'policy_logps/chosen': -367.0492858886719, 'referece_logps/rejected': -274.68133544921875, 'referece_logps/chosen': -326.5357666015625, 'logits/rejected': 0.16742585599422455, 'logits/chosen': 0.14377017319202423, 'epoch': 5.0}


 83%|████████▎ | 8952/10740 [44:22:34<7:55:18, 15.95s/it]

 83%|████████▎ | 8953/10740 [44:22:54<8:30:04, 17.13s/it]

 83%|████████▎ | 8954/10740 [44:23:05<7:33:26, 15.23s/it]

 83%|████████▎ | 8955/10740 [44:23:25<8:15:29, 16.66s/it]

 83%|████████▎ | 8956/10740 [44:23:46<8:52:50, 17.92s/it]

 83%|████████▎ | 8957/10740 [44:24:04<8:53:26, 17.95s/it]

 83%|████████▎ | 8958/10740 [44:24:22<8:57:51, 18.11s/it]

 83%|████████▎ | 8959/10740 [44:24:41<8:59:35, 18.18s/it]
{'loss': 0.0909, 'learning_rate': 1.408140421421079e-07, 'rewards/chosen': -2.8813116550445557, 'rewards/rejected': -6.103005409240723, 'rewards/accuracies': 1.0, 'rewards/margins': 3.221693754196167, 'policy_logps/rejected': -322.076904296875, 'policy_logps/chosen': -320.18536376953125, 'referece_logps/rejected': -261.0468444824219, 'referece_logps/chosen': -291.3722839355469, 'logits/rejected': -0.6503089666366577, 'logits/chosen': -0.6566662788391113, 'epoch': 5.01}


 83%|████████▎ | 8961/10740 [44:25:19<9:16:46, 18.78s/it]
{'loss': 0.1148, 'learning_rate': 1.4050558020248737e-07, 'rewards/chosen': -4.438401222229004, 'rewards/rejected': -7.241083145141602, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8026809692382812, 'policy_logps/rejected': -313.696533203125, 'policy_logps/chosen': -370.45233154296875, 'referece_logps/rejected': -241.28573608398438, 'referece_logps/chosen': -326.06829833984375, 'logits/rejected': -0.20530155301094055, 'logits/chosen': -0.31251728534698486, 'epoch': 5.01}


 83%|████████▎ | 8963/10740 [44:25:50<8:26:29, 17.10s/it]
{'loss': 0.1428, 'learning_rate': 1.401974309553613e-07, 'rewards/chosen': -4.012299060821533, 'rewards/rejected': -8.212148666381836, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1998491287231445, 'policy_logps/rejected': -316.2554626464844, 'policy_logps/chosen': -567.73388671875, 'referece_logps/rejected': -234.1339569091797, 'referece_logps/chosen': -527.6109008789062, 'logits/rejected': -0.3867107033729553, 'logits/chosen': -0.8501439094543457, 'epoch': 5.01}


 83%|████████▎ | 8965/10740 [44:26:13<6:57:45, 14.12s/it]

 83%|████████▎ | 8966/10740 [44:26:28<7:06:27, 14.42s/it]

 83%|████████▎ | 8967/10740 [44:26:47<7:52:32, 15.99s/it]

 84%|████████▎ | 8968/10740 [44:27:07<8:21:06, 16.97s/it]

 84%|████████▎ | 8969/10740 [44:27:26<8:45:35, 17.81s/it]

 84%|████████▎ | 8970/10740 [44:27:48<9:18:05, 18.92s/it]
{'loss': 0.2018, 'learning_rate': 1.391213726613396e-07, 'rewards/chosen': -4.886733055114746, 'rewards/rejected': -7.993643283843994, 'rewards/accuracies': 0.875, 'rewards/margins': 3.106910467147827, 'policy_logps/rejected': -303.03912353515625, 'policy_logps/chosen': -318.70989990234375, 'referece_logps/rejected': -223.1027069091797, 'referece_logps/chosen': -269.842529296875, 'logits/rejected': 0.23032736778259277, 'logits/chosen': 0.1861005425453186, 'epoch': 5.01}


 84%|████████▎ | 8972/10740 [44:28:23<8:48:20, 17.93s/it]

 84%|████████▎ | 8973/10740 [44:28:40<8:38:15, 17.60s/it]

 84%|████████▎ | 8974/10740 [44:28:56<8:30:05, 17.33s/it]

 84%|████████▎ | 8975/10740 [44:29:13<8:23:25, 17.11s/it]

 84%|████████▎ | 8976/10740 [44:29:33<8:47:06, 17.93s/it]

 84%|████████▎ | 8977/10740 [44:29:46<8:09:09, 16.65s/it]

 84%|████████▎ | 8978/10740 [44:30:03<8:08:49, 16.65s/it]

 84%|████████▎ | 8979/10740 [44:30:25<8:58:24, 18.34s/it]

 84%|████████▎ | 8980/10740 [44:30:39<8:15:21, 16.89s/it]

 84%|████████▎ | 8981/10740 [44:30:58<8:37:37, 17.66s/it]

 84%|████████▎ | 8982/10740 [44:31:17<8:47:56, 18.02s/it]

 84%|████████▎ | 8983/10740 [44:31:38<9:09:43, 18.77s/it]

 84%|████████▎ | 8984/10740 [44:31:56<9:07:13, 18.70s/it]

 84%|████████▎ | 8985/10740 [44:32:11<8:30:20, 17.45s/it]

 84%|████████▎ | 8986/10740 [44:32:31<8:52:47, 18.23s/it]

 84%|████████▎ | 8987/10740 [44:32:50<9:05:02, 18.65s/it]

 84%|████████▎ | 8988/10740 [44:33:09<9:00:53, 18.52s/it]
{'loss': 0.2038, 'learning_rate': 1.3637199438088076e-07, 'rewards/chosen': -3.7818233966827393, 'rewards/rejected': -11.908337593078613, 'rewards/accuracies': 1.0, 'rewards/margins': 8.12651538848877, 'policy_logps/rejected': -419.1045837402344, 'policy_logps/chosen': -353.7864074707031, 'referece_logps/rejected': -300.02117919921875, 'referece_logps/chosen': -315.96820068359375, 'logits/rejected': -0.37832319736480713, 'logits/chosen': -0.26449358463287354, 'epoch': 5.02}


 84%|████████▎ | 8990/10740 [44:33:41<8:25:53, 17.34s/it]

 84%|████████▎ | 8991/10740 [44:33:53<7:32:48, 15.53s/it]

 84%|████████▎ | 8992/10740 [44:34:11<7:57:00, 16.37s/it]

 84%|████████▎ | 8993/10740 [44:34:24<7:24:39, 15.27s/it]

 84%|████████▎ | 8994/10740 [44:34:45<8:20:26, 17.20s/it]

 84%|████████▍ | 8995/10740 [44:35:05<8:43:05, 17.99s/it]

 84%|████████▍ | 8996/10740 [44:35:22<8:35:39, 17.74s/it]

 84%|████████▍ | 8997/10740 [44:35:41<8:45:13, 18.08s/it]

 84%|████████▍ | 8998/10740 [44:35:56<8:13:02, 16.98s/it]

 84%|████████▍ | 8999/10740 [44:36:11<7:54:55, 16.37s/it]

 84%|████████▍ | 9000/10740 [44:36:23<7:18:45, 15.13s/it]

 84%|████████▍ | 9001/10740 [44:36:50<9:01:00, 18.67s/it]

 84%|████████▍ | 9002/10740 [44:37:02<8:08:32, 16.87s/it]

 84%|████████▍ | 9003/10740 [44:37:21<8:23:26, 17.39s/it]

 84%|████████▍ | 9004/10740 [44:37:37<8:13:02, 17.04s/it]

 84%|████████▍ | 9005/10740 [44:37:57<8:36:12, 17.85s/it]

 84%|████████▍ | 9006/10740 [44:38:11<8:00:55, 16.64s/it]

 84%|████████▍ | 9007/10740 [44:38:31<8:26:40, 17.54s/it]

 84%|████████▍ | 9008/10740 [44:38:50<8:41:00, 18.05s/it]
{'loss': 0.1293, 'learning_rate': 1.3334698169814475e-07, 'rewards/chosen': -5.175553798675537, 'rewards/rejected': -8.254390716552734, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0788369178771973, 'policy_logps/rejected': -448.3424377441406, 'policy_logps/chosen': -449.738525390625, 'referece_logps/rejected': -365.7985534667969, 'referece_logps/chosen': -397.98297119140625, 'logits/rejected': -0.4522368311882019, 'logits/chosen': -0.4268728196620941, 'epoch': 5.03}


 84%|████████▍ | 9010/10740 [44:39:24<8:31:02, 17.72s/it]

 84%|████████▍ | 9011/10740 [44:39:43<8:45:45, 18.25s/it]
{'loss': 0.2576, 'learning_rate': 1.3289594654864843e-07, 'rewards/chosen': -3.252591133117676, 'rewards/rejected': -6.192541599273682, 'rewards/accuracies': 0.875, 'rewards/margins': 2.939951181411743, 'policy_logps/rejected': -540.1065673828125, 'policy_logps/chosen': -405.7100524902344, 'referece_logps/rejected': -478.18115234375, 'referece_logps/chosen': -373.1841125488281, 'logits/rejected': -0.4173692762851715, 'logits/chosen': -0.2744106352329254, 'epoch': 5.03}


 84%|████████▍ | 9013/10740 [44:40:18<8:29:58, 17.72s/it]

 84%|████████▍ | 9014/10740 [44:40:33<8:08:22, 16.98s/it]
{'loss': 0.1119, 'learning_rate': 1.3244562118627045e-07, 'rewards/chosen': -2.761845827102661, 'rewards/rejected': -8.7122163772583, 'rewards/accuracies': 1.0, 'rewards/margins': 5.950370788574219, 'policy_logps/rejected': -519.5089721679688, 'policy_logps/chosen': -412.9940185546875, 'referece_logps/rejected': -432.38677978515625, 'referece_logps/chosen': -385.37554931640625, 'logits/rejected': -0.334364652633667, 'logits/chosen': -0.27814769744873047, 'epoch': 5.04}


 84%|████████▍ | 9016/10740 [44:41:09<8:18:46, 17.36s/it]
{'loss': 0.1987, 'learning_rate': 1.3214579878625588e-07, 'rewards/chosen': -4.809929370880127, 'rewards/rejected': -7.597363471984863, 'rewards/accuracies': 0.625, 'rewards/margins': 2.7874343395233154, 'policy_logps/rejected': -334.337158203125, 'policy_logps/chosen': -442.07061767578125, 'referece_logps/rejected': -258.363525390625, 'referece_logps/chosen': -393.9713134765625, 'logits/rejected': -0.5102198123931885, 'logits/chosen': -0.6702439785003662, 'epoch': 5.04}

 84%|████████▍ | 9017/10740 [44:41:27<8:17:45, 17.33s/it]

 84%|████████▍ | 9018/10740 [44:41:45<8:23:26, 17.54s/it]


 84%|████████▍ | 9020/10740 [44:42:20<8:16:58, 17.34s/it]
{'loss': 0.3122, 'learning_rate': 1.315471012967837e-07, 'rewards/chosen': -3.71187424659729, 'rewards/rejected': -6.470353126525879, 'rewards/accuracies': 0.625, 'rewards/margins': 2.758479118347168, 'policy_logps/rejected': -495.9532165527344, 'policy_logps/chosen': -394.43597412109375, 'referece_logps/rejected': -431.2496643066406, 'referece_logps/chosen': -357.3172302246094, 'logits/rejected': -0.19352120161056519, 'logits/chosen': -0.24163933098316193, 'epoch': 5.04}

 84%|████████▍ | 9021/10740 [44:42:39<8:27:21, 17.71s/it]


 84%|████████▍ | 9023/10740 [44:43:16<8:34:52, 17.99s/it]
{'loss': 0.2325, 'learning_rate': 1.3109890750517816e-07, 'rewards/chosen': -3.005361318588257, 'rewards/rejected': -5.854367733001709, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8490066528320312, 'policy_logps/rejected': -311.46630859375, 'policy_logps/chosen': -336.0738525390625, 'referece_logps/rejected': -252.92263793945312, 'referece_logps/chosen': -306.0202331542969, 'logits/rejected': -0.7345614433288574, 'logits/chosen': -0.7123226523399353, 'epoch': 5.04}


 84%|████████▍ | 9025/10740 [44:43:42<7:26:37, 15.63s/it]

 84%|████████▍ | 9026/10740 [44:44:00<7:41:51, 16.17s/it]

 84%|████████▍ | 9027/10740 [44:44:20<8:13:06, 17.27s/it]

 84%|████████▍ | 9028/10740 [44:44:38<8:26:01, 17.73s/it]

 84%|████████▍ | 9029/10740 [44:44:56<8:23:21, 17.65s/it]

 84%|████████▍ | 9030/10740 [44:45:12<8:12:18, 17.27s/it]
{'loss': 0.2833, 'learning_rate': 1.3005588862826988e-07, 'rewards/chosen': -2.232760429382324, 'rewards/rejected': -7.606244087219238, 'rewards/accuracies': 0.875, 'rewards/margins': 5.373483180999756, 'policy_logps/rejected': -269.8139953613281, 'policy_logps/chosen': -307.0847473144531, 'referece_logps/rejected': -193.7515411376953, 'referece_logps/chosen': -284.75714111328125, 'logits/rejected': -0.05668333172798157, 'logits/chosen': -0.12053447961807251, 'epoch': 5.04}

 84%|████████▍ | 9031/10740 [44:45:31<8:27:35, 17.82s/it]

 84%|████████▍ | 9032/10740 [44:45:51<8:45:03, 18.44s/it]

 84%|████████▍ | 9033/10740 [44:46:07<8:21:01, 17.61s/it]


 84%|████████▍ | 9035/10740 [44:46:38<7:58:12, 16.83s/it]
{'loss': 0.1796, 'learning_rate': 1.2931324858005265e-07, 'rewards/chosen': -3.558897018432617, 'rewards/rejected': -7.9231109619140625, 'rewards/accuracies': 1.0, 'rewards/margins': 4.364213943481445, 'policy_logps/rejected': -352.2663879394531, 'policy_logps/chosen': -380.9922790527344, 'referece_logps/rejected': -273.0352783203125, 'referece_logps/chosen': -345.4032897949219, 'logits/rejected': 0.7302201986312866, 'logits/chosen': 0.5949744582176208, 'epoch': 5.05}

 84%|████████▍ | 9036/10740 [44:46:55<7:58:54, 16.86s/it]

 84%|████████▍ | 9037/10740 [44:47:15<8:24:49, 17.79s/it]


 84%|████████▍ | 9039/10740 [44:47:51<8:28:58, 17.95s/it]

 84%|████████▍ | 9040/10740 [44:48:07<8:11:44, 17.36s/it]

 84%|████████▍ | 9041/10740 [44:48:22<7:54:34, 16.76s/it]
{'loss': 0.1763, 'learning_rate': 1.2842469397614476e-07, 'rewards/chosen': -3.5689327716827393, 'rewards/rejected': -9.067227363586426, 'rewards/accuracies': 0.875, 'rewards/margins': 5.498294830322266, 'policy_logps/rejected': -281.44366455078125, 'policy_logps/chosen': -350.9906311035156, 'referece_logps/rejected': -190.77137756347656, 'referece_logps/chosen': -315.3013000488281, 'logits/rejected': -0.5129745602607727, 'logits/chosen': -0.6669749617576599, 'epoch': 5.05}

 84%|████████▍ | 9042/10740 [44:48:39<7:55:31, 16.80s/it]


 84%|████████▍ | 9044/10740 [44:49:19<8:39:52, 18.39s/it]

 84%|████████▍ | 9045/10740 [44:49:40<9:04:21, 19.27s/it]

 84%|████████▍ | 9046/10740 [44:50:00<9:10:35, 19.50s/it]

 84%|████████▍ | 9047/10740 [44:50:13<8:10:11, 17.37s/it]

 84%|████████▍ | 9048/10740 [44:50:32<8:28:38, 18.04s/it]

 84%|████████▍ | 9049/10740 [44:50:47<7:58:13, 16.97s/it]
{'loss': 0.1427, 'learning_rate': 1.2724439423164723e-07, 'rewards/chosen': -2.9347593784332275, 'rewards/rejected': -9.743334770202637, 'rewards/accuracies': 1.0, 'rewards/margins': 6.808575630187988, 'policy_logps/rejected': -304.78521728515625, 'policy_logps/chosen': -462.7527770996094, 'referece_logps/rejected': -207.35186767578125, 'referece_logps/chosen': -433.4051818847656, 'logits/rejected': -0.1931571662425995, 'logits/chosen': -0.41066282987594604, 'epoch': 5.06}


 84%|████████▍ | 9051/10740 [44:51:17<7:36:51, 16.23s/it]

 84%|████████▍ | 9052/10740 [44:51:33<7:34:49, 16.17s/it]
{'loss': 0.103, 'learning_rate': 1.2680309121659327e-07, 'rewards/chosen': -4.0884785652160645, 'rewards/rejected': -7.645699501037598, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5572214126586914, 'policy_logps/rejected': -382.6898193359375, 'policy_logps/chosen': -392.8476257324219, 'referece_logps/rejected': -306.23284912109375, 'referece_logps/chosen': -351.96282958984375, 'logits/rejected': -0.4423566162586212, 'logits/chosen': -0.6104344129562378, 'epoch': 5.06}

 84%|████████▍ | 9053/10740 [44:51:50<7:39:06, 16.33s/it]

 84%|████████▍ | 9054/10740 [44:52:06<7:34:53, 16.19s/it]

 84%|████████▍ | 9055/10740 [44:52:18<6:59:17, 14.93s/it]

 84%|████████▍ | 9056/10740 [44:52:40<7:58:22, 17.04s/it]


 84%|████████▍ | 9058/10740 [44:53:14<8:13:21, 17.60s/it]

 84%|████████▍ | 9059/10740 [44:53:27<7:27:21, 15.97s/it]

 84%|████████▍ | 9060/10740 [44:53:43<7:30:42, 16.10s/it]
{'loss': 0.1012, 'learning_rate': 1.2562977860913759e-07, 'rewards/chosen': -2.9018211364746094, 'rewards/rejected': -7.26108980178833, 'rewards/accuracies': 1.0, 'rewards/margins': 4.359269142150879, 'policy_logps/rejected': -327.6985778808594, 'policy_logps/chosen': -372.62786865234375, 'referece_logps/rejected': -255.087646484375, 'referece_logps/chosen': -343.6096496582031, 'logits/rejected': -0.35850727558135986, 'logits/chosen': -0.24443528056144714, 'epoch': 5.06}

 84%|████████▍ | 9061/10740 [44:54:00<7:37:47, 16.36s/it]


 84%|████████▍ | 9063/10740 [44:54:32<7:30:41, 16.12s/it]
{'loss': 0.1652, 'learning_rate': 1.2519109819573802e-07, 'rewards/chosen': -5.182051658630371, 'rewards/rejected': -8.901399612426758, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7193477153778076, 'policy_logps/rejected': -471.965087890625, 'policy_logps/chosen': -422.26007080078125, 'referece_logps/rejected': -382.95111083984375, 'referece_logps/chosen': -370.4395446777344, 'logits/rejected': 0.8694716691970825, 'logits/chosen': 0.9430127739906311, 'epoch': 5.06}

 84%|████████▍ | 9064/10740 [44:54:47<7:22:25, 15.84s/it]


 84%|████████▍ | 9066/10740 [44:55:13<6:33:10, 14.09s/it]

 84%|████████▍ | 9067/10740 [44:55:31<7:06:48, 15.31s/it]

 84%|████████▍ | 9068/10740 [44:55:51<7:44:33, 16.67s/it]

 84%|████████▍ | 9069/10740 [44:56:13<8:27:40, 18.23s/it]
{'loss': 0.2201, 'learning_rate': 1.2431588600971933e-07, 'rewards/chosen': -4.362751483917236, 'rewards/rejected': -8.46450424194336, 'rewards/accuracies': 0.875, 'rewards/margins': 4.101753234863281, 'policy_logps/rejected': -315.91241455078125, 'policy_logps/chosen': -356.2961730957031, 'referece_logps/rejected': -231.26739501953125, 'referece_logps/chosen': -312.66864013671875, 'logits/rejected': -0.05169306695461273, 'logits/chosen': -0.1140999048948288, 'epoch': 5.07}

 84%|████████▍ | 9070/10740 [44:56:32<8:35:05, 18.51s/it]


 84%|████████▍ | 9072/10740 [44:57:02<7:49:41, 16.90s/it]
{'loss': 0.1637, 'learning_rate': 1.2387935495352453e-07, 'rewards/chosen': -3.5201449394226074, 'rewards/rejected': -8.2847261428833, 'rewards/accuracies': 1.0, 'rewards/margins': 4.764581203460693, 'policy_logps/rejected': -349.9815979003906, 'policy_logps/chosen': -316.0419921875, 'referece_logps/rejected': -267.1343688964844, 'referece_logps/chosen': -280.840576171875, 'logits/rejected': 0.29806965589523315, 'logits/chosen': 0.43933868408203125, 'epoch': 5.07}

 84%|████████▍ | 9073/10740 [44:57:22<8:14:13, 17.79s/it]


 84%|████████▍ | 9075/10740 [44:58:03<8:55:16, 19.29s/it]

 85%|████████▍ | 9076/10740 [44:58:21<8:45:22, 18.94s/it]
{'loss': 0.1415, 'learning_rate': 1.2329842920137167e-07, 'rewards/chosen': -4.051648139953613, 'rewards/rejected': -6.995012283325195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.943364143371582, 'policy_logps/rejected': -344.00640869140625, 'policy_logps/chosen': -293.9618835449219, 'referece_logps/rejected': -274.05633544921875, 'referece_logps/chosen': -253.4453582763672, 'logits/rejected': 0.13674840331077576, 'logits/chosen': 0.07281260937452316, 'epoch': 5.07}


 85%|████████▍ | 9078/10740 [44:59:00<8:55:50, 19.34s/it]

 85%|████████▍ | 9079/10740 [44:59:21<9:03:06, 19.62s/it]

 85%|████████▍ | 9080/10740 [44:59:37<8:31:43, 18.50s/it]

 85%|████████▍ | 9081/10740 [44:59:53<8:10:55, 17.76s/it]

 85%|████████▍ | 9082/10740 [45:00:13<8:28:23, 18.40s/it]

 85%|████████▍ | 9083/10740 [45:00:28<8:00:01, 17.38s/it]

 85%|████████▍ | 9084/10740 [45:00:49<8:35:33, 18.68s/it]
{'loss': 0.1133, 'learning_rate': 1.2214040597173946e-07, 'rewards/chosen': -3.744244337081909, 'rewards/rejected': -7.4045939445495605, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6603498458862305, 'policy_logps/rejected': -320.2622985839844, 'policy_logps/chosen': -562.99755859375, 'referece_logps/rejected': -246.21636962890625, 'referece_logps/chosen': -525.5551147460938, 'logits/rejected': 0.7116237282752991, 'logits/chosen': 0.5510924458503723, 'epoch': 5.07}


 85%|████████▍ | 9086/10740 [45:01:23<8:18:26, 18.08s/it]

 85%|████████▍ | 9087/10740 [45:01:43<8:32:07, 18.59s/it]

 85%|████████▍ | 9088/10740 [45:02:03<8:41:46, 18.95s/it]

 85%|████████▍ | 9089/10740 [45:02:19<8:19:18, 18.15s/it]

 85%|████████▍ | 9090/10740 [45:02:33<7:45:09, 16.91s/it]

 85%|████████▍ | 9091/10740 [45:02:53<8:06:52, 17.72s/it]
{'loss': 0.3521, 'learning_rate': 1.2113132719699715e-07, 'rewards/chosen': -5.486651420593262, 'rewards/rejected': -9.549270629882812, 'rewards/accuracies': 0.875, 'rewards/margins': 4.062619686126709, 'policy_logps/rejected': -632.7210693359375, 'policy_logps/chosen': -540.86572265625, 'referece_logps/rejected': -537.2283935546875, 'referece_logps/chosen': -485.9992370605469, 'logits/rejected': 0.9836915135383606, 'logits/chosen': 1.092456579208374, 'epoch': 5.08}


 85%|████████▍ | 9093/10740 [45:03:23<7:18:24, 15.97s/it]

 85%|████████▍ | 9094/10740 [45:03:37<7:03:38, 15.44s/it]

 85%|████████▍ | 9095/10740 [45:03:53<7:08:03, 15.61s/it]

 85%|████████▍ | 9096/10740 [45:04:12<7:31:21, 16.47s/it]

 85%|████████▍ | 9097/10740 [45:04:31<7:58:21, 17.47s/it]

 85%|████████▍ | 9098/10740 [45:04:49<8:01:03, 17.58s/it]

 85%|████████▍ | 9099/10740 [45:05:11<8:35:18, 18.84s/it]

 85%|████████▍ | 9100/10740 [45:05:30<8:33:57, 18.80s/it]

 85%|████████▍ | 9101/10740 [45:05:49<8:35:51, 18.88s/it]
{'loss': 0.208, 'learning_rate': 1.1969658169568953e-07, 'rewards/chosen': -3.3834283351898193, 'rewards/rejected': -9.699834823608398, 'rewards/accuracies': 1.0, 'rewards/margins': 6.316406726837158, 'policy_logps/rejected': -294.4400329589844, 'policy_logps/chosen': -512.0726928710938, 'referece_logps/rejected': -197.44166564941406, 'referece_logps/chosen': -478.2384033203125, 'logits/rejected': -0.041119664907455444, 'logits/chosen': -0.3364661633968353, 'epoch': 5.08}

 85%|████████▍ | 9102/10740 [45:06:07<8:26:16, 18.54s/it]

 85%|████████▍ | 9103/10740 [45:06:23<8:07:09, 17.86s/it]


 85%|████████▍ | 9105/10740 [45:06:56<7:54:02, 17.40s/it]
{'loss': 0.1469, 'learning_rate': 1.1912492460629964e-07, 'rewards/chosen': -3.7249090671539307, 'rewards/rejected': -7.412906169891357, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6879971027374268, 'policy_logps/rejected': -259.74151611328125, 'policy_logps/chosen': -395.9947509765625, 'referece_logps/rejected': -185.6124725341797, 'referece_logps/chosen': -358.74566650390625, 'logits/rejected': -0.6598392724990845, 'logits/chosen': -0.7805415391921997, 'epoch': 5.09}

 85%|████████▍ | 9106/10740 [45:07:10<7:31:10, 16.57s/it]

 85%|████████▍ | 9107/10740 [45:07:30<7:59:50, 17.63s/it]


 85%|████████▍ | 9109/10740 [45:08:12<8:40:17, 19.14s/it]

 85%|████████▍ | 9110/10740 [45:08:32<8:43:17, 19.26s/it]

 85%|████████▍ | 9111/10740 [45:08:47<8:15:21, 18.25s/it]

 85%|████████▍ | 9112/10740 [45:09:03<7:55:20, 17.52s/it]

 85%|████████▍ | 9113/10740 [45:09:20<7:45:15, 17.16s/it]

 85%|████████▍ | 9114/10740 [45:09:31<7:00:46, 15.53s/it]

 85%|████████▍ | 9115/10740 [45:09:51<7:38:58, 16.95s/it]
{'loss': 0.2397, 'learning_rate': 1.1770139193975937e-07, 'rewards/chosen': -3.656428337097168, 'rewards/rejected': -7.1154375076293945, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4590086936950684, 'policy_logps/rejected': -320.9361267089844, 'policy_logps/chosen': -557.1419067382812, 'referece_logps/rejected': -249.78173828125, 'referece_logps/chosen': -520.57763671875, 'logits/rejected': 0.0884837731719017, 'logits/chosen': 0.18817439675331116, 'epoch': 5.09}


 85%|████████▍ | 9117/10740 [45:10:27<7:44:56, 17.19s/it]

 85%|████████▍ | 9118/10740 [45:10:42<7:19:47, 16.27s/it]

 85%|████████▍ | 9119/10740 [45:10:57<7:15:53, 16.13s/it]
{'loss': 0.2147, 'learning_rate': 1.1713422507107651e-07, 'rewards/chosen': -3.590238094329834, 'rewards/rejected': -8.631803512573242, 'rewards/accuracies': 0.875, 'rewards/margins': 5.041565418243408, 'policy_logps/rejected': -275.47821044921875, 'policy_logps/chosen': -382.5616760253906, 'referece_logps/rejected': -189.16018676757812, 'referece_logps/chosen': -346.6593322753906, 'logits/rejected': -0.015762850642204285, 'logits/chosen': -0.2257886379957199, 'epoch': 5.09}

 85%|████████▍ | 9120/10740 [45:11:16<7:37:53, 16.96s/it]

 85%|████████▍ | 9121/10740 [45:11:27<6:47:02, 15.08s/it]

 85%|████████▍ | 9122/10740 [45:11:42<6:49:24, 15.18s/it]


 85%|████████▍ | 9124/10740 [45:12:14<6:58:59, 15.56s/it]

 85%|████████▍ | 9125/10740 [45:12:33<7:31:58, 16.79s/it]

 85%|████████▍ | 9126/10740 [45:12:47<7:08:38, 15.93s/it]
{'loss': 0.2206, 'learning_rate': 1.1614477505006792e-07, 'rewards/chosen': -4.014739036560059, 'rewards/rejected': -7.491203308105469, 'rewards/accuracies': 1.0, 'rewards/margins': 3.47646427154541, 'policy_logps/rejected': -373.5532531738281, 'policy_logps/chosen': -409.2709655761719, 'referece_logps/rejected': -298.6412353515625, 'referece_logps/chosen': -369.1235656738281, 'logits/rejected': -0.05938584357500076, 'logits/chosen': -0.34515994787216187, 'epoch': 5.1}

 85%|████████▍ | 9127/10740 [45:13:09<7:55:18, 17.68s/it]

 85%|████████▍ | 9128/10740 [45:13:29<8:10:24, 18.25s/it]

 85%|████████▌ | 9129/10740 [45:13:49<8:24:57, 18.81s/it]

 85%|████████▌ | 9130/10740 [45:14:09<8:32:22, 19.09s/it]

 85%|████████▌ | 9131/10740 [45:14:29<8:46:01, 19.62s/it]


 85%|████████▌ | 9133/10740 [45:15:04<8:09:43, 18.28s/it]
{'loss': 0.228, 'learning_rate': 1.1515926407884058e-07, 'rewards/chosen': -2.9346141815185547, 'rewards/rejected': -7.540513515472412, 'rewards/accuracies': 1.0, 'rewards/margins': 4.605899810791016, 'policy_logps/rejected': -319.310546875, 'policy_logps/chosen': -388.39263916015625, 'referece_logps/rejected': -243.90538024902344, 'referece_logps/chosen': -359.0465393066406, 'logits/rejected': -0.10590656101703644, 'logits/chosen': -0.3794308602809906, 'epoch': 5.1}

 85%|████████▌ | 9134/10740 [45:15:24<8:20:11, 18.69s/it]


 85%|████████▌ | 9136/10740 [45:16:00<8:09:58, 18.33s/it]

 85%|████████▌ | 9137/10740 [45:16:20<8:21:18, 18.76s/it]
{'loss': 0.2217, 'learning_rate': 1.1459788519636471e-07, 'rewards/chosen': -3.889744997024536, 'rewards/rejected': -6.666558265686035, 'rewards/accuracies': 1.0, 'rewards/margins': 2.776812791824341, 'policy_logps/rejected': -525.60888671875, 'policy_logps/chosen': -486.5660400390625, 'referece_logps/rejected': -458.943359375, 'referece_logps/chosen': -447.6685791015625, 'logits/rejected': 0.018428988754749298, 'logits/chosen': 0.028023872524499893, 'epoch': 5.1}

 85%|████████▌ | 9138/10740 [45:16:39<8:22:11, 18.81s/it]


 85%|████████▌ | 9140/10740 [45:17:00<6:30:41, 14.65s/it]
{'loss': 0.1184, 'learning_rate': 1.1417769654949006e-07, 'rewards/chosen': -3.593850612640381, 'rewards/rejected': -8.415533065795898, 'rewards/accuracies': 0.875, 'rewards/margins': 4.821683406829834, 'policy_logps/rejected': -366.6949462890625, 'policy_logps/chosen': -393.8041687011719, 'referece_logps/rejected': -282.5395812988281, 'referece_logps/chosen': -357.86566162109375, 'logits/rejected': 0.150705948472023, 'logits/chosen': -0.01075463742017746, 'epoch': 5.11}

 85%|████████▌ | 9141/10740 [45:17:11<5:58:33, 13.45s/it]

 85%|████████▌ | 9142/10740 [45:17:22<5:36:01, 12.62s/it]

 85%|████████▌ | 9143/10740 [45:17:42<6:35:36, 14.86s/it]


 85%|████████▌ | 9145/10740 [45:18:18<7:22:23, 16.64s/it]
{'loss': 0.2028, 'learning_rate': 1.1347899366173841e-07, 'rewards/chosen': -3.566065788269043, 'rewards/rejected': -8.518704414367676, 'rewards/accuracies': 1.0, 'rewards/margins': 4.952638626098633, 'policy_logps/rejected': -442.0242919921875, 'policy_logps/chosen': -388.4560241699219, 'referece_logps/rejected': -356.83721923828125, 'referece_logps/chosen': -352.7953796386719, 'logits/rejected': -0.4011702537536621, 'logits/chosen': -0.2589481472969055, 'epoch': 5.11}

 85%|████████▌ | 9146/10740 [45:18:35<7:24:24, 16.73s/it]

 85%|████████▌ | 9147/10740 [45:18:55<7:48:26, 17.64s/it]

 85%|████████▌ | 9148/10740 [45:19:07<7:02:38, 15.93s/it]

 85%|████████▌ | 9149/10740 [45:19:23<7:01:36, 15.90s/it]


 85%|████████▌ | 9151/10740 [45:19:57<7:02:01, 15.94s/it]

 85%|████████▌ | 9152/10740 [45:20:19<7:49:37, 17.74s/it]
{'loss': 0.2081, 'learning_rate': 1.1250419648617504e-07, 'rewards/chosen': -4.708744525909424, 'rewards/rejected': -7.397984027862549, 'rewards/accuracies': 0.625, 'rewards/margins': 2.689239501953125, 'policy_logps/rejected': -457.98895263671875, 'policy_logps/chosen': -645.292724609375, 'referece_logps/rejected': -384.00909423828125, 'referece_logps/chosen': -598.205322265625, 'logits/rejected': -0.2650139331817627, 'logits/chosen': -0.6877028942108154, 'epoch': 5.11}

 85%|████████▌ | 9153/10740 [45:20:36<7:47:52, 17.69s/it]

 85%|████████▌ | 9154/10740 [45:20:58<8:21:41, 18.98s/it]

 85%|████████▌ | 9155/10740 [45:21:15<8:06:23, 18.41s/it]

 85%|████████▌ | 9156/10740 [45:21:34<8:07:53, 18.48s/it]

 85%|████████▌ | 9157/10740 [45:21:48<7:35:20, 17.26s/it]

 85%|████████▌ | 9158/10740 [45:22:08<7:55:23, 18.03s/it]

 85%|████████▌ | 9159/10740 [45:22:24<7:35:34, 17.29s/it]

 85%|████████▌ | 9160/10740 [45:22:42<7:42:14, 17.55s/it]


 85%|████████▌ | 9162/10740 [45:23:19<7:55:47, 18.09s/it]
{'loss': 0.1857, 'learning_rate': 1.1111849135604768e-07, 'rewards/chosen': -4.5636701583862305, 'rewards/rejected': -7.757925510406494, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1942548751831055, 'policy_logps/rejected': -376.9359436035156, 'policy_logps/chosen': -363.9367370605469, 'referece_logps/rejected': -299.3566589355469, 'referece_logps/chosen': -318.300048828125, 'logits/rejected': 0.4075055718421936, 'logits/chosen': 0.6194394826889038, 'epoch': 5.12}

 85%|████████▌ | 9163/10740 [45:23:35<7:42:25, 17.59s/it]

 85%|████████▌ | 9164/10740 [45:23:52<7:36:39, 17.39s/it]


 85%|████████▌ | 9166/10740 [45:24:25<7:22:08, 16.85s/it]
{'loss': 0.0901, 'learning_rate': 1.105664722856694e-07, 'rewards/chosen': -3.6695070266723633, 'rewards/rejected': -7.240281105041504, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5707743167877197, 'policy_logps/rejected': -254.43472290039062, 'policy_logps/chosen': -415.3301696777344, 'referece_logps/rejected': -182.03192138671875, 'referece_logps/chosen': -378.6351318359375, 'logits/rejected': 0.28748035430908203, 'logits/chosen': 0.1342216432094574, 'epoch': 5.12}

 85%|████████▌ | 9167/10740 [45:24:36<6:41:01, 15.30s/it]

 85%|████████▌ | 9168/10740 [45:24:54<7:02:57, 16.14s/it]

 85%|████████▌ | 9169/10740 [45:25:10<7:01:33, 16.10s/it]

 85%|████████▌ | 9170/10740 [45:25:25<6:51:10, 15.71s/it]

 85%|████████▌ | 9171/10740 [45:25:44<7:17:28, 16.73s/it]

 85%|████████▌ | 9172/10740 [45:26:03<7:35:42, 17.44s/it]

 85%|████████▌ | 9173/10740 [45:26:18<7:13:54, 16.61s/it]

 85%|████████▌ | 9174/10740 [45:26:39<7:43:51, 17.77s/it]

 85%|████████▌ | 9175/10740 [45:26:58<7:59:05, 18.37s/it]

 85%|████████▌ | 9176/10740 [45:27:15<7:41:15, 17.70s/it]

 85%|████████▌ | 9177/10740 [45:27:34<7:56:27, 18.29s/it]

 85%|████████▌ | 9178/10740 [45:27:56<8:21:26, 19.26s/it]


 85%|████████▌ | 9180/10740 [45:28:31<8:02:38, 18.56s/it]

 85%|████████▌ | 9181/10740 [45:28:53<8:28:28, 19.57s/it]

 85%|████████▌ | 9182/10740 [45:29:13<8:31:48, 19.71s/it]
{'loss': 0.1608, 'learning_rate': 1.0837134739331844e-07, 'rewards/chosen': -2.9219863414764404, 'rewards/rejected': -8.70111083984375, 'rewards/accuracies': 1.0, 'rewards/margins': 5.7791242599487305, 'policy_logps/rejected': -371.78564453125, 'policy_logps/chosen': -536.7932739257812, 'referece_logps/rejected': -284.7745666503906, 'referece_logps/chosen': -507.5733642578125, 'logits/rejected': 0.7296077609062195, 'logits/chosen': 0.5272964835166931, 'epoch': 5.13}


 86%|████████▌ | 9184/10740 [45:29:51<8:21:47, 19.35s/it]
{'loss': 0.1949, 'learning_rate': 1.0809841546067066e-07, 'rewards/chosen': -3.0211284160614014, 'rewards/rejected': -7.535911560058594, 'rewards/accuracies': 0.875, 'rewards/margins': 4.5147833824157715, 'policy_logps/rejected': -417.3656005859375, 'policy_logps/chosen': -329.8585205078125, 'referece_logps/rejected': -342.0065002441406, 'referece_logps/chosen': -299.647216796875, 'logits/rejected': 0.07274872809648514, 'logits/chosen': -0.16470830142498016, 'epoch': 5.13}

 86%|████████▌ | 9185/10740 [45:30:12<8:29:51, 19.67s/it]


 86%|████████▌ | 9187/10740 [45:30:47<7:56:49, 18.42s/it]
{'loss': 0.1574, 'learning_rate': 1.0768962599745579e-07, 'rewards/chosen': -2.0908823013305664, 'rewards/rejected': -7.025475978851318, 'rewards/accuracies': 1.0, 'rewards/margins': 4.934593200683594, 'policy_logps/rejected': -360.79803466796875, 'policy_logps/chosen': -477.89599609375, 'referece_logps/rejected': -290.5433349609375, 'referece_logps/chosen': -456.9871520996094, 'logits/rejected': -0.052972227334976196, 'logits/chosen': -0.043918024748563766, 'epoch': 5.13}


 86%|████████▌ | 9189/10740 [45:31:29<8:30:46, 19.76s/it]
{'loss': 0.201, 'learning_rate': 1.0741750545707473e-07, 'rewards/chosen': -4.0018110275268555, 'rewards/rejected': -6.554262638092041, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5524520874023438, 'policy_logps/rejected': -351.1051330566406, 'policy_logps/chosen': -418.1513977050781, 'referece_logps/rejected': -285.5625, 'referece_logps/chosen': -378.13330078125, 'logits/rejected': -0.4889817237854004, 'logits/chosen': -0.8549065589904785, 'epoch': 5.13}

 86%|████████▌ | 9190/10740 [45:31:49<8:27:41, 19.65s/it]

 86%|████████▌ | 9191/10740 [45:32:04<7:53:28, 18.34s/it]


 86%|████████▌ | 9193/10740 [45:32:35<7:25:23, 17.27s/it]

 86%|████████▌ | 9194/10740 [45:32:55<7:44:51, 18.04s/it]
{'loss': 0.1647, 'learning_rate': 1.0673862501728104e-07, 'rewards/chosen': -4.200223922729492, 'rewards/rejected': -8.374895095825195, 'rewards/accuracies': 0.875, 'rewards/margins': 4.174671649932861, 'policy_logps/rejected': -490.61846923828125, 'policy_logps/chosen': -414.99664306640625, 'referece_logps/rejected': -406.8695068359375, 'referece_logps/chosen': -372.9944152832031, 'logits/rejected': -0.6041959524154663, 'logits/chosen': -0.6179262399673462, 'epoch': 5.14}

 86%|████████▌ | 9195/10740 [45:33:15<7:56:14, 18.49s/it]

 86%|████████▌ | 9196/10740 [45:33:30<7:28:23, 17.42s/it]

 86%|████████▌ | 9197/10740 [45:33:48<7:34:02, 17.66s/it]

 86%|████████▌ | 9198/10740 [45:34:10<8:07:41, 18.98s/it]


 86%|████████▌ | 9200/10740 [45:34:50<8:17:52, 19.40s/it]
{'loss': 0.1292, 'learning_rate': 1.0592664968687625e-07, 'rewards/chosen': -3.082730770111084, 'rewards/rejected': -7.665678977966309, 'rewards/accuracies': 1.0, 'rewards/margins': 4.582948684692383, 'policy_logps/rejected': -537.1044311523438, 'policy_logps/chosen': -283.79986572265625, 'referece_logps/rejected': -460.4476318359375, 'referece_logps/chosen': -252.9725341796875, 'logits/rejected': -0.3539780378341675, 'logits/chosen': -0.22242072224617004, 'epoch': 5.14}

 86%|████████▌ | 9201/10740 [45:35:06<7:55:37, 18.54s/it]

 86%|████████▌ | 9202/10740 [45:35:24<7:52:56, 18.45s/it]


 86%|████████▌ | 9204/10740 [45:36:03<8:05:15, 18.96s/it]
{'loss': 0.2275, 'learning_rate': 1.0538695899906613e-07, 'rewards/chosen': -3.337935447692871, 'rewards/rejected': -6.245317459106445, 'rewards/accuracies': 1.0, 'rewards/margins': 2.907382011413574, 'policy_logps/rejected': -326.5467529296875, 'policy_logps/chosen': -358.3070068359375, 'referece_logps/rejected': -264.09356689453125, 'referece_logps/chosen': -324.92767333984375, 'logits/rejected': 0.25122496485710144, 'logits/chosen': 0.10281224548816681, 'epoch': 5.14}

 86%|████████▌ | 9205/10740 [45:36:23<8:12:37, 19.26s/it]

 86%|████████▌ | 9206/10740 [45:36:42<8:08:00, 19.09s/it]

 86%|████████▌ | 9207/10740 [45:37:00<7:58:17, 18.72s/it]

 86%|████████▌ | 9208/10740 [45:37:18<7:55:29, 18.62s/it]

 86%|████████▌ | 9209/10740 [45:37:32<7:21:00, 17.28s/it]

 86%|████████▌ | 9210/10740 [45:37:49<7:13:07, 16.99s/it]

 86%|████████▌ | 9211/10740 [45:38:07<7:21:20, 17.32s/it]

 86%|████████▌ | 9212/10740 [45:38:27<7:39:53, 18.06s/it]

 86%|████████▌ | 9213/10740 [45:38:49<8:13:29, 19.39s/it]

 86%|████████▌ | 9214/10740 [45:39:03<7:34:29, 17.87s/it]

 86%|████████▌ | 9215/10740 [45:39:19<7:18:18, 17.24s/it]


 86%|████████▌ | 9217/10740 [45:39:50<6:47:36, 16.06s/it]

 86%|████████▌ | 9218/10740 [45:40:10<7:16:20, 17.20s/it]

 86%|████████▌ | 9219/10740 [45:40:28<7:24:04, 17.52s/it]

 86%|████████▌ | 9220/10740 [45:40:50<7:56:09, 18.80s/it]
{'loss': 0.0907, 'learning_rate': 1.0324122283171765e-07, 'rewards/chosen': -4.49979829788208, 'rewards/rejected': -8.850653648376465, 'rewards/accuracies': 1.0, 'rewards/margins': 4.350856304168701, 'policy_logps/rejected': -483.4004211425781, 'policy_logps/chosen': -567.2865600585938, 'referece_logps/rejected': -394.8938903808594, 'referece_logps/chosen': -522.2886352539062, 'logits/rejected': 1.416897177696228, 'logits/chosen': 1.4104715585708618, 'epoch': 5.15}

 86%|████████▌ | 9221/10740 [45:41:09<8:00:20, 18.97s/it]

 86%|████████▌ | 9222/10740 [45:41:29<8:05:48, 19.20s/it]

 86%|████████▌ | 9223/10740 [45:41:49<8:15:38, 19.60s/it]

 86%|████████▌ | 9224/10740 [45:42:09<8:16:21, 19.65s/it]


 86%|████████▌ | 9226/10740 [45:42:38<7:05:27, 16.86s/it]
{'loss': 0.119, 'learning_rate': 1.0244195220966645e-07, 'rewards/chosen': -4.305543422698975, 'rewards/rejected': -8.826910972595215, 'rewards/accuracies': 1.0, 'rewards/margins': 4.521367073059082, 'policy_logps/rejected': -402.2479553222656, 'policy_logps/chosen': -432.6221618652344, 'referece_logps/rejected': -313.9788818359375, 'referece_logps/chosen': -389.5667419433594, 'logits/rejected': -0.37017548084259033, 'logits/chosen': -0.433933287858963, 'epoch': 5.15}

 86%|████████▌ | 9227/10740 [45:42:57<7:19:51, 17.44s/it]

 86%|████████▌ | 9228/10740 [45:43:17<7:36:45, 18.13s/it]

 86%|████████▌ | 9229/10740 [45:43:34<7:34:23, 18.04s/it]

 86%|████████▌ | 9230/10740 [45:43:52<7:33:20, 18.01s/it]

 86%|████████▌ | 9231/10740 [45:44:11<7:36:14, 18.14s/it]

 86%|████████▌ | 9232/10740 [45:44:31<7:50:45, 18.73s/it]

 86%|████████▌ | 9233/10740 [45:44:51<7:59:23, 19.09s/it]

 86%|████████▌ | 9234/10740 [45:45:07<7:33:58, 18.09s/it]

 86%|████████▌ | 9235/10740 [45:45:24<7:26:28, 17.80s/it]

 86%|████████▌ | 9236/10740 [45:45:42<7:31:31, 18.01s/it]

 86%|████████▌ | 9237/10740 [45:45:58<7:16:37, 17.43s/it]

 86%|████████▌ | 9238/10740 [45:46:18<7:32:41, 18.08s/it]

 86%|████████▌ | 9239/10740 [45:46:39<7:56:43, 19.06s/it]

 86%|████████▌ | 9240/10740 [45:46:57<7:44:20, 18.57s/it]

 86%|████████▌ | 9241/10740 [45:47:17<7:57:01, 19.09s/it]

 86%|████████▌ | 9242/10740 [45:47:37<8:04:12, 19.39s/it]

 86%|████████▌ | 9243/10740 [45:47:58<8:14:40, 19.83s/it]

 86%|████████▌ | 9244/10740 [45:48:16<7:58:56, 19.21s/it]

 86%|████████▌ | 9245/10740 [45:48:31<7:26:16, 17.91s/it]

 86%|████████▌ | 9246/10740 [45:48:41<6:31:46, 15.73s/it]

 86%|████████▌ | 9247/10740 [45:49:01<6:57:50, 16.79s/it]

 86%|████████▌ | 9248/10740 [45:49:13<6:22:11, 15.37s/it]

 86%|████████▌ | 9249/10740 [45:49:34<7:10:11, 17.31s/it]

 86%|████████▌ | 9250/10740 [45:49:49<6:50:31, 16.53s/it]

 86%|████████▌ | 9251/10740 [45:50:09<7:12:32, 17.43s/it]

 86%|████████▌ | 9252/10740 [45:50:27<7:16:20, 17.59s/it]

 86%|████████▌ | 9253/10740 [45:50:46<7:31:54, 18.23s/it]

 86%|████████▌ | 9254/10740 [45:51:06<7:44:18, 18.75s/it]

 86%|████████▌ | 9255/10740 [45:51:27<7:54:41, 19.18s/it]

 86%|████████▌ | 9256/10740 [45:51:47<8:04:12, 19.58s/it]

 86%|████████▌ | 9257/10740 [45:52:04<7:43:32, 18.75s/it]

 86%|████████▌ | 9258/10740 [45:52:24<7:53:54, 19.19s/it]

 86%|████████▌ | 9259/10740 [45:52:40<7:29:18, 18.20s/it]

 86%|████████▌ | 9260/10740 [45:53:00<7:40:10, 18.66s/it]

 86%|████████▌ | 9261/10740 [45:53:20<7:55:52, 19.31s/it]

 86%|████████▌ | 9262/10740 [45:53:41<8:03:31, 19.63s/it]

 86%|████████▌ | 9263/10740 [45:54:03<8:19:44, 20.30s/it]

 86%|████████▋ | 9264/10740 [45:54:22<8:14:08, 20.09s/it]

 86%|████████▋ | 9265/10740 [45:54:42<8:11:47, 20.00s/it]

 86%|████████▋ | 9266/10740 [45:54:57<7:35:45, 18.55s/it]

 86%|████████▋ | 9267/10740 [45:55:16<7:33:31, 18.47s/it]

 86%|████████▋ | 9268/10740 [45:55:32<7:17:14, 17.82s/it]

 86%|████████▋ | 9269/10740 [45:55:52<7:30:47, 18.39s/it]


 86%|████████▋ | 9271/10740 [45:56:26<7:21:19, 18.03s/it]

 86%|████████▋ | 9272/10740 [45:56:46<7:33:41, 18.54s/it]

 86%|████████▋ | 9273/10740 [45:57:00<6:59:17, 17.15s/it]

 86%|████████▋ | 9274/10740 [45:57:19<7:15:50, 17.84s/it]
{'loss': 0.1822, 'learning_rate': 9.615380411069496e-08, 'rewards/chosen': -4.666219711303711, 'rewards/rejected': -7.669858932495117, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0036392211914062, 'policy_logps/rejected': -357.3077087402344, 'policy_logps/chosen': -329.2448425292969, 'referece_logps/rejected': -280.6091003417969, 'referece_logps/chosen': -282.5826110839844, 'logits/rejected': -0.5596957802772522, 'logits/chosen': -0.4722694158554077, 'epoch': 5.18}


 86%|████████▋ | 9276/10740 [45:57:53<7:04:51, 17.41s/it]

 86%|████████▋ | 9277/10740 [45:58:15<7:31:43, 18.53s/it]

 86%|████████▋ | 9278/10740 [45:58:28<6:57:07, 17.12s/it]

 86%|████████▋ | 9279/10740 [45:58:45<6:51:58, 16.92s/it]

 86%|████████▋ | 9280/10740 [45:59:03<6:59:21, 17.23s/it]

 86%|████████▋ | 9281/10740 [45:59:24<7:28:25, 18.44s/it]

 86%|████████▋ | 9282/10740 [45:59:44<7:37:55, 18.84s/it]

 86%|████████▋ | 9283/10740 [45:59:55<6:38:02, 16.39s/it]

 86%|████████▋ | 9284/10740 [46:00:12<6:45:35, 16.71s/it]

 86%|████████▋ | 9285/10740 [46:00:24<6:13:03, 15.38s/it]

 86%|████████▋ | 9286/10740 [46:00:44<6:45:15, 16.72s/it]

 86%|████████▋ | 9287/10740 [46:00:59<6:31:03, 16.15s/it]

 86%|████████▋ | 9288/10740 [46:01:13<6:12:07, 15.38s/it]

 86%|████████▋ | 9289/10740 [46:01:32<6:41:58, 16.62s/it]

 86%|████████▋ | 9290/10740 [46:01:54<7:20:25, 18.22s/it]
{'loss': 0.1433, 'learning_rate': 9.409978007785512e-08, 'rewards/chosen': -5.508439064025879, 'rewards/rejected': -8.658028602600098, 'rewards/accuracies': 1.0, 'rewards/margins': 3.149590253829956, 'policy_logps/rejected': -388.533447265625, 'policy_logps/chosen': -479.6646728515625, 'referece_logps/rejected': -301.95318603515625, 'referece_logps/chosen': -424.58026123046875, 'logits/rejected': 0.6476705074310303, 'logits/chosen': 0.769295334815979, 'epoch': 5.19}


 87%|████████▋ | 9292/10740 [46:02:27<6:49:34, 16.97s/it]

 87%|████████▋ | 9293/10740 [46:02:45<6:55:48, 17.24s/it]

 87%|████████▋ | 9294/10740 [46:03:05<7:14:35, 18.03s/it]

 87%|████████▋ | 9295/10740 [46:03:23<7:14:15, 18.03s/it]

 87%|████████▋ | 9296/10740 [46:03:34<6:20:23, 15.81s/it]

 87%|████████▋ | 9297/10740 [46:03:48<6:12:40, 15.50s/it]

 87%|████████▋ | 9298/10740 [46:03:59<5:37:33, 14.05s/it]

 87%|████████▋ | 9299/10740 [46:04:15<5:51:31, 14.64s/it]

 87%|████████▋ | 9300/10740 [46:04:35<6:27:37, 16.15s/it]
{'loss': 0.1743, 'learning_rate': 9.282672322134366e-08, 'rewards/chosen': -5.192958354949951, 'rewards/rejected': -11.3778076171875, 'rewards/accuracies': 1.0, 'rewards/margins': 6.184848785400391, 'policy_logps/rejected': -610.9041748046875, 'policy_logps/chosen': -600.5120849609375, 'referece_logps/rejected': -497.1261291503906, 'referece_logps/chosen': -548.5824584960938, 'logits/rejected': 0.11732051521539688, 'logits/chosen': 0.15711480379104614, 'epoch': 5.2}


 87%|████████▋ | 9302/10740 [46:05:14<7:09:09, 17.91s/it]

 87%|████████▋ | 9303/10740 [46:05:31<7:00:08, 17.54s/it]

 87%|████████▋ | 9304/10740 [46:05:46<6:43:30, 16.86s/it]

 87%|████████▋ | 9305/10740 [46:06:05<6:54:16, 17.32s/it]

 87%|████████▋ | 9306/10740 [46:06:21<6:49:43, 17.14s/it]

 87%|████████▋ | 9307/10740 [46:06:33<6:11:19, 15.55s/it]

 87%|████████▋ | 9308/10740 [46:06:54<6:48:45, 17.13s/it]

 87%|████████▋ | 9309/10740 [46:07:14<7:06:10, 17.87s/it]

 87%|████████▋ | 9310/10740 [46:07:29<6:50:58, 17.24s/it]

 87%|████████▋ | 9311/10740 [46:07:49<7:10:35, 18.08s/it]

 87%|████████▋ | 9312/10740 [46:08:08<7:13:00, 18.19s/it]

 87%|████████▋ | 9313/10740 [46:08:25<7:04:08, 17.83s/it]

 87%|████████▋ | 9314/10740 [46:08:43<7:04:40, 17.87s/it]

 87%|████████▋ | 9315/10740 [46:09:05<7:34:48, 19.15s/it]

 87%|████████▋ | 9316/10740 [46:09:25<7:39:08, 19.35s/it]
{'loss': 0.123, 'learning_rate': 9.080699903064937e-08, 'rewards/chosen': -4.195358753204346, 'rewards/rejected': -8.117091178894043, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9217324256896973, 'policy_logps/rejected': -340.10955810546875, 'policy_logps/chosen': -403.1615905761719, 'referece_logps/rejected': -258.93865966796875, 'referece_logps/chosen': -361.2080383300781, 'logits/rejected': -0.09329809248447418, 'logits/chosen': -0.2688455581665039, 'epoch': 5.2}


 87%|████████▋ | 9318/10740 [46:09:54<6:47:19, 17.19s/it]

 87%|████████▋ | 9319/10740 [46:10:13<6:57:10, 17.61s/it]
{'loss': 0.2229, 'learning_rate': 9.043065609053236e-08, 'rewards/chosen': -4.153322219848633, 'rewards/rejected': -6.867263317108154, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7139413356781006, 'policy_logps/rejected': -590.8543701171875, 'policy_logps/chosen': -797.0947875976562, 'referece_logps/rejected': -522.1817626953125, 'referece_logps/chosen': -755.5616455078125, 'logits/rejected': 0.7046366333961487, 'logits/chosen': 0.5300326943397522, 'epoch': 5.21}

 87%|████████▋ | 9320/10740 [46:10:28<6:38:09, 16.82s/it]

 87%|████████▋ | 9321/10740 [46:10:48<7:01:55, 17.84s/it]


 87%|████████▋ | 9323/10740 [46:11:14<6:02:10, 15.34s/it]

 87%|████████▋ | 9324/10740 [46:11:29<6:02:29, 15.36s/it]

 87%|████████▋ | 9325/10740 [46:11:50<6:38:10, 16.88s/it]

 87%|████████▋ | 9326/10740 [46:12:04<6:18:42, 16.07s/it]

 87%|████████▋ | 9327/10740 [46:12:17<5:59:23, 15.26s/it]

 87%|████████▋ | 9328/10740 [46:12:32<5:53:25, 15.02s/it]
{'loss': 0.1445, 'learning_rate': 8.930609578749194e-08, 'rewards/chosen': -2.63763427734375, 'rewards/rejected': -7.116015434265137, 'rewards/accuracies': 1.0, 'rewards/margins': 4.4783806800842285, 'policy_logps/rejected': -299.536376953125, 'policy_logps/chosen': -341.6927795410156, 'referece_logps/rejected': -228.376220703125, 'referece_logps/chosen': -315.31646728515625, 'logits/rejected': -0.9457666873931885, 'logits/chosen': -1.0082764625549316, 'epoch': 5.21}


 87%|████████▋ | 9330/10740 [46:13:05<6:09:54, 15.74s/it]

 87%|████████▋ | 9331/10740 [46:13:25<6:36:55, 16.90s/it]

 87%|████████▋ | 9332/10740 [46:13:45<6:58:46, 17.85s/it]
{'loss': 0.1444, 'learning_rate': 8.880844429232959e-08, 'rewards/chosen': -3.9470126628875732, 'rewards/rejected': -7.325674057006836, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3786609172821045, 'policy_logps/rejected': -278.132080078125, 'policy_logps/chosen': -295.1817932128906, 'referece_logps/rejected': -204.8753204345703, 'referece_logps/chosen': -255.71170043945312, 'logits/rejected': -0.989534318447113, 'logits/chosen': -1.0342124700546265, 'epoch': 5.21}


 87%|████████▋ | 9334/10740 [46:14:25<7:26:44, 19.06s/it]

 87%|████████▋ | 9335/10740 [46:14:43<7:20:49, 18.83s/it]
{'loss': 0.1934, 'learning_rate': 8.843607581994583e-08, 'rewards/chosen': -3.906813621520996, 'rewards/rejected': -7.405810832977295, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4989967346191406, 'policy_logps/rejected': -316.2991943359375, 'policy_logps/chosen': -356.3498840332031, 'referece_logps/rejected': -242.24107360839844, 'referece_logps/chosen': -317.28173828125, 'logits/rejected': -0.10970008373260498, 'logits/chosen': -0.3042483925819397, 'epoch': 5.22}


 87%|████████▋ | 9337/10740 [46:15:12<6:16:54, 16.12s/it]

 87%|████████▋ | 9338/10740 [46:15:28<6:17:50, 16.17s/it]

 87%|████████▋ | 9339/10740 [46:15:48<6:43:01, 17.26s/it]

 87%|████████▋ | 9340/10740 [46:16:06<6:46:52, 17.44s/it]

 87%|████████▋ | 9341/10740 [46:16:21<6:33:04, 16.86s/it]
{'loss': 0.1844, 'learning_rate': 8.769357772155262e-08, 'rewards/chosen': -3.570857286453247, 'rewards/rejected': -8.545099258422852, 'rewards/accuracies': 1.0, 'rewards/margins': 4.974241733551025, 'policy_logps/rejected': -280.9443359375, 'policy_logps/chosen': -245.36090087890625, 'referece_logps/rejected': -195.49334716796875, 'referece_logps/chosen': -209.65232849121094, 'logits/rejected': -0.5259538888931274, 'logits/chosen': -0.30163735151290894, 'epoch': 5.22}

 87%|████████▋ | 9342/10740 [46:16:39<6:38:51, 17.12s/it]


 87%|████████▋ | 9344/10740 [46:17:14<6:42:18, 17.29s/it]

 87%|████████▋ | 9345/10740 [46:17:34<7:00:25, 18.08s/it]

 87%|████████▋ | 9346/10740 [46:17:54<7:16:29, 18.79s/it]
{'loss': 0.1128, 'learning_rate': 8.707711105666726e-08, 'rewards/chosen': -3.702068328857422, 'rewards/rejected': -7.222372531890869, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5203044414520264, 'policy_logps/rejected': -428.7104797363281, 'policy_logps/chosen': -541.0438232421875, 'referece_logps/rejected': -356.4867858886719, 'referece_logps/chosen': -504.0231628417969, 'logits/rejected': -0.7386350631713867, 'logits/chosen': -0.8784928321838379, 'epoch': 5.22}


 87%|████████▋ | 9348/10740 [46:18:33<7:26:42, 19.25s/it]
{'loss': 0.1359, 'learning_rate': 8.683110554029871e-08, 'rewards/chosen': -3.14479660987854, 'rewards/rejected': -7.376152992248535, 'rewards/accuracies': 1.0, 'rewards/margins': 4.231355667114258, 'policy_logps/rejected': -361.8066101074219, 'policy_logps/chosen': -382.9368896484375, 'referece_logps/rejected': -288.0451354980469, 'referece_logps/chosen': -351.4888916015625, 'logits/rejected': -0.18064014613628387, 'logits/chosen': -0.2289336621761322, 'epoch': 5.22}

 87%|████████▋ | 9349/10740 [46:18:53<7:29:35, 19.39s/it]


 87%|████████▋ | 9351/10740 [46:19:36<7:52:15, 20.40s/it]

 87%|████████▋ | 9352/10740 [46:19:56<7:48:07, 20.24s/it]

 87%|████████▋ | 9353/10740 [46:20:16<7:45:25, 20.13s/it]

 87%|████████▋ | 9354/10740 [46:20:35<7:41:44, 19.99s/it]
{'loss': 0.1365, 'learning_rate': 8.609508266734977e-08, 'rewards/chosen': -4.848653316497803, 'rewards/rejected': -8.047880172729492, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1992270946502686, 'policy_logps/rejected': -436.330078125, 'policy_logps/chosen': -305.8271484375, 'referece_logps/rejected': -355.8512878417969, 'referece_logps/chosen': -257.3406066894531, 'logits/rejected': 0.023705344647169113, 'logits/chosen': -0.024059627205133438, 'epoch': 5.23}

 87%|████████▋ | 9355/10740 [46:20:55<7:39:46, 19.92s/it]


 87%|████████▋ | 9357/10740 [46:21:27<6:52:00, 17.87s/it]

 87%|████████▋ | 9358/10740 [46:21:48<7:10:40, 18.70s/it]
{'loss': 0.163, 'learning_rate': 8.560606304112061e-08, 'rewards/chosen': -3.3437514305114746, 'rewards/rejected': -7.341593265533447, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9978415966033936, 'policy_logps/rejected': -255.4256591796875, 'policy_logps/chosen': -380.93890380859375, 'referece_logps/rejected': -182.00973510742188, 'referece_logps/chosen': -347.50140380859375, 'logits/rejected': -0.22569194436073303, 'logits/chosen': -0.3838377892971039, 'epoch': 5.23}

 87%|████████▋ | 9359/10740 [46:22:07<7:11:32, 18.75s/it]

 87%|████████▋ | 9360/10740 [46:22:23<6:54:55, 18.04s/it]


 87%|████████▋ | 9362/10740 [46:23:00<6:51:58, 17.94s/it]

 87%|████████▋ | 9363/10740 [46:23:18<6:52:54, 17.99s/it]
{'loss': 0.1901, 'learning_rate': 8.499665983468074e-08, 'rewards/chosen': -3.642554759979248, 'rewards/rejected': -7.049435615539551, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4068806171417236, 'policy_logps/rejected': -306.7318420410156, 'policy_logps/chosen': -300.06182861328125, 'referece_logps/rejected': -236.23745727539062, 'referece_logps/chosen': -263.6362609863281, 'logits/rejected': -0.7212834358215332, 'logits/chosen': -0.8168187737464905, 'epoch': 5.23}

 87%|████████▋ | 9364/10740 [46:23:31<6:19:10, 16.53s/it]

 87%|████████▋ | 9365/10740 [46:23:51<6:42:38, 17.57s/it]


 87%|████████▋ | 9367/10740 [46:24:27<6:39:10, 17.44s/it]

 87%|████████▋ | 9368/10740 [46:24:49<7:10:51, 18.84s/it]

 87%|████████▋ | 9369/10740 [46:25:02<6:31:11, 17.12s/it]

 87%|████████▋ | 9370/10740 [46:25:16<6:10:09, 16.21s/it]

 87%|████████▋ | 9371/10740 [46:25:36<6:34:37, 17.30s/it]
{'loss': 0.1439, 'learning_rate': 8.402594281174025e-08, 'rewards/chosen': -5.111940860748291, 'rewards/rejected': -7.6347336769104, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5227932929992676, 'policy_logps/rejected': -279.05548095703125, 'policy_logps/chosen': -375.2188720703125, 'referece_logps/rejected': -202.70816040039062, 'referece_logps/chosen': -324.0994567871094, 'logits/rejected': 0.9300746917724609, 'logits/chosen': 1.0934208631515503, 'epoch': 5.24}


 87%|████████▋ | 9373/10740 [46:26:11<6:29:39, 17.10s/it]

 87%|████████▋ | 9374/10740 [46:26:33<7:03:44, 18.61s/it]

 87%|████████▋ | 9375/10740 [46:26:54<7:22:29, 19.45s/it]

 87%|████████▋ | 9376/10740 [46:27:14<7:24:07, 19.54s/it]

 87%|████████▋ | 9377/10740 [46:27:32<7:16:37, 19.22s/it]

 87%|████████▋ | 9378/10740 [46:27:52<7:19:15, 19.35s/it]

 87%|████████▋ | 9379/10740 [46:28:08<6:59:48, 18.51s/it]
{'loss': 0.1302, 'learning_rate': 8.306055763181241e-08, 'rewards/chosen': -3.613554000854492, 'rewards/rejected': -7.750792503356934, 'rewards/accuracies': 1.0, 'rewards/margins': 4.137238502502441, 'policy_logps/rejected': -244.3416290283203, 'policy_logps/chosen': -471.0041809082031, 'referece_logps/rejected': -166.83370971679688, 'referece_logps/chosen': -434.86865234375, 'logits/rejected': -0.23459045588970184, 'logits/chosen': -0.7825166583061218, 'epoch': 5.24}

 87%|████████▋ | 9380/10740 [46:28:27<7:02:42, 18.65s/it]

 87%|████████▋ | 9381/10740 [46:28:46<7:00:09, 18.55s/it]


 87%|████████▋ | 9383/10740 [46:29:24<7:04:52, 18.79s/it]

 87%|████████▋ | 9384/10740 [46:29:41<6:51:26, 18.21s/it]

 87%|████████▋ | 9385/10740 [46:29:54<6:16:41, 16.68s/it]

 87%|████████▋ | 9386/10740 [46:30:05<5:36:00, 14.89s/it]

 87%|████████▋ | 9387/10740 [46:30:23<5:56:08, 15.79s/it]

 87%|████████▋ | 9388/10740 [46:30:45<6:37:42, 17.65s/it]

 87%|████████▋ | 9389/10740 [46:31:03<6:42:29, 17.88s/it]
{'loss': 0.1447, 'learning_rate': 8.186133261908212e-08, 'rewards/chosen': -3.954514980316162, 'rewards/rejected': -9.780985832214355, 'rewards/accuracies': 1.0, 'rewards/margins': 5.826470375061035, 'policy_logps/rejected': -413.9445495605469, 'policy_logps/chosen': -412.76727294921875, 'referece_logps/rejected': -316.1347351074219, 'referece_logps/chosen': -373.22210693359375, 'logits/rejected': 0.0327928327023983, 'logits/chosen': -0.10049006342887878, 'epoch': 5.25}

 87%|████████▋ | 9390/10740 [46:31:20<6:33:44, 17.50s/it]


 87%|████████▋ | 9392/10740 [46:31:43<5:25:55, 14.51s/it]

 87%|████████▋ | 9393/10740 [46:31:54<5:04:13, 13.55s/it]
{'loss': 0.187, 'learning_rate': 8.138398019895786e-08, 'rewards/chosen': -4.671926021575928, 'rewards/rejected': -7.958731174468994, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2868049144744873, 'policy_logps/rejected': -351.87432861328125, 'policy_logps/chosen': -677.926025390625, 'referece_logps/rejected': -272.2870178222656, 'referece_logps/chosen': -631.206787109375, 'logits/rejected': 0.04058733582496643, 'logits/chosen': -0.45743560791015625, 'epoch': 5.25}


 87%|████████▋ | 9395/10740 [46:32:24<5:15:46, 14.09s/it]
{'loss': 0.1736, 'learning_rate': 8.114580524777758e-08, 'rewards/chosen': -3.7391586303710938, 'rewards/rejected': -6.885248184204102, 'rewards/accuracies': 0.875, 'rewards/margins': 3.146089553833008, 'policy_logps/rejected': -356.10418701171875, 'policy_logps/chosen': -533.1611938476562, 'referece_logps/rejected': -287.251708984375, 'referece_logps/chosen': -495.7695617675781, 'logits/rejected': 0.022651247680187225, 'logits/chosen': -0.16938379406929016, 'epoch': 5.25}


 87%|████████▋ | 9397/10740 [46:32:59<5:46:56, 15.50s/it]
{'loss': 0.135, 'learning_rate': 8.090796458475924e-08, 'rewards/chosen': -3.3290023803710938, 'rewards/rejected': -9.4176664352417, 'rewards/accuracies': 1.0, 'rewards/margins': 6.0886640548706055, 'policy_logps/rejected': -341.60760498046875, 'policy_logps/chosen': -514.6162719726562, 'referece_logps/rejected': -247.4309539794922, 'referece_logps/chosen': -481.32623291015625, 'logits/rejected': 0.33692920207977295, 'logits/chosen': 0.15537616610527039, 'epoch': 5.25}


 88%|████████▊ | 9399/10740 [46:33:33<6:10:14, 16.57s/it]

 88%|████████▊ | 9400/10740 [46:33:54<6:41:27, 17.98s/it]
{'loss': 0.1531, 'learning_rate': 8.055183056978421e-08, 'rewards/chosen': -2.7986652851104736, 'rewards/rejected': -8.15809154510498, 'rewards/accuracies': 0.875, 'rewards/margins': 5.359426021575928, 'policy_logps/rejected': -249.42330932617188, 'policy_logps/chosen': -339.1595764160156, 'referece_logps/rejected': -167.84239196777344, 'referece_logps/chosen': -311.17291259765625, 'logits/rejected': -0.44652602076530457, 'logits/chosen': -0.5932188034057617, 'epoch': 5.25}


 88%|████████▊ | 9402/10740 [46:34:31<6:40:58, 17.98s/it]

 88%|████████▊ | 9403/10740 [46:34:51<6:54:54, 18.62s/it]

 88%|████████▊ | 9404/10740 [46:35:03<6:08:58, 16.57s/it]

 88%|████████▊ | 9405/10740 [46:35:23<6:35:58, 17.80s/it]

 88%|████████▊ | 9406/10740 [46:35:41<6:33:06, 17.68s/it]
{'loss': 0.1941, 'learning_rate': 7.984182073438117e-08, 'rewards/chosen': -3.990670680999756, 'rewards/rejected': -6.810187816619873, 'rewards/accuracies': 1.0, 'rewards/margins': 2.819516658782959, 'policy_logps/rejected': -310.9587707519531, 'policy_logps/chosen': -369.0855712890625, 'referece_logps/rejected': -242.8568878173828, 'referece_logps/chosen': -329.1788330078125, 'logits/rejected': 0.2665254473686218, 'logits/chosen': 0.14276787638664246, 'epoch': 5.25}


 88%|████████▊ | 9408/10740 [46:36:23<7:13:01, 19.51s/it]

 88%|████████▊ | 9409/10740 [46:36:41<7:02:10, 19.03s/it]
{'loss': 0.166, 'learning_rate': 7.948794549514748e-08, 'rewards/chosen': -4.669480800628662, 'rewards/rejected': -8.091837882995605, 'rewards/accuracies': 0.75, 'rewards/margins': 3.422356605529785, 'policy_logps/rejected': -425.0076599121094, 'policy_logps/chosen': -516.224853515625, 'referece_logps/rejected': -344.0892333984375, 'referece_logps/chosen': -469.53009033203125, 'logits/rejected': 0.48358404636383057, 'logits/chosen': 0.3959864377975464, 'epoch': 5.26}

 88%|████████▊ | 9410/10740 [46:36:58<6:47:47, 18.40s/it]

 88%|████████▊ | 9411/10740 [46:37:18<6:58:57, 18.91s/it]

 88%|████████▊ | 9412/10740 [46:37:38<7:04:19, 19.17s/it]


 88%|████████▊ | 9414/10740 [46:38:17<7:09:31, 19.44s/it]

 88%|████████▊ | 9415/10740 [46:38:33<6:43:20, 18.26s/it]

 88%|████████▊ | 9416/10740 [46:38:55<7:07:04, 19.35s/it]

 88%|████████▊ | 9417/10740 [46:39:15<7:11:41, 19.58s/it]

 88%|████████▊ | 9418/10740 [46:39:34<7:05:19, 19.30s/it]
{'loss': 0.1781, 'learning_rate': 7.843084196553484e-08, 'rewards/chosen': -3.8289501667022705, 'rewards/rejected': -8.496278762817383, 'rewards/accuracies': 1.0, 'rewards/margins': 4.66732931137085, 'policy_logps/rejected': -480.95526123046875, 'policy_logps/chosen': -424.3407287597656, 'referece_logps/rejected': -395.992431640625, 'referece_logps/chosen': -386.05120849609375, 'logits/rejected': -0.03585371375083923, 'logits/chosen': -0.1824319064617157, 'epoch': 5.26}


 88%|████████▊ | 9420/10740 [46:40:03<6:10:11, 16.83s/it]

 88%|████████▊ | 9421/10740 [46:40:23<6:28:49, 17.69s/it]

 88%|████████▊ | 9422/10740 [46:40:37<6:05:24, 16.63s/it]

 88%|████████▊ | 9423/10740 [46:40:57<6:27:51, 17.67s/it]

 88%|████████▊ | 9424/10740 [46:41:14<6:20:29, 17.35s/it]

 88%|████████▊ | 9425/10740 [46:41:34<6:36:31, 18.09s/it]

 88%|████████▊ | 9426/10740 [46:41:55<7:01:28, 19.25s/it]

 88%|████████▊ | 9427/10740 [46:42:15<7:05:34, 19.45s/it]

 88%|████████▊ | 9428/10740 [46:42:34<6:58:04, 19.12s/it]
{'loss': 0.1976, 'learning_rate': 7.726424562258649e-08, 'rewards/chosen': -3.8418755531311035, 'rewards/rejected': -7.281350612640381, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4394755363464355, 'policy_logps/rejected': -412.9080505371094, 'policy_logps/chosen': -376.20904541015625, 'referece_logps/rejected': -340.09454345703125, 'referece_logps/chosen': -337.7903137207031, 'logits/rejected': -0.6936010718345642, 'logits/chosen': -0.6688668131828308, 'epoch': 5.27}

 88%|████████▊ | 9429/10740 [46:42:51<6:45:31, 18.56s/it]

 88%|████████▊ | 9430/10740 [46:43:08<6:35:55, 18.13s/it]

 88%|████████▊ | 9431/10740 [46:43:25<6:25:39, 17.68s/it]


 88%|████████▊ | 9433/10740 [46:44:00<6:23:05, 17.59s/it]

 88%|████████▊ | 9434/10740 [46:44:18<6:22:48, 17.59s/it]
{'loss': 0.1422, 'learning_rate': 7.656831554172915e-08, 'rewards/chosen': -4.934021472930908, 'rewards/rejected': -9.725541114807129, 'rewards/accuracies': 1.0, 'rewards/margins': 4.7915191650390625, 'policy_logps/rejected': -610.8067626953125, 'policy_logps/chosen': -497.2773742675781, 'referece_logps/rejected': -513.5513305664062, 'referece_logps/chosen': -447.93719482421875, 'logits/rejected': -0.04716840013861656, 'logits/chosen': 0.024324089288711548, 'epoch': 5.27}

 88%|████████▊ | 9435/10740 [46:44:36<6:30:41, 17.96s/it]


 88%|████████▊ | 9437/10740 [46:45:17<6:59:32, 19.32s/it]

 88%|████████▊ | 9438/10740 [46:45:33<6:36:44, 18.28s/it]

 88%|████████▊ | 9439/10740 [46:45:52<6:37:17, 18.32s/it]
{'loss': 0.157, 'learning_rate': 7.599068339101267e-08, 'rewards/chosen': -2.305898904800415, 'rewards/rejected': -5.264364719390869, 'rewards/accuracies': 1.0, 'rewards/margins': 2.958466053009033, 'policy_logps/rejected': -327.74468994140625, 'policy_logps/chosen': -419.46966552734375, 'referece_logps/rejected': -275.1010437011719, 'referece_logps/chosen': -396.4106750488281, 'logits/rejected': 0.31886616349220276, 'logits/chosen': 0.09374167770147324, 'epoch': 5.27}


 88%|████████▊ | 9441/10740 [46:46:32<6:55:07, 19.17s/it]

 88%|████████▊ | 9442/10740 [46:46:50<6:45:02, 18.72s/it]

 88%|████████▊ | 9443/10740 [46:47:10<6:55:24, 19.22s/it]

 88%|████████▊ | 9444/10740 [46:47:24<6:22:03, 17.69s/it]
{'loss': 0.3228, 'learning_rate': 7.541515226274887e-08, 'rewards/chosen': -5.748740196228027, 'rewards/rejected': -9.705155372619629, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9564151763916016, 'policy_logps/rejected': -398.3009948730469, 'policy_logps/chosen': -449.06414794921875, 'referece_logps/rejected': -301.24945068359375, 'referece_logps/chosen': -391.5767822265625, 'logits/rejected': -0.07024028897285461, 'logits/chosen': -0.10084905475378036, 'epoch': 5.28}


 88%|████████▊ | 9446/10740 [46:47:58<6:13:26, 17.32s/it]

 88%|████████▊ | 9447/10740 [46:48:20<6:42:02, 18.66s/it]

 88%|████████▊ | 9448/10740 [46:48:42<7:05:13, 19.75s/it]

 88%|████████▊ | 9449/10740 [46:49:02<7:07:34, 19.87s/it]

 88%|████████▊ | 9450/10740 [46:49:22<7:05:43, 19.80s/it]

 88%|████████▊ | 9451/10740 [46:49:42<7:06:33, 19.86s/it]
{'loss': 0.1994, 'learning_rate': 7.461294089165838e-08, 'rewards/chosen': -3.017291307449341, 'rewards/rejected': -6.071706295013428, 'rewards/accuracies': 1.0, 'rewards/margins': 3.054414749145508, 'policy_logps/rejected': -383.4826354980469, 'policy_logps/chosen': -342.3847351074219, 'referece_logps/rejected': -322.76556396484375, 'referece_logps/chosen': -312.2118225097656, 'logits/rejected': 0.25253573060035706, 'logits/chosen': 0.2178618460893631, 'epoch': 5.28}

 88%|████████▊ | 9452/10740 [46:49:59<6:51:54, 19.19s/it]

 88%|████████▊ | 9453/10740 [46:50:17<6:44:22, 18.85s/it]

 88%|████████▊ | 9454/10740 [46:50:38<6:52:45, 19.26s/it]

 88%|████████▊ | 9455/10740 [46:50:58<6:57:25, 19.49s/it]

 88%|████████▊ | 9456/10740 [46:51:17<6:58:02, 19.53s/it]

 88%|████████▊ | 9457/10740 [46:51:35<6:48:32, 19.11s/it]

 88%|████████▊ | 9458/10740 [46:51:51<6:22:58, 17.92s/it]

 88%|████████▊ | 9459/10740 [46:52:04<5:50:23, 16.41s/it]


 88%|████████▊ | 9461/10740 [46:52:43<6:28:19, 18.22s/it]

 88%|████████▊ | 9462/10740 [46:53:04<6:48:39, 19.19s/it]

 88%|████████▊ | 9463/10740 [46:53:18<6:16:27, 17.69s/it]
{'loss': 0.0818, 'learning_rate': 7.324731831520281e-08, 'rewards/chosen': -3.575584888458252, 'rewards/rejected': -9.450074195861816, 'rewards/accuracies': 1.0, 'rewards/margins': 5.874488830566406, 'policy_logps/rejected': -357.0269470214844, 'policy_logps/chosen': -406.86456298828125, 'referece_logps/rejected': -262.52618408203125, 'referece_logps/chosen': -371.1086730957031, 'logits/rejected': 0.0004154220223426819, 'logits/chosen': -0.2531792223453522, 'epoch': 5.29}

 88%|████████▊ | 9464/10740 [46:53:38<6:28:12, 18.25s/it]

 88%|████████▊ | 9465/10740 [46:54:00<6:51:21, 19.36s/it]

 88%|████████▊ | 9466/10740 [46:54:17<6:39:03, 18.79s/it]

 88%|████████▊ | 9467/10740 [46:54:37<6:45:01, 19.09s/it]

 88%|████████▊ | 9468/10740 [46:54:54<6:30:13, 18.41s/it]

 88%|████████▊ | 9469/10740 [46:55:06<5:49:25, 16.50s/it]

 88%|████████▊ | 9470/10740 [46:55:21<5:39:24, 16.03s/it]


 88%|████████▊ | 9472/10740 [46:56:01<6:19:55, 17.98s/it]
{'loss': 0.1784, 'learning_rate': 7.223106584763383e-08, 'rewards/chosen': -3.10831618309021, 'rewards/rejected': -7.509262561798096, 'rewards/accuracies': 1.0, 'rewards/margins': 4.400946617126465, 'policy_logps/rejected': -532.7760620117188, 'policy_logps/chosen': -422.99310302734375, 'referece_logps/rejected': -457.6834411621094, 'referece_logps/chosen': -391.909912109375, 'logits/rejected': -0.17863181233406067, 'logits/chosen': -0.10224753618240356, 'epoch': 5.29}

 88%|████████▊ | 9473/10740 [46:56:17<6:12:34, 17.64s/it]


 88%|████████▊ | 9475/10740 [46:56:45<5:28:41, 15.59s/it]
{'loss': 0.2266, 'learning_rate': 7.189383354739475e-08, 'rewards/chosen': -3.065286159515381, 'rewards/rejected': -7.511057376861572, 'rewards/accuracies': 0.75, 'rewards/margins': 4.445771217346191, 'policy_logps/rejected': -277.4435729980469, 'policy_logps/chosen': -261.90179443359375, 'referece_logps/rejected': -202.33299255371094, 'referece_logps/chosen': -231.24893188476562, 'logits/rejected': -0.36044561862945557, 'logits/chosen': -0.40395739674568176, 'epoch': 5.29}

 88%|████████▊ | 9476/10740 [46:57:04<5:50:36, 16.64s/it]


 88%|████████▊ | 9478/10740 [46:57:42<6:20:45, 18.10s/it]
{'loss': 0.1102, 'learning_rate': 7.155736096889687e-08, 'rewards/chosen': -4.050980567932129, 'rewards/rejected': -8.326773643493652, 'rewards/accuracies': 1.0, 'rewards/margins': 4.275793075561523, 'policy_logps/rejected': -479.3807373046875, 'policy_logps/chosen': -426.0602722167969, 'referece_logps/rejected': -396.113037109375, 'referece_logps/chosen': -385.55047607421875, 'logits/rejected': -0.15612995624542236, 'logits/chosen': -0.2568318247795105, 'epoch': 5.29}

 88%|████████▊ | 9479/10740 [46:58:02<6:29:53, 18.55s/it]

 88%|████████▊ | 9480/10740 [46:58:16<5:57:36, 17.03s/it]

 88%|████████▊ | 9481/10740 [46:58:36<6:18:22, 18.03s/it]


 88%|████████▊ | 9483/10740 [46:59:07<5:42:29, 16.35s/it]

 88%|████████▊ | 9484/10740 [46:59:25<5:53:25, 16.88s/it]
{'loss': 0.0984, 'learning_rate': 7.088669607821185e-08, 'rewards/chosen': -3.9347968101501465, 'rewards/rejected': -8.787426948547363, 'rewards/accuracies': 1.0, 'rewards/margins': 4.852630138397217, 'policy_logps/rejected': -398.0527038574219, 'policy_logps/chosen': -463.1443786621094, 'referece_logps/rejected': -310.178466796875, 'referece_logps/chosen': -423.79644775390625, 'logits/rejected': 0.683854341506958, 'logits/chosen': 0.5169451236724854, 'epoch': 5.3}

 88%|████████▊ | 9485/10740 [46:59:44<6:05:51, 17.49s/it]


 88%|████████▊ | 9487/10740 [47:00:23<6:25:36, 18.46s/it]
{'loss': 0.1317, 'learning_rate': 7.055250431501203e-08, 'rewards/chosen': -2.6925342082977295, 'rewards/rejected': -7.013323783874512, 'rewards/accuracies': 0.875, 'rewards/margins': 4.320789337158203, 'policy_logps/rejected': -354.9141540527344, 'policy_logps/chosen': -321.8809509277344, 'referece_logps/rejected': -284.7809143066406, 'referece_logps/chosen': -294.9555969238281, 'logits/rejected': -0.8262028694152832, 'logits/chosen': -0.7832369208335876, 'epoch': 5.3}


 88%|████████▊ | 9489/10740 [47:01:05<6:53:21, 19.83s/it]

 88%|████████▊ | 9490/10740 [47:01:25<6:52:59, 19.82s/it]
{'loss': 0.1723, 'learning_rate': 7.021907337152799e-08, 'rewards/chosen': -3.5495779514312744, 'rewards/rejected': -7.004190444946289, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4546127319335938, 'policy_logps/rejected': -601.3394165039062, 'policy_logps/chosen': -556.9133911132812, 'referece_logps/rejected': -531.2975463867188, 'referece_logps/chosen': -521.4176635742188, 'logits/rejected': 1.3201664686203003, 'logits/chosen': 1.429683804512024, 'epoch': 5.3}


 88%|████████▊ | 9492/10740 [47:02:05<6:54:59, 19.95s/it]

 88%|████████▊ | 9493/10740 [47:02:23<6:41:33, 19.32s/it]

 88%|████████▊ | 9494/10740 [47:02:43<6:43:36, 19.44s/it]
{'loss': 0.1477, 'learning_rate': 6.977568274920598e-08, 'rewards/chosen': -3.722315549850464, 'rewards/rejected': -8.054498672485352, 'rewards/accuracies': 0.875, 'rewards/margins': 4.33218240737915, 'policy_logps/rejected': -384.40667724609375, 'policy_logps/chosen': -313.93194580078125, 'referece_logps/rejected': -303.8616638183594, 'referece_logps/chosen': -276.70880126953125, 'logits/rejected': 0.260531485080719, 'logits/chosen': 0.2169044464826584, 'epoch': 5.3}


 88%|████████▊ | 9496/10740 [47:03:13<5:59:38, 17.35s/it]
{'loss': 0.2498, 'learning_rate': 6.955449503483457e-08, 'rewards/chosen': -2.9235360622406006, 'rewards/rejected': -9.167537689208984, 'rewards/accuracies': 0.875, 'rewards/margins': 6.2440009117126465, 'policy_logps/rejected': -450.0632629394531, 'policy_logps/chosen': -295.5064392089844, 'referece_logps/rejected': -358.38787841796875, 'referece_logps/chosen': -266.2710876464844, 'logits/rejected': -0.6704157590866089, 'logits/chosen': -0.600124716758728, 'epoch': 5.31}

 88%|████████▊ | 9497/10740 [47:03:32<6:13:49, 18.04s/it]


 88%|████████▊ | 9499/10740 [47:04:09<6:13:10, 18.04s/it]
{'loss': 0.2267, 'learning_rate': 6.922334818563003e-08, 'rewards/chosen': -3.499568462371826, 'rewards/rejected': -7.642495155334473, 'rewards/accuracies': 0.875, 'rewards/margins': 4.142927169799805, 'policy_logps/rejected': -443.3331604003906, 'policy_logps/chosen': -332.8001708984375, 'referece_logps/rejected': -366.9082336425781, 'referece_logps/chosen': -297.80450439453125, 'logits/rejected': 0.07193397730588913, 'logits/chosen': 0.07651934027671814, 'epoch': 5.31}

 88%|████████▊ | 9500/10740 [47:04:28<6:20:07, 18.39s/it]

 88%|████████▊ | 9501/10740 [47:04:58<7:33:06, 21.94s/it]

 88%|████████▊ | 9502/10740 [47:05:20<7:33:21, 21.97s/it]

 88%|████████▊ | 9503/10740 [47:05:40<7:19:12, 21.30s/it]

 88%|████████▊ | 9504/10740 [47:06:00<7:08:12, 20.79s/it]

 89%|████████▊ | 9505/10740 [47:06:20<7:02:13, 20.51s/it]

 89%|████████▊ | 9506/10740 [47:06:33<6:15:35, 18.26s/it]


 89%|████████▊ | 9508/10740 [47:07:07<6:06:40, 17.86s/it]

 89%|████████▊ | 9509/10740 [47:07:27<6:19:31, 18.50s/it]
{'loss': 0.2819, 'learning_rate': 6.812502953985022e-08, 'rewards/chosen': -2.9587371349334717, 'rewards/rejected': -6.1660261154174805, 'rewards/accuracies': 0.875, 'rewards/margins': 3.207289218902588, 'policy_logps/rejected': -319.8863220214844, 'policy_logps/chosen': -304.9300842285156, 'referece_logps/rejected': -258.22607421875, 'referece_logps/chosen': -275.3426818847656, 'logits/rejected': 0.06784579902887344, 'logits/chosen': -0.013850905001163483, 'epoch': 5.31}

 89%|████████▊ | 9510/10740 [47:07:41<5:51:21, 17.14s/it]

 89%|████████▊ | 9511/10740 [47:08:01<6:07:16, 17.93s/it]

 89%|████████▊ | 9512/10740 [47:08:19<6:08:03, 17.98s/it]

 89%|████████▊ | 9513/10740 [47:08:35<5:54:38, 17.34s/it]

 89%|████████▊ | 9514/10740 [47:08:54<6:06:06, 17.92s/it]


 89%|████████▊ | 9516/10740 [47:09:27<5:57:52, 17.54s/it]
{'loss': 0.1786, 'learning_rate': 6.736124889341988e-08, 'rewards/chosen': -3.9554576873779297, 'rewards/rejected': -7.414793014526367, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4593346118927, 'policy_logps/rejected': -373.03564453125, 'policy_logps/chosen': -394.30120849609375, 'referece_logps/rejected': -298.8876953125, 'referece_logps/chosen': -354.74658203125, 'logits/rejected': 0.2018410563468933, 'logits/chosen': 0.010066449642181396, 'epoch': 5.32}

 89%|████████▊ | 9517/10740 [47:09:40<5:27:56, 16.09s/it]


 89%|████████▊ | 9519/10740 [47:10:14<5:28:59, 16.17s/it]
{'loss': 0.1671, 'learning_rate': 6.7035186519102e-08, 'rewards/chosen': -3.4637625217437744, 'rewards/rejected': -8.634967803955078, 'rewards/accuracies': 1.0, 'rewards/margins': 5.171205520629883, 'policy_logps/rejected': -372.8201904296875, 'policy_logps/chosen': -357.45989990234375, 'referece_logps/rejected': -286.4704895019531, 'referece_logps/chosen': -322.822265625, 'logits/rejected': -0.21161048114299774, 'logits/chosen': -0.07499279081821442, 'epoch': 5.32}

 89%|████████▊ | 9520/10740 [47:10:34<5:55:51, 17.50s/it]

 89%|████████▊ | 9521/10740 [47:10:49<5:37:26, 16.61s/it]

 89%|████████▊ | 9522/10740 [47:11:08<5:55:11, 17.50s/it]


 89%|████████▊ | 9524/10740 [47:11:48<6:16:35, 18.58s/it]
{'loss': 0.1941, 'learning_rate': 6.6493446468748e-08, 'rewards/chosen': -4.897188186645508, 'rewards/rejected': -8.36290168762207, 'rewards/accuracies': 0.875, 'rewards/margins': 3.465712547302246, 'policy_logps/rejected': -408.75640869140625, 'policy_logps/chosen': -425.95343017578125, 'referece_logps/rejected': -325.1274108886719, 'referece_logps/chosen': -376.98162841796875, 'logits/rejected': 0.08233107626438141, 'logits/chosen': 0.012599870562553406, 'epoch': 5.32}

 89%|████████▊ | 9525/10740 [47:12:03<5:56:21, 17.60s/it]

 89%|████████▊ | 9526/10740 [47:12:23<6:11:44, 18.37s/it]

 89%|████████▊ | 9527/10740 [47:12:43<6:19:09, 18.76s/it]

 89%|████████▊ | 9528/10740 [47:12:59<6:03:33, 18.00s/it]

 89%|████████▊ | 9529/10740 [47:13:19<6:12:58, 18.48s/it]

 89%|████████▊ | 9530/10740 [47:13:36<6:06:40, 18.18s/it]


 89%|████████▉ | 9532/10740 [47:14:02<5:11:44, 15.48s/it]
{'loss': 0.1932, 'learning_rate': 6.563107794294675e-08, 'rewards/chosen': -4.742188453674316, 'rewards/rejected': -8.935338020324707, 'rewards/accuracies': 1.0, 'rewards/margins': 4.193150043487549, 'policy_logps/rejected': -439.29278564453125, 'policy_logps/chosen': -327.1606140136719, 'referece_logps/rejected': -349.9394226074219, 'referece_logps/chosen': -279.73876953125, 'logits/rejected': -0.5533016920089722, 'logits/chosen': -0.3838452994823456, 'epoch': 5.33}

 89%|████████▉ | 9533/10740 [47:14:18<5:17:37, 15.79s/it]

 89%|████████▉ | 9534/10740 [47:14:40<5:50:11, 17.42s/it]

 89%|████████▉ | 9535/10740 [47:14:57<5:47:22, 17.30s/it]

 89%|████████▉ | 9536/10740 [47:15:18<6:10:27, 18.46s/it]

 89%|████████▉ | 9537/10740 [47:15:38<6:18:37, 18.88s/it]

 89%|████████▉ | 9538/10740 [47:15:58<6:24:06, 19.17s/it]

 89%|████████▉ | 9539/10740 [47:16:17<6:28:10, 19.39s/it]

 89%|████████▉ | 9540/10740 [47:16:37<6:29:10, 19.46s/it]

 89%|████████▉ | 9541/10740 [47:16:55<6:22:16, 19.13s/it]

 89%|████████▉ | 9542/10740 [47:17:17<6:35:20, 19.80s/it]

 89%|████████▉ | 9543/10740 [47:17:36<6:33:40, 19.73s/it]

 89%|████████▉ | 9544/10740 [47:17:56<6:34:20, 19.78s/it]

 89%|████████▉ | 9545/10740 [47:18:15<6:26:59, 19.43s/it]

 89%|████████▉ | 9546/10740 [47:18:34<6:25:03, 19.35s/it]

 89%|████████▉ | 9547/10740 [47:18:47<5:49:28, 17.58s/it]

 89%|████████▉ | 9548/10740 [47:19:06<5:54:55, 17.87s/it]

 89%|████████▉ | 9549/10740 [47:19:28<6:20:06, 19.15s/it]

 89%|████████▉ | 9550/10740 [47:19:40<5:38:59, 17.09s/it]

 89%|████████▉ | 9551/10740 [47:19:52<5:05:23, 15.41s/it]

 89%|████████▉ | 9552/10740 [47:20:09<5:15:13, 15.92s/it]

 89%|████████▉ | 9553/10740 [47:20:20<4:44:49, 14.40s/it]

 89%|████████▉ | 9554/10740 [47:20:31<4:22:30, 13.28s/it]

 89%|████████▉ | 9555/10740 [47:20:47<4:43:31, 14.36s/it]

 89%|████████▉ | 9556/10740 [47:21:07<5:14:18, 15.93s/it]

 89%|████████▉ | 9557/10740 [47:21:27<5:40:04, 17.25s/it]


 89%|████████▉ | 9559/10740 [47:22:08<6:10:55, 18.84s/it]
{'loss': 0.1903, 'learning_rate': 6.276076777586136e-08, 'rewards/chosen': -3.2737531661987305, 'rewards/rejected': -6.473684310913086, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1999309062957764, 'policy_logps/rejected': -326.7191162109375, 'policy_logps/chosen': -346.13311767578125, 'referece_logps/rejected': -261.9822692871094, 'referece_logps/chosen': -313.3956298828125, 'logits/rejected': 0.5528693795204163, 'logits/chosen': 0.5600603222846985, 'epoch': 5.34}

 89%|████████▉ | 9560/10740 [47:22:28<6:14:07, 19.02s/it]

 89%|████████▉ | 9561/10740 [47:22:49<6:24:28, 19.57s/it]

 89%|████████▉ | 9562/10740 [47:23:04<6:00:49, 18.38s/it]

 89%|████████▉ | 9563/10740 [47:23:21<5:49:13, 17.80s/it]

 89%|████████▉ | 9564/10740 [47:23:43<6:13:30, 19.06s/it]

 89%|████████▉ | 9565/10740 [47:24:00<6:01:24, 18.45s/it]

 89%|████████▉ | 9566/10740 [47:24:20<6:08:58, 18.86s/it]

 89%|████████▉ | 9567/10740 [47:24:38<6:05:01, 18.67s/it]

 89%|████████▉ | 9568/10740 [47:25:00<6:23:15, 19.62s/it]

 89%|████████▉ | 9569/10740 [47:25:14<5:50:53, 17.98s/it]

 89%|████████▉ | 9570/10740 [47:25:27<5:20:51, 16.45s/it]

 89%|████████▉ | 9571/10740 [47:25:49<5:53:43, 18.16s/it]

 89%|████████▉ | 9572/10740 [47:26:11<6:15:16, 19.28s/it]

 89%|████████▉ | 9573/10740 [47:26:29<6:10:38, 19.06s/it]

 89%|████████▉ | 9574/10740 [47:26:46<5:54:31, 18.24s/it]

 89%|████████▉ | 9575/10740 [47:27:01<5:35:14, 17.27s/it]

 89%|████████▉ | 9576/10740 [47:27:20<5:48:07, 17.94s/it]

 89%|████████▉ | 9577/10740 [47:27:38<5:50:18, 18.07s/it]

 89%|████████▉ | 9578/10740 [47:27:50<5:14:01, 16.21s/it]

 89%|████████▉ | 9579/10740 [47:28:10<5:34:29, 17.29s/it]

 89%|████████▉ | 9580/10740 [47:28:28<5:38:34, 17.51s/it]

 89%|████████▉ | 9581/10740 [47:28:48<5:52:35, 18.25s/it]

 89%|████████▉ | 9582/10740 [47:29:08<5:59:41, 18.64s/it]

 89%|████████▉ | 9583/10740 [47:29:27<6:06:02, 18.98s/it]

 89%|████████▉ | 9584/10740 [47:29:48<6:11:55, 19.30s/it]

 89%|████████▉ | 9585/10740 [47:30:07<6:13:04, 19.38s/it]

 89%|████████▉ | 9586/10740 [47:30:25<6:02:53, 18.87s/it]

 89%|████████▉ | 9587/10740 [47:30:45<6:08:37, 19.18s/it]

 89%|████████▉ | 9588/10740 [47:31:01<5:53:57, 18.44s/it]

 89%|████████▉ | 9589/10740 [47:31:21<6:02:22, 18.89s/it]

 89%|████████▉ | 9590/10740 [47:31:40<6:03:23, 18.96s/it]

 89%|████████▉ | 9591/10740 [47:32:01<6:11:43, 19.41s/it]

 89%|████████▉ | 9592/10740 [47:32:20<6:11:41, 19.43s/it]

 89%|████████▉ | 9593/10740 [47:32:43<6:28:02, 20.30s/it]

 89%|████████▉ | 9594/10740 [47:33:00<6:11:24, 19.45s/it]

 89%|████████▉ | 9595/10740 [47:33:17<5:57:49, 18.75s/it]

 89%|████████▉ | 9596/10740 [47:33:32<5:36:33, 17.65s/it]

 89%|████████▉ | 9597/10740 [47:33:51<5:41:07, 17.91s/it]

 89%|████████▉ | 9598/10740 [47:34:11<5:51:20, 18.46s/it]

 89%|████████▉ | 9599/10740 [47:34:29<5:49:58, 18.40s/it]

 89%|████████▉ | 9600/10740 [47:34:43<5:24:18, 17.07s/it]

 89%|████████▉ | 9601/10740 [47:34:57<5:05:58, 16.12s/it]

 89%|████████▉ | 9602/10740 [47:35:15<5:20:10, 16.88s/it]

 89%|████████▉ | 9603/10740 [47:35:30<5:07:53, 16.25s/it]

 89%|████████▉ | 9604/10740 [47:35:47<5:09:04, 16.32s/it]

 89%|████████▉ | 9605/10740 [47:36:03<5:10:45, 16.43s/it]

 89%|████████▉ | 9606/10740 [47:36:22<5:25:00, 17.20s/it]

 89%|████████▉ | 9607/10740 [47:36:42<5:37:07, 17.85s/it]


 89%|████████▉ | 9609/10740 [47:37:17<5:37:25, 17.90s/it]
{'loss': 0.2614, 'learning_rate': 5.7609612492098944e-08, 'rewards/chosen': -3.2042486667633057, 'rewards/rejected': -5.1236701011657715, 'rewards/accuracies': 0.75, 'rewards/margins': 1.919421672821045, 'policy_logps/rejected': -391.1879577636719, 'policy_logps/chosen': -375.4019470214844, 'referece_logps/rejected': -339.95123291015625, 'referece_logps/chosen': -343.3594665527344, 'logits/rejected': 0.16464214026927948, 'logits/chosen': 0.2819472849369049, 'epoch': 5.37}

 89%|████████▉ | 9610/10740 [47:37:35<5:36:27, 17.86s/it]

 89%|████████▉ | 9611/10740 [47:37:56<5:51:19, 18.67s/it]

 89%|████████▉ | 9612/10740 [47:38:13<5:41:48, 18.18s/it]

 90%|████████▉ | 9613/10740 [47:38:33<5:51:16, 18.70s/it]

 90%|████████▉ | 9614/10740 [47:38:55<6:09:55, 19.71s/it]

 90%|████████▉ | 9615/10740 [47:39:08<5:34:53, 17.86s/it]

 90%|████████▉ | 9616/10740 [47:39:25<5:29:30, 17.59s/it]

 90%|████████▉ | 9617/10740 [47:39:45<5:40:41, 18.20s/it]

 90%|████████▉ | 9618/10740 [47:40:00<5:21:14, 17.18s/it]

 90%|████████▉ | 9619/10740 [47:40:21<5:44:15, 18.43s/it]

 90%|████████▉ | 9620/10740 [47:40:41<5:52:10, 18.87s/it]

 90%|████████▉ | 9621/10740 [47:40:55<5:26:49, 17.52s/it]

 90%|████████▉ | 9622/10740 [47:41:14<5:32:51, 17.86s/it]

 90%|████████▉ | 9623/10740 [47:41:34<5:43:51, 18.47s/it]

 90%|████████▉ | 9624/10740 [47:41:50<5:29:50, 17.73s/it]

 90%|████████▉ | 9625/10740 [47:42:08<5:34:09, 17.98s/it]

 90%|████████▉ | 9626/10740 [47:42:25<5:23:41, 17.43s/it]

 90%|████████▉ | 9627/10740 [47:42:46<5:43:17, 18.51s/it]

 90%|████████▉ | 9628/10740 [47:43:05<5:49:01, 18.83s/it]

 90%|████████▉ | 9629/10740 [47:43:20<5:27:34, 17.69s/it]


 90%|████████▉ | 9631/10740 [47:44:01<5:56:02, 19.26s/it]

 90%|████████▉ | 9632/10740 [47:44:21<5:57:26, 19.36s/it]

 90%|████████▉ | 9633/10740 [47:44:39<5:50:46, 19.01s/it]
{'loss': 0.1536, 'learning_rate': 5.521309915192718e-08, 'rewards/chosen': -3.567047357559204, 'rewards/rejected': -7.233842372894287, 'rewards/accuracies': 1.0, 'rewards/margins': 3.666795015335083, 'policy_logps/rejected': -489.4127197265625, 'policy_logps/chosen': -361.44317626953125, 'referece_logps/rejected': -417.07427978515625, 'referece_logps/chosen': -325.7727355957031, 'logits/rejected': -0.4605381190776825, 'logits/chosen': -0.33839502930641174, 'epoch': 5.38}


 90%|████████▉ | 9635/10740 [47:45:13<5:35:44, 18.23s/it]

 90%|████████▉ | 9636/10740 [47:45:32<5:38:49, 18.41s/it]
{'loss': 0.3052, 'learning_rate': 5.4917012615486094e-08, 'rewards/chosen': -3.707472801208496, 'rewards/rejected': -6.662107467651367, 'rewards/accuracies': 0.75, 'rewards/margins': 2.954634189605713, 'policy_logps/rejected': -342.8120422363281, 'policy_logps/chosen': -315.024169921875, 'referece_logps/rejected': -276.19097900390625, 'referece_logps/chosen': -277.9494323730469, 'logits/rejected': -0.2331664115190506, 'logits/chosen': -0.19238823652267456, 'epoch': 5.38}

 90%|████████▉ | 9637/10740 [47:45:46<5:17:31, 17.27s/it]


 90%|████████▉ | 9639/10740 [47:46:26<5:40:42, 18.57s/it]

 90%|████████▉ | 9640/10740 [47:46:46<5:48:28, 19.01s/it]

 90%|████████▉ | 9641/10740 [47:47:06<5:52:44, 19.26s/it]

 90%|████████▉ | 9642/10740 [47:47:26<5:55:28, 19.43s/it]

 90%|████████▉ | 9643/10740 [47:47:46<6:03:26, 19.88s/it]

 90%|████████▉ | 9644/10740 [47:48:03<5:45:34, 18.92s/it]

 90%|████████▉ | 9645/10740 [47:48:17<5:15:09, 17.27s/it]
{'loss': 0.1827, 'learning_rate': 5.403339568342158e-08, 'rewards/chosen': -5.040981769561768, 'rewards/rejected': -7.699545383453369, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6585638523101807, 'policy_logps/rejected': -443.5880432128906, 'policy_logps/chosen': -445.2409973144531, 'referece_logps/rejected': -366.5925598144531, 'referece_logps/chosen': -394.8311462402344, 'logits/rejected': -0.14501334726810455, 'logits/chosen': -0.2099965214729309, 'epoch': 5.39}


 90%|████████▉ | 9647/10740 [47:48:56<5:36:25, 18.47s/it]

 90%|████████▉ | 9648/10740 [47:49:15<5:42:16, 18.81s/it]
{'loss': 0.1969, 'learning_rate': 5.374040506882416e-08, 'rewards/chosen': -3.9240708351135254, 'rewards/rejected': -7.0335001945495605, 'rewards/accuracies': 1.0, 'rewards/margins': 3.109428882598877, 'policy_logps/rejected': -337.1200256347656, 'policy_logps/chosen': -316.60162353515625, 'referece_logps/rejected': -266.7850341796875, 'referece_logps/chosen': -277.3609313964844, 'logits/rejected': 0.23595936596393585, 'logits/chosen': 0.2259940505027771, 'epoch': 5.39}

 90%|████████▉ | 9649/10740 [47:49:31<5:21:22, 17.67s/it]

 90%|████████▉ | 9650/10740 [47:49:50<5:32:53, 18.32s/it]

 90%|████████▉ | 9651/10740 [47:50:06<5:20:01, 17.63s/it]

 90%|████████▉ | 9652/10740 [47:50:25<5:22:55, 17.81s/it]


 90%|████████▉ | 9654/10740 [47:51:04<5:38:49, 18.72s/it]
{'loss': 0.1859, 'learning_rate': 5.315674782371948e-08, 'rewards/chosen': -5.434874534606934, 'rewards/rejected': -11.245488166809082, 'rewards/accuracies': 1.0, 'rewards/margins': 5.810614109039307, 'policy_logps/rejected': -535.9379272460938, 'policy_logps/chosen': -391.6680908203125, 'referece_logps/rejected': -423.4830322265625, 'referece_logps/chosen': -337.3193359375, 'logits/rejected': -0.300717830657959, 'logits/chosen': -0.05250418931245804, 'epoch': 5.39}


 90%|████████▉ | 9656/10740 [47:51:46<5:58:46, 19.86s/it]
{'loss': 0.1577, 'learning_rate': 5.296288425614748e-08, 'rewards/chosen': -3.4644346237182617, 'rewards/rejected': -8.03282642364502, 'rewards/accuracies': 1.0, 'rewards/margins': 4.5683913230896, 'policy_logps/rejected': -341.5813293457031, 'policy_logps/chosen': -396.82098388671875, 'referece_logps/rejected': -261.25311279296875, 'referece_logps/chosen': -362.1766357421875, 'logits/rejected': -0.03322094678878784, 'logits/chosen': -0.27428606152534485, 'epoch': 5.39}

 90%|████████▉ | 9657/10740 [47:52:02<5:41:28, 18.92s/it]


 90%|████████▉ | 9659/10740 [47:52:39<5:34:29, 18.57s/it]

 90%|████████▉ | 9660/10740 [47:52:52<5:03:57, 16.89s/it]

 90%|████████▉ | 9661/10740 [47:53:10<5:09:39, 17.22s/it]

 90%|████████▉ | 9662/10740 [47:53:30<5:23:12, 17.99s/it]

 90%|████████▉ | 9663/10740 [47:53:50<5:31:59, 18.50s/it]

 90%|████████▉ | 9664/10740 [47:54:10<5:39:23, 18.93s/it]

 90%|████████▉ | 9665/10740 [47:54:26<5:25:50, 18.19s/it]

 90%|█████████ | 9666/10740 [47:54:38<4:52:41, 16.35s/it]

 90%|█████████ | 9667/10740 [47:54:56<5:01:44, 16.87s/it]

 90%|█████████ | 9668/10740 [47:55:14<5:06:20, 17.15s/it]

 90%|█████████ | 9669/10740 [47:55:34<5:20:29, 17.96s/it]

 90%|█████████ | 9670/10740 [47:55:54<5:32:12, 18.63s/it]

 90%|█████████ | 9671/10740 [47:56:06<4:57:19, 16.69s/it]

 90%|█████████ | 9672/10740 [47:56:28<5:21:44, 18.08s/it]
{'loss': 0.163, 'learning_rate': 5.142438510357805e-08, 'rewards/chosen': -2.7347898483276367, 'rewards/rejected': -7.137109279632568, 'rewards/accuracies': 1.0, 'rewards/margins': 4.402319431304932, 'policy_logps/rejected': -388.46868896484375, 'policy_logps/chosen': -435.5384521484375, 'referece_logps/rejected': -317.0975646972656, 'referece_logps/chosen': -408.1905212402344, 'logits/rejected': -0.3121900260448456, 'logits/chosen': -0.5487068891525269, 'epoch': 5.4}

 90%|█████████ | 9673/10740 [47:56:39<4:44:32, 16.00s/it]


 90%|█████████ | 9675/10740 [47:57:15<5:03:39, 17.11s/it]

 90%|█████████ | 9676/10740 [47:57:36<5:27:27, 18.47s/it]

 90%|█████████ | 9677/10740 [47:57:58<5:43:17, 19.38s/it]
{'loss': 0.1773, 'learning_rate': 5.0948131892198265e-08, 'rewards/chosen': -4.270069599151611, 'rewards/rejected': -6.980515480041504, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7104454040527344, 'policy_logps/rejected': -440.208740234375, 'policy_logps/chosen': -391.03778076171875, 'referece_logps/rejected': -370.4035339355469, 'referece_logps/chosen': -348.33709716796875, 'logits/rejected': -0.06626801192760468, 'logits/chosen': 0.07548204064369202, 'epoch': 5.41}


 90%|█████████ | 9679/10740 [47:58:33<5:33:31, 18.86s/it]

 90%|█████████ | 9680/10740 [47:58:46<4:59:34, 16.96s/it]
{'loss': 0.1337, 'learning_rate': 5.0663415719218175e-08, 'rewards/chosen': -3.6408913135528564, 'rewards/rejected': -6.6621012687683105, 'rewards/accuracies': 1.0, 'rewards/margins': 3.021209955215454, 'policy_logps/rejected': -308.96417236328125, 'policy_logps/chosen': -302.0123291015625, 'referece_logps/rejected': -242.34317016601562, 'referece_logps/chosen': -265.6034240722656, 'logits/rejected': 1.2306519746780396, 'logits/chosen': 1.2295984029769897, 'epoch': 5.41}

 90%|█████████ | 9681/10740 [47:59:03<5:00:14, 17.01s/it]


 90%|█████████ | 9683/10740 [47:59:38<5:01:09, 17.09s/it]

 90%|█████████ | 9684/10740 [48:00:00<5:27:09, 18.59s/it]

 90%|█████████ | 9685/10740 [48:00:14<5:00:42, 17.10s/it]

 90%|█████████ | 9686/10740 [48:00:28<4:44:39, 16.20s/it]

 90%|█████████ | 9687/10740 [48:00:42<4:32:28, 15.53s/it]

 90%|█████████ | 9688/10740 [48:00:54<4:16:10, 14.61s/it]

 90%|█████████ | 9689/10740 [48:01:09<4:13:50, 14.49s/it]

 90%|█████████ | 9690/10740 [48:01:28<4:37:29, 15.86s/it]

 90%|█████████ | 9691/10740 [48:01:46<4:49:07, 16.54s/it]
{'loss': 0.266, 'learning_rate': 4.962610670921641e-08, 'rewards/chosen': -4.83302640914917, 'rewards/rejected': -8.229849815368652, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3968231678009033, 'policy_logps/rejected': -344.37451171875, 'policy_logps/chosen': -341.6259765625, 'referece_logps/rejected': -262.07598876953125, 'referece_logps/chosen': -293.29571533203125, 'logits/rejected': -0.47714608907699585, 'logits/chosen': -0.573340117931366, 'epoch': 5.41}


 90%|█████████ | 9693/10740 [48:02:24<5:10:01, 17.77s/it]

 90%|█████████ | 9694/10740 [48:02:44<5:17:36, 18.22s/it]
{'loss': 0.1929, 'learning_rate': 4.9345018988299794e-08, 'rewards/chosen': -3.7057573795318604, 'rewards/rejected': -6.901617050170898, 'rewards/accuracies': 0.875, 'rewards/margins': 3.19585919380188, 'policy_logps/rejected': -273.4337158203125, 'policy_logps/chosen': -258.5676574707031, 'referece_logps/rejected': -204.41754150390625, 'referece_logps/chosen': -221.51007080078125, 'logits/rejected': -0.018940597772598267, 'logits/chosen': -0.04711952805519104, 'epoch': 5.42}


 90%|█████████ | 9696/10740 [48:03:18<5:13:50, 18.04s/it]
{'loss': 0.2231, 'learning_rate': 4.915805948216201e-08, 'rewards/chosen': -4.1712212562561035, 'rewards/rejected': -5.55557918548584, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3843581676483154, 'policy_logps/rejected': -239.0702667236328, 'policy_logps/chosen': -256.58074951171875, 'referece_logps/rejected': -183.51446533203125, 'referece_logps/chosen': -214.8685302734375, 'logits/rejected': 0.19928854703903198, 'logits/chosen': 0.0207580104470253, 'epoch': 5.42}


 90%|█████████ | 9698/10740 [48:03:48<4:54:08, 16.94s/it]

 90%|█████████ | 9699/10740 [48:04:08<5:07:45, 17.74s/it]
{'loss': 0.1167, 'learning_rate': 4.887826885470936e-08, 'rewards/chosen': -3.3455417156219482, 'rewards/rejected': -8.332979202270508, 'rewards/accuracies': 1.0, 'rewards/margins': 4.987437725067139, 'policy_logps/rejected': -664.1014404296875, 'policy_logps/chosen': -438.91668701171875, 'referece_logps/rejected': -580.7716674804688, 'referece_logps/chosen': -405.4612121582031, 'logits/rejected': -0.42699944972991943, 'logits/chosen': -0.3898511230945587, 'epoch': 5.42}

 90%|█████████ | 9700/10740 [48:04:29<5:26:43, 18.85s/it]


 90%|█████████ | 9702/10740 [48:05:01<5:03:08, 17.52s/it]

 90%|█████████ | 9703/10740 [48:05:20<5:12:26, 18.08s/it]

 90%|█████████ | 9704/10740 [48:05:39<5:15:56, 18.30s/it]

 90%|█████████ | 9705/10740 [48:05:50<4:38:45, 16.16s/it]
{'loss': 0.2783, 'learning_rate': 4.832102351311174e-08, 'rewards/chosen': -4.741496562957764, 'rewards/rejected': -7.977405548095703, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2359089851379395, 'policy_logps/rejected': -544.4097290039062, 'policy_logps/chosen': -568.8194580078125, 'referece_logps/rejected': -464.63568115234375, 'referece_logps/chosen': -521.4044799804688, 'logits/rejected': -0.2862950563430786, 'logits/chosen': -0.4296979606151581, 'epoch': 5.42}


 90%|█████████ | 9707/10740 [48:06:16<4:12:11, 14.65s/it]

 90%|█████████ | 9708/10740 [48:06:37<4:41:59, 16.40s/it]

 90%|█████████ | 9709/10740 [48:06:55<4:49:22, 16.84s/it]

 90%|█████████ | 9710/10740 [48:07:15<5:03:49, 17.70s/it]

 90%|█████████ | 9711/10740 [48:07:33<5:07:04, 17.91s/it]

 90%|█████████ | 9712/10740 [48:07:52<5:14:00, 18.33s/it]

 90%|█████████ | 9713/10740 [48:08:08<5:00:42, 17.57s/it]

 90%|█████████ | 9714/10740 [48:08:27<5:06:17, 17.91s/it]

 90%|█████████ | 9715/10740 [48:08:47<5:19:24, 18.70s/it]

 90%|█████████ | 9716/10740 [48:09:03<5:03:38, 17.79s/it]

 90%|█████████ | 9717/10740 [48:09:23<5:13:32, 18.39s/it]
{'loss': 0.1689, 'learning_rate': 4.72158828557927e-08, 'rewards/chosen': -5.185815334320068, 'rewards/rejected': -7.07516622543335, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8893506526947021, 'policy_logps/rejected': -405.3001708984375, 'policy_logps/chosen': -400.53375244140625, 'referece_logps/rejected': -334.54852294921875, 'referece_logps/chosen': -348.6755676269531, 'logits/rejected': 0.5048718452453613, 'logits/chosen': 0.5681212544441223, 'epoch': 5.43}


 90%|█████████ | 9719/10740 [48:09:55<4:49:28, 17.01s/it]
{'loss': 0.3389, 'learning_rate': 4.7032905569151296e-08, 'rewards/chosen': -5.029102325439453, 'rewards/rejected': -6.943365573883057, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9142634868621826, 'policy_logps/rejected': -303.2673034667969, 'policy_logps/chosen': -514.6500244140625, 'referece_logps/rejected': -233.83363342285156, 'referece_logps/chosen': -464.3590087890625, 'logits/rejected': 0.16942813992500305, 'logits/chosen': -0.25434085726737976, 'epoch': 5.43}


 91%|█████████ | 9721/10740 [48:10:33<5:04:57, 17.96s/it]

 91%|█████████ | 9722/10740 [48:10:51<5:05:46, 18.02s/it]

 91%|█████████ | 9723/10740 [48:11:05<4:43:22, 16.72s/it]

 91%|█████████ | 9724/10740 [48:11:25<5:00:32, 17.75s/it]

 91%|█████████ | 9725/10740 [48:11:43<5:02:21, 17.87s/it]
{'loss': 0.2587, 'learning_rate': 4.648605416748841e-08, 'rewards/chosen': -4.136912822723389, 'rewards/rejected': -5.82492208480835, 'rewards/accuracies': 0.625, 'rewards/margins': 1.68800950050354, 'policy_logps/rejected': -368.0185852050781, 'policy_logps/chosen': -304.162841796875, 'referece_logps/rejected': -309.7693786621094, 'referece_logps/chosen': -262.793701171875, 'logits/rejected': 0.2509189248085022, 'logits/chosen': 0.2342248409986496, 'epoch': 5.43}


 91%|█████████ | 9727/10740 [48:12:16<4:53:37, 17.39s/it]

 91%|█████████ | 9728/10740 [48:12:31<4:43:05, 16.78s/it]

 91%|█████████ | 9729/10740 [48:12:47<4:40:50, 16.67s/it]

 91%|█████████ | 9730/10740 [48:13:04<4:38:46, 16.56s/it]

 91%|█████████ | 9731/10740 [48:13:25<5:02:29, 17.99s/it]

 91%|█████████ | 9732/10740 [48:13:47<5:20:02, 19.05s/it]

 91%|█████████ | 9733/10740 [48:14:03<5:07:17, 18.31s/it]

 91%|█████████ | 9734/10740 [48:14:17<4:43:21, 16.90s/it]
{'loss': 0.1184, 'learning_rate': 4.567163151933839e-08, 'rewards/chosen': -2.811155080795288, 'rewards/rejected': -8.554036140441895, 'rewards/accuracies': 1.0, 'rewards/margins': 5.742880821228027, 'policy_logps/rejected': -400.7390441894531, 'policy_logps/chosen': -306.29388427734375, 'referece_logps/rejected': -315.19866943359375, 'referece_logps/chosen': -278.18231201171875, 'logits/rejected': -1.2106209993362427, 'logits/chosen': -0.998713493347168, 'epoch': 5.44}


 91%|█████████ | 9736/10740 [48:14:53<4:53:19, 17.53s/it]
{'loss': 0.1014, 'learning_rate': 4.5491603281385904e-08, 'rewards/chosen': -3.8859782218933105, 'rewards/rejected': -8.709335327148438, 'rewards/accuracies': 0.875, 'rewards/margins': 4.823357105255127, 'policy_logps/rejected': -317.5660400390625, 'policy_logps/chosen': -449.12139892578125, 'referece_logps/rejected': -230.47267150878906, 'referece_logps/chosen': -410.2615966796875, 'logits/rejected': -0.7991988658905029, 'logits/chosen': -1.2281408309936523, 'epoch': 5.44}


 91%|█████████ | 9738/10740 [48:15:27<4:45:56, 17.12s/it]
{'loss': 0.2626, 'learning_rate': 4.5311922302943673e-08, 'rewards/chosen': -3.891922950744629, 'rewards/rejected': -8.32707691192627, 'rewards/accuracies': 0.875, 'rewards/margins': 4.435154438018799, 'policy_logps/rejected': -439.28302001953125, 'policy_logps/chosen': -511.00311279296875, 'referece_logps/rejected': -356.0122375488281, 'referece_logps/chosen': -472.08392333984375, 'logits/rejected': 0.4743739366531372, 'logits/chosen': 0.23622815310955048, 'epoch': 5.44}

 91%|█████████ | 9739/10740 [48:15:46<4:52:21, 17.52s/it]


 91%|█████████ | 9741/10740 [48:16:19<4:49:31, 17.39s/it]

 91%|█████████ | 9742/10740 [48:16:41<5:07:56, 18.51s/it]

 91%|█████████ | 9743/10740 [48:16:57<4:57:14, 17.89s/it]

 91%|█████████ | 9744/10740 [48:17:19<5:18:45, 19.20s/it]

 91%|█████████ | 9745/10740 [48:17:37<5:10:15, 18.71s/it]
{'loss': 0.2951, 'learning_rate': 4.4685774489673924e-08, 'rewards/chosen': -3.97165846824646, 'rewards/rejected': -5.659432411193848, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6877739429473877, 'policy_logps/rejected': -317.1766052246094, 'policy_logps/chosen': -290.4866027832031, 'referece_logps/rejected': -260.582275390625, 'referece_logps/chosen': -250.77001953125, 'logits/rejected': 0.17351076006889343, 'logits/chosen': 0.17563746869564056, 'epoch': 5.44}

 91%|█████████ | 9746/10740 [48:17:56<5:13:42, 18.94s/it]

 91%|█████████ | 9747/10740 [48:18:16<5:19:01, 19.28s/it]


 91%|█████████ | 9749/10740 [48:18:49<4:52:53, 17.73s/it]

 91%|█████████ | 9750/10740 [48:19:10<5:04:24, 18.45s/it]
{'loss': 0.2179, 'learning_rate': 4.42411325338623e-08, 'rewards/chosen': -4.068603038787842, 'rewards/rejected': -6.285272121429443, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2166686058044434, 'policy_logps/rejected': -250.2606201171875, 'policy_logps/chosen': -355.09722900390625, 'referece_logps/rejected': -187.40789794921875, 'referece_logps/chosen': -314.4111633300781, 'logits/rejected': -0.2585369944572449, 'logits/chosen': -0.3327255845069885, 'epoch': 5.45}

 91%|█████████ | 9751/10740 [48:19:28<5:05:36, 18.54s/it]

 91%|█████████ | 9752/10740 [48:19:47<5:04:14, 18.48s/it]

 91%|█████████ | 9753/10740 [48:20:05<5:01:29, 18.33s/it]


 91%|█████████ | 9755/10740 [48:20:42<5:02:26, 18.42s/it]

 91%|█████████ | 9756/10740 [48:21:01<5:09:15, 18.86s/it]

 91%|█████████ | 9757/10740 [48:21:14<4:35:49, 16.84s/it]
{'loss': 0.1507, 'learning_rate': 4.3622285022334896e-08, 'rewards/chosen': -3.4462177753448486, 'rewards/rejected': -7.351035118103027, 'rewards/accuracies': 0.75, 'rewards/margins': 3.904817581176758, 'policy_logps/rejected': -261.24267578125, 'policy_logps/chosen': -362.2529296875, 'referece_logps/rejected': -187.73231506347656, 'referece_logps/chosen': -327.7907409667969, 'logits/rejected': -0.19308605790138245, 'logits/chosen': -0.29521676898002625, 'epoch': 5.45}

 91%|█████████ | 9758/10740 [48:21:31<4:39:42, 17.09s/it]


 91%|█████████ | 9760/10740 [48:22:03<4:28:45, 16.45s/it]
{'loss': 0.147, 'learning_rate': 4.335836927311232e-08, 'rewards/chosen': -2.475599527359009, 'rewards/rejected': -5.955502986907959, 'rewards/accuracies': 1.0, 'rewards/margins': 3.479902744293213, 'policy_logps/rejected': -411.0624084472656, 'policy_logps/chosen': -319.2411193847656, 'referece_logps/rejected': -351.50738525390625, 'referece_logps/chosen': -294.485107421875, 'logits/rejected': 0.03241633623838425, 'logits/chosen': 0.07470279186964035, 'epoch': 5.45}


 91%|█████████ | 9762/10740 [48:22:40<4:45:23, 17.51s/it]

 91%|█████████ | 9763/10740 [48:23:00<4:56:39, 18.22s/it]
{'loss': 0.288, 'learning_rate': 4.309523660396397e-08, 'rewards/chosen': -4.045755863189697, 'rewards/rejected': -8.229074478149414, 'rewards/accuracies': 0.875, 'rewards/margins': 4.183319091796875, 'policy_logps/rejected': -403.4450988769531, 'policy_logps/chosen': -343.5386657714844, 'referece_logps/rejected': -321.1544189453125, 'referece_logps/chosen': -303.08111572265625, 'logits/rejected': 0.1712949424982071, 'logits/chosen': 0.08188256621360779, 'epoch': 5.45}

 91%|█████████ | 9764/10740 [48:23:21<5:10:16, 19.07s/it]


 91%|█████████ | 9766/10740 [48:23:55<4:57:26, 18.32s/it]

 91%|█████████ | 9767/10740 [48:24:12<4:48:44, 17.81s/it]
{'loss': 0.2898, 'learning_rate': 4.274561154185119e-08, 'rewards/chosen': -5.436089515686035, 'rewards/rejected': -9.030802726745605, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5947134494781494, 'policy_logps/rejected': -499.95367431640625, 'policy_logps/chosen': -399.4208679199219, 'referece_logps/rejected': -409.6456604003906, 'referece_logps/chosen': -345.0599365234375, 'logits/rejected': -0.223575159907341, 'logits/chosen': -0.207383930683136, 'epoch': 5.46}


 91%|█████████ | 9769/10740 [48:24:46<4:43:47, 17.54s/it]

 91%|█████████ | 9770/10740 [48:25:02<4:36:12, 17.09s/it]

 91%|█████████ | 9771/10740 [48:25:22<4:48:46, 17.88s/it]

 91%|█████████ | 9772/10740 [48:25:36<4:31:30, 16.83s/it]

 91%|█████████ | 9773/10740 [48:25:56<4:45:15, 17.70s/it]

 91%|█████████ | 9774/10740 [48:26:16<4:58:45, 18.56s/it]

 91%|█████████ | 9775/10740 [48:26:38<5:15:38, 19.62s/it]

 91%|█████████ | 9776/10740 [48:26:54<4:58:02, 18.55s/it]
{'loss': 0.0885, 'learning_rate': 4.196404920444574e-08, 'rewards/chosen': -3.2959394454956055, 'rewards/rejected': -8.117671012878418, 'rewards/accuracies': 1.0, 'rewards/margins': 4.821732044219971, 'policy_logps/rejected': -321.3900451660156, 'policy_logps/chosen': -267.11834716796875, 'referece_logps/rejected': -240.21331787109375, 'referece_logps/chosen': -234.158935546875, 'logits/rejected': 0.03988194838166237, 'logits/chosen': -0.027797115966677666, 'epoch': 5.46}

 91%|█████████ | 9777/10740 [48:27:09<4:38:13, 17.34s/it]


 91%|█████████ | 9779/10740 [48:27:38<4:09:22, 15.57s/it]
{'loss': 0.1734, 'learning_rate': 4.170509658446342e-08, 'rewards/chosen': -4.401349067687988, 'rewards/rejected': -6.806805610656738, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4054574966430664, 'policy_logps/rejected': -320.4423522949219, 'policy_logps/chosen': -662.6060791015625, 'referece_logps/rejected': -252.37432861328125, 'referece_logps/chosen': -618.5926513671875, 'logits/rejected': -0.1684252768754959, 'logits/chosen': -0.5927877426147461, 'epoch': 5.46}

 91%|█████████ | 9780/10740 [48:27:55<4:14:24, 15.90s/it]

 91%|█████████ | 9781/10740 [48:28:09<4:06:35, 15.43s/it]


 91%|█████████ | 9783/10740 [48:28:44<4:19:17, 16.26s/it]
{'loss': 0.1366, 'learning_rate': 4.1361046690759905e-08, 'rewards/chosen': -3.007888078689575, 'rewards/rejected': -6.791045665740967, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7831573486328125, 'policy_logps/rejected': -281.4692687988281, 'policy_logps/chosen': -345.1007080078125, 'referece_logps/rejected': -213.55882263183594, 'referece_logps/chosen': -315.0218200683594, 'logits/rejected': 0.230470672249794, 'logits/chosen': 0.23508311808109283, 'epoch': 5.47}


 91%|█████████ | 9785/10740 [48:29:20<4:38:21, 17.49s/it]

 91%|█████████ | 9786/10740 [48:29:37<4:31:03, 17.05s/it]

 91%|█████████ | 9787/10740 [48:29:53<4:26:18, 16.77s/it]
{'loss': 0.1753, 'learning_rate': 4.101839184591849e-08, 'rewards/chosen': -4.0760345458984375, 'rewards/rejected': -7.783677577972412, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7076430320739746, 'policy_logps/rejected': -387.96142578125, 'policy_logps/chosen': -499.1193542480469, 'referece_logps/rejected': -310.1246337890625, 'referece_logps/chosen': -458.3589782714844, 'logits/rejected': 0.00013250112533569336, 'logits/chosen': -0.17236079275608063, 'epoch': 5.47}

 91%|█████████ | 9788/10740 [48:30:13<4:42:21, 17.80s/it]

 91%|█████████ | 9789/10740 [48:30:33<4:54:52, 18.60s/it]


 91%|█████████ | 9791/10740 [48:31:05<4:34:34, 17.36s/it]
{'loss': 0.1626, 'learning_rate': 4.067713254858374e-08, 'rewards/chosen': -3.5429816246032715, 'rewards/rejected': -7.886166095733643, 'rewards/accuracies': 0.875, 'rewards/margins': 4.343183994293213, 'policy_logps/rejected': -494.1749267578125, 'policy_logps/chosen': -440.0583190917969, 'referece_logps/rejected': -415.31329345703125, 'referece_logps/chosen': -404.6285095214844, 'logits/rejected': -0.46657848358154297, 'logits/chosen': -0.5175675749778748, 'epoch': 5.47}

 91%|█████████ | 9792/10740 [48:31:24<4:42:26, 17.88s/it]

 91%|█████████ | 9793/10740 [48:31:42<4:42:55, 17.93s/it]


 91%|█████████ | 9795/10740 [48:32:21<4:54:42, 18.71s/it]

 91%|█████████ | 9796/10740 [48:32:40<4:57:20, 18.90s/it]
{'loss': 0.2813, 'learning_rate': 4.0252521671963465e-08, 'rewards/chosen': -5.465811252593994, 'rewards/rejected': -7.9117302894592285, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4459190368652344, 'policy_logps/rejected': -477.9560546875, 'policy_logps/chosen': -450.7491455078125, 'referece_logps/rejected': -398.8387451171875, 'referece_logps/chosen': -396.09100341796875, 'logits/rejected': 0.2563900649547577, 'logits/chosen': 0.23766589164733887, 'epoch': 5.47}

 91%|█████████ | 9797/10740 [48:32:53<4:30:41, 17.22s/it]

 91%|█████████ | 9798/10740 [48:33:13<4:42:08, 17.97s/it]

 91%|█████████ | 9799/10740 [48:33:31<4:43:30, 18.08s/it]


 91%|█████████▏| 9801/10740 [48:34:09<4:46:38, 18.32s/it]

 91%|█████████▏| 9802/10740 [48:34:29<4:53:31, 18.78s/it]
{'loss': 0.1604, 'learning_rate': 3.9745869319875e-08, 'rewards/chosen': -3.094085931777954, 'rewards/rejected': -6.864842891693115, 'rewards/accuracies': 0.875, 'rewards/margins': 3.770756959915161, 'policy_logps/rejected': -430.6558837890625, 'policy_logps/chosen': -347.30810546875, 'referece_logps/rejected': -362.0074768066406, 'referece_logps/chosen': -316.3672790527344, 'logits/rejected': -0.20994797348976135, 'logits/chosen': -0.2601003050804138, 'epoch': 5.48}

 91%|█████████▏| 9803/10740 [48:34:42<4:26:25, 17.06s/it]

 91%|█████████▏| 9804/10740 [48:35:02<4:41:05, 18.02s/it]


 91%|█████████▏| 9806/10740 [48:35:33<4:19:52, 16.69s/it]
{'loss': 0.1313, 'learning_rate': 3.9409847732069236e-08, 'rewards/chosen': -2.7346391677856445, 'rewards/rejected': -8.309993743896484, 'rewards/accuracies': 1.0, 'rewards/margins': 5.575355052947998, 'policy_logps/rejected': -330.58709716796875, 'policy_logps/chosen': -301.4102783203125, 'referece_logps/rejected': -247.48719787597656, 'referece_logps/chosen': -274.0638732910156, 'logits/rejected': -0.03565917909145355, 'logits/chosen': -0.10346774756908417, 'epoch': 5.48}


 91%|█████████▏| 9808/10740 [48:36:07<4:18:01, 16.61s/it]
{'loss': 0.1484, 'learning_rate': 3.9242361115805035e-08, 'rewards/chosen': -3.2766215801239014, 'rewards/rejected': -9.113656044006348, 'rewards/accuracies': 1.0, 'rewards/margins': 5.837035179138184, 'policy_logps/rejected': -311.2885437011719, 'policy_logps/chosen': -485.48876953125, 'referece_logps/rejected': -220.1519775390625, 'referece_logps/chosen': -452.7225646972656, 'logits/rejected': -0.2017221748828888, 'logits/chosen': -0.31207671761512756, 'epoch': 5.48}

 91%|█████████▏| 9809/10740 [48:36:21<4:09:05, 16.05s/it]


 91%|█████████▏| 9811/10740 [48:36:51<4:00:27, 15.53s/it]
{'loss': 0.1695, 'learning_rate': 3.899178658487645e-08, 'rewards/chosen': -4.563060283660889, 'rewards/rejected': -7.138299942016602, 'rewards/accuracies': 0.75, 'rewards/margins': 2.575239658355713, 'policy_logps/rejected': -285.9457092285156, 'policy_logps/chosen': -426.18487548828125, 'referece_logps/rejected': -214.5626983642578, 'referece_logps/chosen': -380.5542907714844, 'logits/rejected': 0.33223599195480347, 'logits/chosen': 0.14066877961158752, 'epoch': 5.48}


 91%|█████████▏| 9813/10740 [48:37:19<3:50:52, 14.94s/it]

 91%|█████████▏| 9814/10740 [48:37:38<4:11:14, 16.28s/it]

 91%|█████████▏| 9815/10740 [48:37:57<4:21:42, 16.98s/it]
{'loss': 0.1441, 'learning_rate': 3.8658910930343124e-08, 'rewards/chosen': -3.231379985809326, 'rewards/rejected': -7.485858917236328, 'rewards/accuracies': 1.0, 'rewards/margins': 4.254478931427002, 'policy_logps/rejected': -379.3114013671875, 'policy_logps/chosen': -225.29299926757812, 'referece_logps/rejected': -304.45281982421875, 'referece_logps/chosen': -192.9792022705078, 'logits/rejected': 0.2959152162075043, 'logits/chosen': 0.1576664298772812, 'epoch': 5.48}

 91%|█████████▏| 9816/10740 [48:38:14<4:21:19, 16.97s/it]

 91%|█████████▏| 9817/10740 [48:38:36<4:44:05, 18.47s/it]

 91%|█████████▏| 9818/10740 [48:38:56<4:49:13, 18.82s/it]

 91%|█████████▏| 9819/10740 [48:39:12<4:38:06, 18.12s/it]

 91%|█████████▏| 9820/10740 [48:39:25<4:15:48, 16.68s/it]

 91%|█████████▏| 9821/10740 [48:39:43<4:21:36, 17.08s/it]

 91%|█████████▏| 9822/10740 [48:39:58<4:10:30, 16.37s/it]

 91%|█████████▏| 9823/10740 [48:40:18<4:26:24, 17.43s/it]

 91%|█████████▏| 9824/10740 [48:40:38<4:35:53, 18.07s/it]


 91%|█████████▏| 9826/10740 [48:41:09<4:19:47, 17.05s/it]
{'loss': 0.181, 'learning_rate': 3.775071782650674e-08, 'rewards/chosen': -3.129059314727783, 'rewards/rejected': -7.850434303283691, 'rewards/accuracies': 1.0, 'rewards/margins': 4.721375465393066, 'policy_logps/rejected': -414.303466796875, 'policy_logps/chosen': -272.95184326171875, 'referece_logps/rejected': -335.79913330078125, 'referece_logps/chosen': -241.66127014160156, 'logits/rejected': -0.5083997249603271, 'logits/chosen': -0.44949308037757874, 'epoch': 5.49}

 91%|█████████▏| 9827/10740 [48:41:23<4:01:51, 15.89s/it]

 92%|█████████▏| 9828/10740 [48:41:42<4:17:01, 16.91s/it]

 92%|█████████▏| 9829/10740 [48:42:00<4:21:20, 17.21s/it]

 92%|█████████▏| 9830/10740 [48:42:14<4:09:41, 16.46s/it]

 92%|█████████▏| 9831/10740 [48:42:32<4:14:58, 16.83s/it]


 92%|█████████▏| 9833/10740 [48:43:01<3:52:06, 15.35s/it]

 92%|█████████▏| 9834/10740 [48:43:17<3:55:40, 15.61s/it]
{'loss': 0.1787, 'learning_rate': 3.7096864617605194e-08, 'rewards/chosen': -5.069474697113037, 'rewards/rejected': -10.472150802612305, 'rewards/accuracies': 1.0, 'rewards/margins': 5.402676582336426, 'policy_logps/rejected': -524.3297729492188, 'policy_logps/chosen': -409.25286865234375, 'referece_logps/rejected': -419.60833740234375, 'referece_logps/chosen': -358.55816650390625, 'logits/rejected': 0.16442859172821045, 'logits/chosen': 0.07628318667411804, 'epoch': 5.49}

 92%|█████████▏| 9835/10740 [48:43:31<3:46:22, 15.01s/it]

 92%|█████████▏| 9836/10740 [48:43:42<3:28:18, 13.83s/it]

 92%|█████████▏| 9837/10740 [48:43:58<3:40:25, 14.65s/it]

 92%|█████████▏| 9838/10740 [48:44:18<4:03:59, 16.23s/it]


 92%|█████████▏| 9840/10740 [48:44:55<4:18:56, 17.26s/it]
{'loss': 0.1448, 'learning_rate': 3.6610152795123364e-08, 'rewards/chosen': -4.997820854187012, 'rewards/rejected': -9.49433708190918, 'rewards/accuracies': 0.75, 'rewards/margins': 4.496516704559326, 'policy_logps/rejected': -392.60223388671875, 'policy_logps/chosen': -329.5920104980469, 'referece_logps/rejected': -297.6588439941406, 'referece_logps/chosen': -279.61376953125, 'logits/rejected': -0.4384615123271942, 'logits/chosen': -0.28674840927124023, 'epoch': 5.5}

 92%|█████████▏| 9841/10740 [48:45:11<4:12:00, 16.82s/it]

 92%|█████████▏| 9842/10740 [48:45:31<4:26:05, 17.78s/it]


 92%|█████████▏| 9844/10740 [48:46:07<4:26:20, 17.84s/it]

 92%|█████████▏| 9845/10740 [48:46:27<4:36:14, 18.52s/it]

 92%|█████████▏| 9846/10740 [48:46:45<4:32:22, 18.28s/it]
{'loss': 0.1823, 'learning_rate': 3.61265953878972e-08, 'rewards/chosen': -1.7798815965652466, 'rewards/rejected': -5.513976097106934, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7340950965881348, 'policy_logps/rejected': -302.6157531738281, 'policy_logps/chosen': -309.1425476074219, 'referece_logps/rejected': -247.47596740722656, 'referece_logps/chosen': -291.34375, 'logits/rejected': -0.3562498688697815, 'logits/chosen': -0.5312318801879883, 'epoch': 5.5}

 92%|█████████▏| 9847/10740 [48:47:06<4:45:50, 19.21s/it]


 92%|█████████▏| 9849/10740 [48:47:35<4:09:53, 16.83s/it]
{'loss': 0.2776, 'learning_rate': 3.588600008519182e-08, 'rewards/chosen': -3.9681625366210938, 'rewards/rejected': -7.998591423034668, 'rewards/accuracies': 1.0, 'rewards/margins': 4.030428886413574, 'policy_logps/rejected': -292.55889892578125, 'policy_logps/chosen': -368.4916076660156, 'referece_logps/rejected': -212.57296752929688, 'referece_logps/chosen': -328.8099670410156, 'logits/rejected': -0.17258551716804504, 'logits/chosen': -0.3949730694293976, 'epoch': 5.5}

 92%|█████████▏| 9850/10740 [48:47:49<3:55:44, 15.89s/it]

 92%|█████████▏| 9851/10740 [48:48:07<4:04:32, 16.50s/it]


 92%|█████████▏| 9853/10740 [48:48:46<4:27:16, 18.08s/it]

 92%|█████████▏| 9854/10740 [48:49:06<4:35:07, 18.63s/it]
{'loss': 0.1746, 'learning_rate': 3.5486761781528096e-08, 'rewards/chosen': -2.9308834075927734, 'rewards/rejected': -8.244571685791016, 'rewards/accuracies': 0.875, 'rewards/margins': 5.313688278198242, 'policy_logps/rejected': -375.2539978027344, 'policy_logps/chosen': -297.3302307128906, 'referece_logps/rejected': -292.80828857421875, 'referece_logps/chosen': -268.0213623046875, 'logits/rejected': -0.6684198379516602, 'logits/chosen': -0.6652470827102661, 'epoch': 5.51}

 92%|█████████▏| 9855/10740 [48:49:25<4:39:11, 18.93s/it]

 92%|█████████▏| 9856/10740 [48:49:43<4:30:45, 18.38s/it]

 92%|█████████▏| 9857/10740 [48:49:54<4:01:43, 16.42s/it]

 92%|█████████▏| 9858/10740 [48:50:15<4:19:13, 17.63s/it]


 92%|█████████▏| 9860/10740 [48:50:42<3:45:30, 15.38s/it]

 92%|█████████▏| 9861/10740 [48:51:02<4:04:53, 16.72s/it]
{'loss': 0.1006, 'learning_rate': 3.4931512801590015e-08, 'rewards/chosen': -3.920949697494507, 'rewards/rejected': -9.895228385925293, 'rewards/accuracies': 1.0, 'rewards/margins': 5.974278926849365, 'policy_logps/rejected': -409.3309326171875, 'policy_logps/chosen': -422.18829345703125, 'referece_logps/rejected': -310.378662109375, 'referece_logps/chosen': -382.9787902832031, 'logits/rejected': 0.00871896743774414, 'logits/chosen': -0.1263524889945984, 'epoch': 5.51}


 92%|█████████▏| 9863/10740 [48:51:34<4:02:47, 16.61s/it]
{'loss': 0.1637, 'learning_rate': 3.477366010597882e-08, 'rewards/chosen': -3.4300320148468018, 'rewards/rejected': -7.012696743011475, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5826640129089355, 'policy_logps/rejected': -352.4285583496094, 'policy_logps/chosen': -459.2095947265625, 'referece_logps/rejected': -282.3016052246094, 'referece_logps/chosen': -424.90924072265625, 'logits/rejected': -0.21302039921283722, 'logits/chosen': -0.4308224022388458, 'epoch': 5.51}

 92%|█████████▏| 9864/10740 [48:51:52<4:10:35, 17.16s/it]


 92%|█████████▏| 9866/10740 [48:52:24<4:01:11, 16.56s/it]
{'loss': 0.1723, 'learning_rate': 3.4537539503229464e-08, 'rewards/chosen': -4.10430908203125, 'rewards/rejected': -8.186785697937012, 'rewards/accuracies': 1.0, 'rewards/margins': 4.082476615905762, 'policy_logps/rejected': -355.4814453125, 'policy_logps/chosen': -402.9588623046875, 'referece_logps/rejected': -273.6136169433594, 'referece_logps/chosen': -361.915771484375, 'logits/rejected': -0.2273774892091751, 'logits/chosen': -0.41834306716918945, 'epoch': 5.51}


 92%|█████████▏| 9868/10740 [48:52:58<4:05:52, 16.92s/it]
{'loss': 0.1685, 'learning_rate': 3.4380564812024674e-08, 'rewards/chosen': -3.9149599075317383, 'rewards/rejected': -7.81502628326416, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9000658988952637, 'policy_logps/rejected': -277.8551940917969, 'policy_logps/chosen': -283.0925598144531, 'referece_logps/rejected': -199.7049560546875, 'referece_logps/chosen': -243.94300842285156, 'logits/rejected': -0.4768964350223541, 'logits/chosen': -0.6794534921646118, 'epoch': 5.51}

 92%|█████████▏| 9869/10740 [48:53:14<3:59:54, 16.53s/it]

 92%|█████████▏| 9870/10740 [48:53:31<4:03:17, 16.78s/it]

 92%|█████████▏| 9871/10740 [48:53:47<3:57:51, 16.42s/it]

 92%|█████████▏| 9872/10740 [48:54:05<4:05:03, 16.94s/it]

 92%|█████████▏| 9873/10740 [48:54:25<4:20:48, 18.05s/it]


 92%|█████████▏| 9875/10740 [48:54:54<3:56:04, 16.37s/it]
{'loss': 0.1616, 'learning_rate': 3.383392026822463e-08, 'rewards/chosen': -2.933990478515625, 'rewards/rejected': -7.963303565979004, 'rewards/accuracies': 1.0, 'rewards/margins': 5.029313087463379, 'policy_logps/rejected': -447.21661376953125, 'policy_logps/chosen': -378.6292419433594, 'referece_logps/rejected': -367.58355712890625, 'referece_logps/chosen': -349.2893371582031, 'logits/rejected': 0.21383816003799438, 'logits/chosen': 0.12381281703710556, 'epoch': 5.52}

 92%|█████████▏| 9876/10740 [48:55:07<3:42:10, 15.43s/it]

 92%|█████████▏| 9877/10740 [48:55:29<4:07:15, 17.19s/it]


 92%|█████████▏| 9879/10740 [48:55:58<3:53:44, 16.29s/it]
{'loss': 0.2428, 'learning_rate': 3.3523485055147795e-08, 'rewards/chosen': -4.531855583190918, 'rewards/rejected': -7.450441360473633, 'rewards/accuracies': 0.875, 'rewards/margins': 2.918585777282715, 'policy_logps/rejected': -235.0983428955078, 'policy_logps/chosen': -303.3984069824219, 'referece_logps/rejected': -160.59390258789062, 'referece_logps/chosen': -258.079833984375, 'logits/rejected': -0.2397654950618744, 'logits/chosen': -0.2725479304790497, 'epoch': 5.52}

 92%|█████████▏| 9880/10740 [48:56:09<3:29:19, 14.60s/it]

 92%|█████████▏| 9881/10740 [48:56:29<3:52:01, 16.21s/it]

 92%|█████████▏| 9882/10740 [48:56:47<3:59:48, 16.77s/it]

 92%|█████████▏| 9883/10740 [48:57:07<4:11:51, 17.63s/it]

 92%|█████████▏| 9884/10740 [48:57:26<4:19:48, 18.21s/it]

 92%|█████████▏| 9885/10740 [48:57:40<3:59:46, 16.83s/it]

 92%|█████████▏| 9886/10740 [48:57:58<4:05:55, 17.28s/it]

 92%|█████████▏| 9887/10740 [48:58:10<3:41:46, 15.60s/it]

 92%|█████████▏| 9888/10740 [48:58:28<3:53:38, 16.45s/it]

 92%|█████████▏| 9889/10740 [48:58:47<4:04:41, 17.25s/it]

 92%|█████████▏| 9890/10740 [48:59:07<4:14:05, 17.94s/it]

 92%|█████████▏| 9891/10740 [48:59:25<4:16:32, 18.13s/it]


 92%|█████████▏| 9893/10740 [48:59:56<3:51:08, 16.37s/it]

 92%|█████████▏| 9894/10740 [49:00:16<4:06:36, 17.49s/it]
{'loss': 0.1367, 'learning_rate': 3.2371882904655536e-08, 'rewards/chosen': -4.397125720977783, 'rewards/rejected': -8.87015438079834, 'rewards/accuracies': 1.0, 'rewards/margins': 4.473029613494873, 'policy_logps/rejected': -474.5162658691406, 'policy_logps/chosen': -448.0358581542969, 'referece_logps/rejected': -385.8147277832031, 'referece_logps/chosen': -404.0646057128906, 'logits/rejected': 0.5051895976066589, 'logits/chosen': 0.522892415523529, 'epoch': 5.53}


 92%|█████████▏| 9896/10740 [49:00:56<4:24:24, 18.80s/it]

 92%|█████████▏| 9897/10740 [49:01:12<4:12:20, 17.96s/it]
{'loss': 0.16, 'learning_rate': 3.2143937943575284e-08, 'rewards/chosen': -4.390705585479736, 'rewards/rejected': -8.273405075073242, 'rewards/accuracies': 1.0, 'rewards/margins': 3.882699489593506, 'policy_logps/rejected': -530.6334838867188, 'policy_logps/chosen': -456.00909423828125, 'referece_logps/rejected': -447.8994445800781, 'referece_logps/chosen': -412.10205078125, 'logits/rejected': 0.21093714237213135, 'logits/chosen': 0.18133927881717682, 'epoch': 5.53}


 92%|█████████▏| 9899/10740 [49:01:48<4:14:22, 18.15s/it]
{'loss': 0.2044, 'learning_rate': 3.1992414769082655e-08, 'rewards/chosen': -6.199568748474121, 'rewards/rejected': -10.121949195861816, 'rewards/accuracies': 1.0, 'rewards/margins': 3.922379970550537, 'policy_logps/rejected': -617.6897583007812, 'policy_logps/chosen': -537.8226928710938, 'referece_logps/rejected': -516.4702758789062, 'referece_logps/chosen': -475.8269958496094, 'logits/rejected': 0.8520861864089966, 'logits/chosen': 0.9949695467948914, 'epoch': 5.53}

 92%|█████████▏| 9900/10740 [49:02:01<3:50:33, 16.47s/it]

 92%|█████████▏| 9901/10740 [49:02:19<3:57:04, 16.95s/it]

 92%|█████████▏| 9902/10740 [49:02:40<4:14:37, 18.23s/it]

 92%|█████████▏| 9903/10740 [49:02:57<4:07:33, 17.75s/it]

 92%|█████████▏| 9904/10740 [49:03:12<3:57:34, 17.05s/it]


 92%|█████████▏| 9906/10740 [49:03:53<4:18:23, 18.59s/it]
{'loss': 0.2167, 'learning_rate': 3.146485736281224e-08, 'rewards/chosen': -2.965648889541626, 'rewards/rejected': -8.926630020141602, 'rewards/accuracies': 0.875, 'rewards/margins': 5.960981369018555, 'policy_logps/rejected': -476.4183349609375, 'policy_logps/chosen': -379.2970886230469, 'referece_logps/rejected': -387.15203857421875, 'referece_logps/chosen': -349.6405944824219, 'logits/rejected': -0.21436269581317902, 'logits/chosen': -0.0711028054356575, 'epoch': 5.53}

 92%|█████████▏| 9907/10740 [49:04:11<4:18:16, 18.60s/it]

 92%|█████████▏| 9908/10740 [49:04:31<4:22:15, 18.91s/it]

 92%|█████████▏| 9909/10740 [49:04:51<4:27:43, 19.33s/it]


 92%|█████████▏| 9911/10740 [49:05:31<4:30:40, 19.59s/it]

 92%|█████████▏| 9912/10740 [49:05:47<4:16:50, 18.61s/it]
{'loss': 0.1734, 'learning_rate': 3.101610074566763e-08, 'rewards/chosen': -2.957392454147339, 'rewards/rejected': -5.665555953979492, 'rewards/accuracies': 1.0, 'rewards/margins': 2.708162784576416, 'policy_logps/rejected': -361.4854736328125, 'policy_logps/chosen': -325.5103759765625, 'referece_logps/rejected': -304.8299255371094, 'referece_logps/chosen': -295.9364318847656, 'logits/rejected': -0.4546037018299103, 'logits/chosen': -0.5352507829666138, 'epoch': 5.54}

 92%|█████████▏| 9913/10740 [49:06:04<4:08:37, 18.04s/it]

 92%|█████████▏| 9914/10740 [49:06:26<4:24:26, 19.21s/it]

 92%|█████████▏| 9915/10740 [49:06:42<4:13:47, 18.46s/it]


 92%|█████████▏| 9917/10740 [49:07:23<4:27:07, 19.47s/it]
{'loss': 0.1766, 'learning_rate': 3.064456045056418e-08, 'rewards/chosen': -4.023040771484375, 'rewards/rejected': -7.060921669006348, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0378806591033936, 'policy_logps/rejected': -332.2081298828125, 'policy_logps/chosen': -409.35516357421875, 'referece_logps/rejected': -261.5989074707031, 'referece_logps/chosen': -369.1247253417969, 'logits/rejected': 0.5678291916847229, 'logits/chosen': 0.38848063349723816, 'epoch': 5.54}

 92%|█████████▏| 9918/10740 [49:07:42<4:27:14, 19.51s/it]


 92%|█████████▏| 9920/10740 [49:08:19<4:12:54, 18.51s/it]
{'loss': 0.1313, 'learning_rate': 3.042269420250409e-08, 'rewards/chosen': -3.315936326980591, 'rewards/rejected': -8.997525215148926, 'rewards/accuracies': 1.0, 'rewards/margins': 5.681589126586914, 'policy_logps/rejected': -367.23626708984375, 'policy_logps/chosen': -376.41632080078125, 'referece_logps/rejected': -277.2610168457031, 'referece_logps/chosen': -343.2569885253906, 'logits/rejected': -0.6490681171417236, 'logits/chosen': -0.8066129088401794, 'epoch': 5.54}

 92%|█████████▏| 9921/10740 [49:08:34<3:56:46, 17.35s/it]

 92%|█████████▏| 9922/10740 [49:08:50<3:54:17, 17.18s/it]

 92%|█████████▏| 9923/10740 [49:09:08<3:57:41, 17.46s/it]

 92%|█████████▏| 9924/10740 [49:09:28<4:07:26, 18.19s/it]

 92%|█████████▏| 9925/10740 [49:09:48<4:13:34, 18.67s/it]

 92%|█████████▏| 9926/10740 [49:10:08<4:16:59, 18.94s/it]

 92%|█████████▏| 9927/10740 [49:10:26<4:14:04, 18.75s/it]

 92%|█████████▏| 9928/10740 [49:10:44<4:08:57, 18.40s/it]


 92%|█████████▏| 9930/10740 [49:11:23<4:18:44, 19.17s/it]
{'loss': 0.1436, 'learning_rate': 2.9688873110459976e-08, 'rewards/chosen': -4.365278720855713, 'rewards/rejected': -6.887380599975586, 'rewards/accuracies': 1.0, 'rewards/margins': 2.522101640701294, 'policy_logps/rejected': -385.8021240234375, 'policy_logps/chosen': -310.36041259765625, 'referece_logps/rejected': -316.9283142089844, 'referece_logps/chosen': -266.7076110839844, 'logits/rejected': -0.0011234432458877563, 'logits/chosen': -0.13267597556114197, 'epoch': 5.55}

 92%|█████████▏| 9931/10740 [49:11:37<3:55:56, 17.50s/it]


 92%|█████████▏| 9933/10740 [49:12:17<4:15:39, 19.01s/it]
{'loss': 0.175, 'learning_rate': 2.9470447397418997e-08, 'rewards/chosen': -2.855526924133301, 'rewards/rejected': -8.001903533935547, 'rewards/accuracies': 1.0, 'rewards/margins': 5.146377086639404, 'policy_logps/rejected': -405.6669616699219, 'policy_logps/chosen': -342.5315856933594, 'referece_logps/rejected': -325.64794921875, 'referece_logps/chosen': -313.976318359375, 'logits/rejected': 0.33394473791122437, 'logits/chosen': 0.28855395317077637, 'epoch': 5.55}

 92%|█████████▏| 9934/10740 [49:12:36<4:12:53, 18.83s/it]

 93%|█████████▎| 9935/10740 [49:12:52<4:00:51, 17.95s/it]

 93%|█████████▎| 9936/10740 [49:13:10<4:03:07, 18.14s/it]


 93%|█████████▎| 9938/10740 [49:13:41<3:42:02, 16.61s/it]
{'loss': 0.1724, 'learning_rate': 2.9108170071103333e-08, 'rewards/chosen': -2.7456250190734863, 'rewards/rejected': -6.482059478759766, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7364342212677, 'policy_logps/rejected': -308.9702453613281, 'policy_logps/chosen': -456.6058349609375, 'referece_logps/rejected': -244.149658203125, 'referece_logps/chosen': -429.1496276855469, 'logits/rejected': 0.9279623031616211, 'logits/chosen': 0.7335070371627808, 'epoch': 5.55}

 93%|█████████▎| 9939/10740 [49:13:55<3:28:29, 15.62s/it]

 93%|█████████▎| 9940/10740 [49:14:07<3:14:06, 14.56s/it]


 93%|█████████▎| 9942/10740 [49:14:35<3:07:50, 14.12s/it]
{'loss': 0.1731, 'learning_rate': 2.8819937660263694e-08, 'rewards/chosen': -3.555232286453247, 'rewards/rejected': -8.226280212402344, 'rewards/accuracies': 1.0, 'rewards/margins': 4.671047210693359, 'policy_logps/rejected': -460.0906982421875, 'policy_logps/chosen': -579.2854614257812, 'referece_logps/rejected': -377.82794189453125, 'referece_logps/chosen': -543.733154296875, 'logits/rejected': -0.648507833480835, 'logits/chosen': -0.7734172940254211, 'epoch': 5.55}

 93%|█████████▎| 9943/10740 [49:14:55<3:28:18, 15.68s/it]


 93%|█████████▎| 9945/10740 [49:15:23<3:19:01, 15.02s/it]
{'loss': 0.0996, 'learning_rate': 2.860469080685568e-08, 'rewards/chosen': -3.1986565589904785, 'rewards/rejected': -11.854046821594238, 'rewards/accuracies': 1.0, 'rewards/margins': 8.655390739440918, 'policy_logps/rejected': -553.4674072265625, 'policy_logps/chosen': -404.59173583984375, 'referece_logps/rejected': -434.9268798828125, 'referece_logps/chosen': -372.605224609375, 'logits/rejected': -0.37314194440841675, 'logits/chosen': -0.22289708256721497, 'epoch': 5.56}

 93%|█████████▎| 9946/10740 [49:15:38<3:18:55, 15.03s/it]

 93%|█████████▎| 9947/10740 [49:15:59<3:38:41, 16.55s/it]


 93%|█████████▎| 9949/10740 [49:16:38<3:57:42, 18.03s/it]
{'loss': 0.275, 'learning_rate': 2.831893194359536e-08, 'rewards/chosen': -5.126198768615723, 'rewards/rejected': -9.005575180053711, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8793773651123047, 'policy_logps/rejected': -453.041259765625, 'policy_logps/chosen': -380.56085205078125, 'referece_logps/rejected': -362.9855041503906, 'referece_logps/chosen': -329.29888916015625, 'logits/rejected': 0.00478256493806839, 'logits/chosen': 0.13522526621818542, 'epoch': 5.56}


 93%|█████████▎| 9951/10740 [49:17:18<4:10:42, 19.07s/it]
{'loss': 0.3436, 'learning_rate': 2.8176582746651358e-08, 'rewards/chosen': -5.349709510803223, 'rewards/rejected': -8.466054916381836, 'rewards/accuracies': 0.75, 'rewards/margins': 3.1163454055786133, 'policy_logps/rejected': -468.3144226074219, 'policy_logps/chosen': -543.2576904296875, 'referece_logps/rejected': -383.65380859375, 'referece_logps/chosen': -489.7605895996094, 'logits/rejected': 0.21342453360557556, 'logits/chosen': 0.02734389901161194, 'epoch': 5.56}

 93%|█████████▎| 9952/10740 [49:17:33<3:54:20, 17.84s/it]

 93%|█████████▎| 9953/10740 [49:17:53<4:03:19, 18.55s/it]

 93%|█████████▎| 9954/10740 [49:18:13<4:08:15, 18.95s/it]

 93%|█████████▎| 9955/10740 [49:18:28<3:53:54, 17.88s/it]

 93%|█████████▎| 9956/10740 [49:18:49<4:03:55, 18.67s/it]

 93%|█████████▎| 9957/10740 [49:19:08<4:07:24, 18.96s/it]

 93%|█████████▎| 9958/10740 [49:19:28<4:10:22, 19.21s/it]

 93%|█████████▎| 9959/10740 [49:19:42<3:49:07, 17.60s/it]

 93%|█████████▎| 9960/10740 [49:20:04<4:05:25, 18.88s/it]

 93%|█████████▎| 9961/10740 [49:20:18<3:47:06, 17.49s/it]

 93%|█████████▎| 9962/10740 [49:20:38<3:57:03, 18.28s/it]

 93%|█████████▎| 9963/10740 [49:20:55<3:49:39, 17.73s/it]

 93%|█████████▎| 9964/10740 [49:21:09<3:35:31, 16.66s/it]

 93%|█████████▎| 9965/10740 [49:21:24<3:28:35, 16.15s/it]

 93%|█████████▎| 9966/10740 [49:21:40<3:28:49, 16.19s/it]

 93%|█████████▎| 9967/10740 [49:21:57<3:33:16, 16.55s/it]

 93%|█████████▎| 9968/10740 [49:22:17<3:42:53, 17.32s/it]

 93%|█████████▎| 9969/10740 [49:22:36<3:52:00, 18.06s/it]

 93%|█████████▎| 9970/10740 [49:22:51<3:37:15, 16.93s/it]

 93%|█████████▎| 9971/10740 [49:23:07<3:35:36, 16.82s/it]

 93%|█████████▎| 9972/10740 [49:23:24<3:34:48, 16.78s/it]

 93%|█████████▎| 9973/10740 [49:23:37<3:21:23, 15.75s/it]

 93%|█████████▎| 9974/10740 [49:23:53<3:21:27, 15.78s/it]

 93%|█████████▎| 9975/10740 [49:24:13<3:36:16, 16.96s/it]

 93%|█████████▎| 9976/10740 [49:24:24<3:12:14, 15.10s/it]

 93%|█████████▎| 9977/10740 [49:24:43<3:27:16, 16.30s/it]

 93%|█████████▎| 9978/10740 [49:25:01<3:32:54, 16.76s/it]

 93%|█████████▎| 9979/10740 [49:25:19<3:38:31, 17.23s/it]

 93%|█████████▎| 9980/10740 [49:25:33<3:25:07, 16.19s/it]

 93%|█████████▎| 9981/10740 [49:25:52<3:36:41, 17.13s/it]

 93%|█████████▎| 9982/10740 [49:26:13<3:49:39, 18.18s/it]

 93%|█████████▎| 9983/10740 [49:26:24<3:25:44, 16.31s/it]

 93%|█████████▎| 9984/10740 [49:26:40<3:23:10, 16.12s/it]

 93%|█████████▎| 9985/10740 [49:26:59<3:31:31, 16.81s/it]

 93%|█████████▎| 9986/10740 [49:27:14<3:25:37, 16.36s/it]

 93%|█████████▎| 9987/10740 [49:27:35<3:42:27, 17.73s/it]

 93%|█████████▎| 9988/10740 [49:27:55<3:50:01, 18.35s/it]

 93%|█████████▎| 9989/10740 [49:28:10<3:39:27, 17.53s/it]

 93%|█████████▎| 9990/10740 [49:28:27<3:35:26, 17.24s/it]

 93%|█████████▎| 9991/10740 [49:28:42<3:27:17, 16.61s/it]

 93%|█████████▎| 9992/10740 [49:29:01<3:36:45, 17.39s/it]

 93%|█████████▎| 9993/10740 [49:29:21<3:45:12, 18.09s/it]

 93%|█████████▎| 9994/10740 [49:29:41<3:50:46, 18.56s/it]

 93%|█████████▎| 9995/10740 [49:29:57<3:43:45, 18.02s/it]

 93%|█████████▎| 9996/10740 [49:30:18<3:55:18, 18.98s/it]

 93%|█████████▎| 9997/10740 [49:30:38<3:58:23, 19.25s/it]

 93%|█████████▎| 9998/10740 [49:30:58<3:59:28, 19.36s/it]

 93%|█████████▎| 9999/10740 [49:31:18<4:00:00, 19.43s/it]


 93%|█████████▎| 10001/10740 [49:32:08<4:42:42, 22.95s/it]

 93%|█████████▎| 10002/10740 [49:32:25<4:20:12, 21.16s/it]

 93%|█████████▎| 10003/10740 [49:32:39<3:54:00, 19.05s/it]

 93%|█████████▎| 10004/10740 [49:32:59<3:56:31, 19.28s/it]

 93%|█████████▎| 10005/10740 [49:33:19<3:59:16, 19.53s/it]

 93%|█████████▎| 10006/10740 [49:33:36<3:47:00, 18.56s/it]

 93%|█████████▎| 10007/10740 [49:33:56<3:53:45, 19.13s/it]

 93%|█████████▎| 10008/10740 [49:34:16<3:55:44, 19.32s/it]

 93%|█████████▎| 10009/10740 [49:34:37<4:02:24, 19.90s/it]
{'loss': 0.1352, 'learning_rate': 2.4202460358978772e-08, 'rewards/chosen': -4.678605079650879, 'rewards/rejected': -8.610780715942383, 'rewards/accuracies': 1.0, 'rewards/margins': 3.932175874710083, 'policy_logps/rejected': -615.111572265625, 'policy_logps/chosen': -604.8803100585938, 'referece_logps/rejected': -529.003662109375, 'referece_logps/chosen': -558.09423828125, 'logits/rejected': 0.5866614580154419, 'logits/chosen': 0.5291569232940674, 'epoch': 5.59}


 93%|█████████▎| 10011/10740 [49:35:15<3:53:35, 19.23s/it]
{'loss': 0.1944, 'learning_rate': 2.4070739877595227e-08, 'rewards/chosen': -3.144801139831543, 'rewards/rejected': -7.388009071350098, 'rewards/accuracies': 0.875, 'rewards/margins': 4.2432074546813965, 'policy_logps/rejected': -361.12481689453125, 'policy_logps/chosen': -479.5807800292969, 'referece_logps/rejected': -287.2447204589844, 'referece_logps/chosen': -448.13275146484375, 'logits/rejected': -0.3194422423839569, 'logits/chosen': -0.7788633108139038, 'epoch': 5.59}


 93%|█████████▎| 10013/10740 [49:35:56<4:00:23, 19.84s/it]
{'loss': 0.1223, 'learning_rate': 2.3939374448841975e-08, 'rewards/chosen': -2.957859516143799, 'rewards/rejected': -6.549742221832275, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5918829441070557, 'policy_logps/rejected': -400.0164489746094, 'policy_logps/chosen': -306.8336486816406, 'referece_logps/rejected': -334.51898193359375, 'referece_logps/chosen': -277.25506591796875, 'logits/rejected': -0.5047466158866882, 'logits/chosen': -0.44422033429145813, 'epoch': 5.59}


 93%|█████████▎| 10015/10740 [49:36:30<3:40:56, 18.29s/it]

 93%|█████████▎| 10016/10740 [49:36:47<3:35:37, 17.87s/it]

 93%|█████████▎| 10017/10740 [49:37:06<3:40:34, 18.30s/it]

 93%|█████████▎| 10018/10740 [49:37:25<3:41:42, 18.42s/it]

 93%|█████████▎| 10019/10740 [49:37:43<3:39:43, 18.28s/it]
{'loss': 0.1569, 'learning_rate': 2.354740895563878e-08, 'rewards/chosen': -2.5004141330718994, 'rewards/rejected': -6.8156819343566895, 'rewards/accuracies': 1.0, 'rewards/margins': 4.315268039703369, 'policy_logps/rejected': -251.03857421875, 'policy_logps/chosen': -392.5304870605469, 'referece_logps/rejected': -182.8817596435547, 'referece_logps/chosen': -367.5263671875, 'logits/rejected': 0.2571927607059479, 'logits/chosen': -0.13194605708122253, 'epoch': 5.6}


 93%|█████████▎| 10021/10740 [49:38:10<3:06:58, 15.60s/it]

 93%|█████████▎| 10022/10740 [49:38:26<3:08:43, 15.77s/it]
{'loss': 0.1409, 'learning_rate': 2.3352625074145704e-08, 'rewards/chosen': -3.561493396759033, 'rewards/rejected': -6.812692165374756, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2511985301971436, 'policy_logps/rejected': -508.7889404296875, 'policy_logps/chosen': -569.9737548828125, 'referece_logps/rejected': -440.6619873046875, 'referece_logps/chosen': -534.3587646484375, 'logits/rejected': -0.7160767316818237, 'logits/chosen': -0.7636910676956177, 'epoch': 5.6}

 93%|█████████▎| 10023/10740 [49:38:39<2:59:01, 14.98s/it]


 93%|█████████▎| 10025/10740 [49:39:18<3:27:36, 17.42s/it]

 93%|█████████▎| 10026/10740 [49:39:35<3:25:24, 17.26s/it]

 93%|█████████▎| 10027/10740 [49:39:47<3:07:24, 15.77s/it]
{'loss': 0.154, 'learning_rate': 2.3029761919469483e-08, 'rewards/chosen': -2.482363700866699, 'rewards/rejected': -6.9582438468933105, 'rewards/accuracies': 1.0, 'rewards/margins': 4.475879669189453, 'policy_logps/rejected': -310.03167724609375, 'policy_logps/chosen': -376.60980224609375, 'referece_logps/rejected': -240.44924926757812, 'referece_logps/chosen': -351.7861328125, 'logits/rejected': 0.10679453611373901, 'logits/chosen': -0.07815995812416077, 'epoch': 5.6}


 93%|█████████▎| 10029/10740 [49:40:27<3:29:28, 17.68s/it]

 93%|█████████▎| 10030/10740 [49:40:44<3:29:15, 17.68s/it]

 93%|█████████▎| 10031/10740 [49:41:04<3:34:41, 18.17s/it]

 93%|█████████▎| 10032/10740 [49:41:25<3:45:16, 19.09s/it]
{'loss': 0.1946, 'learning_rate': 2.2709120210366416e-08, 'rewards/chosen': -2.20865797996521, 'rewards/rejected': -5.922982692718506, 'rewards/accuracies': 1.0, 'rewards/margins': 3.714324712753296, 'policy_logps/rejected': -313.10772705078125, 'policy_logps/chosen': -503.9076843261719, 'referece_logps/rejected': -253.87789916992188, 'referece_logps/chosen': -481.8210754394531, 'logits/rejected': -0.44800156354904175, 'logits/chosen': -0.7641522884368896, 'epoch': 5.6}


 93%|█████████▎| 10034/10740 [49:42:04<3:46:03, 19.21s/it]

 93%|█████████▎| 10035/10740 [49:42:20<3:35:47, 18.36s/it]

 93%|█████████▎| 10036/10740 [49:42:40<3:39:57, 18.75s/it]
{'loss': 0.1797, 'learning_rate': 2.2454206774007335e-08, 'rewards/chosen': -4.489377975463867, 'rewards/rejected': -7.141122817993164, 'rewards/accuracies': 0.875, 'rewards/margins': 2.651744842529297, 'policy_logps/rejected': -331.9210205078125, 'policy_logps/chosen': -418.3662109375, 'referece_logps/rejected': -260.5097961425781, 'referece_logps/chosen': -373.472412109375, 'logits/rejected': 0.4366849660873413, 'logits/chosen': 0.18855467438697815, 'epoch': 5.61}


 93%|█████████▎| 10038/10740 [49:43:19<3:44:46, 19.21s/it]

 93%|█████████▎| 10039/10740 [49:43:37<3:38:22, 18.69s/it]

 93%|█████████▎| 10040/10740 [49:43:54<3:34:54, 18.42s/it]

 93%|█████████▎| 10041/10740 [49:44:16<3:44:42, 19.29s/it]

 94%|█████████▎| 10042/10740 [49:44:32<3:33:51, 18.38s/it]
{'loss': 0.155, 'learning_rate': 2.2074504040141882e-08, 'rewards/chosen': -2.4640438556671143, 'rewards/rejected': -8.45724868774414, 'rewards/accuracies': 1.0, 'rewards/margins': 5.993204116821289, 'policy_logps/rejected': -489.4558410644531, 'policy_logps/chosen': -418.7524108886719, 'referece_logps/rejected': -404.8833312988281, 'referece_logps/chosen': -394.1119384765625, 'logits/rejected': -0.9871310591697693, 'logits/chosen': -1.2062994241714478, 'epoch': 5.61}


 94%|█████████▎| 10044/10740 [49:45:11<3:41:13, 19.07s/it]
{'loss': 0.2181, 'learning_rate': 2.1948647958681677e-08, 'rewards/chosen': -3.2404632568359375, 'rewards/rejected': -6.1073384284973145, 'rewards/accuracies': 0.875, 'rewards/margins': 2.866875410079956, 'policy_logps/rejected': -321.07757568359375, 'policy_logps/chosen': -369.1514892578125, 'referece_logps/rejected': -260.00421142578125, 'referece_logps/chosen': -336.746826171875, 'logits/rejected': -0.0035878047347068787, 'logits/chosen': -0.15883320569992065, 'epoch': 5.61}


 94%|█████████▎| 10046/10740 [49:45:47<3:37:15, 18.78s/it]
{'loss': 0.0783, 'learning_rate': 2.1823147701889756e-08, 'rewards/chosen': -3.207549571990967, 'rewards/rejected': -8.853075981140137, 'rewards/accuracies': 1.0, 'rewards/margins': 5.645525932312012, 'policy_logps/rejected': -510.1144104003906, 'policy_logps/chosen': -457.284423828125, 'referece_logps/rejected': -421.5836486816406, 'referece_logps/chosen': -425.20892333984375, 'logits/rejected': 0.15842348337173462, 'logits/chosen': 0.015961773693561554, 'epoch': 5.61}

 94%|█████████▎| 10047/10740 [49:46:00<3:14:50, 16.87s/it]


 94%|█████████▎| 10049/10740 [49:46:40<3:33:27, 18.53s/it]

 94%|█████████▎| 10050/10740 [49:47:00<3:38:18, 18.98s/it]

 94%|█████████▎| 10051/10740 [49:47:17<3:33:04, 18.56s/it]
{'loss': 0.1526, 'learning_rate': 2.1510954092146426e-08, 'rewards/chosen': -4.242464542388916, 'rewards/rejected': -9.230955123901367, 'rewards/accuracies': 1.0, 'rewards/margins': 4.988491058349609, 'policy_logps/rejected': -423.36688232421875, 'policy_logps/chosen': -329.2359619140625, 'referece_logps/rejected': -331.0573425292969, 'referece_logps/chosen': -286.81134033203125, 'logits/rejected': -0.16448163986206055, 'logits/chosen': -0.24530169367790222, 'epoch': 5.62}


 94%|█████████▎| 10053/10740 [49:47:57<3:37:34, 19.00s/it]
{'loss': 0.2836, 'learning_rate': 2.1386699580401846e-08, 'rewards/chosen': -3.738818883895874, 'rewards/rejected': -6.498422145843506, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7596042156219482, 'policy_logps/rejected': -397.7933044433594, 'policy_logps/chosen': -423.04718017578125, 'referece_logps/rejected': -332.8090515136719, 'referece_logps/chosen': -385.6589660644531, 'logits/rejected': 0.31405967473983765, 'logits/chosen': 0.3497658967971802, 'epoch': 5.62}

 94%|█████████▎| 10054/10740 [49:48:08<3:12:23, 16.83s/it]


 94%|█████████▎| 10056/10740 [49:48:46<3:20:37, 17.60s/it]

 94%|█████████▎| 10057/10740 [49:48:56<2:57:40, 15.61s/it]
{'loss': 0.146, 'learning_rate': 2.113925868931954e-08, 'rewards/chosen': -3.5286717414855957, 'rewards/rejected': -8.54804515838623, 'rewards/accuracies': 1.0, 'rewards/margins': 5.019372940063477, 'policy_logps/rejected': -419.2446594238281, 'policy_logps/chosen': -444.3432312011719, 'referece_logps/rejected': -333.76422119140625, 'referece_logps/chosen': -409.0565490722656, 'logits/rejected': -0.506689190864563, 'logits/chosen': -0.612768292427063, 'epoch': 5.62}


 94%|█████████▎| 10059/10740 [49:49:33<3:13:00, 17.01s/it]
{'loss': 0.2011, 'learning_rate': 2.101607240000336e-08, 'rewards/chosen': -4.214947700500488, 'rewards/rejected': -7.047759056091309, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8328115940093994, 'policy_logps/rejected': -521.2921142578125, 'policy_logps/chosen': -425.6826171875, 'referece_logps/rejected': -450.81451416015625, 'referece_logps/chosen': -383.53314208984375, 'logits/rejected': -0.9733316898345947, 'logits/chosen': -0.8427530527114868, 'epoch': 5.62}


 94%|█████████▎| 10061/10740 [49:50:09<3:16:29, 17.36s/it]
{'loss': 0.1378, 'learning_rate': 2.089324227463529e-08, 'rewards/chosen': -3.7713940143585205, 'rewards/rejected': -6.840885162353516, 'rewards/accuracies': 0.75, 'rewards/margins': 3.069490909576416, 'policy_logps/rejected': -301.01416015625, 'policy_logps/chosen': -483.6507873535156, 'referece_logps/rejected': -232.6053466796875, 'referece_logps/chosen': -445.9368591308594, 'logits/rejected': 0.23558960855007172, 'logits/chosen': 0.010917529463768005, 'epoch': 5.62}

 94%|█████████▎| 10062/10740 [49:50:28<3:23:18, 17.99s/it]

 94%|█████████▎| 10063/10740 [49:50:41<3:03:34, 16.27s/it]

 94%|█████████▎| 10064/10740 [49:50:58<3:07:29, 16.64s/it]


 94%|█████████▎| 10066/10740 [49:51:25<2:46:59, 14.87s/it]

 94%|█████████▎| 10067/10740 [49:51:46<3:05:41, 16.55s/it]
{'loss': 0.1057, 'learning_rate': 2.0526889328441042e-08, 'rewards/chosen': -3.569349765777588, 'rewards/rejected': -5.783071041107178, 'rewards/accuracies': 0.75, 'rewards/margins': 2.21372127532959, 'policy_logps/rejected': -389.3493957519531, 'policy_logps/chosen': -422.15765380859375, 'referece_logps/rejected': -331.5186462402344, 'referece_logps/chosen': -386.46417236328125, 'logits/rejected': -0.25997990369796753, 'logits/chosen': -0.054130278527736664, 'epoch': 5.62}


 94%|█████████▍| 10069/10740 [49:52:23<3:20:09, 17.90s/it]
{'loss': 0.2122, 'learning_rate': 2.0405484304438225e-08, 'rewards/chosen': -4.290133953094482, 'rewards/rejected': -8.092853546142578, 'rewards/accuracies': 1.0, 'rewards/margins': 3.802720069885254, 'policy_logps/rejected': -444.849853515625, 'policy_logps/chosen': -567.1055908203125, 'referece_logps/rejected': -363.92132568359375, 'referece_logps/chosen': -524.2042236328125, 'logits/rejected': 0.4556347131729126, 'logits/chosen': 0.3652556538581848, 'epoch': 5.63}

 94%|█████████▍| 10070/10740 [49:52:41<3:19:48, 17.89s/it]


 94%|█████████▍| 10072/10740 [49:53:12<3:10:03, 17.07s/it]

 94%|█████████▍| 10073/10740 [49:53:25<2:57:05, 15.93s/it]

 94%|█████████▍| 10074/10740 [49:53:47<3:18:08, 17.85s/it]
{'loss': 0.1638, 'learning_rate': 2.0103531029857735e-08, 'rewards/chosen': -2.9615752696990967, 'rewards/rejected': -9.950759887695312, 'rewards/accuracies': 1.0, 'rewards/margins': 6.989184856414795, 'policy_logps/rejected': -499.35986328125, 'policy_logps/chosen': -386.0506896972656, 'referece_logps/rejected': -399.8522644042969, 'referece_logps/chosen': -356.4349365234375, 'logits/rejected': -0.32093483209609985, 'logits/chosen': -0.3357178270816803, 'epoch': 5.63}


 94%|█████████▍| 10076/10740 [49:54:27<3:29:59, 18.98s/it]
{'loss': 0.1876, 'learning_rate': 1.998337354954216e-08, 'rewards/chosen': -4.209298133850098, 'rewards/rejected': -7.602830410003662, 'rewards/accuracies': 1.0, 'rewards/margins': 3.393531560897827, 'policy_logps/rejected': -366.9206848144531, 'policy_logps/chosen': -353.3736877441406, 'referece_logps/rejected': -290.89239501953125, 'referece_logps/chosen': -311.2806701660156, 'logits/rejected': 0.25506627559661865, 'logits/chosen': 0.2971709668636322, 'epoch': 5.63}


 94%|█████████▍| 10078/10740 [49:54:55<2:59:41, 16.29s/it]

 94%|█████████▍| 10079/10740 [49:55:10<2:54:57, 15.88s/it]

 94%|█████████▍| 10080/10740 [49:55:27<2:58:13, 16.20s/it]

 94%|█████████▍| 10081/10740 [49:55:44<2:59:56, 16.38s/it]

 94%|█████████▍| 10082/10740 [49:56:04<3:11:20, 17.45s/it]

 94%|█████████▍| 10083/10740 [49:56:15<2:51:05, 15.63s/it]

 94%|█████████▍| 10084/10740 [49:56:33<2:59:29, 16.42s/it]
{'loss': 0.1367, 'learning_rate': 1.9506309460021743e-08, 'rewards/chosen': -4.652647018432617, 'rewards/rejected': -9.410466194152832, 'rewards/accuracies': 1.0, 'rewards/margins': 4.757819175720215, 'policy_logps/rejected': -353.5332336425781, 'policy_logps/chosen': -338.4855041503906, 'referece_logps/rejected': -259.4285888671875, 'referece_logps/chosen': -291.9590148925781, 'logits/rejected': -0.6314921379089355, 'logits/chosen': -0.9241074323654175, 'epoch': 5.63}

 94%|█████████▍| 10085/10740 [49:56:46<2:47:31, 15.35s/it]


 94%|█████████▍| 10087/10740 [49:57:17<2:45:01, 15.16s/it]
{'loss': 0.1852, 'learning_rate': 1.9328881719699373e-08, 'rewards/chosen': -3.010061740875244, 'rewards/rejected': -6.874874591827393, 'rewards/accuracies': 1.0, 'rewards/margins': 3.864813804626465, 'policy_logps/rejected': -454.7475891113281, 'policy_logps/chosen': -277.6880798339844, 'referece_logps/rejected': -385.99884033203125, 'referece_logps/chosen': -247.5874481201172, 'logits/rejected': -0.4812530279159546, 'logits/chosen': -0.4652334749698639, 'epoch': 5.64}


 94%|█████████▍| 10089/10740 [49:57:51<2:56:53, 16.30s/it]
{'loss': 0.0659, 'learning_rate': 1.9211042522735312e-08, 'rewards/chosen': -4.73159122467041, 'rewards/rejected': -9.360040664672852, 'rewards/accuracies': 1.0, 'rewards/margins': 4.6284499168396, 'policy_logps/rejected': -514.1033325195312, 'policy_logps/chosen': -639.896484375, 'referece_logps/rejected': -420.5029296875, 'referece_logps/chosen': -592.5806274414062, 'logits/rejected': -0.22758080065250397, 'logits/chosen': -0.36553478240966797, 'epoch': 5.64}


 94%|█████████▍| 10091/10740 [49:58:27<3:04:20, 17.04s/it]
{'loss': 0.1559, 'learning_rate': 1.9093560146407174e-08, 'rewards/chosen': -2.8660874366760254, 'rewards/rejected': -8.362525939941406, 'rewards/accuracies': 1.0, 'rewards/margins': 5.4964375495910645, 'policy_logps/rejected': -295.96099853515625, 'policy_logps/chosen': -378.4231262207031, 'referece_logps/rejected': -212.33575439453125, 'referece_logps/chosen': -349.7622985839844, 'logits/rejected': -0.03649851679801941, 'logits/chosen': -0.04121236503124237, 'epoch': 5.64}


 94%|█████████▍| 10093/10740 [49:59:07<3:19:41, 18.52s/it]
{'loss': 0.1266, 'learning_rate': 1.8976434633455995e-08, 'rewards/chosen': -3.951462745666504, 'rewards/rejected': -7.228029251098633, 'rewards/accuracies': 1.0, 'rewards/margins': 3.276566505432129, 'policy_logps/rejected': -326.2356872558594, 'policy_logps/chosen': -383.0556640625, 'referece_logps/rejected': -253.95538330078125, 'referece_logps/chosen': -343.5410461425781, 'logits/rejected': -0.16893073916435242, 'logits/chosen': -0.27020326256752014, 'epoch': 5.64}


 94%|█████████▍| 10095/10740 [49:59:46<3:20:51, 18.69s/it]

 94%|█████████▍| 10096/10740 [49:59:56<2:54:07, 16.22s/it]

 94%|█████████▍| 10097/10740 [50:00:12<2:53:04, 16.15s/it]

 94%|█████████▍| 10098/10740 [50:00:24<2:40:41, 15.02s/it]
{'loss': 0.1177, 'learning_rate': 1.8685182407670807e-08, 'rewards/chosen': -3.6170237064361572, 'rewards/rejected': -8.421661376953125, 'rewards/accuracies': 0.875, 'rewards/margins': 4.804637432098389, 'policy_logps/rejected': -485.7804260253906, 'policy_logps/chosen': -371.53607177734375, 'referece_logps/rejected': -401.5638122558594, 'referece_logps/chosen': -335.3658447265625, 'logits/rejected': -0.004776522517204285, 'logits/chosen': 0.14679059386253357, 'epoch': 5.64}

 94%|█████████▍| 10099/10740 [50:00:40<2:43:45, 15.33s/it]

 94%|█████████▍| 10100/10740 [50:00:59<2:52:09, 16.14s/it]


 94%|█████████▍| 10102/10740 [50:01:29<2:45:13, 15.54s/it]

 94%|█████████▍| 10103/10740 [50:01:42<2:35:13, 14.62s/it]

 94%|█████████▍| 10104/10740 [50:02:02<2:53:51, 16.40s/it]
{'loss': 0.119, 'learning_rate': 1.83386251427593e-08, 'rewards/chosen': -3.4888758659362793, 'rewards/rejected': -9.528112411499023, 'rewards/accuracies': 1.0, 'rewards/margins': 6.039236545562744, 'policy_logps/rejected': -359.9725341796875, 'policy_logps/chosen': -351.45257568359375, 'referece_logps/rejected': -264.69140625, 'referece_logps/chosen': -316.5638122558594, 'logits/rejected': 0.1503733992576599, 'logits/chosen': 0.015659362077713013, 'epoch': 5.64}


 94%|█████████▍| 10106/10740 [50:02:40<3:06:26, 17.64s/it]
{'loss': 0.1403, 'learning_rate': 1.8223820274609114e-08, 'rewards/chosen': -4.3065972328186035, 'rewards/rejected': -8.273896217346191, 'rewards/accuracies': 0.875, 'rewards/margins': 3.967298746109009, 'policy_logps/rejected': -564.6560668945312, 'policy_logps/chosen': -513.5850830078125, 'referece_logps/rejected': -481.9171142578125, 'referece_logps/chosen': -470.51910400390625, 'logits/rejected': 0.9860708117485046, 'logits/chosen': 1.0303620100021362, 'epoch': 5.65}


 94%|█████████▍| 10108/10740 [50:03:10<2:52:20, 16.36s/it]
{'loss': 0.1933, 'learning_rate': 1.8109372586255888e-08, 'rewards/chosen': -4.742949962615967, 'rewards/rejected': -8.58818244934082, 'rewards/accuracies': 0.875, 'rewards/margins': 3.845233201980591, 'policy_logps/rejected': -396.28021240234375, 'policy_logps/chosen': -361.4386291503906, 'referece_logps/rejected': -310.39837646484375, 'referece_logps/chosen': -314.0091247558594, 'logits/rejected': 0.016697680577635765, 'logits/chosen': -0.04161180555820465, 'epoch': 5.65}

 94%|█████████▍| 10109/10740 [50:03:31<3:08:40, 17.94s/it]

 94%|█████████▍| 10110/10740 [50:03:51<3:13:37, 18.44s/it]


 94%|█████████▍| 10112/10740 [50:04:24<3:01:15, 17.32s/it]

 94%|█████████▍| 10113/10740 [50:04:37<2:44:21, 15.73s/it]

 94%|█████████▍| 10114/10740 [50:04:51<2:38:50, 15.22s/it]
{'loss': 0.2379, 'learning_rate': 1.7768173015700106e-08, 'rewards/chosen': -3.8455774784088135, 'rewards/rejected': -7.398151874542236, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5525736808776855, 'policy_logps/rejected': -370.0245056152344, 'policy_logps/chosen': -386.7499084472656, 'referece_logps/rejected': -296.04296875, 'referece_logps/chosen': -348.29412841796875, 'logits/rejected': 0.3251726031303406, 'logits/chosen': 0.050896674394607544, 'epoch': 5.65}


 94%|█████████▍| 10116/10740 [50:05:24<2:43:23, 15.71s/it]

 94%|█████████▍| 10117/10740 [50:05:36<2:32:14, 14.66s/it]
{'loss': 0.2189, 'learning_rate': 1.7598779202001923e-08, 'rewards/chosen': -3.913304328918457, 'rewards/rejected': -7.076459884643555, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1631555557250977, 'policy_logps/rejected': -518.8594360351562, 'policy_logps/chosen': -341.416259765625, 'referece_logps/rejected': -448.09478759765625, 'referece_logps/chosen': -302.283203125, 'logits/rejected': 0.1591639220714569, 'logits/chosen': 0.10027384012937546, 'epoch': 5.65}


 94%|█████████▍| 10119/10740 [50:06:18<3:05:33, 17.93s/it]

 94%|█████████▍| 10120/10740 [50:06:38<3:09:48, 18.37s/it]

 94%|█████████▍| 10121/10740 [50:06:56<3:10:01, 18.42s/it]
{'loss': 0.2113, 'learning_rate': 1.7374171732737143e-08, 'rewards/chosen': -1.877990484237671, 'rewards/rejected': -5.592925071716309, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7149345874786377, 'policy_logps/rejected': -355.9914245605469, 'policy_logps/chosen': -362.8514404296875, 'referece_logps/rejected': -300.06219482421875, 'referece_logps/chosen': -344.071533203125, 'logits/rejected': -0.12627285718917847, 'logits/chosen': -0.26507002115249634, 'epoch': 5.65}


 94%|█████████▍| 10123/10740 [50:07:40<3:27:15, 20.16s/it]

 94%|█████████▍| 10124/10740 [50:08:00<3:26:25, 20.11s/it]

 94%|█████████▍| 10125/10740 [50:08:14<3:07:06, 18.25s/it]
{'loss': 0.1359, 'learning_rate': 1.715099421897215e-08, 'rewards/chosen': -4.18383264541626, 'rewards/rejected': -7.951727390289307, 'rewards/accuracies': 1.0, 'rewards/margins': 3.767894983291626, 'policy_logps/rejected': -358.437744140625, 'policy_logps/chosen': -346.9643859863281, 'referece_logps/rejected': -278.92047119140625, 'referece_logps/chosen': -305.12603759765625, 'logits/rejected': 0.4512787461280823, 'logits/chosen': 0.4460267722606659, 'epoch': 5.66}


 94%|█████████▍| 10127/10740 [50:08:47<2:58:44, 17.49s/it]

 94%|█████████▍| 10128/10740 [50:09:02<2:52:47, 16.94s/it]
{'loss': 0.1095, 'learning_rate': 1.698454968739682e-08, 'rewards/chosen': -3.456170082092285, 'rewards/rejected': -9.165038108825684, 'rewards/accuracies': 1.0, 'rewards/margins': 5.708868503570557, 'policy_logps/rejected': -262.22943115234375, 'policy_logps/chosen': -317.53057861328125, 'referece_logps/rejected': -170.57904052734375, 'referece_logps/chosen': -282.9688720703125, 'logits/rejected': -0.8812059760093689, 'logits/chosen': -0.8901660442352295, 'epoch': 5.66}

 94%|█████████▍| 10129/10740 [50:09:14<2:35:11, 15.24s/it]

 94%|█████████▍| 10130/10740 [50:09:29<2:36:00, 15.35s/it]


 94%|█████████▍| 10132/10740 [50:10:04<2:41:18, 15.92s/it]

 94%|█████████▍| 10133/10740 [50:10:24<2:55:00, 17.30s/it]

 94%|█████████▍| 10134/10740 [50:10:44<3:01:29, 17.97s/it]
{'loss': 0.1908, 'learning_rate': 1.6654074766669137e-08, 'rewards/chosen': -4.612119674682617, 'rewards/rejected': -8.95824146270752, 'rewards/accuracies': 1.0, 'rewards/margins': 4.346121788024902, 'policy_logps/rejected': -307.068359375, 'policy_logps/chosen': -389.0906066894531, 'referece_logps/rejected': -217.48593139648438, 'referece_logps/chosen': -342.9694519042969, 'logits/rejected': -0.10814835131168365, 'logits/chosen': -0.5824874639511108, 'epoch': 5.66}

 94%|█████████▍| 10135/10740 [50:10:56<2:42:52, 16.15s/it]

 94%|█████████▍| 10136/10740 [50:11:18<2:59:56, 17.88s/it]

 94%|█████████▍| 10137/10740 [50:11:37<3:05:03, 18.41s/it]

 94%|█████████▍| 10138/10740 [50:11:55<3:03:15, 18.27s/it]

 94%|█████████▍| 10139/10740 [50:12:07<2:44:23, 16.41s/it]


 94%|█████████▍| 10141/10740 [50:12:48<3:04:31, 18.48s/it]

 94%|█████████▍| 10142/10740 [50:13:05<3:00:22, 18.10s/it]

 94%|█████████▍| 10143/10740 [50:13:25<3:06:51, 18.78s/it]

 94%|█████████▍| 10144/10740 [50:13:42<3:02:05, 18.33s/it]
{'loss': 0.201, 'learning_rate': 1.6110438777686476e-08, 'rewards/chosen': -2.786978244781494, 'rewards/rejected': -6.827566623687744, 'rewards/accuracies': 0.875, 'rewards/margins': 4.040587902069092, 'policy_logps/rejected': -288.456787109375, 'policy_logps/chosen': -360.3516845703125, 'referece_logps/rejected': -220.18112182617188, 'referece_logps/chosen': -332.48193359375, 'logits/rejected': 0.40828219056129456, 'logits/chosen': 0.5019472241401672, 'epoch': 5.67}


 94%|█████████▍| 10146/10740 [50:14:17<2:56:22, 17.82s/it]

 94%|█████████▍| 10147/10740 [50:14:37<3:01:14, 18.34s/it]
{'loss': 0.1117, 'learning_rate': 1.5949092756622373e-08, 'rewards/chosen': -4.804905414581299, 'rewards/rejected': -9.036173820495605, 'rewards/accuracies': 1.0, 'rewards/margins': 4.231269359588623, 'policy_logps/rejected': -422.15203857421875, 'policy_logps/chosen': -526.2523193359375, 'referece_logps/rejected': -331.790283203125, 'referece_logps/chosen': -478.2032165527344, 'logits/rejected': -0.2334364950656891, 'logits/chosen': -0.4710749089717865, 'epoch': 5.67}

 94%|█████████▍| 10148/10740 [50:14:54<2:56:56, 17.93s/it]

 94%|█████████▍| 10149/10740 [50:15:08<2:44:49, 16.73s/it]


 95%|█████████▍| 10151/10740 [50:15:45<2:52:05, 17.53s/it]

 95%|█████████▍| 10152/10740 [50:16:04<2:58:19, 18.20s/it]
{'loss': 0.2322, 'learning_rate': 1.56819728231965e-08, 'rewards/chosen': -3.7280008792877197, 'rewards/rejected': -6.579426288604736, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8514256477355957, 'policy_logps/rejected': -391.82879638671875, 'policy_logps/chosen': -295.3106384277344, 'referece_logps/rejected': -326.0345153808594, 'referece_logps/chosen': -258.03057861328125, 'logits/rejected': 0.23432976007461548, 'logits/chosen': 0.29085609316825867, 'epoch': 5.67}

 95%|█████████▍| 10153/10740 [50:16:24<3:00:43, 18.47s/it]

 95%|█████████▍| 10154/10740 [50:16:44<3:05:53, 19.03s/it]

 95%|█████████▍| 10155/10740 [50:17:06<3:13:07, 19.81s/it]

 95%|█████████▍| 10156/10740 [50:17:24<3:07:56, 19.31s/it]

 95%|█████████▍| 10157/10740 [50:17:42<3:05:10, 19.06s/it]

 95%|█████████▍| 10158/10740 [50:18:02<3:06:47, 19.26s/it]

 95%|█████████▍| 10159/10740 [50:18:24<3:14:52, 20.13s/it]

 95%|█████████▍| 10160/10740 [50:18:46<3:20:16, 20.72s/it]

 95%|█████████▍| 10161/10740 [50:18:58<2:55:33, 18.19s/it]

 95%|█████████▍| 10162/10740 [50:19:16<2:53:53, 18.05s/it]

 95%|█████████▍| 10163/10740 [50:19:38<3:05:23, 19.28s/it]

 95%|█████████▍| 10164/10740 [50:19:52<2:48:26, 17.55s/it]


 95%|█████████▍| 10166/10740 [50:20:31<2:56:59, 18.50s/it]

 95%|█████████▍| 10167/10740 [50:20:44<2:40:05, 16.76s/it]
{'loss': 0.1666, 'learning_rate': 1.4894044345329815e-08, 'rewards/chosen': -4.769356727600098, 'rewards/rejected': -7.325234413146973, 'rewards/accuracies': 0.875, 'rewards/margins': 2.555878162384033, 'policy_logps/rejected': -510.0898132324219, 'policy_logps/chosen': -572.8477783203125, 'referece_logps/rejected': -436.83746337890625, 'referece_logps/chosen': -525.1541748046875, 'logits/rejected': -0.46027112007141113, 'logits/chosen': -0.5524132251739502, 'epoch': 5.68}

 95%|█████████▍| 10168/10740 [50:21:06<2:54:55, 18.35s/it]

 95%|█████████▍| 10169/10740 [50:21:21<2:45:07, 17.35s/it]


 95%|█████████▍| 10171/10740 [50:21:53<2:40:20, 16.91s/it]

 95%|█████████▍| 10172/10740 [50:22:12<2:43:57, 17.32s/it]

 95%|█████████▍| 10173/10740 [50:22:28<2:39:31, 16.88s/it]
{'loss': 0.147, 'learning_rate': 1.4584516717054075e-08, 'rewards/chosen': -3.467129945755005, 'rewards/rejected': -8.749268531799316, 'rewards/accuracies': 1.0, 'rewards/margins': 5.282139301300049, 'policy_logps/rejected': -381.32891845703125, 'policy_logps/chosen': -363.7568054199219, 'referece_logps/rejected': -293.8362121582031, 'referece_logps/chosen': -329.0854797363281, 'logits/rejected': -0.42602208256721497, 'logits/chosen': -0.41774845123291016, 'epoch': 5.68}


 95%|█████████▍| 10175/10740 [50:23:03<2:43:07, 17.32s/it]

 95%|█████████▍| 10176/10740 [50:23:15<2:27:29, 15.69s/it]

 95%|█████████▍| 10177/10740 [50:23:31<2:27:51, 15.76s/it]

 95%|█████████▍| 10178/10740 [50:23:52<2:40:52, 17.18s/it]
{'loss': 0.2469, 'learning_rate': 1.4329041697236144e-08, 'rewards/chosen': -4.127429008483887, 'rewards/rejected': -7.253940105438232, 'rewards/accuracies': 0.75, 'rewards/margins': 3.126511335372925, 'policy_logps/rejected': -279.6927795410156, 'policy_logps/chosen': -231.59127807617188, 'referece_logps/rejected': -207.1533966064453, 'referece_logps/chosen': -190.31700134277344, 'logits/rejected': 0.30734172463417053, 'logits/chosen': 0.39294785261154175, 'epoch': 5.69}

 95%|█████████▍| 10179/10740 [50:24:08<2:38:17, 16.93s/it]

 95%|█████████▍| 10180/10740 [50:24:27<2:43:28, 17.51s/it]

 95%|█████████▍| 10181/10740 [50:24:43<2:38:46, 17.04s/it]


 95%|█████████▍| 10183/10740 [50:25:19<2:42:39, 17.52s/it]
{'loss': 0.1728, 'learning_rate': 1.4075807906783865e-08, 'rewards/chosen': -3.972057342529297, 'rewards/rejected': -8.50279712677002, 'rewards/accuracies': 1.0, 'rewards/margins': 4.530740737915039, 'policy_logps/rejected': -309.03875732421875, 'policy_logps/chosen': -348.9790954589844, 'referece_logps/rejected': -224.01080322265625, 'referece_logps/chosen': -309.25848388671875, 'logits/rejected': 0.3532932698726654, 'logits/chosen': 0.35456934571266174, 'epoch': 5.69}

 95%|█████████▍| 10184/10740 [50:25:39<2:48:22, 18.17s/it]


 95%|█████████▍| 10186/10740 [50:26:18<2:54:55, 18.94s/it]
{'loss': 0.1372, 'learning_rate': 1.3924943662355415e-08, 'rewards/chosen': -3.6402769088745117, 'rewards/rejected': -8.665342330932617, 'rewards/accuracies': 1.0, 'rewards/margins': 5.025066375732422, 'policy_logps/rejected': -446.4700012207031, 'policy_logps/chosen': -434.1395568847656, 'referece_logps/rejected': -359.81658935546875, 'referece_logps/chosen': -397.7367858886719, 'logits/rejected': -0.6810938119888306, 'logits/chosen': -0.6058218479156494, 'epoch': 5.69}

 95%|█████████▍| 10187/10740 [50:26:37<2:52:36, 18.73s/it]

 95%|█████████▍| 10188/10740 [50:26:57<2:57:49, 19.33s/it]


 95%|█████████▍| 10190/10740 [50:27:32<2:46:34, 18.17s/it]

 95%|█████████▍| 10191/10740 [50:27:48<2:42:09, 17.72s/it]

 95%|█████████▍| 10192/10740 [50:28:09<2:49:04, 18.51s/it]

 95%|█████████▍| 10193/10740 [50:28:29<2:52:37, 18.93s/it]
{'loss': 0.1629, 'learning_rate': 1.3576066312102397e-08, 'rewards/chosen': -3.6603055000305176, 'rewards/rejected': -8.34135913848877, 'rewards/accuracies': 1.0, 'rewards/margins': 4.68105411529541, 'policy_logps/rejected': -412.052978515625, 'policy_logps/chosen': -454.82086181640625, 'referece_logps/rejected': -328.639404296875, 'referece_logps/chosen': -418.21783447265625, 'logits/rejected': -0.71746826171875, 'logits/chosen': -0.773474931716919, 'epoch': 5.69}

 95%|█████████▍| 10194/10740 [50:28:43<2:41:15, 17.72s/it]


 95%|█████████▍| 10196/10740 [50:29:14<2:27:09, 16.23s/it]
{'loss': 0.2666, 'learning_rate': 1.34278931223899e-08, 'rewards/chosen': -3.613445997238159, 'rewards/rejected': -8.350055694580078, 'rewards/accuracies': 1.0, 'rewards/margins': 4.73660945892334, 'policy_logps/rejected': -390.5484619140625, 'policy_logps/chosen': -409.52276611328125, 'referece_logps/rejected': -307.04791259765625, 'referece_logps/chosen': -373.3883056640625, 'logits/rejected': 0.23927462100982666, 'logits/chosen': 0.1337864100933075, 'epoch': 5.7}

 95%|█████████▍| 10197/10740 [50:29:35<2:40:52, 17.78s/it]

 95%|█████████▍| 10198/10740 [50:29:55<2:46:00, 18.38s/it]


 95%|█████████▍| 10200/10740 [50:30:36<2:55:51, 19.54s/it]

 95%|█████████▍| 10201/10740 [50:30:53<2:48:42, 18.78s/it]

 95%|█████████▍| 10202/10740 [50:31:08<2:39:42, 17.81s/it]
{'loss': 0.2631, 'learning_rate': 1.3133969604567919e-08, 'rewards/chosen': -2.663245916366577, 'rewards/rejected': -4.2200822830200195, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5568362474441528, 'policy_logps/rejected': -388.2498474121094, 'policy_logps/chosen': -356.3438720703125, 'referece_logps/rejected': -346.049072265625, 'referece_logps/chosen': -329.71136474609375, 'logits/rejected': 0.49436819553375244, 'logits/chosen': 0.5461258292198181, 'epoch': 5.7}

 95%|█████████▌| 10203/10740 [50:31:24<2:32:39, 17.06s/it]

 95%|█████████▌| 10204/10740 [50:31:43<2:39:23, 17.84s/it]


 95%|█████████▌| 10206/10740 [50:32:23<2:47:12, 18.79s/it]

 95%|█████████▌| 10207/10740 [50:32:42<2:49:29, 19.08s/it]
{'loss': 0.2239, 'learning_rate': 1.2891501640393432e-08, 'rewards/chosen': -5.134842872619629, 'rewards/rejected': -8.38613510131836, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2512917518615723, 'policy_logps/rejected': -339.4613952636719, 'policy_logps/chosen': -359.095947265625, 'referece_logps/rejected': -255.6000213623047, 'referece_logps/chosen': -307.74749755859375, 'logits/rejected': 0.06999583542346954, 'logits/chosen': 0.0719878152012825, 'epoch': 5.7}


 95%|█████████▌| 10209/10740 [50:33:25<2:57:12, 20.02s/it]

 95%|█████████▌| 10210/10740 [50:33:45<2:57:04, 20.05s/it]

 95%|█████████▌| 10211/10740 [50:33:58<2:38:36, 17.99s/it]
{'loss': 0.1365, 'learning_rate': 1.2699143281364788e-08, 'rewards/chosen': -4.310802936553955, 'rewards/rejected': -7.957137107849121, 'rewards/accuracies': 0.875, 'rewards/margins': 3.646334171295166, 'policy_logps/rejected': -455.0563049316406, 'policy_logps/chosen': -557.8759765625, 'referece_logps/rejected': -375.4848937988281, 'referece_logps/chosen': -514.7679443359375, 'logits/rejected': -0.2916465997695923, 'logits/chosen': -0.44832971692085266, 'epoch': 5.7}

 95%|█████████▌| 10212/10740 [50:34:09<2:20:33, 15.97s/it]


 95%|█████████▌| 10214/10740 [50:34:39<2:13:38, 15.24s/it]
{'loss': 0.2013, 'learning_rate': 1.2555817369815147e-08, 'rewards/chosen': -4.895881175994873, 'rewards/rejected': -8.405035972595215, 'rewards/accuracies': 0.75, 'rewards/margins': 3.5091552734375, 'policy_logps/rejected': -426.76556396484375, 'policy_logps/chosen': -406.9352722167969, 'referece_logps/rejected': -342.71514892578125, 'referece_logps/chosen': -357.9764404296875, 'logits/rejected': 0.1910514235496521, 'logits/chosen': 0.27634555101394653, 'epoch': 5.71}


 95%|█████████▌| 10216/10740 [50:35:11<2:18:17, 15.83s/it]

 95%|█████████▌| 10217/10740 [50:35:32<2:33:43, 17.64s/it]

 95%|█████████▌| 10218/10740 [50:35:44<2:18:28, 15.92s/it]

 95%|█████████▌| 10219/10740 [50:36:04<2:28:52, 17.15s/it]
{'loss': 0.2385, 'learning_rate': 1.2318737117495426e-08, 'rewards/chosen': -4.271684646606445, 'rewards/rejected': -8.962423324584961, 'rewards/accuracies': 1.0, 'rewards/margins': 4.690738677978516, 'policy_logps/rejected': -327.2016296386719, 'policy_logps/chosen': -240.5331268310547, 'referece_logps/rejected': -237.57740783691406, 'referece_logps/chosen': -197.81625366210938, 'logits/rejected': -0.09244435280561447, 'logits/chosen': -0.08279147744178772, 'epoch': 5.71}

 95%|█████████▌| 10220/10740 [50:36:24<2:35:00, 17.89s/it]


 95%|█████████▌| 10222/10740 [50:37:03<2:40:45, 18.62s/it]
{'loss': 0.2308, 'learning_rate': 1.2177566916015902e-08, 'rewards/chosen': -5.185845375061035, 'rewards/rejected': -8.765275955200195, 'rewards/accuracies': 0.75, 'rewards/margins': 3.579430341720581, 'policy_logps/rejected': -428.51348876953125, 'policy_logps/chosen': -397.4071044921875, 'referece_logps/rejected': -340.8607482910156, 'referece_logps/chosen': -345.5486145019531, 'logits/rejected': 0.2741785943508148, 'logits/chosen': 0.1815553605556488, 'epoch': 5.71}

 95%|█████████▌| 10223/10740 [50:37:22<2:41:25, 18.73s/it]

 95%|█████████▌| 10224/10740 [50:37:42<2:44:32, 19.13s/it]

 95%|█████████▌| 10225/10740 [50:38:04<2:52:19, 20.08s/it]


 95%|█████████▌| 10227/10740 [50:38:40<2:42:15, 18.98s/it]

 95%|█████████▌| 10228/10740 [50:39:01<2:46:48, 19.55s/it]

 95%|█████████▌| 10229/10740 [50:39:16<2:35:16, 18.23s/it]
{'loss': 0.1922, 'learning_rate': 1.1851314548079639e-08, 'rewards/chosen': -3.2279505729675293, 'rewards/rejected': -6.028898239135742, 'rewards/accuracies': 1.0, 'rewards/margins': 2.800947427749634, 'policy_logps/rejected': -346.6585693359375, 'policy_logps/chosen': -324.592529296875, 'referece_logps/rejected': -286.3695983886719, 'referece_logps/chosen': -292.31298828125, 'logits/rejected': -0.1255188286304474, 'logits/chosen': -0.20811909437179565, 'epoch': 5.71}

 95%|█████████▌| 10230/10740 [50:39:36<2:38:30, 18.65s/it]


 95%|█████████▌| 10232/10740 [50:40:13<2:35:34, 18.38s/it]
{'loss': 0.1472, 'learning_rate': 1.1712840138295876e-08, 'rewards/chosen': -4.22609806060791, 'rewards/rejected': -7.5012993812561035, 'rewards/accuracies': 0.875, 'rewards/margins': 3.275200843811035, 'policy_logps/rejected': -320.2997741699219, 'policy_logps/chosen': -423.7738037109375, 'referece_logps/rejected': -245.2867889404297, 'referece_logps/chosen': -381.5128479003906, 'logits/rejected': 0.10884259641170502, 'logits/chosen': -0.04581426829099655, 'epoch': 5.72}


 95%|█████████▌| 10234/10740 [50:40:43<2:19:52, 16.59s/it]

 95%|█████████▌| 10235/10740 [50:40:53<2:03:17, 14.65s/it]
{'loss': 0.1561, 'learning_rate': 1.1575174712731061e-08, 'rewards/chosen': -3.732717752456665, 'rewards/rejected': -6.707244396209717, 'rewards/accuracies': 0.875, 'rewards/margins': 2.97452712059021, 'policy_logps/rejected': -273.478759765625, 'policy_logps/chosen': -271.8550109863281, 'referece_logps/rejected': -206.4063262939453, 'referece_logps/chosen': -234.52786254882812, 'logits/rejected': -0.5236064791679382, 'logits/chosen': -0.25324252247810364, 'epoch': 5.72}

 95%|█████████▌| 10236/10740 [50:41:12<2:15:34, 16.14s/it]


 95%|█████████▌| 10238/10740 [50:41:51<2:29:02, 17.81s/it]
{'loss': 0.2283, 'learning_rate': 1.1438318384074164e-08, 'rewards/chosen': -3.5955662727355957, 'rewards/rejected': -6.481321811676025, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8857555389404297, 'policy_logps/rejected': -331.22100830078125, 'policy_logps/chosen': -266.7635803222656, 'referece_logps/rejected': -266.40777587890625, 'referece_logps/chosen': -230.80792236328125, 'logits/rejected': -0.6707453727722168, 'logits/chosen': -0.6500597596168518, 'epoch': 5.72}


 95%|█████████▌| 10240/10740 [50:42:30<2:34:03, 18.49s/it]
{'loss': 0.1361, 'learning_rate': 1.1347530385267701e-08, 'rewards/chosen': -4.016017436981201, 'rewards/rejected': -9.116686820983887, 'rewards/accuracies': 0.875, 'rewards/margins': 5.100668907165527, 'policy_logps/rejected': -418.3442687988281, 'policy_logps/chosen': -429.3016357421875, 'referece_logps/rejected': -327.1773986816406, 'referece_logps/chosen': -389.1414489746094, 'logits/rejected': 0.14745204150676727, 'logits/chosen': -0.01254168152809143, 'epoch': 5.72}

 95%|█████████▌| 10241/10740 [50:42:50<2:38:05, 19.01s/it]


 95%|█████████▌| 10243/10740 [50:43:25<2:30:38, 18.19s/it]
{'loss': 0.2269, 'learning_rate': 1.1212022800078713e-08, 'rewards/chosen': -5.703168869018555, 'rewards/rejected': -7.583971977233887, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8808025121688843, 'policy_logps/rejected': -378.3355712890625, 'policy_logps/chosen': -357.3628845214844, 'referece_logps/rejected': -302.4958801269531, 'referece_logps/chosen': -300.3312072753906, 'logits/rejected': 0.05283210799098015, 'logits/chosen': 0.0485457181930542, 'epoch': 5.72}

 95%|█████████▌| 10244/10740 [50:43:46<2:37:25, 19.04s/it]

 95%|█████████▌| 10245/10740 [50:44:08<2:44:39, 19.96s/it]


 95%|█████████▌| 10247/10740 [50:44:44<2:33:25, 18.67s/it]

 95%|█████████▌| 10248/10740 [50:45:05<2:40:48, 19.61s/it]

 95%|█████████▌| 10249/10740 [50:45:25<2:40:53, 19.66s/it]
{'loss': 0.1426, 'learning_rate': 1.0943435922482924e-08, 'rewards/chosen': -3.587667942047119, 'rewards/rejected': -7.905006408691406, 'rewards/accuracies': 1.0, 'rewards/margins': 4.317337989807129, 'policy_logps/rejected': -315.443603515625, 'policy_logps/chosen': -232.25682067871094, 'referece_logps/rejected': -236.39352416992188, 'referece_logps/chosen': -196.38015747070312, 'logits/rejected': -0.5675657987594604, 'logits/chosen': -0.5967825651168823, 'epoch': 5.73}

 95%|█████████▌| 10250/10740 [50:45:38<2:24:43, 17.72s/it]


 95%|█████████▌| 10252/10740 [50:46:21<2:40:13, 19.70s/it]

 95%|█████████▌| 10253/10740 [50:46:42<2:41:33, 19.90s/it]

 95%|█████████▌| 10254/10740 [50:47:01<2:40:36, 19.83s/it]
{'loss': 0.1213, 'learning_rate': 1.072208730762969e-08, 'rewards/chosen': -3.0761170387268066, 'rewards/rejected': -7.669867515563965, 'rewards/accuracies': 0.875, 'rewards/margins': 4.59375, 'policy_logps/rejected': -296.9602355957031, 'policy_logps/chosen': -316.887451171875, 'referece_logps/rejected': -220.26156616210938, 'referece_logps/chosen': -286.1262512207031, 'logits/rejected': 0.5990318059921265, 'logits/chosen': 0.4389616847038269, 'epoch': 5.73}

 95%|█████████▌| 10255/10740 [50:47:22<2:43:27, 20.22s/it]


 96%|█████████▌| 10257/10740 [50:47:56<2:25:48, 18.11s/it]

 96%|█████████▌| 10258/10740 [50:48:13<2:24:17, 17.96s/it]
{'loss': 0.1007, 'learning_rate': 1.0546627982006895e-08, 'rewards/chosen': -3.880892753601074, 'rewards/rejected': -9.615622520446777, 'rewards/accuracies': 0.875, 'rewards/margins': 5.734729766845703, 'policy_logps/rejected': -559.0250244140625, 'policy_logps/chosen': -567.3070068359375, 'referece_logps/rejected': -462.8688049316406, 'referece_logps/chosen': -528.498046875, 'logits/rejected': 0.5348043441772461, 'logits/chosen': 0.3973567485809326, 'epoch': 5.73}

 96%|█████████▌| 10259/10740 [50:48:24<2:06:31, 15.78s/it]


 96%|█████████▌| 10261/10740 [50:48:59<2:15:15, 16.94s/it]

 96%|█████████▌| 10262/10740 [50:49:15<2:13:39, 16.78s/it]

 96%|█████████▌| 10263/10740 [50:49:36<2:21:47, 17.84s/it]

 96%|█████████▌| 10264/10740 [50:49:51<2:16:15, 17.17s/it]

 96%|█████████▌| 10265/10740 [50:50:07<2:12:35, 16.75s/it]

 96%|█████████▌| 10266/10740 [50:50:25<2:16:12, 17.24s/it]
{'loss': 0.1474, 'learning_rate': 1.0200029257625641e-08, 'rewards/chosen': -5.205798149108887, 'rewards/rejected': -8.150890350341797, 'rewards/accuracies': 0.75, 'rewards/margins': 2.945091724395752, 'policy_logps/rejected': -312.70849609375, 'policy_logps/chosen': -391.10626220703125, 'referece_logps/rejected': -231.19956970214844, 'referece_logps/chosen': -339.04827880859375, 'logits/rejected': -0.02926000952720642, 'logits/chosen': -0.028049707412719727, 'epoch': 5.74}

 96%|█████████▌| 10267/10740 [50:50:41<2:11:04, 16.63s/it]

 96%|█████████▌| 10268/10740 [50:50:53<2:00:22, 15.30s/it]

 96%|█████████▌| 10269/10740 [50:51:11<2:06:01, 16.05s/it]

 96%|█████████▌| 10270/10740 [50:51:23<1:56:38, 14.89s/it]

 96%|█████████▌| 10271/10740 [50:51:42<2:07:24, 16.30s/it]

 96%|█████████▌| 10272/10740 [50:52:03<2:16:22, 17.48s/it]

 96%|█████████▌| 10273/10740 [50:52:25<2:27:31, 18.95s/it]

 96%|█████████▌| 10274/10740 [50:52:45<2:29:28, 19.25s/it]

 96%|█████████▌| 10275/10740 [50:53:04<2:27:49, 19.07s/it]

 96%|█████████▌| 10276/10740 [50:53:23<2:29:04, 19.28s/it]

 96%|█████████▌| 10277/10740 [50:53:43<2:29:58, 19.44s/it]

 96%|█████████▌| 10278/10740 [50:54:03<2:31:20, 19.65s/it]


 96%|█████████▌| 10280/10740 [50:54:32<2:09:06, 16.84s/it]
{'loss': 0.124, 'learning_rate': 9.607346487248924e-09, 'rewards/chosen': -3.115109920501709, 'rewards/rejected': -7.817711353302002, 'rewards/accuracies': 1.0, 'rewards/margins': 4.702601432800293, 'policy_logps/rejected': -459.85723876953125, 'policy_logps/chosen': -422.0772399902344, 'referece_logps/rejected': -381.6801452636719, 'referece_logps/chosen': -390.9261474609375, 'logits/rejected': -0.7744210362434387, 'logits/chosen': -0.8850774765014648, 'epoch': 5.74}

 96%|█████████▌| 10281/10740 [50:54:47<2:03:45, 16.18s/it]

 96%|█████████▌| 10282/10740 [50:54:59<1:55:18, 15.11s/it]

 96%|█████████▌| 10283/10740 [50:55:11<1:45:53, 13.90s/it]

 96%|█████████▌| 10284/10740 [50:55:27<1:51:35, 14.68s/it]

 96%|█████████▌| 10285/10740 [50:55:47<2:02:34, 16.16s/it]

 96%|█████████▌| 10286/10740 [50:56:05<2:07:07, 16.80s/it]

 96%|█████████▌| 10287/10740 [50:56:25<2:14:26, 17.81s/it]


 96%|█████████▌| 10289/10740 [50:57:06<2:24:41, 19.25s/it]

 96%|█████████▌| 10290/10740 [50:57:20<2:12:53, 17.72s/it]
{'loss': 0.1769, 'learning_rate': 9.19481046855719e-09, 'rewards/chosen': -5.2192583084106445, 'rewards/rejected': -8.885201454162598, 'rewards/accuracies': 1.0, 'rewards/margins': 3.665943145751953, 'policy_logps/rejected': -665.4200439453125, 'policy_logps/chosen': -513.5114135742188, 'referece_logps/rejected': -576.5679931640625, 'referece_logps/chosen': -461.3188781738281, 'logits/rejected': 0.5974841117858887, 'logits/chosen': 0.5336961150169373, 'epoch': 5.75}

 96%|█████████▌| 10291/10740 [50:57:37<2:10:26, 17.43s/it]


 96%|█████████▌| 10293/10740 [50:58:15<2:13:53, 17.97s/it]
{'loss': 0.1971, 'learning_rate': 9.072806758210206e-09, 'rewards/chosen': -4.292059898376465, 'rewards/rejected': -8.23279094696045, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9407315254211426, 'policy_logps/rejected': -324.5497741699219, 'policy_logps/chosen': -509.8281555175781, 'referece_logps/rejected': -242.22183227539062, 'referece_logps/chosen': -466.9075622558594, 'logits/rejected': 0.34763967990875244, 'logits/chosen': 0.18619108200073242, 'epoch': 5.75}

 96%|█████████▌| 10294/10740 [50:58:33<2:14:09, 18.05s/it]


 96%|█████████▌| 10296/10740 [50:59:01<1:57:43, 15.91s/it]
{'loss': 0.1858, 'learning_rate': 8.951614193139611e-09, 'rewards/chosen': -4.6349663734436035, 'rewards/rejected': -8.786260604858398, 'rewards/accuracies': 1.0, 'rewards/margins': 4.151294708251953, 'policy_logps/rejected': -315.2028503417969, 'policy_logps/chosen': -362.58685302734375, 'referece_logps/rejected': -227.34022521972656, 'referece_logps/chosen': -316.2372131347656, 'logits/rejected': 0.9202946424484253, 'logits/chosen': 0.7952859401702881, 'epoch': 5.75}

 96%|█████████▌| 10297/10740 [50:59:13<1:49:56, 14.89s/it]

 96%|█████████▌| 10298/10740 [50:59:33<2:00:25, 16.35s/it]

 96%|█████████▌| 10299/10740 [50:59:45<1:52:00, 15.24s/it]

 96%|█████████▌| 10300/10740 [51:00:05<2:01:38, 16.59s/it]


 96%|█████████▌| 10302/10740 [51:00:44<2:11:59, 18.08s/it]
{'loss': 0.164, 'learning_rate': 8.711662894983041e-09, 'rewards/chosen': -4.3436970710754395, 'rewards/rejected': -8.145954132080078, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8022570610046387, 'policy_logps/rejected': -372.0433349609375, 'policy_logps/chosen': -370.50244140625, 'referece_logps/rejected': -290.5837707519531, 'referece_logps/chosen': -327.06549072265625, 'logits/rejected': 0.29054272174835205, 'logits/chosen': 0.18325389921665192, 'epoch': 5.76}

 96%|█████████▌| 10303/10740 [51:00:55<1:55:46, 15.90s/it]

 96%|█████████▌| 10304/10740 [51:01:13<2:00:01, 16.52s/it]

 96%|█████████▌| 10305/10740 [51:01:30<2:00:27, 16.61s/it]

 96%|█████████▌| 10306/10740 [51:01:50<2:07:09, 17.58s/it]

 96%|█████████▌| 10307/10740 [51:02:09<2:11:36, 18.24s/it]


 96%|█████████▌| 10309/10740 [51:02:45<2:10:24, 18.15s/it]

 96%|█████████▌| 10310/10740 [51:02:59<2:00:39, 16.84s/it]
{'loss': 0.2147, 'learning_rate': 8.396776929596394e-09, 'rewards/chosen': -3.592496395111084, 'rewards/rejected': -8.694828033447266, 'rewards/accuracies': 1.0, 'rewards/margins': 5.10233211517334, 'policy_logps/rejected': -412.5279235839844, 'policy_logps/chosen': -487.481201171875, 'referece_logps/rejected': -325.57965087890625, 'referece_logps/chosen': -451.5562438964844, 'logits/rejected': -0.23788690567016602, 'logits/chosen': -0.27360793948173523, 'epoch': 5.76}


 96%|█████████▌| 10312/10740 [51:03:33<1:58:38, 16.63s/it]
{'loss': 0.2134, 'learning_rate': 8.318957254407899e-09, 'rewards/chosen': -5.672593116760254, 'rewards/rejected': -8.309743881225586, 'rewards/accuracies': 1.0, 'rewards/margins': 2.637150764465332, 'policy_logps/rejected': -364.9534606933594, 'policy_logps/chosen': -449.4738464355469, 'referece_logps/rejected': -281.85601806640625, 'referece_logps/chosen': -392.7478942871094, 'logits/rejected': -0.11591862887144089, 'logits/chosen': -0.23634842038154602, 'epoch': 5.76}

 96%|█████████▌| 10313/10740 [51:03:52<2:03:47, 17.39s/it]

 96%|█████████▌| 10314/10740 [51:04:07<1:58:47, 16.73s/it]

 96%|█████████▌| 10315/10740 [51:04:24<1:59:12, 16.83s/it]

 96%|█████████▌| 10316/10740 [51:04:38<1:52:24, 15.91s/it]

 96%|█████████▌| 10317/10740 [51:04:54<1:51:44, 15.85s/it]

 96%|█████████▌| 10318/10740 [51:05:13<1:59:24, 16.98s/it]

 96%|█████████▌| 10319/10740 [51:05:30<1:58:16, 16.86s/it]

 96%|█████████▌| 10320/10740 [51:05:49<2:03:48, 17.69s/it]

 96%|█████████▌| 10321/10740 [51:06:09<2:08:06, 18.34s/it]

 96%|█████████▌| 10322/10740 [51:06:28<2:09:13, 18.55s/it]


 96%|█████████▌| 10324/10740 [51:07:03<2:05:32, 18.11s/it]
{'loss': 0.1718, 'learning_rate': 7.859616633940525e-09, 'rewards/chosen': -4.862032890319824, 'rewards/rejected': -9.237701416015625, 'rewards/accuracies': 0.75, 'rewards/margins': 4.375668048858643, 'policy_logps/rejected': -372.7616882324219, 'policy_logps/chosen': -320.947265625, 'referece_logps/rejected': -280.3846740722656, 'referece_logps/chosen': -272.3269348144531, 'logits/rejected': 0.17741237580776215, 'logits/chosen': 0.11573541164398193, 'epoch': 5.77}


 96%|█████████▌| 10326/10740 [51:07:43<2:11:51, 19.11s/it]

 96%|█████████▌| 10327/10740 [51:08:03<2:12:32, 19.26s/it]
{'loss': 0.164, 'learning_rate': 7.746811591382907e-09, 'rewards/chosen': -3.585677146911621, 'rewards/rejected': -7.9745402336120605, 'rewards/accuracies': 1.0, 'rewards/margins': 4.388862609863281, 'policy_logps/rejected': -393.2247619628906, 'policy_logps/chosen': -326.9793701171875, 'referece_logps/rejected': -313.4793701171875, 'referece_logps/chosen': -291.12255859375, 'logits/rejected': -0.5046281218528748, 'logits/chosen': -0.4498516023159027, 'epoch': 5.77}

 96%|█████████▌| 10328/10740 [51:08:14<1:56:07, 16.91s/it]

 96%|█████████▌| 10329/10740 [51:08:35<2:04:32, 18.18s/it]

 96%|█████████▌| 10330/10740 [51:08:58<2:12:47, 19.43s/it]

 96%|█████████▌| 10331/10740 [51:09:19<2:15:37, 19.90s/it]

 96%|█████████▌| 10332/10740 [51:09:38<2:15:10, 19.88s/it]


 96%|█████████▌| 10334/10740 [51:10:17<2:12:56, 19.65s/it]

 96%|█████████▌| 10335/10740 [51:10:35<2:09:23, 19.17s/it]
{'loss': 0.189, 'learning_rate': 7.449969298892189e-09, 'rewards/chosen': -3.748105525970459, 'rewards/rejected': -7.947517395019531, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1994123458862305, 'policy_logps/rejected': -357.45245361328125, 'policy_logps/chosen': -343.6012268066406, 'referece_logps/rejected': -277.9772644042969, 'referece_logps/chosen': -306.1202392578125, 'logits/rejected': 0.21698153018951416, 'logits/chosen': 0.055102333426475525, 'epoch': 5.77}


 96%|█████████▌| 10337/10740 [51:11:13<2:09:54, 19.34s/it]

 96%|█████████▋| 10338/10740 [51:11:33<2:10:20, 19.45s/it]
{'loss': 0.1283, 'learning_rate': 7.340142881942735e-09, 'rewards/chosen': -3.2125024795532227, 'rewards/rejected': -7.3959736824035645, 'rewards/accuracies': 1.0, 'rewards/margins': 4.183471202850342, 'policy_logps/rejected': -366.0332336425781, 'policy_logps/chosen': -517.7965087890625, 'referece_logps/rejected': -292.0735168457031, 'referece_logps/chosen': -485.6715087890625, 'logits/rejected': 0.5118704438209534, 'logits/chosen': 0.45175015926361084, 'epoch': 5.78}


 96%|█████████▋| 10340/10740 [51:12:15<2:15:56, 20.39s/it]

 96%|█████████▋| 10341/10740 [51:12:35<2:14:48, 20.27s/it]
{'loss': 0.1805, 'learning_rate': 7.231129028579918e-09, 'rewards/chosen': -4.806798458099365, 'rewards/rejected': -9.99944019317627, 'rewards/accuracies': 0.875, 'rewards/margins': 5.192641735076904, 'policy_logps/rejected': -565.6532592773438, 'policy_logps/chosen': -408.9714050292969, 'referece_logps/rejected': -465.6589050292969, 'referece_logps/chosen': -360.9034423828125, 'logits/rejected': -0.3062297999858856, 'logits/chosen': -0.2423429638147354, 'epoch': 5.78}

 96%|█████████▋| 10342/10740 [51:12:52<2:07:56, 19.29s/it]

 96%|█████████▋| 10343/10740 [51:13:12<2:08:33, 19.43s/it]


 96%|█████████▋| 10345/10740 [51:13:41<1:50:50, 16.84s/it]
{'loss': 0.1141, 'learning_rate': 7.087041366036106e-09, 'rewards/chosen': -3.432401418685913, 'rewards/rejected': -6.6146345138549805, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1822335720062256, 'policy_logps/rejected': -491.8553466796875, 'policy_logps/chosen': -373.20111083984375, 'referece_logps/rejected': -425.7090148925781, 'referece_logps/chosen': -338.8771057128906, 'logits/rejected': 0.13712802529335022, 'logits/chosen': -0.018094472587108612, 'epoch': 5.78}

 96%|█████████▋| 10346/10740 [51:13:54<1:43:38, 15.78s/it]

 96%|█████████▋| 10347/10740 [51:14:14<1:50:08, 16.82s/it]


 96%|█████████▋| 10349/10740 [51:14:49<1:53:17, 17.39s/it]
{'loss': 0.0661, 'learning_rate': 6.944398629226023e-09, 'rewards/chosen': -4.264289855957031, 'rewards/rejected': -9.641249656677246, 'rewards/accuracies': 1.0, 'rewards/margins': 5.376959323883057, 'policy_logps/rejected': -546.819580078125, 'policy_logps/chosen': -368.84014892578125, 'referece_logps/rejected': -450.40704345703125, 'referece_logps/chosen': -326.197265625, 'logits/rejected': 0.6289514303207397, 'logits/chosen': 0.7736773490905762, 'epoch': 5.78}

 96%|█████████▋| 10350/10740 [51:15:09<1:57:21, 18.06s/it]

 96%|█████████▋| 10351/10740 [51:15:23<1:48:36, 16.75s/it]

 96%|█████████▋| 10352/10740 [51:15:42<1:53:07, 17.49s/it]

 96%|█████████▋| 10353/10740 [51:16:01<1:55:32, 17.91s/it]

 96%|█████████▋| 10354/10740 [51:16:14<1:46:39, 16.58s/it]

 96%|█████████▋| 10355/10740 [51:16:34<1:52:48, 17.58s/it]

 96%|█████████▋| 10356/10740 [51:16:52<1:53:51, 17.79s/it]


 96%|█████████▋| 10358/10740 [51:17:32<1:59:08, 18.71s/it]

 96%|█████████▋| 10359/10740 [51:17:52<2:00:54, 19.04s/it]

 96%|█████████▋| 10360/10740 [51:18:03<1:46:52, 16.88s/it]

 96%|█████████▋| 10361/10740 [51:18:17<1:39:36, 15.77s/it]

 96%|█████████▋| 10362/10740 [51:18:37<1:47:33, 17.07s/it]

 96%|█████████▋| 10363/10740 [51:18:58<1:56:11, 18.49s/it]

 96%|█████████▋| 10364/10740 [51:19:18<1:57:56, 18.82s/it]

 97%|█████████▋| 10365/10740 [51:19:40<2:04:11, 19.87s/it]

 97%|█████████▋| 10366/10740 [51:19:55<1:54:49, 18.42s/it]

 97%|█████████▋| 10367/10740 [51:20:15<1:57:14, 18.86s/it]

 97%|█████████▋| 10368/10740 [51:20:34<1:57:02, 18.88s/it]

 97%|█████████▋| 10369/10740 [51:20:47<1:46:10, 17.17s/it]

 97%|█████████▋| 10370/10740 [51:21:05<1:46:41, 17.30s/it]

 97%|█████████▋| 10371/10740 [51:21:25<1:50:57, 18.04s/it]

 97%|█████████▋| 10372/10740 [51:21:37<1:40:18, 16.35s/it]

 97%|█████████▋| 10373/10740 [51:21:57<1:46:55, 17.48s/it]

 97%|█████████▋| 10374/10740 [51:22:15<1:46:54, 17.53s/it]

 97%|█████████▋| 10375/10740 [51:22:26<1:34:46, 15.58s/it]

 97%|█████████▋| 10376/10740 [51:22:46<1:42:07, 16.83s/it]

 97%|█████████▋| 10377/10740 [51:23:01<1:38:16, 16.24s/it]

 97%|█████████▋| 10378/10740 [51:23:11<1:27:46, 14.55s/it]

 97%|█████████▋| 10379/10740 [51:23:31<1:36:57, 16.12s/it]

 97%|█████████▋| 10380/10740 [51:23:51<1:44:25, 17.40s/it]

 97%|█████████▋| 10381/10740 [51:24:03<1:33:54, 15.70s/it]

 97%|█████████▋| 10382/10740 [51:24:23<1:41:32, 17.02s/it]

 97%|█████████▋| 10383/10740 [51:24:43<1:46:06, 17.83s/it]

 97%|█████████▋| 10384/10740 [51:25:03<1:49:01, 18.37s/it]

 97%|█████████▋| 10385/10740 [51:25:21<1:48:58, 18.42s/it]

 97%|█████████▋| 10386/10740 [51:25:41<1:51:01, 18.82s/it]

 97%|█████████▋| 10387/10740 [51:26:01<1:53:28, 19.29s/it]

 97%|█████████▋| 10388/10740 [51:26:22<1:55:17, 19.65s/it]

 97%|█████████▋| 10389/10740 [51:26:35<1:43:26, 17.68s/it]

 97%|█████████▋| 10390/10740 [51:26:54<1:45:49, 18.14s/it]

 97%|█████████▋| 10391/10740 [51:27:13<1:46:30, 18.31s/it]

 97%|█████████▋| 10392/10740 [51:27:30<1:44:54, 18.09s/it]

 97%|█████████▋| 10393/10740 [51:27:49<1:45:02, 18.16s/it]

 97%|█████████▋| 10394/10740 [51:28:11<1:51:30, 19.34s/it]

 97%|█████████▋| 10395/10740 [51:28:30<1:51:50, 19.45s/it]

 97%|█████████▋| 10396/10740 [51:28:45<1:43:40, 18.08s/it]

 97%|█████████▋| 10397/10740 [51:29:05<1:46:19, 18.60s/it]

 97%|█████████▋| 10398/10740 [51:29:23<1:45:23, 18.49s/it]
{'loss': 0.064, 'learning_rate': 5.314367596841673e-09, 'rewards/chosen': -3.6313631534576416, 'rewards/rejected': -11.066149711608887, 'rewards/accuracies': 1.0, 'rewards/margins': 7.434785842895508, 'policy_logps/rejected': -415.5033874511719, 'policy_logps/chosen': -376.9151916503906, 'referece_logps/rejected': -304.8419189453125, 'referece_logps/chosen': -340.6015625, 'logits/rejected': -0.24291446805000305, 'logits/chosen': -0.25884562730789185, 'epoch': 5.81}


 97%|█████████▋| 10400/10740 [51:29:56<1:40:01, 17.65s/it]

 97%|█████████▋| 10401/10740 [51:30:18<1:46:33, 18.86s/it]

 97%|█████████▋| 10402/10740 [51:30:37<1:47:50, 19.14s/it]

 97%|█████████▋| 10403/10740 [51:30:57<1:48:18, 19.28s/it]

 97%|█████████▋| 10404/10740 [51:31:18<1:50:13, 19.68s/it]

 97%|█████████▋| 10405/10740 [51:31:38<1:50:42, 19.83s/it]

 97%|█████████▋| 10406/10740 [51:31:56<1:47:28, 19.31s/it]

 97%|█████████▋| 10407/10740 [51:32:10<1:38:33, 17.76s/it]

 97%|█████████▋| 10408/10740 [51:32:26<1:35:42, 17.30s/it]

 97%|█████████▋| 10409/10740 [51:32:43<1:35:01, 17.23s/it]

 97%|█████████▋| 10410/10740 [51:33:03<1:39:31, 18.10s/it]

 97%|█████████▋| 10411/10740 [51:33:21<1:37:50, 17.84s/it]

 97%|█████████▋| 10412/10740 [51:33:40<1:40:32, 18.39s/it]

 97%|█████████▋| 10413/10740 [51:33:54<1:32:11, 16.92s/it]

 97%|█████████▋| 10414/10740 [51:34:07<1:26:00, 15.83s/it]

 97%|█████████▋| 10415/10740 [51:34:19<1:18:38, 14.52s/it]

 97%|█████████▋| 10416/10740 [51:34:33<1:18:53, 14.61s/it]

 97%|█████████▋| 10417/10740 [51:34:46<1:15:22, 14.00s/it]

 97%|█████████▋| 10418/10740 [51:35:01<1:16:11, 14.20s/it]

 97%|█████████▋| 10419/10740 [51:35:19<1:22:47, 15.48s/it]

 97%|█████████▋| 10420/10740 [51:35:40<1:31:49, 17.22s/it]
{'loss': 0.1228, 'learning_rate': 4.6531524767992044e-09, 'rewards/chosen': -3.366795539855957, 'rewards/rejected': -7.112373352050781, 'rewards/accuracies': 1.0, 'rewards/margins': 3.745577812194824, 'policy_logps/rejected': -358.4007263183594, 'policy_logps/chosen': -464.6895751953125, 'referece_logps/rejected': -287.2770080566406, 'referece_logps/chosen': -431.0216064453125, 'logits/rejected': -0.049905385822057724, 'logits/chosen': -0.15563662350177765, 'epoch': 5.82}


 97%|█████████▋| 10422/10740 [51:36:19<1:37:28, 18.39s/it]

 97%|█████████▋| 10423/10740 [51:36:39<1:39:17, 18.79s/it]

 97%|█████████▋| 10424/10740 [51:36:50<1:27:23, 16.59s/it]

 97%|█████████▋| 10425/10740 [51:37:12<1:35:12, 18.13s/it]

 97%|█████████▋| 10426/10740 [51:37:26<1:27:50, 16.79s/it]

 97%|█████████▋| 10427/10740 [51:37:38<1:19:51, 15.31s/it]
{'loss': 0.1621, 'learning_rate': 4.451953197561642e-09, 'rewards/chosen': -3.059390068054199, 'rewards/rejected': -7.190603256225586, 'rewards/accuracies': 1.0, 'rewards/margins': 4.131213665008545, 'policy_logps/rejected': -247.4879150390625, 'policy_logps/chosen': -316.7265930175781, 'referece_logps/rejected': -175.58187866210938, 'referece_logps/chosen': -286.1326904296875, 'logits/rejected': -0.4012986123561859, 'logits/chosen': -0.5754658579826355, 'epoch': 5.83}


 97%|█████████▋| 10429/10740 [51:38:15<1:27:45, 16.93s/it]

 97%|█████████▋| 10430/10740 [51:38:36<1:33:44, 18.14s/it]

 97%|█████████▋| 10431/10740 [51:38:58<1:39:41, 19.36s/it]

 97%|█████████▋| 10432/10740 [51:39:18<1:39:40, 19.42s/it]

 97%|█████████▋| 10433/10740 [51:39:40<1:43:35, 20.24s/it]

 97%|█████████▋| 10434/10740 [51:40:01<1:44:29, 20.49s/it]

 97%|█████████▋| 10435/10740 [51:40:19<1:40:27, 19.76s/it]

 97%|█████████▋| 10436/10740 [51:40:38<1:39:49, 19.70s/it]

 97%|█████████▋| 10437/10740 [51:40:58<1:39:26, 19.69s/it]
{'loss': 0.1252, 'learning_rate': 4.172222411593207e-09, 'rewards/chosen': -3.3022806644439697, 'rewards/rejected': -7.589406967163086, 'rewards/accuracies': 1.0, 'rewards/margins': 4.287126541137695, 'policy_logps/rejected': -448.2498779296875, 'policy_logps/chosen': -318.41119384765625, 'referece_logps/rejected': -372.3558044433594, 'referece_logps/chosen': -285.38836669921875, 'logits/rejected': -0.1754148006439209, 'logits/chosen': -0.3396536707878113, 'epoch': 5.83}


 97%|█████████▋| 10439/10740 [51:41:32<1:32:55, 18.52s/it]

 97%|█████████▋| 10440/10740 [51:41:51<1:33:19, 18.67s/it]

 97%|█████████▋| 10441/10740 [51:42:08<1:31:00, 18.26s/it]

 97%|█████████▋| 10442/10740 [51:42:26<1:30:36, 18.24s/it]

 97%|█████████▋| 10443/10740 [51:42:45<1:31:32, 18.49s/it]

 97%|█████████▋| 10444/10740 [51:42:58<1:22:05, 16.64s/it]

 97%|█████████▋| 10445/10740 [51:43:14<1:21:45, 16.63s/it]
{'loss': 0.2932, 'learning_rate': 3.9549589130267735e-09, 'rewards/chosen': -3.0651023387908936, 'rewards/rejected': -6.729567527770996, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6644644737243652, 'policy_logps/rejected': -268.1358337402344, 'policy_logps/chosen': -436.10302734375, 'referece_logps/rejected': -200.8401641845703, 'referece_logps/chosen': -405.4520263671875, 'logits/rejected': -0.2993280589580536, 'logits/chosen': -0.7162315249443054, 'epoch': 5.84}


 97%|█████████▋| 10447/10740 [51:43:48<1:21:02, 16.60s/it]

 97%|█████████▋| 10448/10740 [51:44:08<1:26:01, 17.67s/it]

 97%|█████████▋| 10449/10740 [51:44:24<1:23:32, 17.22s/it]
{'loss': 0.2759, 'learning_rate': 3.848501310722008e-09, 'rewards/chosen': -3.5242438316345215, 'rewards/rejected': -7.8766188621521, 'rewards/accuracies': 0.875, 'rewards/margins': 4.352375030517578, 'policy_logps/rejected': -461.3944396972656, 'policy_logps/chosen': -363.79144287109375, 'referece_logps/rejected': -382.6282653808594, 'referece_logps/chosen': -328.5489807128906, 'logits/rejected': 0.5033448338508606, 'logits/chosen': 0.43430230021476746, 'epoch': 5.84}


 97%|█████████▋| 10451/10740 [51:45:04<1:30:42, 18.83s/it]

 97%|█████████▋| 10452/10740 [51:45:24<1:31:55, 19.15s/it]

 97%|█████████▋| 10453/10740 [51:45:43<1:30:56, 19.01s/it]

 97%|█████████▋| 10454/10740 [51:46:03<1:31:20, 19.16s/it]

 97%|█████████▋| 10455/10740 [51:46:25<1:35:05, 20.02s/it]

 97%|█████████▋| 10456/10740 [51:46:47<1:37:53, 20.68s/it]

 97%|█████████▋| 10457/10740 [51:47:07<1:37:04, 20.58s/it]

 97%|█████████▋| 10458/10740 [51:47:27<1:35:20, 20.29s/it]

 97%|█████████▋| 10459/10740 [51:47:48<1:36:25, 20.59s/it]

 97%|█████████▋| 10460/10740 [51:48:10<1:38:30, 21.11s/it]

 97%|█████████▋| 10461/10740 [51:48:27<1:31:42, 19.72s/it]

 97%|█████████▋| 10462/10740 [51:48:47<1:31:35, 19.77s/it]

 97%|█████████▋| 10463/10740 [51:49:05<1:29:11, 19.32s/it]

 97%|█████████▋| 10464/10740 [51:49:23<1:26:49, 18.88s/it]

 97%|█████████▋| 10465/10740 [51:49:40<1:24:22, 18.41s/it]

 97%|█████████▋| 10466/10740 [51:49:59<1:24:19, 18.46s/it]

 97%|█████████▋| 10467/10740 [51:50:18<1:25:04, 18.70s/it]

 97%|█████████▋| 10468/10740 [51:50:36<1:24:01, 18.54s/it]

 97%|█████████▋| 10469/10740 [51:50:57<1:27:17, 19.32s/it]

 97%|█████████▋| 10470/10740 [51:51:16<1:26:25, 19.21s/it]

 97%|█████████▋| 10471/10740 [51:51:32<1:22:08, 18.32s/it]
{'loss': 0.2529, 'learning_rate': 3.2889008354948234e-09, 'rewards/chosen': -3.911489486694336, 'rewards/rejected': -10.354963302612305, 'rewards/accuracies': 1.0, 'rewards/margins': 6.443473815917969, 'policy_logps/rejected': -847.216552734375, 'policy_logps/chosen': -429.1943054199219, 'referece_logps/rejected': -743.6669311523438, 'referece_logps/chosen': -390.0794372558594, 'logits/rejected': -0.6364850401878357, 'logits/chosen': -0.3778945803642273, 'epoch': 5.85}

 98%|█████████▊| 10472/10740 [51:51:52<1:23:56, 18.79s/it]

 98%|█████████▊| 10473/10740 [51:52:12<1:24:55, 19.08s/it]


 98%|█████████▊| 10475/10740 [51:52:51<1:25:20, 19.32s/it]

 98%|█████████▊| 10476/10740 [51:53:05<1:17:13, 17.55s/it]
{'loss': 0.1881, 'learning_rate': 3.167837139641305e-09, 'rewards/chosen': -3.192312240600586, 'rewards/rejected': -7.18564510345459, 'rewards/accuracies': 1.0, 'rewards/margins': 3.993332862854004, 'policy_logps/rejected': -581.5781860351562, 'policy_logps/chosen': -522.1588745117188, 'referece_logps/rejected': -509.72174072265625, 'referece_logps/chosen': -490.23577880859375, 'logits/rejected': 0.04639197885990143, 'logits/chosen': 0.10174638032913208, 'epoch': 5.85}


 98%|█████████▊| 10478/10740 [51:53:34<1:10:07, 16.06s/it]
{'loss': 0.1985, 'learning_rate': 3.1200462962512885e-09, 'rewards/chosen': -4.201877593994141, 'rewards/rejected': -10.431172370910645, 'rewards/accuracies': 1.0, 'rewards/margins': 6.2292938232421875, 'policy_logps/rejected': -385.1605224609375, 'policy_logps/chosen': -436.131103515625, 'referece_logps/rejected': -280.8488464355469, 'referece_logps/chosen': -394.11236572265625, 'logits/rejected': 0.22574400901794434, 'logits/chosen': 0.2461777776479721, 'epoch': 5.85}

 98%|█████████▊| 10479/10740 [51:53:53<1:13:28, 16.89s/it]


 98%|█████████▊| 10481/10740 [51:54:33<1:20:16, 18.60s/it]

 98%|█████████▊| 10482/10740 [51:54:46<1:12:34, 16.88s/it]

 98%|█████████▊| 10483/10740 [51:55:05<1:15:41, 17.67s/it]

 98%|█████████▊| 10484/10740 [51:55:21<1:12:48, 17.06s/it]
{'loss': 0.1339, 'learning_rate': 2.978849883156331e-09, 'rewards/chosen': -4.078105926513672, 'rewards/rejected': -6.855566024780273, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7774605751037598, 'policy_logps/rejected': -522.5883178710938, 'policy_logps/chosen': -455.81585693359375, 'referece_logps/rejected': -454.0326232910156, 'referece_logps/chosen': -415.0347900390625, 'logits/rejected': 0.04247573763132095, 'logits/chosen': 0.01393859088420868, 'epoch': 5.86}

 98%|█████████▊| 10485/10740 [51:55:37<1:10:53, 16.68s/it]

 98%|█████████▊| 10486/10740 [51:55:58<1:17:00, 18.19s/it]


 98%|█████████▊| 10488/10740 [51:56:36<1:16:07, 18.13s/it]
{'loss': 0.2881, 'learning_rate': 2.886532542888065e-09, 'rewards/chosen': -3.3419718742370605, 'rewards/rejected': -6.747582912445068, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4056103229522705, 'policy_logps/rejected': -504.96112060546875, 'policy_logps/chosen': -450.93170166015625, 'referece_logps/rejected': -437.48529052734375, 'referece_logps/chosen': -417.51202392578125, 'logits/rejected': 0.5868775844573975, 'logits/chosen': 0.49927079677581787, 'epoch': 5.86}


 98%|█████████▊| 10490/10740 [51:57:10<1:14:27, 17.87s/it]

 98%|█████████▊| 10491/10740 [51:57:30<1:16:33, 18.45s/it]

 98%|█████████▊| 10492/10740 [51:57:50<1:18:21, 18.96s/it]

 98%|█████████▊| 10493/10740 [51:58:06<1:14:27, 18.09s/it]

 98%|█████████▊| 10494/10740 [51:58:21<1:10:30, 17.20s/it]

 98%|█████████▊| 10495/10740 [51:58:41<1:13:38, 18.03s/it]
{'loss': 0.1752, 'learning_rate': 2.728468838382403e-09, 'rewards/chosen': -4.625941276550293, 'rewards/rejected': -8.898345947265625, 'rewards/accuracies': 1.0, 'rewards/margins': 4.272403717041016, 'policy_logps/rejected': -559.120361328125, 'policy_logps/chosen': -475.415771484375, 'referece_logps/rejected': -470.13690185546875, 'referece_logps/chosen': -429.1563720703125, 'logits/rejected': -0.17381176352500916, 'logits/chosen': -0.19810299575328827, 'epoch': 5.86}


 98%|█████████▊| 10497/10740 [51:59:18<1:12:45, 17.96s/it]

 98%|█████████▊| 10498/10740 [51:59:38<1:14:46, 18.54s/it]

 98%|█████████▊| 10499/10740 [51:59:58<1:15:56, 18.91s/it]

 98%|█████████▊| 10500/10740 [52:00:10<1:07:43, 16.93s/it]
{'loss': 0.2346, 'learning_rate': 2.6182872797702038e-09, 'rewards/chosen': -3.9178855419158936, 'rewards/rejected': -9.214178085327148, 'rewards/accuracies': 0.875, 'rewards/margins': 5.296292304992676, 'policy_logps/rejected': -329.6728820800781, 'policy_logps/chosen': -435.4497375488281, 'referece_logps/rejected': -237.5311279296875, 'referece_logps/chosen': -396.2708435058594, 'logits/rejected': 0.6049596071243286, 'logits/chosen': 0.35011088848114014, 'epoch': 5.87}

 98%|█████████▊| 10501/10740 [52:00:45<1:29:08, 22.38s/it]


 98%|█████████▊| 10503/10740 [52:01:25<1:23:58, 21.26s/it]
{'loss': 0.1819, 'learning_rate': 2.5532669003386354e-09, 'rewards/chosen': -2.749054193496704, 'rewards/rejected': -7.876930236816406, 'rewards/accuracies': 1.0, 'rewards/margins': 5.127876281738281, 'policy_logps/rejected': -317.7419738769531, 'policy_logps/chosen': -266.435302734375, 'referece_logps/rejected': -238.97262573242188, 'referece_logps/chosen': -238.9447479248047, 'logits/rejected': -0.005408618599176407, 'logits/chosen': 0.21789465844631195, 'epoch': 5.87}

 98%|█████████▊| 10504/10740 [52:01:41<1:16:49, 19.53s/it]


 98%|█████████▊| 10506/10740 [52:02:15<1:11:28, 18.33s/it]
{'loss': 0.1894, 'learning_rate': 2.4890630028963078e-09, 'rewards/chosen': -3.5639028549194336, 'rewards/rejected': -8.219216346740723, 'rewards/accuracies': 1.0, 'rewards/margins': 4.6553144454956055, 'policy_logps/rejected': -371.4996337890625, 'policy_logps/chosen': -436.560791015625, 'referece_logps/rejected': -289.3074645996094, 'referece_logps/chosen': -400.92181396484375, 'logits/rejected': -0.035082608461380005, 'logits/chosen': -0.20884238183498383, 'epoch': 5.87}


 98%|█████████▊| 10508/10740 [52:02:47<1:06:16, 17.14s/it]

 98%|█████████▊| 10509/10740 [52:03:04<1:05:43, 17.07s/it]

 98%|█████████▊| 10510/10740 [52:03:25<1:09:12, 18.06s/it]
{'loss': 0.1294, 'learning_rate': 2.404727980141463e-09, 'rewards/chosen': -3.1413192749023438, 'rewards/rejected': -6.557597637176514, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4162771701812744, 'policy_logps/rejected': -311.11529541015625, 'policy_logps/chosen': -310.5538024902344, 'referece_logps/rejected': -245.53932189941406, 'referece_logps/chosen': -279.1405944824219, 'logits/rejected': 0.4558223485946655, 'logits/chosen': 0.5028891563415527, 'epoch': 5.87}

 98%|█████████▊| 10511/10740 [52:03:41<1:07:00, 17.56s/it]


 98%|█████████▊| 10513/10740 [52:04:18<1:08:18, 18.06s/it]

 98%|█████████▊| 10514/10740 [52:04:38<1:10:13, 18.64s/it]

 98%|█████████▊| 10515/10740 [52:04:56<1:09:18, 18.48s/it]

 98%|█████████▊| 10516/10740 [52:05:14<1:08:41, 18.40s/it]

 98%|█████████▊| 10517/10740 [52:05:33<1:08:39, 18.47s/it]

 98%|█████████▊| 10518/10740 [52:05:51<1:07:36, 18.27s/it]

 98%|█████████▊| 10519/10740 [52:06:11<1:09:28, 18.86s/it]

 98%|█████████▊| 10520/10740 [52:06:25<1:03:46, 17.39s/it]
{'loss': 0.1692, 'learning_rate': 2.2002420473558048e-09, 'rewards/chosen': -3.5280683040618896, 'rewards/rejected': -7.241950988769531, 'rewards/accuracies': 0.75, 'rewards/margins': 3.7138824462890625, 'policy_logps/rejected': -464.8397521972656, 'policy_logps/chosen': -382.1813659667969, 'referece_logps/rejected': -392.420166015625, 'referece_logps/chosen': -346.90069580078125, 'logits/rejected': 0.6220657825469971, 'logits/chosen': 0.6690108776092529, 'epoch': 5.88}


 98%|█████████▊| 10522/10740 [52:06:53<55:50, 15.37s/it]

 98%|█████████▊| 10523/10740 [52:07:11<58:40, 16.22s/it]

 98%|█████████▊| 10524/10740 [52:07:30<1:01:15, 17.02s/it]

 98%|█████████▊| 10525/10740 [52:07:48<1:02:41, 17.50s/it]

 98%|█████████▊| 10526/10740 [52:07:59<55:07, 15.46s/it]
{'loss': 0.1964, 'learning_rate': 2.0819064808995646e-09, 'rewards/chosen': -4.586985111236572, 'rewards/rejected': -9.834673881530762, 'rewards/accuracies': 0.875, 'rewards/margins': 5.2476887702941895, 'policy_logps/rejected': -436.1063537597656, 'policy_logps/chosen': -471.0538635253906, 'referece_logps/rejected': -337.75958251953125, 'referece_logps/chosen': -425.18402099609375, 'logits/rejected': -0.09643346071243286, 'logits/chosen': -0.15554948151111603, 'epoch': 5.88}

 98%|█████████▊| 10527/10740 [52:08:18<58:14, 16.41s/it]


 98%|█████████▊| 10529/10740 [52:08:55<1:01:23, 17.46s/it]

 98%|█████████▊| 10530/10740 [52:09:14<1:02:52, 17.96s/it]

 98%|█████████▊| 10531/10740 [52:09:28<58:20, 16.75s/it]

 98%|█████████▊| 10532/10740 [52:09:48<1:01:53, 17.85s/it]

 98%|█████████▊| 10533/10740 [52:10:05<59:55, 17.37s/it]

 98%|█████████▊| 10534/10740 [52:10:21<58:48, 17.13s/it]

 98%|█████████▊| 10535/10740 [52:10:35<54:54, 16.07s/it]

 98%|█████████▊| 10536/10740 [52:10:55<58:28, 17.20s/it]

 98%|█████████▊| 10537/10740 [52:11:10<56:28, 16.69s/it]
{'loss': 0.1634, 'learning_rate': 1.873444561940496e-09, 'rewards/chosen': -2.410886764526367, 'rewards/rejected': -6.242287635803223, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8314013481140137, 'policy_logps/rejected': -467.80517578125, 'policy_logps/chosen': -354.7854919433594, 'referece_logps/rejected': -405.38232421875, 'referece_logps/chosen': -330.6766052246094, 'logits/rejected': -0.46368032693862915, 'logits/chosen': -0.4279288053512573, 'epoch': 5.89}

 98%|█████████▊| 10538/10740 [52:11:29<58:55, 17.50s/it]


 98%|█████████▊| 10540/10740 [52:12:09<1:02:16, 18.68s/it]

 98%|█████████▊| 10541/10740 [52:12:29<1:02:55, 18.97s/it]

 98%|█████████▊| 10542/10740 [52:12:41<55:31, 16.83s/it]
{'loss': 0.1644, 'learning_rate': 1.7823202898175072e-09, 'rewards/chosen': -4.930167198181152, 'rewards/rejected': -9.955452919006348, 'rewards/accuracies': 1.0, 'rewards/margins': 5.025285243988037, 'policy_logps/rejected': -469.3693542480469, 'policy_logps/chosen': -394.9134826660156, 'referece_logps/rejected': -369.8148193359375, 'referece_logps/chosen': -345.61181640625, 'logits/rejected': -0.31377822160720825, 'logits/chosen': -0.2504372000694275, 'epoch': 5.89}


 98%|█████████▊| 10544/10740 [52:13:15<54:54, 16.81s/it]

 98%|█████████▊| 10545/10740 [52:13:35<58:00, 17.85s/it]

 98%|█████████▊| 10546/10740 [52:13:53<57:44, 17.86s/it]

 98%|█████████▊| 10547/10740 [52:14:14<1:00:32, 18.82s/it]

 98%|█████████▊| 10548/10740 [52:14:27<54:13, 16.95s/it]
{'loss': 0.1628, 'learning_rate': 1.675967262013911e-09, 'rewards/chosen': -4.83727502822876, 'rewards/rejected': -9.070073127746582, 'rewards/accuracies': 1.0, 'rewards/margins': 4.232797622680664, 'policy_logps/rejected': -503.1799011230469, 'policy_logps/chosen': -427.0546569824219, 'referece_logps/rejected': -412.47918701171875, 'referece_logps/chosen': -378.681884765625, 'logits/rejected': -0.32564952969551086, 'logits/chosen': -0.2852071225643158, 'epoch': 5.89}


 98%|█████████▊| 10550/10740 [52:14:57<50:27, 15.93s/it]

 98%|█████████▊| 10551/10740 [52:15:13<50:46, 16.12s/it]

 98%|█████████▊| 10552/10740 [52:15:32<53:12, 16.98s/it]

 98%|█████████▊| 10553/10740 [52:15:50<54:06, 17.36s/it]

 98%|█████████▊| 10554/10740 [52:16:05<50:47, 16.39s/it]

 98%|█████████▊| 10555/10740 [52:16:18<48:00, 15.57s/it]
{'loss': 0.2382, 'learning_rate': 1.5560201532052176e-09, 'rewards/chosen': -4.294543743133545, 'rewards/rejected': -7.796523571014404, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5019795894622803, 'policy_logps/rejected': -420.25225830078125, 'policy_logps/chosen': -275.8824157714844, 'referece_logps/rejected': -342.2870178222656, 'referece_logps/chosen': -232.93698120117188, 'logits/rejected': -0.24915069341659546, 'logits/chosen': -0.1036262959241867, 'epoch': 5.9}

 98%|█████████▊| 10556/10740 [52:16:38<51:29, 16.79s/it]


 98%|█████████▊| 10558/10740 [52:17:14<54:12, 17.87s/it]
{'loss': 0.1291, 'learning_rate': 1.5059763823952464e-09, 'rewards/chosen': -3.611901044845581, 'rewards/rejected': -9.625327110290527, 'rewards/accuracies': 1.0, 'rewards/margins': 6.013426303863525, 'policy_logps/rejected': -434.8442077636719, 'policy_logps/chosen': -289.0496520996094, 'referece_logps/rejected': -338.5909423828125, 'referece_logps/chosen': -252.93064880371094, 'logits/rejected': 0.009860590100288391, 'logits/chosen': 0.019822776317596436, 'epoch': 5.9}


 98%|█████████▊| 10560/10740 [52:17:45<50:01, 16.67s/it]

 98%|█████████▊| 10561/10740 [52:18:04<52:21, 17.55s/it]
{'loss': 0.154, 'learning_rate': 1.4567499508572056e-09, 'rewards/chosen': -3.651304006576538, 'rewards/rejected': -7.775371551513672, 'rewards/accuracies': 0.875, 'rewards/margins': 4.124067306518555, 'policy_logps/rejected': -412.98638916015625, 'policy_logps/chosen': -405.4576416015625, 'referece_logps/rejected': -335.232666015625, 'referece_logps/chosen': -368.944580078125, 'logits/rejected': 0.38844361901283264, 'logits/chosen': 0.2920483946800232, 'epoch': 5.9}


 98%|█████████▊| 10563/10740 [52:18:44<55:32, 18.83s/it]
{'loss': 0.1243, 'learning_rate': 1.4243864271317184e-09, 'rewards/chosen': -4.16805362701416, 'rewards/rejected': -7.008273601531982, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8402199745178223, 'policy_logps/rejected': -353.5937805175781, 'policy_logps/chosen': -333.93505859375, 'referece_logps/rejected': -283.51104736328125, 'referece_logps/chosen': -292.2545471191406, 'logits/rejected': 0.07233338057994843, 'logits/chosen': -0.030115246772766113, 'epoch': 5.9}


 98%|█████████▊| 10565/10740 [52:19:23<55:34, 19.05s/it]

 98%|█████████▊| 10566/10740 [52:19:43<56:10, 19.37s/it]

 98%|█████████▊| 10567/10740 [52:20:03<55:53, 19.38s/it]

 98%|█████████▊| 10568/10740 [52:20:23<56:03, 19.55s/it]

 98%|█████████▊| 10569/10740 [52:20:37<51:31, 18.08s/it]
{'loss': 0.1684, 'learning_rate': 1.3294756519650617e-09, 'rewards/chosen': -3.987837314605713, 'rewards/rejected': -8.805432319641113, 'rewards/accuracies': 1.0, 'rewards/margins': 4.817595481872559, 'policy_logps/rejected': -255.3992156982422, 'policy_logps/chosen': -290.3879699707031, 'referece_logps/rejected': -167.3448944091797, 'referece_logps/chosen': -250.5095977783203, 'logits/rejected': -0.593457043170929, 'logits/chosen': -0.8486167192459106, 'epoch': 5.9}


 98%|█████████▊| 10571/10740 [52:21:17<54:02, 19.19s/it]

 98%|█████████▊| 10572/10740 [52:21:37<54:06, 19.32s/it]
{'loss': 0.1913, 'learning_rate': 1.283246470752064e-09, 'rewards/chosen': -3.397871971130371, 'rewards/rejected': -7.700140476226807, 'rewards/accuracies': 1.0, 'rewards/margins': 4.3022685050964355, 'policy_logps/rejected': -403.4924621582031, 'policy_logps/chosen': -380.5147705078125, 'referece_logps/rejected': -326.4910583496094, 'referece_logps/chosen': -346.53607177734375, 'logits/rejected': 0.02335386350750923, 'logits/chosen': -0.0488249808549881, 'epoch': 5.91}


 98%|█████████▊| 10574/10740 [52:22:16<53:34, 19.37s/it]

 98%|█████████▊| 10575/10740 [52:22:28<47:11, 17.16s/it]
{'loss': 0.1375, 'learning_rate': 1.2378348111314885e-09, 'rewards/chosen': -3.1675233840942383, 'rewards/rejected': -7.518276214599609, 'rewards/accuracies': 0.875, 'rewards/margins': 4.350753307342529, 'policy_logps/rejected': -394.23651123046875, 'policy_logps/chosen': -317.87274169921875, 'referece_logps/rejected': -319.0537414550781, 'referece_logps/chosen': -286.197509765625, 'logits/rejected': 0.17227622866630554, 'logits/chosen': 0.21243879199028015, 'epoch': 5.91}


 98%|█████████▊| 10577/10740 [52:23:05<48:11, 17.74s/it]
{'loss': 0.1144, 'learning_rate': 1.208014568431448e-09, 'rewards/chosen': -3.957341194152832, 'rewards/rejected': -6.939497470855713, 'rewards/accuracies': 1.0, 'rewards/margins': 2.982156753540039, 'policy_logps/rejected': -285.8739013671875, 'policy_logps/chosen': -431.53778076171875, 'referece_logps/rejected': -216.47891235351562, 'referece_logps/chosen': -391.96435546875, 'logits/rejected': 0.0566939115524292, 'logits/chosen': -0.1745685189962387, 'epoch': 5.91}


 99%|█████████▊| 10579/10740 [52:23:36<44:21, 16.53s/it]
{'loss': 0.2812, 'learning_rate': 1.178557696045135e-09, 'rewards/chosen': -3.241387128829956, 'rewards/rejected': -7.465950012207031, 'rewards/accuracies': 0.875, 'rewards/margins': 4.224563121795654, 'policy_logps/rejected': -442.1238098144531, 'policy_logps/chosen': -444.61016845703125, 'referece_logps/rejected': -367.4643249511719, 'referece_logps/chosen': -412.1962890625, 'logits/rejected': -0.07237926870584488, 'logits/chosen': -0.3308759033679962, 'epoch': 5.91}


 99%|█████████▊| 10581/10740 [52:24:14<47:15, 17.83s/it]

 99%|█████████▊| 10582/10740 [52:24:27<43:33, 16.54s/it]
{'loss': 0.1461, 'learning_rate': 1.1350537302110551e-09, 'rewards/chosen': -3.5417823791503906, 'rewards/rejected': -8.81102466583252, 'rewards/accuracies': 0.875, 'rewards/margins': 5.269243240356445, 'policy_logps/rejected': -384.610107421875, 'policy_logps/chosen': -481.85406494140625, 'referece_logps/rejected': -296.4998474121094, 'referece_logps/chosen': -446.43621826171875, 'logits/rejected': -0.07840342819690704, 'logits/chosen': -0.04466786980628967, 'epoch': 5.91}

 99%|█████████▊| 10583/10740 [52:24:45<43:52, 16.77s/it]

 99%|█████████▊| 10584/10740 [52:25:07<47:39, 18.33s/it]


 99%|█████████▊| 10586/10740 [52:25:43<47:37, 18.55s/it]

 99%|█████████▊| 10587/10740 [52:25:58<44:11, 17.33s/it]
{'loss': 0.0951, 'learning_rate': 1.0643641219899579e-09, 'rewards/chosen': -3.8722026348114014, 'rewards/rejected': -9.650121688842773, 'rewards/accuracies': 1.0, 'rewards/margins': 5.777919769287109, 'policy_logps/rejected': -397.8341979980469, 'policy_logps/chosen': -473.0806884765625, 'referece_logps/rejected': -301.3329772949219, 'referece_logps/chosen': -434.35870361328125, 'logits/rejected': 0.14896824955940247, 'logits/chosen': -0.011191964149475098, 'epoch': 5.91}

 99%|█████████▊| 10588/10740 [52:26:15<43:29, 17.17s/it]

 99%|█████████▊| 10589/10740 [52:26:31<42:15, 16.79s/it]

 99%|█████████▊| 10590/10740 [52:26:52<45:38, 18.25s/it]

 99%|█████████▊| 10591/10740 [52:27:11<45:47, 18.44s/it]

 99%|█████████▊| 10592/10740 [52:27:27<43:40, 17.71s/it]

 99%|█████████▊| 10593/10740 [52:27:41<40:25, 16.50s/it]

 99%|█████████▊| 10594/10740 [52:28:01<42:27, 17.45s/it]


 99%|█████████▊| 10596/10740 [52:28:42<46:00, 19.17s/it]

 99%|█████████▊| 10597/10740 [52:28:56<42:00, 17.62s/it]

 99%|█████████▊| 10598/10740 [52:29:16<43:38, 18.44s/it]

 99%|█████████▊| 10599/10740 [52:29:36<44:18, 18.85s/it]
{'loss': 0.1514, 'learning_rate': 9.039766316769614e-10, 'rewards/chosen': -4.35640811920166, 'rewards/rejected': -9.065468788146973, 'rewards/accuracies': 1.0, 'rewards/margins': 4.7090606689453125, 'policy_logps/rejected': -394.6735534667969, 'policy_logps/chosen': -559.632568359375, 'referece_logps/rejected': -304.01885986328125, 'referece_logps/chosen': -516.0684814453125, 'logits/rejected': 0.3645835816860199, 'logits/chosen': 0.17341619729995728, 'epoch': 5.92}

 99%|█████████▊| 10600/10740 [52:29:55<43:45, 18.76s/it]

 99%|█████████▊| 10601/10740 [52:30:11<41:47, 18.04s/it]

 99%|█████████▊| 10602/10740 [52:30:32<43:54, 19.09s/it]

 99%|█████████▊| 10603/10740 [52:30:53<44:15, 19.39s/it]

 99%|█████████▊| 10604/10740 [52:31:12<43:38, 19.26s/it]


 99%|█████████▉| 10606/10740 [52:31:52<44:06, 19.75s/it]
{'loss': 0.2055, 'learning_rate': 8.164599986738574e-10, 'rewards/chosen': -3.9000813961029053, 'rewards/rejected': -8.495729446411133, 'rewards/accuracies': 0.875, 'rewards/margins': 4.595648765563965, 'policy_logps/rejected': -452.8620910644531, 'policy_logps/chosen': -295.4044189453125, 'referece_logps/rejected': -367.90478515625, 'referece_logps/chosen': -256.4035949707031, 'logits/rejected': -0.7314497828483582, 'logits/chosen': -0.6365140676498413, 'epoch': 5.93}

 99%|█████████▉| 10607/10740 [52:32:12<43:45, 19.74s/it]


 99%|█████████▉| 10609/10740 [52:32:36<34:32, 15.82s/it]

 99%|█████████▉| 10610/10740 [52:32:52<34:23, 15.87s/it]
{'loss': 0.1883, 'learning_rate': 7.684497894292485e-10, 'rewards/chosen': -3.9735426902770996, 'rewards/rejected': -10.077669143676758, 'rewards/accuracies': 1.0, 'rewards/margins': 6.104127883911133, 'policy_logps/rejected': -450.8698425292969, 'policy_logps/chosen': -361.9029846191406, 'referece_logps/rejected': -350.0931701660156, 'referece_logps/chosen': -322.1675720214844, 'logits/rejected': -0.3513749837875366, 'logits/chosen': -0.3308149576187134, 'epoch': 5.93}


 99%|█████████▉| 10612/10740 [52:33:14<28:24, 13.31s/it]
{'loss': 0.1495, 'learning_rate': 7.449899758211753e-10, 'rewards/chosen': -4.006512641906738, 'rewards/rejected': -9.32729434967041, 'rewards/accuracies': 0.875, 'rewards/margins': 5.320782661437988, 'policy_logps/rejected': -457.5661926269531, 'policy_logps/chosen': -412.3093566894531, 'referece_logps/rejected': -364.29327392578125, 'referece_logps/chosen': -372.2442321777344, 'logits/rejected': -0.09997310489416122, 'logits/chosen': -0.1034649983048439, 'epoch': 5.93}


 99%|█████████▉| 10614/10740 [52:33:48<32:10, 15.32s/it]

 99%|█████████▉| 10615/10740 [52:34:08<35:06, 16.85s/it]

 99%|█████████▉| 10616/10740 [52:34:27<35:39, 17.26s/it]

 99%|█████████▉| 10617/10740 [52:34:48<37:52, 18.47s/it]

 99%|█████████▉| 10618/10740 [52:35:04<36:15, 17.83s/it]

 99%|█████████▉| 10619/10740 [52:35:22<35:53, 17.80s/it]
{'loss': 0.207, 'learning_rate': 6.657435507978482e-10, 'rewards/chosen': -3.9013164043426514, 'rewards/rejected': -7.92345666885376, 'rewards/accuracies': 1.0, 'rewards/margins': 4.0221405029296875, 'policy_logps/rejected': -505.61492919921875, 'policy_logps/chosen': -499.54791259765625, 'referece_logps/rejected': -426.38037109375, 'referece_logps/chosen': -460.5346984863281, 'logits/rejected': 0.737969160079956, 'logits/chosen': 0.7808570265769958, 'epoch': 5.93}


 99%|█████████▉| 10621/10740 [52:36:00<36:11, 18.25s/it]
{'loss': 0.1161, 'learning_rate': 6.439197269429986e-10, 'rewards/chosen': -3.646130323410034, 'rewards/rejected': -10.298330307006836, 'rewards/accuracies': 1.0, 'rewards/margins': 6.652199745178223, 'policy_logps/rejected': -417.31402587890625, 'policy_logps/chosen': -425.4091491699219, 'referece_logps/rejected': -314.3306884765625, 'referece_logps/chosen': -388.94781494140625, 'logits/rejected': 0.04848100617527962, 'logits/chosen': -0.012308094650506973, 'epoch': 5.93}


 99%|█████████▉| 10623/10740 [52:36:40<37:18, 19.13s/it]

 99%|█████████▉| 10624/10740 [52:36:53<33:03, 17.10s/it]
{'loss': 0.217, 'learning_rate': 6.118656977378567e-10, 'rewards/chosen': -3.2117066383361816, 'rewards/rejected': -6.918293476104736, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7065868377685547, 'policy_logps/rejected': -356.8584899902344, 'policy_logps/chosen': -359.8896789550781, 'referece_logps/rejected': -287.6755676269531, 'referece_logps/chosen': -327.7726135253906, 'logits/rejected': -0.21374094486236572, 'logits/chosen': -0.10180917382240295, 'epoch': 5.94}


 99%|█████████▉| 10626/10740 [52:37:30<33:44, 17.76s/it]
{'loss': 0.1221, 'learning_rate': 5.90950827323522e-10, 'rewards/chosen': -4.18196439743042, 'rewards/rejected': -8.15941333770752, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9774491786956787, 'policy_logps/rejected': -525.6527099609375, 'policy_logps/chosen': -388.4751281738281, 'referece_logps/rejected': -444.05859375, 'referece_logps/chosen': -346.6554870605469, 'logits/rejected': 0.42866000533103943, 'logits/chosen': 0.580916166305542, 'epoch': 5.94}

 99%|█████████▉| 10627/10740 [52:37:42<30:02, 15.95s/it]


 99%|█████████▉| 10629/10740 [52:38:17<30:30, 16.49s/it]
{'loss': 0.1381, 'learning_rate': 5.60260264307999e-10, 'rewards/chosen': -3.3681323528289795, 'rewards/rejected': -8.046215057373047, 'rewards/accuracies': 1.0, 'rewards/margins': 4.6780829429626465, 'policy_logps/rejected': -433.48724365234375, 'policy_logps/chosen': -525.7918701171875, 'referece_logps/rejected': -353.02508544921875, 'referece_logps/chosen': -492.11053466796875, 'logits/rejected': 0.6029435992240906, 'logits/chosen': 0.50934237241745, 'epoch': 5.94}


 99%|█████████▉| 10631/10740 [52:38:45<27:45, 15.28s/it]

 99%|█████████▉| 10632/10740 [52:39:00<27:48, 15.45s/it]

 99%|█████████▉| 10633/10740 [52:39:20<29:52, 16.75s/it]
{'loss': 0.2027, 'learning_rate': 5.206121387243411e-10, 'rewards/chosen': -2.6400978565216064, 'rewards/rejected': -6.602543830871582, 'rewards/accuracies': 0.875, 'rewards/margins': 3.962446451187134, 'policy_logps/rejected': -304.6537170410156, 'policy_logps/chosen': -342.47314453125, 'referece_logps/rejected': -238.62828063964844, 'referece_logps/chosen': -316.0721435546875, 'logits/rejected': -0.2177196741104126, 'logits/chosen': -0.23501205444335938, 'epoch': 5.94}

 99%|█████████▉| 10634/10740 [52:39:42<32:01, 18.13s/it]


 99%|█████████▉| 10636/10740 [52:40:19<32:11, 18.57s/it]
{'loss': 0.1225, 'learning_rate': 4.918305449178595e-10, 'rewards/chosen': -3.616676092147827, 'rewards/rejected': -7.773868560791016, 'rewards/accuracies': 0.875, 'rewards/margins': 4.157193183898926, 'policy_logps/rejected': -313.32568359375, 'policy_logps/chosen': -387.99700927734375, 'referece_logps/rejected': -235.5869903564453, 'referece_logps/chosen': -351.8302001953125, 'logits/rejected': 0.0971730649471283, 'logits/chosen': 0.19200339913368225, 'epoch': 5.94}

 99%|█████████▉| 10637/10740 [52:40:38<32:09, 18.73s/it]


 99%|█████████▉| 10639/10740 [52:41:19<32:53, 19.53s/it]
{'loss': 0.2196, 'learning_rate': 4.6386712053481105e-10, 'rewards/chosen': -3.5686211585998535, 'rewards/rejected': -7.603536128997803, 'rewards/accuracies': 0.875, 'rewards/margins': 4.034915447235107, 'policy_logps/rejected': -362.687744140625, 'policy_logps/chosen': -392.6242370605469, 'referece_logps/rejected': -286.65240478515625, 'referece_logps/chosen': -356.93804931640625, 'logits/rejected': -0.015540659427642822, 'logits/chosen': -0.25085654854774475, 'epoch': 5.94}

 99%|█████████▉| 10640/10740 [52:41:38<32:30, 19.50s/it]

 99%|█████████▉| 10641/10740 [52:41:58<32:15, 19.55s/it]

 99%|█████████▉| 10642/10740 [52:42:20<33:05, 20.26s/it]

 99%|█████████▉| 10643/10740 [52:42:36<30:52, 19.09s/it]

 99%|█████████▉| 10644/10740 [52:42:56<30:56, 19.34s/it]

 99%|█████████▉| 10645/10740 [52:43:16<30:50, 19.47s/it]

 99%|█████████▉| 10646/10740 [52:43:38<31:45, 20.27s/it]


 99%|█████████▉| 10648/10740 [52:44:15<29:57, 19.54s/it]

 99%|█████████▉| 10649/10740 [52:44:31<27:57, 18.44s/it]
{'loss': 0.2239, 'learning_rate': 3.765649961969819e-10, 'rewards/chosen': -4.293004989624023, 'rewards/rejected': -8.280455589294434, 'rewards/accuracies': 1.0, 'rewards/margins': 3.98745059967041, 'policy_logps/rejected': -427.0377197265625, 'policy_logps/chosen': -380.1498107910156, 'referece_logps/rejected': -344.233154296875, 'referece_logps/chosen': -337.2197265625, 'logits/rejected': 0.5869009494781494, 'logits/chosen': 0.6636309623718262, 'epoch': 5.95}

 99%|█████████▉| 10650/10740 [52:44:52<28:49, 19.21s/it]


 99%|█████████▉| 10652/10740 [52:45:21<24:57, 17.02s/it]

 99%|█████████▉| 10653/10740 [52:45:37<24:28, 16.88s/it]
{'loss': 0.1668, 'learning_rate': 3.4418981254569213e-10, 'rewards/chosen': -4.368945598602295, 'rewards/rejected': -8.577649116516113, 'rewards/accuracies': 1.0, 'rewards/margins': 4.208703517913818, 'policy_logps/rejected': -419.117919921875, 'policy_logps/chosen': -360.7101745605469, 'referece_logps/rejected': -333.3414306640625, 'referece_logps/chosen': -317.02069091796875, 'logits/rejected': 0.22748948633670807, 'logits/chosen': 0.09063943475484848, 'epoch': 5.95}

 99%|█████████▉| 10654/10740 [52:45:54<24:12, 16.89s/it]


 99%|█████████▉| 10656/10740 [52:46:37<26:49, 19.16s/it]
{'loss': 0.2676, 'learning_rate': 3.2086309425061185e-10, 'rewards/chosen': -3.6852071285247803, 'rewards/rejected': -6.019280910491943, 'rewards/accuracies': 0.875, 'rewards/margins': 2.334073543548584, 'policy_logps/rejected': -381.2431335449219, 'policy_logps/chosen': -367.548828125, 'referece_logps/rejected': -321.05029296875, 'referece_logps/chosen': -330.6967468261719, 'logits/rejected': -0.48800933361053467, 'logits/chosen': -0.5758808255195618, 'epoch': 5.95}

 99%|█████████▉| 10657/10740 [52:46:56<26:14, 18.98s/it]

 99%|█████████▉| 10658/10740 [52:47:15<26:18, 19.25s/it]

 99%|█████████▉| 10659/10740 [52:47:34<25:33, 18.94s/it]

 99%|█████████▉| 10660/10740 [52:47:54<25:38, 19.23s/it]

 99%|█████████▉| 10661/10740 [52:48:10<24:18, 18.46s/it]


 99%|█████████▉| 10663/10740 [52:48:47<23:26, 18.26s/it]
{'loss': 0.1946, 'learning_rate': 2.696164308710891e-10, 'rewards/chosen': -3.47454833984375, 'rewards/rejected': -7.610936164855957, 'rewards/accuracies': 1.0, 'rewards/margins': 4.136387825012207, 'policy_logps/rejected': -260.0128173828125, 'policy_logps/chosen': -483.37469482421875, 'referece_logps/rejected': -183.90345764160156, 'referece_logps/chosen': -448.62921142578125, 'logits/rejected': 0.11116302758455276, 'logits/chosen': 0.0921393409371376, 'epoch': 5.96}

 99%|█████████▉| 10664/10740 [52:49:06<23:23, 18.46s/it]

 99%|█████████▉| 10665/10740 [52:49:24<23:00, 18.40s/it]

 99%|█████████▉| 10666/10740 [52:49:42<22:24, 18.17s/it]

 99%|█████████▉| 10667/10740 [52:49:55<20:18, 16.70s/it]


 99%|█████████▉| 10669/10740 [52:50:28<18:59, 16.05s/it]
{'loss': 0.2209, 'learning_rate': 2.292368992697913e-10, 'rewards/chosen': -5.1687397956848145, 'rewards/rejected': -6.769184589385986, 'rewards/accuracies': 0.625, 'rewards/margins': 1.60044527053833, 'policy_logps/rejected': -355.3926696777344, 'policy_logps/chosen': -393.29144287109375, 'referece_logps/rejected': -287.7008056640625, 'referece_logps/chosen': -341.6040954589844, 'logits/rejected': -0.7310018539428711, 'logits/chosen': -0.642320454120636, 'epoch': 5.96}

 99%|█████████▉| 10670/10740 [52:50:49<20:28, 17.55s/it]

 99%|█████████▉| 10671/10740 [52:51:07<20:25, 17.76s/it]

 99%|█████████▉| 10672/10740 [52:51:27<20:53, 18.44s/it]

 99%|█████████▉| 10673/10740 [52:51:45<20:28, 18.33s/it]

 99%|█████████▉| 10674/10740 [52:52:03<20:04, 18.24s/it]

 99%|█████████▉| 10675/10740 [52:52:18<18:48, 17.36s/it]

 99%|█████████▉| 10676/10740 [52:52:33<17:45, 16.65s/it]


 99%|█████████▉| 10678/10740 [52:53:08<17:57, 17.37s/it]
{'loss': 0.1648, 'learning_rate': 1.7480552181214915e-10, 'rewards/chosen': -3.59232234954834, 'rewards/rejected': -8.598776817321777, 'rewards/accuracies': 1.0, 'rewards/margins': 5.006454944610596, 'policy_logps/rejected': -440.27099609375, 'policy_logps/chosen': -332.12286376953125, 'referece_logps/rejected': -354.2831726074219, 'referece_logps/chosen': -296.1996154785156, 'logits/rejected': 0.048223137855529785, 'logits/chosen': 0.1901262253522873, 'epoch': 5.97}

 99%|█████████▉| 10679/10740 [52:53:20<16:14, 15.98s/it]

 99%|█████████▉| 10680/10740 [52:53:31<14:23, 14.39s/it]

 99%|█████████▉| 10681/10740 [52:53:51<15:40, 15.94s/it]

 99%|█████████▉| 10682/10740 [52:54:05<14:51, 15.36s/it]

 99%|█████████▉| 10683/10740 [52:54:16<13:31, 14.23s/it]

 99%|█████████▉| 10684/10740 [52:54:32<13:42, 14.68s/it]


 99%|█████████▉| 10686/10740 [52:55:02<13:02, 14.49s/it]
{'loss': 0.1319, 'learning_rate': 1.3260574581719364e-10, 'rewards/chosen': -3.673035144805908, 'rewards/rejected': -7.994531154632568, 'rewards/accuracies': 1.0, 'rewards/margins': 4.321495056152344, 'policy_logps/rejected': -314.2641296386719, 'policy_logps/chosen': -296.8271789550781, 'referece_logps/rejected': -234.31881713867188, 'referece_logps/chosen': -260.0968322753906, 'logits/rejected': 0.44913986325263977, 'logits/chosen': 0.49257928133010864, 'epoch': 5.97}

100%|█████████▉| 10687/10740 [52:55:23<14:31, 16.44s/it]

100%|█████████▉| 10688/10740 [52:55:42<15:00, 17.33s/it]


100%|█████████▉| 10690/10740 [52:56:10<13:13, 15.86s/it]
{'loss': 0.1394, 'learning_rate': 1.1368841214676183e-10, 'rewards/chosen': -4.261745929718018, 'rewards/rejected': -7.298988342285156, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0372424125671387, 'policy_logps/rejected': -314.8987731933594, 'policy_logps/chosen': -358.3056640625, 'referece_logps/rejected': -241.90887451171875, 'referece_logps/chosen': -315.68817138671875, 'logits/rejected': -0.04823434725403786, 'logits/chosen': 0.00217399001121521, 'epoch': 5.97}

100%|█████████▉| 10691/10740 [52:56:26<13:01, 15.95s/it]


100%|█████████▉| 10693/10740 [52:56:58<12:39, 16.17s/it]
{'loss': 0.144, 'learning_rate': 1.0045530253732781e-10, 'rewards/chosen': -4.621943473815918, 'rewards/rejected': -7.624890327453613, 'rewards/accuracies': 0.875, 'rewards/margins': 3.002946615219116, 'policy_logps/rejected': -361.42828369140625, 'policy_logps/chosen': -336.09716796875, 'referece_logps/rejected': -285.17938232421875, 'referece_logps/chosen': -289.8777770996094, 'logits/rejected': -0.4545077979564667, 'logits/chosen': -0.5511265397071838, 'epoch': 5.97}

100%|█████████▉| 10694/10740 [52:57:10<11:30, 15.00s/it]


100%|█████████▉| 10696/10740 [52:57:50<12:48, 17.47s/it]
{'loss': 0.1502, 'learning_rate': 8.804068272016962e-11, 'rewards/chosen': -3.5775749683380127, 'rewards/rejected': -5.653445720672607, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0758705139160156, 'policy_logps/rejected': -404.7987060546875, 'policy_logps/chosen': -485.0334777832031, 'referece_logps/rejected': -348.26422119140625, 'referece_logps/chosen': -449.2576904296875, 'logits/rejected': -0.7244158983230591, 'logits/chosen': -0.5331634283065796, 'epoch': 5.98}

100%|█████████▉| 10697/10740 [52:58:11<13:21, 18.64s/it]

100%|█████████▉| 10698/10740 [52:58:29<12:46, 18.26s/it]

100%|█████████▉| 10699/10740 [52:58:49<12:53, 18.87s/it]

100%|█████████▉| 10700/10740 [52:59:10<13:05, 19.63s/it]

100%|█████████▉| 10701/10740 [52:59:28<12:25, 19.13s/it]

100%|█████████▉| 10702/10740 [52:59:48<12:16, 19.39s/it]

100%|█████████▉| 10703/10740 [53:00:10<12:22, 20.06s/it]

100%|█████████▉| 10704/10740 [53:00:32<12:20, 20.58s/it]

100%|█████████▉| 10705/10740 [53:00:50<11:32, 19.79s/it]

100%|█████████▉| 10706/10740 [53:01:05<10:25, 18.40s/it]

100%|█████████▉| 10707/10740 [53:01:21<09:40, 17.60s/it]

100%|█████████▉| 10708/10740 [53:01:39<09:29, 17.79s/it]


100%|█████████▉| 10710/10740 [53:02:10<08:27, 16.92s/it]
{'loss': 0.2119, 'learning_rate': 4.092832470858898e-11, 'rewards/chosen': -4.797154903411865, 'rewards/rejected': -7.47482967376709, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6776747703552246, 'policy_logps/rejected': -363.6005859375, 'policy_logps/chosen': -443.1959533691406, 'referece_logps/rejected': -288.852294921875, 'referece_logps/chosen': -395.224365234375, 'logits/rejected': 0.07427673041820526, 'logits/chosen': 0.009795665740966797, 'epoch': 5.98}

100%|█████████▉| 10711/10740 [53:02:30<08:33, 17.70s/it]

100%|█████████▉| 10712/10740 [53:02:45<07:57, 17.04s/it]


100%|█████████▉| 10714/10740 [53:03:18<07:08, 16.47s/it]

100%|█████████▉| 10715/10740 [53:03:38<07:17, 17.48s/it]

100%|█████████▉| 10716/10740 [53:04:00<07:27, 18.67s/it]

100%|█████████▉| 10717/10740 [53:04:18<07:08, 18.61s/it]

100%|█████████▉| 10718/10740 [53:04:34<06:30, 17.74s/it]

100%|█████████▉| 10719/10740 [53:04:46<05:34, 15.93s/it]

100%|█████████▉| 10720/10740 [53:05:05<05:40, 17.04s/it]

100%|█████████▉| 10721/10740 [53:05:25<05:39, 17.85s/it]

100%|█████████▉| 10722/10740 [53:05:43<05:25, 18.07s/it]

100%|█████████▉| 10723/10740 [53:06:02<05:08, 18.16s/it]

100%|█████████▉| 10724/10740 [53:06:21<04:57, 18.58s/it]

100%|█████████▉| 10725/10740 [53:06:40<04:39, 18.62s/it]

100%|█████████▉| 10726/10740 [53:07:00<04:24, 18.88s/it]

100%|█████████▉| 10727/10740 [53:07:20<04:11, 19.35s/it]

100%|█████████▉| 10728/10740 [53:07:40<03:53, 19.49s/it]

100%|█████████▉| 10729/10740 [53:07:58<03:29, 19.08s/it]

100%|█████████▉| 10730/10740 [53:08:16<03:06, 18.69s/it]

100%|█████████▉| 10731/10740 [53:08:35<02:50, 18.97s/it]

100%|█████████▉| 10732/10740 [53:08:57<02:37, 19.66s/it]

100%|█████████▉| 10733/10740 [53:09:14<02:13, 19.06s/it]

100%|█████████▉| 10734/10740 [53:09:29<01:46, 17.82s/it]

100%|█████████▉| 10735/10740 [53:09:46<01:27, 17.45s/it]

100%|█████████▉| 10736/10740 [53:10:06<01:12, 18.16s/it]

100%|█████████▉| 10737/10740 [53:10:27<00:57, 19.12s/it]

100%|█████████▉| 10738/10740 [53:10:46<00:38, 19.17s/it]

100%|█████████▉| 10739/10740 [53:11:01<00:17, 17.90s/it]

100%|██████████| 10740/10740 [53:11:14<00:00, 17.83s/it]
{'loss': 0.228, 'learning_rate': 0.0, 'rewards/chosen': -3.6168978214263916, 'rewards/rejected': -7.787077903747559, 'rewards/accuracies': 0.875, 'rewards/margins': 4.17017936706543, 'policy_logps/rejected': -386.16705322265625, 'policy_logps/chosen': -483.05975341796875, 'referece_logps/rejected': -308.2962646484375, 'referece_logps/chosen': -446.8907775878906, 'logits/rejected': 0.3330582082271576, 'logits/chosen': 0.20356440544128418, 'epoch': 6.0}
{'train_runtime': 191482.6513, 'train_samples_per_second': 3.588, 'train_steps_per_second': 0.056, 'train_loss': 0.26257530707532967, 'epoch': 6.0}