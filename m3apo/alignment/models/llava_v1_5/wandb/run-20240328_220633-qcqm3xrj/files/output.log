  0%|          | 0/2685 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/code/mm/MAPO/m3apo/alignment/trainer/llava_dpo_trainer.py:179: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/2685 [00:22<16:43:05, 22.42s/it]

  0%|          | 2/2685 [00:44<16:44:31, 22.46s/it]
[2024-03-28 22:07:24,576] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6931, 'learning_rate': 4.938271604938271e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'policy_logps/rejected': -413.13543701171875, 'policy_logps/chosen': -385.1992492675781, 'referece_logps/rejected': -413.13543701171875, 'referece_logps/chosen': -385.1992492675781, 'logits/rejected': -0.24519029259681702, 'logits/chosen': -0.44251513481140137, 'epoch': 0.0}

  0%|          | 3/2685 [01:08<17:07:01, 22.98s/it]

  0%|          | 4/2685 [01:23<14:51:54, 19.96s/it]

  0%|          | 5/2685 [01:49<16:19:08, 21.92s/it]

  0%|          | 6/2685 [02:06<15:01:13, 20.18s/it]

  0%|          | 7/2685 [02:27<15:19:44, 20.61s/it]

  0%|          | 8/2685 [02:47<15:08:31, 20.36s/it]

  0%|          | 9/2685 [03:05<14:36:09, 19.64s/it]

  0%|          | 10/2685 [03:20<13:35:44, 18.30s/it]

  0%|          | 11/2685 [03:40<13:50:40, 18.64s/it]

  0%|          | 12/2685 [04:00<14:13:23, 19.16s/it]

  0%|          | 13/2685 [04:22<14:47:17, 19.92s/it]

  1%|          | 14/2685 [04:44<15:13:32, 20.52s/it]

  1%|          | 15/2685 [05:00<14:17:45, 19.28s/it]

  1%|          | 16/2685 [05:22<14:57:09, 20.17s/it]

  1%|          | 17/2685 [05:44<15:15:53, 20.60s/it]

  1%|          | 18/2685 [06:02<14:45:23, 19.92s/it]

  1%|          | 19/2685 [06:19<14:03:41, 18.99s/it]

  1%|          | 20/2685 [06:41<14:39:17, 19.80s/it]
[2024-03-28 22:13:20,782] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 21/2685 [07:00<14:27:29, 19.54s/it]
[2024-03-28 22:13:39,717] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 22/2685 [07:21<14:56:43, 20.20s/it]

  1%|          | 23/2685 [07:42<15:02:00, 20.33s/it]

  1%|          | 24/2685 [08:00<14:35:41, 19.74s/it]

  1%|          | 25/2685 [08:20<14:40:52, 19.87s/it]

  1%|          | 26/2685 [08:39<14:25:28, 19.53s/it]

  1%|          | 27/2685 [08:58<14:13:11, 19.26s/it]

  1%|          | 28/2685 [09:22<15:19:52, 20.77s/it]

  1%|          | 29/2685 [09:40<14:46:30, 20.03s/it]

  1%|          | 30/2685 [10:02<15:05:39, 20.47s/it]

  1%|          | 31/2685 [10:21<14:51:36, 20.16s/it]

  1%|          | 32/2685 [10:43<15:13:36, 20.66s/it]

  1%|          | 33/2685 [11:02<14:46:40, 20.06s/it]

  1%|▏         | 34/2685 [11:23<15:05:48, 20.50s/it]

  1%|▏         | 35/2685 [11:45<15:25:46, 20.96s/it]
[2024-03-28 22:18:25,582] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|▏         | 36/2685 [12:06<15:22:23, 20.89s/it]
[2024-03-28 22:18:46,313] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|▏         | 37/2685 [12:27<15:19:14, 20.83s/it]

  1%|▏         | 38/2685 [12:46<15:03:19, 20.48s/it]

  1%|▏         | 39/2685 [13:07<14:59:35, 20.40s/it]

  1%|▏         | 40/2685 [13:27<15:01:27, 20.45s/it]

  2%|▏         | 41/2685 [13:47<14:52:07, 20.24s/it]

  2%|▏         | 42/2685 [14:05<14:23:50, 19.61s/it]

  2%|▏         | 43/2685 [14:26<14:40:13, 19.99s/it]
[2024-03-28 22:21:06,206] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 44/2685 [14:46<14:40:25, 20.00s/it]
[2024-03-28 22:21:26,237] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 45/2685 [15:06<14:45:18, 20.12s/it]

  2%|▏         | 46/2685 [15:30<15:33:23, 21.22s/it]
[2024-03-28 22:22:10,424] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 47/2685 [15:53<15:58:04, 21.79s/it]

  2%|▏         | 48/2685 [16:12<15:11:18, 20.74s/it]

  2%|▏         | 49/2685 [16:33<15:16:06, 20.85s/it]

  2%|▏         | 50/2685 [16:53<15:13:38, 20.80s/it]

  2%|▏         | 51/2685 [17:14<15:15:12, 20.85s/it]

  2%|▏         | 52/2685 [17:36<15:18:22, 20.93s/it]

  2%|▏         | 53/2685 [17:59<15:54:55, 21.77s/it]
[2024-03-28 22:24:39,427] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 54/2685 [18:16<14:43:55, 20.16s/it]

  2%|▏         | 55/2685 [18:29<13:20:06, 18.25s/it]

  2%|▏         | 56/2685 [18:51<14:00:36, 19.18s/it]

  2%|▏         | 57/2685 [19:14<14:53:17, 20.39s/it]

  2%|▏         | 58/2685 [19:37<15:29:00, 21.22s/it]

  2%|▏         | 59/2685 [20:02<16:09:33, 22.15s/it]
[2024-03-28 22:26:41,686] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 60/2685 [20:20<15:17:43, 20.98s/it]

  2%|▏         | 61/2685 [20:40<15:11:53, 20.85s/it]

  2%|▏         | 62/2685 [21:01<15:08:29, 20.78s/it]

  2%|▏         | 63/2685 [21:22<15:15:08, 20.94s/it]
[2024-03-28 22:28:02,410] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 64/2685 [21:43<15:08:49, 20.80s/it]
[2024-03-28 22:28:22,897] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 65/2685 [22:02<14:45:12, 20.27s/it]

  2%|▏         | 66/2685 [22:24<15:14:47, 20.96s/it]

  2%|▏         | 67/2685 [22:43<14:38:09, 20.13s/it]

  3%|▎         | 68/2685 [23:05<15:12:16, 20.92s/it]

  3%|▎         | 69/2685 [23:21<14:03:57, 19.36s/it]

  3%|▎         | 70/2685 [23:39<13:40:53, 18.84s/it]

  3%|▎         | 71/2685 [24:00<14:12:54, 19.58s/it]
[2024-03-28 22:30:40,070] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 72/2685 [24:16<13:27:11, 18.53s/it]

  3%|▎         | 73/2685 [24:37<14:03:49, 19.38s/it]

  3%|▎         | 74/2685 [24:58<14:19:21, 19.75s/it]

  3%|▎         | 75/2685 [25:18<14:19:36, 19.76s/it]

  3%|▎         | 76/2685 [25:38<14:29:23, 19.99s/it]

  3%|▎         | 77/2685 [25:59<14:44:14, 20.34s/it]

  3%|▎         | 78/2685 [26:22<15:07:47, 20.89s/it]
{'loss': 0.6466, 'learning_rate': 1.9259259259259256e-06, 'rewards/chosen': 0.04350852966308594, 'rewards/rejected': -0.14267940819263458, 'rewards/accuracies': 0.75, 'rewards/margins': 0.18618792295455933, 'policy_logps/rejected': -335.3385314941406, 'policy_logps/chosen': -348.6846923828125, 'referece_logps/rejected': -333.9117126464844, 'referece_logps/chosen': -349.11981201171875, 'logits/rejected': -0.09303507208824158, 'logits/chosen': -0.1641097068786621, 'epoch': 0.09}


  3%|▎         | 80/2685 [27:00<14:22:22, 19.86s/it]

  3%|▎         | 81/2685 [27:21<14:37:02, 20.21s/it]

  3%|▎         | 82/2685 [27:42<14:55:47, 20.65s/it]
[2024-03-28 22:34:22,565] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6288, 'learning_rate': 1.9999992722407448e-06, 'rewards/chosen': 0.1099800169467926, 'rewards/rejected': -0.16065654158592224, 'rewards/accuracies': 0.875, 'rewards/margins': 0.27063655853271484, 'policy_logps/rejected': -400.8535461425781, 'policy_logps/chosen': -374.7281799316406, 'referece_logps/rejected': -399.24700927734375, 'referece_logps/chosen': -375.8279724121094, 'logits/rejected': -0.7066694498062134, 'logits/chosen': -0.8160285353660583, 'epoch': 0.09}
[2024-03-28 22:34:43,822] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  3%|▎         | 84/2685 [28:25<15:13:22, 21.07s/it]

  3%|▎         | 85/2685 [28:46<15:02:47, 20.83s/it]

  3%|▎         | 86/2685 [29:09<15:37:05, 21.63s/it]
[2024-03-28 22:35:49,232] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 87/2685 [29:31<15:36:55, 21.64s/it]
[2024-03-28 22:36:10,880] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 88/2685 [29:50<15:08:12, 20.98s/it]

  3%|▎         | 89/2685 [30:11<15:03:31, 20.88s/it]

  3%|▎         | 90/2685 [30:31<14:58:44, 20.78s/it]
{'loss': 0.6265, 'learning_rate': 1.999941052072329e-06, 'rewards/chosen': 0.15074767172336578, 'rewards/rejected': -0.03627758100628853, 'rewards/accuracies': 0.875, 'rewards/margins': 0.187025249004364, 'policy_logps/rejected': -243.81253051757812, 'policy_logps/chosen': -274.70111083984375, 'referece_logps/rejected': -243.44976806640625, 'referece_logps/chosen': -276.2085876464844, 'logits/rejected': -0.7457668781280518, 'logits/chosen': -0.7845746278762817, 'epoch': 0.1}


  3%|▎         | 92/2685 [31:09<14:02:35, 19.50s/it]

  3%|▎         | 93/2685 [31:26<13:32:01, 18.80s/it]

  4%|▎         | 94/2685 [31:46<13:46:12, 19.13s/it]

  4%|▎         | 95/2685 [32:04<13:36:43, 18.92s/it]

  4%|▎         | 96/2685 [32:25<14:03:13, 19.54s/it]

  4%|▎         | 97/2685 [32:46<14:16:50, 19.87s/it]
[2024-03-28 22:39:25,837] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▎         | 98/2685 [33:06<14:23:17, 20.02s/it]

  4%|▎         | 99/2685 [33:27<14:40:56, 20.44s/it]

  4%|▎         | 100/2685 [33:46<14:21:40, 20.00s/it]

  4%|▍         | 101/2685 [34:08<14:41:38, 20.47s/it]

  4%|▍         | 102/2685 [34:29<14:42:02, 20.49s/it]
[2024-03-28 22:41:08,715] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 103/2685 [34:49<14:42:17, 20.50s/it]

  4%|▍         | 104/2685 [35:06<13:56:44, 19.45s/it]
{'loss': 0.5921, 'learning_rate': 1.9996150400088572e-06, 'rewards/chosen': -0.035051919519901276, 'rewards/rejected': -0.28006038069725037, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24500848352909088, 'policy_logps/rejected': -315.3715515136719, 'policy_logps/chosen': -329.17498779296875, 'referece_logps/rejected': -312.57098388671875, 'referece_logps/chosen': -328.8244323730469, 'logits/rejected': 0.07451513409614563, 'logits/chosen': -0.031062930822372437, 'epoch': 0.12}


  4%|▍         | 106/2685 [35:46<14:12:46, 19.84s/it]
[2024-03-28 22:42:26,164] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 107/2685 [36:07<14:25:32, 20.14s/it]

  4%|▍         | 108/2685 [36:27<14:26:15, 20.17s/it]
{'loss': 0.6016, 'learning_rate': 1.9994695103484374e-06, 'rewards/chosen': 0.05953216180205345, 'rewards/rejected': -0.08574419468641281, 'rewards/accuracies': 0.625, 'rewards/margins': 0.14527633786201477, 'policy_logps/rejected': -212.44993591308594, 'policy_logps/chosen': -200.865478515625, 'referece_logps/rejected': -211.59249877929688, 'referece_logps/chosen': -201.46080017089844, 'logits/rejected': -0.45479899644851685, 'logits/chosen': -0.5082816481590271, 'epoch': 0.12}
[2024-03-28 22:43:28,896] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 110/2685 [37:08<14:29:39, 20.26s/it]

  4%|▍         | 111/2685 [37:25<13:44:34, 19.22s/it]
[2024-03-28 22:44:05,132] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 112/2685 [37:44<13:36:35, 19.04s/it]
{'loss': 0.5728, 'learning_rate': 1.9993007047883984e-06, 'rewards/chosen': -0.10028763860464096, 'rewards/rejected': -0.34061574935913086, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2403281331062317, 'policy_logps/rejected': -306.01287841796875, 'policy_logps/chosen': -317.8890075683594, 'referece_logps/rejected': -302.60675048828125, 'referece_logps/chosen': -316.8861083984375, 'logits/rejected': -0.5839548707008362, 'logits/chosen': -0.7382174134254456, 'epoch': 0.13}


  4%|▍         | 114/2685 [38:25<14:20:34, 20.08s/it]
[2024-03-28 22:45:05,321] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 115/2685 [38:48<14:57:46, 20.96s/it]
[2024-03-28 22:45:28,325] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5574, 'learning_rate': 1.9991588281537217e-06, 'rewards/chosen': -0.031033894047141075, 'rewards/rejected': -0.21486645936965942, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1838325560092926, 'policy_logps/rejected': -335.54547119140625, 'policy_logps/chosen': -378.58416748046875, 'referece_logps/rejected': -333.3968200683594, 'referece_logps/chosen': -378.2738342285156, 'logits/rejected': -0.8100200891494751, 'logits/chosen': -0.826791524887085, 'epoch': 0.13}
[2024-03-28 22:45:50,753] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 117/2685 [39:31<15:04:28, 21.13s/it]

  4%|▍         | 118/2685 [39:51<14:49:14, 20.78s/it]
{'loss': 0.5195, 'learning_rate': 1.9990038628842216e-06, 'rewards/chosen': -0.22137975692749023, 'rewards/rejected': -0.763681173324585, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5423014163970947, 'policy_logps/rejected': -429.93585205078125, 'policy_logps/chosen': -375.13922119140625, 'referece_logps/rejected': -422.29901123046875, 'referece_logps/chosen': -372.9254150390625, 'logits/rejected': -0.2468007504940033, 'logits/chosen': -0.28283578157424927, 'epoch': 0.13}


  4%|▍         | 120/2685 [40:32<14:33:51, 20.44s/it]

  5%|▍         | 121/2685 [40:53<14:48:50, 20.80s/it]

  5%|▍         | 122/2685 [41:16<15:15:18, 21.43s/it]

  5%|▍         | 123/2685 [41:40<15:49:03, 22.23s/it]

  5%|▍         | 124/2685 [41:58<14:46:53, 20.78s/it]

  5%|▍         | 125/2685 [42:18<14:42:00, 20.67s/it]
{'loss': 0.4969, 'learning_rate': 1.99859138873288e-06, 'rewards/chosen': -0.0639645904302597, 'rewards/rejected': -0.4899905323982239, 'rewards/accuracies': 1.0, 'rewards/margins': 0.42602595686912537, 'policy_logps/rejected': -290.4046936035156, 'policy_logps/chosen': -287.46295166015625, 'referece_logps/rejected': -285.5047912597656, 'referece_logps/chosen': -286.82330322265625, 'logits/rejected': -0.5307092666625977, 'logits/chosen': -0.5696067810058594, 'epoch': 0.14}

  5%|▍         | 126/2685 [42:35<13:54:38, 19.57s/it]

  5%|▍         | 127/2685 [42:55<13:59:21, 19.69s/it]

  5%|▍         | 128/2685 [43:15<14:01:40, 19.75s/it]


  5%|▍         | 130/2685 [43:55<14:03:45, 19.81s/it]

  5%|▍         | 131/2685 [44:12<13:25:15, 18.92s/it]

  5%|▍         | 132/2685 [44:35<14:20:27, 20.22s/it]

  5%|▍         | 133/2685 [44:52<13:37:15, 19.21s/it]
{'loss': 0.5332, 'learning_rate': 1.9980327840635064e-06, 'rewards/chosen': -0.08542689681053162, 'rewards/rejected': -0.7199451923370361, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6345182657241821, 'policy_logps/rejected': -377.234375, 'policy_logps/chosen': -324.771484375, 'referece_logps/rejected': -370.034912109375, 'referece_logps/chosen': -323.917236328125, 'logits/rejected': -1.1500827074050903, 'logits/chosen': -1.114800214767456, 'epoch': 0.15}


  5%|▌         | 135/2685 [45:37<14:40:44, 20.72s/it]
{'loss': 0.5934, 'learning_rate': 1.997878604232291e-06, 'rewards/chosen': -0.3640163540840149, 'rewards/rejected': -0.4820135235786438, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11799715459346771, 'policy_logps/rejected': -312.5775451660156, 'policy_logps/chosen': -331.09259033203125, 'referece_logps/rejected': -307.7574157714844, 'referece_logps/chosen': -327.45245361328125, 'logits/rejected': -0.43522822856903076, 'logits/chosen': -0.4263417422771454, 'epoch': 0.15}


  5%|▌         | 137/2685 [46:18<14:41:33, 20.76s/it]

  5%|▌         | 138/2685 [46:37<14:20:59, 20.28s/it]

  5%|▌         | 139/2685 [46:58<14:18:53, 20.24s/it]

  5%|▌         | 140/2685 [47:14<13:30:34, 19.11s/it]
{'loss': 0.528, 'learning_rate': 1.9974677391716964e-06, 'rewards/chosen': -0.21347656846046448, 'rewards/rejected': -0.3272615373134613, 'rewards/accuracies': 0.625, 'rewards/margins': 0.11378498375415802, 'policy_logps/rejected': -284.5733642578125, 'policy_logps/chosen': -305.3953857421875, 'referece_logps/rejected': -281.3007507324219, 'referece_logps/chosen': -303.2606506347656, 'logits/rejected': -1.5045020580291748, 'logits/chosen': -1.569396734237671, 'epoch': 0.16}


  5%|▌         | 142/2685 [47:58<14:37:27, 20.70s/it]

  5%|▌         | 143/2685 [48:18<14:26:33, 20.45s/it]

  5%|▌         | 144/2685 [48:39<14:29:47, 20.54s/it]

  5%|▌         | 145/2685 [48:55<13:27:32, 19.08s/it]

  5%|▌         | 146/2685 [49:15<13:41:35, 19.42s/it]

  5%|▌         | 147/2685 [49:36<14:06:12, 20.01s/it]
{'loss': 0.4808, 'learning_rate': 1.996831554888928e-06, 'rewards/chosen': -0.7369849681854248, 'rewards/rejected': -1.3004177808761597, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5634328126907349, 'policy_logps/rejected': -442.0781555175781, 'policy_logps/chosen': -409.07806396484375, 'referece_logps/rejected': -429.0740051269531, 'referece_logps/chosen': -401.708251953125, 'logits/rejected': 0.11673009395599365, 'logits/chosen': 0.08281391859054565, 'epoch': 0.16}


  6%|▌         | 149/2685 [50:19<14:35:35, 20.72s/it]
{'loss': 0.5315, 'learning_rate': 1.9966367277550354e-06, 'rewards/chosen': -0.6541699767112732, 'rewards/rejected': -1.749413251876831, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0952430963516235, 'policy_logps/rejected': -618.2060546875, 'policy_logps/chosen': -488.7269592285156, 'referece_logps/rejected': -600.7119140625, 'referece_logps/chosen': -482.1852722167969, 'logits/rejected': -0.6165131330490112, 'logits/chosen': -0.5619979500770569, 'epoch': 0.17}


  6%|▌         | 151/2685 [50:56<13:43:45, 19.51s/it]

  6%|▌         | 152/2685 [51:15<13:28:16, 19.15s/it]

  6%|▌         | 153/2685 [51:35<13:44:08, 19.53s/it]

  6%|▌         | 154/2685 [51:57<14:11:59, 20.20s/it]

  6%|▌         | 155/2685 [52:17<14:09:45, 20.15s/it]
{'loss': 0.5105, 'learning_rate': 1.996017436115193e-06, 'rewards/chosen': -1.0631623268127441, 'rewards/rejected': -1.5870548486709595, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5238924622535706, 'policy_logps/rejected': -395.06646728515625, 'policy_logps/chosen': -456.451904296875, 'referece_logps/rejected': -379.1959228515625, 'referece_logps/chosen': -445.8203125, 'logits/rejected': 0.19212090969085693, 'logits/chosen': 0.10801094770431519, 'epoch': 0.17}


  6%|▌         | 157/2685 [52:54<13:33:18, 19.30s/it]

  6%|▌         | 158/2685 [53:15<13:56:49, 19.87s/it]

  6%|▌         | 159/2685 [53:31<13:02:36, 18.59s/it]
[2024-03-28 23:00:10,885] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 160/2685 [53:50<13:14:12, 18.87s/it]
{'loss': 0.4841, 'learning_rate': 1.9954614911086552e-06, 'rewards/chosen': -0.2975744307041168, 'rewards/rejected': -1.0065054893493652, 'rewards/accuracies': 0.875, 'rewards/margins': 0.708931028842926, 'policy_logps/rejected': -388.53973388671875, 'policy_logps/chosen': -311.01800537109375, 'referece_logps/rejected': -378.474609375, 'referece_logps/chosen': -308.0422668457031, 'logits/rejected': -0.6377423405647278, 'logits/chosen': -0.6516600251197815, 'epoch': 0.18}


  6%|▌         | 162/2685 [54:32<13:48:25, 19.70s/it]

  6%|▌         | 163/2685 [54:52<13:55:49, 19.88s/it]
[2024-03-28 23:01:32,496] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.492, 'learning_rate': 1.9951105358542877e-06, 'rewards/chosen': -0.3085230886936188, 'rewards/rejected': -1.0595803260803223, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7510572671890259, 'policy_logps/rejected': -371.17584228515625, 'policy_logps/chosen': -439.68524169921875, 'referece_logps/rejected': -360.580078125, 'referece_logps/chosen': -436.6000061035156, 'logits/rejected': -1.1220167875289917, 'logits/chosen': -1.0612684488296509, 'epoch': 0.18}

  6%|▌         | 164/2685 [55:12<13:47:56, 19.70s/it]
[2024-03-28 23:02:11,893] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  6%|▌         | 165/2685 [55:32<13:52:44, 19.83s/it]


  6%|▌         | 167/2685 [56:11<13:38:49, 19.51s/it]

  6%|▋         | 168/2685 [56:31<13:48:05, 19.74s/it]
{'loss': 0.5309, 'learning_rate': 1.9944966447699727e-06, 'rewards/chosen': -0.05471038818359375, 'rewards/rejected': -0.6662245392799377, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6115140318870544, 'policy_logps/rejected': -272.780029296875, 'policy_logps/chosen': -321.99755859375, 'referece_logps/rejected': -266.1177978515625, 'referece_logps/chosen': -321.4504699707031, 'logits/rejected': -1.6041010618209839, 'logits/chosen': -1.5722250938415527, 'epoch': 0.19}

  6%|▋         | 169/2685 [56:54<14:21:29, 20.54s/it]


  6%|▋         | 171/2685 [57:36<14:33:41, 20.85s/it]

  6%|▋         | 172/2685 [57:56<14:12:43, 20.36s/it]
{'loss': 0.4644, 'learning_rate': 1.9939794757129333e-06, 'rewards/chosen': -0.01124715805053711, 'rewards/rejected': -0.7516213655471802, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7403742074966431, 'policy_logps/rejected': -256.19854736328125, 'policy_logps/chosen': -235.7345733642578, 'referece_logps/rejected': -248.6823272705078, 'referece_logps/chosen': -235.62210083007812, 'logits/rejected': -0.7133153080940247, 'logits/chosen': -0.7289881706237793, 'epoch': 0.19}

  6%|▋         | 173/2685 [58:10<13:00:27, 18.64s/it]


  7%|▋         | 175/2685 [58:53<14:03:22, 20.16s/it]
[2024-03-28 23:05:32,983] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 176/2685 [59:15<14:25:29, 20.70s/it]
[2024-03-28 23:05:54,933] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 177/2685 [59:32<13:36:13, 19.53s/it]
[2024-03-28 23:06:11,729] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  7%|▋         | 178/2685 [59:53<14:03:36, 20.19s/it]
[2024-03-28 23:06:33,467] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5393, 'learning_rate': 1.993160323451714e-06, 'rewards/chosen': -0.814548909664154, 'rewards/rejected': -1.8169270753860474, 'rewards/accuracies': 1.0, 'rewards/margins': 1.002378225326538, 'policy_logps/rejected': -517.2554321289062, 'policy_logps/chosen': -477.01788330078125, 'referece_logps/rejected': -499.086181640625, 'referece_logps/chosen': -468.87237548828125, 'logits/rejected': 0.5551697611808777, 'logits/chosen': 0.5749356746673584, 'epoch': 0.2}


  7%|▋         | 180/2685 [1:00:29<13:18:47, 19.13s/it]

  7%|▋         | 181/2685 [1:00:51<13:51:49, 19.93s/it]

  7%|▋         | 182/2685 [1:01:13<14:19:25, 20.60s/it]

  7%|▋         | 183/2685 [1:01:33<14:04:55, 20.26s/it]

  7%|▋         | 184/2685 [1:01:55<14:35:19, 21.00s/it]

  7%|▋         | 185/2685 [1:02:15<14:19:44, 20.63s/it]

  7%|▋         | 186/2685 [1:02:37<14:40:27, 21.14s/it]

  7%|▋         | 187/2685 [1:02:57<14:21:13, 20.69s/it]

  7%|▋         | 188/2685 [1:03:17<14:15:18, 20.55s/it]
{'loss': 0.4981, 'learning_rate': 1.9916794475458213e-06, 'rewards/chosen': 0.27818775177001953, 'rewards/rejected': -0.649763286113739, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9279510974884033, 'policy_logps/rejected': -305.3114318847656, 'policy_logps/chosen': -352.5245056152344, 'referece_logps/rejected': -298.81378173828125, 'referece_logps/chosen': -355.30633544921875, 'logits/rejected': -1.190181851387024, 'logits/chosen': -1.3513095378875732, 'epoch': 0.21}

  7%|▋         | 189/2685 [1:03:36<13:57:37, 20.14s/it]


  7%|▋         | 191/2685 [1:04:16<13:59:40, 20.20s/it]
{'loss': 0.4705, 'learning_rate': 1.9912070283030093e-06, 'rewards/chosen': -0.08110713958740234, 'rewards/rejected': -0.9761477708816528, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8950406908988953, 'policy_logps/rejected': -394.1510314941406, 'policy_logps/chosen': -372.737060546875, 'referece_logps/rejected': -384.3895263671875, 'referece_logps/chosen': -371.92596435546875, 'logits/rejected': 0.015385687351226807, 'logits/chosen': -0.20461781322956085, 'epoch': 0.21}

  7%|▋         | 192/2685 [1:04:34<13:40:45, 19.75s/it]

  7%|▋         | 193/2685 [1:04:56<14:06:16, 20.38s/it]

  7%|▋         | 194/2685 [1:05:19<14:30:10, 20.96s/it]


  7%|▋         | 196/2685 [1:06:01<14:32:38, 21.04s/it]
[2024-03-28 23:12:41,034] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3811, 'learning_rate': 1.990390811655836e-06, 'rewards/chosen': -1.091562271118164, 'rewards/rejected': -2.9553427696228027, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8637806177139282, 'policy_logps/rejected': -571.80078125, 'policy_logps/chosen': -444.8919677734375, 'referece_logps/rejected': -542.247314453125, 'referece_logps/chosen': -433.976318359375, 'logits/rejected': 0.37924063205718994, 'logits/chosen': 0.37693530321121216, 'epoch': 0.22}
[2024-03-28 23:13:00,373] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  7%|▋         | 198/2685 [1:06:43<14:41:30, 21.27s/it]

  7%|▋         | 199/2685 [1:07:04<14:31:41, 21.04s/it]
[2024-03-28 23:13:43,872] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4302, 'learning_rate': 1.989883781376592e-06, 'rewards/chosen': -1.1301023960113525, 'rewards/rejected': -2.691530704498291, 'rewards/accuracies': 0.625, 'rewards/margins': 1.561428189277649, 'policy_logps/rejected': -446.65179443359375, 'policy_logps/chosen': -461.327880859375, 'referece_logps/rejected': -419.7364501953125, 'referece_logps/chosen': -450.0268249511719, 'logits/rejected': -0.42867231369018555, 'logits/chosen': -0.40274903178215027, 'epoch': 0.22}


  7%|▋         | 201/2685 [1:07:47<14:36:50, 21.18s/it]
{'loss': 0.4398, 'learning_rate': 1.9895385568095978e-06, 'rewards/chosen': -0.08982696384191513, 'rewards/rejected': -1.9562675952911377, 'rewards/accuracies': 0.875, 'rewards/margins': 1.866440773010254, 'policy_logps/rejected': -409.234375, 'policy_logps/chosen': -326.2133483886719, 'referece_logps/rejected': -389.67169189453125, 'referece_logps/chosen': -325.3150634765625, 'logits/rejected': -0.36457592248916626, 'logits/chosen': -0.3204040825366974, 'epoch': 0.22}

  8%|▊         | 202/2685 [1:08:05<13:53:11, 20.13s/it]


  8%|▊         | 204/2685 [1:08:48<14:18:19, 20.76s/it]
{'loss': 0.4317, 'learning_rate': 1.9890099184126636e-06, 'rewards/chosen': -0.32243403792381287, 'rewards/rejected': -1.6971378326416016, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3747037649154663, 'policy_logps/rejected': -321.3575439453125, 'policy_logps/chosen': -308.772705078125, 'referece_logps/rejected': -304.3861999511719, 'referece_logps/chosen': -305.54833984375, 'logits/rejected': -0.33423200249671936, 'logits/chosen': -0.4205954372882843, 'epoch': 0.23}

  8%|▊         | 205/2685 [1:09:07<13:59:23, 20.31s/it]

  8%|▊         | 206/2685 [1:09:30<14:31:16, 21.09s/it]


  8%|▊         | 208/2685 [1:10:10<14:10:06, 20.59s/it]
[2024-03-28 23:16:50,372] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 209/2685 [1:10:32<14:24:56, 20.96s/it]
{'loss': 0.5385, 'learning_rate': 1.9881000674974163e-06, 'rewards/chosen': -0.4388260245323181, 'rewards/rejected': -1.5925822257995605, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1537562608718872, 'policy_logps/rejected': -280.8327331542969, 'policy_logps/chosen': -327.7583923339844, 'referece_logps/rejected': -264.90692138671875, 'referece_logps/chosen': -323.37017822265625, 'logits/rejected': -1.7153213024139404, 'logits/chosen': -1.7541179656982422, 'epoch': 0.23}

  8%|▊         | 210/2685 [1:10:51<13:54:59, 20.24s/it]


  8%|▊         | 212/2685 [1:11:34<14:21:41, 20.91s/it]
[2024-03-28 23:18:13,973] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5914, 'learning_rate': 1.987536896449924e-06, 'rewards/chosen': -1.0719082355499268, 'rewards/rejected': -1.3163518905639648, 'rewards/accuracies': 0.625, 'rewards/margins': 0.24444369971752167, 'policy_logps/rejected': -499.31439208984375, 'policy_logps/chosen': -391.807861328125, 'referece_logps/rejected': -486.15087890625, 'referece_logps/chosen': -381.0887451171875, 'logits/rejected': -0.6617763042449951, 'logits/chosen': -0.605919361114502, 'epoch': 0.24}

  8%|▊         | 213/2685 [1:11:55<14:23:33, 20.96s/it]


  8%|▊         | 215/2685 [1:12:38<14:33:21, 21.22s/it]

  8%|▊         | 216/2685 [1:12:56<13:50:17, 20.18s/it]

  8%|▊         | 217/2685 [1:13:18<14:14:47, 20.78s/it]
{'loss': 0.4823, 'learning_rate': 1.98656953422053e-06, 'rewards/chosen': -0.3041343688964844, 'rewards/rejected': -1.3448208570480347, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0406864881515503, 'policy_logps/rejected': -309.0030517578125, 'policy_logps/chosen': -328.2342834472656, 'referece_logps/rejected': -295.5548400878906, 'referece_logps/chosen': -325.19293212890625, 'logits/rejected': -0.6084383726119995, 'logits/chosen': -0.48280835151672363, 'epoch': 0.24}

  8%|▊         | 218/2685 [1:13:35<13:30:24, 19.71s/it]


  8%|▊         | 220/2685 [1:14:19<14:09:47, 20.68s/it]
[2024-03-28 23:20:58,708] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  8%|▊         | 221/2685 [1:14:42<14:47:50, 21.62s/it]
[2024-03-28 23:21:22,508] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4284, 'learning_rate': 1.9857697953148035e-06, 'rewards/chosen': -0.7427472472190857, 'rewards/rejected': -2.6292724609375, 'rewards/accuracies': 0.875, 'rewards/margins': 1.886525273323059, 'policy_logps/rejected': -379.2403869628906, 'policy_logps/chosen': -386.5279846191406, 'referece_logps/rejected': -352.9476318359375, 'referece_logps/chosen': -379.10052490234375, 'logits/rejected': 0.16133293509483337, 'logits/chosen': 0.1872660368680954, 'epoch': 0.25}


  8%|▊         | 223/2685 [1:15:20<13:52:01, 20.28s/it]
{'loss': 0.5318, 'learning_rate': 1.985361315855577e-06, 'rewards/chosen': 0.12746383249759674, 'rewards/rejected': -0.689703106880188, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8171669840812683, 'policy_logps/rejected': -235.19276428222656, 'policy_logps/chosen': -178.66885375976562, 'referece_logps/rejected': -228.29574584960938, 'referece_logps/chosen': -179.94349670410156, 'logits/rejected': -1.7005994319915771, 'logits/chosen': -1.7439545392990112, 'epoch': 0.25}


  8%|▊         | 225/2685 [1:16:08<15:07:11, 22.13s/it]
{'loss': 0.4712, 'learning_rate': 1.9849470995518993e-06, 'rewards/chosen': -0.5843837857246399, 'rewards/rejected': -1.139816164970398, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5554324388504028, 'policy_logps/rejected': -362.40850830078125, 'policy_logps/chosen': -381.7037353515625, 'referece_logps/rejected': -351.0103759765625, 'referece_logps/chosen': -375.8598937988281, 'logits/rejected': -0.2552799582481384, 'logits/chosen': -0.3180781304836273, 'epoch': 0.25}

  8%|▊         | 226/2685 [1:16:25<13:58:31, 20.46s/it]

  8%|▊         | 227/2685 [1:16:43<13:35:33, 19.91s/it]

  8%|▊         | 228/2685 [1:17:01<13:09:27, 19.28s/it]

  9%|▊         | 229/2685 [1:17:17<12:28:11, 18.28s/it]

  9%|▊         | 230/2685 [1:17:37<12:53:06, 18.89s/it]
[2024-03-28 23:24:38,137] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▊         | 231/2685 [1:17:58<13:12:58, 19.39s/it]

  9%|▊         | 232/2685 [1:18:17<13:14:12, 19.43s/it]

  9%|▊         | 233/2685 [1:18:38<13:24:57, 19.70s/it]

  9%|▊         | 234/2685 [1:18:59<13:46:11, 20.22s/it]
[2024-03-28 23:26:02,202] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 235/2685 [1:19:22<14:16:57, 20.99s/it]
[2024-03-28 23:26:25,695] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 236/2685 [1:19:46<14:47:17, 21.74s/it]


  9%|▉         | 238/2685 [1:20:29<14:40:19, 21.59s/it]

  9%|▉         | 239/2685 [1:20:49<14:21:40, 21.14s/it]
{'loss': 0.3654, 'learning_rate': 1.981887160560535e-06, 'rewards/chosen': -0.5694067478179932, 'rewards/rejected': -1.8574670553207397, 'rewards/accuracies': 0.875, 'rewards/margins': 1.288060188293457, 'policy_logps/rejected': -480.1455383300781, 'policy_logps/chosen': -360.1163330078125, 'referece_logps/rejected': -461.5708923339844, 'referece_logps/chosen': -354.4222412109375, 'logits/rejected': -0.6202716827392578, 'logits/chosen': -0.7750535011291504, 'epoch': 0.27}

  9%|▉         | 240/2685 [1:21:10<14:22:54, 21.18s/it]

  9%|▉         | 241/2685 [1:21:31<14:23:38, 21.20s/it]


  9%|▉         | 243/2685 [1:22:12<14:03:46, 20.73s/it]
[2024-03-28 23:28:52,654] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4319, 'learning_rate': 1.9809614037428174e-06, 'rewards/chosen': -0.20382118225097656, 'rewards/rejected': -0.5629556179046631, 'rewards/accuracies': 0.75, 'rewards/margins': 0.35913437604904175, 'policy_logps/rejected': -298.983642578125, 'policy_logps/chosen': -190.0426483154297, 'referece_logps/rejected': -293.3541259765625, 'referece_logps/chosen': -188.0044403076172, 'logits/rejected': -0.7041400074958801, 'logits/chosen': -0.7577325105667114, 'epoch': 0.27}

  9%|▉         | 244/2685 [1:22:34<14:13:32, 20.98s/it]

  9%|▉         | 245/2685 [1:22:54<14:03:54, 20.75s/it]
[2024-03-28 23:29:55,374] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  9%|▉         | 246/2685 [1:23:15<14:05:51, 20.81s/it]


  9%|▉         | 248/2685 [1:23:57<13:57:25, 20.62s/it]
{'loss': 0.4893, 'learning_rate': 1.9797720846879322e-06, 'rewards/chosen': -0.6210636496543884, 'rewards/rejected': -1.3124504089355469, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6913866996765137, 'policy_logps/rejected': -232.69595336914062, 'policy_logps/chosen': -252.82901000976562, 'referece_logps/rejected': -219.5714569091797, 'referece_logps/chosen': -246.61837768554688, 'logits/rejected': -0.9627982974052429, 'logits/chosen': -0.8982098698616028, 'epoch': 0.28}

  9%|▉         | 249/2685 [1:24:20<14:33:03, 21.50s/it]

  9%|▉         | 250/2685 [1:24:42<14:31:08, 21.47s/it]


  9%|▉         | 252/2685 [1:25:25<14:40:07, 21.70s/it]
[2024-03-28 23:32:05,057] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4959, 'learning_rate': 1.9787949580225664e-06, 'rewards/chosen': -0.8410308361053467, 'rewards/rejected': -2.5764241218566895, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7353931665420532, 'policy_logps/rejected': -438.4642333984375, 'policy_logps/chosen': -336.3746337890625, 'referece_logps/rejected': -412.7000427246094, 'referece_logps/chosen': -327.96435546875, 'logits/rejected': -0.4173201620578766, 'logits/chosen': -0.4590936303138733, 'epoch': 0.28}

  9%|▉         | 253/2685 [1:25:40<13:17:11, 19.67s/it]

  9%|▉         | 254/2685 [1:25:57<12:40:57, 18.78s/it]


 10%|▉         | 256/2685 [1:26:39<13:29:59, 20.01s/it]

 10%|▉         | 257/2685 [1:27:01<13:50:43, 20.53s/it]
[2024-03-28 23:33:40,927] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.358, 'learning_rate': 1.9775414977473645e-06, 'rewards/chosen': -0.49651265144348145, 'rewards/rejected': -2.3102097511291504, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8136968612670898, 'policy_logps/rejected': -421.1585693359375, 'policy_logps/chosen': -331.766845703125, 'referece_logps/rejected': -398.0564880371094, 'referece_logps/chosen': -326.80169677734375, 'logits/rejected': -0.35277819633483887, 'logits/chosen': -0.2512332499027252, 'epoch': 0.29}
[2024-03-28 23:34:02,210] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|▉         | 258/2685 [1:27:22<13:59:31, 20.75s/it]


 10%|▉         | 260/2685 [1:27:59<13:19:54, 19.79s/it]
{'loss': 0.3931, 'learning_rate': 1.976772344702059e-06, 'rewards/chosen': -0.8622599244117737, 'rewards/rejected': -2.0536739826202393, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1914143562316895, 'policy_logps/rejected': -320.66668701171875, 'policy_logps/chosen': -314.0272521972656, 'referece_logps/rejected': -300.1299743652344, 'referece_logps/chosen': -305.4046630859375, 'logits/rejected': -0.21128760278224945, 'logits/chosen': -0.2642834484577179, 'epoch': 0.29}


 10%|▉         | 262/2685 [1:28:35<12:35:26, 18.71s/it]
{'loss': 0.4022, 'learning_rate': 1.976252466832093e-06, 'rewards/chosen': -0.49784326553344727, 'rewards/rejected': -1.034482479095459, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5366392135620117, 'policy_logps/rejected': -277.328125, 'policy_logps/chosen': -256.02850341796875, 'referece_logps/rejected': -266.9832763671875, 'referece_logps/chosen': -251.05010986328125, 'logits/rejected': -1.7479737997055054, 'logits/chosen': -1.7191226482391357, 'epoch': 0.29}


 10%|▉         | 264/2685 [1:29:17<13:21:16, 19.86s/it]
{'loss': 0.4219, 'learning_rate': 1.97572690515005e-06, 'rewards/chosen': -0.2531988024711609, 'rewards/rejected': -1.5040340423583984, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2508351802825928, 'policy_logps/rejected': -265.2396240234375, 'policy_logps/chosen': -264.35516357421875, 'referece_logps/rejected': -250.19927978515625, 'referece_logps/chosen': -261.82318115234375, 'logits/rejected': -0.6248552799224854, 'logits/chosen': -0.6955812573432922, 'epoch': 0.29}


 10%|▉         | 266/2685 [1:30:02<14:04:30, 20.95s/it]

 10%|▉         | 267/2685 [1:30:25<14:37:04, 21.76s/it]

 10%|▉         | 268/2685 [1:30:41<13:25:58, 20.01s/it]
{'loss': 0.4596, 'learning_rate': 1.974658742622237e-06, 'rewards/chosen': -0.5855510830879211, 'rewards/rejected': -1.5061256885528564, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9205746054649353, 'policy_logps/rejected': -235.43402099609375, 'policy_logps/chosen': -266.4898681640625, 'referece_logps/rejected': -220.37277221679688, 'referece_logps/chosen': -260.6343688964844, 'logits/rejected': -1.5991040468215942, 'logits/chosen': -1.5914716720581055, 'epoch': 0.3}

 10%|█         | 269/2685 [1:30:59<12:54:51, 19.24s/it]


 10%|█         | 271/2685 [1:31:39<13:16:22, 19.79s/it]

 10%|█         | 272/2685 [1:31:58<12:55:32, 19.28s/it]
{'loss': 0.4273, 'learning_rate': 1.9735678819942616e-06, 'rewards/chosen': -0.1990199089050293, 'rewards/rejected': -1.5237020254135132, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3246819972991943, 'policy_logps/rejected': -267.8749084472656, 'policy_logps/chosen': -196.541015625, 'referece_logps/rejected': -252.6378936767578, 'referece_logps/chosen': -194.55081176757812, 'logits/rejected': -0.9521428346633911, 'logits/chosen': -1.0048680305480957, 'epoch': 0.3}

 10%|█         | 273/2685 [1:32:21<13:43:24, 20.48s/it]

 10%|█         | 274/2685 [1:32:43<13:57:22, 20.84s/it]
[2024-03-28 23:39:42,351] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 10%|█         | 276/2685 [1:33:21<13:23:45, 20.02s/it]
[2024-03-28 23:40:01,277] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4482, 'learning_rate': 1.972454348670362e-06, 'rewards/chosen': -1.1496275663375854, 'rewards/rejected': -1.9325757026672363, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7829481363296509, 'policy_logps/rejected': -307.2560119628906, 'policy_logps/chosen': -257.8670959472656, 'referece_logps/rejected': -287.9302673339844, 'referece_logps/chosen': -246.37081909179688, 'logits/rejected': -0.6666802167892456, 'logits/chosen': -0.7397583723068237, 'epoch': 0.31}

 10%|█         | 277/2685 [1:33:37<12:30:05, 18.69s/it]

 10%|█         | 278/2685 [1:33:56<12:41:09, 18.97s/it]


 10%|█         | 280/2685 [1:34:40<13:32:51, 20.28s/it]
[2024-03-28 23:41:19,685] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 10%|█         | 281/2685 [1:34:58<13:07:50, 19.66s/it]

 11%|█         | 282/2685 [1:35:15<12:41:43, 19.02s/it]

 11%|█         | 283/2685 [1:35:35<12:53:36, 19.32s/it]

 11%|█         | 284/2685 [1:35:53<12:37:20, 18.93s/it]
{'loss': 0.4377, 'learning_rate': 1.9701593681911798e-06, 'rewards/chosen': -0.9574716687202454, 'rewards/rejected': -2.1078388690948486, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1503671407699585, 'policy_logps/rejected': -422.9670104980469, 'policy_logps/chosen': -385.5660400390625, 'referece_logps/rejected': -401.8886413574219, 'referece_logps/chosen': -375.9913330078125, 'logits/rejected': -0.2310744673013687, 'logits/chosen': -0.2783781886100769, 'epoch': 0.32}
[2024-03-28 23:42:56,869] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 11%|█         | 285/2685 [1:36:17<13:30:50, 20.27s/it]


 11%|█         | 287/2685 [1:36:50<12:09:12, 18.25s/it]
{'loss': 0.4215, 'learning_rate': 1.969275439537199e-06, 'rewards/chosen': -0.5053335428237915, 'rewards/rejected': -2.112802743911743, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6074692010879517, 'policy_logps/rejected': -391.3593444824219, 'policy_logps/chosen': -405.015625, 'referece_logps/rejected': -370.2312927246094, 'referece_logps/chosen': -399.9622802734375, 'logits/rejected': -1.1364918947219849, 'logits/chosen': -1.2634484767913818, 'epoch': 0.32}


 11%|█         | 289/2685 [1:37:31<13:01:02, 19.56s/it]
{'loss': 0.3826, 'learning_rate': 1.968679099061394e-06, 'rewards/chosen': 0.0367547944188118, 'rewards/rejected': -1.8114850521087646, 'rewards/accuracies': 1.0, 'rewards/margins': 1.848239779472351, 'policy_logps/rejected': -505.3013610839844, 'policy_logps/chosen': -453.016357421875, 'referece_logps/rejected': -487.1865234375, 'referece_logps/chosen': -453.3839111328125, 'logits/rejected': -0.40658247470855713, 'logits/chosen': -0.2969650626182556, 'epoch': 0.32}

 11%|█         | 290/2685 [1:37:52<13:17:43, 19.98s/it]


 11%|█         | 292/2685 [1:38:34<13:36:28, 20.47s/it]
{'loss': 0.3812, 'learning_rate': 1.9677740149677987e-06, 'rewards/chosen': -0.9483741521835327, 'rewards/rejected': -2.0538063049316406, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1054319143295288, 'policy_logps/rejected': -296.18524169921875, 'policy_logps/chosen': -280.309326171875, 'referece_logps/rejected': -275.647216796875, 'referece_logps/chosen': -270.8255920410156, 'logits/rejected': -1.1337599754333496, 'logits/chosen': -1.2554259300231934, 'epoch': 0.33}

 11%|█         | 293/2685 [1:38:55<13:51:46, 20.86s/it]


 11%|█         | 295/2685 [1:39:32<12:59:21, 19.57s/it]
{'loss': 0.5034, 'learning_rate': 1.9668562533695704e-06, 'rewards/chosen': -0.16457206010818481, 'rewards/rejected': -0.5670119524002075, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4024398624897003, 'policy_logps/rejected': -331.0273742675781, 'policy_logps/chosen': -359.6111145019531, 'referece_logps/rejected': -325.35723876953125, 'referece_logps/chosen': -357.96539306640625, 'logits/rejected': -0.31971102952957153, 'logits/chosen': -0.18438240885734558, 'epoch': 0.33}

 11%|█         | 296/2685 [1:39:53<13:15:09, 19.97s/it]

 11%|█         | 297/2685 [1:40:15<13:41:44, 20.65s/it]

 11%|█         | 298/2685 [1:40:35<13:31:34, 20.40s/it]

 11%|█         | 299/2685 [1:40:56<13:36:51, 20.54s/it]

 11%|█         | 300/2685 [1:41:18<13:54:49, 21.00s/it]

 11%|█         | 301/2685 [1:41:36<13:18:21, 20.09s/it]

 11%|█         | 302/2685 [1:41:53<12:44:02, 19.24s/it]


 11%|█▏        | 304/2685 [1:42:32<13:00:26, 19.67s/it]
{'loss': 0.5584, 'learning_rate': 1.9640270246000876e-06, 'rewards/chosen': -0.39871206879615784, 'rewards/rejected': -1.3568083047866821, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9580962657928467, 'policy_logps/rejected': -207.21856689453125, 'policy_logps/chosen': -248.3426971435547, 'referece_logps/rejected': -193.65048217773438, 'referece_logps/chosen': -244.35556030273438, 'logits/rejected': -0.9298311471939087, 'logits/chosen': -0.6550681591033936, 'epoch': 0.34}

 11%|█▏        | 305/2685 [1:42:55<13:32:23, 20.48s/it]

 11%|█▏        | 306/2685 [1:43:16<13:38:21, 20.64s/it]


 11%|█▏        | 308/2685 [1:43:48<12:11:12, 18.46s/it]

 12%|█▏        | 309/2685 [1:44:06<12:06:17, 18.34s/it]
{'loss': 0.3955, 'learning_rate': 1.9624060988436964e-06, 'rewards/chosen': -0.5545662045478821, 'rewards/rejected': -1.90125572681427, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3466895818710327, 'policy_logps/rejected': -325.9058532714844, 'policy_logps/chosen': -266.18658447265625, 'referece_logps/rejected': -306.893310546875, 'referece_logps/chosen': -260.640869140625, 'logits/rejected': -0.9480649828910828, 'logits/chosen': -0.8134883642196655, 'epoch': 0.35}

 12%|█▏        | 310/2685 [1:44:27<12:32:09, 19.00s/it]

 12%|█▏        | 311/2685 [1:44:46<12:28:23, 18.91s/it]
[2024-03-28 23:51:48,077] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 312/2685 [1:45:08<13:07:58, 19.92s/it]
[2024-03-28 23:52:09,419] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 313/2685 [1:45:29<13:24:27, 20.35s/it]

 12%|█▏        | 314/2685 [1:45:51<13:44:07, 20.86s/it]

 12%|█▏        | 315/2685 [1:46:12<13:42:31, 20.82s/it]

 12%|█▏        | 316/2685 [1:46:31<13:25:24, 20.40s/it]

 12%|█▏        | 317/2685 [1:46:55<14:04:22, 21.39s/it]

 12%|█▏        | 318/2685 [1:47:09<12:37:19, 19.20s/it]

 12%|█▏        | 319/2685 [1:47:32<13:19:51, 20.28s/it]

 12%|█▏        | 320/2685 [1:47:51<13:07:20, 19.97s/it]

 12%|█▏        | 321/2685 [1:48:13<13:25:58, 20.46s/it]

 12%|█▏        | 322/2685 [1:48:29<12:33:25, 19.13s/it]
[2024-03-28 23:55:28,668] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 12%|█▏        | 323/2685 [1:48:49<12:38:23, 19.26s/it]

 12%|█▏        | 324/2685 [1:49:04<11:59:06, 18.27s/it]
[2024-03-28 23:56:05,274] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 12%|█▏        | 326/2685 [1:49:47<12:56:48, 19.76s/it]
[2024-03-28 23:56:26,834] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4162, 'learning_rate': 1.9566333656925145e-06, 'rewards/chosen': -0.42418697476387024, 'rewards/rejected': -1.709683895111084, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2854969501495361, 'policy_logps/rejected': -312.9523010253906, 'policy_logps/chosen': -343.3658752441406, 'referece_logps/rejected': -295.8554382324219, 'referece_logps/chosen': -339.12396240234375, 'logits/rejected': -1.5006742477416992, 'logits/chosen': -1.582556962966919, 'epoch': 0.36}

 12%|█▏        | 327/2685 [1:50:05<12:39:56, 19.34s/it]

 12%|█▏        | 328/2685 [1:50:25<12:51:45, 19.65s/it]


 12%|█▏        | 330/2685 [1:51:07<13:09:06, 20.10s/it]
[2024-03-28 23:57:46,951] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4461, 'learning_rate': 1.9552165034214368e-06, 'rewards/chosen': -1.5802092552185059, 'rewards/rejected': -2.4728305339813232, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8926211595535278, 'policy_logps/rejected': -317.1292724609375, 'policy_logps/chosen': -338.66650390625, 'referece_logps/rejected': -292.4009704589844, 'referece_logps/chosen': -322.86444091796875, 'logits/rejected': -1.139638900756836, 'logits/chosen': -0.9794094562530518, 'epoch': 0.37}


 12%|█▏        | 332/2685 [1:51:49<13:30:52, 20.68s/it]
{'loss': 0.2748, 'learning_rate': 1.9544997282067514e-06, 'rewards/chosen': 0.042051684111356735, 'rewards/rejected': -2.209362268447876, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2514138221740723, 'policy_logps/rejected': -291.9983215332031, 'policy_logps/chosen': -251.2395477294922, 'referece_logps/rejected': -269.9046630859375, 'referece_logps/chosen': -251.66006469726562, 'logits/rejected': -1.1213109493255615, 'logits/chosen': -1.0905274152755737, 'epoch': 0.37}


 12%|█▏        | 334/2685 [1:52:31<13:42:16, 20.99s/it]

 12%|█▏        | 335/2685 [1:52:53<13:50:18, 21.20s/it]

 13%|█▎        | 336/2685 [1:53:10<12:57:42, 19.86s/it]

 13%|█▎        | 337/2685 [1:53:32<13:23:40, 20.54s/it]
[2024-03-29 00:00:11,805] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 338/2685 [1:53:52<13:21:05, 20.48s/it]
[2024-03-29 00:00:32,150] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 339/2685 [1:54:14<13:42:45, 21.04s/it]

 13%|█▎        | 340/2685 [1:54:31<12:49:34, 19.69s/it]

 13%|█▎        | 341/2685 [1:54:51<12:50:39, 19.73s/it]

 13%|█▎        | 342/2685 [1:55:14<13:29:11, 20.72s/it]
[2024-03-29 00:01:53,898] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 343/2685 [1:55:31<12:49:39, 19.72s/it]

 13%|█▎        | 344/2685 [1:55:49<12:32:00, 19.27s/it]

 13%|█▎        | 345/2685 [1:56:11<12:59:34, 19.99s/it]
[2024-03-29 00:02:51,169] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 346/2685 [1:56:34<13:32:07, 20.83s/it]

 13%|█▎        | 347/2685 [1:56:55<13:35:37, 20.93s/it]

 13%|█▎        | 348/2685 [1:57:17<13:51:15, 21.34s/it]

 13%|█▎        | 349/2685 [1:57:37<13:29:13, 20.79s/it]

 13%|█▎        | 350/2685 [1:57:55<13:02:06, 20.10s/it]

 13%|█▎        | 351/2685 [1:58:15<13:01:33, 20.09s/it]

 13%|█▎        | 352/2685 [1:58:40<13:49:47, 21.34s/it]

 13%|█▎        | 353/2685 [1:59:00<13:38:27, 21.06s/it]

 13%|█▎        | 354/2685 [1:59:24<14:08:59, 21.85s/it]
[2024-03-29 00:06:03,850] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 355/2685 [1:59:41<13:13:48, 20.44s/it]
[2024-03-29 00:06:20,997] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 356/2685 [2:00:01<13:11:55, 20.40s/it]

 13%|█▎        | 357/2685 [2:00:19<12:44:43, 19.71s/it]

 13%|█▎        | 358/2685 [2:00:37<12:21:57, 19.13s/it]

 13%|█▎        | 359/2685 [2:00:58<12:45:57, 19.76s/it]

 13%|█▎        | 360/2685 [2:01:18<12:49:07, 19.85s/it]

 13%|█▎        | 361/2685 [2:01:36<12:20:25, 19.12s/it]
[2024-03-29 00:08:15,870] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 13%|█▎        | 362/2685 [2:01:50<11:22:36, 17.63s/it]

 14%|█▎        | 363/2685 [2:02:09<11:38:12, 18.04s/it]
[2024-03-29 00:08:49,034] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▎        | 364/2685 [2:02:27<11:38:10, 18.05s/it]

 14%|█▎        | 365/2685 [2:02:43<11:12:32, 17.39s/it]

 14%|█▎        | 366/2685 [2:03:04<11:56:25, 18.54s/it]

 14%|█▎        | 367/2685 [2:03:24<12:12:04, 18.95s/it]

 14%|█▎        | 368/2685 [2:03:45<12:39:39, 19.67s/it]

 14%|█▎        | 369/2685 [2:04:08<13:11:26, 20.50s/it]

 14%|█▍        | 370/2685 [2:04:31<13:46:06, 21.41s/it]
[2024-03-29 00:11:11,410] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 371/2685 [2:04:52<13:40:41, 21.28s/it]
[2024-03-29 00:11:32,383] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 372/2685 [2:05:12<13:27:14, 20.94s/it]

 14%|█▍        | 373/2685 [2:05:27<12:19:03, 19.18s/it]

 14%|█▍        | 374/2685 [2:05:46<12:09:09, 18.93s/it]

 14%|█▍        | 375/2685 [2:06:02<11:39:20, 18.16s/it]

 14%|█▍        | 376/2685 [2:06:23<12:14:02, 19.07s/it]

 14%|█▍        | 377/2685 [2:06:45<12:39:43, 19.75s/it]
[2024-03-29 00:13:24,854] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 378/2685 [2:07:07<13:11:49, 20.59s/it]

 14%|█▍        | 379/2685 [2:07:28<13:13:27, 20.65s/it]

 14%|█▍        | 380/2685 [2:07:49<13:20:53, 20.85s/it]

 14%|█▍        | 381/2685 [2:08:12<13:43:41, 21.45s/it]

 14%|█▍        | 382/2685 [2:08:32<13:24:19, 20.96s/it]

 14%|█▍        | 383/2685 [2:08:51<12:56:56, 20.25s/it]

 14%|█▍        | 384/2685 [2:09:12<13:13:32, 20.69s/it]

 14%|█▍        | 385/2685 [2:09:34<13:19:49, 20.86s/it]

 14%|█▍        | 386/2685 [2:09:55<13:25:23, 21.02s/it]
[2024-03-29 00:16:35,133] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 14%|█▍        | 387/2685 [2:10:16<13:23:31, 20.98s/it]

 14%|█▍        | 388/2685 [2:10:35<13:03:18, 20.46s/it]

 14%|█▍        | 389/2685 [2:10:55<12:54:32, 20.24s/it]

 15%|█▍        | 390/2685 [2:11:13<12:36:00, 19.76s/it]

 15%|█▍        | 391/2685 [2:11:35<12:59:08, 20.38s/it]

 15%|█▍        | 392/2685 [2:11:59<13:40:48, 21.48s/it]

 15%|█▍        | 393/2685 [2:12:20<13:32:53, 21.28s/it]

 15%|█▍        | 394/2685 [2:12:41<13:31:43, 21.26s/it]

 15%|█▍        | 395/2685 [2:13:00<13:00:04, 20.44s/it]

 15%|█▍        | 396/2685 [2:13:21<13:10:45, 20.73s/it]

 15%|█▍        | 397/2685 [2:13:41<12:54:32, 20.31s/it]

 15%|█▍        | 398/2685 [2:13:58<12:21:48, 19.46s/it]

 15%|█▍        | 399/2685 [2:14:19<12:36:34, 19.86s/it]

 15%|█▍        | 400/2685 [2:14:40<12:53:11, 20.30s/it]

 15%|█▍        | 401/2685 [2:15:03<13:20:15, 21.02s/it]

 15%|█▍        | 402/2685 [2:15:25<13:33:11, 21.37s/it]

 15%|█▌        | 403/2685 [2:15:41<12:32:56, 19.80s/it]

 15%|█▌        | 404/2685 [2:16:01<12:36:08, 19.89s/it]

 15%|█▌        | 405/2685 [2:16:21<12:33:40, 19.83s/it]

 15%|█▌        | 406/2685 [2:16:42<12:40:15, 20.02s/it]

 15%|█▌        | 407/2685 [2:17:03<12:59:24, 20.53s/it]

 15%|█▌        | 408/2685 [2:17:23<12:53:20, 20.38s/it]
{'loss': 0.356, 'learning_rate': 1.923185488128411e-06, 'rewards/chosen': -0.5195634961128235, 'rewards/rejected': -2.0024983882904053, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4829347133636475, 'policy_logps/rejected': -278.48419189453125, 'policy_logps/chosen': -215.40023803710938, 'referece_logps/rejected': -258.4591979980469, 'referece_logps/chosen': -210.20462036132812, 'logits/rejected': -0.9215523600578308, 'logits/chosen': -0.9264995455741882, 'epoch': 0.46}


 15%|█▌        | 410/2685 [2:18:03<12:48:33, 20.27s/it]

 15%|█▌        | 411/2685 [2:18:21<12:19:11, 19.50s/it]

 15%|█▌        | 412/2685 [2:18:38<11:52:05, 18.80s/it]

 15%|█▌        | 413/2685 [2:18:58<12:06:22, 19.18s/it]

 15%|█▌        | 414/2685 [2:19:17<12:01:55, 19.07s/it]

 15%|█▌        | 415/2685 [2:19:35<11:51:17, 18.80s/it]
{'loss': 0.3916, 'learning_rate': 1.9199066758674746e-06, 'rewards/chosen': -1.6990914344787598, 'rewards/rejected': -2.2585339546203613, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5594425201416016, 'policy_logps/rejected': -441.277099609375, 'policy_logps/chosen': -438.6719665527344, 'referece_logps/rejected': -418.6917419433594, 'referece_logps/chosen': -421.68109130859375, 'logits/rejected': -0.9715535640716553, 'logits/chosen': -0.873133659362793, 'epoch': 0.46}

 15%|█▌        | 416/2685 [2:19:53<11:35:13, 18.38s/it]

 16%|█▌        | 417/2685 [2:20:13<11:53:47, 18.88s/it]


 16%|█▌        | 419/2685 [2:20:46<11:14:28, 17.86s/it]

 16%|█▌        | 420/2685 [2:21:10<12:17:41, 19.54s/it]

 16%|█▌        | 421/2685 [2:21:33<12:53:55, 20.51s/it]
{'loss': 0.3933, 'learning_rate': 1.917044037965827e-06, 'rewards/chosen': -0.5318724513053894, 'rewards/rejected': -2.8729751110076904, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3411028385162354, 'policy_logps/rejected': -375.28472900390625, 'policy_logps/chosen': -309.8771667480469, 'referece_logps/rejected': -346.5550231933594, 'referece_logps/chosen': -304.5584411621094, 'logits/rejected': -1.0425735712051392, 'logits/chosen': -1.0072510242462158, 'epoch': 0.47}

 16%|█▌        | 422/2685 [2:21:55<13:13:24, 21.04s/it]


 16%|█▌        | 424/2685 [2:22:34<12:37:25, 20.10s/it]

 16%|█▌        | 425/2685 [2:22:54<12:39:38, 20.17s/it]

 16%|█▌        | 426/2685 [2:23:11<12:10:31, 19.40s/it]

 16%|█▌        | 427/2685 [2:23:32<12:26:20, 19.83s/it]

 16%|█▌        | 428/2685 [2:23:52<12:26:58, 19.86s/it]

 16%|█▌        | 429/2685 [2:24:13<12:35:44, 20.10s/it]

 16%|█▌        | 430/2685 [2:24:33<12:33:27, 20.05s/it]

 16%|█▌        | 431/2685 [2:24:53<12:29:29, 19.95s/it]
{'loss': 0.3396, 'learning_rate': 1.912166268496933e-06, 'rewards/chosen': -1.5611491203308105, 'rewards/rejected': -3.103557586669922, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5424082279205322, 'policy_logps/rejected': -297.36395263671875, 'policy_logps/chosen': -367.47113037109375, 'referece_logps/rejected': -266.328369140625, 'referece_logps/chosen': -351.859619140625, 'logits/rejected': -0.31570950150489807, 'logits/chosen': -0.39570072293281555, 'epoch': 0.48}


 16%|█▌        | 433/2685 [2:25:39<13:28:52, 21.55s/it]

 16%|█▌        | 434/2685 [2:25:59<13:11:28, 21.10s/it]

 16%|█▌        | 435/2685 [2:26:17<12:38:22, 20.22s/it]

 16%|█▌        | 436/2685 [2:26:38<12:50:58, 20.57s/it]
{'loss': 0.3354, 'learning_rate': 1.9096775514234535e-06, 'rewards/chosen': -0.7375624179840088, 'rewards/rejected': -1.927655816078186, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1900933980941772, 'policy_logps/rejected': -342.931640625, 'policy_logps/chosen': -384.1328430175781, 'referece_logps/rejected': -323.6550598144531, 'referece_logps/chosen': -376.7572021484375, 'logits/rejected': -0.5813111066818237, 'logits/chosen': -0.5982480645179749, 'epoch': 0.49}

 16%|█▋        | 437/2685 [2:26:58<12:37:40, 20.22s/it]

 16%|█▋        | 438/2685 [2:27:17<12:33:37, 20.12s/it]


 16%|█▋        | 440/2685 [2:27:58<12:40:34, 20.33s/it]

 16%|█▋        | 441/2685 [2:28:20<12:58:06, 20.80s/it]

 16%|█▋        | 442/2685 [2:28:41<12:52:58, 20.68s/it]

 16%|█▋        | 443/2685 [2:28:55<11:43:53, 18.84s/it]
{'loss': 0.4063, 'learning_rate': 1.9061377579914937e-06, 'rewards/chosen': -0.8781388998031616, 'rewards/rejected': -1.385960578918457, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5078216791152954, 'policy_logps/rejected': -327.3683776855469, 'policy_logps/chosen': -300.25616455078125, 'referece_logps/rejected': -313.5087585449219, 'referece_logps/chosen': -291.4747619628906, 'logits/rejected': -0.25346821546554565, 'logits/chosen': -0.1789274364709854, 'epoch': 0.49}

 17%|█▋        | 444/2685 [2:29:12<11:16:27, 18.11s/it]


 17%|█▋        | 446/2685 [2:29:51<11:45:38, 18.91s/it]

 17%|█▋        | 447/2685 [2:30:13<12:17:43, 19.78s/it]

 17%|█▋        | 448/2685 [2:30:29<11:41:16, 18.81s/it]
{'loss': 0.3485, 'learning_rate': 1.9035697524303523e-06, 'rewards/chosen': -1.7818200588226318, 'rewards/rejected': -3.930701494216919, 'rewards/accuracies': 1.0, 'rewards/margins': 2.148881435394287, 'policy_logps/rejected': -456.4036560058594, 'policy_logps/chosen': -424.99285888671875, 'referece_logps/rejected': -417.09661865234375, 'referece_logps/chosen': -407.1746826171875, 'logits/rejected': 0.26738542318344116, 'logits/chosen': 0.23969846963882446, 'epoch': 0.5}

 17%|█▋        | 449/2685 [2:30:46<11:13:37, 18.08s/it]

 17%|█▋        | 450/2685 [2:31:07<11:55:31, 19.21s/it]

 17%|█▋        | 451/2685 [2:31:27<12:02:59, 19.42s/it]


 17%|█▋        | 453/2685 [2:32:07<12:13:32, 19.72s/it]
{'loss': 0.3528, 'learning_rate': 1.9009688679024189e-06, 'rewards/chosen': -1.8112409114837646, 'rewards/rejected': -2.821312427520752, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0100715160369873, 'policy_logps/rejected': -512.577392578125, 'policy_logps/chosen': -555.5158081054688, 'referece_logps/rejected': -484.3642272949219, 'referece_logps/chosen': -537.4033813476562, 'logits/rejected': 0.3671525716781616, 'logits/chosen': 0.3253937363624573, 'epoch': 0.51}


 17%|█▋        | 455/2685 [2:32:45<11:50:26, 19.12s/it]

 17%|█▋        | 456/2685 [2:33:04<11:57:25, 19.31s/it]

 17%|█▋        | 457/2685 [2:33:25<12:09:51, 19.66s/it]

 17%|█▋        | 458/2685 [2:33:45<12:11:22, 19.70s/it]

 17%|█▋        | 459/2685 [2:34:04<12:09:45, 19.67s/it]

 17%|█▋        | 460/2685 [2:34:26<12:33:39, 20.32s/it]

 17%|█▋        | 461/2685 [2:34:47<12:37:07, 20.43s/it]

 17%|█▋        | 462/2685 [2:35:04<12:04:57, 19.57s/it]
{'loss': 0.4496, 'learning_rate': 1.8962047236103328e-06, 'rewards/chosen': -2.1101865768432617, 'rewards/rejected': -2.8251423835754395, 'rewards/accuracies': 0.625, 'rewards/margins': 0.714955747127533, 'policy_logps/rejected': -564.7537841796875, 'policy_logps/chosen': -584.6405639648438, 'referece_logps/rejected': -536.5023803710938, 'referece_logps/chosen': -563.5386962890625, 'logits/rejected': 0.27481314539909363, 'logits/chosen': 0.28986483812332153, 'epoch': 0.52}


 17%|█▋        | 464/2685 [2:35:46<12:21:04, 20.02s/it]

 17%|█▋        | 465/2685 [2:36:08<12:43:13, 20.63s/it]

 17%|█▋        | 466/2685 [2:36:27<12:32:16, 20.34s/it]

 17%|█▋        | 467/2685 [2:36:47<12:23:59, 20.13s/it]

 17%|█▋        | 468/2685 [2:37:10<12:59:30, 21.10s/it]

 17%|█▋        | 469/2685 [2:37:27<12:07:17, 19.69s/it]

 18%|█▊        | 470/2685 [2:37:47<12:10:56, 19.80s/it]

 18%|█▊        | 471/2685 [2:38:08<12:30:26, 20.34s/it]
{'loss': 0.4006, 'learning_rate': 1.8913349204957947e-06, 'rewards/chosen': -0.4916917681694031, 'rewards/rejected': -1.5791839361190796, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0874921083450317, 'policy_logps/rejected': -425.21575927734375, 'policy_logps/chosen': -389.1160888671875, 'referece_logps/rejected': -409.4239501953125, 'referece_logps/chosen': -384.1991882324219, 'logits/rejected': -0.5273965001106262, 'logits/chosen': -0.6256376504898071, 'epoch': 0.53}


 18%|█▊        | 473/2685 [2:38:49<12:33:20, 20.43s/it]
{'loss': 0.3561, 'learning_rate': 1.8902384508083516e-06, 'rewards/chosen': -0.9844529628753662, 'rewards/rejected': -2.6062827110290527, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6218297481536865, 'policy_logps/rejected': -314.59283447265625, 'policy_logps/chosen': -268.43853759765625, 'referece_logps/rejected': -288.530029296875, 'referece_logps/chosen': -258.5940246582031, 'logits/rejected': -1.0232775211334229, 'logits/chosen': -1.0529667139053345, 'epoch': 0.53}


 18%|█▊        | 475/2685 [2:39:23<11:27:22, 18.66s/it]

 18%|█▊        | 476/2685 [2:39:44<11:57:27, 19.49s/it]
{'loss': 0.3245, 'learning_rate': 1.8885840300948104e-06, 'rewards/chosen': -0.26063793897628784, 'rewards/rejected': -1.988349437713623, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7277114391326904, 'policy_logps/rejected': -338.4168395996094, 'policy_logps/chosen': -396.7259826660156, 'referece_logps/rejected': -318.5333557128906, 'referece_logps/chosen': -394.1195983886719, 'logits/rejected': -1.9970998764038086, 'logits/chosen': -1.873900294303894, 'epoch': 0.53}

 18%|█▊        | 477/2685 [2:40:04<11:58:41, 19.53s/it]


 18%|█▊        | 479/2685 [2:40:47<12:28:49, 20.37s/it]

 18%|█▊        | 480/2685 [2:41:05<12:00:48, 19.61s/it]
{'loss': 0.442, 'learning_rate': 1.8863600326884082e-06, 'rewards/chosen': -1.1607762575149536, 'rewards/rejected': -2.0419440269470215, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8811678886413574, 'policy_logps/rejected': -281.2480163574219, 'policy_logps/chosen': -278.7702941894531, 'referece_logps/rejected': -260.8285827636719, 'referece_logps/chosen': -267.16253662109375, 'logits/rejected': -0.6258889436721802, 'logits/chosen': -0.6455726623535156, 'epoch': 0.54}


 18%|█▊        | 482/2685 [2:41:44<12:00:53, 19.63s/it]

 18%|█▊        | 483/2685 [2:42:01<11:34:16, 18.92s/it]

 18%|█▊        | 484/2685 [2:42:18<11:12:49, 18.34s/it]
{'loss': 0.3486, 'learning_rate': 1.8841153935046096e-06, 'rewards/chosen': -0.9923283457756042, 'rewards/rejected': -2.6896727085113525, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6973443031311035, 'policy_logps/rejected': -386.85833740234375, 'policy_logps/chosen': -390.56353759765625, 'referece_logps/rejected': -359.9616394042969, 'referece_logps/chosen': -380.6402587890625, 'logits/rejected': -1.0290076732635498, 'logits/chosen': -0.9610897898674011, 'epoch': 0.54}


 18%|█▊        | 486/2685 [2:42:51<10:35:16, 17.33s/it]

 18%|█▊        | 487/2685 [2:43:09<10:39:05, 17.45s/it]

 18%|█▊        | 488/2685 [2:43:24<10:06:22, 16.56s/it]

 18%|█▊        | 489/2685 [2:43:45<10:58:00, 17.98s/it]

 18%|█▊        | 490/2685 [2:44:05<11:20:41, 18.61s/it]

 18%|█▊        | 491/2685 [2:44:23<11:13:25, 18.42s/it]

 18%|█▊        | 492/2685 [2:44:42<11:21:57, 18.66s/it]

 18%|█▊        | 493/2685 [2:45:02<11:34:19, 19.01s/it]

 18%|█▊        | 494/2685 [2:45:22<11:47:14, 19.37s/it]

 18%|█▊        | 495/2685 [2:45:39<11:25:15, 18.77s/it]

 18%|█▊        | 496/2685 [2:46:01<11:58:49, 19.70s/it]
{'loss': 0.3256, 'learning_rate': 1.8772581504223453e-06, 'rewards/chosen': -0.9723705053329468, 'rewards/rejected': -4.301443099975586, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3290724754333496, 'policy_logps/rejected': -561.5653076171875, 'policy_logps/chosen': -404.2367248535156, 'referece_logps/rejected': -518.5509643554688, 'referece_logps/chosen': -394.5130310058594, 'logits/rejected': 0.4153856933116913, 'logits/chosen': 0.42788976430892944, 'epoch': 0.55}

 19%|█▊        | 497/2685 [2:46:23<12:16:24, 20.19s/it]


 19%|█▊        | 499/2685 [2:46:56<11:07:47, 18.33s/it]
{'loss': 0.4292, 'learning_rate': 1.8755150536864922e-06, 'rewards/chosen': -2.0300471782684326, 'rewards/rejected': -3.791879892349243, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7618328332901, 'policy_logps/rejected': -380.65203857421875, 'policy_logps/chosen': -333.5660400390625, 'referece_logps/rejected': -342.7332458496094, 'referece_logps/chosen': -313.26556396484375, 'logits/rejected': 0.5628032088279724, 'logits/chosen': 0.5645982027053833, 'epoch': 0.56}

 19%|█▊        | 500/2685 [2:47:11<10:29:38, 17.29s/it]/mnt/petrelfs/songmingyang/anaconda3/envs/vcd_origin/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 19%|█▊        | 501/2685 [2:47:48<14:12:17, 23.41s/it]

 19%|█▊        | 502/2685 [2:48:08<13:35:35, 22.42s/it]

 19%|█▊        | 503/2685 [2:48:28<13:06:53, 21.64s/it]

 19%|█▉        | 504/2685 [2:48:46<12:27:23, 20.56s/it]

 19%|█▉        | 505/2685 [2:49:08<12:36:35, 20.82s/it]
{'loss': 0.5347, 'learning_rate': 1.8719944763664715e-06, 'rewards/chosen': -1.0698732137680054, 'rewards/rejected': -2.1770153045654297, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1071422100067139, 'policy_logps/rejected': -373.21441650390625, 'policy_logps/chosen': -199.2879180908203, 'referece_logps/rejected': -351.4443054199219, 'referece_logps/chosen': -188.5891876220703, 'logits/rejected': -0.7611437439918518, 'logits/chosen': -0.6837384104728699, 'epoch': 0.56}

 19%|█▉        | 506/2685 [2:49:27<12:21:50, 20.43s/it]

 19%|█▉        | 507/2685 [2:49:47<12:14:19, 20.23s/it]


 19%|█▉        | 509/2685 [2:50:26<11:57:16, 19.78s/it]
{'loss': 0.337, 'learning_rate': 1.8696220293596858e-06, 'rewards/chosen': -0.790060818195343, 'rewards/rejected': -3.169504404067993, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3794429302215576, 'policy_logps/rejected': -411.8642578125, 'policy_logps/chosen': -340.9395751953125, 'referece_logps/rejected': -380.169189453125, 'referece_logps/chosen': -333.03900146484375, 'logits/rejected': -0.8861626386642456, 'logits/chosen': -0.797226071357727, 'epoch': 0.57}


 19%|█▉        | 511/2685 [2:51:06<12:06:35, 20.05s/it]

 19%|█▉        | 512/2685 [2:51:24<11:39:37, 19.32s/it]
{'loss': 0.311, 'learning_rate': 1.8678294007044515e-06, 'rewards/chosen': -1.085873007774353, 'rewards/rejected': -3.568958044052124, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4830849170684814, 'policy_logps/rejected': -438.11126708984375, 'policy_logps/chosen': -532.246337890625, 'referece_logps/rejected': -402.42169189453125, 'referece_logps/chosen': -521.3876342773438, 'logits/rejected': 0.422815203666687, 'logits/chosen': 0.3540668487548828, 'epoch': 0.57}

 19%|█▉        | 513/2685 [2:51:43<11:43:18, 19.43s/it]


 19%|█▉        | 515/2685 [2:52:21<11:24:38, 18.93s/it]
{'loss': 0.4411, 'learning_rate': 1.8660254037844386e-06, 'rewards/chosen': -1.0947614908218384, 'rewards/rejected': -2.475466251373291, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3807048797607422, 'policy_logps/rejected': -304.0815734863281, 'policy_logps/chosen': -301.3353271484375, 'referece_logps/rejected': -279.326904296875, 'referece_logps/chosen': -290.3876953125, 'logits/rejected': -0.28665226697921753, 'logits/chosen': -0.35647353529930115, 'epoch': 0.58}

 19%|█▉        | 516/2685 [2:52:40<11:24:50, 18.94s/it]


 19%|█▉        | 518/2685 [2:53:14<10:44:02, 17.83s/it]
{'loss': 0.3167, 'learning_rate': 1.8642100622313822e-06, 'rewards/chosen': -0.767224907875061, 'rewards/rejected': -3.268085241317749, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5008602142333984, 'policy_logps/rejected': -432.2460632324219, 'policy_logps/chosen': -442.92388916015625, 'referece_logps/rejected': -399.5652160644531, 'referece_logps/chosen': -435.251708984375, 'logits/rejected': -0.7486839890480042, 'logits/chosen': -0.8560099601745605, 'epoch': 0.58}


 19%|█▉        | 520/2685 [2:53:51<10:48:31, 17.97s/it]

 19%|█▉        | 521/2685 [2:54:03<9:43:08, 16.17s/it]
{'loss': 0.3714, 'learning_rate': 1.8623833998256286e-06, 'rewards/chosen': -0.7343512773513794, 'rewards/rejected': -1.838138461112976, 'rewards/accuracies': 0.5, 'rewards/margins': 1.1037871837615967, 'policy_logps/rejected': -373.424072265625, 'policy_logps/chosen': -358.68927001953125, 'referece_logps/rejected': -355.04266357421875, 'referece_logps/chosen': -351.34576416015625, 'logits/rejected': -1.072730302810669, 'logits/chosen': -1.0170059204101562, 'epoch': 0.58}

 19%|█▉        | 522/2685 [2:54:20<9:53:43, 16.47s/it]

 19%|█▉        | 523/2685 [2:54:39<10:27:20, 17.41s/it]


 20%|█▉        | 525/2685 [2:55:19<11:08:20, 18.56s/it]
{'loss': 0.4029, 'learning_rate': 1.859930281115457e-06, 'rewards/chosen': -1.0909374952316284, 'rewards/rejected': -2.8255069255828857, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7345693111419678, 'policy_logps/rejected': -274.3677673339844, 'policy_logps/chosen': -318.0809020996094, 'referece_logps/rejected': -246.1127166748047, 'referece_logps/chosen': -307.1715087890625, 'logits/rejected': -1.6587648391723633, 'logits/chosen': -1.6043989658355713, 'epoch': 0.59}

 20%|█▉        | 526/2685 [2:55:39<11:29:39, 19.17s/it]


 20%|█▉        | 528/2685 [2:56:19<11:37:33, 19.40s/it]
{'loss': 0.3657, 'learning_rate': 1.858077296698318e-06, 'rewards/chosen': -0.5172353982925415, 'rewards/rejected': -2.7727959156036377, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2555603981018066, 'policy_logps/rejected': -333.0493469238281, 'policy_logps/chosen': -305.73956298828125, 'referece_logps/rejected': -305.3214111328125, 'referece_logps/chosen': -300.5672302246094, 'logits/rejected': -0.6144247055053711, 'logits/chosen': -0.6147832870483398, 'epoch': 0.59}

 20%|█▉        | 529/2685 [2:56:39<11:48:03, 19.70s/it]


 20%|█▉        | 531/2685 [2:57:22<12:28:40, 20.85s/it]

 20%|█▉        | 532/2685 [2:57:41<12:01:23, 20.10s/it]

 20%|█▉        | 533/2685 [2:58:03<12:21:29, 20.67s/it]

 20%|█▉        | 534/2685 [2:58:23<12:11:46, 20.41s/it]

 20%|█▉        | 535/2685 [2:58:39<11:29:27, 19.24s/it]

 20%|█▉        | 536/2685 [2:58:59<11:35:23, 19.42s/it]

 20%|██        | 537/2685 [2:59:18<11:33:57, 19.38s/it]
{'loss': 0.4018, 'learning_rate': 1.852450998183085e-06, 'rewards/chosen': -1.4837009906768799, 'rewards/rejected': -2.8876357078552246, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4039349555969238, 'policy_logps/rejected': -277.90576171875, 'policy_logps/chosen': -278.27191162109375, 'referece_logps/rejected': -249.02938842773438, 'referece_logps/chosen': -263.4349060058594, 'logits/rejected': -0.5850309729576111, 'logits/chosen': -0.723208487033844, 'epoch': 0.6}


 20%|██        | 539/2685 [3:00:00<12:08:56, 20.38s/it]
{'loss': 0.3715, 'learning_rate': 1.851187037829888e-06, 'rewards/chosen': 0.18100127577781677, 'rewards/rejected': -2.3487606048583984, 'rewards/accuracies': 1.0, 'rewards/margins': 2.529761791229248, 'policy_logps/rejected': -411.10223388671875, 'policy_logps/chosen': -339.5898132324219, 'referece_logps/rejected': -387.6146240234375, 'referece_logps/chosen': -341.39984130859375, 'logits/rejected': -0.294761598110199, 'logits/chosen': -0.4655720591545105, 'epoch': 0.6}

 20%|██        | 540/2685 [3:00:18<11:35:31, 19.46s/it]

 20%|██        | 541/2685 [3:00:37<11:36:42, 19.50s/it]

 20%|██        | 542/2685 [3:00:56<11:29:18, 19.30s/it]

 20%|██        | 543/2685 [3:01:16<11:32:01, 19.38s/it]

 20%|██        | 544/2685 [3:01:32<11:01:34, 18.54s/it]

 20%|██        | 545/2685 [3:01:49<10:45:55, 18.11s/it]

 20%|██        | 546/2685 [3:02:04<10:10:39, 17.13s/it]

 20%|██        | 547/2685 [3:02:24<10:41:52, 18.01s/it]

 20%|██        | 548/2685 [3:02:44<10:55:59, 18.42s/it]


 20%|██        | 550/2685 [3:03:21<11:03:24, 18.64s/it]

 21%|██        | 551/2685 [3:03:39<10:56:11, 18.45s/it]
{'loss': 0.3814, 'learning_rate': 1.8434994661730916e-06, 'rewards/chosen': -0.4706740379333496, 'rewards/rejected': -2.3396620750427246, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8689881563186646, 'policy_logps/rejected': -343.0823974609375, 'policy_logps/chosen': -302.01947021484375, 'referece_logps/rejected': -319.6857604980469, 'referece_logps/chosen': -297.312744140625, 'logits/rejected': -1.400132656097412, 'logits/chosen': -1.2987949848175049, 'epoch': 0.62}


 21%|██        | 553/2685 [3:04:17<11:11:08, 18.89s/it]
{'loss': 0.399, 'learning_rate': 1.8422009722637509e-06, 'rewards/chosen': -0.6224000453948975, 'rewards/rejected': -2.3585398197174072, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7361398935317993, 'policy_logps/rejected': -548.3843994140625, 'policy_logps/chosen': -555.9854125976562, 'referece_logps/rejected': -524.7989501953125, 'referece_logps/chosen': -549.7614135742188, 'logits/rejected': -1.5483496189117432, 'logits/chosen': -1.5565524101257324, 'epoch': 0.62}


 21%|██        | 555/2685 [3:04:55<11:13:57, 18.98s/it]

 21%|██        | 556/2685 [3:05:13<11:06:48, 18.79s/it]
{'loss': 0.3004, 'learning_rate': 1.8402440399800838e-06, 'rewards/chosen': -1.1737513542175293, 'rewards/rejected': -2.6640219688415527, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4902706146240234, 'policy_logps/rejected': -286.7552490234375, 'policy_logps/chosen': -340.7726745605469, 'referece_logps/rejected': -260.11505126953125, 'referece_logps/chosen': -329.03521728515625, 'logits/rejected': -0.9667041301727295, 'logits/chosen': -1.0438541173934937, 'epoch': 0.62}

 21%|██        | 557/2685 [3:05:30<10:43:28, 18.14s/it]


 21%|██        | 559/2685 [3:06:09<11:11:41, 18.96s/it]

 21%|██        | 560/2685 [3:06:28<11:03:03, 18.72s/it]
{'loss': 0.4318, 'learning_rate': 1.8376176795327982e-06, 'rewards/chosen': -1.039947509765625, 'rewards/rejected': -4.099951267242432, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0600037574768066, 'policy_logps/rejected': -302.3804931640625, 'policy_logps/chosen': -211.46018981933594, 'referece_logps/rejected': -261.3809814453125, 'referece_logps/chosen': -201.0607147216797, 'logits/rejected': -0.37678658962249756, 'logits/chosen': -0.30061304569244385, 'epoch': 0.63}


 21%|██        | 562/2685 [3:07:05<11:06:04, 18.82s/it]

 21%|██        | 563/2685 [3:07:25<11:23:01, 19.31s/it]
{'loss': 0.3833, 'learning_rate': 1.83563510459427e-06, 'rewards/chosen': -0.8414031267166138, 'rewards/rejected': -2.7347331047058105, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8933299779891968, 'policy_logps/rejected': -442.99334716796875, 'policy_logps/chosen': -343.56231689453125, 'referece_logps/rejected': -415.64599609375, 'referece_logps/chosen': -335.1483154296875, 'logits/rejected': -1.023207426071167, 'logits/chosen': -1.055606722831726, 'epoch': 0.63}

 21%|██        | 564/2685 [3:07:46<11:38:00, 19.75s/it]

 21%|██        | 565/2685 [3:08:10<12:24:24, 21.07s/it]


 21%|██        | 567/2685 [3:08:47<11:31:39, 19.59s/it]
{'loss': 0.3562, 'learning_rate': 1.8329746479158263e-06, 'rewards/chosen': -0.8718839883804321, 'rewards/rejected': -1.4654717445373535, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5935877561569214, 'policy_logps/rejected': -431.35205078125, 'policy_logps/chosen': -448.77264404296875, 'referece_logps/rejected': -416.6972961425781, 'referece_logps/chosen': -440.0538024902344, 'logits/rejected': 0.19943219423294067, 'logits/chosen': 0.19511151313781738, 'epoch': 0.63}


 21%|██        | 569/2685 [3:09:23<10:49:57, 18.43s/it]
{'loss': 0.3575, 'learning_rate': 1.8316371412397267e-06, 'rewards/chosen': -0.7379438877105713, 'rewards/rejected': -1.7330145835876465, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9950708150863647, 'policy_logps/rejected': -333.6455078125, 'policy_logps/chosen': -264.0470886230469, 'referece_logps/rejected': -316.3153381347656, 'referece_logps/chosen': -256.66766357421875, 'logits/rejected': -0.42513737082481384, 'logits/chosen': -0.42434653639793396, 'epoch': 0.64}


 21%|██▏       | 571/2685 [3:10:05<11:40:38, 19.89s/it]

 21%|██▏       | 572/2685 [3:10:27<12:07:37, 20.66s/it]
{'loss': 0.3038, 'learning_rate': 1.8296218051956477e-06, 'rewards/chosen': -0.603615403175354, 'rewards/rejected': -1.999888300895691, 'rewards/accuracies': 1.0, 'rewards/margins': 1.396272897720337, 'policy_logps/rejected': -299.97723388671875, 'policy_logps/chosen': -281.5628662109375, 'referece_logps/rejected': -279.97833251953125, 'referece_logps/chosen': -275.5267333984375, 'logits/rejected': -1.3615341186523438, 'logits/chosen': -1.3907711505889893, 'epoch': 0.64}


 21%|██▏       | 574/2685 [3:11:07<11:45:06, 20.04s/it]
{'loss': 0.3204, 'learning_rate': 1.828272208553812e-06, 'rewards/chosen': -0.9627072811126709, 'rewards/rejected': -2.4857139587402344, 'rewards/accuracies': 0.875, 'rewards/margins': 1.523006558418274, 'policy_logps/rejected': -306.5765686035156, 'policy_logps/chosen': -244.85560607910156, 'referece_logps/rejected': -281.7194519042969, 'referece_logps/chosen': -235.22850036621094, 'logits/rejected': -0.20600712299346924, 'logits/chosen': -0.22724425792694092, 'epoch': 0.64}


 21%|██▏       | 576/2685 [3:11:49<12:04:42, 20.62s/it]

 21%|██▏       | 577/2685 [3:12:06<11:20:59, 19.38s/it]

 22%|██▏       | 578/2685 [3:12:27<11:42:32, 20.01s/it]

 22%|██▏       | 579/2685 [3:12:48<11:49:33, 20.22s/it]
{'loss': 0.2577, 'learning_rate': 1.8248771368173522e-06, 'rewards/chosen': -1.0831478834152222, 'rewards/rejected': -3.6709201335906982, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5877718925476074, 'policy_logps/rejected': -334.3421325683594, 'policy_logps/chosen': -284.24169921875, 'referece_logps/rejected': -297.6329345703125, 'referece_logps/chosen': -273.4102478027344, 'logits/rejected': -0.42542165517807007, 'logits/chosen': -0.4754275977611542, 'epoch': 0.65}


 22%|██▏       | 581/2685 [3:13:32<12:18:32, 21.06s/it]
{'loss': 0.3452, 'learning_rate': 1.8235106968246914e-06, 'rewards/chosen': -1.761482834815979, 'rewards/rejected': -3.9484610557556152, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1869778633117676, 'policy_logps/rejected': -392.01983642578125, 'policy_logps/chosen': -355.6051025390625, 'referece_logps/rejected': -352.53521728515625, 'referece_logps/chosen': -337.99029541015625, 'logits/rejected': 0.2791748642921448, 'logits/chosen': 0.2729473114013672, 'epoch': 0.65}

 22%|██▏       | 582/2685 [3:13:53<12:14:48, 20.96s/it]

 22%|██▏       | 583/2685 [3:14:11<11:43:41, 20.09s/it]


 22%|██▏       | 585/2685 [3:14:47<11:09:42, 19.13s/it]
{'loss': 0.3646, 'learning_rate': 1.8207634412072764e-06, 'rewards/chosen': -0.7623882293701172, 'rewards/rejected': -2.443225383758545, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6808371543884277, 'policy_logps/rejected': -453.27191162109375, 'policy_logps/chosen': -431.27215576171875, 'referece_logps/rejected': -428.83966064453125, 'referece_logps/chosen': -423.6482849121094, 'logits/rejected': -0.2625845968723297, 'logits/chosen': -0.3369026780128479, 'epoch': 0.65}

 22%|██▏       | 586/2685 [3:15:09<11:34:53, 19.86s/it]

 22%|██▏       | 587/2685 [3:15:31<11:56:09, 20.48s/it]


 22%|██▏       | 589/2685 [3:16:10<11:40:18, 20.05s/it]
{'loss': 0.3375, 'learning_rate': 1.8179970714425352e-06, 'rewards/chosen': -0.5269272327423096, 'rewards/rejected': -2.8520212173461914, 'rewards/accuracies': 0.875, 'rewards/margins': 2.325093984603882, 'policy_logps/rejected': -388.97174072265625, 'policy_logps/chosen': -390.0186767578125, 'referece_logps/rejected': -360.4515380859375, 'referece_logps/chosen': -384.7493896484375, 'logits/rejected': -1.184232234954834, 'logits/chosen': -1.2899311780929565, 'epoch': 0.66}

 22%|██▏       | 590/2685 [3:16:31<11:48:20, 20.29s/it]

 22%|██▏       | 591/2685 [3:16:48<11:17:44, 19.42s/it]

 22%|██▏       | 592/2685 [3:17:06<11:03:07, 19.01s/it]

 22%|██▏       | 593/2685 [3:17:29<11:45:20, 20.23s/it]

 22%|██▏       | 594/2685 [3:17:50<11:47:31, 20.30s/it]


 22%|██▏       | 596/2685 [3:18:30<11:39:12, 20.08s/it]
{'loss': 0.29, 'learning_rate': 1.8131101249625483e-06, 'rewards/chosen': -1.1923441886901855, 'rewards/rejected': -3.6314187049865723, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4390740394592285, 'policy_logps/rejected': -271.77447509765625, 'policy_logps/chosen': -219.95082092285156, 'referece_logps/rejected': -235.4602813720703, 'referece_logps/chosen': -208.02735900878906, 'logits/rejected': -1.3178446292877197, 'logits/chosen': -1.3642780780792236, 'epoch': 0.67}
[2024-03-29 01:25:30,727] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 597/2685 [3:18:51<11:46:10, 20.29s/it]

 22%|██▏       | 598/2685 [3:19:12<11:54:33, 20.54s/it]

 22%|██▏       | 599/2685 [3:19:32<11:47:21, 20.35s/it]

 22%|██▏       | 600/2685 [3:19:47<10:57:23, 18.92s/it]

 22%|██▏       | 601/2685 [3:20:07<11:04:23, 19.13s/it]

 22%|██▏       | 602/2685 [3:20:25<10:55:46, 18.89s/it]
[2024-03-29 01:27:23,683] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 603/2685 [3:20:44<10:50:25, 18.74s/it]
[2024-03-29 01:27:47,416] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 22%|██▏       | 604/2685 [3:21:07<11:42:01, 20.24s/it]


 23%|██▎       | 606/2685 [3:21:46<11:27:24, 19.84s/it]
{'loss': 0.2998, 'learning_rate': 1.806028263454005e-06, 'rewards/chosen': -1.4227322340011597, 'rewards/rejected': -2.6491708755493164, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2264388799667358, 'policy_logps/rejected': -498.6253662109375, 'policy_logps/chosen': -386.032958984375, 'referece_logps/rejected': -472.13372802734375, 'referece_logps/chosen': -371.8056335449219, 'logits/rejected': -0.31309249997138977, 'logits/chosen': -0.18562686443328857, 'epoch': 0.68}

 23%|██▎       | 607/2685 [3:22:06<11:24:56, 19.78s/it]

 23%|██▎       | 608/2685 [3:22:27<11:39:01, 20.19s/it]


 23%|██▎       | 610/2685 [3:23:08<11:47:45, 20.47s/it]

 23%|██▎       | 611/2685 [3:23:31<12:04:53, 20.97s/it]
{'loss': 0.2931, 'learning_rate': 1.802443273546722e-06, 'rewards/chosen': -0.29959601163864136, 'rewards/rejected': -2.353301763534546, 'rewards/accuracies': 0.625, 'rewards/margins': 2.0537056922912598, 'policy_logps/rejected': -274.5933837890625, 'policy_logps/chosen': -229.4537353515625, 'referece_logps/rejected': -251.06039428710938, 'referece_logps/chosen': -226.45777893066406, 'logits/rejected': -0.33340632915496826, 'logits/chosen': -0.4051707684993744, 'epoch': 0.68}

 23%|██▎       | 612/2685 [3:23:49<11:35:12, 20.12s/it]


 23%|██▎       | 614/2685 [3:24:31<11:50:48, 20.59s/it]

 23%|██▎       | 615/2685 [3:24:47<11:02:47, 19.21s/it]
{'loss': 0.2973, 'learning_rate': 1.7995542519060644e-06, 'rewards/chosen': -0.21512433886528015, 'rewards/rejected': -3.1340246200561523, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9189000129699707, 'policy_logps/rejected': -445.2822265625, 'policy_logps/chosen': -380.9287109375, 'referece_logps/rejected': -413.94195556640625, 'referece_logps/chosen': -378.7774353027344, 'logits/rejected': -0.5320562124252319, 'logits/chosen': -0.6772088408470154, 'epoch': 0.69}

 23%|██▎       | 616/2685 [3:25:08<11:27:03, 19.92s/it]

 23%|██▎       | 617/2685 [3:25:28<11:28:52, 19.99s/it]

 23%|██▎       | 618/2685 [3:25:46<11:03:18, 19.25s/it]

 23%|██▎       | 619/2685 [3:26:06<11:09:31, 19.44s/it]

 23%|██▎       | 620/2685 [3:26:24<10:57:29, 19.10s/it]

 23%|██▎       | 621/2685 [3:26:44<11:04:48, 19.33s/it]

 23%|██▎       | 622/2685 [3:27:05<11:28:07, 20.01s/it]


 23%|██▎       | 624/2685 [3:27:47<11:33:41, 20.19s/it]
{'loss': 0.3332, 'learning_rate': 1.7929859762257027e-06, 'rewards/chosen': -0.20475329458713531, 'rewards/rejected': -1.9454444646835327, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7406913042068481, 'policy_logps/rejected': -360.4823303222656, 'policy_logps/chosen': -327.54254150390625, 'referece_logps/rejected': -341.02789306640625, 'referece_logps/chosen': -325.4949951171875, 'logits/rejected': -0.6725553274154663, 'logits/chosen': -0.7644000053405762, 'epoch': 0.7}

 23%|██▎       | 625/2685 [3:28:02<10:46:23, 18.83s/it]

 23%|██▎       | 626/2685 [3:28:21<10:47:37, 18.87s/it]


 23%|██▎       | 628/2685 [3:29:01<11:06:18, 19.44s/it]

 23%|██▎       | 629/2685 [3:29:21<11:11:53, 19.61s/it]
{'loss': 0.3478, 'learning_rate': 1.7892964873481664e-06, 'rewards/chosen': -1.060835599899292, 'rewards/rejected': -3.5051283836364746, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4442927837371826, 'policy_logps/rejected': -387.84429931640625, 'policy_logps/chosen': -418.21246337890625, 'referece_logps/rejected': -352.79302978515625, 'referece_logps/chosen': -407.6041259765625, 'logits/rejected': -0.7034334540367126, 'logits/chosen': -0.6225814819335938, 'epoch': 0.7}

 23%|██▎       | 630/2685 [3:29:39<11:00:43, 19.29s/it]

 24%|██▎       | 631/2685 [3:29:59<11:05:21, 19.44s/it]

 24%|██▎       | 632/2685 [3:30:20<11:20:03, 19.88s/it]

 24%|██▎       | 633/2685 [3:30:39<11:05:50, 19.47s/it]

 24%|██▎       | 634/2685 [3:30:58<11:06:51, 19.51s/it]

 24%|██▎       | 635/2685 [3:31:20<11:31:36, 20.24s/it]

 24%|██▎       | 636/2685 [3:31:43<11:53:06, 20.88s/it]

 24%|██▎       | 637/2685 [3:32:02<11:38:05, 20.45s/it]

 24%|██▍       | 638/2685 [3:32:19<10:57:54, 19.28s/it]

 24%|██▍       | 639/2685 [3:32:39<11:07:36, 19.58s/it]


 24%|██▍       | 641/2685 [3:33:17<10:58:00, 19.32s/it]
{'loss': 0.3351, 'learning_rate': 1.7803247909520918e-06, 'rewards/chosen': -0.6463589072227478, 'rewards/rejected': -2.8454596996307373, 'rewards/accuracies': 1.0, 'rewards/margins': 2.199101209640503, 'policy_logps/rejected': -326.40777587890625, 'policy_logps/chosen': -285.03253173828125, 'referece_logps/rejected': -297.95318603515625, 'referece_logps/chosen': -278.5689392089844, 'logits/rejected': -1.2281396389007568, 'logits/chosen': -1.2496602535247803, 'epoch': 0.72}

 24%|██▍       | 642/2685 [3:33:34<10:38:35, 18.75s/it]


 24%|██▍       | 644/2685 [3:34:11<10:24:18, 18.35s/it]

 24%|██▍       | 645/2685 [3:34:31<10:44:04, 18.94s/it]
{'loss': 0.3068, 'learning_rate': 1.777297787397563e-06, 'rewards/chosen': -1.379634141921997, 'rewards/rejected': -3.7020866870880127, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3224527835845947, 'policy_logps/rejected': -408.10296630859375, 'policy_logps/chosen': -371.6573181152344, 'referece_logps/rejected': -371.08209228515625, 'referece_logps/chosen': -357.8609619140625, 'logits/rejected': -0.5691206455230713, 'logits/chosen': -0.638556718826294, 'epoch': 0.72}

 24%|██▍       | 646/2685 [3:34:52<11:05:04, 19.57s/it]

 24%|██▍       | 647/2685 [3:35:14<11:30:05, 20.32s/it]


 24%|██▍       | 649/2685 [3:35:55<11:33:56, 20.45s/it]

 24%|██▍       | 650/2685 [3:36:15<11:26:54, 20.25s/it]

 24%|██▍       | 651/2685 [3:36:35<11:26:31, 20.25s/it]
{'loss': 0.2782, 'learning_rate': 1.7727233631325663e-06, 'rewards/chosen': -0.4613712430000305, 'rewards/rejected': -2.8891899585723877, 'rewards/accuracies': 0.875, 'rewards/margins': 2.427818775177002, 'policy_logps/rejected': -304.6871337890625, 'policy_logps/chosen': -270.1302795410156, 'referece_logps/rejected': -275.7952575683594, 'referece_logps/chosen': -265.51654052734375, 'logits/rejected': -0.5335250496864319, 'logits/chosen': -0.6254547834396362, 'epoch': 0.73}

 24%|██▍       | 652/2685 [3:36:56<11:25:35, 20.23s/it]


 24%|██▍       | 654/2685 [3:37:37<11:37:09, 20.60s/it]
{'loss': 0.3187, 'learning_rate': 1.7704209523730693e-06, 'rewards/chosen': -1.0616236925125122, 'rewards/rejected': -2.1037845611572266, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0421608686447144, 'policy_logps/rejected': -414.5677795410156, 'policy_logps/chosen': -375.3865661621094, 'referece_logps/rejected': -393.5299072265625, 'referece_logps/chosen': -364.7703552246094, 'logits/rejected': -0.04514592885971069, 'logits/chosen': 0.03377056121826172, 'epoch': 0.73}

 24%|██▍       | 655/2685 [3:38:00<11:55:11, 21.14s/it]

 24%|██▍       | 656/2685 [3:38:15<10:54:20, 19.35s/it]


 25%|██▍       | 658/2685 [3:38:49<10:10:11, 18.06s/it]
{'loss': 0.2991, 'learning_rate': 1.7673353775402665e-06, 'rewards/chosen': -0.22220927476882935, 'rewards/rejected': -3.0193023681640625, 'rewards/accuracies': 1.0, 'rewards/margins': 2.797093152999878, 'policy_logps/rejected': -337.7979736328125, 'policy_logps/chosen': -272.00323486328125, 'referece_logps/rejected': -307.60491943359375, 'referece_logps/chosen': -269.7811279296875, 'logits/rejected': -0.8354537487030029, 'logits/chosen': -0.6498484015464783, 'epoch': 0.74}

 25%|██▍       | 659/2685 [3:39:07<10:07:38, 18.00s/it]

 25%|██▍       | 660/2685 [3:39:29<10:41:01, 18.99s/it]

 25%|██▍       | 661/2685 [3:39:46<10:30:07, 18.68s/it]

 25%|██▍       | 662/2685 [3:40:00<9:36:57, 17.11s/it]

 25%|██▍       | 663/2685 [3:40:19<9:55:48, 17.68s/it]

 25%|██▍       | 664/2685 [3:40:43<10:57:48, 19.53s/it]

 25%|██▍       | 665/2685 [3:41:03<11:02:10, 19.67s/it]

 25%|██▍       | 666/2685 [3:41:21<10:48:57, 19.29s/it]

 25%|██▍       | 667/2685 [3:41:41<10:52:35, 19.40s/it]

 25%|██▍       | 668/2685 [3:41:55<9:56:47, 17.75s/it]

 25%|██▍       | 669/2685 [3:42:14<10:09:51, 18.15s/it]

 25%|██▍       | 670/2685 [3:42:27<9:23:15, 16.77s/it]

 25%|██▍       | 671/2685 [3:42:43<9:13:09, 16.48s/it]


 25%|██▌       | 673/2685 [3:43:26<10:37:34, 19.01s/it]
{'loss': 0.3513, 'learning_rate': 1.7556059096387021e-06, 'rewards/chosen': -0.6373947858810425, 'rewards/rejected': -3.006711006164551, 'rewards/accuracies': 1.0, 'rewards/margins': 2.369316339492798, 'policy_logps/rejected': -476.5661315917969, 'policy_logps/chosen': -406.444580078125, 'referece_logps/rejected': -446.4990234375, 'referece_logps/chosen': -400.0706787109375, 'logits/rejected': -0.8833544850349426, 'logits/chosen': -0.7808753252029419, 'epoch': 0.75}

 25%|██▌       | 674/2685 [3:43:43<10:16:17, 18.39s/it]

 25%|██▌       | 675/2685 [3:43:59<9:59:50, 17.91s/it]

 25%|██▌       | 676/2685 [3:44:18<10:03:17, 18.02s/it]

 25%|██▌       | 677/2685 [3:44:34<9:42:25, 17.40s/it]

 25%|██▌       | 678/2685 [3:44:55<10:17:39, 18.47s/it]

 25%|██▌       | 679/2685 [3:45:15<10:42:32, 19.22s/it]


 25%|██▌       | 681/2685 [3:45:58<11:14:02, 20.18s/it]

 25%|██▌       | 682/2685 [3:46:16<11:01:19, 19.81s/it]

 25%|██▌       | 683/2685 [3:46:38<11:14:23, 20.21s/it]

 25%|██▌       | 684/2685 [3:47:00<11:36:30, 20.88s/it]

 26%|██▌       | 685/2685 [3:47:20<11:25:16, 20.56s/it]
{'loss': 0.3058, 'learning_rate': 1.7460439857086916e-06, 'rewards/chosen': -0.6413437724113464, 'rewards/rejected': -2.59555983543396, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9542160034179688, 'policy_logps/rejected': -344.95623779296875, 'policy_logps/chosen': -372.3962097167969, 'referece_logps/rejected': -319.00067138671875, 'referece_logps/chosen': -365.98272705078125, 'logits/rejected': -1.687997817993164, 'logits/chosen': -1.697418212890625, 'epoch': 0.77}


 26%|██▌       | 687/2685 [3:47:55<10:36:33, 19.12s/it]

 26%|██▌       | 688/2685 [3:48:16<10:52:54, 19.62s/it]

 26%|██▌       | 689/2685 [3:48:36<10:55:20, 19.70s/it]

 26%|██▌       | 690/2685 [3:48:55<10:54:16, 19.68s/it]

 26%|██▌       | 691/2685 [3:49:17<11:12:28, 20.23s/it]

 26%|██▌       | 692/2685 [3:49:38<11:24:22, 20.60s/it]

 26%|██▌       | 693/2685 [3:49:56<10:57:30, 19.80s/it]
{'loss': 0.3321, 'learning_rate': 1.7395823757328442e-06, 'rewards/chosen': -0.04744374752044678, 'rewards/rejected': -3.146277666091919, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0988337993621826, 'policy_logps/rejected': -336.0294189453125, 'policy_logps/chosen': -340.35394287109375, 'referece_logps/rejected': -304.56658935546875, 'referece_logps/chosen': -339.8795166015625, 'logits/rejected': -0.5457611680030823, 'logits/chosen': -0.6900432109832764, 'epoch': 0.77}

 26%|██▌       | 694/2685 [3:50:12<10:21:11, 18.72s/it]


 26%|██▌       | 696/2685 [3:50:52<10:41:21, 19.35s/it]

 26%|██▌       | 697/2685 [3:51:13<10:57:34, 19.85s/it]

 26%|██▌       | 698/2685 [3:51:35<11:17:38, 20.46s/it]
{'loss': 0.384, 'learning_rate': 1.7355088458250738e-06, 'rewards/chosen': -0.38490620255470276, 'rewards/rejected': -1.3701337575912476, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9852275252342224, 'policy_logps/rejected': -373.0233154296875, 'policy_logps/chosen': -497.2554931640625, 'referece_logps/rejected': -359.32196044921875, 'referece_logps/chosen': -493.4064025878906, 'logits/rejected': 0.40470898151397705, 'logits/chosen': 0.38088974356651306, 'epoch': 0.78}

 26%|██▌       | 699/2685 [3:51:55<11:07:04, 20.15s/it]


 26%|██▌       | 701/2685 [3:52:34<10:57:53, 19.90s/it]

 26%|██▌       | 702/2685 [3:52:52<10:34:07, 19.19s/it]
{'loss': 0.4151, 'learning_rate': 1.732230744965975e-06, 'rewards/chosen': -1.1610358953475952, 'rewards/rejected': -2.9351158142089844, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7740801572799683, 'policy_logps/rejected': -353.8890075683594, 'policy_logps/chosen': -302.8392333984375, 'referece_logps/rejected': -324.537841796875, 'referece_logps/chosen': -291.2288818359375, 'logits/rejected': -1.6723183393478394, 'logits/chosen': -1.6772149801254272, 'epoch': 0.78}


 26%|██▌       | 704/2685 [3:53:34<11:07:13, 20.21s/it]

 26%|██▋       | 705/2685 [3:53:52<10:49:45, 19.69s/it]

 26%|██▋       | 706/2685 [3:54:11<10:41:12, 19.44s/it]
{'loss': 0.3149, 'learning_rate': 1.72893559173145e-06, 'rewards/chosen': -0.5277763605117798, 'rewards/rejected': -2.0403571128845215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5125806331634521, 'policy_logps/rejected': -379.07623291015625, 'policy_logps/chosen': -342.26373291015625, 'referece_logps/rejected': -358.67266845703125, 'referece_logps/chosen': -336.9859619140625, 'logits/rejected': -0.3319226801395416, 'logits/chosen': -0.37512338161468506, 'epoch': 0.79}

 26%|██▋       | 707/2685 [3:54:25<9:43:09, 17.69s/it]


 26%|██▋       | 709/2685 [3:55:07<10:36:10, 19.32s/it]

 26%|██▋       | 710/2685 [3:55:29<11:07:13, 20.27s/it]

 26%|██▋       | 711/2685 [3:55:51<11:22:07, 20.73s/it]
{'loss': 0.324, 'learning_rate': 1.7247927872291198e-06, 'rewards/chosen': -1.4627065658569336, 'rewards/rejected': -3.0706684589385986, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6079622507095337, 'policy_logps/rejected': -365.78363037109375, 'policy_logps/chosen': -344.8887939453125, 'referece_logps/rejected': -335.0769958496094, 'referece_logps/chosen': -330.2617492675781, 'logits/rejected': -1.2243235111236572, 'logits/chosen': -1.2956902980804443, 'epoch': 0.79}


 27%|██▋       | 713/2685 [3:56:33<11:25:08, 20.85s/it]
[2024-03-29 02:03:13,446] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 714/2685 [3:56:54<11:19:07, 20.67s/it]
{'loss': 0.406, 'learning_rate': 1.7222944354849126e-06, 'rewards/chosen': -1.4167858362197876, 'rewards/rejected': -1.448801875114441, 'rewards/accuracies': 0.5, 'rewards/margins': 0.032016217708587646, 'policy_logps/rejected': -245.66729736328125, 'policy_logps/chosen': -236.5686798095703, 'referece_logps/rejected': -231.17929077148438, 'referece_logps/chosen': -222.40084838867188, 'logits/rejected': -0.7955406904220581, 'logits/chosen': -0.8007757067680359, 'epoch': 0.8}


 27%|██▋       | 716/2685 [3:57:35<11:20:39, 20.74s/it]

 27%|██▋       | 717/2685 [3:57:55<11:06:11, 20.31s/it]

 27%|██▋       | 718/2685 [3:58:14<10:56:43, 20.03s/it]

 27%|██▋       | 719/2685 [3:58:35<11:08:19, 20.40s/it]

 27%|██▋       | 720/2685 [3:58:54<10:57:14, 20.07s/it]

 27%|██▋       | 721/2685 [3:59:17<11:22:50, 20.86s/it]
[2024-03-29 02:05:57,355] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 722/2685 [3:59:35<10:51:17, 19.91s/it]
[2024-03-29 02:06:15,036] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 723/2685 [3:59:55<10:56:55, 20.09s/it]

 27%|██▋       | 724/2685 [4:00:11<10:15:00, 18.82s/it]
{'loss': 0.3527, 'learning_rate': 1.7138984458313745e-06, 'rewards/chosen': -1.393609642982483, 'rewards/rejected': -2.434793472290039, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0411837100982666, 'policy_logps/rejected': -456.552978515625, 'policy_logps/chosen': -365.734375, 'referece_logps/rejected': -432.20501708984375, 'referece_logps/chosen': -351.7982482910156, 'logits/rejected': 0.4609651267528534, 'logits/chosen': 0.38673073053359985, 'epoch': 0.81}


 27%|██▋       | 726/2685 [4:00:52<10:44:55, 19.75s/it]

 27%|██▋       | 727/2685 [4:01:08<10:07:46, 18.62s/it]

 27%|██▋       | 728/2685 [4:01:29<10:31:45, 19.37s/it]

 27%|██▋       | 729/2685 [4:01:50<10:41:19, 19.67s/it]
[2024-03-29 02:08:29,970] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 27%|██▋       | 730/2685 [4:02:06<10:08:19, 18.67s/it]

 27%|██▋       | 731/2685 [4:02:22<9:36:45, 17.71s/it]

 27%|██▋       | 732/2685 [4:02:43<10:16:13, 18.93s/it]

 27%|██▋       | 733/2685 [4:03:05<10:41:03, 19.70s/it]

 27%|██▋       | 734/2685 [4:03:24<10:38:22, 19.63s/it]

 27%|██▋       | 735/2685 [4:03:44<10:42:51, 19.78s/it]

 27%|██▋       | 736/2685 [4:04:07<11:07:14, 20.54s/it]

 27%|██▋       | 737/2685 [4:04:27<10:58:54, 20.30s/it]
{'loss': 0.3859, 'learning_rate': 1.7028285013959614e-06, 'rewards/chosen': -1.1677576303482056, 'rewards/rejected': -2.859572649002075, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6918150186538696, 'policy_logps/rejected': -315.0522155761719, 'policy_logps/chosen': -326.63427734375, 'referece_logps/rejected': -286.45648193359375, 'referece_logps/chosen': -314.95672607421875, 'logits/rejected': -0.7468110918998718, 'logits/chosen': -0.7553665041923523, 'epoch': 0.82}
[2024-03-29 02:11:27,456] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 28%|██▊       | 739/2685 [4:05:11<11:31:59, 21.34s/it]

 28%|██▊       | 740/2685 [4:05:34<11:52:07, 21.97s/it]

 28%|██▊       | 741/2685 [4:05:55<11:36:51, 21.51s/it]

 28%|██▊       | 742/2685 [4:06:17<11:43:52, 21.74s/it]
{'loss': 0.4051, 'learning_rate': 1.6985246471824868e-06, 'rewards/chosen': -1.1872137784957886, 'rewards/rejected': -2.615513563156128, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4282996654510498, 'policy_logps/rejected': -348.32537841796875, 'policy_logps/chosen': -289.4844055175781, 'referece_logps/rejected': -322.1702575683594, 'referece_logps/chosen': -277.6123046875, 'logits/rejected': -0.5266028046607971, 'logits/chosen': -0.6067832112312317, 'epoch': 0.83}

 28%|██▊       | 743/2685 [4:06:37<11:31:15, 21.36s/it]


 28%|██▊       | 745/2685 [4:07:18<11:07:26, 20.64s/it]

 28%|██▊       | 746/2685 [4:07:36<10:44:16, 19.94s/it]

 28%|██▊       | 747/2685 [4:07:55<10:35:24, 19.67s/it]

 28%|██▊       | 748/2685 [4:08:18<11:02:18, 20.52s/it]
{'loss': 0.4147, 'learning_rate': 1.69332648445739e-06, 'rewards/chosen': 0.27975139021873474, 'rewards/rejected': -1.5477381944656372, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8274894952774048, 'policy_logps/rejected': -241.51364135742188, 'policy_logps/chosen': -189.8428955078125, 'referece_logps/rejected': -226.03622436523438, 'referece_logps/chosen': -192.64041137695312, 'logits/rejected': -2.037645101547241, 'logits/chosen': -1.9475383758544922, 'epoch': 0.84}


 28%|██▊       | 750/2685 [4:08:56<10:41:53, 19.90s/it]

 28%|██▊       | 751/2685 [4:09:13<10:11:09, 18.96s/it]
{'loss': 0.3071, 'learning_rate': 1.6907137625359918e-06, 'rewards/chosen': -1.2029821872711182, 'rewards/rejected': -2.9753692150115967, 'rewards/accuracies': 0.75, 'rewards/margins': 1.772386908531189, 'policy_logps/rejected': -235.91897583007812, 'policy_logps/chosen': -203.20596313476562, 'referece_logps/rejected': -206.16529846191406, 'referece_logps/chosen': -191.1761474609375, 'logits/rejected': -1.364960789680481, 'logits/chosen': -1.372018814086914, 'epoch': 0.84}


 28%|██▊       | 753/2685 [4:09:57<11:00:11, 20.50s/it]
{'loss': 0.4122, 'learning_rate': 1.6889669190756866e-06, 'rewards/chosen': -1.4149781465530396, 'rewards/rejected': -2.5348708629608154, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1198927164077759, 'policy_logps/rejected': -520.5322265625, 'policy_logps/chosen': -619.423583984375, 'referece_logps/rejected': -495.18353271484375, 'referece_logps/chosen': -605.2738647460938, 'logits/rejected': -0.5816774964332581, 'logits/chosen': -0.6286300420761108, 'epoch': 0.84}


 28%|██▊       | 755/2685 [4:10:41<11:24:37, 21.28s/it]

 28%|██▊       | 756/2685 [4:11:02<11:12:24, 20.91s/it]

 28%|██▊       | 757/2685 [4:11:21<10:58:06, 20.48s/it]

 28%|██▊       | 758/2685 [4:11:43<11:12:54, 20.95s/it]
[2024-03-29 02:18:23,210] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 759/2685 [4:12:03<10:58:52, 20.53s/it]
[2024-03-29 02:18:42,742] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 760/2685 [4:12:21<10:39:59, 19.95s/it]

 28%|██▊       | 761/2685 [4:12:41<10:42:02, 20.02s/it]

 28%|██▊       | 762/2685 [4:12:57<10:02:35, 18.80s/it]
{'loss': 0.2682, 'learning_rate': 1.6810566320993967e-06, 'rewards/chosen': -0.7625101804733276, 'rewards/rejected': -3.1603527069091797, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3978424072265625, 'policy_logps/rejected': -272.9032287597656, 'policy_logps/chosen': -208.02297973632812, 'referece_logps/rejected': -241.29969787597656, 'referece_logps/chosen': -200.39785766601562, 'logits/rejected': -1.5774083137512207, 'logits/chosen': -1.7118490934371948, 'epoch': 0.85}


 28%|██▊       | 764/2685 [4:13:39<10:33:36, 19.79s/it]
[2024-03-29 02:20:18,887] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 28%|██▊       | 765/2685 [4:13:57<10:20:29, 19.39s/it]

 29%|██▊       | 766/2685 [4:14:19<10:40:20, 20.02s/it]

 29%|██▊       | 767/2685 [4:14:39<10:38:27, 19.97s/it]
{'loss': 0.2899, 'learning_rate': 1.6766272733037574e-06, 'rewards/chosen': -0.3724985122680664, 'rewards/rejected': -3.528001070022583, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1555023193359375, 'policy_logps/rejected': -297.1631164550781, 'policy_logps/chosen': -282.8919372558594, 'referece_logps/rejected': -261.88311767578125, 'referece_logps/chosen': -279.16693115234375, 'logits/rejected': -1.5159363746643066, 'logits/chosen': -1.4356971979141235, 'epoch': 0.86}
[2024-03-29 02:21:39,868] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 29%|██▊       | 769/2685 [4:15:22<11:10:31, 21.00s/it]
[2024-03-29 02:22:02,418] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▊       | 770/2685 [4:15:45<11:23:58, 21.43s/it]

 29%|██▊       | 771/2685 [4:16:02<10:48:51, 20.34s/it]

 29%|██▉       | 772/2685 [4:16:23<10:46:25, 20.27s/it]

 29%|██▉       | 773/2685 [4:16:40<10:22:19, 19.53s/it]

 29%|██▉       | 774/2685 [4:17:01<10:29:59, 19.78s/it]

 29%|██▉       | 775/2685 [4:17:17<9:51:46, 18.59s/it]
{'loss': 0.2478, 'learning_rate': 1.6694891549010744e-06, 'rewards/chosen': -0.37189996242523193, 'rewards/rejected': -2.1138501167297363, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7419501543045044, 'policy_logps/rejected': -331.8286437988281, 'policy_logps/chosen': -290.0389709472656, 'referece_logps/rejected': -310.6901550292969, 'referece_logps/chosen': -286.3199768066406, 'logits/rejected': -1.1186139583587646, 'logits/chosen': -1.151023030281067, 'epoch': 0.87}
[2024-03-29 02:24:16,823] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 29%|██▉       | 777/2685 [4:17:58<10:23:17, 19.60s/it]

 29%|██▉       | 778/2685 [4:18:19<10:40:12, 20.14s/it]

 29%|██▉       | 779/2685 [4:18:36<10:07:26, 19.12s/it]
{'loss': 0.3425, 'learning_rate': 1.6658966671998664e-06, 'rewards/chosen': -0.4479938745498657, 'rewards/rejected': -2.626873016357422, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1788792610168457, 'policy_logps/rejected': -338.04193115234375, 'policy_logps/chosen': -345.21099853515625, 'referece_logps/rejected': -311.773193359375, 'referece_logps/chosen': -340.7311096191406, 'logits/rejected': -0.5593212842941284, 'logits/chosen': -0.7699445486068726, 'epoch': 0.87}


 29%|██▉       | 781/2685 [4:19:15<10:11:04, 19.26s/it]
{'loss': 0.3691, 'learning_rate': 1.664094602767239e-06, 'rewards/chosen': -0.7469866871833801, 'rewards/rejected': -1.6995021104812622, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9525155425071716, 'policy_logps/rejected': -363.59881591796875, 'policy_logps/chosen': -370.6994323730469, 'referece_logps/rejected': -346.60382080078125, 'referece_logps/chosen': -363.2295837402344, 'logits/rejected': 0.16627830266952515, 'logits/chosen': 0.06363143026828766, 'epoch': 0.87}


 29%|██▉       | 783/2685 [4:19:57<10:39:46, 20.18s/it]
{'loss': 0.3493, 'learning_rate': 1.6622886719280703e-06, 'rewards/chosen': -0.3818630874156952, 'rewards/rejected': -2.8666436672210693, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4847805500030518, 'policy_logps/rejected': -317.42938232421875, 'policy_logps/chosen': -305.0208740234375, 'referece_logps/rejected': -288.7629699707031, 'referece_logps/chosen': -301.2022705078125, 'logits/rejected': -0.20068611204624176, 'logits/chosen': -0.29784464836120605, 'epoch': 0.87}


 29%|██▉       | 785/2685 [4:20:33<10:03:54, 19.07s/it]
{'loss': 0.3186, 'learning_rate': 1.6604788851966198e-06, 'rewards/chosen': -1.1035953760147095, 'rewards/rejected': -2.469473361968994, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3658781051635742, 'policy_logps/rejected': -267.61602783203125, 'policy_logps/chosen': -282.9130859375, 'referece_logps/rejected': -242.92129516601562, 'referece_logps/chosen': -271.87713623046875, 'logits/rejected': -0.756920337677002, 'logits/chosen': -0.801658034324646, 'epoch': 0.88}

 29%|██▉       | 786/2685 [4:20:55<10:26:18, 19.79s/it]

 29%|██▉       | 787/2685 [4:21:17<10:49:51, 20.54s/it]
[2024-03-29 02:28:16,602] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 29%|██▉       | 788/2685 [4:21:36<10:38:48, 20.20s/it]
[2024-03-29 02:28:42,690] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 29%|██▉       | 790/2685 [4:22:24<11:25:04, 21.69s/it]

 29%|██▉       | 791/2685 [4:22:46<11:29:19, 21.84s/it]
{'loss': 0.2573, 'learning_rate': 1.6550264951275402e-06, 'rewards/chosen': -0.6277389526367188, 'rewards/rejected': -3.4939656257629395, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8662266731262207, 'policy_logps/rejected': -352.68865966796875, 'policy_logps/chosen': -319.676513671875, 'referece_logps/rejected': -317.7489929199219, 'referece_logps/chosen': -313.39910888671875, 'logits/rejected': 0.11421901732683182, 'logits/chosen': 0.1799359917640686, 'epoch': 0.88}

 29%|██▉       | 792/2685 [4:23:05<11:03:02, 21.02s/it]

 30%|██▉       | 793/2685 [4:23:23<10:33:39, 20.10s/it]


 30%|██▉       | 795/2685 [4:24:00<10:12:47, 19.45s/it]

 30%|██▉       | 796/2685 [4:24:18<9:54:23, 18.88s/it]

 30%|██▉       | 797/2685 [4:24:38<10:06:10, 19.26s/it]

 30%|██▉       | 798/2685 [4:24:58<10:13:08, 19.50s/it]
{'loss': 0.3279, 'learning_rate': 1.64862201387997e-06, 'rewards/chosen': -0.8030490875244141, 'rewards/rejected': -3.340036392211914, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5369873046875, 'policy_logps/rejected': -320.31292724609375, 'policy_logps/chosen': -334.8204040527344, 'referece_logps/rejected': -286.9125671386719, 'referece_logps/chosen': -326.7899169921875, 'logits/rejected': -0.5784100890159607, 'logits/chosen': -0.5092757344245911, 'epoch': 0.89}

 30%|██▉       | 799/2685 [4:25:17<10:06:42, 19.30s/it]

 30%|██▉       | 800/2685 [4:25:37<10:11:26, 19.46s/it]
[2024-03-29 02:32:40,923] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|██▉       | 801/2685 [4:26:01<10:56:01, 20.89s/it]
[2024-03-29 02:32:59,276] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|██▉       | 802/2685 [4:26:19<10:31:45, 20.13s/it]


 30%|██▉       | 804/2685 [4:26:55<10:06:56, 19.36s/it]
{'loss': 0.3004, 'learning_rate': 1.6430956221653122e-06, 'rewards/chosen': -1.2747554779052734, 'rewards/rejected': -2.7391414642333984, 'rewards/accuracies': 0.625, 'rewards/margins': 1.464385986328125, 'policy_logps/rejected': -239.22557067871094, 'policy_logps/chosen': -278.7110290527344, 'referece_logps/rejected': -211.83416748046875, 'referece_logps/chosen': -265.9634704589844, 'logits/rejected': -0.2850843667984009, 'logits/chosen': -0.3362506031990051, 'epoch': 0.9}

 30%|██▉       | 805/2685 [4:27:11<9:30:10, 18.20s/it]


 30%|███       | 807/2685 [4:27:40<8:34:10, 16.43s/it]
{'loss': 0.2846, 'learning_rate': 1.640319771686725e-06, 'rewards/chosen': -0.3094787001609802, 'rewards/rejected': -2.7172131538391113, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4077343940734863, 'policy_logps/rejected': -259.7622985839844, 'policy_logps/chosen': -277.0234680175781, 'referece_logps/rejected': -232.59017944335938, 'referece_logps/chosen': -273.92864990234375, 'logits/rejected': -0.413593590259552, 'logits/chosen': -0.3566959500312805, 'epoch': 0.9}

 30%|███       | 808/2685 [4:27:51<7:46:00, 14.90s/it]

 30%|███       | 809/2685 [4:28:11<8:33:56, 16.44s/it]

 30%|███       | 810/2685 [4:28:23<7:52:04, 15.11s/it]

 30%|███       | 811/2685 [4:28:43<8:40:23, 16.66s/it]
[2024-03-29 02:35:46,935] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 812/2685 [4:29:07<9:44:43, 18.73s/it]

 30%|███       | 813/2685 [4:29:27<10:00:10, 19.24s/it]
[2024-03-29 02:36:29,156] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 815/2685 [4:30:08<10:14:10, 19.71s/it]
[2024-03-29 02:36:48,160] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 816/2685 [4:30:28<10:15:06, 19.75s/it]

 30%|███       | 817/2685 [4:30:48<10:23:09, 20.02s/it]
{'loss': 0.2663, 'learning_rate': 1.6310065620751774e-06, 'rewards/chosen': -0.5749659538269043, 'rewards/rejected': -3.68570876121521, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1107428073883057, 'policy_logps/rejected': -358.7530212402344, 'policy_logps/chosen': -284.02569580078125, 'referece_logps/rejected': -321.89593505859375, 'referece_logps/chosen': -278.2760009765625, 'logits/rejected': -1.6692681312561035, 'logits/chosen': -1.6379125118255615, 'epoch': 0.91}


 31%|███       | 819/2685 [4:31:26<10:03:22, 19.40s/it]
{'loss': 0.4155, 'learning_rate': 1.6291328553706702e-06, 'rewards/chosen': -1.3674890995025635, 'rewards/rejected': -2.8982276916503906, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5307388305664062, 'policy_logps/rejected': -473.4007873535156, 'policy_logps/chosen': -371.4891357421875, 'referece_logps/rejected': -444.41851806640625, 'referece_logps/chosen': -357.8142395019531, 'logits/rejected': -0.9297775626182556, 'logits/chosen': -1.0365744829177856, 'epoch': 0.92}

 31%|███       | 820/2685 [4:31:47<10:18:46, 19.91s/it]

 31%|███       | 821/2685 [4:32:07<10:22:40, 20.04s/it]

 31%|███       | 822/2685 [4:32:29<10:40:22, 20.62s/it]


 31%|███       | 824/2685 [4:33:05<9:52:59, 19.12s/it]
{'loss': 0.3226, 'learning_rate': 1.6244325875267802e-06, 'rewards/chosen': -0.9006087183952332, 'rewards/rejected': -2.9917373657226562, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0911288261413574, 'policy_logps/rejected': -460.9246520996094, 'policy_logps/chosen': -365.3658752441406, 'referece_logps/rejected': -431.0072937011719, 'referece_logps/chosen': -356.3597717285156, 'logits/rejected': -1.0483107566833496, 'logits/chosen': -0.986687958240509, 'epoch': 0.92}


 31%|███       | 826/2685 [4:33:47<10:20:58, 20.04s/it]
{'loss': 0.3648, 'learning_rate': 1.6225461086897392e-06, 'rewards/chosen': -0.9593711495399475, 'rewards/rejected': -3.644453287124634, 'rewards/accuracies': 0.75, 'rewards/margins': 2.685081720352173, 'policy_logps/rejected': -400.8251647949219, 'policy_logps/chosen': -472.511474609375, 'referece_logps/rejected': -364.380615234375, 'referece_logps/chosen': -462.9177551269531, 'logits/rejected': -0.80894935131073, 'logits/chosen': -0.7768266797065735, 'epoch': 0.92}
[2024-03-29 02:40:47,713] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 31%|███       | 828/2685 [4:34:26<10:11:20, 19.75s/it]

 31%|███       | 829/2685 [4:34:45<10:01:45, 19.45s/it]
{'loss': 0.2893, 'learning_rate': 1.6197095979192917e-06, 'rewards/chosen': -1.13481867313385, 'rewards/rejected': -3.587615728378296, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4527969360351562, 'policy_logps/rejected': -357.44244384765625, 'policy_logps/chosen': -281.7048034667969, 'referece_logps/rejected': -321.5663146972656, 'referece_logps/chosen': -270.35662841796875, 'logits/rejected': -0.33676061034202576, 'logits/chosen': -0.34180352091789246, 'epoch': 0.93}

 31%|███       | 830/2685 [4:35:05<10:11:55, 19.79s/it]


 31%|███       | 832/2685 [4:35:45<10:13:49, 19.88s/it]
{'loss': 0.364, 'learning_rate': 1.616864969167604e-06, 'rewards/chosen': -0.6887392401695251, 'rewards/rejected': -2.896279811859131, 'rewards/accuracies': 0.75, 'rewards/margins': 2.20754075050354, 'policy_logps/rejected': -386.6666259765625, 'policy_logps/chosen': -346.00927734375, 'referece_logps/rejected': -357.703857421875, 'referece_logps/chosen': -339.12188720703125, 'logits/rejected': -0.5860542058944702, 'logits/chosen': -0.5296618938446045, 'epoch': 0.93}

 31%|███       | 833/2685 [4:36:03<10:00:19, 19.45s/it]

 31%|███       | 834/2685 [4:36:25<10:24:52, 20.26s/it]

 31%|███       | 835/2685 [4:36:45<10:22:33, 20.19s/it]


 31%|███       | 837/2685 [4:37:29<10:44:03, 20.91s/it]

 31%|███       | 838/2685 [4:37:46<10:12:11, 19.89s/it]
{'loss': 0.3044, 'learning_rate': 1.6111515068809685e-06, 'rewards/chosen': -0.9239456057548523, 'rewards/rejected': -2.6110405921936035, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6870951652526855, 'policy_logps/rejected': -401.34893798828125, 'policy_logps/chosen': -390.08355712890625, 'referece_logps/rejected': -375.238525390625, 'referece_logps/chosen': -380.8440856933594, 'logits/rejected': -0.25622642040252686, 'logits/chosen': -0.2942168116569519, 'epoch': 0.94}

 31%|███       | 839/2685 [4:38:06<10:11:03, 19.86s/it]


 31%|███▏      | 841/2685 [4:38:48<10:33:43, 20.62s/it]
[2024-03-29 02:45:28,504] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3547, 'learning_rate': 1.6082827481903997e-06, 'rewards/chosen': -1.4159225225448608, 'rewards/rejected': -4.212133407592773, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7962112426757812, 'policy_logps/rejected': -553.989013671875, 'policy_logps/chosen': -482.2521057128906, 'referece_logps/rejected': -511.86767578125, 'referece_logps/chosen': -468.0928955078125, 'logits/rejected': 0.6732438802719116, 'logits/chosen': 0.5626450777053833, 'epoch': 0.94}
[2024-03-29 02:45:44,289] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 31%|███▏      | 843/2685 [4:39:23<9:47:24, 19.13s/it]
{'loss': 0.2628, 'learning_rate': 1.6063658132429468e-06, 'rewards/chosen': -0.851955771446228, 'rewards/rejected': -2.2726964950561523, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4207408428192139, 'policy_logps/rejected': -324.282958984375, 'policy_logps/chosen': -485.3478698730469, 'referece_logps/rejected': -301.55596923828125, 'referece_logps/chosen': -476.828369140625, 'logits/rejected': -0.40041083097457886, 'logits/chosen': -0.48662278056144714, 'epoch': 0.94}

 31%|███▏      | 844/2685 [4:39:38<9:08:40, 17.88s/it]


 32%|███▏      | 846/2685 [4:40:19<9:49:34, 19.24s/it]

 32%|███▏      | 847/2685 [4:40:37<9:38:19, 18.88s/it]

 32%|███▏      | 848/2685 [4:40:57<9:45:58, 19.14s/it]
{'loss': 0.3583, 'learning_rate': 1.6015580552525232e-06, 'rewards/chosen': -1.4602675437927246, 'rewards/rejected': -2.263653039932251, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8033855557441711, 'policy_logps/rejected': -318.3647155761719, 'policy_logps/chosen': -304.05560302734375, 'referece_logps/rejected': -295.7281799316406, 'referece_logps/chosen': -289.45294189453125, 'logits/rejected': -0.04016800969839096, 'logits/chosen': -0.013673193752765656, 'epoch': 0.95}

 32%|███▏      | 849/2685 [4:41:10<8:53:33, 17.44s/it]

 32%|███▏      | 850/2685 [4:41:30<9:13:16, 18.09s/it]
[2024-03-29 02:48:34,251] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 32%|███▏      | 852/2685 [4:42:09<9:22:31, 18.41s/it]
[2024-03-29 02:48:49,102] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 853/2685 [4:42:29<9:39:47, 18.99s/it]

 32%|███▏      | 854/2685 [4:42:47<9:27:55, 18.61s/it]

 32%|███▏      | 855/2685 [4:43:09<9:56:00, 19.54s/it]
[2024-03-29 02:49:48,875] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3904, 'learning_rate': 1.5947904591990904e-06, 'rewards/chosen': -1.0349007844924927, 'rewards/rejected': -3.097775936126709, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0628750324249268, 'policy_logps/rejected': -350.657470703125, 'policy_logps/chosen': -349.8569030761719, 'referece_logps/rejected': -319.6796875, 'referece_logps/chosen': -339.5079040527344, 'logits/rejected': -0.36349502205848694, 'logits/chosen': -0.4366447329521179, 'epoch': 0.96}


 32%|███▏      | 857/2685 [4:43:47<9:53:38, 19.48s/it]
{'loss': 0.3705, 'learning_rate': 1.5928490476316407e-06, 'rewards/chosen': -1.965924859046936, 'rewards/rejected': -5.455508708953857, 'rewards/accuracies': 0.875, 'rewards/margins': 3.48958420753479, 'policy_logps/rejected': -500.0032043457031, 'policy_logps/chosen': -478.64935302734375, 'referece_logps/rejected': -445.4481201171875, 'referece_logps/chosen': -458.9900817871094, 'logits/rejected': 0.7291150689125061, 'logits/chosen': 0.7960411906242371, 'epoch': 0.96}


 32%|███▏      | 859/2685 [4:44:29<10:10:55, 20.07s/it]

 32%|███▏      | 860/2685 [4:44:46<9:40:03, 19.07s/it]

 32%|███▏      | 861/2685 [4:45:05<9:42:07, 19.15s/it]
{'loss': 0.2583, 'learning_rate': 1.58895588099049e-06, 'rewards/chosen': -1.3626086711883545, 'rewards/rejected': -2.9115476608276367, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5489391088485718, 'policy_logps/rejected': -343.37969970703125, 'policy_logps/chosen': -375.79730224609375, 'referece_logps/rejected': -314.26422119140625, 'referece_logps/chosen': -362.1712341308594, 'logits/rejected': -0.5560189485549927, 'logits/chosen': -0.6371275782585144, 'epoch': 0.96}


 32%|███▏      | 863/2685 [4:45:37<8:49:39, 17.44s/it]
{'loss': 0.2966, 'learning_rate': 1.587004148583085e-06, 'rewards/chosen': -1.0925568342208862, 'rewards/rejected': -2.5387790203094482, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4462220668792725, 'policy_logps/rejected': -319.7998046875, 'policy_logps/chosen': -240.32467651367188, 'referece_logps/rejected': -294.4120178222656, 'referece_logps/chosen': -229.39910888671875, 'logits/rejected': -1.130570411682129, 'logits/chosen': -1.2014169692993164, 'epoch': 0.96}


 32%|███▏      | 865/2685 [4:46:19<9:43:35, 19.24s/it]
{'loss': 0.3506, 'learning_rate': 1.5850489985953074e-06, 'rewards/chosen': -0.9115827679634094, 'rewards/rejected': -2.415163516998291, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5035808086395264, 'policy_logps/rejected': -279.00732421875, 'policy_logps/chosen': -238.0213623046875, 'referece_logps/rejected': -254.85569763183594, 'referece_logps/chosen': -228.90553283691406, 'logits/rejected': -0.59609055519104, 'logits/chosen': -0.6755426526069641, 'epoch': 0.97}

 32%|███▏      | 866/2685 [4:46:40<9:58:38, 19.75s/it]

 32%|███▏      | 867/2685 [4:47:02<10:16:09, 20.34s/it]

 32%|███▏      | 868/2685 [4:47:22<10:11:39, 20.20s/it]


 32%|███▏      | 870/2685 [4:48:00<9:51:35, 19.56s/it]

 32%|███▏      | 871/2685 [4:48:19<9:50:14, 19.52s/it]
{'loss': 0.3733, 'learning_rate': 1.5791631570790696e-06, 'rewards/chosen': -0.8978435397148132, 'rewards/rejected': -2.6915440559387207, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7937004566192627, 'policy_logps/rejected': -340.9505615234375, 'policy_logps/chosen': -297.1766662597656, 'referece_logps/rejected': -314.0351257324219, 'referece_logps/chosen': -288.1982421875, 'logits/rejected': -0.5858370065689087, 'logits/chosen': -0.6506441831588745, 'epoch': 0.97}
[2024-03-29 02:55:22,342] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 33%|███▎      | 873/2685 [4:49:05<10:40:45, 21.22s/it]
[2024-03-29 02:55:45,124] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 874/2685 [4:49:27<10:52:02, 21.60s/it]
[2024-03-29 02:56:07,628] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.343, 'learning_rate': 1.5762088367650068e-06, 'rewards/chosen': -0.9387280344963074, 'rewards/rejected': -4.013769149780273, 'rewards/accuracies': 1.0, 'rewards/margins': 3.075040817260742, 'policy_logps/rejected': -551.1467895507812, 'policy_logps/chosen': -473.7105407714844, 'referece_logps/rejected': -511.00909423828125, 'referece_logps/chosen': -464.3232727050781, 'logits/rejected': 0.5225720405578613, 'logits/chosen': 0.5408761501312256, 'epoch': 0.98}
[2024-03-29 02:56:28,512] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 33%|███▎      | 876/2685 [4:50:08<10:27:45, 20.82s/it]

 33%|███▎      | 877/2685 [4:50:30<10:36:53, 21.14s/it]
{'loss': 0.2783, 'learning_rate': 1.5732469683146185e-06, 'rewards/chosen': -1.0542633533477783, 'rewards/rejected': -3.637192726135254, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5829296112060547, 'policy_logps/rejected': -323.0792236328125, 'policy_logps/chosen': -352.53619384765625, 'referece_logps/rejected': -286.7073059082031, 'referece_logps/chosen': -341.9935607910156, 'logits/rejected': -0.9042501449584961, 'logits/chosen': -0.8840901851654053, 'epoch': 0.98}
[2024-03-29 02:57:26,397] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 33%|███▎      | 879/2685 [4:51:06<9:52:06, 19.67s/it]
{'loss': 0.3044, 'learning_rate': 1.5712682150947922e-06, 'rewards/chosen': -0.861641526222229, 'rewards/rejected': -2.576159954071045, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7145185470581055, 'policy_logps/rejected': -262.878173828125, 'policy_logps/chosen': -325.9217834472656, 'referece_logps/rejected': -237.1165771484375, 'referece_logps/chosen': -317.30535888671875, 'logits/rejected': -1.6215167045593262, 'logits/chosen': -1.5533124208450317, 'epoch': 0.98}

 33%|███▎      | 880/2685 [4:51:21<9:14:03, 18.42s/it]

 33%|███▎      | 881/2685 [4:51:42<9:38:22, 19.24s/it]


 33%|███▎      | 883/2685 [4:52:22<9:48:25, 19.59s/it]

 33%|███▎      | 884/2685 [4:52:42<9:53:40, 19.78s/it]

 33%|███▎      | 885/2685 [4:53:00<9:35:58, 19.20s/it]
{'loss': 0.2148, 'learning_rate': 1.5653120458260261e-06, 'rewards/chosen': -0.6150864362716675, 'rewards/rejected': -3.2963430881500244, 'rewards/accuracies': 0.625, 'rewards/margins': 2.6812565326690674, 'policy_logps/rejected': -335.61517333984375, 'policy_logps/chosen': -338.5931396484375, 'referece_logps/rejected': -302.6517333984375, 'referece_logps/chosen': -332.4422607421875, 'logits/rejected': -0.8531515598297119, 'logits/chosen': -0.7206994295120239, 'epoch': 0.99}

 33%|███▎      | 886/2685 [4:53:20<9:40:05, 19.35s/it]

 33%|███▎      | 887/2685 [4:53:40<9:45:28, 19.54s/it]

 33%|███▎      | 888/2685 [4:53:59<9:48:05, 19.64s/it]
[2024-03-29 03:00:59,539] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 889/2685 [4:54:19<9:50:31, 19.73s/it]


 33%|███▎      | 891/2685 [4:54:56<9:37:20, 19.31s/it]
{'loss': 0.3168, 'learning_rate': 1.5593262550857232e-06, 'rewards/chosen': -1.4586052894592285, 'rewards/rejected': -2.968757390975952, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5101516246795654, 'policy_logps/rejected': -310.0222473144531, 'policy_logps/chosen': -361.112548828125, 'referece_logps/rejected': -280.33465576171875, 'referece_logps/chosen': -346.5264892578125, 'logits/rejected': -0.42973417043685913, 'logits/chosen': -0.5363563299179077, 'epoch': 1.0}

 33%|███▎      | 892/2685 [4:55:13<9:17:48, 18.67s/it]


 33%|███▎      | 894/2685 [4:55:52<9:35:23, 19.28s/it]

 33%|███▎      | 895/2685 [4:56:14<10:02:02, 20.18s/it]
{'loss': 0.2426, 'learning_rate': 1.5553194263760406e-06, 'rewards/chosen': -1.7588444948196411, 'rewards/rejected': -4.062468528747559, 'rewards/accuracies': 0.875, 'rewards/margins': 2.303624391555786, 'policy_logps/rejected': -365.6560363769531, 'policy_logps/chosen': -382.08251953125, 'referece_logps/rejected': -325.0313415527344, 'referece_logps/chosen': -364.49407958984375, 'logits/rejected': 0.3730688691139221, 'logits/chosen': 0.4736466705799103, 'epoch': 1.0}
[2024-03-29 03:03:15,810] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 896/2685 [4:56:36<10:11:39, 20.51s/it]

 33%|███▎      | 897/2685 [4:56:55<10:02:19, 20.21s/it]

 33%|███▎      | 898/2685 [4:57:17<10:13:38, 20.60s/it]

 33%|███▎      | 899/2685 [4:57:38<10:16:16, 20.70s/it]
[2024-03-29 03:04:39,650] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▎      | 900/2685 [4:57:59<10:26:24, 21.06s/it]

 34%|███▎      | 901/2685 [4:58:22<10:36:46, 21.42s/it]

 34%|███▎      | 902/2685 [4:58:44<10:39:52, 21.53s/it]

 34%|███▎      | 903/2685 [4:59:03<10:24:59, 21.04s/it]

 34%|███▎      | 904/2685 [4:59:21<9:50:22, 19.89s/it]

 34%|███▎      | 905/2685 [4:59:42<9:59:50, 20.22s/it]
[2024-03-29 03:06:43,955] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▎      | 906/2685 [5:00:04<10:16:44, 20.80s/it]

 34%|███▍      | 907/2685 [5:00:24<10:06:39, 20.47s/it]


 34%|███▍      | 909/2685 [5:00:54<8:43:43, 17.69s/it]
{'loss': 0.3361, 'learning_rate': 1.5411942981587077e-06, 'rewards/chosen': -1.6339097023010254, 'rewards/rejected': -2.8689351081848145, 'rewards/accuracies': 0.625, 'rewards/margins': 1.23502516746521, 'policy_logps/rejected': -272.7971496582031, 'policy_logps/chosen': -261.295654296875, 'referece_logps/rejected': -244.10777282714844, 'referece_logps/chosen': -244.95657348632812, 'logits/rejected': -0.8027343153953552, 'logits/chosen': -0.8845020532608032, 'epoch': 1.02}

 34%|███▍      | 910/2685 [5:01:12<8:39:30, 17.56s/it]


 34%|███▍      | 912/2685 [5:01:53<9:24:17, 19.10s/it]

 34%|███▍      | 913/2685 [5:02:09<8:56:23, 18.16s/it]
{'loss': 0.3152, 'learning_rate': 1.5371300127210445e-06, 'rewards/chosen': -1.843993067741394, 'rewards/rejected': -2.642000198364258, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7980071902275085, 'policy_logps/rejected': -297.1991882324219, 'policy_logps/chosen': -268.7287292480469, 'referece_logps/rejected': -270.7792053222656, 'referece_logps/chosen': -250.288818359375, 'logits/rejected': -1.0951948165893555, 'logits/chosen': -1.271712303161621, 'epoch': 1.02}

 34%|███▍      | 914/2685 [5:02:29<9:18:45, 18.93s/it]


 34%|███▍      | 916/2685 [5:03:09<9:25:52, 19.19s/it]
{'loss': 0.328, 'learning_rate': 1.5340735845424824e-06, 'rewards/chosen': -1.6122528314590454, 'rewards/rejected': -3.6357762813568115, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0235233306884766, 'policy_logps/rejected': -404.16253662109375, 'policy_logps/chosen': -360.13140869140625, 'referece_logps/rejected': -367.80474853515625, 'referece_logps/chosen': -344.0088806152344, 'logits/rejected': 0.0632123053073883, 'logits/chosen': 0.03234061598777771, 'epoch': 1.02}

 34%|███▍      | 917/2685 [5:03:28<9:25:38, 19.20s/it]


 34%|███▍      | 919/2685 [5:04:03<8:50:37, 18.03s/it]
{'loss': 0.2928, 'learning_rate': 1.5310101601848148e-06, 'rewards/chosen': -1.4858973026275635, 'rewards/rejected': -3.505901575088501, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0200040340423584, 'policy_logps/rejected': -366.179443359375, 'policy_logps/chosen': -322.9060363769531, 'referece_logps/rejected': -331.1204528808594, 'referece_logps/chosen': -308.0470275878906, 'logits/rejected': -0.8457823991775513, 'logits/chosen': -0.7838906645774841, 'epoch': 1.03}


 34%|███▍      | 921/2685 [5:04:35<8:24:31, 17.16s/it]
{'loss': 0.3361, 'learning_rate': 1.5289640103269623e-06, 'rewards/chosen': -1.7788503170013428, 'rewards/rejected': -4.402643203735352, 'rewards/accuracies': 1.0, 'rewards/margins': 2.623793601989746, 'policy_logps/rejected': -312.590576171875, 'policy_logps/chosen': -246.30198669433594, 'referece_logps/rejected': -268.5641174316406, 'referece_logps/chosen': -228.5135040283203, 'logits/rejected': -0.5371752977371216, 'logits/chosen': -0.5179107189178467, 'epoch': 1.03}


 34%|███▍      | 923/2685 [5:05:11<8:33:30, 17.49s/it]
{'loss': 0.3212, 'learning_rate': 1.5269147808025974e-06, 'rewards/chosen': -0.4449313282966614, 'rewards/rejected': -2.089293956756592, 'rewards/accuracies': 0.75, 'rewards/margins': 1.644362449645996, 'policy_logps/rejected': -282.8901672363281, 'policy_logps/chosen': -166.72921752929688, 'referece_logps/rejected': -261.9971923828125, 'referece_logps/chosen': -162.2799072265625, 'logits/rejected': -1.245139479637146, 'logits/chosen': -1.3789353370666504, 'epoch': 1.03}

 34%|███▍      | 924/2685 [5:05:30<8:47:02, 17.96s/it]
[2024-03-29 03:12:31,954] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 34%|███▍      | 925/2685 [5:05:52<9:20:49, 19.12s/it]
[2024-03-29 03:12:53,566] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 35%|███▍      | 927/2685 [5:06:31<9:20:14, 19.12s/it]
{'loss': 0.3449, 'learning_rate': 1.5228071304952348e-06, 'rewards/chosen': -0.5341619253158569, 'rewards/rejected': -1.989396572113037, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4552346467971802, 'policy_logps/rejected': -250.07113647460938, 'policy_logps/chosen': -253.48255920410156, 'referece_logps/rejected': -230.1772003173828, 'referece_logps/chosen': -248.1409454345703, 'logits/rejected': -0.4959648847579956, 'logits/chosen': -0.6392157673835754, 'epoch': 1.04}

 35%|███▍      | 928/2685 [5:06:50<9:23:53, 19.26s/it]


 35%|███▍      | 930/2685 [5:07:29<9:27:23, 19.40s/it]
{'loss': 0.3298, 'learning_rate': 1.5197183975048756e-06, 'rewards/chosen': -1.7163721323013306, 'rewards/rejected': -5.159542560577393, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4431703090667725, 'policy_logps/rejected': -380.1092529296875, 'policy_logps/chosen': -334.9295654296875, 'referece_logps/rejected': -328.5138244628906, 'referece_logps/chosen': -317.765869140625, 'logits/rejected': -0.7320882081985474, 'logits/chosen': -0.713796079158783, 'epoch': 1.04}

 35%|███▍      | 931/2685 [5:07:47<9:19:16, 19.13s/it]

 35%|███▍      | 932/2685 [5:08:06<9:11:39, 18.88s/it]

 35%|███▍      | 933/2685 [5:08:25<9:17:46, 19.10s/it]
[2024-03-29 03:15:28,336] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 934/2685 [5:08:48<9:51:03, 20.25s/it]

 35%|███▍      | 935/2685 [5:09:08<9:49:43, 20.22s/it]

 35%|███▍      | 936/2685 [5:09:30<10:06:07, 20.79s/it]
[2024-03-29 03:16:29,882] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▍      | 937/2685 [5:09:50<9:52:30, 20.34s/it]


 35%|███▍      | 939/2685 [5:10:29<9:42:28, 20.02s/it]
{'loss': 0.2737, 'learning_rate': 1.5104115120377783e-06, 'rewards/chosen': -0.6335490345954895, 'rewards/rejected': -2.991980791091919, 'rewards/accuracies': 0.875, 'rewards/margins': 2.358431816101074, 'policy_logps/rejected': -273.0024108886719, 'policy_logps/chosen': -257.9309997558594, 'referece_logps/rejected': -243.08261108398438, 'referece_logps/chosen': -251.59552001953125, 'logits/rejected': -0.8345240354537964, 'logits/chosen': -0.8315654993057251, 'epoch': 1.05}
[2024-03-29 03:17:28,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 35%|███▌      | 940/2685 [5:10:49<9:36:45, 19.83s/it]

 35%|███▌      | 941/2685 [5:11:12<10:12:01, 21.06s/it]

 35%|███▌      | 942/2685 [5:11:32<9:54:36, 20.47s/it]

 35%|███▌      | 943/2685 [5:11:52<9:55:38, 20.52s/it]

 35%|███▌      | 944/2685 [5:12:13<10:01:34, 20.73s/it]


 35%|███▌      | 946/2685 [5:12:51<9:29:18, 19.64s/it]
{'loss': 0.2115, 'learning_rate': 1.5031311641491015e-06, 'rewards/chosen': -1.1177289485931396, 'rewards/rejected': -2.291127920150757, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1733988523483276, 'policy_logps/rejected': -307.05804443359375, 'policy_logps/chosen': -418.1956481933594, 'referece_logps/rejected': -284.14678955078125, 'referece_logps/chosen': -407.01837158203125, 'logits/rejected': -0.7620827555656433, 'logits/chosen': -0.7677716016769409, 'epoch': 1.06}

 35%|███▌      | 947/2685 [5:13:13<9:43:55, 20.16s/it]

 35%|███▌      | 948/2685 [5:13:30<9:19:03, 19.31s/it]

 35%|███▌      | 949/2685 [5:13:48<9:06:59, 18.90s/it]

 35%|███▌      | 950/2685 [5:14:08<9:20:36, 19.39s/it]

 35%|███▌      | 951/2685 [5:14:28<9:25:25, 19.56s/it]
[2024-03-29 03:21:30,265] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 35%|███▌      | 953/2685 [5:15:11<9:52:40, 20.53s/it]
[2024-03-29 03:21:51,517] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.227, 'learning_rate': 1.495814932949933e-06, 'rewards/chosen': -1.4700649976730347, 'rewards/rejected': -5.604170322418213, 'rewards/accuracies': 1.0, 'rewards/margins': 4.134105205535889, 'policy_logps/rejected': -622.2177124023438, 'policy_logps/chosen': -614.0374755859375, 'referece_logps/rejected': -566.176025390625, 'referece_logps/chosen': -599.3367919921875, 'logits/rejected': 0.5796058773994446, 'logits/chosen': 0.5584443211555481, 'epoch': 1.06}


 36%|███▌      | 955/2685 [5:15:53<10:01:31, 20.86s/it]
{'loss': 0.2742, 'learning_rate': 1.4937180633239395e-06, 'rewards/chosen': -0.48806267976760864, 'rewards/rejected': -2.2345356941223145, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7464730739593506, 'policy_logps/rejected': -260.5960998535156, 'policy_logps/chosen': -268.80950927734375, 'referece_logps/rejected': -238.25074768066406, 'referece_logps/chosen': -263.92889404296875, 'logits/rejected': -0.855414092540741, 'logits/chosen': -0.8554130792617798, 'epoch': 1.07}

 36%|███▌      | 956/2685 [5:16:14<10:00:47, 20.85s/it]

 36%|███▌      | 957/2685 [5:16:37<10:16:26, 21.40s/it]
[2024-03-29 03:23:38,482] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 958/2685 [5:16:58<10:15:21, 21.38s/it]

 36%|███▌      | 959/2685 [5:17:16<9:42:24, 20.25s/it]

 36%|███▌      | 960/2685 [5:17:37<9:50:17, 20.53s/it]
[2024-03-29 03:24:37,326] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 961/2685 [5:17:57<9:45:43, 20.39s/it]

 36%|███▌      | 962/2685 [5:18:16<9:36:14, 20.07s/it]

 36%|███▌      | 963/2685 [5:18:35<9:18:34, 19.46s/it]

 36%|███▌      | 964/2685 [5:18:50<8:45:06, 18.31s/it]

 36%|███▌      | 965/2685 [5:19:11<9:03:48, 18.97s/it]

 36%|███▌      | 966/2685 [5:19:32<9:24:24, 19.70s/it]
[2024-03-29 03:26:34,285] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 36%|███▌      | 967/2685 [5:19:54<9:44:15, 20.41s/it]


 36%|███▌      | 969/2685 [5:20:32<9:20:49, 19.61s/it]

 36%|███▌      | 970/2685 [5:20:50<9:06:25, 19.12s/it]

 36%|███▌      | 971/2685 [5:21:10<9:12:28, 19.34s/it]

 36%|███▌      | 972/2685 [5:21:30<9:16:33, 19.49s/it]

 36%|███▌      | 973/2685 [5:21:48<9:06:52, 19.17s/it]
{'loss': 0.2581, 'learning_rate': 1.474718356361469e-06, 'rewards/chosen': -0.8517827987670898, 'rewards/rejected': -2.912186861038208, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0604043006896973, 'policy_logps/rejected': -400.45343017578125, 'policy_logps/chosen': -329.3002624511719, 'referece_logps/rejected': -371.3315734863281, 'referece_logps/chosen': -320.78240966796875, 'logits/rejected': -0.22711679339408875, 'logits/chosen': -0.2325889766216278, 'epoch': 1.09}

 36%|███▋      | 974/2685 [5:22:06<8:59:36, 18.92s/it]

 36%|███▋      | 975/2685 [5:22:27<9:15:01, 19.47s/it]

 36%|███▋      | 976/2685 [5:22:45<9:01:07, 19.00s/it]

 36%|███▋      | 977/2685 [5:23:04<9:04:24, 19.12s/it]

 36%|███▋      | 978/2685 [5:23:23<9:02:22, 19.06s/it]

 36%|███▋      | 979/2685 [5:23:41<8:47:05, 18.54s/it]

 36%|███▋      | 980/2685 [5:24:03<9:16:13, 19.57s/it]

 37%|███▋      | 981/2685 [5:24:23<9:19:08, 19.69s/it]

 37%|███▋      | 982/2685 [5:24:45<9:40:21, 20.45s/it]

 37%|███▋      | 983/2685 [5:25:02<9:10:16, 19.40s/it]

 37%|███▋      | 984/2685 [5:25:24<9:32:21, 20.19s/it]

 37%|███▋      | 985/2685 [5:25:44<9:28:24, 20.06s/it]

 37%|███▋      | 986/2685 [5:26:05<9:36:31, 20.36s/it]

 37%|███▋      | 987/2685 [5:26:25<9:40:01, 20.50s/it]

 37%|███▋      | 988/2685 [5:26:40<8:45:27, 18.58s/it]
[2024-03-29 03:33:41,301] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 989/2685 [5:27:01<9:10:36, 19.48s/it]

 37%|███▋      | 990/2685 [5:27:21<9:12:10, 19.55s/it]
[2024-03-29 03:34:22,384] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 991/2685 [5:27:42<9:27:22, 20.10s/it]
[2024-03-29 03:34:45,269] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 992/2685 [5:28:05<9:50:39, 20.93s/it]

 37%|███▋      | 993/2685 [5:28:25<9:39:33, 20.55s/it]

 37%|███▋      | 994/2685 [5:28:45<9:33:36, 20.35s/it]
[2024-03-29 03:35:44,880] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 995/2685 [5:29:05<9:30:47, 20.26s/it]

 37%|███▋      | 996/2685 [5:29:25<9:34:46, 20.42s/it]
[2024-03-29 03:36:24,454] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 37%|███▋      | 997/2685 [5:29:44<9:20:45, 19.93s/it]

 37%|███▋      | 998/2685 [5:30:02<8:58:40, 19.16s/it]

 37%|███▋      | 999/2685 [5:30:18<8:32:19, 18.23s/it]

 37%|███▋      | 1000/2685 [5:30:40<9:02:25, 19.31s/it]

 37%|███▋      | 1001/2685 [5:31:11<10:43:42, 22.93s/it]

 37%|███▋      | 1002/2685 [5:31:31<10:14:54, 21.92s/it]

 37%|███▋      | 1003/2685 [5:31:50<9:56:08, 21.27s/it]

 37%|███▋      | 1004/2685 [5:32:09<9:30:48, 20.37s/it]

 37%|███▋      | 1005/2685 [5:32:25<8:54:08, 19.08s/it]

 37%|███▋      | 1006/2685 [5:32:44<8:57:33, 19.21s/it]

 38%|███▊      | 1007/2685 [5:33:03<8:50:28, 18.97s/it]

 38%|███▊      | 1008/2685 [5:33:23<9:04:34, 19.48s/it]

 38%|███▊      | 1009/2685 [5:33:46<9:35:29, 20.60s/it]
[2024-03-29 03:40:48,673] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 1010/2685 [5:34:09<9:47:47, 21.06s/it]
[2024-03-29 03:41:11,107] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 1011/2685 [5:34:31<9:58:58, 21.47s/it]

 38%|███▊      | 1012/2685 [5:34:51<9:44:20, 20.96s/it]

 38%|███▊      | 1013/2685 [5:35:11<9:35:12, 20.64s/it]
[2024-03-29 03:42:12,580] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 38%|███▊      | 1014/2685 [5:35:32<9:44:35, 20.99s/it]

 38%|███▊      | 1015/2685 [5:35:56<10:03:52, 21.70s/it]

 38%|███▊      | 1016/2685 [5:36:16<9:47:34, 21.12s/it]

 38%|███▊      | 1017/2685 [5:36:37<9:48:51, 21.18s/it]

 38%|███▊      | 1018/2685 [5:36:55<9:23:40, 20.29s/it]

 38%|███▊      | 1019/2685 [5:37:13<9:07:49, 19.73s/it]

 38%|███▊      | 1020/2685 [5:37:33<9:02:39, 19.56s/it]

 38%|███▊      | 1021/2685 [5:37:50<8:44:15, 18.90s/it]

 38%|███▊      | 1022/2685 [5:38:10<8:51:39, 19.18s/it]

 38%|███▊      | 1023/2685 [5:38:29<8:47:37, 19.05s/it]

 38%|███▊      | 1024/2685 [5:38:48<8:49:06, 19.11s/it]

 38%|███▊      | 1025/2685 [5:39:08<8:54:21, 19.31s/it]

 38%|███▊      | 1026/2685 [5:39:31<9:25:17, 20.44s/it]

 38%|███▊      | 1027/2685 [5:39:50<9:17:14, 20.17s/it]

 38%|███▊      | 1028/2685 [5:40:08<8:55:11, 19.38s/it]


 38%|███▊      | 1030/2685 [5:40:49<9:05:24, 19.77s/it]

 38%|███▊      | 1031/2685 [5:41:09<9:08:00, 19.88s/it]

 38%|███▊      | 1032/2685 [5:41:32<9:34:06, 20.84s/it]

 38%|███▊      | 1033/2685 [5:41:50<9:14:11, 20.13s/it]

 39%|███▊      | 1034/2685 [5:42:12<9:26:00, 20.57s/it]

 39%|███▊      | 1035/2685 [5:42:32<9:19:47, 20.36s/it]

 39%|███▊      | 1036/2685 [5:42:52<9:15:35, 20.22s/it]

 39%|███▊      | 1037/2685 [5:43:12<9:14:35, 20.19s/it]
[2024-03-29 03:49:51,995] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▊      | 1038/2685 [5:43:30<9:00:50, 19.70s/it]

 39%|███▊      | 1039/2685 [5:43:52<9:20:10, 20.42s/it]

 39%|███▊      | 1040/2685 [5:44:13<9:16:39, 20.30s/it]

 39%|███▉      | 1041/2685 [5:44:33<9:15:31, 20.27s/it]

 39%|███▉      | 1042/2685 [5:44:53<9:14:34, 20.25s/it]

 39%|███▉      | 1043/2685 [5:45:10<8:44:56, 19.18s/it]

 39%|███▉      | 1044/2685 [5:45:30<8:53:38, 19.51s/it]

 39%|███▉      | 1045/2685 [5:45:54<9:33:23, 20.98s/it]
[2024-03-29 03:52:34,455] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 1046/2685 [5:46:13<9:18:28, 20.44s/it]
[2024-03-29 03:52:53,656] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 1047/2685 [5:46:33<9:10:47, 20.18s/it]

 39%|███▉      | 1048/2685 [5:46:56<9:29:20, 20.87s/it]
[2024-03-29 03:53:35,686] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 1049/2685 [5:47:17<9:35:23, 21.10s/it]

 39%|███▉      | 1050/2685 [5:47:36<9:18:54, 20.51s/it]

 39%|███▉      | 1051/2685 [5:47:58<9:25:00, 20.75s/it]
[2024-03-29 03:54:37,764] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 1052/2685 [5:48:18<9:20:18, 20.59s/it]
[2024-03-29 03:54:57,978] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 39%|███▉      | 1053/2685 [5:48:36<9:01:50, 19.92s/it]

 39%|███▉      | 1054/2685 [5:48:56<9:01:02, 19.90s/it]

 39%|███▉      | 1055/2685 [5:49:16<9:01:17, 19.92s/it]

 39%|███▉      | 1056/2685 [5:49:33<8:40:04, 19.16s/it]

 39%|███▉      | 1057/2685 [5:49:56<9:05:39, 20.11s/it]

 39%|███▉      | 1058/2685 [5:50:15<8:59:07, 19.88s/it]

 39%|███▉      | 1059/2685 [5:50:36<9:11:14, 20.34s/it]

 39%|███▉      | 1060/2685 [5:50:58<9:18:22, 20.62s/it]
[2024-03-29 03:57:37,902] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 1061/2685 [5:51:16<8:56:29, 19.82s/it]
[2024-03-29 03:57:55,864] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 40%|███▉      | 1062/2685 [5:51:37<9:10:50, 20.36s/it]

 40%|███▉      | 1063/2685 [5:51:59<9:24:43, 20.89s/it]

 40%|███▉      | 1064/2685 [5:52:20<9:20:33, 20.75s/it]

 40%|███▉      | 1065/2685 [5:52:39<9:05:24, 20.20s/it]

 40%|███▉      | 1066/2685 [5:52:54<8:27:17, 18.80s/it]

 40%|███▉      | 1067/2685 [5:53:15<8:40:18, 19.29s/it]

 40%|███▉      | 1068/2685 [5:53:34<8:37:08, 19.19s/it]
{'loss': 0.246, 'learning_rate': 1.370949600869768e-06, 'rewards/chosen': -0.08092689514160156, 'rewards/rejected': -3.4825661182403564, 'rewards/accuracies': 0.875, 'rewards/margins': 3.401639223098755, 'policy_logps/rejected': -335.26593017578125, 'policy_logps/chosen': -330.053955078125, 'referece_logps/rejected': -300.4402770996094, 'referece_logps/chosen': -329.24468994140625, 'logits/rejected': -0.6246652007102966, 'logits/chosen': -0.6532902717590332, 'epoch': 1.19}


 40%|███▉      | 1070/2685 [5:54:07<7:54:41, 17.64s/it]
{'loss': 0.3411, 'learning_rate': 1.3687077786179488e-06, 'rewards/chosen': -1.3983449935913086, 'rewards/rejected': -2.682589054107666, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2842440605163574, 'policy_logps/rejected': -228.83961486816406, 'policy_logps/chosen': -221.86624145507812, 'referece_logps/rejected': -202.01370239257812, 'referece_logps/chosen': -207.88278198242188, 'logits/rejected': -1.5160679817199707, 'logits/chosen': -1.51205313205719, 'epoch': 1.2}

 40%|███▉      | 1071/2685 [5:54:22<7:38:14, 17.04s/it]

 40%|███▉      | 1072/2685 [5:54:43<8:03:07, 17.97s/it]


 40%|████      | 1074/2685 [5:55:24<8:41:03, 19.41s/it]

 40%|████      | 1075/2685 [5:55:41<8:22:47, 18.74s/it]

 40%|████      | 1076/2685 [5:55:58<8:07:25, 18.18s/it]
{'loss': 0.349, 'learning_rate': 1.3619694842738727e-06, 'rewards/chosen': -0.4838798940181732, 'rewards/rejected': -2.2135910987854004, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7297110557556152, 'policy_logps/rejected': -206.84445190429688, 'policy_logps/chosen': -164.96334838867188, 'referece_logps/rejected': -184.7085418701172, 'referece_logps/chosen': -160.12457275390625, 'logits/rejected': -1.796769380569458, 'logits/chosen': -1.8490750789642334, 'epoch': 1.2}

 40%|████      | 1077/2685 [5:56:18<8:24:26, 18.82s/it]


 40%|████      | 1079/2685 [5:56:56<8:16:30, 18.55s/it]

 40%|████      | 1080/2685 [5:57:16<8:29:35, 19.05s/it]

 40%|████      | 1081/2685 [5:57:35<8:30:39, 19.10s/it]

 40%|████      | 1082/2685 [5:57:56<8:39:54, 19.46s/it]

 40%|████      | 1083/2685 [5:58:16<8:42:20, 19.56s/it]

 40%|████      | 1084/2685 [5:58:33<8:26:10, 18.97s/it]

 40%|████      | 1085/2685 [5:58:48<7:52:41, 17.73s/it]

 40%|████      | 1086/2685 [5:59:08<8:07:51, 18.31s/it]

 40%|████      | 1087/2685 [5:59:25<8:03:36, 18.16s/it]
{'loss': 0.2782, 'learning_rate': 1.349566939484627e-06, 'rewards/chosen': -1.4458106756210327, 'rewards/rejected': -4.560778617858887, 'rewards/accuracies': 0.75, 'rewards/margins': 3.1149678230285645, 'policy_logps/rejected': -380.13641357421875, 'policy_logps/chosen': -294.05511474609375, 'referece_logps/rejected': -334.52862548828125, 'referece_logps/chosen': -279.59698486328125, 'logits/rejected': -0.30984753370285034, 'logits/chosen': -0.40583688020706177, 'epoch': 1.21}

 41%|████      | 1088/2685 [5:59:47<8:26:45, 19.04s/it]


 41%|████      | 1090/2685 [6:00:28<8:45:53, 19.78s/it]

 41%|████      | 1091/2685 [6:00:46<8:32:38, 19.30s/it]

 41%|████      | 1092/2685 [6:01:06<8:37:02, 19.47s/it]

 41%|████      | 1093/2685 [6:01:28<8:58:19, 20.29s/it]
{'loss': 0.3006, 'learning_rate': 1.3427758266361972e-06, 'rewards/chosen': -1.866511344909668, 'rewards/rejected': -3.702136754989624, 'rewards/accuracies': 0.5, 'rewards/margins': 1.835625410079956, 'policy_logps/rejected': -293.0505676269531, 'policy_logps/chosen': -381.92071533203125, 'referece_logps/rejected': -256.02923583984375, 'referece_logps/chosen': -363.255615234375, 'logits/rejected': -1.0230143070220947, 'logits/chosen': -1.0413289070129395, 'epoch': 1.22}


 41%|████      | 1095/2685 [6:02:07<8:50:12, 20.01s/it]

 41%|████      | 1096/2685 [6:02:24<8:26:17, 19.12s/it]
{'loss': 0.3226, 'learning_rate': 1.3393735125897923e-06, 'rewards/chosen': -0.643786609172821, 'rewards/rejected': -3.719560384750366, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0757737159729004, 'policy_logps/rejected': -402.69317626953125, 'policy_logps/chosen': -408.4437255859375, 'referece_logps/rejected': -365.49755859375, 'referece_logps/chosen': -402.005859375, 'logits/rejected': -0.7601785659790039, 'logits/chosen': -0.7039514780044556, 'epoch': 1.22}

 41%|████      | 1097/2685 [6:02:45<8:36:00, 19.50s/it]

 41%|████      | 1098/2685 [6:03:05<8:40:39, 19.68s/it]


 41%|████      | 1100/2685 [6:03:45<8:48:15, 20.00s/it]
{'loss': 0.3306, 'learning_rate': 1.3348301860776557e-06, 'rewards/chosen': -1.7644342184066772, 'rewards/rejected': -4.606782913208008, 'rewards/accuracies': 0.875, 'rewards/margins': 2.84234881401062, 'policy_logps/rejected': -436.1386413574219, 'policy_logps/chosen': -512.467041015625, 'referece_logps/rejected': -390.07080078125, 'referece_logps/chosen': -494.82269287109375, 'logits/rejected': -0.6304295659065247, 'logits/chosen': -0.7222470045089722, 'epoch': 1.23}


 41%|████      | 1102/2685 [6:04:24<8:43:57, 19.86s/it]

 41%|████      | 1103/2685 [6:04:44<8:43:01, 19.84s/it]

 41%|████      | 1104/2685 [6:05:05<8:49:31, 20.10s/it]
{'loss': 0.3312, 'learning_rate': 1.3302790619551672e-06, 'rewards/chosen': -1.932501196861267, 'rewards/rejected': -5.635490417480469, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7029895782470703, 'policy_logps/rejected': -402.0472412109375, 'policy_logps/chosen': -405.2653503417969, 'referece_logps/rejected': -345.6922912597656, 'referece_logps/chosen': -385.9403381347656, 'logits/rejected': 0.6071727275848389, 'logits/chosen': 0.5401935577392578, 'epoch': 1.23}

 41%|████      | 1105/2685 [6:05:27<9:05:44, 20.72s/it]


 41%|████      | 1107/2685 [6:06:10<9:15:43, 21.13s/it]

 41%|████▏     | 1108/2685 [6:06:26<8:37:09, 19.68s/it]

 41%|████▏     | 1109/2685 [6:06:46<8:42:41, 19.90s/it]

 41%|████▏     | 1110/2685 [6:07:07<8:52:10, 20.27s/it]
{'loss': 0.3109, 'learning_rate': 1.323437987149238e-06, 'rewards/chosen': -1.2866352796554565, 'rewards/rejected': -3.126744031906128, 'rewards/accuracies': 0.75, 'rewards/margins': 1.840108871459961, 'policy_logps/rejected': -325.61669921875, 'policy_logps/chosen': -321.5874938964844, 'referece_logps/rejected': -294.3492736816406, 'referece_logps/chosen': -308.72113037109375, 'logits/rejected': -1.12535560131073, 'logits/chosen': -1.1508632898330688, 'epoch': 1.24}
[2024-03-29 04:14:09,303] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 41%|████▏     | 1112/2685 [6:07:50<9:00:40, 20.62s/it]
[2024-03-29 04:14:29,752] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2681, 'learning_rate': 1.3211538450091923e-06, 'rewards/chosen': -0.7400341033935547, 'rewards/rejected': -3.4050936698913574, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6650595664978027, 'policy_logps/rejected': -442.25189208984375, 'policy_logps/chosen': -435.35162353515625, 'referece_logps/rejected': -408.2009582519531, 'referece_logps/chosen': -427.9513244628906, 'logits/rejected': 0.6480139493942261, 'logits/chosen': 0.6605417132377625, 'epoch': 1.24}


 41%|████▏     | 1114/2685 [6:08:32<9:10:15, 21.02s/it]

 42%|████▏     | 1115/2685 [6:08:52<9:00:08, 20.64s/it]
{'loss': 0.2643, 'learning_rate': 1.3177241301188948e-06, 'rewards/chosen': -1.8839502334594727, 'rewards/rejected': -4.823649883270264, 'rewards/accuracies': 1.0, 'rewards/margins': 2.93969988822937, 'policy_logps/rejected': -642.4547729492188, 'policy_logps/chosen': -549.23046875, 'referece_logps/rejected': -594.21826171875, 'referece_logps/chosen': -530.3909912109375, 'logits/rejected': 0.059184812009334564, 'logits/chosen': 0.11908555775880814, 'epoch': 1.25}

 42%|████▏     | 1116/2685 [6:09:11<8:50:21, 20.28s/it]


 42%|████▏     | 1118/2685 [6:09:54<9:04:54, 20.86s/it]
{'loss': 0.2793, 'learning_rate': 1.3142902531524625e-06, 'rewards/chosen': -0.8324161767959595, 'rewards/rejected': -3.910644054412842, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0782272815704346, 'policy_logps/rejected': -365.06304931640625, 'policy_logps/chosen': -390.89013671875, 'referece_logps/rejected': -325.9566345214844, 'referece_logps/chosen': -382.5659484863281, 'logits/rejected': -0.06872092187404633, 'logits/chosen': -0.071868896484375, 'epoch': 1.25}
[2024-03-29 04:16:53,434] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 42%|████▏     | 1120/2685 [6:10:33<8:46:31, 20.19s/it]

 42%|████▏     | 1121/2685 [6:10:54<8:54:01, 20.49s/it]

 42%|████▏     | 1122/2685 [6:11:10<8:19:03, 19.16s/it]

 42%|████▏     | 1123/2685 [6:11:33<8:48:00, 20.28s/it]
[2024-03-29 04:18:13,069] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 1124/2685 [6:11:50<8:23:46, 19.36s/it]

 42%|████▏     | 1125/2685 [6:12:15<9:04:38, 20.95s/it]
[2024-03-29 04:18:54,933] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 42%|████▏     | 1126/2685 [6:12:35<8:55:41, 20.62s/it]

 42%|████▏     | 1127/2685 [6:12:56<9:00:46, 20.83s/it]

 42%|████▏     | 1128/2685 [6:13:19<9:18:51, 21.54s/it]
{'loss': 0.3259, 'learning_rate': 1.3028145151160641e-06, 'rewards/chosen': -1.3750481605529785, 'rewards/rejected': -3.5071706771850586, 'rewards/accuracies': 0.875, 'rewards/margins': 2.13212251663208, 'policy_logps/rejected': -319.9674987792969, 'policy_logps/chosen': -249.4321746826172, 'referece_logps/rejected': -284.8957824707031, 'referece_logps/chosen': -235.68167114257812, 'logits/rejected': -0.7016403079032898, 'logits/chosen': -0.6724379062652588, 'epoch': 1.26}


 42%|████▏     | 1130/2685 [6:14:03<9:20:06, 21.61s/it]

 42%|████▏     | 1131/2685 [6:14:22<9:04:16, 21.01s/it]
{'loss': 0.3191, 'learning_rate': 1.2993631229733582e-06, 'rewards/chosen': -0.46013012528419495, 'rewards/rejected': -2.6495330333709717, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1894028186798096, 'policy_logps/rejected': -320.0093994140625, 'policy_logps/chosen': -345.6078186035156, 'referece_logps/rejected': -293.5140686035156, 'referece_logps/chosen': -341.00653076171875, 'logits/rejected': -1.1919639110565186, 'logits/chosen': -0.9762730598449707, 'epoch': 1.26}


 42%|████▏     | 1133/2685 [6:15:01<8:42:15, 20.19s/it]

 42%|████▏     | 1134/2685 [6:15:21<8:36:46, 19.99s/it]
{'loss': 0.2366, 'learning_rate': 1.2959078092773557e-06, 'rewards/chosen': -0.6149682998657227, 'rewards/rejected': -3.4567511081695557, 'rewards/accuracies': 0.875, 'rewards/margins': 2.841783046722412, 'policy_logps/rejected': -214.62319946289062, 'policy_logps/chosen': -205.20437622070312, 'referece_logps/rejected': -180.0556640625, 'referece_logps/chosen': -199.05467224121094, 'logits/rejected': -1.1028680801391602, 'logits/chosen': -0.9782888293266296, 'epoch': 1.27}


 42%|████▏     | 1136/2685 [6:16:01<8:43:01, 20.26s/it]

 42%|████▏     | 1137/2685 [6:16:21<8:36:43, 20.03s/it]
{'loss': 0.2397, 'learning_rate': 1.2924486192914704e-06, 'rewards/chosen': -0.1917787492275238, 'rewards/rejected': -3.411910057067871, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2201313972473145, 'policy_logps/rejected': -329.72540283203125, 'policy_logps/chosen': -408.29425048828125, 'referece_logps/rejected': -295.60626220703125, 'referece_logps/chosen': -406.37646484375, 'logits/rejected': -0.0873170867562294, 'logits/chosen': -0.11627103388309479, 'epoch': 1.27}


 42%|████▏     | 1139/2685 [6:17:05<9:02:20, 21.05s/it]

 42%|████▏     | 1140/2685 [6:17:23<8:39:22, 20.17s/it]

 42%|████▏     | 1141/2685 [6:17:43<8:34:24, 19.99s/it]

 43%|████▎     | 1142/2685 [6:18:05<8:47:54, 20.53s/it]

 43%|████▎     | 1143/2685 [6:18:21<8:15:35, 19.28s/it]

 43%|████▎     | 1144/2685 [6:18:39<8:07:57, 19.00s/it]

 43%|████▎     | 1145/2685 [6:19:01<8:28:36, 19.82s/it]
[2024-03-29 04:25:41,185] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 1146/2685 [6:19:15<7:44:21, 18.10s/it]
[2024-03-29 04:25:55,294] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 43%|████▎     | 1147/2685 [6:19:37<8:09:35, 19.10s/it]

 43%|████▎     | 1148/2685 [6:19:56<8:14:28, 19.30s/it]

 43%|████▎     | 1149/2685 [6:20:16<8:19:58, 19.53s/it]

 43%|████▎     | 1150/2685 [6:20:35<8:11:40, 19.22s/it]

 43%|████▎     | 1151/2685 [6:20:52<7:52:02, 18.46s/it]
{'loss': 0.2806, 'learning_rate': 1.2762558123216042e-06, 'rewards/chosen': -0.8523910045623779, 'rewards/rejected': -2.7425358295440674, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8901448249816895, 'policy_logps/rejected': -252.51617431640625, 'policy_logps/chosen': -236.653076171875, 'referece_logps/rejected': -225.0908203125, 'referece_logps/chosen': -228.129150390625, 'logits/rejected': -0.9256304502487183, 'logits/chosen': -0.8171855211257935, 'epoch': 1.29}


 43%|████▎     | 1153/2685 [6:21:27<7:38:41, 17.96s/it]

 43%|████▎     | 1154/2685 [6:21:49<8:11:37, 19.27s/it]

 43%|████▎     | 1155/2685 [6:22:09<8:15:31, 19.43s/it]

 43%|████▎     | 1156/2685 [6:22:31<8:32:59, 20.13s/it]

 43%|████▎     | 1157/2685 [6:22:51<8:33:17, 20.16s/it]
[2024-03-29 04:29:31,087] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.351, 'learning_rate': 1.2692916425862123e-06, 'rewards/chosen': -1.2048966884613037, 'rewards/rejected': -2.8480608463287354, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6431642770767212, 'policy_logps/rejected': -441.86810302734375, 'policy_logps/chosen': -370.8147888183594, 'referece_logps/rejected': -413.3874206542969, 'referece_logps/chosen': -358.7658386230469, 'logits/rejected': -0.052267178893089294, 'logits/chosen': -0.10506945103406906, 'epoch': 1.29}
[2024-03-29 04:29:52,420] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 43%|████▎     | 1159/2685 [6:23:34<8:48:08, 20.77s/it]
[2024-03-29 04:30:13,786] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4049, 'learning_rate': 1.2669670989741516e-06, 'rewards/chosen': -1.1090266704559326, 'rewards/rejected': -3.7531843185424805, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6441574096679688, 'policy_logps/rejected': -419.8664245605469, 'policy_logps/chosen': -452.9476623535156, 'referece_logps/rejected': -382.3345947265625, 'referece_logps/chosen': -441.85736083984375, 'logits/rejected': 0.5256050825119019, 'logits/chosen': 0.5057214498519897, 'epoch': 1.29}
[2024-03-29 04:30:32,624] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 43%|████▎     | 1161/2685 [6:24:07<7:52:15, 18.59s/it]

 43%|████▎     | 1162/2685 [6:24:23<7:27:56, 17.65s/it]
{'loss': 0.3079, 'learning_rate': 1.2634773734722095e-06, 'rewards/chosen': 0.7665106058120728, 'rewards/rejected': -1.9149012565612793, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6814117431640625, 'policy_logps/rejected': -315.1399230957031, 'policy_logps/chosen': -267.6792297363281, 'referece_logps/rejected': -295.9909362792969, 'referece_logps/chosen': -275.3443603515625, 'logits/rejected': -0.8448473215103149, 'logits/chosen': -0.7399744987487793, 'epoch': 1.3}

 43%|████▎     | 1163/2685 [6:24:43<7:43:37, 18.28s/it]

 43%|████▎     | 1164/2685 [6:25:06<8:23:15, 19.85s/it]
[2024-03-29 04:32:08,522] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 43%|████▎     | 1166/2685 [6:25:50<8:49:29, 20.91s/it]

 43%|████▎     | 1167/2685 [6:26:09<8:34:24, 20.33s/it]
{'loss': 0.2685, 'learning_rate': 1.2576535169812614e-06, 'rewards/chosen': -1.5583322048187256, 'rewards/rejected': -4.769442558288574, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2111103534698486, 'policy_logps/rejected': -394.941650390625, 'policy_logps/chosen': -345.40008544921875, 'referece_logps/rejected': -347.2472229003906, 'referece_logps/chosen': -329.8167419433594, 'logits/rejected': -0.41094645857810974, 'logits/chosen': -0.5409459471702576, 'epoch': 1.3}
[2024-03-29 04:33:08,404] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 44%|████▎     | 1168/2685 [6:26:28<8:25:45, 20.00s/it]

 44%|████▎     | 1169/2685 [6:26:51<8:48:06, 20.90s/it]


 44%|████▎     | 1171/2685 [6:27:30<8:21:45, 19.88s/it]

 44%|████▎     | 1172/2685 [6:27:49<8:19:49, 19.82s/it]
{'loss': 0.3728, 'learning_rate': 1.2518202850310247e-06, 'rewards/chosen': -0.7412211298942566, 'rewards/rejected': -5.176620006561279, 'rewards/accuracies': 1.0, 'rewards/margins': 4.435398578643799, 'policy_logps/rejected': -368.3845520019531, 'policy_logps/chosen': -389.54058837890625, 'referece_logps/rejected': -316.61834716796875, 'referece_logps/chosen': -382.1283874511719, 'logits/rejected': -0.5614388585090637, 'logits/chosen': -0.7439941763877869, 'epoch': 1.31}

 44%|████▎     | 1173/2685 [6:28:09<8:15:18, 19.66s/it]


 44%|████▍     | 1175/2685 [6:28:50<8:25:06, 20.07s/it]

 44%|████▍     | 1176/2685 [6:29:11<8:33:41, 20.42s/it]
{'loss': 0.4106, 'learning_rate': 1.2471470917668015e-06, 'rewards/chosen': -1.3695710897445679, 'rewards/rejected': -2.394796848297119, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0252258777618408, 'policy_logps/rejected': -361.1260986328125, 'policy_logps/chosen': -218.06427001953125, 'referece_logps/rejected': -337.1780700683594, 'referece_logps/chosen': -204.36856079101562, 'logits/rejected': -0.6107816100120544, 'logits/chosen': -0.6666334271430969, 'epoch': 1.31}

 44%|████▍     | 1177/2685 [6:29:35<8:54:25, 21.26s/it]

 44%|████▍     | 1178/2685 [6:29:56<8:57:28, 21.40s/it]


 44%|████▍     | 1180/2685 [6:30:34<8:20:15, 19.94s/it]
{'loss': 0.2976, 'learning_rate': 1.24246814287838e-06, 'rewards/chosen': -0.27019450068473816, 'rewards/rejected': -2.723477363586426, 'rewards/accuracies': 1.0, 'rewards/margins': 2.453282594680786, 'policy_logps/rejected': -349.7692565917969, 'policy_logps/chosen': -304.1658935546875, 'referece_logps/rejected': -322.5344543457031, 'referece_logps/chosen': -301.4639587402344, 'logits/rejected': -0.5403600335121155, 'logits/chosen': -0.5255162715911865, 'epoch': 1.32}


 44%|████▍     | 1182/2685 [6:31:14<8:20:15, 19.97s/it]
{'loss': 0.4162, 'learning_rate': 1.2401265441213501e-06, 'rewards/chosen': -1.4803868532180786, 'rewards/rejected': -1.8536438941955566, 'rewards/accuracies': 0.5, 'rewards/margins': 0.37325721979141235, 'policy_logps/rejected': -353.0679931640625, 'policy_logps/chosen': -372.23248291015625, 'referece_logps/rejected': -334.5315246582031, 'referece_logps/chosen': -357.4285888671875, 'logits/rejected': 0.10136888921260834, 'logits/chosen': 0.1915070116519928, 'epoch': 1.32}


 44%|████▍     | 1184/2685 [6:31:50<7:52:53, 18.90s/it]

 44%|████▍     | 1185/2685 [6:32:10<8:03:51, 19.35s/it]
[2024-03-29 04:38:50,487] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3786, 'learning_rate': 1.236611528934562e-06, 'rewards/chosen': -1.7302584648132324, 'rewards/rejected': -4.075641632080078, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3453831672668457, 'policy_logps/rejected': -444.37646484375, 'policy_logps/chosen': -419.24652099609375, 'referece_logps/rejected': -403.62005615234375, 'referece_logps/chosen': -401.9439392089844, 'logits/rejected': 0.044998105615377426, 'logits/chosen': 0.08877910673618317, 'epoch': 1.32}

 44%|████▍     | 1186/2685 [6:32:34<8:33:12, 20.54s/it]

 44%|████▍     | 1187/2685 [6:32:54<8:29:23, 20.40s/it]

 44%|████▍     | 1188/2685 [6:33:14<8:28:00, 20.36s/it]
{'loss': 0.3726, 'learning_rate': 1.2330934142186403e-06, 'rewards/chosen': -1.7790545225143433, 'rewards/rejected': -2.859714984893799, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0806602239608765, 'policy_logps/rejected': -361.3276672363281, 'policy_logps/chosen': -266.2667541503906, 'referece_logps/rejected': -332.73052978515625, 'referece_logps/chosen': -248.4761962890625, 'logits/rejected': -1.3886380195617676, 'logits/chosen': -1.3300654888153076, 'epoch': 1.33}

 44%|████▍     | 1189/2685 [6:33:34<8:21:42, 20.12s/it]

 44%|████▍     | 1190/2685 [6:33:55<8:31:33, 20.53s/it]

 44%|████▍     | 1191/2685 [6:34:15<8:29:26, 20.46s/it]


 44%|████▍     | 1193/2685 [6:34:54<8:11:05, 19.75s/it]

 44%|████▍     | 1194/2685 [6:35:16<8:26:38, 20.39s/it]

 45%|████▍     | 1195/2685 [6:35:38<8:37:14, 20.83s/it]
[2024-03-29 04:42:18,191] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 1196/2685 [6:35:54<8:03:57, 19.50s/it]
[2024-03-29 04:42:34,596] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 1197/2685 [6:36:10<7:36:31, 18.41s/it]

 45%|████▍     | 1198/2685 [6:36:33<8:04:59, 19.57s/it]
[2024-03-29 04:43:12,731] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.336, 'learning_rate': 1.221344571698328e-06, 'rewards/chosen': -0.8803696632385254, 'rewards/rejected': -3.151238203048706, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2708683013916016, 'policy_logps/rejected': -401.51702880859375, 'policy_logps/chosen': -183.29396057128906, 'referece_logps/rejected': -370.004638671875, 'referece_logps/chosen': -174.49026489257812, 'logits/rejected': -0.4716147184371948, 'logits/chosen': -0.4868907332420349, 'epoch': 1.34}

 45%|████▍     | 1199/2685 [6:36:49<7:41:59, 18.65s/it]


 45%|████▍     | 1201/2685 [6:37:29<7:58:54, 19.36s/it]
[2024-03-29 04:44:08,755] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▍     | 1202/2685 [6:37:47<7:47:58, 18.93s/it]

 45%|████▍     | 1203/2685 [6:38:06<7:55:04, 19.23s/it]
{'loss': 0.324, 'learning_rate': 1.2154579621022776e-06, 'rewards/chosen': -1.7818410396575928, 'rewards/rejected': -3.9515721797943115, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1697306632995605, 'policy_logps/rejected': -456.7221984863281, 'policy_logps/chosen': -390.5204772949219, 'referece_logps/rejected': -417.20648193359375, 'referece_logps/chosen': -372.7020263671875, 'logits/rejected': -0.19361288845539093, 'logits/chosen': -0.19528573751449585, 'epoch': 1.34}
[2024-03-29 04:45:07,137] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 45%|████▍     | 1205/2685 [6:38:47<8:04:38, 19.65s/it]

 45%|████▍     | 1206/2685 [6:39:07<8:05:48, 19.71s/it]
{'loss': 0.2419, 'learning_rate': 1.2119222193981483e-06, 'rewards/chosen': -1.2764558792114258, 'rewards/rejected': -4.932955741882324, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6564996242523193, 'policy_logps/rejected': -271.4229431152344, 'policy_logps/chosen': -287.79833984375, 'referece_logps/rejected': -222.09336853027344, 'referece_logps/chosen': -275.0337829589844, 'logits/rejected': -1.6451503038406372, 'logits/chosen': -1.5850826501846313, 'epoch': 1.35}


 45%|████▍     | 1208/2685 [6:39:46<7:59:07, 19.46s/it]
{'loss': 0.3172, 'learning_rate': 1.2095635124527487e-06, 'rewards/chosen': -1.2570031881332397, 'rewards/rejected': -3.0637807846069336, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8067777156829834, 'policy_logps/rejected': -338.52264404296875, 'policy_logps/chosen': -303.7348937988281, 'referece_logps/rejected': -307.88482666015625, 'referece_logps/chosen': -291.1648864746094, 'logits/rejected': -0.9082576036453247, 'logits/chosen': -0.9325645565986633, 'epoch': 1.35}


 45%|████▌     | 1210/2685 [6:40:32<8:42:09, 21.24s/it]
[2024-03-29 04:47:12,572] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 45%|████▌     | 1211/2685 [6:40:53<8:33:03, 20.88s/it]
[2024-03-29 04:47:32,626] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3282, 'learning_rate': 1.2060231686520652e-06, 'rewards/chosen': -1.438894510269165, 'rewards/rejected': -4.013341903686523, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5744471549987793, 'policy_logps/rejected': -356.3809814453125, 'policy_logps/chosen': -315.8013000488281, 'referece_logps/rejected': -316.24755859375, 'referece_logps/chosen': -301.41229248046875, 'logits/rejected': -0.4791637659072876, 'logits/chosen': -0.47568631172180176, 'epoch': 1.35}


 45%|████▌     | 1213/2685 [6:41:35<8:34:53, 20.99s/it]

 45%|████▌     | 1214/2685 [6:41:57<8:41:34, 21.27s/it]
{'loss': 0.2514, 'learning_rate': 1.202480126019181e-06, 'rewards/chosen': -2.1977035999298096, 'rewards/rejected': -5.1426167488098145, 'rewards/accuracies': 0.875, 'rewards/margins': 2.944913148880005, 'policy_logps/rejected': -375.8448486328125, 'policy_logps/chosen': -330.4261474609375, 'referece_logps/rejected': -324.418701171875, 'referece_logps/chosen': -308.4490966796875, 'logits/rejected': -0.5679353475570679, 'logits/chosen': -0.4994838237762451, 'epoch': 1.36}

 45%|████▌     | 1215/2685 [6:42:18<8:42:19, 21.32s/it]

 45%|████▌     | 1216/2685 [6:42:40<8:45:13, 21.45s/it]

 45%|████▌     | 1217/2685 [6:43:01<8:45:18, 21.47s/it]

 45%|████▌     | 1218/2685 [6:43:20<8:21:50, 20.53s/it]


 45%|████▌     | 1220/2685 [6:44:05<8:49:13, 21.67s/it]

 45%|████▌     | 1221/2685 [6:44:27<8:53:02, 21.85s/it]
{'loss': 0.3703, 'learning_rate': 1.1942027918618073e-06, 'rewards/chosen': -0.5907466411590576, 'rewards/rejected': -3.480508804321289, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8897619247436523, 'policy_logps/rejected': -362.6899719238281, 'policy_logps/chosen': -352.10858154296875, 'referece_logps/rejected': -327.8848876953125, 'referece_logps/chosen': -346.2010803222656, 'logits/rejected': -0.20088055729866028, 'logits/chosen': -0.21320310235023499, 'epoch': 1.36}

 46%|████▌     | 1222/2685 [6:44:46<8:32:57, 21.04s/it]


 46%|████▌     | 1224/2685 [6:45:25<8:18:30, 20.47s/it]

 46%|████▌     | 1225/2685 [6:45:47<8:26:10, 20.80s/it]
{'loss': 0.3075, 'learning_rate': 1.1894666301129824e-06, 'rewards/chosen': -0.6574336886405945, 'rewards/rejected': -3.1436867713928223, 'rewards/accuracies': 1.0, 'rewards/margins': 2.486253261566162, 'policy_logps/rejected': -364.1593017578125, 'policy_logps/chosen': -338.07232666015625, 'referece_logps/rejected': -332.722412109375, 'referece_logps/chosen': -331.498046875, 'logits/rejected': -0.3529016971588135, 'logits/chosen': -0.3901911973953247, 'epoch': 1.37}
[2024-03-29 04:52:50,527] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 1226/2685 [6:46:10<8:47:45, 21.70s/it]

 46%|████▌     | 1227/2685 [6:46:31<8:36:17, 21.25s/it]
{'loss': 0.2347, 'learning_rate': 1.1870968877108545e-06, 'rewards/chosen': -1.264898657798767, 'rewards/rejected': -4.249031066894531, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9841322898864746, 'policy_logps/rejected': -358.2904357910156, 'policy_logps/chosen': -334.66424560546875, 'referece_logps/rejected': -315.8001403808594, 'referece_logps/chosen': -322.0152282714844, 'logits/rejected': -0.2284063696861267, 'logits/chosen': -0.22628027200698853, 'epoch': 1.37}

 46%|████▌     | 1228/2685 [6:46:50<8:20:49, 20.62s/it]

 46%|████▌     | 1229/2685 [6:47:10<8:14:52, 20.39s/it]


 46%|████▌     | 1231/2685 [6:47:43<7:18:22, 18.09s/it]

 46%|████▌     | 1232/2685 [6:48:05<7:46:54, 19.28s/it]
{'loss': 0.2419, 'learning_rate': 1.181167796251171e-06, 'rewards/chosen': -1.2157944440841675, 'rewards/rejected': -3.4324655532836914, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2166709899902344, 'policy_logps/rejected': -350.7979736328125, 'policy_logps/chosen': -364.2779541015625, 'referece_logps/rejected': -316.4732971191406, 'referece_logps/chosen': -352.1200256347656, 'logits/rejected': -1.1470980644226074, 'logits/chosen': -1.0958342552185059, 'epoch': 1.38}

 46%|████▌     | 1233/2685 [6:48:25<7:46:50, 19.29s/it]

 46%|████▌     | 1234/2685 [6:48:42<7:34:39, 18.80s/it]
[2024-03-29 04:55:43,894] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 1235/2685 [6:49:04<7:53:25, 19.59s/it]
[2024-03-29 04:56:06,572] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 46%|████▌     | 1237/2685 [6:49:49<8:30:57, 21.17s/it]
[2024-03-29 04:56:29,275] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▌     | 1238/2685 [6:50:06<7:56:12, 19.75s/it]

 46%|████▌     | 1239/2685 [6:50:27<8:10:10, 20.34s/it]
{'loss': 0.2778, 'learning_rate': 1.1728560415036199e-06, 'rewards/chosen': -2.597426652908325, 'rewards/rejected': -5.587208271026611, 'rewards/accuracies': 0.75, 'rewards/margins': 2.989781379699707, 'policy_logps/rejected': -414.55731201171875, 'policy_logps/chosen': -380.0118713378906, 'referece_logps/rejected': -358.68524169921875, 'referece_logps/chosen': -354.0376281738281, 'logits/rejected': -0.12158394604921341, 'logits/chosen': -0.10437603294849396, 'epoch': 1.38}

 46%|████▌     | 1240/2685 [6:50:48<8:15:44, 20.58s/it]

 46%|████▌     | 1241/2685 [6:51:04<7:41:34, 19.18s/it]

 46%|████▋     | 1242/2685 [6:51:26<7:56:43, 19.82s/it]
[2024-03-29 04:58:26,599] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 1243/2685 [6:51:46<8:03:27, 20.12s/it]

 46%|████▋     | 1244/2685 [6:52:06<8:02:22, 20.08s/it]
[2024-03-29 04:59:08,399] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 46%|████▋     | 1245/2685 [6:52:28<8:14:17, 20.60s/it]


 46%|████▋     | 1247/2685 [6:53:09<8:14:18, 20.62s/it]
{'loss': 0.2799, 'learning_rate': 1.163341832204388e-06, 'rewards/chosen': -1.473531723022461, 'rewards/rejected': -4.182964324951172, 'rewards/accuracies': 0.875, 'rewards/margins': 2.709432601928711, 'policy_logps/rejected': -627.2371826171875, 'policy_logps/chosen': -428.4143371582031, 'referece_logps/rejected': -585.407470703125, 'referece_logps/chosen': -413.67901611328125, 'logits/rejected': 0.9015706181526184, 'logits/chosen': 0.8333273530006409, 'epoch': 1.39}

 46%|████▋     | 1248/2685 [6:53:33<8:34:20, 21.48s/it]
[2024-03-29 05:00:35,013] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1249/2685 [6:53:55<8:37:52, 21.64s/it]

 47%|████▋     | 1250/2685 [6:54:09<7:42:19, 19.33s/it]


 47%|████▋     | 1252/2685 [6:54:50<7:57:44, 20.00s/it]
[2024-03-29 05:01:29,943] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1253/2685 [6:55:09<7:53:37, 19.84s/it]
{'loss': 0.2131, 'learning_rate': 1.1561961414076597e-06, 'rewards/chosen': -0.9071308374404907, 'rewards/rejected': -3.844803810119629, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9376730918884277, 'policy_logps/rejected': -288.0238037109375, 'policy_logps/chosen': -322.060791015625, 'referece_logps/rejected': -249.57577514648438, 'referece_logps/chosen': -312.9894714355469, 'logits/rejected': -0.7323135137557983, 'logits/chosen': -0.7309303283691406, 'epoch': 1.4}
[2024-03-29 05:02:13,231] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 47%|████▋     | 1255/2685 [6:55:53<8:16:35, 20.84s/it]

 47%|████▋     | 1256/2685 [6:56:11<7:53:57, 19.90s/it]

 47%|████▋     | 1257/2685 [6:56:34<8:11:33, 20.65s/it]

 47%|████▋     | 1258/2685 [6:56:54<8:09:14, 20.57s/it]
{'loss': 0.235, 'learning_rate': 1.1502351311753827e-06, 'rewards/chosen': -1.4544018507003784, 'rewards/rejected': -4.223439693450928, 'rewards/accuracies': 0.875, 'rewards/margins': 2.769037961959839, 'policy_logps/rejected': -290.9574890136719, 'policy_logps/chosen': -352.1294250488281, 'referece_logps/rejected': -248.7230987548828, 'referece_logps/chosen': -337.58538818359375, 'logits/rejected': 0.008576810359954834, 'logits/chosen': -0.11072537302970886, 'epoch': 1.41}

 47%|████▋     | 1259/2685 [6:57:15<8:13:32, 20.77s/it]
[2024-03-29 05:04:14,953] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1260/2685 [6:57:35<8:05:02, 20.42s/it]

 47%|████▋     | 1261/2685 [6:57:54<7:57:11, 20.11s/it]

 47%|████▋     | 1262/2685 [6:58:13<7:46:25, 19.67s/it]

 47%|████▋     | 1263/2685 [6:58:33<7:50:52, 19.87s/it]

 47%|████▋     | 1264/2685 [6:58:54<7:58:52, 20.22s/it]

 47%|████▋     | 1265/2685 [6:59:13<7:48:40, 19.80s/it]
[2024-03-29 05:06:16,382] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1266/2685 [6:59:36<8:12:31, 20.83s/it]
[2024-03-29 05:06:37,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1267/2685 [6:59:57<8:11:29, 20.80s/it]


 47%|████▋     | 1269/2685 [7:00:24<6:39:36, 16.93s/it]
{'loss': 0.429, 'learning_rate': 1.137101971459756e-06, 'rewards/chosen': -0.7115368843078613, 'rewards/rejected': -2.242757558822632, 'rewards/accuracies': 0.875, 'rewards/margins': 1.531220555305481, 'policy_logps/rejected': -354.16888427734375, 'policy_logps/chosen': -343.458740234375, 'referece_logps/rejected': -331.7413330078125, 'referece_logps/chosen': -336.3433837890625, 'logits/rejected': -1.4238078594207764, 'logits/chosen': -1.3585821390151978, 'epoch': 1.42}


 47%|████▋     | 1271/2685 [7:01:04<7:22:00, 18.76s/it]
{'loss': 0.2446, 'learning_rate': 1.1347114622258611e-06, 'rewards/chosen': -1.0327379703521729, 'rewards/rejected': -5.011581897735596, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9788436889648438, 'policy_logps/rejected': -522.1174926757812, 'policy_logps/chosen': -466.00457763671875, 'referece_logps/rejected': -472.001708984375, 'referece_logps/chosen': -455.67724609375, 'logits/rejected': -0.36159804463386536, 'logits/chosen': -0.2679641842842102, 'epoch': 1.42}


 47%|████▋     | 1273/2685 [7:01:44<7:34:55, 19.33s/it]
{'loss': 0.3228, 'learning_rate': 1.1323201686921437e-06, 'rewards/chosen': -1.9637106657028198, 'rewards/rejected': -4.6681904792785645, 'rewards/accuracies': 0.75, 'rewards/margins': 2.704479694366455, 'policy_logps/rejected': -357.2886047363281, 'policy_logps/chosen': -302.33184814453125, 'referece_logps/rejected': -310.606689453125, 'referece_logps/chosen': -282.6947326660156, 'logits/rejected': -0.813289999961853, 'logits/chosen': -0.6888948678970337, 'epoch': 1.42}
[2024-03-29 05:08:46,056] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1274/2685 [7:02:06<7:52:11, 20.08s/it]
[2024-03-29 05:09:07,619] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 47%|████▋     | 1275/2685 [7:02:27<8:02:18, 20.52s/it]

 48%|████▊     | 1276/2685 [7:02:46<7:47:04, 19.89s/it]


 48%|████▊     | 1278/2685 [7:03:22<7:30:15, 19.20s/it]
[2024-03-29 05:10:02,378] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3375, 'learning_rate': 1.1263385949221293e-06, 'rewards/chosen': -1.3915343284606934, 'rewards/rejected': -4.6223273277282715, 'rewards/accuracies': 0.75, 'rewards/margins': 3.230792999267578, 'policy_logps/rejected': -397.4351501464844, 'policy_logps/chosen': -421.029296875, 'referece_logps/rejected': -351.2119140625, 'referece_logps/chosen': -407.1139221191406, 'logits/rejected': -0.13272500038146973, 'logits/chosen': -0.09172786772251129, 'epoch': 1.43}

 48%|████▊     | 1279/2685 [7:03:41<7:29:42, 19.19s/it]

 48%|████▊     | 1280/2685 [7:04:02<7:39:42, 19.63s/it]

 48%|████▊     | 1281/2685 [7:04:24<7:54:19, 20.27s/it]

 48%|████▊     | 1282/2685 [7:04:45<8:01:03, 20.57s/it]

 48%|████▊     | 1283/2685 [7:05:04<7:47:11, 19.99s/it]


 48%|████▊     | 1285/2685 [7:05:44<7:48:44, 20.09s/it]
{'loss': 0.2331, 'learning_rate': 1.1179567171508461e-06, 'rewards/chosen': -1.1924790143966675, 'rewards/rejected': -3.318540334701538, 'rewards/accuracies': 0.875, 'rewards/margins': 2.12606143951416, 'policy_logps/rejected': -457.37054443359375, 'policy_logps/chosen': -368.8186340332031, 'referece_logps/rejected': -424.18511962890625, 'referece_logps/chosen': -356.89385986328125, 'logits/rejected': -0.11908284574747086, 'logits/chosen': -0.16528236865997314, 'epoch': 1.44}

 48%|████▊     | 1286/2685 [7:06:03<7:37:31, 19.62s/it]

 48%|████▊     | 1287/2685 [7:06:23<7:43:10, 19.88s/it]
[2024-03-29 05:13:25,826] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 1288/2685 [7:06:46<8:00:05, 20.62s/it]

 48%|████▊     | 1289/2685 [7:07:06<7:54:21, 20.39s/it]

 48%|████▊     | 1290/2685 [7:07:26<7:56:16, 20.49s/it]

 48%|████▊     | 1291/2685 [7:07:42<7:25:17, 19.17s/it]

 48%|████▊     | 1292/2685 [7:08:02<7:29:58, 19.38s/it]

 48%|████▊     | 1293/2685 [7:08:22<7:33:02, 19.53s/it]


 48%|████▊     | 1295/2685 [7:09:07<8:05:59, 20.98s/it]
{'loss': 0.2335, 'learning_rate': 1.1059681609084428e-06, 'rewards/chosen': -1.6461255550384521, 'rewards/rejected': -3.8979575634002686, 'rewards/accuracies': 0.875, 'rewards/margins': 2.251831531524658, 'policy_logps/rejected': -408.2655334472656, 'policy_logps/chosen': -414.39422607421875, 'referece_logps/rejected': -369.28594970703125, 'referece_logps/chosen': -397.9330139160156, 'logits/rejected': -0.3528973162174225, 'logits/chosen': -0.28466880321502686, 'epoch': 1.45}

 48%|████▊     | 1296/2685 [7:09:28<8:12:02, 21.25s/it]

 48%|████▊     | 1297/2685 [7:09:48<8:02:26, 20.85s/it]
[2024-03-29 05:16:49,672] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 1298/2685 [7:10:10<8:03:46, 20.93s/it]

 48%|████▊     | 1299/2685 [7:10:28<7:47:24, 20.23s/it]
[2024-03-29 05:17:29,818] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 1300/2685 [7:10:50<7:56:02, 20.62s/it]
[2024-03-29 05:17:53,460] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 48%|████▊     | 1301/2685 [7:11:13<8:16:35, 21.53s/it]

 48%|████▊     | 1302/2685 [7:11:26<7:13:57, 18.83s/it]

 49%|████▊     | 1303/2685 [7:11:44<7:09:04, 18.63s/it]

 49%|████▊     | 1304/2685 [7:12:04<7:18:01, 19.03s/it]
[2024-03-29 05:19:02,889] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▊     | 1305/2685 [7:12:23<7:15:54, 18.95s/it]

 49%|████▊     | 1306/2685 [7:12:44<7:34:20, 19.77s/it]

 49%|████▊     | 1307/2685 [7:13:04<7:35:39, 19.84s/it]

 49%|████▊     | 1308/2685 [7:13:21<7:11:16, 18.79s/it]

 49%|████▉     | 1309/2685 [7:13:38<7:00:34, 18.34s/it]
[2024-03-29 05:20:38,912] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 1310/2685 [7:13:59<7:16:36, 19.05s/it]
[2024-03-29 05:20:59,619] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 1311/2685 [7:14:19<7:27:39, 19.55s/it]
[2024-03-29 05:21:20,486] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 1312/2685 [7:14:40<7:36:23, 19.94s/it]

 49%|████▉     | 1313/2685 [7:15:02<7:44:46, 20.33s/it]

 49%|████▉     | 1314/2685 [7:15:24<7:57:29, 20.90s/it]

 49%|████▉     | 1315/2685 [7:15:45<7:57:41, 20.92s/it]

 49%|████▉     | 1316/2685 [7:16:05<7:51:40, 20.67s/it]


 49%|████▉     | 1318/2685 [7:16:43<7:36:19, 20.03s/it]
{'loss': 0.2931, 'learning_rate': 1.0783388221858724e-06, 'rewards/chosen': -0.13694307208061218, 'rewards/rejected': -2.7677505016326904, 'rewards/accuracies': 0.875, 'rewards/margins': 2.630807399749756, 'policy_logps/rejected': -238.97726440429688, 'policy_logps/chosen': -200.0518035888672, 'referece_logps/rejected': -211.29977416992188, 'referece_logps/chosen': -198.68238830566406, 'logits/rejected': -1.4673293828964233, 'logits/chosen': -1.3927226066589355, 'epoch': 1.47}

 49%|████▉     | 1319/2685 [7:17:04<7:41:44, 20.28s/it]

 49%|████▉     | 1320/2685 [7:17:23<7:35:29, 20.02s/it]

 49%|████▉     | 1321/2685 [7:17:43<7:31:02, 19.84s/it]

 49%|████▉     | 1322/2685 [7:18:02<7:26:39, 19.66s/it]

 49%|████▉     | 1323/2685 [7:18:23<7:37:47, 20.17s/it]

 49%|████▉     | 1324/2685 [7:18:44<7:38:11, 20.20s/it]

 49%|████▉     | 1325/2685 [7:19:04<7:35:26, 20.09s/it]
[2024-03-29 05:26:04,974] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 49%|████▉     | 1326/2685 [7:19:25<7:43:01, 20.44s/it]

 49%|████▉     | 1327/2685 [7:19:48<8:01:20, 21.27s/it]

 49%|████▉     | 1328/2685 [7:20:07<7:45:12, 20.57s/it]


 50%|████▉     | 1330/2685 [7:20:45<7:31:48, 20.01s/it]
{'loss': 0.2634, 'learning_rate': 1.0638982233137977e-06, 'rewards/chosen': -1.2257871627807617, 'rewards/rejected': -4.592065811157227, 'rewards/accuracies': 1.0, 'rewards/margins': 3.366278886795044, 'policy_logps/rejected': -421.48553466796875, 'policy_logps/chosen': -304.26788330078125, 'referece_logps/rejected': -375.5648498535156, 'referece_logps/chosen': -292.0100402832031, 'logits/rejected': -0.5837921500205994, 'logits/chosen': -0.6808317303657532, 'epoch': 1.49}
[2024-03-29 05:27:48,806] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|████▉     | 1331/2685 [7:21:09<7:53:36, 20.99s/it]
[2024-03-29 05:28:11,791] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|████▉     | 1332/2685 [7:21:32<8:06:47, 21.59s/it]
[2024-03-29 05:28:32,978] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 50%|████▉     | 1334/2685 [7:22:13<7:57:54, 21.22s/it]
{'loss': 0.3116, 'learning_rate': 1.0590815646968934e-06, 'rewards/chosen': -0.9216559529304504, 'rewards/rejected': -2.655069351196289, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7334134578704834, 'policy_logps/rejected': -235.35888671875, 'policy_logps/chosen': -187.14654541015625, 'referece_logps/rejected': -208.8081817626953, 'referece_logps/chosen': -177.92999267578125, 'logits/rejected': -0.7988841533660889, 'logits/chosen': -0.8689814805984497, 'epoch': 1.49}
[2024-03-29 05:29:17,089] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 50%|████▉     | 1336/2685 [7:23:00<8:17:25, 22.12s/it]
[2024-03-29 05:29:39,752] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3183, 'learning_rate': 1.0566727124115084e-06, 'rewards/chosen': -1.1763492822647095, 'rewards/rejected': -4.551979064941406, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3756299018859863, 'policy_logps/rejected': -406.9691162109375, 'policy_logps/chosen': -350.4616394042969, 'referece_logps/rejected': -361.4493408203125, 'referece_logps/chosen': -338.69818115234375, 'logits/rejected': 0.03953084349632263, 'logits/chosen': 0.007117956876754761, 'epoch': 1.49}

 50%|████▉     | 1337/2685 [7:23:21<8:10:11, 21.82s/it]

 50%|████▉     | 1338/2685 [7:23:41<8:01:31, 21.45s/it]

 50%|████▉     | 1339/2685 [7:23:59<7:38:52, 20.46s/it]

 50%|████▉     | 1340/2685 [7:24:19<7:30:36, 20.10s/it]

 50%|████▉     | 1341/2685 [7:24:41<7:44:02, 20.72s/it]
[2024-03-29 05:31:42,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|████▉     | 1342/2685 [7:25:03<7:50:28, 21.02s/it]
[2024-03-29 05:32:04,094] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 1343/2685 [7:25:24<7:52:24, 21.12s/it]


 50%|█████     | 1345/2685 [7:26:02<7:27:33, 20.04s/it]

 50%|█████     | 1346/2685 [7:26:20<7:14:21, 19.46s/it]
{'loss': 0.2664, 'learning_rate': 1.0446237822512128e-06, 'rewards/chosen': -1.1908377408981323, 'rewards/rejected': -2.8175907135009766, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6267529726028442, 'policy_logps/rejected': -350.91937255859375, 'policy_logps/chosen': -285.64935302734375, 'referece_logps/rejected': -322.74346923828125, 'referece_logps/chosen': -273.7409973144531, 'logits/rejected': -0.05949658155441284, 'logits/chosen': -0.06514713168144226, 'epoch': 1.5}
[2024-03-29 05:33:22,616] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 1347/2685 [7:26:42<7:35:33, 20.43s/it]

 50%|█████     | 1348/2685 [7:27:02<7:29:52, 20.19s/it]
[2024-03-29 05:34:03,122] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 1349/2685 [7:27:23<7:34:08, 20.40s/it]

 50%|█████     | 1350/2685 [7:27:41<7:15:42, 19.58s/it]

 50%|█████     | 1351/2685 [7:28:00<7:15:06, 19.57s/it]

 50%|█████     | 1352/2685 [7:28:19<7:10:24, 19.37s/it]

 50%|█████     | 1353/2685 [7:28:38<7:04:20, 19.11s/it]

 50%|█████     | 1354/2685 [7:28:55<6:51:38, 18.56s/it]
[2024-03-29 05:35:57,925] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 50%|█████     | 1355/2685 [7:29:18<7:20:12, 19.86s/it]

 51%|█████     | 1356/2685 [7:29:38<7:19:03, 19.82s/it]
[2024-03-29 05:36:37,624] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 1357/2685 [7:29:57<7:19:39, 19.86s/it]
[2024-03-29 05:36:58,516] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 1358/2685 [7:30:18<7:26:08, 20.17s/it]

 51%|█████     | 1359/2685 [7:30:37<7:14:31, 19.66s/it]

 51%|█████     | 1360/2685 [7:30:57<7:15:45, 19.73s/it]


 51%|█████     | 1362/2685 [7:31:38<7:29:19, 20.38s/it]
[2024-03-29 05:38:18,546] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1945, 'learning_rate': 1.0253327143131877e-06, 'rewards/chosen': -1.3577693700790405, 'rewards/rejected': -4.707225322723389, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3494558334350586, 'policy_logps/rejected': -434.900634765625, 'policy_logps/chosen': -394.4616394042969, 'referece_logps/rejected': -387.828369140625, 'referece_logps/chosen': -380.8839111328125, 'logits/rejected': -0.6709433794021606, 'logits/chosen': -0.742455005645752, 'epoch': 1.52}
[2024-03-29 05:38:34,839] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 1363/2685 [7:31:55<7:01:59, 19.15s/it]
[2024-03-29 05:38:55,480] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 1364/2685 [7:32:15<7:11:30, 19.60s/it]

 51%|█████     | 1365/2685 [7:32:38<7:31:32, 20.52s/it]
[2024-03-29 05:39:35,958] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 51%|█████     | 1366/2685 [7:32:56<7:13:12, 19.71s/it]

 51%|█████     | 1367/2685 [7:33:15<7:07:55, 19.48s/it]


 51%|█████     | 1369/2685 [7:33:57<7:27:07, 20.39s/it]
[2024-03-29 05:40:37,023] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2025, 'learning_rate': 1.0168894800139308e-06, 'rewards/chosen': -2.4265408515930176, 'rewards/rejected': -5.326602458953857, 'rewards/accuracies': 1.0, 'rewards/margins': 2.900062084197998, 'policy_logps/rejected': -475.4628601074219, 'policy_logps/chosen': -423.79803466796875, 'referece_logps/rejected': -422.19683837890625, 'referece_logps/chosen': -399.5326843261719, 'logits/rejected': 0.4042501449584961, 'logits/chosen': 0.31753742694854736, 'epoch': 1.53}


 51%|█████     | 1371/2685 [7:34:32<6:49:28, 18.70s/it]

 51%|█████     | 1372/2685 [7:34:51<6:54:41, 18.95s/it]

 51%|█████     | 1373/2685 [7:35:08<6:41:53, 18.38s/it]

 51%|█████     | 1374/2685 [7:35:25<6:32:55, 17.98s/it]

 51%|█████     | 1375/2685 [7:35:44<6:36:50, 18.18s/it]

 51%|█████     | 1376/2685 [7:36:00<6:23:36, 17.58s/it]

 51%|█████▏    | 1377/2685 [7:36:19<6:32:44, 18.02s/it]
{'loss': 0.2908, 'learning_rate': 1.0072386295340571e-06, 'rewards/chosen': -2.706275224685669, 'rewards/rejected': -6.199456691741943, 'rewards/accuracies': 1.0, 'rewards/margins': 3.493180751800537, 'policy_logps/rejected': -422.820556640625, 'policy_logps/chosen': -409.64666748046875, 'referece_logps/rejected': -360.8260498046875, 'referece_logps/chosen': -382.58392333984375, 'logits/rejected': -0.7382389903068542, 'logits/chosen': -0.8300279974937439, 'epoch': 1.54}


 51%|█████▏    | 1379/2685 [7:36:56<6:32:14, 18.02s/it]

 51%|█████▏    | 1380/2685 [7:37:13<6:24:53, 17.70s/it]
{'loss': 0.2823, 'learning_rate': 1.0036193384730692e-06, 'rewards/chosen': -0.3995059132575989, 'rewards/rejected': -2.586573839187622, 'rewards/accuracies': 0.75, 'rewards/margins': 2.187067747116089, 'policy_logps/rejected': -197.32754516601562, 'policy_logps/chosen': -238.31884765625, 'referece_logps/rejected': -171.46180725097656, 'referece_logps/chosen': -234.32379150390625, 'logits/rejected': -0.7647280693054199, 'logits/chosen': -0.7753072381019592, 'epoch': 1.54}


 51%|█████▏    | 1382/2685 [7:37:52<6:40:58, 18.46s/it]
{'loss': 0.3267, 'learning_rate': 1.001206448499033e-06, 'rewards/chosen': -0.7242456674575806, 'rewards/rejected': -2.7886829376220703, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0644371509552, 'policy_logps/rejected': -282.0556335449219, 'policy_logps/chosen': -242.1746826171875, 'referece_logps/rejected': -254.16879272460938, 'referece_logps/chosen': -234.93223571777344, 'logits/rejected': -0.6214004755020142, 'logits/chosen': -0.8170887231826782, 'epoch': 1.54}


 52%|█████▏    | 1384/2685 [7:38:30<6:46:50, 18.76s/it]

 52%|█████▏    | 1385/2685 [7:38:47<6:36:17, 18.29s/it]

 52%|█████▏    | 1386/2685 [7:39:07<6:45:21, 18.72s/it]

 52%|█████▏    | 1387/2685 [7:39:27<6:55:30, 19.21s/it]

 52%|█████▏    | 1388/2685 [7:39:40<6:14:53, 17.34s/it]
{'loss': 0.2866, 'learning_rate': 9.939677926249436e-07, 'rewards/chosen': -0.6605125665664673, 'rewards/rejected': -3.247288942337036, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5867764949798584, 'policy_logps/rejected': -280.58770751953125, 'policy_logps/chosen': -252.54981994628906, 'referece_logps/rejected': -248.1147918701172, 'referece_logps/chosen': -245.94468688964844, 'logits/rejected': -1.0753461122512817, 'logits/chosen': -1.223290205001831, 'epoch': 1.55}


 52%|█████▏    | 1390/2685 [7:40:24<7:02:03, 19.55s/it]

 52%|█████▏    | 1391/2685 [7:40:42<6:52:04, 19.11s/it]

 52%|█████▏    | 1392/2685 [7:41:00<6:47:01, 18.89s/it]

 52%|█████▏    | 1393/2685 [7:41:22<7:04:13, 19.70s/it]

 52%|█████▏    | 1394/2685 [7:41:41<7:01:12, 19.58s/it]

 52%|█████▏    | 1395/2685 [7:41:58<6:43:23, 18.76s/it]

 52%|█████▏    | 1396/2685 [7:42:20<7:00:51, 19.59s/it]
{'loss': 0.2953, 'learning_rate': 9.843168086918515e-07, 'rewards/chosen': -0.9425449371337891, 'rewards/rejected': -2.9761831760406494, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0336384773254395, 'policy_logps/rejected': -308.76318359375, 'policy_logps/chosen': -227.85736083984375, 'referece_logps/rejected': -279.0013427734375, 'referece_logps/chosen': -218.43194580078125, 'logits/rejected': -0.9421023726463318, 'logits/chosen': -0.9672144651412964, 'epoch': 1.56}


 52%|█████▏    | 1398/2685 [7:42:56<6:48:50, 19.06s/it]

 52%|█████▏    | 1399/2685 [7:43:15<6:43:15, 18.81s/it]

 52%|█████▏    | 1400/2685 [7:43:35<6:49:39, 19.13s/it]
{'loss': 0.3257, 'learning_rate': 9.794918083893466e-07, 'rewards/chosen': -1.2146923542022705, 'rewards/rejected': -4.056429386138916, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8417375087738037, 'policy_logps/rejected': -333.2807312011719, 'policy_logps/chosen': -353.4840393066406, 'referece_logps/rejected': -292.7164611816406, 'referece_logps/chosen': -341.3371276855469, 'logits/rejected': -1.147472620010376, 'logits/chosen': -1.098562240600586, 'epoch': 1.56}


 52%|█████▏    | 1402/2685 [7:44:13<6:49:24, 19.15s/it]

 52%|█████▏    | 1403/2685 [7:44:33<6:59:23, 19.63s/it]

 52%|█████▏    | 1404/2685 [7:44:48<6:27:10, 18.13s/it]
{'loss': 0.4396, 'learning_rate': 9.746672856868122e-07, 'rewards/chosen': -1.87654447555542, 'rewards/rejected': -3.1248221397399902, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2482777833938599, 'policy_logps/rejected': -354.5011291503906, 'policy_logps/chosen': -305.20892333984375, 'referece_logps/rejected': -323.2528991699219, 'referece_logps/chosen': -286.4435119628906, 'logits/rejected': -0.33087480068206787, 'logits/chosen': -0.3580072522163391, 'epoch': 1.57}

 52%|█████▏    | 1405/2685 [7:45:07<6:32:31, 18.40s/it]


 52%|█████▏    | 1407/2685 [7:45:44<6:29:34, 18.29s/it]
{'loss': 0.2828, 'learning_rate': 9.710492746737642e-07, 'rewards/chosen': -2.526867389678955, 'rewards/rejected': -6.320989608764648, 'rewards/accuracies': 1.0, 'rewards/margins': 3.794121503829956, 'policy_logps/rejected': -419.1942138671875, 'policy_logps/chosen': -408.54443359375, 'referece_logps/rejected': -355.9843444824219, 'referece_logps/chosen': -383.2757568359375, 'logits/rejected': -0.41154107451438904, 'logits/chosen': -0.4269653558731079, 'epoch': 1.57}


 52%|█████▏    | 1409/2685 [7:46:22<6:35:23, 18.59s/it]

 53%|█████▎    | 1410/2685 [7:46:44<6:57:15, 19.64s/it]

 53%|█████▎    | 1411/2685 [7:47:04<6:59:57, 19.78s/it]

 53%|█████▎    | 1412/2685 [7:47:19<6:27:30, 18.26s/it]
{'loss': 0.2998, 'learning_rate': 9.650201224867545e-07, 'rewards/chosen': -2.116556167602539, 'rewards/rejected': -4.751203536987305, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6346471309661865, 'policy_logps/rejected': -430.1725769042969, 'policy_logps/chosen': -385.7488708496094, 'referece_logps/rejected': -382.6605224609375, 'referece_logps/chosen': -364.58331298828125, 'logits/rejected': -0.3633742928504944, 'logits/chosen': -0.31968599557876587, 'epoch': 1.58}


 53%|█████▎    | 1414/2685 [7:48:03<7:05:22, 20.08s/it]

 53%|█████▎    | 1415/2685 [7:48:20<6:49:01, 19.32s/it]

 53%|█████▎    | 1416/2685 [7:48:39<6:44:39, 19.13s/it]
{'loss': 0.3609, 'learning_rate': 9.601977066548165e-07, 'rewards/chosen': -1.383662223815918, 'rewards/rejected': -3.736856460571289, 'rewards/accuracies': 0.875, 'rewards/margins': 2.353194236755371, 'policy_logps/rejected': -459.4273681640625, 'policy_logps/chosen': -369.3374938964844, 'referece_logps/rejected': -422.05877685546875, 'referece_logps/chosen': -355.5008544921875, 'logits/rejected': -0.15600913763046265, 'logits/chosen': -0.13816705346107483, 'epoch': 1.58}


 53%|█████▎    | 1418/2685 [7:49:18<6:50:44, 19.45s/it]

 53%|█████▎    | 1419/2685 [7:49:38<6:53:12, 19.58s/it]

 53%|█████▎    | 1420/2685 [7:49:58<6:57:54, 19.82s/it]

 53%|█████▎    | 1421/2685 [7:50:20<7:09:03, 20.37s/it]
[2024-03-29 05:57:00,100] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1422/2685 [7:50:41<7:11:31, 20.50s/it]
[2024-03-29 05:57:20,910] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1423/2685 [7:51:00<7:02:16, 20.08s/it]

 53%|█████▎    | 1424/2685 [7:51:19<6:58:01, 19.89s/it]
[2024-03-29 05:57:59,456] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1425/2685 [7:51:39<6:56:57, 19.86s/it]

 53%|█████▎    | 1426/2685 [7:51:59<6:55:06, 19.78s/it]

 53%|█████▎    | 1427/2685 [7:52:14<6:28:55, 18.55s/it]

 53%|█████▎    | 1428/2685 [7:52:34<6:37:11, 18.96s/it]

 53%|█████▎    | 1429/2685 [7:52:52<6:30:15, 18.64s/it]

 53%|█████▎    | 1430/2685 [7:53:13<6:44:44, 19.35s/it]
[2024-03-29 05:59:53,333] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1431/2685 [7:53:32<6:41:53, 19.23s/it]
[2024-03-29 06:00:12,279] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 53%|█████▎    | 1432/2685 [7:53:51<6:39:04, 19.11s/it]

 53%|█████▎    | 1433/2685 [7:54:11<6:43:38, 19.34s/it]

 53%|█████▎    | 1434/2685 [7:54:31<6:47:49, 19.56s/it]
{'loss': 0.2655, 'learning_rate': 9.385099269948407e-07, 'rewards/chosen': -1.5334326028823853, 'rewards/rejected': -3.4247257709503174, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8912932872772217, 'policy_logps/rejected': -322.08392333984375, 'policy_logps/chosen': -328.3611755371094, 'referece_logps/rejected': -287.8366394042969, 'referece_logps/chosen': -313.02685546875, 'logits/rejected': -0.7543373107910156, 'logits/chosen': -0.6531051993370056, 'epoch': 1.6}

 53%|█████▎    | 1435/2685 [7:54:50<6:44:27, 19.41s/it]

 53%|█████▎    | 1436/2685 [7:55:08<6:34:43, 18.96s/it]

 54%|█████▎    | 1437/2685 [7:55:26<6:28:35, 18.68s/it]

 54%|█████▎    | 1438/2685 [7:55:46<6:35:46, 19.04s/it]


 54%|█████▎    | 1440/2685 [7:56:30<7:09:56, 20.72s/it]
{'loss': 0.3501, 'learning_rate': 9.312866061473396e-07, 'rewards/chosen': -2.145944356918335, 'rewards/rejected': -3.076097011566162, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9301525354385376, 'policy_logps/rejected': -253.73623657226562, 'policy_logps/chosen': -247.48455810546875, 'referece_logps/rejected': -222.9752960205078, 'referece_logps/chosen': -226.0251007080078, 'logits/rejected': -0.8079802393913269, 'logits/chosen': -0.7965372204780579, 'epoch': 1.61}
[2024-03-29 06:03:33,952] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 54%|█████▎    | 1442/2685 [7:57:17<7:37:26, 22.08s/it]

 54%|█████▎    | 1443/2685 [7:57:36<7:19:35, 21.24s/it]

 54%|█████▍    | 1444/2685 [7:57:57<7:12:56, 20.93s/it]

 54%|█████▍    | 1445/2685 [7:58:17<7:06:49, 20.65s/it]

 54%|█████▍    | 1446/2685 [7:58:38<7:13:38, 21.00s/it]

 54%|█████▍    | 1447/2685 [7:58:58<7:06:43, 20.68s/it]

 54%|█████▍    | 1448/2685 [7:59:17<6:51:33, 19.96s/it]

 54%|█████▍    | 1449/2685 [7:59:34<6:32:33, 19.06s/it]

 54%|█████▍    | 1450/2685 [7:59:53<6:34:49, 19.18s/it]

 54%|█████▍    | 1451/2685 [8:00:16<6:56:53, 20.27s/it]
[2024-03-29 06:06:56,039] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2453, 'learning_rate': 9.180534754261358e-07, 'rewards/chosen': -1.4155356884002686, 'rewards/rejected': -4.824044227600098, 'rewards/accuracies': 1.0, 'rewards/margins': 3.408508539199829, 'policy_logps/rejected': -465.32318115234375, 'policy_logps/chosen': -447.6349792480469, 'referece_logps/rejected': -417.082763671875, 'referece_logps/chosen': -433.4796142578125, 'logits/rejected': -0.30590900778770447, 'logits/chosen': -0.3303852677345276, 'epoch': 1.62}


 54%|█████▍    | 1453/2685 [8:00:53<6:36:57, 19.33s/it]

 54%|█████▍    | 1454/2685 [8:01:12<6:39:15, 19.46s/it]

 54%|█████▍    | 1455/2685 [8:01:32<6:37:24, 19.39s/it]

 54%|█████▍    | 1456/2685 [8:01:53<6:51:47, 20.10s/it]
[2024-03-29 06:08:33,618] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 54%|█████▍    | 1457/2685 [8:02:13<6:48:53, 19.98s/it]

 54%|█████▍    | 1458/2685 [8:02:32<6:41:15, 19.62s/it]

 54%|█████▍    | 1459/2685 [8:02:52<6:42:01, 19.67s/it]

 54%|█████▍    | 1460/2685 [8:03:11<6:42:13, 19.70s/it]

 54%|█████▍    | 1461/2685 [8:03:33<6:51:03, 20.15s/it]
{'loss': 0.247, 'learning_rate': 9.060358190107233e-07, 'rewards/chosen': -1.4647313356399536, 'rewards/rejected': -3.4517407417297363, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9870095252990723, 'policy_logps/rejected': -360.80792236328125, 'policy_logps/chosen': -302.11541748046875, 'referece_logps/rejected': -326.29052734375, 'referece_logps/chosen': -287.46807861328125, 'logits/rejected': -0.3169224262237549, 'logits/chosen': -0.20369526743888855, 'epoch': 1.63}


 54%|█████▍    | 1463/2685 [8:04:14<6:53:55, 20.32s/it]

 55%|█████▍    | 1464/2685 [8:04:33<6:48:15, 20.06s/it]
[2024-03-29 06:11:13,594] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 55%|█████▍    | 1465/2685 [8:04:52<6:36:15, 19.49s/it]

 55%|█████▍    | 1466/2685 [8:05:12<6:42:55, 19.83s/it]

 55%|█████▍    | 1467/2685 [8:05:34<6:52:58, 20.34s/it]
[2024-03-29 06:12:13,915] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 55%|█████▍    | 1468/2685 [8:05:56<7:04:37, 20.93s/it]
{'loss': 0.2641, 'learning_rate': 8.976314930368103e-07, 'rewards/chosen': -1.1612279415130615, 'rewards/rejected': -3.2698330879211426, 'rewards/accuracies': 0.875, 'rewards/margins': 2.108605146408081, 'policy_logps/rejected': -320.89361572265625, 'policy_logps/chosen': -367.06866455078125, 'referece_logps/rejected': -288.1952819824219, 'referece_logps/chosen': -355.4563903808594, 'logits/rejected': -0.31440240144729614, 'logits/chosen': -0.37121933698654175, 'epoch': 1.64}
[2024-03-29 06:12:55,291] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 55%|█████▍    | 1470/2685 [8:06:38<7:05:16, 21.00s/it]
{'loss': 0.2766, 'learning_rate': 8.952315718297402e-07, 'rewards/chosen': -1.0102641582489014, 'rewards/rejected': -2.9941136837005615, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9838494062423706, 'policy_logps/rejected': -266.4688720703125, 'policy_logps/chosen': -251.25962829589844, 'referece_logps/rejected': -236.5277099609375, 'referece_logps/chosen': -241.156982421875, 'logits/rejected': -1.5903619527816772, 'logits/chosen': -1.7710556983947754, 'epoch': 1.64}

 55%|█████▍    | 1471/2685 [8:06:54<6:39:24, 19.74s/it]


 55%|█████▍    | 1473/2685 [8:07:34<6:45:54, 20.09s/it]
[2024-03-29 06:14:14,349] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 55%|█████▍    | 1474/2685 [8:07:52<6:32:50, 19.46s/it]
{'loss': 0.2595, 'learning_rate': 8.904335732925349e-07, 'rewards/chosen': -0.22492942214012146, 'rewards/rejected': -1.9075838327407837, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6826543807983398, 'policy_logps/rejected': -215.29612731933594, 'policy_logps/chosen': -201.6988067626953, 'referece_logps/rejected': -196.2202606201172, 'referece_logps/chosen': -199.4495086669922, 'logits/rejected': -0.9270028471946716, 'logits/chosen': -0.8052317500114441, 'epoch': 1.65}

 55%|█████▍    | 1475/2685 [8:08:15<6:52:24, 20.45s/it]


 55%|█████▌    | 1477/2685 [8:08:50<6:16:41, 18.71s/it]
{'loss': 0.3729, 'learning_rate': 8.868367427757792e-07, 'rewards/chosen': -1.8458319902420044, 'rewards/rejected': -3.7939658164978027, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9481337070465088, 'policy_logps/rejected': -328.6915283203125, 'policy_logps/chosen': -320.1195373535156, 'referece_logps/rejected': -290.7518615722656, 'referece_logps/chosen': -301.6612243652344, 'logits/rejected': -0.692609429359436, 'logits/chosen': -0.6153836846351624, 'epoch': 1.65}

 55%|█████▌    | 1478/2685 [8:09:07<6:05:57, 18.19s/it]

 55%|█████▌    | 1479/2685 [8:09:27<6:17:31, 18.78s/it]

 55%|█████▌    | 1480/2685 [8:09:45<6:15:41, 18.71s/it]


 55%|█████▌    | 1482/2685 [8:10:22<6:10:18, 18.47s/it]

 55%|█████▌    | 1483/2685 [8:10:42<6:16:01, 18.77s/it]

 55%|█████▌    | 1484/2685 [8:11:03<6:29:22, 19.45s/it]

 55%|█████▌    | 1485/2685 [8:11:23<6:30:55, 19.55s/it]

 55%|█████▌    | 1486/2685 [8:11:44<6:40:09, 20.02s/it]
{'loss': 0.244, 'learning_rate': 8.760553339937524e-07, 'rewards/chosen': -1.1269959211349487, 'rewards/rejected': -3.311795473098755, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1847994327545166, 'policy_logps/rejected': -369.6582946777344, 'policy_logps/chosen': -291.9757080078125, 'referece_logps/rejected': -336.54034423828125, 'referece_logps/chosen': -280.70574951171875, 'logits/rejected': -1.4741300344467163, 'logits/chosen': -1.4051885604858398, 'epoch': 1.66}


 55%|█████▌    | 1488/2685 [8:12:22<6:34:24, 19.77s/it]

 55%|█████▌    | 1489/2685 [8:12:35<5:50:35, 17.59s/it]

 55%|█████▌    | 1490/2685 [8:12:50<5:37:44, 16.96s/it]
{'loss': 0.3269, 'learning_rate': 8.712682117143758e-07, 'rewards/chosen': -1.4864931106567383, 'rewards/rejected': -4.393845081329346, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9073519706726074, 'policy_logps/rejected': -429.3883056640625, 'policy_logps/chosen': -307.90313720703125, 'referece_logps/rejected': -385.4497985839844, 'referece_logps/chosen': -293.0381774902344, 'logits/rejected': -0.24133315682411194, 'logits/chosen': -0.3277624249458313, 'epoch': 1.66}

 56%|█████▌    | 1491/2685 [8:13:07<5:38:34, 17.01s/it]


 56%|█████▌    | 1493/2685 [8:13:45<6:00:21, 18.14s/it]

 56%|█████▌    | 1494/2685 [8:14:07<6:22:06, 19.25s/it]
{'loss': 0.2231, 'learning_rate': 8.664840873735565e-07, 'rewards/chosen': -1.4300979375839233, 'rewards/rejected': -5.277988910675049, 'rewards/accuracies': 1.0, 'rewards/margins': 3.847890853881836, 'policy_logps/rejected': -454.9023132324219, 'policy_logps/chosen': -491.89385986328125, 'referece_logps/rejected': -402.1224060058594, 'referece_logps/chosen': -477.5928955078125, 'logits/rejected': 0.2729291021823883, 'logits/chosen': 0.21716684103012085, 'epoch': 1.67}


 56%|█████▌    | 1496/2685 [8:14:42<6:10:13, 18.68s/it]

 56%|█████▌    | 1497/2685 [8:15:00<6:04:43, 18.42s/it]

 56%|█████▌    | 1498/2685 [8:15:19<6:10:27, 18.73s/it]
{'loss': 0.3251, 'learning_rate': 8.617030723851964e-07, 'rewards/chosen': -0.11568136513233185, 'rewards/rejected': -3.220374584197998, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1046934127807617, 'policy_logps/rejected': -267.6520080566406, 'policy_logps/chosen': -244.7336883544922, 'referece_logps/rejected': -235.44827270507812, 'referece_logps/chosen': -243.57687377929688, 'logits/rejected': -0.43022340536117554, 'logits/chosen': -0.5083274245262146, 'epoch': 1.67}


 56%|█████▌    | 1500/2685 [8:16:02<6:38:04, 20.16s/it]

 56%|█████▌    | 1501/2685 [8:16:37<8:04:53, 24.57s/it]

 56%|█████▌    | 1502/2685 [8:16:58<7:45:00, 23.58s/it]

 56%|█████▌    | 1503/2685 [8:17:17<7:18:15, 22.25s/it]

 56%|█████▌    | 1504/2685 [8:17:39<7:15:22, 22.12s/it]
{'loss': 0.2613, 'learning_rate': 8.545376234775896e-07, 'rewards/chosen': -1.1823523044586182, 'rewards/rejected': -3.5256614685058594, 'rewards/accuracies': 0.875, 'rewards/margins': 2.343309164047241, 'policy_logps/rejected': -273.2293395996094, 'policy_logps/chosen': -312.4520568847656, 'referece_logps/rejected': -237.97271728515625, 'referece_logps/chosen': -300.6285400390625, 'logits/rejected': -0.8446715474128723, 'logits/chosen': -0.738533079624176, 'epoch': 1.68}


 56%|█████▌    | 1506/2685 [8:18:10<6:08:07, 18.73s/it]

 56%|█████▌    | 1507/2685 [8:18:31<6:17:47, 19.24s/it]

 56%|█████▌    | 1508/2685 [8:18:51<6:21:37, 19.45s/it]

 56%|█████▌    | 1509/2685 [8:19:11<6:26:00, 19.69s/it]

 56%|█████▌    | 1510/2685 [8:19:33<6:39:53, 20.42s/it]

 56%|█████▋    | 1511/2685 [8:19:53<6:34:17, 20.15s/it]

 56%|█████▋    | 1512/2685 [8:20:13<6:35:16, 20.22s/it]
[2024-03-29 06:26:53,245] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 56%|█████▋    | 1513/2685 [8:20:33<6:32:17, 20.08s/it]

 56%|█████▋    | 1514/2685 [8:20:52<6:28:59, 19.93s/it]

 56%|█████▋    | 1515/2685 [8:21:11<6:18:17, 19.40s/it]
{'loss': 0.399, 'learning_rate': 8.414210337794165e-07, 'rewards/chosen': -1.135573148727417, 'rewards/rejected': -3.623720169067383, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4881467819213867, 'policy_logps/rejected': -452.8106384277344, 'policy_logps/chosen': -303.283203125, 'referece_logps/rejected': -416.5734558105469, 'referece_logps/chosen': -291.927490234375, 'logits/rejected': 0.29620251059532166, 'logits/chosen': 0.20719969272613525, 'epoch': 1.69}


 56%|█████▋    | 1517/2685 [8:21:49<6:18:45, 19.46s/it]
[2024-03-29 06:28:29,174] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 1518/2685 [8:22:09<6:20:28, 19.56s/it]

 57%|█████▋    | 1519/2685 [8:22:27<6:11:08, 19.10s/it]

 57%|█████▋    | 1520/2685 [8:22:47<6:19:24, 19.54s/it]

 57%|█████▋    | 1521/2685 [8:23:05<6:04:55, 18.81s/it]
{'loss': 0.3766, 'learning_rate': 8.342781543544796e-07, 'rewards/chosen': -1.087721347808838, 'rewards/rejected': -3.439335346221924, 'rewards/accuracies': 0.75, 'rewards/margins': 2.351614236831665, 'policy_logps/rejected': -288.861328125, 'policy_logps/chosen': -289.772705078125, 'referece_logps/rejected': -254.4679412841797, 'referece_logps/chosen': -278.8954772949219, 'logits/rejected': -0.5449478030204773, 'logits/chosen': -0.39941835403442383, 'epoch': 1.7}
[2024-03-29 06:30:06,120] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 57%|█████▋    | 1523/2685 [8:23:43<6:05:15, 18.86s/it]

 57%|█████▋    | 1524/2685 [8:24:03<6:09:47, 19.11s/it]

 57%|█████▋    | 1525/2685 [8:24:23<6:15:26, 19.42s/it]

 57%|█████▋    | 1526/2685 [8:24:40<5:59:13, 18.60s/it]

 57%|█████▋    | 1527/2685 [8:25:00<6:07:44, 19.05s/it]
{'loss': 0.3197, 'learning_rate': 8.2714395849638e-07, 'rewards/chosen': -2.0918092727661133, 'rewards/rejected': -3.5550949573516846, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4632854461669922, 'policy_logps/rejected': -335.4515380859375, 'policy_logps/chosen': -366.273681640625, 'referece_logps/rejected': -299.90057373046875, 'referece_logps/chosen': -345.3555908203125, 'logits/rejected': -0.6261014938354492, 'logits/chosen': -0.6846756935119629, 'epoch': 1.71}

 57%|█████▋    | 1528/2685 [8:25:22<6:27:04, 20.07s/it]


 57%|█████▋    | 1530/2685 [8:26:00<6:07:18, 19.08s/it]

 57%|█████▋    | 1531/2685 [8:26:19<6:10:09, 19.25s/it]

 57%|█████▋    | 1532/2685 [8:26:37<5:59:38, 18.71s/it]

 57%|█████▋    | 1533/2685 [8:26:56<6:00:16, 18.76s/it]

 57%|█████▋    | 1534/2685 [8:27:15<6:05:41, 19.06s/it]

 57%|█████▋    | 1535/2685 [8:27:35<6:10:18, 19.32s/it]

 57%|█████▋    | 1536/2685 [8:27:56<6:15:35, 19.61s/it]

 57%|█████▋    | 1537/2685 [8:28:13<6:03:17, 18.99s/it]
{'loss': 0.3076, 'learning_rate': 8.152739439828109e-07, 'rewards/chosen': -1.6337330341339111, 'rewards/rejected': -3.549499988555908, 'rewards/accuracies': 0.875, 'rewards/margins': 1.915766954421997, 'policy_logps/rejected': -515.5068969726562, 'policy_logps/chosen': -408.8742370605469, 'referece_logps/rejected': -480.0118713378906, 'referece_logps/chosen': -392.5368957519531, 'logits/rejected': 0.4977726936340332, 'logits/chosen': 0.301950067281723, 'epoch': 1.72}

 57%|█████▋    | 1538/2685 [8:28:33<6:05:56, 19.14s/it]


 57%|█████▋    | 1540/2685 [8:29:08<5:45:17, 18.09s/it]

 57%|█████▋    | 1541/2685 [8:29:29<6:02:46, 19.03s/it]

 57%|█████▋    | 1542/2685 [8:29:48<6:03:27, 19.08s/it]

 57%|█████▋    | 1543/2685 [8:30:10<6:17:14, 19.82s/it]

 58%|█████▊    | 1544/2685 [8:30:28<6:09:50, 19.45s/it]

 58%|█████▊    | 1545/2685 [8:30:49<6:19:56, 20.00s/it]

 58%|█████▊    | 1546/2685 [8:31:11<6:28:03, 20.44s/it]

 58%|█████▊    | 1547/2685 [8:31:31<6:26:17, 20.37s/it]
{'loss': 0.2797, 'learning_rate': 8.034308163657699e-07, 'rewards/chosen': -1.6326043605804443, 'rewards/rejected': -4.094802379608154, 'rewards/accuracies': 0.75, 'rewards/margins': 2.46219801902771, 'policy_logps/rejected': -370.701171875, 'policy_logps/chosen': -284.80963134765625, 'referece_logps/rejected': -329.7531433105469, 'referece_logps/chosen': -268.48358154296875, 'logits/rejected': -0.13762366771697998, 'logits/chosen': -0.0512315034866333, 'epoch': 1.73}


 58%|█████▊    | 1549/2685 [8:32:11<6:23:44, 20.27s/it]
{'loss': 0.3372, 'learning_rate': 8.010655690332723e-07, 'rewards/chosen': -1.7443718910217285, 'rewards/rejected': -4.998418807983398, 'rewards/accuracies': 0.75, 'rewards/margins': 3.254047155380249, 'policy_logps/rejected': -410.1807861328125, 'policy_logps/chosen': -350.400634765625, 'referece_logps/rejected': -360.19659423828125, 'referece_logps/chosen': -332.9569396972656, 'logits/rejected': -0.30584579706192017, 'logits/chosen': -0.3872550129890442, 'epoch': 1.73}


 58%|█████▊    | 1551/2685 [8:32:43<5:40:26, 18.01s/it]

 58%|█████▊    | 1552/2685 [8:32:59<5:28:35, 17.40s/it]
{'loss': 0.3509, 'learning_rate': 7.97519873980819e-07, 'rewards/chosen': -0.6984731554985046, 'rewards/rejected': -3.6185851097106934, 'rewards/accuracies': 0.75, 'rewards/margins': 2.920111656188965, 'policy_logps/rejected': -394.4776611328125, 'policy_logps/chosen': -276.1462707519531, 'referece_logps/rejected': -358.29180908203125, 'referece_logps/chosen': -269.1615295410156, 'logits/rejected': -0.7265518307685852, 'logits/chosen': -0.7521406412124634, 'epoch': 1.73}

 58%|█████▊    | 1553/2685 [8:33:21<5:50:23, 18.57s/it]


 58%|█████▊    | 1555/2685 [8:33:53<5:21:54, 17.09s/it]

 58%|█████▊    | 1556/2685 [8:34:09<5:14:23, 16.71s/it]

 58%|█████▊    | 1557/2685 [8:34:24<5:02:47, 16.11s/it]

 58%|█████▊    | 1558/2685 [8:34:43<5:17:00, 16.88s/it]
{'loss': 0.2931, 'learning_rate': 7.904364875472512e-07, 'rewards/chosen': -1.8351324796676636, 'rewards/rejected': -3.1042428016662598, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2691102027893066, 'policy_logps/rejected': -216.74822998046875, 'policy_logps/chosen': -199.09901428222656, 'referece_logps/rejected': -185.70582580566406, 'referece_logps/chosen': -180.7476806640625, 'logits/rejected': -1.095205307006836, 'logits/chosen': -1.1579973697662354, 'epoch': 1.74}


 58%|█████▊    | 1560/2685 [8:35:23<5:46:32, 18.48s/it]
{'loss': 0.3718, 'learning_rate': 7.880777806018521e-07, 'rewards/chosen': -1.371709942817688, 'rewards/rejected': -3.4854040145874023, 'rewards/accuracies': 0.875, 'rewards/margins': 2.113694190979004, 'policy_logps/rejected': -279.229736328125, 'policy_logps/chosen': -251.53533935546875, 'referece_logps/rejected': -244.3756866455078, 'referece_logps/chosen': -237.81822204589844, 'logits/rejected': -0.41311779618263245, 'logits/chosen': -0.4642294943332672, 'epoch': 1.74}

 58%|█████▊    | 1561/2685 [8:35:41<5:41:48, 18.25s/it]

 58%|█████▊    | 1562/2685 [8:36:01<5:53:06, 18.87s/it]

 58%|█████▊    | 1563/2685 [8:36:21<5:58:58, 19.20s/it]


 58%|█████▊    | 1565/2685 [8:37:00<5:58:26, 19.20s/it]

 58%|█████▊    | 1566/2685 [8:37:20<6:02:15, 19.42s/it]

 58%|█████▊    | 1567/2685 [8:37:36<5:45:54, 18.56s/it]

 58%|█████▊    | 1568/2685 [8:37:58<6:01:22, 19.41s/it]

 58%|█████▊    | 1569/2685 [8:38:15<5:47:33, 18.69s/it]
{'loss': 0.3133, 'learning_rate': 7.774790660436857e-07, 'rewards/chosen': -1.0762195587158203, 'rewards/rejected': -2.6390061378479004, 'rewards/accuracies': 0.75, 'rewards/margins': 1.562786340713501, 'policy_logps/rejected': -291.82257080078125, 'policy_logps/chosen': -224.05255126953125, 'referece_logps/rejected': -265.4325256347656, 'referece_logps/chosen': -213.29034423828125, 'logits/rejected': -0.5414468050003052, 'logits/chosen': -0.6593877673149109, 'epoch': 1.75}


 59%|█████▊    | 1571/2685 [8:38:57<6:06:40, 19.75s/it]

 59%|█████▊    | 1572/2685 [8:39:15<5:56:42, 19.23s/it]
{'loss': 0.1892, 'learning_rate': 7.73951929416265e-07, 'rewards/chosen': -1.3634952306747437, 'rewards/rejected': -5.226988315582275, 'rewards/accuracies': 1.0, 'rewards/margins': 3.863492488861084, 'policy_logps/rejected': -274.1937255859375, 'policy_logps/chosen': -250.40528869628906, 'referece_logps/rejected': -221.923828125, 'referece_logps/chosen': -236.7703399658203, 'logits/rejected': -0.7351148128509521, 'logits/chosen': -0.8452059626579285, 'epoch': 1.76}

 59%|█████▊    | 1573/2685 [8:39:31<5:42:22, 18.47s/it]

 59%|█████▊    | 1574/2685 [8:39:51<5:50:11, 18.91s/it]


 59%|█████▊    | 1576/2685 [8:40:35<6:19:16, 20.52s/it]
{'loss': 0.2501, 'learning_rate': 7.692536947956964e-07, 'rewards/chosen': -1.2820549011230469, 'rewards/rejected': -3.247540235519409, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9654850959777832, 'policy_logps/rejected': -314.58599853515625, 'policy_logps/chosen': -419.15667724609375, 'referece_logps/rejected': -282.1106262207031, 'referece_logps/chosen': -406.33612060546875, 'logits/rejected': -0.3547050356864929, 'logits/chosen': -0.3538963198661804, 'epoch': 1.76}

 59%|█████▊    | 1577/2685 [8:40:53<6:07:14, 19.89s/it]


 59%|█████▉    | 1579/2685 [8:41:30<5:51:45, 19.08s/it]
{'loss': 0.2432, 'learning_rate': 7.657335393279178e-07, 'rewards/chosen': -1.036049246788025, 'rewards/rejected': -3.5621750354766846, 'rewards/accuracies': 0.75, 'rewards/margins': 2.526125431060791, 'policy_logps/rejected': -336.1053466796875, 'policy_logps/chosen': -347.4664611816406, 'referece_logps/rejected': -300.4836120605469, 'referece_logps/chosen': -337.10601806640625, 'logits/rejected': -1.3421788215637207, 'logits/chosen': -1.3558660745620728, 'epoch': 1.76}


 59%|█████▉    | 1581/2685 [8:42:04<5:32:56, 18.09s/it]

 59%|█████▉    | 1582/2685 [8:42:20<5:22:31, 17.54s/it]

 59%|█████▉    | 1583/2685 [8:42:38<5:23:56, 17.64s/it]
{'loss': 0.2987, 'learning_rate': 7.610447803722977e-07, 'rewards/chosen': -1.627288818359375, 'rewards/rejected': -3.5200977325439453, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8928089141845703, 'policy_logps/rejected': -304.5774230957031, 'policy_logps/chosen': -341.2198791503906, 'referece_logps/rejected': -269.37646484375, 'referece_logps/chosen': -324.9469909667969, 'logits/rejected': -0.9392279386520386, 'logits/chosen': -1.0136122703552246, 'epoch': 1.77}

 59%|█████▉    | 1584/2685 [8:42:58<5:34:49, 18.25s/it]

 59%|█████▉    | 1585/2685 [8:43:20<5:53:59, 19.31s/it]

 59%|█████▉    | 1586/2685 [8:43:40<5:58:14, 19.56s/it]


 59%|█████▉    | 1588/2685 [8:44:16<5:42:47, 18.75s/it]
{'loss': 0.334, 'learning_rate': 7.551916700315572e-07, 'rewards/chosen': -1.540116786956787, 'rewards/rejected': -4.067166805267334, 'rewards/accuracies': 1.0, 'rewards/margins': 2.527050256729126, 'policy_logps/rejected': -409.16448974609375, 'policy_logps/chosen': -420.3086853027344, 'referece_logps/rejected': -368.4928283691406, 'referece_logps/chosen': -404.907470703125, 'logits/rejected': -0.6847832202911377, 'logits/chosen': -0.6871085166931152, 'epoch': 1.77}


 59%|█████▉    | 1590/2685 [8:44:53<5:35:18, 18.37s/it]
{'loss': 0.2952, 'learning_rate': 7.528529082331988e-07, 'rewards/chosen': -1.474334955215454, 'rewards/rejected': -3.832312822341919, 'rewards/accuracies': 0.875, 'rewards/margins': 2.357977867126465, 'policy_logps/rejected': -340.1418151855469, 'policy_logps/chosen': -299.48455810546875, 'referece_logps/rejected': -301.8186950683594, 'referece_logps/chosen': -284.7412109375, 'logits/rejected': -0.4051929712295532, 'logits/chosen': -0.5097674131393433, 'epoch': 1.78}

 59%|█████▉    | 1591/2685 [8:45:14<5:50:54, 19.25s/it]

 59%|█████▉    | 1592/2685 [8:45:34<5:54:01, 19.43s/it]


 59%|█████▉    | 1594/2685 [8:46:17<6:12:06, 20.46s/it]
{'loss': 0.3235, 'learning_rate': 7.481797149689754e-07, 'rewards/chosen': -0.3477628529071808, 'rewards/rejected': -3.813606023788452, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4658432006835938, 'policy_logps/rejected': -432.76092529296875, 'policy_logps/chosen': -448.4132995605469, 'referece_logps/rejected': -394.6248474121094, 'referece_logps/chosen': -444.9356689453125, 'logits/rejected': -0.6342904567718506, 'logits/chosen': -0.6541658639907837, 'epoch': 1.78}

 59%|█████▉    | 1595/2685 [8:46:34<5:51:34, 19.35s/it]


 59%|█████▉    | 1597/2685 [8:47:15<6:03:19, 20.04s/it]

 60%|█████▉    | 1598/2685 [8:47:31<5:41:38, 18.86s/it]

 60%|█████▉    | 1599/2685 [8:47:49<5:36:41, 18.60s/it]
{'loss': 0.2885, 'learning_rate': 7.423464830187386e-07, 'rewards/chosen': -2.054718017578125, 'rewards/rejected': -3.4369378089904785, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3822200298309326, 'policy_logps/rejected': -254.45632934570312, 'policy_logps/chosen': -261.7217712402344, 'referece_logps/rejected': -220.08694458007812, 'referece_logps/chosen': -241.17459106445312, 'logits/rejected': -0.6241264939308167, 'logits/chosen': -0.7078180313110352, 'epoch': 1.79}


 60%|█████▉    | 1601/2685 [8:48:27<5:38:08, 18.72s/it]
{'loss': 0.2678, 'learning_rate': 7.400158034921311e-07, 'rewards/chosen': -1.9032989740371704, 'rewards/rejected': -5.399305820465088, 'rewards/accuracies': 0.75, 'rewards/margins': 3.496006965637207, 'policy_logps/rejected': -439.6869201660156, 'policy_logps/chosen': -440.77581787109375, 'referece_logps/rejected': -385.6938781738281, 'referece_logps/chosen': -421.7427978515625, 'logits/rejected': 0.41323190927505493, 'logits/chosen': 0.3644813299179077, 'epoch': 1.79}


 60%|█████▉    | 1603/2685 [8:49:09<6:00:19, 19.98s/it]

 60%|█████▉    | 1604/2685 [8:49:31<6:07:58, 20.42s/it]
{'loss': 0.2038, 'learning_rate': 7.365226265277907e-07, 'rewards/chosen': -1.0202007293701172, 'rewards/rejected': -4.047145843505859, 'rewards/accuracies': 1.0, 'rewards/margins': 3.026945114135742, 'policy_logps/rejected': -340.0413818359375, 'policy_logps/chosen': -300.7347106933594, 'referece_logps/rejected': -299.56988525390625, 'referece_logps/chosen': -290.5326843261719, 'logits/rejected': -0.9877579212188721, 'logits/chosen': -1.0810413360595703, 'epoch': 1.79}

 60%|█████▉    | 1605/2685 [8:49:50<6:03:16, 20.18s/it]


 60%|█████▉    | 1607/2685 [8:50:25<5:31:23, 18.45s/it]
{'loss': 0.3239, 'learning_rate': 7.330329010258482e-07, 'rewards/chosen': -0.4828609824180603, 'rewards/rejected': -3.102595567703247, 'rewards/accuracies': 1.0, 'rewards/margins': 2.619734525680542, 'policy_logps/rejected': -447.79217529296875, 'policy_logps/chosen': -339.6886901855469, 'referece_logps/rejected': -416.7662353515625, 'referece_logps/chosen': -334.86004638671875, 'logits/rejected': -1.1422467231750488, 'logits/chosen': -1.1906018257141113, 'epoch': 1.8}

 60%|█████▉    | 1608/2685 [8:50:46<5:42:10, 19.06s/it]

 60%|█████▉    | 1609/2685 [8:51:06<5:46:18, 19.31s/it]


 60%|██████    | 1611/2685 [8:51:45<5:49:45, 19.54s/it]

 60%|██████    | 1612/2685 [8:52:05<5:52:38, 19.72s/it]
{'loss': 0.2489, 'learning_rate': 7.272244859136805e-07, 'rewards/chosen': -0.27448558807373047, 'rewards/rejected': -4.364663124084473, 'rewards/accuracies': 0.875, 'rewards/margins': 4.090177536010742, 'policy_logps/rejected': -383.24365234375, 'policy_logps/chosen': -379.5641784667969, 'referece_logps/rejected': -339.5970458984375, 'referece_logps/chosen': -376.8193359375, 'logits/rejected': -0.44768697023391724, 'logits/chosen': -0.48036396503448486, 'epoch': 1.8}


 60%|██████    | 1614/2685 [8:52:41<5:42:08, 19.17s/it]

 60%|██████    | 1615/2685 [8:53:01<5:44:17, 19.31s/it]
{'loss': 0.2934, 'learning_rate': 7.237441876783956e-07, 'rewards/chosen': -2.0508525371551514, 'rewards/rejected': -2.6868627071380615, 'rewards/accuracies': 0.625, 'rewards/margins': 0.636009931564331, 'policy_logps/rejected': -335.55523681640625, 'policy_logps/chosen': -313.49444580078125, 'referece_logps/rejected': -308.68658447265625, 'referece_logps/chosen': -292.98590087890625, 'logits/rejected': -0.11166386306285858, 'logits/chosen': -0.15273821353912354, 'epoch': 1.8}

 60%|██████    | 1616/2685 [8:53:20<5:43:49, 19.30s/it]

 60%|██████    | 1617/2685 [8:53:41<5:49:10, 19.62s/it]

 60%|██████    | 1618/2685 [8:54:00<5:46:32, 19.49s/it]


 60%|██████    | 1620/2685 [8:54:42<5:57:33, 20.14s/it]

 60%|██████    | 1621/2685 [8:55:02<5:56:58, 20.13s/it]

 60%|██████    | 1622/2685 [8:55:22<5:54:58, 20.04s/it]

 60%|██████    | 1623/2685 [8:55:44<6:05:33, 20.65s/it]

 60%|██████    | 1624/2685 [8:56:03<5:59:22, 20.32s/it]

 61%|██████    | 1625/2685 [8:56:23<5:56:24, 20.17s/it]

 61%|██████    | 1626/2685 [8:56:42<5:48:19, 19.73s/it]

 61%|██████    | 1627/2685 [8:57:04<5:58:42, 20.34s/it]

 61%|██████    | 1628/2685 [8:57:21<5:44:23, 19.55s/it]

 61%|██████    | 1629/2685 [8:57:41<5:46:21, 19.68s/it]

 61%|██████    | 1630/2685 [8:58:01<5:47:23, 19.76s/it]
{'loss': 0.2728, 'learning_rate': 7.063978894771594e-07, 'rewards/chosen': -0.3054809272289276, 'rewards/rejected': -2.952479839324951, 'rewards/accuracies': 1.0, 'rewards/margins': 2.646998882293701, 'policy_logps/rejected': -293.67205810546875, 'policy_logps/chosen': -288.9150695800781, 'referece_logps/rejected': -264.14727783203125, 'referece_logps/chosen': -285.8602294921875, 'logits/rejected': -0.968097984790802, 'logits/chosen': -0.8705631494522095, 'epoch': 1.82}


 61%|██████    | 1632/2685 [8:58:40<5:43:54, 19.60s/it]
{'loss': 0.3684, 'learning_rate': 7.040921907226447e-07, 'rewards/chosen': -1.5186666250228882, 'rewards/rejected': -5.602158069610596, 'rewards/accuracies': 0.875, 'rewards/margins': 4.083491325378418, 'policy_logps/rejected': -459.49334716796875, 'policy_logps/chosen': -393.8399353027344, 'referece_logps/rejected': -403.4717712402344, 'referece_logps/chosen': -378.6532897949219, 'logits/rejected': -0.2752833068370819, 'logits/chosen': -0.2721025347709656, 'epoch': 1.82}


 61%|██████    | 1634/2685 [8:59:20<5:45:40, 19.73s/it]

 61%|██████    | 1635/2685 [8:59:38<5:37:26, 19.28s/it]

 61%|██████    | 1636/2685 [8:59:58<5:41:03, 19.51s/it]

 61%|██████    | 1637/2685 [9:00:18<5:41:55, 19.58s/it]

 61%|██████    | 1638/2685 [9:00:40<5:55:42, 20.38s/it]

 61%|██████    | 1639/2685 [9:01:04<6:13:12, 21.41s/it]
{'loss': 0.3302, 'learning_rate': 6.960359001085732e-07, 'rewards/chosen': -2.022458791732788, 'rewards/rejected': -4.3011860847473145, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2787270545959473, 'policy_logps/rejected': -366.36407470703125, 'policy_logps/chosen': -346.6410827636719, 'referece_logps/rejected': -323.3522033691406, 'referece_logps/chosen': -326.4164733886719, 'logits/rejected': -0.33435070514678955, 'logits/chosen': -0.4094058871269226, 'epoch': 1.83}


 61%|██████    | 1641/2685 [9:01:38<5:28:25, 18.87s/it]

 61%|██████    | 1642/2685 [9:02:00<5:46:52, 19.95s/it]

 61%|██████    | 1643/2685 [9:02:20<5:45:59, 19.92s/it]

 61%|██████    | 1644/2685 [9:02:42<5:55:58, 20.52s/it]
{'loss': 0.2473, 'learning_rate': 6.902946460546186e-07, 'rewards/chosen': -2.7648627758026123, 'rewards/rejected': -4.86381196975708, 'rewards/accuracies': 0.75, 'rewards/margins': 2.098949670791626, 'policy_logps/rejected': -325.6602783203125, 'policy_logps/chosen': -404.4521179199219, 'referece_logps/rejected': -277.0221862792969, 'referece_logps/chosen': -376.803466796875, 'logits/rejected': -1.0401971340179443, 'logits/chosen': -0.956330418586731, 'epoch': 1.84}


 61%|██████▏   | 1646/2685 [9:03:20<5:43:58, 19.86s/it]

 61%|██████▏   | 1647/2685 [9:03:34<5:15:44, 18.25s/it]

 61%|██████▏   | 1648/2685 [9:03:56<5:32:01, 19.21s/it]

 61%|██████▏   | 1649/2685 [9:04:16<5:35:41, 19.44s/it]
{'loss': 0.308, 'learning_rate': 6.845646615147445e-07, 'rewards/chosen': -1.0336625576019287, 'rewards/rejected': -3.305199146270752, 'rewards/accuracies': 0.875, 'rewards/margins': 2.271536350250244, 'policy_logps/rejected': -514.2529296875, 'policy_logps/chosen': -503.8580627441406, 'referece_logps/rejected': -481.2008972167969, 'referece_logps/chosen': -493.5214538574219, 'logits/rejected': -0.4456057846546173, 'logits/chosen': -0.3974698781967163, 'epoch': 1.84}


 61%|██████▏   | 1651/2685 [9:04:58<5:48:31, 20.22s/it]
{'loss': 0.3782, 'learning_rate': 6.822758698811051e-07, 'rewards/chosen': -0.8227243423461914, 'rewards/rejected': -3.4934275150299072, 'rewards/accuracies': 1.0, 'rewards/margins': 2.670703411102295, 'policy_logps/rejected': -237.6756134033203, 'policy_logps/chosen': -207.358642578125, 'referece_logps/rejected': -202.74134826660156, 'referece_logps/chosen': -199.13140869140625, 'logits/rejected': -1.5620768070220947, 'logits/chosen': -1.489622950553894, 'epoch': 1.84}


 62%|██████▏   | 1653/2685 [9:05:32<5:24:28, 18.86s/it]

 62%|██████▏   | 1654/2685 [9:05:52<5:27:43, 19.07s/it]
{'loss': 0.297, 'learning_rate': 6.788461549908077e-07, 'rewards/chosen': -0.4109624922275543, 'rewards/rejected': -2.789386034011841, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3784234523773193, 'policy_logps/rejected': -304.0313415527344, 'policy_logps/chosen': -270.8802795410156, 'referece_logps/rejected': -276.137451171875, 'referece_logps/chosen': -266.7706604003906, 'logits/rejected': -1.2178981304168701, 'logits/chosen': -1.2405145168304443, 'epoch': 1.85}

 62%|██████▏   | 1655/2685 [9:06:14<5:41:20, 19.88s/it]

 62%|██████▏   | 1656/2685 [9:06:31<5:29:40, 19.22s/it]


 62%|██████▏   | 1658/2685 [9:07:04<5:01:26, 17.61s/it]
{'loss': 0.2071, 'learning_rate': 6.7427975378994e-07, 'rewards/chosen': -0.7983064651489258, 'rewards/rejected': -3.768651008605957, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9703445434570312, 'policy_logps/rejected': -334.6792907714844, 'policy_logps/chosen': -376.50140380859375, 'referece_logps/rejected': -296.9927673339844, 'referece_logps/chosen': -368.51837158203125, 'logits/rejected': -0.079024538397789, 'logits/chosen': -0.0058568790555000305, 'epoch': 1.85}

 62%|██████▏   | 1659/2685 [9:07:25<5:20:12, 18.73s/it]


 62%|██████▏   | 1661/2685 [9:08:06<5:37:14, 19.76s/it]
{'loss': 0.3449, 'learning_rate': 6.7085992503559e-07, 'rewards/chosen': -1.7902066707611084, 'rewards/rejected': -3.3714191913604736, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5812126398086548, 'policy_logps/rejected': -340.7428894042969, 'policy_logps/chosen': -264.75262451171875, 'referece_logps/rejected': -307.0286560058594, 'referece_logps/chosen': -246.85057067871094, 'logits/rejected': -0.44869834184646606, 'logits/chosen': -0.39445459842681885, 'epoch': 1.86}

 62%|██████▏   | 1662/2685 [9:08:26<5:35:06, 19.65s/it]


 62%|██████▏   | 1664/2685 [9:09:08<5:47:49, 20.44s/it]

 62%|██████▏   | 1665/2685 [9:09:26<5:35:10, 19.72s/it]

 62%|██████▏   | 1666/2685 [9:09:46<5:35:27, 19.75s/it]
{'loss': 0.34, 'learning_rate': 6.651698139223445e-07, 'rewards/chosen': -1.1721723079681396, 'rewards/rejected': -4.104490756988525, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9323182106018066, 'policy_logps/rejected': -248.1661834716797, 'policy_logps/chosen': -270.0444030761719, 'referece_logps/rejected': -207.12127685546875, 'referece_logps/chosen': -258.3226623535156, 'logits/rejected': -1.0431342124938965, 'logits/chosen': -1.004111409187317, 'epoch': 1.86}

 62%|██████▏   | 1667/2685 [9:10:06<5:34:25, 19.71s/it]

 62%|██████▏   | 1668/2685 [9:10:26<5:38:30, 19.97s/it]

 62%|██████▏   | 1669/2685 [9:10:44<5:26:16, 19.27s/it]

 62%|██████▏   | 1670/2685 [9:10:59<5:06:21, 18.11s/it]

 62%|██████▏   | 1671/2685 [9:11:19<5:14:59, 18.64s/it]


 62%|██████▏   | 1673/2685 [9:11:59<5:25:56, 19.32s/it]
{'loss': 0.337, 'learning_rate': 6.572241733638027e-07, 'rewards/chosen': -0.7929795980453491, 'rewards/rejected': -4.135512351989746, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3425326347351074, 'policy_logps/rejected': -392.92620849609375, 'policy_logps/chosen': -383.1035461425781, 'referece_logps/rejected': -351.5710754394531, 'referece_logps/chosen': -375.1737365722656, 'logits/rejected': -1.3961482048034668, 'logits/chosen': -1.5700629949569702, 'epoch': 1.87}

 62%|██████▏   | 1674/2685 [9:12:21<5:37:43, 20.04s/it]

 62%|██████▏   | 1675/2685 [9:12:40<5:35:48, 19.95s/it]

 62%|██████▏   | 1676/2685 [9:13:04<5:54:44, 21.10s/it]

 62%|██████▏   | 1677/2685 [9:13:25<5:53:54, 21.07s/it]

 62%|██████▏   | 1678/2685 [9:13:45<5:47:24, 20.70s/it]

 63%|██████▎   | 1679/2685 [9:14:04<5:38:15, 20.17s/it]

 63%|██████▎   | 1680/2685 [9:14:22<5:29:12, 19.65s/it]

 63%|██████▎   | 1681/2685 [9:14:45<5:41:40, 20.42s/it]

 63%|██████▎   | 1682/2685 [9:15:04<5:38:20, 20.24s/it]

 63%|██████▎   | 1683/2685 [9:15:25<5:38:57, 20.30s/it]


 63%|██████▎   | 1685/2685 [9:16:05<5:37:31, 20.25s/it]
{'loss': 0.2468, 'learning_rate': 6.436602644306222e-07, 'rewards/chosen': -0.770256757736206, 'rewards/rejected': -3.6985301971435547, 'rewards/accuracies': 0.875, 'rewards/margins': 2.928273916244507, 'policy_logps/rejected': -335.5047302246094, 'policy_logps/chosen': -275.93914794921875, 'referece_logps/rejected': -298.5194091796875, 'referece_logps/chosen': -268.2366027832031, 'logits/rejected': -0.1594713032245636, 'logits/chosen': -0.18538740277290344, 'epoch': 1.88}


 63%|██████▎   | 1687/2685 [9:16:35<4:52:17, 17.57s/it]
{'loss': 0.2683, 'learning_rate': 6.414067974936098e-07, 'rewards/chosen': -2.01574969291687, 'rewards/rejected': -4.210918426513672, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1951684951782227, 'policy_logps/rejected': -540.4188842773438, 'policy_logps/chosen': -485.6802062988281, 'referece_logps/rejected': -498.3096923828125, 'referece_logps/chosen': -465.522705078125, 'logits/rejected': -1.5864876508712769, 'logits/chosen': -1.4879480600357056, 'epoch': 1.88}

 63%|██████▎   | 1688/2685 [9:16:54<4:57:48, 17.92s/it]


 63%|██████▎   | 1690/2685 [9:17:37<5:29:44, 19.88s/it]
{'loss': 0.3754, 'learning_rate': 6.380305157261272e-07, 'rewards/chosen': -2.2294390201568604, 'rewards/rejected': -4.324767589569092, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0953285694122314, 'policy_logps/rejected': -383.35205078125, 'policy_logps/chosen': -394.3182373046875, 'referece_logps/rejected': -340.1043701171875, 'referece_logps/chosen': -372.0238342285156, 'logits/rejected': 0.3081549108028412, 'logits/chosen': 0.3968752324581146, 'epoch': 1.89}

 63%|██████▎   | 1691/2685 [9:17:58<5:32:53, 20.09s/it]

 63%|██████▎   | 1692/2685 [9:18:15<5:16:30, 19.12s/it]

 63%|██████▎   | 1693/2685 [9:18:36<5:28:46, 19.89s/it]

 63%|██████▎   | 1694/2685 [9:18:50<4:56:39, 17.96s/it]

 63%|██████▎   | 1695/2685 [9:19:10<5:06:16, 18.56s/it]

 63%|██████▎   | 1696/2685 [9:19:24<4:47:30, 17.44s/it]

 63%|██████▎   | 1697/2685 [9:19:44<4:59:16, 18.17s/it]

 63%|██████▎   | 1698/2685 [9:20:01<4:51:32, 17.72s/it]

 63%|██████▎   | 1699/2685 [9:20:21<5:00:14, 18.27s/it]

 63%|██████▎   | 1700/2685 [9:20:40<5:04:43, 18.56s/it]

 63%|██████▎   | 1701/2685 [9:20:56<4:53:57, 17.92s/it]

 63%|██████▎   | 1702/2685 [9:21:18<5:12:54, 19.10s/it]

 63%|██████▎   | 1703/2685 [9:21:40<5:27:15, 20.00s/it]

 63%|██████▎   | 1704/2685 [9:22:00<5:27:54, 20.06s/it]

 64%|██████▎   | 1705/2685 [9:22:21<5:30:14, 20.22s/it]

 64%|██████▎   | 1706/2685 [9:22:42<5:34:57, 20.53s/it]

 64%|██████▎   | 1707/2685 [9:23:03<5:36:16, 20.63s/it]

 64%|██████▎   | 1708/2685 [9:23:24<5:37:50, 20.75s/it]

 64%|██████▎   | 1709/2685 [9:23:45<5:38:43, 20.82s/it]

 64%|██████▎   | 1710/2685 [9:24:06<5:39:28, 20.89s/it]

 64%|██████▎   | 1711/2685 [9:24:26<5:33:04, 20.52s/it]

 64%|██████▍   | 1712/2685 [9:24:45<5:24:48, 20.03s/it]

 64%|██████▍   | 1713/2685 [9:25:04<5:22:56, 19.93s/it]

 64%|██████▍   | 1714/2685 [9:25:24<5:20:51, 19.83s/it]

 64%|██████▍   | 1715/2685 [9:25:44<5:23:32, 20.01s/it]

 64%|██████▍   | 1716/2685 [9:26:05<5:27:42, 20.29s/it]

 64%|██████▍   | 1717/2685 [9:26:27<5:31:53, 20.57s/it]

 64%|██████▍   | 1718/2685 [9:26:46<5:28:00, 20.35s/it]

 64%|██████▍   | 1719/2685 [9:27:09<5:39:28, 21.08s/it]


 64%|██████▍   | 1721/2685 [9:27:49<5:34:08, 20.80s/it]

 64%|██████▍   | 1722/2685 [9:28:10<5:36:09, 20.94s/it]

 64%|██████▍   | 1723/2685 [9:28:30<5:29:41, 20.56s/it]

 64%|██████▍   | 1724/2685 [9:28:50<5:27:11, 20.43s/it]

 64%|██████▍   | 1725/2685 [9:29:11<5:29:54, 20.62s/it]

 64%|██████▍   | 1726/2685 [9:29:31<5:25:47, 20.38s/it]

 64%|██████▍   | 1727/2685 [9:29:50<5:19:33, 20.01s/it]

 64%|██████▍   | 1728/2685 [9:30:10<5:18:17, 19.96s/it]

 64%|██████▍   | 1729/2685 [9:30:30<5:16:44, 19.88s/it]

 64%|██████▍   | 1730/2685 [9:30:49<5:14:38, 19.77s/it]

 64%|██████▍   | 1731/2685 [9:31:11<5:22:28, 20.28s/it]

 65%|██████▍   | 1732/2685 [9:31:25<4:53:43, 18.49s/it]

 65%|██████▍   | 1733/2685 [9:31:47<5:11:45, 19.65s/it]

 65%|██████▍   | 1734/2685 [9:32:07<5:12:28, 19.71s/it]

 65%|██████▍   | 1735/2685 [9:32:24<4:58:15, 18.84s/it]

 65%|██████▍   | 1736/2685 [9:32:43<4:59:16, 18.92s/it]

 65%|██████▍   | 1737/2685 [9:33:05<5:14:39, 19.91s/it]

 65%|██████▍   | 1738/2685 [9:33:26<5:16:35, 20.06s/it]

 65%|██████▍   | 1739/2685 [9:33:45<5:13:17, 19.87s/it]

 65%|██████▍   | 1740/2685 [9:34:02<4:58:36, 18.96s/it]

 65%|██████▍   | 1741/2685 [9:34:21<5:01:26, 19.16s/it]

 65%|██████▍   | 1742/2685 [9:34:42<5:05:37, 19.45s/it]

 65%|██████▍   | 1743/2685 [9:35:03<5:14:22, 20.02s/it]

 65%|██████▍   | 1744/2685 [9:35:24<5:19:51, 20.40s/it]

 65%|██████▍   | 1745/2685 [9:35:45<5:22:47, 20.60s/it]

 65%|██████▌   | 1746/2685 [9:36:04<5:11:13, 19.89s/it]

 65%|██████▌   | 1747/2685 [9:36:23<5:09:00, 19.77s/it]

 65%|██████▌   | 1748/2685 [9:36:42<5:06:51, 19.65s/it]

 65%|██████▌   | 1749/2685 [9:36:56<4:36:15, 17.71s/it]

 65%|██████▌   | 1750/2685 [9:37:15<4:44:09, 18.23s/it]

 65%|██████▌   | 1751/2685 [9:37:35<4:50:45, 18.68s/it]

 65%|██████▌   | 1752/2685 [9:37:53<4:49:51, 18.64s/it]

 65%|██████▌   | 1753/2685 [9:38:13<4:55:26, 19.02s/it]

 65%|██████▌   | 1754/2685 [9:38:33<4:58:10, 19.22s/it]

 65%|██████▌   | 1755/2685 [9:38:52<4:55:32, 19.07s/it]

 65%|██████▌   | 1756/2685 [9:39:13<5:05:22, 19.72s/it]

 65%|██████▌   | 1757/2685 [9:39:32<5:03:24, 19.62s/it]

 65%|██████▌   | 1758/2685 [9:39:52<5:01:57, 19.54s/it]

 66%|██████▌   | 1759/2685 [9:40:12<5:04:25, 19.73s/it]

 66%|██████▌   | 1760/2685 [9:40:30<4:54:54, 19.13s/it]

 66%|██████▌   | 1761/2685 [9:40:49<4:57:00, 19.29s/it]

 66%|██████▌   | 1762/2685 [9:41:12<5:11:35, 20.26s/it]

 66%|██████▌   | 1763/2685 [9:41:30<5:01:49, 19.64s/it]
{'loss': 0.2492, 'learning_rate': 5.574408221437498e-07, 'rewards/chosen': -0.8189759254455566, 'rewards/rejected': -5.271115303039551, 'rewards/accuracies': 1.0, 'rewards/margins': 4.452139854431152, 'policy_logps/rejected': -275.77764892578125, 'policy_logps/chosen': -385.3284912109375, 'referece_logps/rejected': -223.0665283203125, 'referece_logps/chosen': -377.13873291015625, 'logits/rejected': 0.4400225877761841, 'logits/chosen': 0.41396456956863403, 'epoch': 1.97}

 66%|██████▌   | 1764/2685 [9:41:47<4:49:57, 18.89s/it]


 66%|██████▌   | 1766/2685 [9:42:23<4:41:09, 18.36s/it]

 66%|██████▌   | 1767/2685 [9:42:42<4:42:22, 18.46s/it]

 66%|██████▌   | 1768/2685 [9:43:03<4:54:47, 19.29s/it]

 66%|██████▌   | 1769/2685 [9:43:24<5:00:21, 19.67s/it]

 66%|██████▌   | 1770/2685 [9:43:40<4:46:11, 18.77s/it]

 66%|██████▌   | 1771/2685 [9:44:03<5:01:36, 19.80s/it]

 66%|██████▌   | 1772/2685 [9:44:21<4:54:04, 19.33s/it]

 66%|██████▌   | 1773/2685 [9:44:41<4:56:01, 19.48s/it]

 66%|██████▌   | 1774/2685 [9:45:01<5:01:37, 19.87s/it]
{'loss': 0.2773, 'learning_rate': 5.455795716426718e-07, 'rewards/chosen': -1.4850819110870361, 'rewards/rejected': -3.7189223766326904, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2338404655456543, 'policy_logps/rejected': -257.4566955566406, 'policy_logps/chosen': -215.24383544921875, 'referece_logps/rejected': -220.2674560546875, 'referece_logps/chosen': -200.39300537109375, 'logits/rejected': -0.9368417859077454, 'logits/chosen': -0.9575403332710266, 'epoch': 1.98}


 66%|██████▌   | 1776/2685 [9:45:40<4:54:25, 19.43s/it]

 66%|██████▌   | 1777/2685 [9:45:56<4:41:52, 18.63s/it]

 66%|██████▌   | 1778/2685 [9:46:18<4:54:35, 19.49s/it]

 66%|██████▋   | 1779/2685 [9:46:38<4:55:16, 19.55s/it]

 66%|██████▋   | 1780/2685 [9:47:00<5:06:30, 20.32s/it]

 66%|██████▋   | 1781/2685 [9:47:17<4:50:14, 19.26s/it]

 66%|██████▋   | 1782/2685 [9:47:34<4:43:35, 18.84s/it]

 66%|██████▋   | 1783/2685 [9:47:51<4:31:58, 18.09s/it]

 66%|██████▋   | 1784/2685 [9:48:11<4:39:37, 18.62s/it]

 66%|██████▋   | 1785/2685 [9:48:29<4:36:17, 18.42s/it]

 67%|██████▋   | 1786/2685 [9:48:50<4:50:08, 19.36s/it]

 67%|██████▋   | 1787/2685 [9:49:10<4:51:09, 19.45s/it]

 67%|██████▋   | 1788/2685 [9:49:24<4:25:46, 17.78s/it]

 67%|██████▋   | 1789/2685 [9:49:43<4:33:12, 18.30s/it]

 67%|██████▋   | 1790/2685 [9:50:02<4:36:15, 18.52s/it]

 67%|██████▋   | 1791/2685 [9:50:18<4:26:03, 17.86s/it]
{'loss': 0.2817, 'learning_rate': 5.27406705781813e-07, 'rewards/chosen': -1.3741326332092285, 'rewards/rejected': -3.0563178062438965, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6821850538253784, 'policy_logps/rejected': -296.21441650390625, 'policy_logps/chosen': -348.62750244140625, 'referece_logps/rejected': -265.6512451171875, 'referece_logps/chosen': -334.88616943359375, 'logits/rejected': -0.04225068539381027, 'logits/chosen': -0.1995009183883667, 'epoch': 2.0}


 67%|██████▋   | 1793/2685 [9:50:57<4:39:23, 18.79s/it]

 67%|██████▋   | 1794/2685 [9:51:15<4:34:53, 18.51s/it]

 67%|██████▋   | 1795/2685 [9:51:36<4:46:07, 19.29s/it]

 67%|██████▋   | 1796/2685 [9:51:53<4:34:27, 18.52s/it]

 67%|██████▋   | 1797/2685 [9:52:10<4:29:41, 18.22s/it]

 67%|██████▋   | 1798/2685 [9:52:30<4:36:49, 18.73s/it]

 67%|██████▋   | 1799/2685 [9:52:47<4:29:12, 18.23s/it]

 67%|██████▋   | 1800/2685 [9:53:07<4:35:41, 18.69s/it]

 67%|██████▋   | 1801/2685 [9:53:27<4:41:35, 19.11s/it]

 67%|██████▋   | 1802/2685 [9:53:47<4:45:49, 19.42s/it]

 67%|██████▋   | 1803/2685 [9:54:06<4:43:44, 19.30s/it]

 67%|██████▋   | 1804/2685 [9:54:26<4:45:22, 19.43s/it]

 67%|██████▋   | 1805/2685 [9:54:47<4:52:29, 19.94s/it]

 67%|██████▋   | 1806/2685 [9:55:07<4:52:41, 19.98s/it]

 67%|██████▋   | 1807/2685 [9:55:27<4:52:45, 20.01s/it]

 67%|██████▋   | 1808/2685 [9:55:49<4:58:41, 20.44s/it]

 67%|██████▋   | 1809/2685 [9:56:09<4:58:21, 20.44s/it]

 67%|██████▋   | 1810/2685 [9:56:29<4:54:59, 20.23s/it]

 67%|██████▋   | 1811/2685 [9:56:50<4:58:52, 20.52s/it]
{'loss': 0.2745, 'learning_rate': 5.062819366760603e-07, 'rewards/chosen': -1.6040068864822388, 'rewards/rejected': -4.404106140136719, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8000993728637695, 'policy_logps/rejected': -468.3643798828125, 'policy_logps/chosen': -500.3428955078125, 'referece_logps/rejected': -424.3233337402344, 'referece_logps/chosen': -484.30279541015625, 'logits/rejected': -0.11000123620033264, 'logits/chosen': -0.04036295786499977, 'epoch': 2.02}


 68%|██████▊   | 1813/2685 [9:57:31<4:58:35, 20.54s/it]

 68%|██████▊   | 1814/2685 [9:57:48<4:43:24, 19.52s/it]
{'loss': 0.2387, 'learning_rate': 5.031377139760431e-07, 'rewards/chosen': -1.064957618713379, 'rewards/rejected': -3.6307084560394287, 'rewards/accuracies': 0.875, 'rewards/margins': 2.565751075744629, 'policy_logps/rejected': -353.8275146484375, 'policy_logps/chosen': -387.1072998046875, 'referece_logps/rejected': -317.5203857421875, 'referece_logps/chosen': -376.45770263671875, 'logits/rejected': -0.312656044960022, 'logits/chosen': -0.4078409671783447, 'epoch': 2.03}


 68%|██████▊   | 1816/2685 [9:58:27<4:43:27, 19.57s/it]
{'loss': 0.1914, 'learning_rate': 5.010451789281478e-07, 'rewards/chosen': -0.9367128610610962, 'rewards/rejected': -4.665378093719482, 'rewards/accuracies': 1.0, 'rewards/margins': 3.728665351867676, 'policy_logps/rejected': -246.4871368408203, 'policy_logps/chosen': -178.41477966308594, 'referece_logps/rejected': -199.83334350585938, 'referece_logps/chosen': -169.0476531982422, 'logits/rejected': -1.382201075553894, 'logits/chosen': -1.513659119606018, 'epoch': 2.03}


 68%|██████▊   | 1818/2685 [9:59:05<4:34:58, 19.03s/it]

 68%|██████▊   | 1819/2685 [9:59:22<4:23:47, 18.28s/it]

 68%|██████▊   | 1820/2685 [9:59:43<4:35:41, 19.12s/it]

 68%|██████▊   | 1821/2685 [10:00:03<4:39:04, 19.38s/it]

 68%|██████▊   | 1822/2685 [10:00:22<4:40:18, 19.49s/it]

 68%|██████▊   | 1823/2685 [10:00:43<4:43:50, 19.76s/it]

 68%|██████▊   | 1824/2685 [10:01:01<4:37:57, 19.37s/it]
{'loss': 0.2274, 'learning_rate': 4.927042098198929e-07, 'rewards/chosen': -1.1472629308700562, 'rewards/rejected': -3.603935718536377, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4566726684570312, 'policy_logps/rejected': -350.66278076171875, 'policy_logps/chosen': -360.1292724609375, 'referece_logps/rejected': -314.6234130859375, 'referece_logps/chosen': -348.6567077636719, 'logits/rejected': -0.7430835962295532, 'logits/chosen': -0.7383092641830444, 'epoch': 2.04}


 68%|██████▊   | 1826/2685 [10:01:40<4:34:35, 19.18s/it]

 68%|██████▊   | 1827/2685 [10:01:55<4:16:00, 17.90s/it]

 68%|██████▊   | 1828/2685 [10:02:15<4:24:20, 18.51s/it]

 68%|██████▊   | 1829/2685 [10:02:30<4:09:28, 17.49s/it]

 68%|██████▊   | 1830/2685 [10:02:50<4:19:36, 18.22s/it]

 68%|██████▊   | 1831/2685 [10:03:09<4:25:21, 18.64s/it]

 68%|██████▊   | 1832/2685 [10:03:29<4:31:38, 19.11s/it]

 68%|██████▊   | 1833/2685 [10:03:49<4:33:49, 19.28s/it]

 68%|██████▊   | 1834/2685 [10:04:08<4:30:34, 19.08s/it]
{'loss': 0.2508, 'learning_rate': 4.823445426183878e-07, 'rewards/chosen': -1.6223859786987305, 'rewards/rejected': -3.5747432708740234, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9523570537567139, 'policy_logps/rejected': -329.0183410644531, 'policy_logps/chosen': -352.3822021484375, 'referece_logps/rejected': -293.2708740234375, 'referece_logps/chosen': -336.1583251953125, 'logits/rejected': -0.15967652201652527, 'logits/chosen': -0.11697570979595184, 'epoch': 2.05}


 68%|██████▊   | 1836/2685 [10:04:51<4:47:55, 20.35s/it]

 68%|██████▊   | 1837/2685 [10:05:11<4:46:20, 20.26s/it]
{'loss': 0.282, 'learning_rate': 4.792512663727267e-07, 'rewards/chosen': -1.585096001625061, 'rewards/rejected': -5.088735103607178, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5036392211914062, 'policy_logps/rejected': -348.3022766113281, 'policy_logps/chosen': -313.8828125, 'referece_logps/rejected': -297.41497802734375, 'referece_logps/chosen': -298.0318603515625, 'logits/rejected': -0.5102815628051758, 'logits/chosen': -0.4457613527774811, 'epoch': 2.05}


 68%|██████▊   | 1839/2685 [10:05:49<4:36:41, 19.62s/it]
{'loss': 0.2337, 'learning_rate': 4.771928695047652e-07, 'rewards/chosen': -1.2937207221984863, 'rewards/rejected': -4.378855228424072, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0851340293884277, 'policy_logps/rejected': -382.3135986328125, 'policy_logps/chosen': -302.5929870605469, 'referece_logps/rejected': -338.52508544921875, 'referece_logps/chosen': -289.65576171875, 'logits/rejected': -0.75014728307724, 'logits/chosen': -0.7454550862312317, 'epoch': 2.05}


 69%|██████▊   | 1841/2685 [10:06:28<4:34:55, 19.54s/it]

 69%|██████▊   | 1842/2685 [10:06:48<4:35:38, 19.62s/it]

 69%|██████▊   | 1843/2685 [10:07:07<4:35:58, 19.67s/it]

 69%|██████▊   | 1844/2685 [10:07:27<4:35:57, 19.69s/it]

 69%|██████▊   | 1845/2685 [10:07:48<4:41:17, 20.09s/it]
{'loss': 0.3237, 'learning_rate': 4.710359896730378e-07, 'rewards/chosen': -1.4680405855178833, 'rewards/rejected': -4.8292670249938965, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3612263202667236, 'policy_logps/rejected': -277.8646545410156, 'policy_logps/chosen': -270.2981262207031, 'referece_logps/rejected': -229.57199096679688, 'referece_logps/chosen': -255.6177215576172, 'logits/rejected': -0.055739372968673706, 'logits/chosen': -0.24430173635482788, 'epoch': 2.06}


 69%|██████▉   | 1847/2685 [10:08:28<4:40:16, 20.07s/it]

 69%|██████▉   | 1848/2685 [10:08:46<4:31:15, 19.45s/it]

 69%|██████▉   | 1849/2685 [10:09:04<4:24:08, 18.96s/it]

 69%|██████▉   | 1850/2685 [10:09:23<4:27:45, 19.24s/it]

 69%|██████▉   | 1851/2685 [10:09:41<4:22:19, 18.87s/it]
{'loss': 0.203, 'learning_rate': 4.649068267323465e-07, 'rewards/chosen': -0.9442497491836548, 'rewards/rejected': -4.70028829574585, 'rewards/accuracies': 1.0, 'rewards/margins': 3.756038188934326, 'policy_logps/rejected': -510.36456298828125, 'policy_logps/chosen': -366.0981140136719, 'referece_logps/rejected': -463.3616638183594, 'referece_logps/chosen': -356.6556091308594, 'logits/rejected': -0.29583168029785156, 'logits/chosen': -0.2503129243850708, 'epoch': 2.07}


 69%|██████▉   | 1853/2685 [10:10:20<4:26:24, 19.21s/it]

 69%|██████▉   | 1854/2685 [10:10:40<4:28:20, 19.38s/it]

 69%|██████▉   | 1855/2685 [10:11:00<4:29:39, 19.49s/it]

 69%|██████▉   | 1856/2685 [10:11:20<4:30:15, 19.56s/it]

 69%|██████▉   | 1857/2685 [10:11:40<4:33:06, 19.79s/it]

 69%|██████▉   | 1858/2685 [10:11:57<4:19:27, 18.82s/it]

 69%|██████▉   | 1859/2685 [10:12:18<4:31:36, 19.73s/it]

 69%|██████▉   | 1860/2685 [10:12:35<4:20:00, 18.91s/it]
{'loss': 0.2587, 'learning_rate': 4.5576575362517957e-07, 'rewards/chosen': -1.5990084409713745, 'rewards/rejected': -4.208608627319336, 'rewards/accuracies': 1.0, 'rewards/margins': 2.609600305557251, 'policy_logps/rejected': -304.03228759765625, 'policy_logps/chosen': -299.26727294921875, 'referece_logps/rejected': -261.94622802734375, 'referece_logps/chosen': -283.2772216796875, 'logits/rejected': -0.22600629925727844, 'logits/chosen': -0.18681544065475464, 'epoch': 2.08}

 69%|██████▉   | 1861/2685 [10:12:53<4:13:56, 18.49s/it]

 69%|██████▉   | 1862/2685 [10:13:11<4:13:57, 18.51s/it]

 69%|██████▉   | 1863/2685 [10:13:32<4:20:23, 19.01s/it]

 69%|██████▉   | 1864/2685 [10:13:52<4:23:49, 19.28s/it]

 69%|██████▉   | 1865/2685 [10:14:05<4:01:22, 17.66s/it]


 70%|██████▉   | 1867/2685 [10:14:41<3:57:38, 17.43s/it]

 70%|██████▉   | 1868/2685 [10:15:00<4:07:31, 18.18s/it]
{'loss': 0.3585, 'learning_rate': 4.4769418717102146e-07, 'rewards/chosen': -0.5822136402130127, 'rewards/rejected': -2.6134254932403564, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0312118530273438, 'policy_logps/rejected': -490.68975830078125, 'policy_logps/chosen': -412.84454345703125, 'referece_logps/rejected': -464.5555114746094, 'referece_logps/chosen': -407.02239990234375, 'logits/rejected': 0.2875952422618866, 'logits/chosen': 0.3001195788383484, 'epoch': 2.09}


 70%|██████▉   | 1870/2685 [10:15:39<4:14:06, 18.71s/it]

 70%|██████▉   | 1871/2685 [10:15:55<4:03:14, 17.93s/it]
{'loss': 0.2796, 'learning_rate': 4.446805736239594e-07, 'rewards/chosen': -0.40638619661331177, 'rewards/rejected': -4.809155464172363, 'rewards/accuracies': 1.0, 'rewards/margins': 4.402769088745117, 'policy_logps/rejected': -331.18157958984375, 'policy_logps/chosen': -382.55987548828125, 'referece_logps/rejected': -283.09002685546875, 'referece_logps/chosen': -378.4960021972656, 'logits/rejected': -0.4495835304260254, 'logits/chosen': -0.4419153332710266, 'epoch': 2.09}

 70%|██████▉   | 1872/2685 [10:16:18<4:23:18, 19.43s/it]


 70%|██████▉   | 1874/2685 [10:16:55<4:20:07, 19.25s/it]
{'loss': 0.3057, 'learning_rate': 4.4167423456917683e-07, 'rewards/chosen': -2.3233144283294678, 'rewards/rejected': -4.597432613372803, 'rewards/accuracies': 0.875, 'rewards/margins': 2.274118661880493, 'policy_logps/rejected': -359.4114685058594, 'policy_logps/chosen': -365.9229736328125, 'referece_logps/rejected': -313.4371337890625, 'referece_logps/chosen': -342.6897888183594, 'logits/rejected': -0.9417625665664673, 'logits/chosen': -0.9574584364891052, 'epoch': 2.09}


 70%|██████▉   | 1876/2685 [10:17:35<4:26:02, 19.73s/it]
{'loss': 0.1865, 'learning_rate': 4.3967406936909414e-07, 'rewards/chosen': -0.4757244288921356, 'rewards/rejected': -2.7946507930755615, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3189260959625244, 'policy_logps/rejected': -378.533447265625, 'policy_logps/chosen': -328.0643615722656, 'referece_logps/rejected': -350.5869445800781, 'referece_logps/chosen': -323.3070983886719, 'logits/rejected': -0.1404029130935669, 'logits/chosen': -0.2278582900762558, 'epoch': 2.1}

 70%|██████▉   | 1877/2685 [10:17:54<4:22:21, 19.48s/it]

 70%|██████▉   | 1878/2685 [10:18:15<4:30:03, 20.08s/it]

 70%|██████▉   | 1879/2685 [10:18:34<4:23:49, 19.64s/it]

 70%|███████   | 1880/2685 [10:18:54<4:23:55, 19.67s/it]


 70%|███████   | 1882/2685 [10:19:33<4:23:14, 19.67s/it]

 70%|███████   | 1883/2685 [10:19:53<4:23:29, 19.71s/it]

 70%|███████   | 1884/2685 [10:20:15<4:32:39, 20.42s/it]

 70%|███████   | 1885/2685 [10:20:34<4:29:07, 20.18s/it]
{'loss': 0.3096, 'learning_rate': 4.3071386408966957e-07, 'rewards/chosen': -1.5688503980636597, 'rewards/rejected': -3.1476125717163086, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5787620544433594, 'policy_logps/rejected': -505.0885925292969, 'policy_logps/chosen': -384.8041076660156, 'referece_logps/rejected': -473.6124267578125, 'referece_logps/chosen': -369.1156005859375, 'logits/rejected': -0.7246811389923096, 'logits/chosen': -0.7365757822990417, 'epoch': 2.11}


 70%|███████   | 1887/2685 [10:21:12<4:19:50, 19.54s/it]

 70%|███████   | 1888/2685 [10:21:28<4:05:11, 18.46s/it]

 70%|███████   | 1889/2685 [10:21:48<4:10:57, 18.92s/it]

 70%|███████   | 1890/2685 [10:22:08<4:14:13, 19.19s/it]
{'loss': 0.2034, 'learning_rate': 4.2576490591310345e-07, 'rewards/chosen': -0.7080721259117126, 'rewards/rejected': -3.9260094165802, 'rewards/accuracies': 1.0, 'rewards/margins': 3.217937469482422, 'policy_logps/rejected': -562.89990234375, 'policy_logps/chosen': -423.51470947265625, 'referece_logps/rejected': -523.6398315429688, 'referece_logps/chosen': -416.4339599609375, 'logits/rejected': 0.11515574157238007, 'logits/chosen': 0.09161114692687988, 'epoch': 2.11}


 70%|███████   | 1892/2685 [10:22:47<4:14:13, 19.24s/it]
{'loss': 0.3144, 'learning_rate': 4.237911632349929e-07, 'rewards/chosen': -1.379826545715332, 'rewards/rejected': -3.7304513454437256, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3506245613098145, 'policy_logps/rejected': -332.2550048828125, 'policy_logps/chosen': -312.6404113769531, 'referece_logps/rejected': -294.95050048828125, 'referece_logps/chosen': -298.8421325683594, 'logits/rejected': -1.0332258939743042, 'logits/chosen': -1.0821231603622437, 'epoch': 2.11}

 71%|███████   | 1893/2685 [10:23:06<4:12:12, 19.11s/it]


 71%|███████   | 1895/2685 [10:23:49<4:27:52, 20.34s/it]

 71%|███████   | 1896/2685 [10:24:09<4:26:55, 20.30s/it]
{'loss': 0.3584, 'learning_rate': 4.1985375353838383e-07, 'rewards/chosen': -1.6658838987350464, 'rewards/rejected': -3.6976418495178223, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0317580699920654, 'policy_logps/rejected': -351.2925720214844, 'policy_logps/chosen': -319.05023193359375, 'referece_logps/rejected': -314.316162109375, 'referece_logps/chosen': -302.39141845703125, 'logits/rejected': -1.0001599788665771, 'logits/chosen': -1.1807061433792114, 'epoch': 2.12}


 71%|███████   | 1898/2685 [10:24:51<4:29:16, 20.53s/it]
{'loss': 0.278, 'learning_rate': 4.1789010944376746e-07, 'rewards/chosen': -1.271606206893921, 'rewards/rejected': -3.7836828231811523, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5120766162872314, 'policy_logps/rejected': -350.71533203125, 'policy_logps/chosen': -271.0941467285156, 'referece_logps/rejected': -312.8785095214844, 'referece_logps/chosen': -258.3780822753906, 'logits/rejected': -1.2153445482254028, 'logits/chosen': -1.1437554359436035, 'epoch': 2.12}


 71%|███████   | 1900/2685 [10:25:29<4:19:19, 19.82s/it]

 71%|███████   | 1901/2685 [10:25:47<4:10:30, 19.17s/it]
{'loss': 0.3268, 'learning_rate': 4.149510014046922e-07, 'rewards/chosen': -1.4510161876678467, 'rewards/rejected': -5.0247955322265625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.573779582977295, 'policy_logps/rejected': -422.74420166015625, 'policy_logps/chosen': -389.3130798339844, 'referece_logps/rejected': -372.49627685546875, 'referece_logps/chosen': -374.8029479980469, 'logits/rejected': -0.6225454807281494, 'logits/chosen': -0.601949155330658, 'epoch': 2.12}


 71%|███████   | 1903/2685 [10:26:27<4:15:01, 19.57s/it]

 71%|███████   | 1904/2685 [10:26:45<4:07:46, 19.04s/it]
{'loss': 0.1711, 'learning_rate': 4.1201955730500337e-07, 'rewards/chosen': -0.6392248868942261, 'rewards/rejected': -4.155680179595947, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5164551734924316, 'policy_logps/rejected': -385.84100341796875, 'policy_logps/chosen': -304.41607666015625, 'referece_logps/rejected': -344.2842102050781, 'referece_logps/chosen': -298.0238037109375, 'logits/rejected': -1.2859086990356445, 'logits/chosen': -1.3285915851593018, 'epoch': 2.13}

 71%|███████   | 1905/2685 [10:27:06<4:15:41, 19.67s/it]

 71%|███████   | 1906/2685 [10:27:26<4:16:23, 19.75s/it]

 71%|███████   | 1907/2685 [10:27:46<4:16:11, 19.76s/it]

 71%|███████   | 1908/2685 [10:28:06<4:16:27, 19.80s/it]


 71%|███████   | 1910/2685 [10:28:45<4:15:13, 19.76s/it]
{'loss': 0.2518, 'learning_rate': 4.061798144264985e-07, 'rewards/chosen': -1.46204674243927, 'rewards/rejected': -3.136963367462158, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6749166250228882, 'policy_logps/rejected': -316.55859375, 'policy_logps/chosen': -272.18634033203125, 'referece_logps/rejected': -285.18896484375, 'referece_logps/chosen': -257.5658874511719, 'logits/rejected': 0.14641286432743073, 'logits/chosen': 0.14345772564411163, 'epoch': 2.13}


 71%|███████   | 1912/2685 [10:29:24<4:12:55, 19.63s/it]

 71%|███████   | 1913/2685 [10:29:43<4:12:12, 19.60s/it]

 71%|███████▏  | 1914/2685 [10:30:03<4:13:27, 19.72s/it]
{'loss': 0.2738, 'learning_rate': 4.0230391993798483e-07, 'rewards/chosen': -1.5502943992614746, 'rewards/rejected': -4.13896369934082, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5886693000793457, 'policy_logps/rejected': -333.54449462890625, 'policy_logps/chosen': -372.40252685546875, 'referece_logps/rejected': -292.15484619140625, 'referece_logps/chosen': -356.8996276855469, 'logits/rejected': -0.3965063691139221, 'logits/chosen': -0.33572816848754883, 'epoch': 2.14}


 71%|███████▏  | 1916/2685 [10:30:47<4:27:38, 20.88s/it]

 71%|███████▏  | 1917/2685 [10:31:07<4:23:34, 20.59s/it]
{'loss': 0.2, 'learning_rate': 3.994061286868361e-07, 'rewards/chosen': -1.0294893980026245, 'rewards/rejected': -5.023213863372803, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9937245845794678, 'policy_logps/rejected': -413.7016296386719, 'policy_logps/chosen': -401.4350280761719, 'referece_logps/rejected': -363.4695129394531, 'referece_logps/chosen': -391.14013671875, 'logits/rejected': 0.10194703936576843, 'logits/chosen': 0.09140034019947052, 'epoch': 2.14}

 71%|███████▏  | 1918/2685 [10:31:26<4:18:15, 20.20s/it]


 72%|███████▏  | 1920/2685 [10:32:03<4:04:40, 19.19s/it]
{'loss': 0.2319, 'learning_rate': 3.9651620500752536e-07, 'rewards/chosen': -1.0836727619171143, 'rewards/rejected': -2.935232162475586, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8515594005584717, 'policy_logps/rejected': -314.884521484375, 'policy_logps/chosen': -207.8052215576172, 'referece_logps/rejected': -285.5321960449219, 'referece_logps/chosen': -196.96849060058594, 'logits/rejected': -1.4208449125289917, 'logits/chosen': -1.4913744926452637, 'epoch': 2.15}

 72%|███████▏  | 1921/2685 [10:32:19<3:49:09, 18.00s/it]

 72%|███████▏  | 1922/2685 [10:32:40<4:01:57, 19.03s/it]

 72%|███████▏  | 1923/2685 [10:33:00<4:05:03, 19.30s/it]


 72%|███████▏  | 1925/2685 [10:33:43<4:18:11, 20.38s/it]

 72%|███████▏  | 1926/2685 [10:33:59<4:01:28, 19.09s/it]

 72%|███████▏  | 1927/2685 [10:34:22<4:14:42, 20.16s/it]
{'loss': 0.2692, 'learning_rate': 3.898038583280565e-07, 'rewards/chosen': -1.9764554500579834, 'rewards/rejected': -4.151416301727295, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1749606132507324, 'policy_logps/rejected': -456.41632080078125, 'policy_logps/chosen': -423.799560546875, 'referece_logps/rejected': -414.9021301269531, 'referece_logps/chosen': -404.03497314453125, 'logits/rejected': -0.39130038022994995, 'logits/chosen': -0.35587239265441895, 'epoch': 2.15}

 72%|███████▏  | 1928/2685 [10:34:37<3:54:05, 18.55s/it]

 72%|███████▏  | 1929/2685 [10:34:57<3:58:53, 18.96s/it]

 72%|███████▏  | 1930/2685 [10:35:12<3:46:40, 18.01s/it]


 72%|███████▏  | 1932/2685 [10:35:48<3:43:30, 17.81s/it]

 72%|███████▏  | 1933/2685 [10:36:07<3:49:45, 18.33s/it]
{'loss': 0.3506, 'learning_rate': 3.840850379745465e-07, 'rewards/chosen': -1.3464428186416626, 'rewards/rejected': -5.782196044921875, 'rewards/accuracies': 1.0, 'rewards/margins': 4.43575382232666, 'policy_logps/rejected': -508.35369873046875, 'policy_logps/chosen': -495.959716796875, 'referece_logps/rejected': -450.53173828125, 'referece_logps/chosen': -482.49530029296875, 'logits/rejected': 0.8333054184913635, 'logits/chosen': 0.9246739745140076, 'epoch': 2.16}


 72%|███████▏  | 1935/2685 [10:36:48<4:01:33, 19.32s/it]
{'loss': 0.2721, 'learning_rate': 3.821859215486274e-07, 'rewards/chosen': -1.3375202417373657, 'rewards/rejected': -2.9550395011901855, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6175190210342407, 'policy_logps/rejected': -237.15611267089844, 'policy_logps/chosen': -195.73846435546875, 'referece_logps/rejected': -207.60572814941406, 'referece_logps/chosen': -182.36328125, 'logits/rejected': -1.2321557998657227, 'logits/chosen': -1.0740551948547363, 'epoch': 2.16}

 72%|███████▏  | 1936/2685 [10:37:10<4:11:25, 20.14s/it]


 72%|███████▏  | 1938/2685 [10:37:46<3:56:43, 19.01s/it]

 72%|███████▏  | 1939/2685 [10:38:05<3:56:45, 19.04s/it]

 72%|███████▏  | 1940/2685 [10:38:22<3:46:37, 18.25s/it]

 72%|███████▏  | 1941/2685 [10:38:44<4:00:32, 19.40s/it]

 72%|███████▏  | 1942/2685 [10:39:02<3:56:46, 19.12s/it]
{'loss': 0.2598, 'learning_rate': 3.755674124732202e-07, 'rewards/chosen': -1.4325941801071167, 'rewards/rejected': -4.4197998046875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.987205982208252, 'policy_logps/rejected': -531.5170288085938, 'policy_logps/chosen': -498.5799255371094, 'referece_logps/rejected': -487.3190002441406, 'referece_logps/chosen': -484.2540283203125, 'logits/rejected': 0.116922527551651, 'logits/chosen': 0.11999110877513885, 'epoch': 2.17}

 72%|███████▏  | 1943/2685 [10:39:21<3:54:46, 18.98s/it]

 72%|███████▏  | 1944/2685 [10:39:43<4:06:58, 20.00s/it]


 72%|███████▏  | 1946/2685 [10:40:26<4:13:49, 20.61s/it]

 73%|███████▎  | 1947/2685 [10:40:44<4:06:26, 20.04s/it]

 73%|███████▎  | 1948/2685 [10:41:06<4:10:36, 20.40s/it]

 73%|███████▎  | 1949/2685 [10:41:26<4:08:26, 20.25s/it]
{'loss': 0.3247, 'learning_rate': 3.6899343792482286e-07, 'rewards/chosen': -1.9935271739959717, 'rewards/rejected': -5.41321325302124, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4196863174438477, 'policy_logps/rejected': -292.04852294921875, 'policy_logps/chosen': -273.9620361328125, 'referece_logps/rejected': -237.9163818359375, 'referece_logps/chosen': -254.0267333984375, 'logits/rejected': -0.9732367992401123, 'logits/chosen': -0.8519594073295593, 'epoch': 2.18}

 73%|███████▎  | 1950/2685 [10:41:43<3:57:43, 19.41s/it]


 73%|███████▎  | 1952/2685 [10:42:23<3:59:37, 19.61s/it]

 73%|███████▎  | 1953/2685 [10:42:42<3:59:53, 19.66s/it]

 73%|███████▎  | 1954/2685 [10:43:05<4:09:03, 20.44s/it]
{'loss': 0.316, 'learning_rate': 3.643252676984729e-07, 'rewards/chosen': -1.1667429208755493, 'rewards/rejected': -3.855586290359497, 'rewards/accuracies': 0.75, 'rewards/margins': 2.688843250274658, 'policy_logps/rejected': -297.9275817871094, 'policy_logps/chosen': -241.8441162109375, 'referece_logps/rejected': -259.3717041015625, 'referece_logps/chosen': -230.1766815185547, 'logits/rejected': -0.2790375351905823, 'logits/chosen': -0.25303640961647034, 'epoch': 2.18}

 73%|███████▎  | 1955/2685 [10:43:25<4:07:57, 20.38s/it]


 73%|███████▎  | 1957/2685 [10:44:09<4:16:13, 21.12s/it]
{'loss': 0.2824, 'learning_rate': 3.6153545753001655e-07, 'rewards/chosen': -1.648354411125183, 'rewards/rejected': -4.07260274887085, 'rewards/accuracies': 0.75, 'rewards/margins': 2.424248456954956, 'policy_logps/rejected': -331.39825439453125, 'policy_logps/chosen': -333.58978271484375, 'referece_logps/rejected': -290.6722412109375, 'referece_logps/chosen': -317.1062316894531, 'logits/rejected': -0.684587836265564, 'logits/chosen': -0.6817600131034851, 'epoch': 2.19}

 73%|███████▎  | 1958/2685 [10:44:29<4:12:22, 20.83s/it]

 73%|███████▎  | 1959/2685 [10:44:51<4:17:52, 21.31s/it]

 73%|███████▎  | 1960/2685 [10:45:08<4:01:23, 19.98s/it]


 73%|███████▎  | 1962/2685 [10:45:44<3:48:26, 18.96s/it]

 73%|███████▎  | 1963/2685 [10:46:04<3:52:01, 19.28s/it]

 73%|███████▎  | 1964/2685 [10:46:24<3:54:10, 19.49s/it]
{'loss': 0.2433, 'learning_rate': 3.550584887922582e-07, 'rewards/chosen': -2.5139832496643066, 'rewards/rejected': -4.112265110015869, 'rewards/accuracies': 1.0, 'rewards/margins': 1.598282814025879, 'policy_logps/rejected': -345.4557189941406, 'policy_logps/chosen': -297.40576171875, 'referece_logps/rejected': -304.3330383300781, 'referece_logps/chosen': -272.26593017578125, 'logits/rejected': -0.44914740324020386, 'logits/chosen': -0.5135586261749268, 'epoch': 2.19}


 73%|███████▎  | 1966/2685 [10:47:03<3:53:11, 19.46s/it]
{'loss': 0.2334, 'learning_rate': 3.53216354645693e-07, 'rewards/chosen': -0.8044555187225342, 'rewards/rejected': -2.5886638164520264, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7842081785202026, 'policy_logps/rejected': -359.4474792480469, 'policy_logps/chosen': -295.75970458984375, 'referece_logps/rejected': -333.5608215332031, 'referece_logps/chosen': -287.71514892578125, 'logits/rejected': -0.9582003355026245, 'logits/chosen': -0.8891139626502991, 'epoch': 2.2}

 73%|███████▎  | 1967/2685 [10:47:22<3:53:06, 19.48s/it]

 73%|███████▎  | 1968/2685 [10:47:42<3:53:00, 19.50s/it]


 73%|███████▎  | 1970/2685 [10:48:19<3:48:48, 19.20s/it]
{'loss': 0.2774, 'learning_rate': 3.4954339391838284e-07, 'rewards/chosen': -1.287199854850769, 'rewards/rejected': -3.068773031234741, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7815730571746826, 'policy_logps/rejected': -264.37811279296875, 'policy_logps/chosen': -247.83377075195312, 'referece_logps/rejected': -233.6903839111328, 'referece_logps/chosen': -234.96176147460938, 'logits/rejected': -0.16662558913230896, 'logits/chosen': -0.21992920339107513, 'epoch': 2.2}

 73%|███████▎  | 1971/2685 [10:48:35<3:39:52, 18.48s/it]

 73%|███████▎  | 1972/2685 [10:48:56<3:47:31, 19.15s/it]

 73%|███████▎  | 1973/2685 [10:49:14<3:42:21, 18.74s/it]

 74%|███████▎  | 1974/2685 [10:49:34<3:45:53, 19.06s/it]


 74%|███████▎  | 1976/2685 [10:50:17<3:58:22, 20.17s/it]

 74%|███████▎  | 1977/2685 [10:50:35<3:51:13, 19.60s/it]

 74%|███████▎  | 1978/2685 [10:50:57<3:59:11, 20.30s/it]

 74%|███████▎  | 1979/2685 [10:51:17<3:57:20, 20.17s/it]
{'loss': 0.2137, 'learning_rate': 3.4133474689040396e-07, 'rewards/chosen': -0.5668377876281738, 'rewards/rejected': -5.012077808380127, 'rewards/accuracies': 1.0, 'rewards/margins': 4.445240020751953, 'policy_logps/rejected': -475.1441955566406, 'policy_logps/chosen': -305.06317138671875, 'referece_logps/rejected': -425.02337646484375, 'referece_logps/chosen': -299.39483642578125, 'logits/rejected': -0.29053372144699097, 'logits/chosen': -0.31971704959869385, 'epoch': 2.21}


 74%|███████▍  | 1981/2685 [10:51:55<3:49:14, 19.54s/it]

 74%|███████▍  | 1982/2685 [10:52:17<3:57:43, 20.29s/it]
{'loss': 0.3028, 'learning_rate': 3.386157401091385e-07, 'rewards/chosen': -0.6897818446159363, 'rewards/rejected': -4.531978130340576, 'rewards/accuracies': 1.0, 'rewards/margins': 3.842196226119995, 'policy_logps/rejected': -360.1244812011719, 'policy_logps/chosen': -331.9144592285156, 'referece_logps/rejected': -314.80474853515625, 'referece_logps/chosen': -325.01666259765625, 'logits/rejected': -0.9086169600486755, 'logits/chosen': -0.8028504252433777, 'epoch': 2.21}


 74%|███████▍  | 1984/2685 [10:52:53<3:43:43, 19.15s/it]
{'loss': 0.2417, 'learning_rate': 3.36807880008142e-07, 'rewards/chosen': -0.824150562286377, 'rewards/rejected': -3.468486785888672, 'rewards/accuracies': 0.75, 'rewards/margins': 2.644336223602295, 'policy_logps/rejected': -282.54656982421875, 'policy_logps/chosen': -287.9347229003906, 'referece_logps/rejected': -247.8616943359375, 'referece_logps/chosen': -279.6932067871094, 'logits/rejected': -0.536048412322998, 'logits/chosen': -0.48015356063842773, 'epoch': 2.22}

 74%|███████▍  | 1985/2685 [10:53:12<3:43:49, 19.18s/it]

 74%|███████▍  | 1986/2685 [10:53:32<3:45:45, 19.38s/it]


 74%|███████▍  | 1988/2685 [10:54:13<3:53:35, 20.11s/it]
{'loss': 0.3398, 'learning_rate': 3.332037537658252e-07, 'rewards/chosen': 0.32595038414001465, 'rewards/rejected': -2.3326313495635986, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6585817337036133, 'policy_logps/rejected': -325.5489501953125, 'policy_logps/chosen': -479.538818359375, 'referece_logps/rejected': -302.22265625, 'referece_logps/chosen': -482.7983093261719, 'logits/rejected': -1.4991670846939087, 'logits/chosen': -1.4540622234344482, 'epoch': 2.22}

 74%|███████▍  | 1989/2685 [10:54:34<3:56:59, 20.43s/it]


 74%|███████▍  | 1991/2685 [10:55:13<3:50:28, 19.93s/it]
{'loss': 0.2745, 'learning_rate': 3.305108450989257e-07, 'rewards/chosen': -1.5105257034301758, 'rewards/rejected': -4.02010440826416, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5095784664154053, 'policy_logps/rejected': -339.182373046875, 'policy_logps/chosen': -355.16314697265625, 'referece_logps/rejected': -298.9813232421875, 'referece_logps/chosen': -340.0578918457031, 'logits/rejected': -1.0719966888427734, 'logits/chosen': -1.170150876045227, 'epoch': 2.22}

 74%|███████▍  | 1992/2685 [10:55:35<3:55:30, 20.39s/it]

 74%|███████▍  | 1993/2685 [10:55:55<3:54:00, 20.29s/it]

 74%|███████▍  | 1994/2685 [10:56:11<3:39:01, 19.02s/it]


 74%|███████▍  | 1996/2685 [10:56:51<3:43:50, 19.49s/it]

 74%|███████▍  | 1997/2685 [10:57:13<3:52:06, 20.24s/it]

 74%|███████▍  | 1998/2685 [10:57:31<3:43:16, 19.50s/it]
{'loss': 0.2753, 'learning_rate': 3.2426155815073397e-07, 'rewards/chosen': -0.6396857500076294, 'rewards/rejected': -4.253328800201416, 'rewards/accuracies': 1.0, 'rewards/margins': 3.613643169403076, 'policy_logps/rejected': -428.105224609375, 'policy_logps/chosen': -288.0526123046875, 'referece_logps/rejected': -385.57196044921875, 'referece_logps/chosen': -281.65570068359375, 'logits/rejected': -1.6029932498931885, 'logits/chosen': -1.6273506879806519, 'epoch': 2.23}


 74%|███████▍  | 2000/2685 [10:58:15<3:57:15, 20.78s/it]
{'loss': 0.2527, 'learning_rate': 3.2248488008527243e-07, 'rewards/chosen': -1.499523401260376, 'rewards/rejected': -4.45078182220459, 'rewards/accuracies': 1.0, 'rewards/margins': 2.951258420944214, 'policy_logps/rejected': -438.1144104003906, 'policy_logps/chosen': -330.3647766113281, 'referece_logps/rejected': -393.6065673828125, 'referece_logps/chosen': -315.36956787109375, 'logits/rejected': -0.6104195713996887, 'logits/chosen': -0.6903455853462219, 'epoch': 2.23}

 75%|███████▍  | 2001/2685 [10:58:52<4:51:25, 25.56s/it]

 75%|███████▍  | 2002/2685 [10:59:12<4:31:51, 23.88s/it]

 75%|███████▍  | 2003/2685 [10:59:29<4:08:04, 21.82s/it]

 75%|███████▍  | 2004/2685 [10:59:51<4:08:37, 21.90s/it]

 75%|███████▍  | 2005/2685 [11:00:09<3:56:00, 20.82s/it]

 75%|███████▍  | 2006/2685 [11:00:27<3:46:02, 19.97s/it]

 75%|███████▍  | 2007/2685 [11:00:47<3:43:25, 19.77s/it]


 75%|███████▍  | 2009/2685 [11:01:28<3:46:35, 20.11s/it]

 75%|███████▍  | 2010/2685 [11:01:45<3:38:30, 19.42s/it]
{'loss': 0.2598, 'learning_rate': 3.13660863958223e-07, 'rewards/chosen': -1.2177296876907349, 'rewards/rejected': -3.6125340461730957, 'rewards/accuracies': 1.0, 'rewards/margins': 2.394804000854492, 'policy_logps/rejected': -290.36865234375, 'policy_logps/chosen': -284.90313720703125, 'referece_logps/rejected': -254.24334716796875, 'referece_logps/chosen': -272.7258605957031, 'logits/rejected': -0.9750194549560547, 'logits/chosen': -0.9995908141136169, 'epoch': 2.25}

 75%|███████▍  | 2011/2685 [11:02:03<3:31:25, 18.82s/it]

 75%|███████▍  | 2012/2685 [11:02:22<3:32:45, 18.97s/it]


 75%|███████▌  | 2014/2685 [11:03:00<3:29:27, 18.73s/it]
{'loss': 0.3437, 'learning_rate': 3.1015915715610286e-07, 'rewards/chosen': -1.1093753576278687, 'rewards/rejected': -2.1245834827423096, 'rewards/accuracies': 0.625, 'rewards/margins': 1.015208125114441, 'policy_logps/rejected': -303.4635314941406, 'policy_logps/chosen': -312.914794921875, 'referece_logps/rejected': -282.21771240234375, 'referece_logps/chosen': -301.8210754394531, 'logits/rejected': -2.1000823974609375, 'logits/chosen': -2.148533344268799, 'epoch': 2.25}

 75%|███████▌  | 2015/2685 [11:03:14<3:15:49, 17.54s/it]

 75%|███████▌  | 2016/2685 [11:03:33<3:18:35, 17.81s/it]


 75%|███████▌  | 2018/2685 [11:04:13<3:30:34, 18.94s/it]
{'loss': 0.3342, 'learning_rate': 3.066735155426099e-07, 'rewards/chosen': -1.0314750671386719, 'rewards/rejected': -3.0161309242248535, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9846558570861816, 'policy_logps/rejected': -370.9662170410156, 'policy_logps/chosen': -310.09088134765625, 'referece_logps/rejected': -340.804931640625, 'referece_logps/chosen': -299.776123046875, 'logits/rejected': -1.054215431213379, 'logits/chosen': -1.0069688558578491, 'epoch': 2.25}


 75%|███████▌  | 2020/2685 [11:04:46<3:14:56, 17.59s/it]
{'loss': 0.2896, 'learning_rate': 3.049367445632981e-07, 'rewards/chosen': -2.0722620487213135, 'rewards/rejected': -3.3729891777038574, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3007270097732544, 'policy_logps/rejected': -345.55377197265625, 'policy_logps/chosen': -319.1470031738281, 'referece_logps/rejected': -311.8238830566406, 'referece_logps/chosen': -298.4244079589844, 'logits/rejected': -0.300418883562088, 'logits/chosen': -0.27081918716430664, 'epoch': 2.26}

 75%|███████▌  | 2021/2685 [11:05:01<3:06:36, 16.86s/it]

 75%|███████▌  | 2022/2685 [11:05:22<3:20:01, 18.10s/it]


 75%|███████▌  | 2024/2685 [11:06:00<3:24:11, 18.53s/it]

 75%|███████▌  | 2025/2685 [11:06:22<3:34:32, 19.50s/it]
{'loss': 0.3724, 'learning_rate': 3.00612543524818e-07, 'rewards/chosen': -0.7141838073730469, 'rewards/rejected': -3.1964569091796875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4822731018066406, 'policy_logps/rejected': -408.95343017578125, 'policy_logps/chosen': -362.41778564453125, 'referece_logps/rejected': -376.9888610839844, 'referece_logps/chosen': -355.2759094238281, 'logits/rejected': -0.22450906038284302, 'logits/chosen': -0.25551748275756836, 'epoch': 2.26}

 75%|███████▌  | 2026/2685 [11:06:44<3:43:49, 20.38s/it]

 75%|███████▌  | 2027/2685 [11:07:01<3:31:00, 19.24s/it]

 76%|███████▌  | 2028/2685 [11:07:21<3:32:58, 19.45s/it]

 76%|███████▌  | 2029/2685 [11:07:41<3:34:17, 19.60s/it]

 76%|███████▌  | 2030/2685 [11:08:00<3:34:44, 19.67s/it]


 76%|███████▌  | 2032/2685 [11:08:42<3:40:13, 20.24s/it]
{'loss': 0.3058, 'learning_rate': 2.9460145181467324e-07, 'rewards/chosen': -2.018221855163574, 'rewards/rejected': -2.693108081817627, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6748861074447632, 'policy_logps/rejected': -305.3494567871094, 'policy_logps/chosen': -335.0190734863281, 'referece_logps/rejected': -278.4183654785156, 'referece_logps/chosen': -314.83685302734375, 'logits/rejected': -1.3878861665725708, 'logits/chosen': -1.2480616569519043, 'epoch': 2.27}

 76%|███████▌  | 2033/2685 [11:09:00<3:31:09, 19.43s/it]

 76%|███████▌  | 2034/2685 [11:09:19<3:30:31, 19.40s/it]


 76%|███████▌  | 2036/2685 [11:09:58<3:30:16, 19.44s/it]
{'loss': 0.2069, 'learning_rate': 2.911891026387693e-07, 'rewards/chosen': -1.1456718444824219, 'rewards/rejected': -4.691975116729736, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5463035106658936, 'policy_logps/rejected': -408.4212646484375, 'policy_logps/chosen': -395.5473937988281, 'referece_logps/rejected': -361.50146484375, 'referece_logps/chosen': -384.0906677246094, 'logits/rejected': 0.48423993587493896, 'logits/chosen': 0.4377500116825104, 'epoch': 2.27}

 76%|███████▌  | 2037/2685 [11:10:19<3:35:31, 19.96s/it]

 76%|███████▌  | 2038/2685 [11:10:39<3:35:19, 19.97s/it]

 76%|███████▌  | 2039/2685 [11:10:55<3:21:27, 18.71s/it]

 76%|███████▌  | 2040/2685 [11:11:15<3:24:35, 19.03s/it]

 76%|███████▌  | 2041/2685 [11:11:35<3:28:09, 19.39s/it]

 76%|███████▌  | 2042/2685 [11:11:55<3:29:42, 19.57s/it]


 76%|███████▌  | 2044/2685 [11:12:34<3:29:46, 19.64s/it]
{'loss': 0.2593, 'learning_rate': 2.844140042744114e-07, 'rewards/chosen': -0.4516901969909668, 'rewards/rejected': -3.9593801498413086, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5076897144317627, 'policy_logps/rejected': -271.158447265625, 'policy_logps/chosen': -241.93804931640625, 'referece_logps/rejected': -231.56460571289062, 'referece_logps/chosen': -237.421142578125, 'logits/rejected': -0.4715861678123474, 'logits/chosen': -0.4691249132156372, 'epoch': 2.28}

 76%|███████▌  | 2045/2685 [11:12:53<3:25:56, 19.31s/it]

 76%|███████▌  | 2046/2685 [11:13:13<3:27:05, 19.44s/it]

 76%|███████▌  | 2047/2685 [11:13:34<3:32:15, 19.96s/it]


 76%|███████▋  | 2049/2685 [11:14:14<3:32:38, 20.06s/it]

 76%|███████▋  | 2050/2685 [11:14:30<3:19:20, 18.84s/it]
{'loss': 0.2168, 'learning_rate': 2.793763909293867e-07, 'rewards/chosen': 0.21496835350990295, 'rewards/rejected': -3.79177188873291, 'rewards/accuracies': 0.875, 'rewards/margins': 4.006740093231201, 'policy_logps/rejected': -332.02325439453125, 'policy_logps/chosen': -304.019287109375, 'referece_logps/rejected': -294.10552978515625, 'referece_logps/chosen': -306.1689758300781, 'logits/rejected': -0.9227805733680725, 'logits/chosen': -0.954540491104126, 'epoch': 2.29}

 76%|███████▋  | 2051/2685 [11:14:48<3:15:30, 18.50s/it]

 76%|███████▋  | 2052/2685 [11:15:10<3:25:16, 19.46s/it]

 76%|███████▋  | 2053/2685 [11:15:29<3:25:15, 19.49s/it]

 76%|███████▋  | 2054/2685 [11:15:48<3:21:36, 19.17s/it]

 77%|███████▋  | 2055/2685 [11:16:05<3:16:32, 18.72s/it]

 77%|███████▋  | 2056/2685 [11:16:23<3:14:00, 18.51s/it]

 77%|███████▋  | 2057/2685 [11:16:44<3:21:04, 19.21s/it]

 77%|███████▋  | 2058/2685 [11:17:00<3:10:57, 18.27s/it]

 77%|███████▋  | 2059/2685 [11:17:22<3:20:51, 19.25s/it]

 77%|███████▋  | 2060/2685 [11:17:41<3:18:49, 19.09s/it]

 77%|███████▋  | 2061/2685 [11:18:01<3:21:28, 19.37s/it]

 77%|███████▋  | 2062/2685 [11:18:21<3:25:21, 19.78s/it]

 77%|███████▋  | 2063/2685 [11:18:40<3:20:31, 19.34s/it]

 77%|███████▋  | 2064/2685 [11:19:01<3:25:29, 19.85s/it]

 77%|███████▋  | 2065/2685 [11:19:20<3:24:16, 19.77s/it]

 77%|███████▋  | 2066/2685 [11:19:39<3:19:05, 19.30s/it]


 77%|███████▋  | 2068/2685 [11:20:22<3:31:46, 20.59s/it]

 77%|███████▋  | 2069/2685 [11:20:43<3:33:32, 20.80s/it]

 77%|███████▋  | 2070/2685 [11:21:04<3:31:12, 20.61s/it]

 77%|███████▋  | 2071/2685 [11:21:25<3:31:40, 20.68s/it]

 77%|███████▋  | 2072/2685 [11:21:44<3:26:44, 20.24s/it]

 77%|███████▋  | 2073/2685 [11:21:58<3:09:10, 18.55s/it]
{'loss': 0.2247, 'learning_rate': 2.6041762426715563e-07, 'rewards/chosen': -1.2020708322525024, 'rewards/rejected': -3.097620964050293, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8955501317977905, 'policy_logps/rejected': -259.6402587890625, 'policy_logps/chosen': -304.2010803222656, 'referece_logps/rejected': -228.66404724121094, 'referece_logps/chosen': -292.18035888671875, 'logits/rejected': -0.588489294052124, 'logits/chosen': -0.39427345991134644, 'epoch': 2.32}


 77%|███████▋  | 2075/2685 [11:22:35<3:07:54, 18.48s/it]

 77%|███████▋  | 2076/2685 [11:22:55<3:13:08, 19.03s/it]

 77%|███████▋  | 2077/2685 [11:23:15<3:14:49, 19.23s/it]

 77%|███████▋  | 2078/2685 [11:23:35<3:16:39, 19.44s/it]

 77%|███████▋  | 2079/2685 [11:23:53<3:14:31, 19.26s/it]

 77%|███████▋  | 2080/2685 [11:24:13<3:14:26, 19.28s/it]

 78%|███████▊  | 2081/2685 [11:24:34<3:21:35, 20.03s/it]

 78%|███████▊  | 2082/2685 [11:24:56<3:24:59, 20.40s/it]

 78%|███████▊  | 2083/2685 [11:25:16<3:24:21, 20.37s/it]

 78%|███████▊  | 2084/2685 [11:25:36<3:22:54, 20.26s/it]

 78%|███████▊  | 2085/2685 [11:25:55<3:19:18, 19.93s/it]

 78%|███████▊  | 2086/2685 [11:26:15<3:18:01, 19.84s/it]

 78%|███████▊  | 2087/2685 [11:26:34<3:17:04, 19.77s/it]

 78%|███████▊  | 2088/2685 [11:26:57<3:24:02, 20.51s/it]

 78%|███████▊  | 2089/2685 [11:27:16<3:21:23, 20.27s/it]

 78%|███████▊  | 2090/2685 [11:27:38<3:26:01, 20.78s/it]

 78%|███████▊  | 2091/2685 [11:27:58<3:22:44, 20.48s/it]

 78%|███████▊  | 2092/2685 [11:28:11<3:00:33, 18.27s/it]

 78%|███████▊  | 2093/2685 [11:28:30<3:01:08, 18.36s/it]

 78%|███████▊  | 2094/2685 [11:28:47<2:58:22, 18.11s/it]

 78%|███████▊  | 2095/2685 [11:29:07<3:03:40, 18.68s/it]

 78%|███████▊  | 2096/2685 [11:29:25<3:00:12, 18.36s/it]

 78%|███████▊  | 2097/2685 [11:29:44<3:02:52, 18.66s/it]

 78%|███████▊  | 2098/2685 [11:30:04<3:06:08, 19.03s/it]

 78%|███████▊  | 2099/2685 [11:30:21<2:58:00, 18.23s/it]

 78%|███████▊  | 2100/2685 [11:30:42<3:06:18, 19.11s/it]

 78%|███████▊  | 2101/2685 [11:31:02<3:08:33, 19.37s/it]

 78%|███████▊  | 2102/2685 [11:31:22<3:11:37, 19.72s/it]

 78%|███████▊  | 2103/2685 [11:31:42<3:11:52, 19.78s/it]

 78%|███████▊  | 2104/2685 [11:32:02<3:13:03, 19.94s/it]

 78%|███████▊  | 2105/2685 [11:32:21<3:07:08, 19.36s/it]

 78%|███████▊  | 2106/2685 [11:32:42<3:12:23, 19.94s/it]

 78%|███████▊  | 2107/2685 [11:32:59<3:04:59, 19.20s/it]

 79%|███████▊  | 2108/2685 [11:33:19<3:06:06, 19.35s/it]

 79%|███████▊  | 2109/2685 [11:33:37<3:00:41, 18.82s/it]

 79%|███████▊  | 2110/2685 [11:33:56<3:02:59, 19.09s/it]

 79%|███████▊  | 2111/2685 [11:34:18<3:11:22, 20.00s/it]

 79%|███████▊  | 2112/2685 [11:34:39<3:13:47, 20.29s/it]

 79%|███████▊  | 2113/2685 [11:34:59<3:11:45, 20.11s/it]

 79%|███████▊  | 2114/2685 [11:35:19<3:10:08, 19.98s/it]

 79%|███████▉  | 2115/2685 [11:35:37<3:05:32, 19.53s/it]

 79%|███████▉  | 2116/2685 [11:35:57<3:06:42, 19.69s/it]

 79%|███████▉  | 2117/2685 [11:36:17<3:07:28, 19.80s/it]

 79%|███████▉  | 2118/2685 [11:36:34<2:59:15, 18.97s/it]

 79%|███████▉  | 2119/2685 [11:36:56<3:06:21, 19.75s/it]

 79%|███████▉  | 2120/2685 [11:37:13<2:58:15, 18.93s/it]

 79%|███████▉  | 2121/2685 [11:37:33<3:01:29, 19.31s/it]

 79%|███████▉  | 2122/2685 [11:37:53<3:02:58, 19.50s/it]

 79%|███████▉  | 2123/2685 [11:38:11<2:58:24, 19.05s/it]

 79%|███████▉  | 2124/2685 [11:38:30<2:56:45, 18.91s/it]

 79%|███████▉  | 2125/2685 [11:38:49<2:58:50, 19.16s/it]

 79%|███████▉  | 2126/2685 [11:39:09<3:00:28, 19.37s/it]

 79%|███████▉  | 2127/2685 [11:39:25<2:50:43, 18.36s/it]

 79%|███████▉  | 2128/2685 [11:39:43<2:47:53, 18.09s/it]

 79%|███████▉  | 2129/2685 [11:40:01<2:48:41, 18.20s/it]

 79%|███████▉  | 2130/2685 [11:40:22<2:54:48, 18.90s/it]

 79%|███████▉  | 2131/2685 [11:40:42<2:56:56, 19.16s/it]

 79%|███████▉  | 2132/2685 [11:41:01<2:58:13, 19.34s/it]

 79%|███████▉  | 2133/2685 [11:41:23<3:04:10, 20.02s/it]

 79%|███████▉  | 2134/2685 [11:41:45<3:09:11, 20.60s/it]

 80%|███████▉  | 2135/2685 [11:42:05<3:06:42, 20.37s/it]

 80%|███████▉  | 2136/2685 [11:42:26<3:08:01, 20.55s/it]

 80%|███████▉  | 2137/2685 [11:42:44<3:01:55, 19.92s/it]

 80%|███████▉  | 2138/2685 [11:43:03<2:59:38, 19.70s/it]

 80%|███████▉  | 2139/2685 [11:43:23<2:58:15, 19.59s/it]

 80%|███████▉  | 2140/2685 [11:43:43<3:01:13, 19.95s/it]

 80%|███████▉  | 2141/2685 [11:44:03<2:59:37, 19.81s/it]

 80%|███████▉  | 2142/2685 [11:44:23<2:58:51, 19.76s/it]

 80%|███████▉  | 2143/2685 [11:44:38<2:45:32, 18.33s/it]

 80%|███████▉  | 2144/2685 [11:44:59<2:54:38, 19.37s/it]

 80%|███████▉  | 2145/2685 [11:45:21<3:01:50, 20.21s/it]

 80%|███████▉  | 2146/2685 [11:45:42<3:02:12, 20.28s/it]

 80%|███████▉  | 2147/2685 [11:46:04<3:07:01, 20.86s/it]
{'loss': 0.2728, 'learning_rate': 2.0335338995693562e-07, 'rewards/chosen': -1.1175998449325562, 'rewards/rejected': -2.5115156173706055, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3939158916473389, 'policy_logps/rejected': -278.15972900390625, 'policy_logps/chosen': -269.6708679199219, 'referece_logps/rejected': -253.0445556640625, 'referece_logps/chosen': -258.494873046875, 'logits/rejected': -1.1224470138549805, 'logits/chosen': -1.1371279954910278, 'epoch': 2.4}


 80%|████████  | 2149/2685 [11:46:47<3:09:12, 21.18s/it]

 80%|████████  | 2150/2685 [11:47:06<3:02:56, 20.52s/it]
{'loss': 0.2217, 'learning_rate': 2.0117091555151666e-07, 'rewards/chosen': -1.531247854232788, 'rewards/rejected': -3.4227235317230225, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8914755582809448, 'policy_logps/rejected': -456.4233093261719, 'policy_logps/chosen': -337.58984375, 'referece_logps/rejected': -422.19610595703125, 'referece_logps/chosen': -322.2773742675781, 'logits/rejected': -0.07329833507537842, 'logits/chosen': -0.13522428274154663, 'epoch': 2.4}


 80%|████████  | 2152/2685 [11:47:46<2:56:28, 19.87s/it]

 80%|████████  | 2153/2685 [11:48:04<2:52:22, 19.44s/it]

 80%|████████  | 2154/2685 [11:48:24<2:52:30, 19.49s/it]

 80%|████████  | 2155/2685 [11:48:46<2:58:15, 20.18s/it]

 80%|████████  | 2156/2685 [11:49:05<2:56:25, 20.01s/it]
{'loss': 0.2783, 'learning_rate': 1.968373883468115e-07, 'rewards/chosen': -1.2961554527282715, 'rewards/rejected': -3.5426833629608154, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2465274333953857, 'policy_logps/rejected': -337.2496337890625, 'policy_logps/chosen': -366.546875, 'referece_logps/rejected': -301.8227844238281, 'referece_logps/chosen': -353.5853576660156, 'logits/rejected': -0.6101152300834656, 'logits/chosen': -0.574123740196228, 'epoch': 2.41}


 80%|████████  | 2158/2685 [11:49:47<2:57:25, 20.20s/it]
{'loss': 0.2854, 'learning_rate': 1.9540222023333163e-07, 'rewards/chosen': -1.4814504384994507, 'rewards/rejected': -3.4631364345550537, 'rewards/accuracies': 1.0, 'rewards/margins': 1.981685996055603, 'policy_logps/rejected': -259.71282958984375, 'policy_logps/chosen': -253.25750732421875, 'referece_logps/rejected': -225.08145141601562, 'referece_logps/chosen': -238.4429931640625, 'logits/rejected': -0.5879951119422913, 'logits/chosen': -0.4579359292984009, 'epoch': 2.41}


 80%|████████  | 2160/2685 [11:50:26<2:53:09, 19.79s/it]

 80%|████████  | 2161/2685 [11:50:42<2:43:53, 18.77s/it]

 81%|████████  | 2162/2685 [11:51:02<2:46:18, 19.08s/it]

 81%|████████  | 2163/2685 [11:51:18<2:38:22, 18.20s/it]
{'loss': 0.3742, 'learning_rate': 1.9183481252486767e-07, 'rewards/chosen': -1.551155686378479, 'rewards/rejected': -2.690340280532837, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1391844749450684, 'policy_logps/rejected': -562.5890502929688, 'policy_logps/chosen': -429.81005859375, 'referece_logps/rejected': -535.6856689453125, 'referece_logps/chosen': -414.2984924316406, 'logits/rejected': -0.3521255850791931, 'logits/chosen': -0.48317191004753113, 'epoch': 2.42}


 81%|████████  | 2165/2685 [11:52:00<2:49:53, 19.60s/it]

 81%|████████  | 2166/2685 [11:52:16<2:42:41, 18.81s/it]
{'loss': 0.2569, 'learning_rate': 1.897084751879765e-07, 'rewards/chosen': -0.6635926961898804, 'rewards/rejected': -4.851169586181641, 'rewards/accuracies': 0.75, 'rewards/margins': 4.187576770782471, 'policy_logps/rejected': -423.1917419433594, 'policy_logps/chosen': -394.6453857421875, 'referece_logps/rejected': -374.67999267578125, 'referece_logps/chosen': -388.0094299316406, 'logits/rejected': -1.296770691871643, 'logits/chosen': -1.326163649559021, 'epoch': 2.42}


 81%|████████  | 2168/2685 [11:52:50<2:31:42, 17.61s/it]

 81%|████████  | 2169/2685 [11:53:12<2:43:53, 19.06s/it]

 81%|████████  | 2170/2685 [11:53:33<2:47:05, 19.47s/it]

 81%|████████  | 2171/2685 [11:53:55<2:52:18, 20.11s/it]

 81%|████████  | 2172/2685 [11:54:14<2:50:54, 19.99s/it]

 81%|████████  | 2173/2685 [11:54:34<2:49:53, 19.91s/it]

 81%|████████  | 2174/2685 [11:54:52<2:45:29, 19.43s/it]

 81%|████████  | 2175/2685 [11:55:13<2:47:16, 19.68s/it]

 81%|████████  | 2176/2685 [11:55:34<2:51:40, 20.24s/it]

 81%|████████  | 2177/2685 [11:55:52<2:45:44, 19.57s/it]

 81%|████████  | 2178/2685 [11:56:12<2:45:58, 19.64s/it]

 81%|████████  | 2179/2685 [11:56:32<2:47:01, 19.80s/it]
{'loss': 0.3351, 'learning_rate': 1.8061735842275828e-07, 'rewards/chosen': -0.4325942099094391, 'rewards/rejected': -2.929255485534668, 'rewards/accuracies': 0.75, 'rewards/margins': 2.496661424636841, 'policy_logps/rejected': -363.9096374511719, 'policy_logps/chosen': -372.51788330078125, 'referece_logps/rejected': -334.6170959472656, 'referece_logps/chosen': -368.19195556640625, 'logits/rejected': -1.0651533603668213, 'logits/chosen': -1.0969231128692627, 'epoch': 2.43}


 81%|████████  | 2181/2685 [11:57:14<2:51:53, 20.46s/it]

 81%|████████▏ | 2182/2685 [11:57:35<2:51:52, 20.50s/it]

 81%|████████▏ | 2183/2685 [11:57:51<2:40:51, 19.23s/it]

 81%|████████▏ | 2184/2685 [11:58:11<2:41:57, 19.40s/it]

 81%|████████▏ | 2185/2685 [11:58:32<2:45:52, 19.91s/it]
{'loss': 0.197, 'learning_rate': 1.7648930317530864e-07, 'rewards/chosen': -1.8062455654144287, 'rewards/rejected': -3.930960178375244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1247146129608154, 'policy_logps/rejected': -348.52606201171875, 'policy_logps/chosen': -297.3183288574219, 'referece_logps/rejected': -309.2164611816406, 'referece_logps/chosen': -279.2558898925781, 'logits/rejected': -0.20300747454166412, 'logits/chosen': -0.17425058782100677, 'epoch': 2.44}

 81%|████████▏ | 2186/2685 [11:58:52<2:44:51, 19.82s/it]


 81%|████████▏ | 2188/2685 [11:59:33<2:47:16, 20.19s/it]

 82%|████████▏ | 2189/2685 [11:59:52<2:45:32, 20.03s/it]

 82%|████████▏ | 2190/2685 [12:00:05<2:26:46, 17.79s/it]

 82%|████████▏ | 2191/2685 [12:00:25<2:31:27, 18.40s/it]

 82%|████████▏ | 2192/2685 [12:00:42<2:29:05, 18.15s/it]

 82%|████████▏ | 2193/2685 [12:01:00<2:29:08, 18.19s/it]

 82%|████████▏ | 2194/2685 [12:01:21<2:35:40, 19.02s/it]
{'loss': 0.2324, 'learning_rate': 1.7037819480435224e-07, 'rewards/chosen': -0.9332489371299744, 'rewards/rejected': -5.169468879699707, 'rewards/accuracies': 1.0, 'rewards/margins': 4.236220359802246, 'policy_logps/rejected': -349.660400390625, 'policy_logps/chosen': -372.6942443847656, 'referece_logps/rejected': -297.9656982421875, 'referece_logps/chosen': -363.36175537109375, 'logits/rejected': -0.011678695678710938, 'logits/chosen': -0.008993387222290039, 'epoch': 2.45}


 82%|████████▏ | 2196/2685 [12:01:57<2:33:31, 18.84s/it]
{'loss': 0.2212, 'learning_rate': 1.690334282803353e-07, 'rewards/chosen': -1.5593509674072266, 'rewards/rejected': -4.240615367889404, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6812639236450195, 'policy_logps/rejected': -359.3343811035156, 'policy_logps/chosen': -270.73504638671875, 'referece_logps/rejected': -316.92822265625, 'referece_logps/chosen': -255.14154052734375, 'logits/rejected': -0.3300868570804596, 'logits/chosen': -0.29550355672836304, 'epoch': 2.45}


 82%|████████▏ | 2198/2685 [12:02:32<2:26:27, 18.04s/it]
{'loss': 0.2744, 'learning_rate': 1.676934997034647e-07, 'rewards/chosen': -1.2866199016571045, 'rewards/rejected': -3.2456488609313965, 'rewards/accuracies': 1.0, 'rewards/margins': 1.959029197692871, 'policy_logps/rejected': -385.0399475097656, 'policy_logps/chosen': -381.96173095703125, 'referece_logps/rejected': -352.5834655761719, 'referece_logps/chosen': -369.0955505371094, 'logits/rejected': -0.7333398461341858, 'logits/chosen': -0.7768118381500244, 'epoch': 2.46}

 82%|████████▏ | 2199/2685 [12:02:50<2:24:39, 17.86s/it]

 82%|████████▏ | 2200/2685 [12:03:12<2:34:49, 19.15s/it]


 82%|████████▏ | 2202/2685 [12:03:52<2:36:57, 19.50s/it]
{'loss': 0.3563, 'learning_rate': 1.6502818756759273e-07, 'rewards/chosen': -1.2095811367034912, 'rewards/rejected': -3.022895097732544, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8133143186569214, 'policy_logps/rejected': -223.97950744628906, 'policy_logps/chosen': -255.81199645996094, 'referece_logps/rejected': -193.7505645751953, 'referece_logps/chosen': -243.7161865234375, 'logits/rejected': -0.21783941984176636, 'logits/chosen': -0.18193094432353973, 'epoch': 2.46}


 82%|████████▏ | 2204/2685 [12:04:34<2:43:27, 20.39s/it]
{'loss': 0.2816, 'learning_rate': 1.637028195262301e-07, 'rewards/chosen': -0.1393735259771347, 'rewards/rejected': -2.883227586746216, 'rewards/accuracies': 0.75, 'rewards/margins': 2.74385404586792, 'policy_logps/rejected': -339.5791015625, 'policy_logps/chosen': -331.08563232421875, 'referece_logps/rejected': -310.746826171875, 'referece_logps/chosen': -329.69189453125, 'logits/rejected': -1.152280330657959, 'logits/chosen': -1.1406378746032715, 'epoch': 2.46}


 82%|████████▏ | 2206/2685 [12:05:11<2:33:43, 19.26s/it]

 82%|████████▏ | 2207/2685 [12:05:34<2:40:52, 20.19s/it]

 82%|████████▏ | 2208/2685 [12:05:52<2:35:45, 19.59s/it]

 82%|████████▏ | 2209/2685 [12:06:11<2:35:38, 19.62s/it]

 82%|████████▏ | 2210/2685 [12:06:32<2:36:54, 19.82s/it]

 82%|████████▏ | 2211/2685 [12:06:53<2:40:54, 20.37s/it]

 82%|████████▏ | 2212/2685 [12:07:16<2:44:48, 20.91s/it]

 82%|████████▏ | 2213/2685 [12:07:35<2:41:58, 20.59s/it]
{'loss': 0.2885, 'learning_rate': 1.577990277362491e-07, 'rewards/chosen': -0.24997851252555847, 'rewards/rejected': -3.436152219772339, 'rewards/accuracies': 1.0, 'rewards/margins': 3.186173677444458, 'policy_logps/rejected': -263.792236328125, 'policy_logps/chosen': -202.66444396972656, 'referece_logps/rejected': -229.43072509765625, 'referece_logps/chosen': -200.16464233398438, 'logits/rejected': -0.8757356405258179, 'logits/chosen': -1.0361534357070923, 'epoch': 2.47}


 82%|████████▏ | 2215/2685 [12:08:11<2:31:32, 19.35s/it]

 83%|████████▎ | 2216/2685 [12:08:30<2:30:36, 19.27s/it]

 83%|████████▎ | 2217/2685 [12:08:52<2:36:15, 20.03s/it]

 83%|████████▎ | 2218/2685 [12:09:12<2:36:19, 20.08s/it]

 83%|████████▎ | 2219/2685 [12:09:34<2:40:48, 20.70s/it]

 83%|████████▎ | 2220/2685 [12:09:54<2:37:56, 20.38s/it]

 83%|████████▎ | 2221/2685 [12:10:13<2:34:59, 20.04s/it]

 83%|████████▎ | 2222/2685 [12:10:35<2:40:14, 20.77s/it]
{'loss': 0.2275, 'learning_rate': 1.5199452795026335e-07, 'rewards/chosen': -1.12101411819458, 'rewards/rejected': -4.927732944488525, 'rewards/accuracies': 0.75, 'rewards/margins': 3.8067188262939453, 'policy_logps/rejected': -465.4500427246094, 'policy_logps/chosen': -426.3256530761719, 'referece_logps/rejected': -416.1726989746094, 'referece_logps/chosen': -415.1155090332031, 'logits/rejected': -0.9353170394897461, 'logits/chosen': -0.9999220371246338, 'epoch': 2.48}

 83%|████████▎ | 2223/2685 [12:10:55<2:36:48, 20.36s/it]


 83%|████████▎ | 2225/2685 [12:11:36<2:37:39, 20.57s/it]
{'loss': 0.2324, 'learning_rate': 1.5008187819546348e-07, 'rewards/chosen': -1.1809688806533813, 'rewards/rejected': -3.9849796295166016, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8040106296539307, 'policy_logps/rejected': -333.5582275390625, 'policy_logps/chosen': -331.423828125, 'referece_logps/rejected': -293.7084045410156, 'referece_logps/chosen': -319.6141052246094, 'logits/rejected': -0.3721727430820465, 'logits/chosen': -0.3266667425632477, 'epoch': 2.49}


 83%|████████▎ | 2227/2685 [12:12:15<2:33:30, 20.11s/it]

 83%|████████▎ | 2228/2685 [12:12:36<2:33:08, 20.11s/it]
{'loss': 0.2868, 'learning_rate': 1.4818036207388796e-07, 'rewards/chosen': -1.1090278625488281, 'rewards/rejected': -2.6231870651245117, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5141589641571045, 'policy_logps/rejected': -278.618896484375, 'policy_logps/chosen': -256.6434326171875, 'referece_logps/rejected': -252.3870391845703, 'referece_logps/chosen': -245.55313110351562, 'logits/rejected': -1.473082184791565, 'logits/chosen': -1.5863547325134277, 'epoch': 2.49}


 83%|████████▎ | 2230/2685 [12:13:09<2:18:42, 18.29s/it]
{'loss': 0.3587, 'learning_rate': 1.4691888231814842e-07, 'rewards/chosen': -1.272895336151123, 'rewards/rejected': -2.5096933841705322, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2367980480194092, 'policy_logps/rejected': -422.9349060058594, 'policy_logps/chosen': -480.7135009765625, 'referece_logps/rejected': -397.8380126953125, 'referece_logps/chosen': -467.9845275878906, 'logits/rejected': 0.2115127146244049, 'logits/chosen': 0.13335418701171875, 'epoch': 2.49}


 83%|████████▎ | 2232/2685 [12:13:48<2:19:46, 18.51s/it]

 83%|████████▎ | 2233/2685 [12:14:10<2:28:30, 19.71s/it]

 83%|████████▎ | 2234/2685 [12:14:29<2:26:44, 19.52s/it]

 83%|████████▎ | 2235/2685 [12:14:50<2:28:18, 19.77s/it]
{'loss': 0.2369, 'learning_rate': 1.4378692823441207e-07, 'rewards/chosen': -0.6366106867790222, 'rewards/rejected': -3.38694167137146, 'rewards/accuracies': 1.0, 'rewards/margins': 2.750330686569214, 'policy_logps/rejected': -286.4127502441406, 'policy_logps/chosen': -231.0177764892578, 'referece_logps/rejected': -252.5433349609375, 'referece_logps/chosen': -224.65167236328125, 'logits/rejected': -0.9470846652984619, 'logits/chosen': -0.9705102443695068, 'epoch': 2.5}

 83%|████████▎ | 2236/2685 [12:15:07<2:22:32, 19.05s/it]

 83%|████████▎ | 2237/2685 [12:15:23<2:14:58, 18.08s/it]


 83%|████████▎ | 2239/2685 [12:15:58<2:12:37, 17.84s/it]
{'loss': 0.3744, 'learning_rate': 1.413037916814036e-07, 'rewards/chosen': -2.1843276023864746, 'rewards/rejected': -4.520959854125977, 'rewards/accuracies': 0.875, 'rewards/margins': 2.336632251739502, 'policy_logps/rejected': -367.3891906738281, 'policy_logps/chosen': -340.32562255859375, 'referece_logps/rejected': -322.17962646484375, 'referece_logps/chosen': -318.48236083984375, 'logits/rejected': -0.4094542860984802, 'logits/chosen': -0.36356592178344727, 'epoch': 2.5}


 83%|████████▎ | 2241/2685 [12:16:36<2:14:07, 18.13s/it]
{'loss': 0.3169, 'learning_rate': 1.400697188845432e-07, 'rewards/chosen': -1.5617223978042603, 'rewards/rejected': -2.9652395248413086, 'rewards/accuracies': 0.5, 'rewards/margins': 1.403517246246338, 'policy_logps/rejected': -217.59783935546875, 'policy_logps/chosen': -273.45965576171875, 'referece_logps/rejected': -187.9454803466797, 'referece_logps/chosen': -257.8424377441406, 'logits/rejected': -1.3070546388626099, 'logits/chosen': -1.1771275997161865, 'epoch': 2.5}

 84%|████████▎ | 2242/2685 [12:16:53<2:10:37, 17.69s/it]

 84%|████████▎ | 2243/2685 [12:17:11<2:11:58, 17.91s/it]

 84%|████████▎ | 2244/2685 [12:17:31<2:15:51, 18.48s/it]


 84%|████████▎ | 2246/2685 [12:18:05<2:08:24, 17.55s/it]
{'loss': 0.3348, 'learning_rate': 1.3700645630729356e-07, 'rewards/chosen': -0.010787099599838257, 'rewards/rejected': -1.9258520603179932, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9150651693344116, 'policy_logps/rejected': -252.3431854248047, 'policy_logps/chosen': -361.13262939453125, 'referece_logps/rejected': -233.08465576171875, 'referece_logps/chosen': -361.0247497558594, 'logits/rejected': -0.23169556260108948, 'logits/chosen': -0.18729914724826813, 'epoch': 2.51}


 84%|████████▎ | 2248/2685 [12:18:40<2:07:57, 17.57s/it]

 84%|████████▍ | 2249/2685 [12:18:59<2:09:36, 17.84s/it]

 84%|████████▍ | 2250/2685 [12:19:16<2:08:43, 17.76s/it]

 84%|████████▍ | 2251/2685 [12:19:34<2:08:18, 17.74s/it]

 84%|████████▍ | 2252/2685 [12:19:55<2:14:31, 18.64s/it]
{'loss': 0.3687, 'learning_rate': 1.3337200222404778e-07, 'rewards/chosen': -0.24422837793827057, 'rewards/rejected': -3.0889501571655273, 'rewards/accuracies': 1.0, 'rewards/margins': 2.844721794128418, 'policy_logps/rejected': -268.0642395019531, 'policy_logps/chosen': -247.9138946533203, 'referece_logps/rejected': -237.17474365234375, 'referece_logps/chosen': -245.47161865234375, 'logits/rejected': -1.1265438795089722, 'logits/chosen': -1.2217378616333008, 'epoch': 2.52}


 84%|████████▍ | 2254/2685 [12:20:24<1:59:40, 16.66s/it]

 84%|████████▍ | 2255/2685 [12:20:44<2:06:25, 17.64s/it]
{'loss': 0.1596, 'learning_rate': 1.3157179210722714e-07, 'rewards/chosen': -1.1404964923858643, 'rewards/rejected': -4.274426460266113, 'rewards/accuracies': 1.0, 'rewards/margins': 3.13392972946167, 'policy_logps/rejected': -384.06036376953125, 'policy_logps/chosen': -326.8631591796875, 'referece_logps/rejected': -341.3161315917969, 'referece_logps/chosen': -315.458251953125, 'logits/rejected': -0.13346435129642487, 'logits/chosen': -0.16439178586006165, 'epoch': 2.52}

 84%|████████▍ | 2256/2685 [12:21:06<2:14:45, 18.85s/it]


 84%|████████▍ | 2258/2685 [12:21:48<2:22:25, 20.01s/it]
{'loss': 0.2143, 'learning_rate': 1.297829580993518e-07, 'rewards/chosen': -1.4088581800460815, 'rewards/rejected': -4.656312465667725, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2474541664123535, 'policy_logps/rejected': -323.4498596191406, 'policy_logps/chosen': -283.6429443359375, 'referece_logps/rejected': -276.8867492675781, 'referece_logps/chosen': -269.55438232421875, 'logits/rejected': -1.2884278297424316, 'logits/chosen': -1.256303310394287, 'epoch': 2.52}


 84%|████████▍ | 2260/2685 [12:22:28<2:22:39, 20.14s/it]
{'loss': 0.3214, 'learning_rate': 1.2859673373267332e-07, 'rewards/chosen': -1.327085018157959, 'rewards/rejected': -2.620394229888916, 'rewards/accuracies': 0.875, 'rewards/margins': 1.293309211730957, 'policy_logps/rejected': -212.65675354003906, 'policy_logps/chosen': -229.31295776367188, 'referece_logps/rejected': -186.4528045654297, 'referece_logps/chosen': -216.04209899902344, 'logits/rejected': -1.015641689300537, 'logits/chosen': -0.9275500774383545, 'epoch': 2.53}

 84%|████████▍ | 2261/2685 [12:22:48<2:20:20, 19.86s/it]


 84%|████████▍ | 2263/2685 [12:23:25<2:15:56, 19.33s/it]
{'loss': 0.2892, 'learning_rate': 1.268269119062131e-07, 'rewards/chosen': -1.886756181716919, 'rewards/rejected': -4.896040916442871, 'rewards/accuracies': 0.75, 'rewards/margins': 3.0092849731445312, 'policy_logps/rejected': -373.51611328125, 'policy_logps/chosen': -330.9532470703125, 'referece_logps/rejected': -324.55572509765625, 'referece_logps/chosen': -312.085693359375, 'logits/rejected': -0.30069878697395325, 'logits/chosen': -0.3238145709037781, 'epoch': 2.53}

 84%|████████▍ | 2264/2685 [12:23:45<2:18:04, 19.68s/it]

 84%|████████▍ | 2265/2685 [12:24:05<2:17:33, 19.65s/it]

 84%|████████▍ | 2266/2685 [12:24:25<2:18:44, 19.87s/it]

 84%|████████▍ | 2267/2685 [12:24:48<2:23:47, 20.64s/it]


 85%|████████▍ | 2269/2685 [12:25:29<2:22:50, 20.60s/it]

 85%|████████▍ | 2270/2685 [12:25:45<2:12:44, 19.19s/it]
{'loss': 0.2636, 'learning_rate': 1.2274184957765464e-07, 'rewards/chosen': -0.363421231508255, 'rewards/rejected': -2.2831764221191406, 'rewards/accuracies': 0.875, 'rewards/margins': 1.919755220413208, 'policy_logps/rejected': -293.015625, 'policy_logps/chosen': -310.69500732421875, 'referece_logps/rejected': -270.1838684082031, 'referece_logps/chosen': -307.060791015625, 'logits/rejected': -0.08720853924751282, 'logits/chosen': -0.056835342198610306, 'epoch': 2.54}


 85%|████████▍ | 2272/2685 [12:26:27<2:19:09, 20.22s/it]

 85%|████████▍ | 2273/2685 [12:26:47<2:18:13, 20.13s/it]
{'loss': 0.2729, 'learning_rate': 1.2101024461993802e-07, 'rewards/chosen': -1.3279942274093628, 'rewards/rejected': -2.2410032749176025, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9130092263221741, 'policy_logps/rejected': -187.9483642578125, 'policy_logps/chosen': -186.760986328125, 'referece_logps/rejected': -165.53834533691406, 'referece_logps/chosen': -173.48104858398438, 'logits/rejected': -1.5087131261825562, 'logits/chosen': -1.4846372604370117, 'epoch': 2.54}

 85%|████████▍ | 2274/2685 [12:27:09<2:22:31, 20.81s/it]

 85%|████████▍ | 2275/2685 [12:27:28<2:17:44, 20.16s/it]


 85%|████████▍ | 2277/2685 [12:28:07<2:14:36, 19.79s/it]

 85%|████████▍ | 2278/2685 [12:28:27<2:15:07, 19.92s/it]

 85%|████████▍ | 2279/2685 [12:28:47<2:14:31, 19.88s/it]

 85%|████████▍ | 2280/2685 [12:29:07<2:14:23, 19.91s/it]
{'loss': 0.365, 'learning_rate': 1.1701465043701808e-07, 'rewards/chosen': -2.7772414684295654, 'rewards/rejected': -4.1732869148254395, 'rewards/accuracies': 0.5, 'rewards/margins': 1.3960449695587158, 'policy_logps/rejected': -414.0924072265625, 'policy_logps/chosen': -360.311279296875, 'referece_logps/rejected': -372.3595886230469, 'referece_logps/chosen': -332.53887939453125, 'logits/rejected': -1.1017465591430664, 'logits/chosen': -1.0492908954620361, 'epoch': 2.55}

 85%|████████▍ | 2281/2685 [12:29:28<2:16:04, 20.21s/it]

 85%|████████▍ | 2282/2685 [12:29:49<2:18:37, 20.64s/it]


 85%|████████▌ | 2284/2685 [12:30:33<2:21:21, 21.15s/it]

 85%|████████▌ | 2285/2685 [12:30:55<2:22:47, 21.42s/it]
{'loss': 0.2621, 'learning_rate': 1.1419919397464717e-07, 'rewards/chosen': -1.6876193284988403, 'rewards/rejected': -4.743002414703369, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0553832054138184, 'policy_logps/rejected': -275.2259521484375, 'policy_logps/chosen': -257.3394775390625, 'referece_logps/rejected': -227.7958984375, 'referece_logps/chosen': -240.46331787109375, 'logits/rejected': -1.0766503810882568, 'logits/chosen': -1.0120269060134888, 'epoch': 2.55}

 85%|████████▌ | 2286/2685 [12:31:12<2:12:55, 19.99s/it]

 85%|████████▌ | 2287/2685 [12:31:32<2:12:40, 20.00s/it]

 85%|████████▌ | 2288/2685 [12:31:53<2:14:19, 20.30s/it]

 85%|████████▌ | 2289/2685 [12:32:13<2:13:15, 20.19s/it]


 85%|████████▌ | 2291/2685 [12:32:53<2:13:02, 20.26s/it]

 85%|████████▌ | 2292/2685 [12:33:11<2:07:40, 19.49s/it]

 85%|████████▌ | 2293/2685 [12:33:31<2:08:06, 19.61s/it]
{'loss': 0.2211, 'learning_rate': 1.0976154919164849e-07, 'rewards/chosen': -1.1090223789215088, 'rewards/rejected': -3.2447593212127686, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1357369422912598, 'policy_logps/rejected': -567.2852783203125, 'policy_logps/chosen': -448.818359375, 'referece_logps/rejected': -534.8375244140625, 'referece_logps/chosen': -437.7281494140625, 'logits/rejected': -0.1703168749809265, 'logits/chosen': -0.1118479073047638, 'epoch': 2.56}

 85%|████████▌ | 2294/2685 [12:33:52<2:11:01, 20.11s/it]

 85%|████████▌ | 2295/2685 [12:34:12<2:10:02, 20.01s/it]

 86%|████████▌ | 2296/2685 [12:34:32<2:09:07, 19.92s/it]

 86%|████████▌ | 2297/2685 [12:34:52<2:09:58, 20.10s/it]

 86%|████████▌ | 2298/2685 [12:35:13<2:09:51, 20.13s/it]

 86%|████████▌ | 2299/2685 [12:35:32<2:09:01, 20.06s/it]


 86%|████████▌ | 2301/2685 [12:36:15<2:13:29, 20.86s/it]

 86%|████████▌ | 2302/2685 [12:36:36<2:12:17, 20.72s/it]
{'loss': 0.2085, 'learning_rate': 1.0486834386974186e-07, 'rewards/chosen': -0.9061390161514282, 'rewards/rejected': -4.888932228088379, 'rewards/accuracies': 1.0, 'rewards/margins': 3.982792854309082, 'policy_logps/rejected': -489.24072265625, 'policy_logps/chosen': -414.6229553222656, 'referece_logps/rejected': -440.3514404296875, 'referece_logps/chosen': -405.5615234375, 'logits/rejected': -0.28145912289619446, 'logits/chosen': -0.3413533866405487, 'epoch': 2.57}

 86%|████████▌ | 2303/2685 [12:36:57<2:12:45, 20.85s/it]

 86%|████████▌ | 2304/2685 [12:37:12<2:02:26, 19.28s/it]

 86%|████████▌ | 2305/2685 [12:37:34<2:06:57, 20.05s/it]


 86%|████████▌ | 2307/2685 [12:38:07<1:53:21, 17.99s/it]
{'loss': 0.3019, 'learning_rate': 1.0219546042925841e-07, 'rewards/chosen': -0.7581992745399475, 'rewards/rejected': -4.570644855499268, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8124449253082275, 'policy_logps/rejected': -440.5019836425781, 'policy_logps/chosen': -347.533447265625, 'referece_logps/rejected': -394.7955627441406, 'referece_logps/chosen': -339.95147705078125, 'logits/rejected': -0.7423263192176819, 'logits/chosen': -0.736489474773407, 'epoch': 2.58}

 86%|████████▌ | 2308/2685 [12:38:27<1:55:09, 18.33s/it]


 86%|████████▌ | 2310/2685 [12:39:06<1:58:53, 19.02s/it]

 86%|████████▌ | 2311/2685 [12:39:26<2:00:08, 19.27s/it]

 86%|████████▌ | 2312/2685 [12:39:47<2:04:27, 20.02s/it]
{'loss': 0.2399, 'learning_rate': 9.955524617182631e-08, 'rewards/chosen': -1.0010058879852295, 'rewards/rejected': -2.924870252609253, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9238646030426025, 'policy_logps/rejected': -230.39804077148438, 'policy_logps/chosen': -229.5543212890625, 'referece_logps/rejected': -201.14932250976562, 'referece_logps/chosen': -219.54428100585938, 'logits/rejected': -1.609145998954773, 'logits/chosen': -1.619323492050171, 'epoch': 2.58}


 86%|████████▌ | 2314/2685 [12:40:27<2:03:33, 19.98s/it]

 86%|████████▌ | 2315/2685 [12:40:50<2:07:59, 20.75s/it]
{'loss': 0.3893, 'learning_rate': 9.798683884063263e-08, 'rewards/chosen': -1.3584160804748535, 'rewards/rejected': -4.784289360046387, 'rewards/accuracies': 0.875, 'rewards/margins': 3.425873279571533, 'policy_logps/rejected': -499.5224304199219, 'policy_logps/chosen': -554.7323608398438, 'referece_logps/rejected': -451.6795349121094, 'referece_logps/chosen': -541.148193359375, 'logits/rejected': -0.1813284158706665, 'logits/chosen': -0.11065315455198288, 'epoch': 2.59}

 86%|████████▋ | 2316/2685 [12:41:12<2:10:56, 21.29s/it]


 86%|████████▋ | 2318/2685 [12:41:50<2:01:49, 19.92s/it]
{'loss': 0.3526, 'learning_rate': 9.643024756964768e-08, 'rewards/chosen': -1.6715145111083984, 'rewards/rejected': -3.5009517669677734, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8294371366500854, 'policy_logps/rejected': -438.3268127441406, 'policy_logps/chosen': -489.8735046386719, 'referece_logps/rejected': -403.3172912597656, 'referece_logps/chosen': -473.1583557128906, 'logits/rejected': -1.1676980257034302, 'logits/chosen': -1.1319785118103027, 'epoch': 2.59}

 86%|████████▋ | 2319/2685 [12:42:07<1:55:54, 19.00s/it]

 86%|████████▋ | 2320/2685 [12:42:26<1:56:40, 19.18s/it]


 86%|████████▋ | 2322/2685 [12:43:08<2:01:00, 20.00s/it]

 87%|████████▋ | 2323/2685 [12:43:28<2:00:27, 19.97s/it]
{'loss': 0.2062, 'learning_rate': 9.386224200850624e-08, 'rewards/chosen': -0.5872151851654053, 'rewards/rejected': -3.3131942749023438, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7259790897369385, 'policy_logps/rejected': -465.2606201171875, 'policy_logps/chosen': -353.05352783203125, 'referece_logps/rejected': -432.128662109375, 'referece_logps/chosen': -347.181396484375, 'logits/rejected': -0.03876587748527527, 'logits/chosen': -0.09568674862384796, 'epoch': 2.6}

 87%|████████▋ | 2324/2685 [12:43:43<1:52:16, 18.66s/it]

 87%|████████▋ | 2325/2685 [12:44:01<1:50:32, 18.42s/it]


 87%|████████▋ | 2327/2685 [12:44:40<1:53:37, 19.04s/it]
{'loss': 0.3015, 'learning_rate': 9.183157325048396e-08, 'rewards/chosen': -0.6329116225242615, 'rewards/rejected': -2.851151704788208, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2182400226593018, 'policy_logps/rejected': -307.86651611328125, 'policy_logps/chosen': -319.77325439453125, 'referece_logps/rejected': -279.35491943359375, 'referece_logps/chosen': -313.4441223144531, 'logits/rejected': -0.3640792667865753, 'logits/chosen': -0.3367007374763489, 'epoch': 2.6}

 87%|████████▋ | 2328/2685 [12:44:59<1:53:04, 19.00s/it]

 87%|████████▋ | 2329/2685 [12:45:19<1:53:48, 19.18s/it]


 87%|████████▋ | 2331/2685 [12:45:58<1:54:41, 19.44s/it]

 87%|████████▋ | 2332/2685 [12:46:18<1:55:34, 19.64s/it]
{'loss': 0.2731, 'learning_rate': 8.932298450284903e-08, 'rewards/chosen': -1.4733904600143433, 'rewards/rejected': -4.0041823387146, 'rewards/accuracies': 0.875, 'rewards/margins': 2.530791997909546, 'policy_logps/rejected': -357.45745849609375, 'policy_logps/chosen': -415.30242919921875, 'referece_logps/rejected': -317.4156494140625, 'referece_logps/chosen': -400.5685729980469, 'logits/rejected': -0.07787847518920898, 'logits/chosen': -0.21799248456954956, 'epoch': 2.61}

 87%|████████▋ | 2333/2685 [12:46:35<1:50:46, 18.88s/it]

 87%|████████▋ | 2334/2685 [12:46:56<1:53:12, 19.35s/it]

 87%|████████▋ | 2335/2685 [12:47:15<1:53:14, 19.41s/it]

 87%|████████▋ | 2336/2685 [12:47:34<1:51:14, 19.13s/it]

 87%|████████▋ | 2337/2685 [12:47:53<1:51:26, 19.22s/it]

 87%|████████▋ | 2338/2685 [12:48:14<1:53:18, 19.59s/it]

 87%|████████▋ | 2339/2685 [12:48:35<1:55:54, 20.10s/it]

 87%|████████▋ | 2340/2685 [12:48:54<1:53:35, 19.75s/it]

 87%|████████▋ | 2341/2685 [12:49:12<1:49:51, 19.16s/it]

 87%|████████▋ | 2342/2685 [12:49:32<1:50:54, 19.40s/it]

 87%|████████▋ | 2343/2685 [12:49:52<1:51:31, 19.57s/it]

 87%|████████▋ | 2344/2685 [12:50:12<1:51:46, 19.67s/it]

 87%|████████▋ | 2345/2685 [12:50:34<1:55:32, 20.39s/it]

 87%|████████▋ | 2346/2685 [12:50:54<1:54:43, 20.31s/it]

 87%|████████▋ | 2347/2685 [12:51:16<1:57:23, 20.84s/it]

 87%|████████▋ | 2348/2685 [12:51:35<1:54:56, 20.46s/it]

 87%|████████▋ | 2349/2685 [12:51:55<1:53:11, 20.21s/it]

 88%|████████▊ | 2350/2685 [12:52:12<1:46:53, 19.15s/it]


 88%|████████▊ | 2352/2685 [12:52:51<1:48:04, 19.47s/it]

 88%|████████▊ | 2353/2685 [12:53:11<1:48:28, 19.60s/it]
{'loss': 0.179, 'learning_rate': 7.914981564811141e-08, 'rewards/chosen': -0.9717869162559509, 'rewards/rejected': -2.800140142440796, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8283530473709106, 'policy_logps/rejected': -382.3706970214844, 'policy_logps/chosen': -318.9989318847656, 'referece_logps/rejected': -354.3692932128906, 'referece_logps/chosen': -309.28106689453125, 'logits/rejected': -0.4198417663574219, 'logits/chosen': -0.4503783881664276, 'epoch': 2.63}

 88%|████████▊ | 2354/2685 [12:53:30<1:48:27, 19.66s/it]

 88%|████████▊ | 2355/2685 [12:53:50<1:48:59, 19.82s/it]

 88%|████████▊ | 2356/2685 [12:54:12<1:51:22, 20.31s/it]

 88%|████████▊ | 2357/2685 [12:54:34<1:54:05, 20.87s/it]

 88%|████████▊ | 2358/2685 [12:54:51<1:47:28, 19.72s/it]


 88%|████████▊ | 2360/2685 [12:55:29<1:44:28, 19.29s/it]
{'loss': 0.3192, 'learning_rate': 7.588979161090814e-08, 'rewards/chosen': -2.0191755294799805, 'rewards/rejected': -4.155341148376465, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1361656188964844, 'policy_logps/rejected': -379.4855651855469, 'policy_logps/chosen': -358.1497802734375, 'referece_logps/rejected': -337.93212890625, 'referece_logps/chosen': -337.9580078125, 'logits/rejected': -0.9639778137207031, 'logits/chosen': -0.9833621978759766, 'epoch': 2.64}


 88%|████████▊ | 2362/2685 [12:56:11<1:49:09, 20.28s/it]
{'loss': 0.2517, 'learning_rate': 7.497045158632553e-08, 'rewards/chosen': -0.8715944886207581, 'rewards/rejected': -4.996762752532959, 'rewards/accuracies': 1.0, 'rewards/margins': 4.125168800354004, 'policy_logps/rejected': -550.782958984375, 'policy_logps/chosen': -394.78955078125, 'referece_logps/rejected': -500.8153991699219, 'referece_logps/chosen': -386.0736083984375, 'logits/rejected': 0.17862200736999512, 'logits/chosen': 0.07260087132453918, 'epoch': 2.64}

 88%|████████▊ | 2363/2685 [12:56:29<1:46:03, 19.76s/it]

 88%|████████▊ | 2364/2685 [12:56:50<1:47:45, 20.14s/it]

 88%|████████▊ | 2365/2685 [12:57:10<1:47:24, 20.14s/it]


 88%|████████▊ | 2367/2685 [12:57:49<1:43:59, 19.62s/it]
{'loss': 0.238, 'learning_rate': 7.269567510617126e-08, 'rewards/chosen': -0.5694714188575745, 'rewards/rejected': -1.5990787744522095, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0296072959899902, 'policy_logps/rejected': -271.5751647949219, 'policy_logps/chosen': -243.16885375976562, 'referece_logps/rejected': -255.58438110351562, 'referece_logps/chosen': -237.47412109375, 'logits/rejected': -0.7530733346939087, 'logits/chosen': -0.7114210724830627, 'epoch': 2.64}


 88%|████████▊ | 2369/2685 [12:58:27<1:43:24, 19.64s/it]
{'loss': 0.2279, 'learning_rate': 7.179520785273941e-08, 'rewards/chosen': -0.9340280890464783, 'rewards/rejected': -3.4898266792297363, 'rewards/accuracies': 1.0, 'rewards/margins': 2.555798292160034, 'policy_logps/rejected': -445.1356201171875, 'policy_logps/chosen': -379.40533447265625, 'referece_logps/rejected': -410.23736572265625, 'referece_logps/chosen': -370.0650634765625, 'logits/rejected': -0.836312472820282, 'logits/chosen': -0.7968236804008484, 'epoch': 2.65}

 88%|████████▊ | 2370/2685 [12:58:46<1:41:27, 19.33s/it]

 88%|████████▊ | 2371/2685 [12:59:03<1:38:41, 18.86s/it]

 88%|████████▊ | 2372/2685 [12:59:23<1:39:18, 19.04s/it]

 88%|████████▊ | 2373/2685 [12:59:44<1:42:55, 19.79s/it]

 88%|████████▊ | 2374/2685 [13:00:05<1:44:26, 20.15s/it]

 88%|████████▊ | 2375/2685 [13:00:26<1:45:04, 20.34s/it]

 88%|████████▊ | 2376/2685 [13:00:46<1:44:03, 20.20s/it]

 89%|████████▊ | 2377/2685 [13:00:59<1:32:01, 17.93s/it]

 89%|████████▊ | 2378/2685 [13:01:17<1:31:56, 17.97s/it]

 89%|████████▊ | 2379/2685 [13:01:38<1:36:50, 18.99s/it]

 89%|████████▊ | 2380/2685 [13:01:58<1:38:26, 19.37s/it]

 89%|████████▊ | 2381/2685 [13:02:19<1:40:38, 19.86s/it]

 89%|████████▊ | 2382/2685 [13:02:40<1:41:00, 20.00s/it]

 89%|████████▉ | 2383/2685 [13:03:01<1:42:30, 20.36s/it]

 89%|████████▉ | 2384/2685 [13:03:21<1:41:55, 20.32s/it]

 89%|████████▉ | 2385/2685 [13:03:44<1:44:57, 20.99s/it]

 89%|████████▉ | 2386/2685 [13:04:01<1:38:57, 19.86s/it]

 89%|████████▉ | 2387/2685 [13:04:21<1:38:25, 19.82s/it]


 89%|████████▉ | 2389/2685 [13:04:59<1:36:31, 19.57s/it]
{'loss': 0.3162, 'learning_rate': 6.308860887523049e-08, 'rewards/chosen': -1.6103359460830688, 'rewards/rejected': -3.5350496768951416, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9247136116027832, 'policy_logps/rejected': -424.44793701171875, 'policy_logps/chosen': -404.486328125, 'referece_logps/rejected': -389.0974426269531, 'referece_logps/chosen': -388.3829345703125, 'logits/rejected': -0.6292978525161743, 'logits/chosen': -0.6859915256500244, 'epoch': 2.67}

 89%|████████▉ | 2390/2685 [13:05:19<1:36:22, 19.60s/it]

 89%|████████▉ | 2391/2685 [13:05:34<1:29:39, 18.30s/it]

 89%|████████▉ | 2392/2685 [13:05:55<1:33:10, 19.08s/it]

 89%|████████▉ | 2393/2685 [13:06:12<1:29:18, 18.35s/it]

 89%|████████▉ | 2394/2685 [13:06:32<1:32:15, 19.02s/it]

 89%|████████▉ | 2395/2685 [13:06:54<1:35:34, 19.77s/it]


 89%|████████▉ | 2397/2685 [13:07:30<1:29:09, 18.57s/it]
{'loss': 0.3372, 'learning_rate': 5.975842216860238e-08, 'rewards/chosen': 0.24105484783649445, 'rewards/rejected': -3.8236770629882812, 'rewards/accuracies': 0.875, 'rewards/margins': 4.064732551574707, 'policy_logps/rejected': -488.0602111816406, 'policy_logps/chosen': -364.8648376464844, 'referece_logps/rejected': -449.8234558105469, 'referece_logps/chosen': -367.275390625, 'logits/rejected': -0.22494859993457794, 'logits/chosen': -0.37890124320983887, 'epoch': 2.68}


 89%|████████▉ | 2399/2685 [13:08:06<1:26:07, 18.07s/it]
{'loss': 0.3056, 'learning_rate': 5.893954883885133e-08, 'rewards/chosen': -1.7903770208358765, 'rewards/rejected': -2.5273962020874023, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7370191812515259, 'policy_logps/rejected': -210.7917022705078, 'policy_logps/chosen': -212.5947265625, 'referece_logps/rejected': -185.51773071289062, 'referece_logps/chosen': -194.69097900390625, 'logits/rejected': -1.4105883836746216, 'logits/chosen': -1.418264389038086, 'epoch': 2.68}

 89%|████████▉ | 2400/2685 [13:08:26<1:28:38, 18.66s/it]

 89%|████████▉ | 2401/2685 [13:08:48<1:33:28, 19.75s/it]

 89%|████████▉ | 2402/2685 [13:09:02<1:24:50, 17.99s/it]

 89%|████████▉ | 2403/2685 [13:09:22<1:27:59, 18.72s/it]

 90%|████████▉ | 2404/2685 [13:09:42<1:29:18, 19.07s/it]

 90%|████████▉ | 2405/2685 [13:10:03<1:31:42, 19.65s/it]

 90%|████████▉ | 2406/2685 [13:10:21<1:28:32, 19.04s/it]

 90%|████████▉ | 2407/2685 [13:10:41<1:29:11, 19.25s/it]

 90%|████████▉ | 2408/2685 [13:11:01<1:30:44, 19.65s/it]

 90%|████████▉ | 2409/2685 [13:11:20<1:28:48, 19.31s/it]

 90%|████████▉ | 2410/2685 [13:11:40<1:29:12, 19.46s/it]

 90%|████████▉ | 2411/2685 [13:12:01<1:31:26, 20.02s/it]


 90%|████████▉ | 2413/2685 [13:12:42<1:31:29, 20.18s/it]

 90%|████████▉ | 2414/2685 [13:13:03<1:32:08, 20.40s/it]

 90%|████████▉ | 2415/2685 [13:13:22<1:30:06, 20.02s/it]

 90%|████████▉ | 2416/2685 [13:13:42<1:29:24, 19.94s/it]

 90%|█████████ | 2417/2685 [13:13:58<1:24:32, 18.93s/it]

 90%|█████████ | 2418/2685 [13:14:18<1:25:24, 19.19s/it]

 90%|█████████ | 2419/2685 [13:14:38<1:26:26, 19.50s/it]

 90%|█████████ | 2420/2685 [13:14:58<1:26:51, 19.67s/it]

 90%|█████████ | 2421/2685 [13:15:17<1:25:11, 19.36s/it]

 90%|█████████ | 2422/2685 [13:15:37<1:25:49, 19.58s/it]

 90%|█████████ | 2423/2685 [13:15:54<1:22:01, 18.78s/it]

 90%|█████████ | 2424/2685 [13:16:15<1:24:42, 19.47s/it]

 90%|█████████ | 2425/2685 [13:16:36<1:26:41, 20.00s/it]

 90%|█████████ | 2426/2685 [13:16:52<1:20:46, 18.71s/it]

 90%|█████████ | 2427/2685 [13:17:12<1:21:41, 19.00s/it]

 90%|█████████ | 2428/2685 [13:17:32<1:22:37, 19.29s/it]

 90%|█████████ | 2429/2685 [13:17:48<1:17:50, 18.25s/it]

 91%|█████████ | 2430/2685 [13:18:10<1:23:17, 19.60s/it]

 91%|█████████ | 2431/2685 [13:18:28<1:20:46, 19.08s/it]
{'loss': 0.3001, 'learning_rate': 4.6585852988708985e-08, 'rewards/chosen': -2.5334179401397705, 'rewards/rejected': -4.680994033813477, 'rewards/accuracies': 0.75, 'rewards/margins': 2.147576332092285, 'policy_logps/rejected': -523.5866088867188, 'policy_logps/chosen': -436.88482666015625, 'referece_logps/rejected': -476.776611328125, 'referece_logps/chosen': -411.55072021484375, 'logits/rejected': 0.08624309301376343, 'logits/chosen': 0.09269370138645172, 'epoch': 2.72}


 91%|█████████ | 2433/2685 [13:19:08<1:22:15, 19.59s/it]

 91%|█████████ | 2434/2685 [13:19:29<1:23:19, 19.92s/it]
{'loss': 0.2803, 'learning_rate': 4.5500271793248665e-08, 'rewards/chosen': -1.5660431385040283, 'rewards/rejected': -5.111093044281006, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5450501441955566, 'policy_logps/rejected': -348.8374328613281, 'policy_logps/chosen': -307.7190856933594, 'referece_logps/rejected': -297.72650146484375, 'referece_logps/chosen': -292.0586853027344, 'logits/rejected': -0.26192140579223633, 'logits/chosen': -0.2083764225244522, 'epoch': 2.72}

 91%|█████████ | 2435/2685 [13:19:51<1:24:53, 20.38s/it]


 91%|█████████ | 2437/2685 [13:20:33<1:26:06, 20.83s/it]

 91%|█████████ | 2438/2685 [13:20:55<1:27:10, 21.17s/it]
{'loss': 0.2474, 'learning_rate': 4.4072282703062515e-08, 'rewards/chosen': -2.020256519317627, 'rewards/rejected': -4.3575334548950195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3372769355773926, 'policy_logps/rejected': -288.45318603515625, 'policy_logps/chosen': -253.3861083984375, 'referece_logps/rejected': -244.8778533935547, 'referece_logps/chosen': -233.1835479736328, 'logits/rejected': -1.0556058883666992, 'logits/chosen': -1.1083364486694336, 'epoch': 2.72}


 91%|█████████ | 2440/2685 [13:21:35<1:24:31, 20.70s/it]

 91%|█████████ | 2441/2685 [13:21:56<1:24:18, 20.73s/it]

 91%|█████████ | 2442/2685 [13:22:14<1:21:07, 20.03s/it]

 91%|█████████ | 2443/2685 [13:22:34<1:20:56, 20.07s/it]

 91%|█████████ | 2444/2685 [13:22:54<1:19:58, 19.91s/it]

 91%|█████████ | 2445/2685 [13:23:14<1:20:25, 20.10s/it]

 91%|█████████ | 2446/2685 [13:23:36<1:21:31, 20.47s/it]

 91%|█████████ | 2447/2685 [13:23:52<1:16:47, 19.36s/it]

 91%|█████████ | 2448/2685 [13:24:14<1:19:05, 20.02s/it]

 91%|█████████ | 2449/2685 [13:24:33<1:17:55, 19.81s/it]

 91%|█████████ | 2450/2685 [13:24:56<1:20:29, 20.55s/it]

 91%|█████████▏| 2451/2685 [13:25:18<1:22:00, 21.03s/it]

 91%|█████████▏| 2452/2685 [13:25:35<1:16:49, 19.78s/it]

 91%|█████████▏| 2453/2685 [13:25:56<1:18:15, 20.24s/it]

 91%|█████████▏| 2454/2685 [13:26:15<1:17:07, 20.03s/it]

 91%|█████████▏| 2455/2685 [13:26:37<1:18:48, 20.56s/it]

 91%|█████████▏| 2456/2685 [13:26:58<1:19:08, 20.73s/it]

 92%|█████████▏| 2457/2685 [13:27:19<1:18:28, 20.65s/it]

 92%|█████████▏| 2458/2685 [13:27:39<1:17:06, 20.38s/it]
{'loss': 0.2376, 'learning_rate': 3.7266912507609935e-08, 'rewards/chosen': -2.8704028129577637, 'rewards/rejected': -5.308232307434082, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4378302097320557, 'policy_logps/rejected': -473.2144470214844, 'policy_logps/chosen': -445.6087951660156, 'referece_logps/rejected': -420.132080078125, 'referece_logps/chosen': -416.90472412109375, 'logits/rejected': -0.2974649965763092, 'logits/chosen': -0.3084881603717804, 'epoch': 2.75}

 92%|█████████▏| 2459/2685 [13:27:57<1:14:41, 19.83s/it]


 92%|█████████▏| 2461/2685 [13:28:34<1:11:02, 19.03s/it]

 92%|█████████▏| 2462/2685 [13:28:56<1:14:10, 19.96s/it]

 92%|█████████▏| 2463/2685 [13:29:16<1:13:25, 19.84s/it]

 92%|█████████▏| 2464/2685 [13:29:38<1:15:49, 20.59s/it]

 92%|█████████▏| 2465/2685 [13:29:58<1:15:08, 20.49s/it]

 92%|█████████▏| 2466/2685 [13:30:20<1:16:21, 20.92s/it]

 92%|█████████▏| 2467/2685 [13:30:40<1:14:31, 20.51s/it]

 92%|█████████▏| 2468/2685 [13:31:01<1:15:20, 20.83s/it]
{'loss': 0.3827, 'learning_rate': 3.4074173710931796e-08, 'rewards/chosen': -1.8672388792037964, 'rewards/rejected': -3.9323487281799316, 'rewards/accuracies': 0.75, 'rewards/margins': 2.065109968185425, 'policy_logps/rejected': -307.245361328125, 'policy_logps/chosen': -290.1269226074219, 'referece_logps/rejected': -267.921875, 'referece_logps/chosen': -271.4545593261719, 'logits/rejected': -0.5180467963218689, 'logits/chosen': -0.6320284008979797, 'epoch': 2.76}


 92%|█████████▏| 2470/2685 [13:31:46<1:16:53, 21.46s/it]
{'loss': 0.2768, 'learning_rate': 3.3452482313266475e-08, 'rewards/chosen': -0.8238581418991089, 'rewards/rejected': -4.318521022796631, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4946630001068115, 'policy_logps/rejected': -361.2071838378906, 'policy_logps/chosen': -308.8650207519531, 'referece_logps/rejected': -318.02197265625, 'referece_logps/chosen': -300.6264343261719, 'logits/rejected': -0.3213633894920349, 'logits/chosen': -0.36322227120399475, 'epoch': 2.76}


 92%|█████████▏| 2472/2685 [13:32:23<1:11:03, 20.02s/it]
{'loss': 0.1689, 'learning_rate': 3.283641822476646e-08, 'rewards/chosen': -1.6913433074951172, 'rewards/rejected': -4.684009552001953, 'rewards/accuracies': 0.875, 'rewards/margins': 2.992666244506836, 'policy_logps/rejected': -268.7688903808594, 'policy_logps/chosen': -312.44085693359375, 'referece_logps/rejected': -221.9287872314453, 'referece_logps/chosen': -295.52740478515625, 'logits/rejected': -0.9600626230239868, 'logits/chosen': -0.8785262703895569, 'epoch': 2.76}


 92%|█████████▏| 2474/2685 [13:33:07<1:13:10, 20.81s/it]

 92%|█████████▏| 2475/2685 [13:33:26<1:11:40, 20.48s/it]

 92%|█████████▏| 2476/2685 [13:33:46<1:10:28, 20.23s/it]

 92%|█████████▏| 2477/2685 [13:34:05<1:09:16, 19.98s/it]

 92%|█████████▏| 2478/2685 [13:34:27<1:10:38, 20.48s/it]

 92%|█████████▏| 2479/2685 [13:34:44<1:07:05, 19.54s/it]

 92%|█████████▏| 2480/2685 [13:35:04<1:06:47, 19.55s/it]
{'loss': 0.1464, 'learning_rate': 3.042850620593118e-08, 'rewards/chosen': -0.08509010076522827, 'rewards/rejected': -4.005459308624268, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9203691482543945, 'policy_logps/rejected': -329.1535339355469, 'policy_logps/chosen': -318.26666259765625, 'referece_logps/rejected': -289.09893798828125, 'referece_logps/chosen': -317.415771484375, 'logits/rejected': -1.0318838357925415, 'logits/chosen': -1.106627106666565, 'epoch': 2.77}


 92%|█████████▏| 2482/2685 [13:35:39<1:02:24, 18.44s/it]

 92%|█████████▏| 2483/2685 [13:35:59<1:03:40, 18.91s/it]

 93%|█████████▎| 2484/2685 [13:36:20<1:05:35, 19.58s/it]

 93%|█████████▎| 2485/2685 [13:36:37<1:01:42, 18.51s/it]

 93%|█████████▎| 2486/2685 [13:36:56<1:02:37, 18.88s/it]

 93%|█████████▎| 2487/2685 [13:37:13<59:57, 18.17s/it]

 93%|█████████▎| 2488/2685 [13:37:32<1:01:10, 18.63s/it]

 93%|█████████▎| 2489/2685 [13:37:54<1:04:11, 19.65s/it]

 93%|█████████▎| 2490/2685 [13:38:15<1:04:42, 19.91s/it]

 93%|█████████▎| 2491/2685 [13:38:35<1:04:39, 20.00s/it]

 93%|█████████▎| 2492/2685 [13:38:56<1:05:32, 20.37s/it]

 93%|█████████▎| 2493/2685 [13:39:13<1:01:13, 19.13s/it]

 93%|█████████▎| 2494/2685 [13:39:32<1:01:06, 19.19s/it]
{'loss': 0.314, 'learning_rate': 2.6432118005738413e-08, 'rewards/chosen': -1.1167839765548706, 'rewards/rejected': -3.828843593597412, 'rewards/accuracies': 0.875, 'rewards/margins': 2.71205997467041, 'policy_logps/rejected': -282.5675048828125, 'policy_logps/chosen': -330.54327392578125, 'referece_logps/rejected': -244.27906799316406, 'referece_logps/chosen': -319.3753967285156, 'logits/rejected': -0.22182339429855347, 'logits/chosen': -0.28071579337120056, 'epoch': 2.79}


 93%|█████████▎| 2496/2685 [13:40:06<56:22, 17.90s/it]

 93%|█████████▎| 2497/2685 [13:40:23<55:09, 17.60s/it]

 93%|█████████▎| 2498/2685 [13:40:46<59:29, 19.09s/it]

 93%|█████████▎| 2499/2685 [13:41:06<1:00:20, 19.47s/it]

 93%|█████████▎| 2500/2685 [13:41:25<59:57, 19.44s/it]
{'loss': 0.2553, 'learning_rate': 2.4804337284212717e-08, 'rewards/chosen': -1.9797929525375366, 'rewards/rejected': -5.83050012588501, 'rewards/accuracies': 1.0, 'rewards/margins': 3.850707530975342, 'policy_logps/rejected': -421.73114013671875, 'policy_logps/chosen': -408.42681884765625, 'referece_logps/rejected': -363.4260559082031, 'referece_logps/chosen': -388.62890625, 'logits/rejected': 0.43511658906936646, 'logits/chosen': 0.4994240999221802, 'epoch': 2.79}


 93%|█████████▎| 2502/2685 [13:42:17<1:06:54, 21.94s/it]

 93%|█████████▎| 2503/2685 [13:42:36<1:04:26, 21.24s/it]

 93%|█████████▎| 2504/2685 [13:42:57<1:03:43, 21.12s/it]
{'loss': 0.3017, 'learning_rate': 2.374753316790701e-08, 'rewards/chosen': -2.6731514930725098, 'rewards/rejected': -4.808435440063477, 'rewards/accuracies': 0.625, 'rewards/margins': 2.1352832317352295, 'policy_logps/rejected': -395.39306640625, 'policy_logps/chosen': -404.8109436035156, 'referece_logps/rejected': -347.3087158203125, 'referece_logps/chosen': -378.0794372558594, 'logits/rejected': -0.2678529620170593, 'logits/chosen': -0.1990429311990738, 'epoch': 2.8}


 93%|█████████▎| 2506/2685 [13:43:38<1:02:13, 20.86s/it]

 93%|█████████▎| 2507/2685 [13:43:53<56:36, 19.08s/it]

 93%|█████████▎| 2508/2685 [13:44:15<58:51, 19.95s/it]

 93%|█████████▎| 2509/2685 [13:44:35<58:27, 19.93s/it]

 93%|█████████▎| 2510/2685 [13:44:52<55:36, 19.07s/it]

 94%|█████████▎| 2511/2685 [13:45:07<52:27, 18.09s/it]

 94%|█████████▎| 2512/2685 [13:45:28<54:05, 18.76s/it]

 94%|█████████▎| 2513/2685 [13:45:46<52:53, 18.45s/it]

 94%|█████████▎| 2514/2685 [13:46:00<48:56, 17.17s/it]
{'loss': 0.2483, 'learning_rate': 2.1205041977433557e-08, 'rewards/chosen': -0.8287062048912048, 'rewards/rejected': -3.302185535430908, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4734792709350586, 'policy_logps/rejected': -379.996826171875, 'policy_logps/chosen': -371.8328857421875, 'referece_logps/rejected': -346.9749450683594, 'referece_logps/chosen': -363.5458679199219, 'logits/rejected': -0.47663456201553345, 'logits/chosen': -0.3993576765060425, 'epoch': 2.81}


 94%|█████████▎| 2516/2685 [13:46:34<48:50, 17.34s/it]

 94%|█████████▎| 2517/2685 [13:46:51<47:59, 17.14s/it]

 94%|█████████▍| 2518/2685 [13:47:08<47:43, 17.15s/it]

 94%|█████████▍| 2519/2685 [13:47:31<52:39, 19.03s/it]

 94%|█████████▍| 2520/2685 [13:47:51<53:09, 19.33s/it]
{'loss': 0.2729, 'learning_rate': 1.9747907020775888e-08, 'rewards/chosen': -1.6108366250991821, 'rewards/rejected': -3.4549641609191895, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8441276550292969, 'policy_logps/rejected': -366.8426818847656, 'policy_logps/chosen': -329.03887939453125, 'referece_logps/rejected': -332.29302978515625, 'referece_logps/chosen': -312.9305114746094, 'logits/rejected': 0.20372426509857178, 'logits/chosen': 0.025378763675689697, 'epoch': 2.82}


 94%|█████████▍| 2522/2685 [13:48:33<54:25, 20.03s/it]

 94%|█████████▍| 2523/2685 [13:48:54<55:00, 20.37s/it]

 94%|█████████▍| 2524/2685 [13:49:15<55:04, 20.53s/it]
{'loss': 0.316, 'learning_rate': 1.8805014494173156e-08, 'rewards/chosen': -1.3043285608291626, 'rewards/rejected': -5.1831841468811035, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8788559436798096, 'policy_logps/rejected': -417.72216796875, 'policy_logps/chosen': -423.2384033203125, 'referece_logps/rejected': -365.8903503417969, 'referece_logps/chosen': -410.19512939453125, 'logits/rejected': -0.109429270029068, 'logits/chosen': 0.021525435149669647, 'epoch': 2.82}


 94%|█████████▍| 2526/2685 [13:49:56<54:21, 20.51s/it]

 94%|█████████▍| 2527/2685 [13:50:14<51:31, 19.57s/it]

 94%|█████████▍| 2528/2685 [13:50:33<51:15, 19.59s/it]

 94%|█████████▍| 2529/2685 [13:50:54<52:07, 20.05s/it]

 94%|█████████▍| 2530/2685 [13:51:11<49:29, 19.16s/it]

 94%|█████████▍| 2531/2685 [13:51:34<51:28, 20.05s/it]
{'loss': 0.2649, 'learning_rate': 1.720994905914208e-08, 'rewards/chosen': -1.6797574758529663, 'rewards/rejected': -3.2382843494415283, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5585267543792725, 'policy_logps/rejected': -332.4705505371094, 'policy_logps/chosen': -325.7945251464844, 'referece_logps/rejected': -300.08770751953125, 'referece_logps/chosen': -308.9969177246094, 'logits/rejected': -0.969524085521698, 'logits/chosen': -0.9634164571762085, 'epoch': 2.83}

 94%|█████████▍| 2532/2685 [13:51:55<51:58, 20.38s/it]

 94%|█████████▍| 2533/2685 [13:52:11<48:30, 19.15s/it]


 94%|█████████▍| 2535/2685 [13:52:44<44:21, 17.74s/it]

 94%|█████████▍| 2536/2685 [13:53:02<44:35, 17.95s/it]
{'loss': 0.3178, 'learning_rate': 1.611352399350607e-08, 'rewards/chosen': -1.4723868370056152, 'rewards/rejected': -3.1135263442993164, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6411396265029907, 'policy_logps/rejected': -382.8256530761719, 'policy_logps/chosen': -367.2601318359375, 'referece_logps/rejected': -351.6903991699219, 'referece_logps/chosen': -352.5362243652344, 'logits/rejected': -1.0845677852630615, 'logits/chosen': -1.1062891483306885, 'epoch': 2.83}


 95%|█████████▍| 2538/2685 [13:53:46<48:53, 19.96s/it]
{'loss': 0.3271, 'learning_rate': 1.5684976202465783e-08, 'rewards/chosen': -1.390993595123291, 'rewards/rejected': -3.5068318843841553, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1158382892608643, 'policy_logps/rejected': -338.6517639160156, 'policy_logps/chosen': -306.4574279785156, 'referece_logps/rejected': -303.58343505859375, 'referece_logps/chosen': -292.5474853515625, 'logits/rejected': -0.933603823184967, 'logits/chosen': -0.9138663411140442, 'epoch': 2.84}


 95%|█████████▍| 2540/2685 [13:54:26<48:23, 20.03s/it]
{'loss': 0.2878, 'learning_rate': 1.5262159164289302e-08, 'rewards/chosen': -1.8885445594787598, 'rewards/rejected': -4.694955348968506, 'rewards/accuracies': 1.0, 'rewards/margins': 2.806411027908325, 'policy_logps/rejected': -446.09918212890625, 'policy_logps/chosen': -421.30072021484375, 'referece_logps/rejected': -399.1496276855469, 'referece_logps/chosen': -402.4152526855469, 'logits/rejected': -0.3011075258255005, 'logits/chosen': -0.3210815191268921, 'epoch': 2.84}

 95%|█████████▍| 2541/2685 [13:54:48<48:48, 20.34s/it]

 95%|█████████▍| 2542/2685 [13:55:05<46:14, 19.40s/it]

 95%|█████████▍| 2543/2685 [13:55:25<46:28, 19.64s/it]

 95%|█████████▍| 2544/2685 [13:55:45<46:20, 19.72s/it]


 95%|█████████▍| 2546/2685 [13:56:28<48:10, 20.80s/it]

 95%|█████████▍| 2547/2685 [13:56:48<47:22, 20.60s/it]

 95%|█████████▍| 2548/2685 [13:57:08<46:38, 20.42s/it]

 95%|█████████▍| 2549/2685 [13:57:28<45:49, 20.22s/it]

 95%|█████████▍| 2550/2685 [13:57:46<44:21, 19.72s/it]

 95%|█████████▌| 2551/2685 [13:58:04<42:57, 19.23s/it]
{'loss': 0.3199, 'learning_rate': 1.3039210989107763e-08, 'rewards/chosen': -0.9293931722640991, 'rewards/rejected': -2.6289501190185547, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6995571851730347, 'policy_logps/rejected': -373.3426818847656, 'policy_logps/chosen': -294.6261291503906, 'referece_logps/rejected': -347.0531311035156, 'referece_logps/chosen': -285.3321838378906, 'logits/rejected': -0.22367802262306213, 'logits/chosen': -0.19590593874454498, 'epoch': 2.85}

 95%|█████████▌| 2552/2685 [13:58:24<42:38, 19.24s/it]

 95%|█████████▌| 2553/2685 [13:58:44<42:45, 19.43s/it]


 95%|█████████▌| 2555/2685 [13:59:20<40:48, 18.84s/it]
{'loss': 0.3801, 'learning_rate': 1.227394212293953e-08, 'rewards/chosen': -1.4432240724563599, 'rewards/rejected': -3.9276459217071533, 'rewards/accuracies': 0.75, 'rewards/margins': 2.484421968460083, 'policy_logps/rejected': -317.5305480957031, 'policy_logps/chosen': -265.4090576171875, 'referece_logps/rejected': -278.25408935546875, 'referece_logps/chosen': -250.976806640625, 'logits/rejected': -0.5296354293823242, 'logits/chosen': -0.5352095365524292, 'epoch': 2.85}

 95%|█████████▌| 2556/2685 [13:59:41<41:47, 19.44s/it]

 95%|█████████▌| 2557/2685 [14:00:03<43:04, 20.20s/it]


 95%|█████████▌| 2559/2685 [14:00:43<42:03, 20.03s/it]
{'loss': 0.3444, 'learning_rate': 1.1531675671888619e-08, 'rewards/chosen': -1.9293224811553955, 'rewards/rejected': -4.43696403503418, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5076420307159424, 'policy_logps/rejected': -347.123779296875, 'policy_logps/chosen': -353.645751953125, 'referece_logps/rejected': -302.7541198730469, 'referece_logps/chosen': -334.3525390625, 'logits/rejected': -0.07635672390460968, 'logits/chosen': -0.11588926613330841, 'epoch': 2.86}


 95%|█████████▌| 2561/2685 [14:01:17<37:49, 18.30s/it]
{'loss': 0.311, 'learning_rate': 1.1169173774871477e-08, 'rewards/chosen': -1.0768938064575195, 'rewards/rejected': -2.699321746826172, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6224279403686523, 'policy_logps/rejected': -253.56600952148438, 'policy_logps/chosen': -269.8526306152344, 'referece_logps/rejected': -226.57276916503906, 'referece_logps/chosen': -259.0837097167969, 'logits/rejected': -0.9510674476623535, 'logits/chosen': -0.9103933572769165, 'epoch': 2.86}

 95%|█████████▌| 2562/2685 [14:01:37<38:58, 19.01s/it]

 95%|█████████▌| 2563/2685 [14:01:55<38:11, 18.78s/it]

 95%|█████████▌| 2564/2685 [14:02:16<38:52, 19.28s/it]


 96%|█████████▌| 2566/2685 [14:02:53<37:28, 18.89s/it]

 96%|█████████▌| 2567/2685 [14:03:12<37:40, 19.16s/it]

 96%|█████████▌| 2568/2685 [14:03:29<35:41, 18.30s/it]

 96%|█████████▌| 2569/2685 [14:03:49<36:19, 18.79s/it]
{'loss': 0.2808, 'learning_rate': 9.776757230985144e-09, 'rewards/chosen': -0.9141308069229126, 'rewards/rejected': -4.214318752288818, 'rewards/accuracies': 0.875, 'rewards/margins': 3.300187826156616, 'policy_logps/rejected': -539.5908813476562, 'policy_logps/chosen': -480.9488525390625, 'referece_logps/rejected': -497.44769287109375, 'referece_logps/chosen': -471.80755615234375, 'logits/rejected': 0.06627124547958374, 'logits/chosen': 0.2097930908203125, 'epoch': 2.87}

 96%|█████████▌| 2570/2685 [14:04:02<32:39, 17.04s/it]


 96%|█████████▌| 2572/2685 [14:04:35<32:30, 17.26s/it]

 96%|█████████▌| 2573/2685 [14:04:51<31:16, 16.75s/it]

 96%|█████████▌| 2574/2685 [14:05:09<31:51, 17.22s/it]

 96%|█████████▌| 2575/2685 [14:05:29<33:02, 18.02s/it]

 96%|█████████▌| 2576/2685 [14:05:50<34:38, 19.07s/it]

 96%|█████████▌| 2577/2685 [14:06:09<34:11, 19.00s/it]

 96%|█████████▌| 2578/2685 [14:06:25<32:17, 18.11s/it]

 96%|█████████▌| 2579/2685 [14:06:44<32:34, 18.44s/it]

 96%|█████████▌| 2580/2685 [14:07:05<33:25, 19.10s/it]

 96%|█████████▌| 2581/2685 [14:07:23<32:18, 18.64s/it]

 96%|█████████▌| 2582/2685 [14:07:43<32:47, 19.10s/it]

 96%|█████████▌| 2583/2685 [14:08:03<32:53, 19.35s/it]

 96%|█████████▌| 2584/2685 [14:08:25<33:57, 20.17s/it]

 96%|█████████▋| 2585/2685 [14:08:47<34:25, 20.66s/it]
{'loss': 0.3288, 'learning_rate': 7.2687704889036195e-09, 'rewards/chosen': -0.5461475253105164, 'rewards/rejected': -3.8185646533966064, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2724170684814453, 'policy_logps/rejected': -268.765625, 'policy_logps/chosen': -289.1533508300781, 'referece_logps/rejected': -230.57997131347656, 'referece_logps/chosen': -283.6918640136719, 'logits/rejected': -0.25565534830093384, 'logits/chosen': -0.1508839875459671, 'epoch': 2.89}


 96%|█████████▋| 2587/2685 [14:09:27<33:19, 20.40s/it]

 96%|█████████▋| 2588/2685 [14:09:45<31:56, 19.76s/it]

 96%|█████████▋| 2589/2685 [14:10:05<31:20, 19.58s/it]
{'loss': 0.2264, 'learning_rate': 6.699536085739588e-09, 'rewards/chosen': -1.9128789901733398, 'rewards/rejected': -3.353372812271118, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4404937028884888, 'policy_logps/rejected': -274.8695983886719, 'policy_logps/chosen': -243.4925537109375, 'referece_logps/rejected': -241.33587646484375, 'referece_logps/chosen': -224.36376953125, 'logits/rejected': -0.5908068418502808, 'logits/chosen': -0.4839861989021301, 'epoch': 2.89}

 96%|█████████▋| 2590/2685 [14:10:27<32:06, 20.28s/it]


 97%|█████████▋| 2592/2685 [14:11:05<30:26, 19.63s/it]

 97%|█████████▋| 2593/2685 [14:11:21<28:41, 18.71s/it]
{'loss': 0.3113, 'learning_rate': 6.1534339158754165e-09, 'rewards/chosen': -1.5263538360595703, 'rewards/rejected': -3.623812675476074, 'rewards/accuracies': 0.75, 'rewards/margins': 2.097458839416504, 'policy_logps/rejected': -376.4376220703125, 'policy_logps/chosen': -351.7064208984375, 'referece_logps/rejected': -340.19952392578125, 'referece_logps/chosen': -336.4429016113281, 'logits/rejected': -1.7219802141189575, 'logits/chosen': -1.8439679145812988, 'epoch': 2.9}


 97%|█████████▋| 2595/2685 [14:11:57<27:17, 18.19s/it]

 97%|█████████▋| 2596/2685 [14:12:19<28:29, 19.21s/it]
{'loss': 0.3346, 'learning_rate': 5.759045487373937e-09, 'rewards/chosen': -0.6722801923751831, 'rewards/rejected': -1.533703088760376, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8614230155944824, 'policy_logps/rejected': -381.69036865234375, 'policy_logps/chosen': -356.1932373046875, 'referece_logps/rejected': -366.3533020019531, 'referece_logps/chosen': -349.470458984375, 'logits/rejected': -1.5412662029266357, 'logits/chosen': -1.61208176612854, 'epoch': 2.9}


 97%|█████████▋| 2598/2685 [14:12:57<27:51, 19.21s/it]
{'loss': 0.2911, 'learning_rate': 5.503355230027429e-09, 'rewards/chosen': -1.291501760482788, 'rewards/rejected': -3.7190675735473633, 'rewards/accuracies': 1.0, 'rewards/margins': 2.427565336227417, 'policy_logps/rejected': -392.39471435546875, 'policy_logps/chosen': -354.89501953125, 'referece_logps/rejected': -355.20404052734375, 'referece_logps/chosen': -341.97998046875, 'logits/rejected': -0.5907719731330872, 'logits/chosen': -0.6407297253608704, 'epoch': 2.9}


 97%|█████████▋| 2600/2685 [14:13:34<26:05, 18.42s/it]
{'loss': 0.2868, 'learning_rate': 5.253455003674135e-09, 'rewards/chosen': -1.3506762981414795, 'rewards/rejected': -4.432892799377441, 'rewards/accuracies': 0.875, 'rewards/margins': 3.082216262817383, 'policy_logps/rejected': -421.4671630859375, 'policy_logps/chosen': -360.2644958496094, 'referece_logps/rejected': -377.13824462890625, 'referece_logps/chosen': -346.75775146484375, 'logits/rejected': -1.6102648973464966, 'logits/chosen': -1.575422763824463, 'epoch': 2.91}

 97%|█████████▋| 2601/2685 [14:13:54<26:42, 19.08s/it]


 97%|█████████▋| 2603/2685 [14:14:38<28:12, 20.64s/it]
{'loss': 0.2932, 'learning_rate': 4.889464145711896e-09, 'rewards/chosen': -0.48977357149124146, 'rewards/rejected': -4.596359729766846, 'rewards/accuracies': 1.0, 'rewards/margins': 4.106585502624512, 'policy_logps/rejected': -307.4040222167969, 'policy_logps/chosen': -313.72564697265625, 'referece_logps/rejected': -261.4404296875, 'referece_logps/chosen': -308.82794189453125, 'logits/rejected': -0.1859319806098938, 'logits/chosen': -0.19106779992580414, 'epoch': 2.91}


 97%|█████████▋| 2605/2685 [14:15:19<27:34, 20.69s/it]
{'loss': 0.218, 'learning_rate': 4.654045288430386e-09, 'rewards/chosen': -1.5770444869995117, 'rewards/rejected': -4.741672039031982, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1646273136138916, 'policy_logps/rejected': -464.71954345703125, 'policy_logps/chosen': -254.76559448242188, 'referece_logps/rejected': -417.30279541015625, 'referece_logps/chosen': -238.9951629638672, 'logits/rejected': -0.5575507283210754, 'logits/chosen': -0.5421963334083557, 'epoch': 2.91}

 97%|█████████▋| 2606/2685 [14:15:40<27:11, 20.65s/it]

 97%|█████████▋| 2607/2685 [14:16:00<26:35, 20.46s/it]

 97%|█████████▋| 2608/2685 [14:16:20<26:13, 20.43s/it]

 97%|█████████▋| 2609/2685 [14:16:41<25:50, 20.41s/it]

 97%|█████████▋| 2610/2685 [14:17:02<26:01, 20.82s/it]

 97%|█████████▋| 2611/2685 [14:17:23<25:28, 20.65s/it]


 97%|█████████▋| 2613/2685 [14:18:04<24:34, 20.47s/it]

 97%|█████████▋| 2614/2685 [14:18:21<23:15, 19.65s/it]
{'loss': 0.2745, 'learning_rate': 3.6663922521791292e-09, 'rewards/chosen': -1.9135615825653076, 'rewards/rejected': -4.409947395324707, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4963855743408203, 'policy_logps/rejected': -448.8922119140625, 'policy_logps/chosen': -465.41729736328125, 'referece_logps/rejected': -404.7926940917969, 'referece_logps/chosen': -446.2817077636719, 'logits/rejected': -0.12941347062587738, 'logits/chosen': -0.20920680463314056, 'epoch': 2.92}

 97%|█████████▋| 2615/2685 [14:18:41<22:46, 19.52s/it]

 97%|█████████▋| 2616/2685 [14:19:00<22:28, 19.54s/it]


 98%|█████████▊| 2618/2685 [14:19:41<22:25, 20.08s/it]

 98%|█████████▊| 2619/2685 [14:20:02<22:07, 20.12s/it]

 98%|█████████▊| 2620/2685 [14:20:22<21:51, 20.17s/it]
{'loss': 0.3003, 'learning_rate': 3.0732078339301692e-09, 'rewards/chosen': -1.9274479150772095, 'rewards/rejected': -4.831834316253662, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9043867588043213, 'policy_logps/rejected': -414.9481201171875, 'policy_logps/chosen': -359.6588439941406, 'referece_logps/rejected': -366.6297912597656, 'referece_logps/chosen': -340.3843994140625, 'logits/rejected': -0.09830936789512634, 'logits/chosen': -0.004829332232475281, 'epoch': 2.93}


 98%|█████████▊| 2622/2685 [14:21:03<21:26, 20.43s/it]
{'loss': 0.3027, 'learning_rate': 2.887086552352591e-09, 'rewards/chosen': -1.3245306015014648, 'rewards/rejected': -4.024281024932861, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6997504234313965, 'policy_logps/rejected': -417.43121337890625, 'policy_logps/chosen': -407.4587097167969, 'referece_logps/rejected': -377.18841552734375, 'referece_logps/chosen': -394.2134094238281, 'logits/rejected': -0.4423554241657257, 'logits/chosen': -0.4505006968975067, 'epoch': 2.93}


 98%|█████████▊| 2624/2685 [14:21:46<21:04, 20.73s/it]

 98%|█████████▊| 2625/2685 [14:21:59<18:37, 18.63s/it]
{'loss': 0.242, 'learning_rate': 2.618789827681378e-09, 'rewards/chosen': -0.8120490908622742, 'rewards/rejected': -5.025465965270996, 'rewards/accuracies': 1.0, 'rewards/margins': 4.213417053222656, 'policy_logps/rejected': -400.0749206542969, 'policy_logps/chosen': -286.5198974609375, 'referece_logps/rejected': -349.8202819824219, 'referece_logps/chosen': -278.3994140625, 'logits/rejected': -0.2839924395084381, 'logits/chosen': -0.2077048420906067, 'epoch': 2.93}

 98%|█████████▊| 2626/2685 [14:22:19<18:37, 18.95s/it]


 98%|█████████▊| 2628/2685 [14:23:00<18:34, 19.55s/it]
{'loss': 0.3512, 'learning_rate': 2.363558451652592e-09, 'rewards/chosen': -1.0863815546035767, 'rewards/rejected': -5.159081935882568, 'rewards/accuracies': 0.875, 'rewards/margins': 4.072700500488281, 'policy_logps/rejected': -431.0113830566406, 'policy_logps/chosen': -344.4447021484375, 'referece_logps/rejected': -379.4205627441406, 'referece_logps/chosen': -333.58087158203125, 'logits/rejected': -0.007653027772903442, 'logits/chosen': 0.002336174249649048, 'epoch': 2.94}


 98%|█████████▊| 2630/2685 [14:23:38<17:56, 19.56s/it]
{'loss': 0.2919, 'learning_rate': 2.2006643861178785e-09, 'rewards/chosen': -1.711276650428772, 'rewards/rejected': -4.076083660125732, 'rewards/accuracies': 1.0, 'rewards/margins': 2.364806890487671, 'policy_logps/rejected': -369.6976318359375, 'policy_logps/chosen': -304.7608642578125, 'referece_logps/rejected': -328.936767578125, 'referece_logps/chosen': -287.64813232421875, 'logits/rejected': -0.20873773097991943, 'logits/chosen': -0.21763861179351807, 'epoch': 2.94}

 98%|█████████▊| 2631/2685 [14:23:57<17:20, 19.27s/it]

 98%|█████████▊| 2632/2685 [14:24:17<17:10, 19.44s/it]

 98%|█████████▊| 2633/2685 [14:24:37<17:02, 19.67s/it]

 98%|█████████▊| 2634/2685 [14:24:57<16:42, 19.67s/it]

 98%|█████████▊| 2635/2685 [14:25:18<16:50, 20.20s/it]


 98%|█████████▊| 2637/2685 [14:26:00<16:40, 20.84s/it]
{'loss': 0.2405, 'learning_rate': 1.6762889938303215e-09, 'rewards/chosen': -1.0348299741744995, 'rewards/rejected': -3.6527647972106934, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6179354190826416, 'policy_logps/rejected': -553.656005859375, 'policy_logps/chosen': -427.96429443359375, 'referece_logps/rejected': -517.1282958984375, 'referece_logps/chosen': -417.6159973144531, 'logits/rejected': -0.41023391485214233, 'logits/chosen': -0.393606573343277, 'epoch': 2.95}

 98%|█████████▊| 2638/2685 [14:26:22<16:28, 21.02s/it]


 98%|█████████▊| 2640/2685 [14:27:04<15:56, 21.27s/it]
{'loss': 0.2473, 'learning_rate': 1.4733507346396112e-09, 'rewards/chosen': -0.7387109994888306, 'rewards/rejected': -4.626203536987305, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8874921798706055, 'policy_logps/rejected': -417.0363464355469, 'policy_logps/chosen': -427.28643798828125, 'referece_logps/rejected': -370.7743225097656, 'referece_logps/chosen': -419.8993835449219, 'logits/rejected': -0.012434111908078194, 'logits/chosen': 0.16492366790771484, 'epoch': 2.95}

 98%|█████████▊| 2641/2685 [14:27:25<15:27, 21.08s/it]

 98%|█████████▊| 2642/2685 [14:27:44<14:39, 20.45s/it]

 98%|█████████▊| 2643/2685 [14:28:04<14:11, 20.28s/it]

 98%|█████████▊| 2644/2685 [14:28:22<13:26, 19.66s/it]

 99%|█████████▊| 2645/2685 [14:28:39<12:38, 18.96s/it]

 99%|█████████▊| 2646/2685 [14:29:02<12:56, 19.92s/it]

 99%|█████████▊| 2647/2685 [14:29:24<13:04, 20.65s/it]

 99%|█████████▊| 2648/2685 [14:29:44<12:33, 20.36s/it]

 99%|█████████▊| 2649/2685 [14:30:03<12:07, 20.21s/it]

 99%|█████████▊| 2650/2685 [14:30:25<12:06, 20.74s/it]

 99%|█████████▊| 2651/2685 [14:30:48<12:04, 21.32s/it]


 99%|█████████▉| 2653/2685 [14:31:27<10:42, 20.08s/it]

 99%|█████████▉| 2654/2685 [14:31:49<10:40, 20.65s/it]
{'loss': 0.2503, 'learning_rate': 6.992952116013917e-10, 'rewards/chosen': -1.4904451370239258, 'rewards/rejected': -4.74212121963501, 'rewards/accuracies': 1.0, 'rewards/margins': 3.251676559448242, 'policy_logps/rejected': -325.91912841796875, 'policy_logps/chosen': -287.87103271484375, 'referece_logps/rejected': -278.4979248046875, 'referece_logps/chosen': -272.9665832519531, 'logits/rejected': -0.520145058631897, 'logits/chosen': -0.6630293130874634, 'epoch': 2.97}

 99%|█████████▉| 2655/2685 [14:32:10<10:27, 20.91s/it]

 99%|█████████▉| 2656/2685 [14:32:30<09:58, 20.65s/it]


 99%|█████████▉| 2658/2685 [14:33:11<09:14, 20.52s/it]
{'loss': 0.2449, 'learning_rate': 5.30489651562438e-10, 'rewards/chosen': -2.4626047611236572, 'rewards/rejected': -4.517275810241699, 'rewards/accuracies': 0.875, 'rewards/margins': 2.054670810699463, 'policy_logps/rejected': -445.8255615234375, 'policy_logps/chosen': -428.3556823730469, 'referece_logps/rejected': -400.65283203125, 'referece_logps/chosen': -403.7296447753906, 'logits/rejected': -1.013911485671997, 'logits/chosen': -0.9888559579849243, 'epoch': 2.97}


 99%|█████████▉| 2660/2685 [14:33:53<08:43, 20.94s/it]
{'loss': 0.2608, 'learning_rate': 4.5481510937395163e-10, 'rewards/chosen': -1.73529851436615, 'rewards/rejected': -3.8240010738372803, 'rewards/accuracies': 0.875, 'rewards/margins': 2.08870267868042, 'policy_logps/rejected': -428.84912109375, 'policy_logps/chosen': -416.9991760253906, 'referece_logps/rejected': -390.60906982421875, 'referece_logps/chosen': -399.6462097167969, 'logits/rejected': 0.013863623142242432, 'logits/chosen': -0.0047597140073776245, 'epoch': 2.97}

 99%|█████████▉| 2661/2685 [14:34:12<08:11, 20.48s/it]


 99%|█████████▉| 2663/2685 [14:34:57<07:49, 21.35s/it]
{'loss': 0.24, 'learning_rate': 3.522148444283779e-10, 'rewards/chosen': -1.4586997032165527, 'rewards/rejected': -3.284266948699951, 'rewards/accuracies': 1.0, 'rewards/margins': 1.825567364692688, 'policy_logps/rejected': -224.38131713867188, 'policy_logps/chosen': -176.29859924316406, 'referece_logps/rejected': -191.5386505126953, 'referece_logps/chosen': -161.7115936279297, 'logits/rejected': -1.4080311059951782, 'logits/chosen': -1.33954656124115, 'epoch': 2.98}


 99%|█████████▉| 2665/2685 [14:35:39<07:07, 21.39s/it]

 99%|█████████▉| 2666/2685 [14:35:59<06:36, 20.85s/it]

 99%|█████████▉| 2667/2685 [14:36:15<05:50, 19.45s/it]
{'loss': 0.3052, 'learning_rate': 2.357847609686381e-10, 'rewards/chosen': -1.2113080024719238, 'rewards/rejected': -3.0814239978790283, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8701157569885254, 'policy_logps/rejected': -306.1562194824219, 'policy_logps/chosen': -309.17999267578125, 'referece_logps/rejected': -275.34197998046875, 'referece_logps/chosen': -297.06689453125, 'logits/rejected': -0.7693280577659607, 'logits/chosen': -0.9513231515884399, 'epoch': 2.98}

 99%|█████████▉| 2668/2685 [14:36:39<05:52, 20.71s/it]

 99%|█████████▉| 2669/2685 [14:36:54<05:04, 19.03s/it]

 99%|█████████▉| 2670/2685 [14:37:06<04:16, 17.12s/it]

 99%|█████████▉| 2671/2685 [14:37:26<04:11, 17.93s/it]

100%|█████████▉| 2672/2685 [14:37:46<03:59, 18.44s/it]

100%|█████████▉| 2673/2685 [14:38:06<03:46, 18.86s/it]


100%|█████████▉| 2675/2685 [14:38:37<02:53, 17.32s/it]
{'loss': 0.2309, 'learning_rate': 7.277505163139075e-11, 'rewards/chosen': -0.7237758636474609, 'rewards/rejected': -5.201717853546143, 'rewards/accuracies': 0.875, 'rewards/margins': 4.477941513061523, 'policy_logps/rejected': -316.6159362792969, 'policy_logps/chosen': -296.4000244140625, 'referece_logps/rejected': -264.5987548828125, 'referece_logps/chosen': -289.1622619628906, 'logits/rejected': 0.11577251553535461, 'logits/chosen': 0.12503546476364136, 'epoch': 2.99}

100%|█████████▉| 2676/2685 [14:38:57<02:42, 18.04s/it]

100%|█████████▉| 2677/2685 [14:39:16<02:28, 18.55s/it]

100%|█████████▉| 2678/2685 [14:39:34<02:08, 18.30s/it]

100%|█████████▉| 2679/2685 [14:39:56<01:55, 19.24s/it]

100%|█████████▉| 2680/2685 [14:40:14<01:35, 19.09s/it]

100%|█████████▉| 2681/2685 [14:40:35<01:18, 19.54s/it]


100%|█████████▉| 2683/2685 [14:41:09<00:37, 18.56s/it]

100%|█████████▉| 2684/2685 [14:41:29<00:19, 19.00s/it]
{'loss': 0.2801, 'learning_rate': 7.277592551924172e-13, 'rewards/chosen': -1.2625479698181152, 'rewards/rejected': -3.779670476913452, 'rewards/accuracies': 0.875, 'rewards/margins': 2.517122268676758, 'policy_logps/rejected': -334.3053283691406, 'policy_logps/chosen': -359.8292541503906, 'referece_logps/rejected': -296.5086364746094, 'referece_logps/chosen': -347.20379638671875, 'logits/rejected': -1.200434684753418, 'logits/chosen': -1.318329930305481, 'epoch': 3.0}
{'loss': 0.2489, 'learning_rate': 0.0, 'rewards/chosen': -1.0467811822891235, 'rewards/rejected': -4.891552925109863, 'rewards/accuracies': 0.875, 'rewards/margins': 3.844771385192871, 'policy_logps/rejected': -394.09027099609375, 'policy_logps/chosen': -319.896484375, 'referece_logps/rejected': -345.1747741699219, 'referece_logps/chosen': -309.42864990234375, 'logits/rejected': -1.107479453086853, 'logits/chosen': -1.2069123983383179, 'epoch': 3.0}

100%|██████████| 2685/2685 [14:41:49<00:00, 19.71s/it]