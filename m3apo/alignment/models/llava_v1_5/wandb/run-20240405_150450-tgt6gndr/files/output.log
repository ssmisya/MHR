
  0%|          | 0/24156 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/code/mm/MAPO/m3apo/alignment/trainer/llava_dpo_trainer.py:179: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
 66%|██████▌   | 16001/24156 [00:20<00:10, 773.57it/s]
{'loss': 0.4549, 'learning_rate': 2.758620689655172e-09, 'rewards/chosen': -1.0949358940124512, 'rewards/rejected': -2.508345603942871, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4134095907211304, 'policy_logps/rejected': -268.66278076171875, 'policy_logps/chosen': -395.771240234375, 'referece_logps/rejected': -243.5792999267578, 'referece_logps/chosen': -384.8218994140625, 'logits/rejected': -0.11889974772930145, 'logits/chosen': -0.1440412849187851, 'epoch': 5.96}

 66%|██████▌   | 16002/24156 [00:40<00:10, 773.57it/s]


 66%|██████▋   | 16004/24156 [01:05<00:47, 171.17it/s]

 66%|██████▋   | 16005/24156 [01:26<01:18, 104.07it/s]

 66%|██████▋   | 16006/24156 [01:48<02:05, 65.11it/s]

 66%|██████▋   | 16007/24156 [02:06<02:58, 45.59it/s]
{'loss': 0.4326, 'learning_rate': 1.9310344827586208e-08, 'rewards/chosen': -1.057114839553833, 'rewards/rejected': -2.3265469074249268, 'rewards/accuracies': 0.5, 'rewards/margins': 1.2694323062896729, 'policy_logps/rejected': -448.03564453125, 'policy_logps/chosen': -482.33001708984375, 'referece_logps/rejected': -424.77020263671875, 'referece_logps/chosen': -471.7588806152344, 'logits/rejected': 0.35183078050613403, 'logits/chosen': 0.32122576236724854, 'epoch': 5.96}


 66%|██████▋   | 16009/24156 [02:42<06:06, 22.21it/s]

 66%|██████▋   | 16010/24156 [02:59<08:33, 15.87it/s]
[2024-04-05 15:07:56,039] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16011/24156 [03:19<12:51, 10.55it/s]

 66%|██████▋   | 16012/24156 [03:34<17:14,  7.87it/s]

 66%|██████▋   | 16013/24156 [03:47<22:47,  5.95it/s]

 66%|██████▋   | 16014/24156 [03:58<29:20,  4.62it/s]

 66%|██████▋   | 16015/24156 [04:10<39:00,  3.48it/s]

 66%|██████▋   | 16016/24156 [04:26<58:47,  2.31it/s]

 66%|██████▋   | 16017/24156 [04:43<1:27:13,  1.56it/s]
[2024-04-05 15:09:40,899] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16018/24156 [05:01<2:08:10,  1.06it/s]

 66%|██████▋   | 16019/24156 [05:14<2:46:58,  1.23s/it]
{'loss': 0.3379, 'learning_rate': 5.241379310344828e-08, 'rewards/chosen': -1.1754001379013062, 'rewards/rejected': -3.186720132827759, 'rewards/accuracies': 0.875, 'rewards/margins': 2.011319875717163, 'policy_logps/rejected': -275.52569580078125, 'policy_logps/chosen': -464.3783874511719, 'referece_logps/rejected': -243.6584930419922, 'referece_logps/chosen': -452.6243896484375, 'logits/rejected': -1.1121529340744019, 'logits/chosen': -1.0956965684890747, 'epoch': 5.97}


 66%|██████▋   | 16021/24156 [05:48<5:48:17,  2.57s/it]

 66%|██████▋   | 16022/24156 [06:03<7:35:52,  3.36s/it]

 66%|██████▋   | 16023/24156 [06:17<9:37:04,  4.26s/it]
{'loss': 0.4466, 'learning_rate': 6.344827586206897e-08, 'rewards/chosen': -1.623444676399231, 'rewards/rejected': -3.112849712371826, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4894049167633057, 'policy_logps/rejected': -382.7940673828125, 'policy_logps/chosen': -589.4769897460938, 'referece_logps/rejected': -351.66552734375, 'referece_logps/chosen': -573.2425537109375, 'logits/rejected': -0.06699153780937195, 'logits/chosen': -0.0578702837228775, 'epoch': 5.97}

 66%|██████▋   | 16024/24156 [06:28<11:07:53,  4.93s/it]

 66%|██████▋   | 16025/24156 [06:44<14:21:47,  6.36s/it]


 66%|██████▋   | 16027/24156 [07:11<19:29:53,  8.63s/it]

 66%|██████▋   | 16028/24156 [07:22<20:28:55,  9.07s/it]

 66%|██████▋   | 16029/24156 [07:43<26:49:56, 11.89s/it]
[2024-04-05 15:12:40,694] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16030/24156 [08:04<31:30:32, 13.96s/it]
{'loss': 0.5158, 'learning_rate': 8.275862068965517e-08, 'rewards/chosen': -1.2663599252700806, 'rewards/rejected': -3.001185417175293, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7348254919052124, 'policy_logps/rejected': -481.7930908203125, 'policy_logps/chosen': -336.37371826171875, 'referece_logps/rejected': -451.78125, 'referece_logps/chosen': -323.7100830078125, 'logits/rejected': -0.6796603798866272, 'logits/chosen': -0.6844040155410767, 'epoch': 5.97}


 66%|██████▋   | 16032/24156 [08:37<34:08:39, 15.13s/it]
[2024-04-05 15:13:34,689] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16033/24156 [08:50<32:45:03, 14.51s/it]

 66%|██████▋   | 16034/24156 [09:12<37:29:24, 16.62s/it]

 66%|██████▋   | 16035/24156 [09:33<40:14:31, 17.84s/it]

 66%|██████▋   | 16036/24156 [09:48<38:10:24, 16.92s/it]

 66%|██████▋   | 16037/24156 [10:05<38:48:56, 17.21s/it]

 66%|██████▋   | 16038/24156 [10:23<39:04:03, 17.32s/it]

 66%|██████▋   | 16039/24156 [10:43<40:49:13, 18.10s/it]

 66%|██████▋   | 16040/24156 [11:06<43:51:35, 19.45s/it]
[2024-04-05 15:16:03,132] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16041/24156 [11:19<39:55:40, 17.71s/it]

 66%|██████▋   | 16042/24156 [11:37<40:11:51, 17.83s/it]

 66%|██████▋   | 16043/24156 [11:49<36:06:54, 16.03s/it]

 66%|██████▋   | 16044/24156 [12:00<32:45:14, 14.54s/it]

 66%|██████▋   | 16045/24156 [12:22<37:18:36, 16.56s/it]
[2024-04-05 15:17:19,008] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16046/24156 [12:43<40:43:29, 18.08s/it]

 66%|██████▋   | 16047/24156 [12:58<38:26:52, 17.07s/it]
[2024-04-05 15:17:55,345] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3941, 'learning_rate': 1.2965517241379309e-07, 'rewards/chosen': -1.5507326126098633, 'rewards/rejected': -2.905388355255127, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3546556234359741, 'policy_logps/rejected': -303.9856262207031, 'policy_logps/chosen': -256.81805419921875, 'referece_logps/rejected': -274.93170166015625, 'referece_logps/chosen': -241.31069946289062, 'logits/rejected': -0.46475180983543396, 'logits/chosen': -0.4038163125514984, 'epoch': 5.98}


 66%|██████▋   | 16049/24156 [13:37<40:42:31, 18.08s/it]

 66%|██████▋   | 16050/24156 [13:54<40:04:19, 17.80s/it]

 66%|██████▋   | 16051/24156 [14:09<38:12:52, 16.97s/it]

 66%|██████▋   | 16052/24156 [14:20<33:57:44, 15.09s/it]

 66%|██████▋   | 16053/24156 [14:39<36:42:39, 16.31s/it]

 66%|██████▋   | 16054/24156 [15:00<39:38:44, 17.62s/it]
[2024-04-05 15:19:57,329] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 16055/24156 [15:18<40:11:04, 17.86s/it]
{'loss': 0.393, 'learning_rate': 1.5172413793103449e-07, 'rewards/chosen': -1.1317671537399292, 'rewards/rejected': -2.459522247314453, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3277549743652344, 'policy_logps/rejected': -433.6939392089844, 'policy_logps/chosen': -378.3753662109375, 'referece_logps/rejected': -409.0987243652344, 'referece_logps/chosen': -367.05767822265625, 'logits/rejected': 0.028510525822639465, 'logits/chosen': 0.10395270586013794, 'epoch': 5.98}
[2024-04-05 15:20:36,686] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 66%|██████▋   | 16057/24156 [15:54<39:52:32, 17.72s/it]

 66%|██████▋   | 16058/24156 [16:12<39:40:30, 17.64s/it]

 66%|██████▋   | 16059/24156 [16:33<41:40:48, 18.53s/it]

 66%|██████▋   | 16060/24156 [16:46<37:56:59, 16.87s/it]

 66%|██████▋   | 16061/24156 [17:05<39:58:22, 17.78s/it]

 66%|██████▋   | 16062/24156 [17:24<40:19:24, 17.93s/it]

 66%|██████▋   | 16063/24156 [17:43<41:20:54, 18.39s/it]

 67%|██████▋   | 16064/24156 [18:05<43:45:02, 19.46s/it]
[2024-04-05 15:23:02,617] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16065/24156 [18:17<38:33:39, 17.16s/it]
[2024-04-05 15:23:14,390] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16066/24156 [18:28<34:12:16, 15.22s/it]

 67%|██████▋   | 16067/24156 [18:38<31:11:09, 13.88s/it]

 67%|██████▋   | 16068/24156 [18:49<29:02:04, 12.92s/it]

 67%|██████▋   | 16069/24156 [19:07<32:21:51, 14.41s/it]

 67%|██████▋   | 16070/24156 [19:28<36:54:57, 16.44s/it]

 67%|██████▋   | 16071/24156 [19:46<37:46:55, 16.82s/it]
[2024-04-05 15:24:43,302] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16072/24156 [20:04<38:28:37, 17.13s/it]
{'loss': 0.4183, 'learning_rate': 1.986206896551724e-07, 'rewards/chosen': -0.5934901833534241, 'rewards/rejected': -2.6537625789642334, 'rewards/accuracies': 0.875, 'rewards/margins': 2.060272455215454, 'policy_logps/rejected': -406.4017028808594, 'policy_logps/chosen': -597.0272216796875, 'referece_logps/rejected': -379.8640441894531, 'referece_logps/chosen': -591.0923461914062, 'logits/rejected': -0.8524425029754639, 'logits/chosen': -0.6020433902740479, 'epoch': 5.99}


 67%|██████▋   | 16074/24156 [20:39<39:16:50, 17.50s/it]

 67%|██████▋   | 16075/24156 [20:54<38:06:32, 16.98s/it]

 67%|██████▋   | 16076/24156 [21:17<41:48:37, 18.63s/it]

 67%|██████▋   | 16077/24156 [21:38<43:14:58, 19.27s/it]

 67%|██████▋   | 16078/24156 [21:53<40:29:04, 18.04s/it]
{'loss': 0.3685, 'learning_rate': 2.1517241379310345e-07, 'rewards/chosen': -1.7393580675125122, 'rewards/rejected': -3.126586675643921, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3872283697128296, 'policy_logps/rejected': -361.7157897949219, 'policy_logps/chosen': -387.1063537597656, 'referece_logps/rejected': -330.44989013671875, 'referece_logps/chosen': -369.7127685546875, 'logits/rejected': -1.0308717489242554, 'logits/chosen': -0.9808167815208435, 'epoch': 5.99}

 67%|██████▋   | 16079/24156 [22:07<37:55:52, 16.91s/it]

 67%|██████▋   | 16080/24156 [22:25<38:53:22, 17.34s/it]

 67%|██████▋   | 16081/24156 [22:47<41:46:00, 18.62s/it]

 67%|██████▋   | 16082/24156 [23:04<40:20:09, 17.98s/it]
[2024-04-05 15:28:22,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16083/24156 [23:25<42:47:48, 19.08s/it]

 67%|██████▋   | 16084/24156 [23:42<40:57:15, 18.27s/it]

 67%|██████▋   | 16085/24156 [23:55<37:57:01, 16.93s/it]

 67%|██████▋   | 16086/24156 [24:16<40:16:38, 17.97s/it]

 67%|██████▋   | 16087/24156 [24:28<36:07:29, 16.12s/it]
[2024-04-05 15:29:38,789] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16088/24156 [24:41<34:32:29, 15.41s/it]

 67%|██████▋   | 16089/24156 [25:03<38:57:24, 17.38s/it]


 67%|██████▋   | 16091/24156 [25:33<36:31:58, 16.31s/it]
[2024-04-05 15:30:30,426] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16092/24156 [25:54<39:47:19, 17.76s/it]
{'loss': 0.3943, 'learning_rate': 2.5379310344827586e-07, 'rewards/chosen': -0.7924946546554565, 'rewards/rejected': -2.086024045944214, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2935295104980469, 'policy_logps/rejected': -309.5924987792969, 'policy_logps/chosen': -378.26885986328125, 'referece_logps/rejected': -288.7322692871094, 'referece_logps/chosen': -370.34393310546875, 'logits/rejected': -0.17288197576999664, 'logits/chosen': -0.3831694722175598, 'epoch': 6.0}


 67%|██████▋   | 16094/24156 [26:33<41:21:03, 18.46s/it]

 67%|██████▋   | 16095/24156 [26:45<37:01:10, 16.53s/it]

 67%|██████▋   | 16096/24156 [27:07<41:04:45, 18.35s/it]
[2024-04-05 15:32:04,612] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3222, 'learning_rate': 2.6482758620689653e-07, 'rewards/chosen': -1.8182554244995117, 'rewards/rejected': -4.032813549041748, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2145586013793945, 'policy_logps/rejected': -399.1466369628906, 'policy_logps/chosen': -335.72674560546875, 'referece_logps/rejected': -358.8184814453125, 'referece_logps/chosen': -317.5442199707031, 'logits/rejected': 0.14921563863754272, 'logits/chosen': 0.236199751496315, 'epoch': 6.0}

 67%|██████▋   | 16097/24156 [27:19<37:00:24, 16.53s/it]


 67%|██████▋   | 16099/24156 [27:50<35:30:42, 15.87s/it]
{'loss': 0.4144, 'learning_rate': 2.7310344827586205e-07, 'rewards/chosen': -1.3115514516830444, 'rewards/rejected': -2.518087387084961, 'rewards/accuracies': 0.875, 'rewards/margins': 1.206535816192627, 'policy_logps/rejected': -363.1665954589844, 'policy_logps/chosen': -399.5865478515625, 'referece_logps/rejected': -337.9857177734375, 'referece_logps/chosen': -386.47100830078125, 'logits/rejected': -0.1245369017124176, 'logits/chosen': -0.057730525732040405, 'epoch': 6.0}


 67%|██████▋   | 16101/24156 [28:13<30:26:47, 13.61s/it]

 67%|██████▋   | 16102/24156 [28:29<32:01:26, 14.31s/it]
{'loss': 0.4175, 'learning_rate': 2.8137931034482757e-07, 'rewards/chosen': -1.5579707622528076, 'rewards/rejected': -2.992753744125366, 'rewards/accuracies': 0.875, 'rewards/margins': 1.434782862663269, 'policy_logps/rejected': -377.50848388671875, 'policy_logps/chosen': -457.8563232421875, 'referece_logps/rejected': -347.5809326171875, 'referece_logps/chosen': -442.27667236328125, 'logits/rejected': -0.3482688367366791, 'logits/chosen': -0.3363321125507355, 'epoch': 6.0}


 67%|██████▋   | 16104/24156 [29:03<35:34:08, 15.90s/it]
{'loss': 0.4168, 'learning_rate': 2.868965517241379e-07, 'rewards/chosen': -1.3515491485595703, 'rewards/rejected': -3.0249624252319336, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6734132766723633, 'policy_logps/rejected': -432.1714782714844, 'policy_logps/chosen': -460.27740478515625, 'referece_logps/rejected': -401.9218444824219, 'referece_logps/chosen': -446.76190185546875, 'logits/rejected': -0.11921858787536621, 'logits/chosen': -0.3180282413959503, 'epoch': 6.0}


 67%|██████▋   | 16106/24156 [29:35<36:28:09, 16.31s/it]
{'loss': 0.4732, 'learning_rate': 2.9241379310344824e-07, 'rewards/chosen': -1.723868489265442, 'rewards/rejected': -2.148526191711426, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4246574640274048, 'policy_logps/rejected': -379.4669494628906, 'policy_logps/chosen': -396.7663269042969, 'referece_logps/rejected': -357.981689453125, 'referece_logps/chosen': -379.527587890625, 'logits/rejected': -0.5356160998344421, 'logits/chosen': -0.44023603200912476, 'epoch': 6.0}


 67%|██████▋   | 16108/24156 [29:58<31:12:42, 13.96s/it]
{'loss': 0.4935, 'learning_rate': 2.979310344827586e-07, 'rewards/chosen': -1.6987881660461426, 'rewards/rejected': -2.482020139694214, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7832323312759399, 'policy_logps/rejected': -420.982177734375, 'policy_logps/chosen': -392.0890197753906, 'referece_logps/rejected': -396.1619873046875, 'referece_logps/chosen': -375.1011657714844, 'logits/rejected': -0.9793856143951416, 'logits/chosen': -0.8044661283493042, 'epoch': 6.0}

 67%|██████▋   | 16109/24156 [30:12<31:03:06, 13.89s/it]


 67%|██████▋   | 16111/24156 [30:45<33:45:16, 15.10s/it]
{'loss': 0.4262, 'learning_rate': 3.062068965517241e-07, 'rewards/chosen': -1.5391520261764526, 'rewards/rejected': -2.993467092514038, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4543153047561646, 'policy_logps/rejected': -319.7135925292969, 'policy_logps/chosen': -333.3761901855469, 'referece_logps/rejected': -289.7789306640625, 'referece_logps/chosen': -317.9846496582031, 'logits/rejected': -0.6788123250007629, 'logits/chosen': -0.5943906307220459, 'epoch': 6.0}


 67%|██████▋   | 16113/24156 [31:07<29:02:20, 13.00s/it]

 67%|██████▋   | 16114/24156 [31:22<30:50:12, 13.80s/it]

 67%|██████▋   | 16115/24156 [31:38<32:22:14, 14.49s/it]

 67%|██████▋   | 16116/24156 [31:49<29:51:26, 13.37s/it]
{'loss': 0.3551, 'learning_rate': 3.2e-07, 'rewards/chosen': -1.587671160697937, 'rewards/rejected': -2.2602758407592773, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6726045608520508, 'policy_logps/rejected': -368.74169921875, 'policy_logps/chosen': -454.4202880859375, 'referece_logps/rejected': -346.13897705078125, 'referece_logps/chosen': -438.5435485839844, 'logits/rejected': -0.6307715177536011, 'logits/chosen': -0.7170414924621582, 'epoch': 6.0}


 67%|██████▋   | 16118/24156 [32:13<28:12:35, 12.63s/it]
{'loss': 0.5577, 'learning_rate': 3.255172413793103e-07, 'rewards/chosen': -1.170225977897644, 'rewards/rejected': -2.8553924560546875, 'rewards/accuracies': 0.875, 'rewards/margins': 1.685166597366333, 'policy_logps/rejected': -441.6140441894531, 'policy_logps/chosen': -447.3846435546875, 'referece_logps/rejected': -413.06011962890625, 'referece_logps/chosen': -435.6824035644531, 'logits/rejected': -0.42968201637268066, 'logits/chosen': -0.463137149810791, 'epoch': 6.01}


 67%|██████▋   | 16120/24156 [32:41<29:43:22, 13.32s/it]

 67%|██████▋   | 16121/24156 [33:03<35:24:07, 15.86s/it]

 67%|██████▋   | 16122/24156 [33:14<31:59:57, 14.34s/it]

 67%|██████▋   | 16123/24156 [33:25<29:57:16, 13.42s/it]
{'loss': 0.3033, 'learning_rate': 3.393103448275862e-07, 'rewards/chosen': -1.2140257358551025, 'rewards/rejected': -2.8027455806732178, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5887197256088257, 'policy_logps/rejected': -391.65557861328125, 'policy_logps/chosen': -347.6208190917969, 'referece_logps/rejected': -363.6281433105469, 'referece_logps/chosen': -335.4805603027344, 'logits/rejected': 0.5334494113922119, 'logits/chosen': 0.6158481240272522, 'epoch': 6.01}

 67%|██████▋   | 16124/24156 [33:36<28:16:01, 12.67s/it]


 67%|██████▋   | 16126/24156 [34:01<27:48:43, 12.47s/it]
{'loss': 0.4563, 'learning_rate': 3.475862068965517e-07, 'rewards/chosen': -1.0017039775848389, 'rewards/rejected': -2.706062078475952, 'rewards/accuracies': 0.5, 'rewards/margins': 1.7043581008911133, 'policy_logps/rejected': -354.2379455566406, 'policy_logps/chosen': -278.7879638671875, 'referece_logps/rejected': -327.17730712890625, 'referece_logps/chosen': -268.77093505859375, 'logits/rejected': -1.0006023645401, 'logits/chosen': -0.8102941513061523, 'epoch': 6.01}


 67%|██████▋   | 16128/24156 [34:34<31:07:34, 13.96s/it]

 67%|██████▋   | 16129/24156 [34:47<30:49:56, 13.83s/it]
{'loss': 0.3223, 'learning_rate': 3.5586206896551724e-07, 'rewards/chosen': -1.9336410760879517, 'rewards/rejected': -3.006120204925537, 'rewards/accuracies': 0.875, 'rewards/margins': 1.072479248046875, 'policy_logps/rejected': -198.3507080078125, 'policy_logps/chosen': -322.6896057128906, 'referece_logps/rejected': -168.2894744873047, 'referece_logps/chosen': -303.3532409667969, 'logits/rejected': -0.9901505708694458, 'logits/chosen': -1.0846383571624756, 'epoch': 6.01}


 67%|██████▋   | 16131/24156 [35:26<37:28:17, 16.81s/it]

 67%|██████▋   | 16132/24156 [35:37<34:09:06, 15.32s/it]

 67%|██████▋   | 16133/24156 [35:59<38:20:00, 17.20s/it]

 67%|██████▋   | 16134/24156 [36:15<37:39:06, 16.90s/it]

 67%|██████▋   | 16135/24156 [36:37<41:05:52, 18.45s/it]
[2024-04-05 15:41:34,769] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4356, 'learning_rate': 3.7241379310344827e-07, 'rewards/chosen': -1.394466757774353, 'rewards/rejected': -2.745366334915161, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3508992195129395, 'policy_logps/rejected': -412.7420349121094, 'policy_logps/chosen': -493.41046142578125, 'referece_logps/rejected': -385.2883605957031, 'referece_logps/chosen': -479.46575927734375, 'logits/rejected': 0.452900230884552, 'logits/chosen': 0.42345014214515686, 'epoch': 6.01}

 67%|██████▋   | 16136/24156 [37:00<44:01:34, 19.76s/it]

 67%|██████▋   | 16137/24156 [37:21<44:26:54, 19.95s/it]


 67%|██████▋   | 16139/24156 [37:52<39:24:36, 17.70s/it]
[2024-04-05 15:42:49,172] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16140/24156 [38:07<37:41:19, 16.93s/it]

 67%|██████▋   | 16141/24156 [38:24<37:42:29, 16.94s/it]

 67%|██████▋   | 16142/24156 [38:47<41:53:36, 18.82s/it]

 67%|██████▋   | 16143/24156 [39:00<37:40:58, 16.93s/it]
{'loss': 0.4782, 'learning_rate': 3.944827586206896e-07, 'rewards/chosen': -0.9233298301696777, 'rewards/rejected': -1.6930601596832275, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7697303295135498, 'policy_logps/rejected': -228.88983154296875, 'policy_logps/chosen': -334.3396301269531, 'referece_logps/rejected': -211.959228515625, 'referece_logps/chosen': -325.1063537597656, 'logits/rejected': -0.4840298593044281, 'logits/chosen': -0.5160751938819885, 'epoch': 6.01}


 67%|██████▋   | 16145/24156 [39:38<40:28:42, 18.19s/it]

 67%|██████▋   | 16146/24156 [39:58<41:57:41, 18.86s/it]

 67%|██████▋   | 16147/24156 [40:17<42:21:17, 19.04s/it]

 67%|██████▋   | 16148/24156 [40:31<38:59:52, 17.53s/it]
{'loss': 0.4705, 'learning_rate': 4.082758620689655e-07, 'rewards/chosen': -1.329226016998291, 'rewards/rejected': -2.824303388595581, 'rewards/accuracies': 0.875, 'rewards/margins': 1.495077133178711, 'policy_logps/rejected': -306.47186279296875, 'policy_logps/chosen': -300.8265075683594, 'referece_logps/rejected': -278.22882080078125, 'referece_logps/chosen': -287.53424072265625, 'logits/rejected': -1.1875399351119995, 'logits/chosen': -1.0914617776870728, 'epoch': 6.02}


 67%|██████▋   | 16150/24156 [41:02<35:52:51, 16.13s/it]

 67%|██████▋   | 16151/24156 [41:18<35:50:21, 16.12s/it]
{'loss': 0.4246, 'learning_rate': 4.16551724137931e-07, 'rewards/chosen': -1.224948763847351, 'rewards/rejected': -3.0253281593322754, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8003792762756348, 'policy_logps/rejected': -432.5523986816406, 'policy_logps/chosen': -426.13922119140625, 'referece_logps/rejected': -402.299072265625, 'referece_logps/chosen': -413.88970947265625, 'logits/rejected': -0.3122887909412384, 'logits/chosen': -0.31587329506874084, 'epoch': 6.02}


 67%|██████▋   | 16153/24156 [41:52<35:46:04, 16.09s/it]
{'loss': 0.4597, 'learning_rate': 4.220689655172414e-07, 'rewards/chosen': -0.9051399827003479, 'rewards/rejected': -2.5368237495422363, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6316840648651123, 'policy_logps/rejected': -466.52008056640625, 'policy_logps/chosen': -588.8787841796875, 'referece_logps/rejected': -441.15185546875, 'referece_logps/chosen': -579.8274536132812, 'logits/rejected': -0.31071236729621887, 'logits/chosen': -0.5979154109954834, 'epoch': 6.02}

 67%|██████▋   | 16154/24156 [42:03<32:47:41, 14.75s/it]


 67%|██████▋   | 16156/24156 [42:26<28:48:26, 12.96s/it]
{'loss': 0.4934, 'learning_rate': 4.303448275862069e-07, 'rewards/chosen': -1.8008912801742554, 'rewards/rejected': -3.6581459045410156, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8572546243667603, 'policy_logps/rejected': -377.67864990234375, 'policy_logps/chosen': -379.55340576171875, 'referece_logps/rejected': -341.09716796875, 'referece_logps/chosen': -361.5444641113281, 'logits/rejected': -0.5943455100059509, 'logits/chosen': -0.6527186632156372, 'epoch': 6.02}


 67%|██████▋   | 16158/24156 [43:00<34:18:29, 15.44s/it]
{'loss': 0.4716, 'learning_rate': 4.3586206896551726e-07, 'rewards/chosen': -0.993369996547699, 'rewards/rejected': -1.865749716758728, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8723797798156738, 'policy_logps/rejected': -465.4931640625, 'policy_logps/chosen': -524.291015625, 'referece_logps/rejected': -446.835693359375, 'referece_logps/chosen': -514.3572998046875, 'logits/rejected': 0.4390692114830017, 'logits/chosen': 0.4682842493057251, 'epoch': 6.02}

 67%|██████▋   | 16159/24156 [43:15<34:30:13, 15.53s/it]

 67%|██████▋   | 16160/24156 [43:35<36:57:33, 16.64s/it]


 67%|██████▋   | 16162/24156 [44:06<35:24:16, 15.94s/it]

 67%|██████▋   | 16163/24156 [44:24<36:49:39, 16.59s/it]

 67%|██████▋   | 16164/24156 [44:46<40:23:39, 18.20s/it]
[2024-04-05 15:49:43,479] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4123, 'learning_rate': 4.5241379310344825e-07, 'rewards/chosen': -2.0519165992736816, 'rewards/rejected': -3.5360677242279053, 'rewards/accuracies': 0.75, 'rewards/margins': 1.484151005744934, 'policy_logps/rejected': -391.4612731933594, 'policy_logps/chosen': -454.89666748046875, 'referece_logps/rejected': -356.1006164550781, 'referece_logps/chosen': -434.37744140625, 'logits/rejected': -0.9152613878250122, 'logits/chosen': -0.9273896813392639, 'epoch': 6.02}
[2024-04-05 15:49:58,215] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16165/24156 [45:01<38:05:05, 17.16s/it]


 67%|██████▋   | 16167/24156 [45:36<37:49:13, 17.04s/it]

 67%|██████▋   | 16168/24156 [45:58<41:32:51, 18.72s/it]
[2024-04-05 15:50:55,865] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3584, 'learning_rate': 4.6344827586206897e-07, 'rewards/chosen': -0.9718424677848816, 'rewards/rejected': -2.1966943740844727, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2248518466949463, 'policy_logps/rejected': -496.201416015625, 'policy_logps/chosen': -413.48614501953125, 'referece_logps/rejected': -474.2344665527344, 'referece_logps/chosen': -403.76776123046875, 'logits/rejected': 0.07388767600059509, 'logits/chosen': 0.11466050148010254, 'epoch': 6.02}
[2024-04-05 15:51:08,430] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16169/24156 [46:11<37:26:34, 16.88s/it]


 67%|██████▋   | 16171/24156 [46:32<30:25:36, 13.72s/it]
{'loss': 0.3928, 'learning_rate': 4.7172413793103444e-07, 'rewards/chosen': -1.5010318756103516, 'rewards/rejected': -2.5784759521484375, 'rewards/accuracies': 0.625, 'rewards/margins': 1.077444076538086, 'policy_logps/rejected': -367.1520080566406, 'policy_logps/chosen': -496.0245361328125, 'referece_logps/rejected': -341.36724853515625, 'referece_logps/chosen': -481.01422119140625, 'logits/rejected': -0.9056631326675415, 'logits/chosen': -0.9239723086357117, 'epoch': 6.02}

 67%|██████▋   | 16172/24156 [46:43<28:25:07, 12.81s/it]

 67%|██████▋   | 16173/24156 [46:59<30:34:21, 13.79s/it]
[2024-04-05 15:52:14,512] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16174/24156 [47:17<33:20:24, 15.04s/it]

 67%|██████▋   | 16175/24156 [47:39<37:59:10, 17.13s/it]


 67%|██████▋   | 16177/24156 [48:22<42:43:15, 19.28s/it]
{'loss': 0.2595, 'learning_rate': 4.882758620689655e-07, 'rewards/chosen': -1.9202615022659302, 'rewards/rejected': -2.7477991580963135, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8275378942489624, 'policy_logps/rejected': -302.00213623046875, 'policy_logps/chosen': -261.54754638671875, 'referece_logps/rejected': -274.5241394042969, 'referece_logps/chosen': -242.3449249267578, 'logits/rejected': -0.5614840984344482, 'logits/chosen': -0.4935387372970581, 'epoch': 6.03}


 67%|██████▋   | 16179/24156 [48:58<42:04:11, 18.99s/it]
{'loss': 0.3637, 'learning_rate': 4.937931034482759e-07, 'rewards/chosen': -1.9070650339126587, 'rewards/rejected': -3.699382781982422, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7923179864883423, 'policy_logps/rejected': -331.2906494140625, 'policy_logps/chosen': -213.51217651367188, 'referece_logps/rejected': -294.29681396484375, 'referece_logps/chosen': -194.44151306152344, 'logits/rejected': -0.5854651927947998, 'logits/chosen': -0.5386732220649719, 'epoch': 6.03}

 67%|██████▋   | 16180/24156 [49:22<45:02:00, 20.33s/it]

 67%|██████▋   | 16181/24156 [49:37<41:29:35, 18.73s/it]

 67%|██████▋   | 16182/24156 [49:51<38:42:59, 17.48s/it]
[2024-04-05 15:55:07,374] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16183/24156 [50:10<39:26:26, 17.81s/it]


 67%|██████▋   | 16185/24156 [50:50<41:43:13, 18.84s/it]
{'loss': 0.266, 'learning_rate': 5.103448275862069e-07, 'rewards/chosen': -1.5587925910949707, 'rewards/rejected': -3.8665153980255127, 'rewards/accuracies': 0.875, 'rewards/margins': 2.307722806930542, 'policy_logps/rejected': -505.642333984375, 'policy_logps/chosen': -376.97637939453125, 'referece_logps/rejected': -466.9771728515625, 'referece_logps/chosen': -361.388427734375, 'logits/rejected': -0.6414101123809814, 'logits/chosen': -0.511685311794281, 'epoch': 6.03}

 67%|██████▋   | 16186/24156 [51:04<38:04:46, 17.20s/it]


 67%|██████▋   | 16188/24156 [51:37<37:30:15, 16.94s/it]
{'loss': 0.4078, 'learning_rate': 5.186206896551723e-07, 'rewards/chosen': -1.2446998357772827, 'rewards/rejected': -2.733582019805908, 'rewards/accuracies': 0.875, 'rewards/margins': 1.488882064819336, 'policy_logps/rejected': -464.96905517578125, 'policy_logps/chosen': -503.25885009765625, 'referece_logps/rejected': -437.6332702636719, 'referece_logps/chosen': -490.8118591308594, 'logits/rejected': -0.5144566893577576, 'logits/chosen': -0.3759199380874634, 'epoch': 6.03}


 67%|██████▋   | 16190/24156 [51:58<30:29:30, 13.78s/it]
{'loss': 0.4342, 'learning_rate': 5.241379310344828e-07, 'rewards/chosen': -1.4309360980987549, 'rewards/rejected': -2.487617015838623, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0566811561584473, 'policy_logps/rejected': -653.395751953125, 'policy_logps/chosen': -482.50360107421875, 'referece_logps/rejected': -628.5196533203125, 'referece_logps/chosen': -468.1942138671875, 'logits/rejected': -1.0339332818984985, 'logits/chosen': -0.9694944024085999, 'epoch': 6.03}

 67%|██████▋   | 16191/24156 [52:19<35:02:15, 15.84s/it]

 67%|██████▋   | 16192/24156 [52:39<38:06:41, 17.23s/it]

 67%|██████▋   | 16193/24156 [52:54<36:02:32, 16.29s/it]

 67%|██████▋   | 16194/24156 [53:04<32:18:45, 14.61s/it]

 67%|██████▋   | 16195/24156 [53:26<36:48:37, 16.65s/it]

 67%|██████▋   | 16196/24156 [53:37<33:32:34, 15.17s/it]

 67%|██████▋   | 16197/24156 [53:57<36:33:11, 16.53s/it]

 67%|██████▋   | 16198/24156 [54:15<37:29:31, 16.96s/it]
[2024-04-05 15:59:31,478] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16199/24156 [54:34<38:49:51, 17.57s/it]

 67%|██████▋   | 16200/24156 [54:53<39:58:21, 18.09s/it]
[2024-04-05 16:00:12,899] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16201/24156 [55:15<42:38:35, 19.30s/it]
[2024-04-05 16:00:24,798] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16202/24156 [55:27<37:44:02, 17.08s/it]

 67%|██████▋   | 16203/24156 [55:47<39:45:57, 18.00s/it]


 67%|██████▋   | 16205/24156 [56:23<39:44:55, 18.00s/it]
{'loss': 0.4005, 'learning_rate': 5.655172413793103e-07, 'rewards/chosen': -2.2114171981811523, 'rewards/rejected': -3.390448808670044, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1790317296981812, 'policy_logps/rejected': -387.2709655761719, 'policy_logps/chosen': -367.59124755859375, 'referece_logps/rejected': -353.3665466308594, 'referece_logps/chosen': -345.47705078125, 'logits/rejected': -0.9370377063751221, 'logits/chosen': -0.9596859812736511, 'epoch': 6.04}


 67%|██████▋   | 16207/24156 [56:59<39:55:03, 18.08s/it]

 67%|██████▋   | 16208/24156 [57:15<38:27:27, 17.42s/it]
{'loss': 0.4976, 'learning_rate': 5.737931034482758e-07, 'rewards/chosen': -1.2314594984054565, 'rewards/rejected': -2.7672722339630127, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5358127355575562, 'policy_logps/rejected': -541.3819580078125, 'policy_logps/chosen': -369.997314453125, 'referece_logps/rejected': -513.709228515625, 'referece_logps/chosen': -357.68267822265625, 'logits/rejected': 0.48172223567962646, 'logits/chosen': 0.5298604965209961, 'epoch': 6.04}


 67%|██████▋   | 16210/24156 [57:55<41:16:41, 18.70s/it]

 67%|██████▋   | 16211/24156 [58:13<40:47:33, 18.48s/it]
{'loss': 0.3465, 'learning_rate': 5.820689655172414e-07, 'rewards/chosen': -0.6236544847488403, 'rewards/rejected': -2.620393753051758, 'rewards/accuracies': 0.875, 'rewards/margins': 1.996739149093628, 'policy_logps/rejected': -337.32720947265625, 'policy_logps/chosen': -456.9612121582031, 'referece_logps/rejected': -311.1232604980469, 'referece_logps/chosen': -450.7246398925781, 'logits/rejected': -1.3110607862472534, 'logits/chosen': -1.6585965156555176, 'epoch': 6.04}


 67%|██████▋   | 16213/24156 [58:55<43:50:11, 19.87s/it]
{'loss': 0.3393, 'learning_rate': 5.875862068965517e-07, 'rewards/chosen': -1.587363362312317, 'rewards/rejected': -2.749943494796753, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1625802516937256, 'policy_logps/rejected': -448.9872131347656, 'policy_logps/chosen': -311.3236999511719, 'referece_logps/rejected': -421.48779296875, 'referece_logps/chosen': -295.4500732421875, 'logits/rejected': -0.5648297071456909, 'logits/chosen': -0.34505999088287354, 'epoch': 6.04}

 67%|██████▋   | 16214/24156 [59:12<41:50:34, 18.97s/it]

 67%|██████▋   | 16215/24156 [59:34<43:45:03, 19.83s/it]

 67%|██████▋   | 16216/24156 [59:47<39:13:01, 17.78s/it]

 67%|██████▋   | 16217/24156 [1:00:02<37:49:09, 17.15s/it]


 67%|██████▋   | 16219/24156 [1:00:35<37:24:11, 16.97s/it]
{'loss': 0.448, 'learning_rate': 6.041379310344827e-07, 'rewards/chosen': -1.190372109413147, 'rewards/rejected': -2.7532927989959717, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5629206895828247, 'policy_logps/rejected': -406.2330017089844, 'policy_logps/chosen': -382.7498474121094, 'referece_logps/rejected': -378.7000732421875, 'referece_logps/chosen': -370.84613037109375, 'logits/rejected': -0.4819375276565552, 'logits/chosen': -0.344038724899292, 'epoch': 6.04}


 67%|██████▋   | 16221/24156 [1:01:15<40:27:42, 18.36s/it]
{'loss': 0.3503, 'learning_rate': 6.09655172413793e-07, 'rewards/chosen': -0.8175623416900635, 'rewards/rejected': -3.5483689308166504, 'rewards/accuracies': 0.875, 'rewards/margins': 2.730806350708008, 'policy_logps/rejected': -398.30029296875, 'policy_logps/chosen': -364.03515625, 'referece_logps/rejected': -362.81658935546875, 'referece_logps/chosen': -355.8594970703125, 'logits/rejected': 0.1573978066444397, 'logits/chosen': 0.13050779700279236, 'epoch': 6.04}

 67%|██████▋   | 16222/24156 [1:01:37<42:29:47, 19.28s/it]
[2024-04-05 16:06:55,570] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16223/24156 [1:01:58<43:48:32, 19.88s/it]
[2024-04-05 16:07:16,216] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 16225/24156 [1:02:35<42:05:31, 19.11s/it]
{'loss': 0.3305, 'learning_rate': 6.206896551724138e-07, 'rewards/chosen': -2.450448751449585, 'rewards/rejected': -4.734086036682129, 'rewards/accuracies': 0.875, 'rewards/margins': 2.283637523651123, 'policy_logps/rejected': -556.18701171875, 'policy_logps/chosen': -464.318115234375, 'referece_logps/rejected': -508.84619140625, 'referece_logps/chosen': -439.8136291503906, 'logits/rejected': -1.407131552696228, 'logits/chosen': -1.0457583665847778, 'epoch': 6.05}

 67%|██████▋   | 16226/24156 [1:02:50<38:59:50, 17.70s/it]

 67%|██████▋   | 16227/24156 [1:03:02<35:15:22, 16.01s/it]

 67%|██████▋   | 16228/24156 [1:03:23<38:20:21, 17.41s/it]

 67%|██████▋   | 16229/24156 [1:03:40<38:02:56, 17.28s/it]


 67%|██████▋   | 16231/24156 [1:04:21<42:10:20, 19.16s/it]
{'loss': 0.317, 'learning_rate': 6.372413793103448e-07, 'rewards/chosen': -1.4984972476959229, 'rewards/rejected': -3.77866530418396, 'rewards/accuracies': 1.0, 'rewards/margins': 2.280168056488037, 'policy_logps/rejected': -284.44384765625, 'policy_logps/chosen': -279.4299011230469, 'referece_logps/rejected': -246.65719604492188, 'referece_logps/chosen': -264.4449157714844, 'logits/rejected': -0.26084840297698975, 'logits/chosen': -0.31911683082580566, 'epoch': 6.05}

 67%|██████▋   | 16232/24156 [1:04:41<42:12:05, 19.17s/it]

 67%|██████▋   | 16233/24156 [1:04:54<38:12:42, 17.36s/it]

 67%|██████▋   | 16234/24156 [1:05:10<37:40:19, 17.12s/it]

 67%|██████▋   | 16235/24156 [1:05:24<35:29:23, 16.13s/it]

 67%|██████▋   | 16236/24156 [1:05:42<36:51:12, 16.75s/it]
[2024-04-05 16:11:00,153] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16237/24156 [1:06:03<39:13:49, 17.83s/it]
[2024-04-05 16:11:18,248] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 16239/24156 [1:06:38<38:38:47, 17.57s/it]

 67%|██████▋   | 16240/24156 [1:06:59<41:24:24, 18.83s/it]

 67%|██████▋   | 16241/24156 [1:07:23<44:56:12, 20.44s/it]

 67%|██████▋   | 16242/24156 [1:07:39<41:56:25, 19.08s/it]
{'loss': 0.4199, 'learning_rate': 6.675862068965517e-07, 'rewards/chosen': -1.916108250617981, 'rewards/rejected': -3.3941903114318848, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4780819416046143, 'policy_logps/rejected': -356.31134033203125, 'policy_logps/chosen': -526.88916015625, 'referece_logps/rejected': -322.36944580078125, 'referece_logps/chosen': -507.7281188964844, 'logits/rejected': -0.6103085279464722, 'logits/chosen': -0.7437840700149536, 'epoch': 6.05}

 67%|██████▋   | 16243/24156 [1:08:01<43:18:13, 19.70s/it]
[2024-04-05 16:13:18,222] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 16244/24156 [1:08:21<43:36:48, 19.84s/it]


 67%|██████▋   | 16246/24156 [1:08:55<41:09:22, 18.73s/it]
[2024-04-05 16:13:52,886] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.425, 'learning_rate': 6.786206896551724e-07, 'rewards/chosen': -0.852435290813446, 'rewards/rejected': -2.2766170501708984, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4241818189620972, 'policy_logps/rejected': -520.0595703125, 'policy_logps/chosen': -590.1190185546875, 'referece_logps/rejected': -497.29345703125, 'referece_logps/chosen': -581.5946655273438, 'logits/rejected': 0.926760196685791, 'logits/chosen': 0.8540991544723511, 'epoch': 6.05}


 67%|██████▋   | 16248/24156 [1:09:20<33:35:56, 15.30s/it]
{'loss': 0.5157, 'learning_rate': 6.841379310344827e-07, 'rewards/chosen': -1.5222349166870117, 'rewards/rejected': -2.768289804458618, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2460548877716064, 'policy_logps/rejected': -293.5823669433594, 'policy_logps/chosen': -354.31024169921875, 'referece_logps/rejected': -265.89947509765625, 'referece_logps/chosen': -339.087890625, 'logits/rejected': -0.9491662383079529, 'logits/chosen': -0.9104983806610107, 'epoch': 6.05}

 67%|██████▋   | 16249/24156 [1:09:42<38:24:21, 17.49s/it]

 67%|██████▋   | 16250/24156 [1:09:57<36:28:21, 16.61s/it]


 67%|██████▋   | 16252/24156 [1:10:30<36:11:10, 16.48s/it]

 67%|██████▋   | 16253/24156 [1:10:48<37:03:18, 16.88s/it]

 67%|██████▋   | 16254/24156 [1:11:02<35:19:12, 16.09s/it]

 67%|██████▋   | 16255/24156 [1:11:22<37:47:47, 17.22s/it]

 67%|██████▋   | 16256/24156 [1:11:44<40:54:35, 18.64s/it]
{'loss': 0.325, 'learning_rate': 7.062068965517241e-07, 'rewards/chosen': -0.8579586148262024, 'rewards/rejected': -2.4919192790985107, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6339606046676636, 'policy_logps/rejected': -404.9909973144531, 'policy_logps/chosen': -384.4260559082031, 'referece_logps/rejected': -380.07177734375, 'referece_logps/chosen': -375.8465270996094, 'logits/rejected': 0.2606170177459717, 'logits/chosen': 0.15531465411186218, 'epoch': 6.06}


 67%|██████▋   | 16258/24156 [1:12:16<38:58:54, 17.77s/it]
{'loss': 0.324, 'learning_rate': 7.117241379310345e-07, 'rewards/chosen': -1.7340527772903442, 'rewards/rejected': -3.178311824798584, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4442589282989502, 'policy_logps/rejected': -246.89117431640625, 'policy_logps/chosen': -417.7330322265625, 'referece_logps/rejected': -215.10806274414062, 'referece_logps/chosen': -400.39251708984375, 'logits/rejected': -0.3231005370616913, 'logits/chosen': -0.2740415632724762, 'epoch': 6.06}

 67%|██████▋   | 16259/24156 [1:12:27<34:29:55, 15.73s/it]

 67%|██████▋   | 16260/24156 [1:12:47<37:07:21, 16.93s/it]

 67%|██████▋   | 16261/24156 [1:12:59<33:59:42, 15.50s/it]

 67%|██████▋   | 16262/24156 [1:13:13<33:00:35, 15.05s/it]


 67%|██████▋   | 16264/24156 [1:13:46<35:36:07, 16.24s/it]
{'loss': 0.3414, 'learning_rate': 7.282758620689655e-07, 'rewards/chosen': -2.178781270980835, 'rewards/rejected': -2.718140125274658, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5393592119216919, 'policy_logps/rejected': -402.7546691894531, 'policy_logps/chosen': -332.6423645019531, 'referece_logps/rejected': -375.5732727050781, 'referece_logps/chosen': -310.8545837402344, 'logits/rejected': 0.6808700561523438, 'logits/chosen': 0.7219575643539429, 'epoch': 6.06}

 67%|██████▋   | 16265/24156 [1:14:02<35:52:46, 16.37s/it]

 67%|██████▋   | 16266/24156 [1:14:15<33:12:10, 15.15s/it]

 67%|██████▋   | 16267/24156 [1:14:31<33:42:31, 15.38s/it]

 67%|██████▋   | 16268/24156 [1:14:52<37:22:45, 17.06s/it]


 67%|██████▋   | 16270/24156 [1:15:30<39:16:31, 17.93s/it]
{'loss': 0.4234, 'learning_rate': 7.448275862068965e-07, 'rewards/chosen': -1.3655447959899902, 'rewards/rejected': -3.207624912261963, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8420803546905518, 'policy_logps/rejected': -328.33966064453125, 'policy_logps/chosen': -384.408935546875, 'referece_logps/rejected': -296.263427734375, 'referece_logps/chosen': -370.75347900390625, 'logits/rejected': -1.1140367984771729, 'logits/chosen': -0.9396135210990906, 'epoch': 6.06}

 67%|██████▋   | 16271/24156 [1:15:44<36:23:24, 16.61s/it]

 67%|██████▋   | 16272/24156 [1:15:58<34:46:41, 15.88s/it]

 67%|██████▋   | 16273/24156 [1:16:08<31:22:03, 14.32s/it]

 67%|██████▋   | 16274/24156 [1:16:21<30:13:09, 13.80s/it]

 67%|██████▋   | 16275/24156 [1:16:41<34:31:55, 15.77s/it]

 67%|██████▋   | 16276/24156 [1:17:02<37:28:38, 17.12s/it]


 67%|██████▋   | 16278/24156 [1:17:30<33:51:04, 15.47s/it]
{'loss': 0.5194, 'learning_rate': 7.668965517241379e-07, 'rewards/chosen': -1.6710995435714722, 'rewards/rejected': -2.1820192337036133, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5109195709228516, 'policy_logps/rejected': -470.7868347167969, 'policy_logps/chosen': -475.855712890625, 'referece_logps/rejected': -448.9666748046875, 'referece_logps/chosen': -459.1446838378906, 'logits/rejected': -0.2848178744316101, 'logits/chosen': -0.2529911994934082, 'epoch': 6.06}

 67%|██████▋   | 16279/24156 [1:17:43<31:41:19, 14.48s/it]

 67%|██████▋   | 16280/24156 [1:18:00<33:30:28, 15.32s/it]

 67%|██████▋   | 16281/24156 [1:18:11<30:30:11, 13.94s/it]

 67%|██████▋   | 16282/24156 [1:18:32<35:28:02, 16.22s/it]

 67%|██████▋   | 16283/24156 [1:18:54<39:27:32, 18.04s/it]

 67%|██████▋   | 16284/24156 [1:19:06<35:10:32, 16.09s/it]

 67%|██████▋   | 16285/24156 [1:19:25<37:03:15, 16.95s/it]

 67%|██████▋   | 16286/24156 [1:19:36<33:13:09, 15.20s/it]

 67%|██████▋   | 16287/24156 [1:19:53<34:23:37, 15.73s/it]

 67%|██████▋   | 16288/24156 [1:20:10<35:01:23, 16.02s/it]

 67%|██████▋   | 16289/24156 [1:20:32<39:21:49, 18.01s/it]

 67%|██████▋   | 16290/24156 [1:20:54<41:53:13, 19.17s/it]

 67%|██████▋   | 16291/24156 [1:21:19<45:19:01, 20.74s/it]

 67%|██████▋   | 16292/24156 [1:21:40<45:45:29, 20.95s/it]

 67%|██████▋   | 16293/24156 [1:21:59<44:21:50, 20.31s/it]

 67%|██████▋   | 16294/24156 [1:22:13<40:12:24, 18.41s/it]

 67%|██████▋   | 16295/24156 [1:22:24<35:08:37, 16.09s/it]

 67%|██████▋   | 16296/24156 [1:22:37<33:11:21, 15.20s/it]

 67%|██████▋   | 16297/24156 [1:22:58<37:03:11, 16.97s/it]

 67%|██████▋   | 16298/24156 [1:23:14<36:31:19, 16.73s/it]

 67%|██████▋   | 16299/24156 [1:23:35<39:07:10, 17.92s/it]

 67%|██████▋   | 16300/24156 [1:23:51<38:12:34, 17.51s/it]

 67%|██████▋   | 16301/24156 [1:24:07<36:57:37, 16.94s/it]

 67%|██████▋   | 16302/24156 [1:24:17<32:51:40, 15.06s/it]

 67%|██████▋   | 16303/24156 [1:24:38<36:37:06, 16.79s/it]

 67%|██████▋   | 16304/24156 [1:24:55<36:23:49, 16.69s/it]

 67%|██████▋   | 16305/24156 [1:25:08<33:56:40, 15.56s/it]

 68%|██████▊   | 16306/24156 [1:25:23<34:06:17, 15.64s/it]

 68%|██████▊   | 16307/24156 [1:25:45<37:42:57, 17.30s/it]

 68%|██████▊   | 16308/24156 [1:26:05<39:31:45, 18.13s/it]

 68%|██████▊   | 16309/24156 [1:26:17<35:31:52, 16.30s/it]

 68%|██████▊   | 16310/24156 [1:26:29<33:08:47, 15.21s/it]

 68%|██████▊   | 16311/24156 [1:26:40<30:09:08, 13.84s/it]

 68%|██████▊   | 16312/24156 [1:26:52<28:37:08, 13.13s/it]

 68%|██████▊   | 16313/24156 [1:27:05<28:40:18, 13.16s/it]

 68%|██████▊   | 16314/24156 [1:27:24<32:28:24, 14.91s/it]

 68%|██████▊   | 16315/24156 [1:27:39<32:51:05, 15.08s/it]

 68%|██████▊   | 16316/24156 [1:28:00<36:16:50, 16.66s/it]

 68%|██████▊   | 16317/24156 [1:28:18<37:18:36, 17.13s/it]

 68%|██████▊   | 16318/24156 [1:28:39<40:03:39, 18.40s/it]

 68%|██████▊   | 16319/24156 [1:28:57<39:23:25, 18.09s/it]

 68%|██████▊   | 16320/24156 [1:29:09<35:29:30, 16.31s/it]

 68%|██████▊   | 16321/24156 [1:29:20<32:31:31, 14.94s/it]

 68%|██████▊   | 16322/24156 [1:29:31<29:49:13, 13.70s/it]

 68%|██████▊   | 16323/24156 [1:29:43<28:34:03, 13.13s/it]

 68%|██████▊   | 16324/24156 [1:29:57<28:57:49, 13.31s/it]

 68%|██████▊   | 16325/24156 [1:30:18<34:13:44, 15.74s/it]

 68%|██████▊   | 16326/24156 [1:30:32<33:07:46, 15.23s/it]

 68%|██████▊   | 16327/24156 [1:30:53<36:47:37, 16.92s/it]

 68%|██████▊   | 16328/24156 [1:31:13<38:45:02, 17.82s/it]

 68%|██████▊   | 16329/24156 [1:31:27<36:07:36, 16.62s/it]

 68%|██████▊   | 16330/24156 [1:31:44<36:39:23, 16.86s/it]

 68%|██████▊   | 16331/24156 [1:31:59<34:58:49, 16.09s/it]

 68%|██████▊   | 16332/24156 [1:32:14<34:34:20, 15.91s/it]

 68%|██████▊   | 16333/24156 [1:32:26<32:15:02, 14.84s/it]

 68%|██████▊   | 16334/24156 [1:32:40<31:38:05, 14.56s/it]

 68%|██████▊   | 16335/24156 [1:32:58<33:47:26, 15.55s/it]

 68%|██████▊   | 16336/24156 [1:33:17<36:01:30, 16.58s/it]

 68%|██████▊   | 16337/24156 [1:33:38<38:49:04, 17.87s/it]

 68%|██████▊   | 16338/24156 [1:33:57<39:18:58, 18.10s/it]

 68%|██████▊   | 16339/24156 [1:34:15<39:10:08, 18.04s/it]

 68%|██████▊   | 16340/24156 [1:34:35<40:46:01, 18.78s/it]

 68%|██████▊   | 16341/24156 [1:34:56<42:00:49, 19.35s/it]
[2024-04-05 16:40:16,274] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16342/24156 [1:35:19<44:24:38, 20.46s/it]
[2024-04-05 16:40:35,148] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16343/24156 [1:35:38<43:22:18, 19.98s/it]

 68%|██████▊   | 16344/24156 [1:35:54<40:57:39, 18.88s/it]
[2024-04-05 16:41:14,279] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16345/24156 [1:36:17<43:32:10, 20.07s/it]

 68%|██████▊   | 16346/24156 [1:36:37<43:52:59, 20.23s/it]

 68%|██████▊   | 16347/24156 [1:36:52<39:57:48, 18.42s/it]
[2024-04-05 16:42:13,420] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16348/24156 [1:37:16<43:47:47, 20.19s/it]

 68%|██████▊   | 16349/24156 [1:37:35<43:16:52, 19.96s/it]

 68%|██████▊   | 16350/24156 [1:37:46<37:29:38, 17.29s/it]

 68%|██████▊   | 16351/24156 [1:38:02<36:18:01, 16.74s/it]

 68%|██████▊   | 16352/24156 [1:38:23<39:19:34, 18.14s/it]

 68%|██████▊   | 16353/24156 [1:38:43<40:11:58, 18.55s/it]

 68%|██████▊   | 16354/24156 [1:39:00<39:26:57, 18.20s/it]

 68%|██████▊   | 16355/24156 [1:39:14<36:53:56, 17.03s/it]

 68%|██████▊   | 16356/24156 [1:39:25<32:43:45, 15.11s/it]

 68%|██████▊   | 16357/24156 [1:39:44<35:17:06, 16.29s/it]

 68%|██████▊   | 16358/24156 [1:39:58<33:48:58, 15.61s/it]

 68%|██████▊   | 16359/24156 [1:40:19<36:56:49, 17.06s/it]

 68%|██████▊   | 16360/24156 [1:40:40<39:35:30, 18.28s/it]

 68%|██████▊   | 16361/24156 [1:41:01<41:43:19, 19.27s/it]

 68%|██████▊   | 16362/24156 [1:41:18<40:12:55, 18.58s/it]

 68%|██████▊   | 16363/24156 [1:41:41<42:57:30, 19.84s/it]

 68%|██████▊   | 16364/24156 [1:41:55<39:17:35, 18.15s/it]

 68%|██████▊   | 16365/24156 [1:42:13<39:19:46, 18.17s/it]

 68%|██████▊   | 16366/24156 [1:42:27<36:35:59, 16.91s/it]

 68%|██████▊   | 16367/24156 [1:42:41<34:27:37, 15.93s/it]

 68%|██████▊   | 16368/24156 [1:43:02<37:26:52, 17.31s/it]

 68%|██████▊   | 16369/24156 [1:43:19<37:16:57, 17.24s/it]

 68%|██████▊   | 16370/24156 [1:43:34<36:00:27, 16.65s/it]

 68%|██████▊   | 16371/24156 [1:43:56<39:10:06, 18.11s/it]

 68%|██████▊   | 16372/24156 [1:44:11<37:18:56, 17.26s/it]

 68%|██████▊   | 16373/24156 [1:44:23<33:50:41, 15.65s/it]

 68%|██████▊   | 16374/24156 [1:44:39<34:33:52, 15.99s/it]

 68%|██████▊   | 16375/24156 [1:44:56<35:01:02, 16.20s/it]

 68%|██████▊   | 16376/24156 [1:45:12<34:44:58, 16.08s/it]

 68%|██████▊   | 16377/24156 [1:45:23<31:21:40, 14.51s/it]

 68%|██████▊   | 16378/24156 [1:45:36<30:23:51, 14.07s/it]

 68%|██████▊   | 16379/24156 [1:45:57<34:53:32, 16.15s/it]


 68%|██████▊   | 16381/24156 [1:46:37<39:11:48, 18.15s/it]

 68%|██████▊   | 16382/24156 [1:46:59<41:17:14, 19.12s/it]

 68%|██████▊   | 16383/24156 [1:47:13<38:11:39, 17.69s/it]

 68%|██████▊   | 16384/24156 [1:47:26<35:10:28, 16.29s/it]

 68%|██████▊   | 16385/24156 [1:47:40<33:42:38, 15.62s/it]

 68%|██████▊   | 16386/24156 [1:47:53<31:37:23, 14.65s/it]

 68%|██████▊   | 16387/24156 [1:48:04<29:24:39, 13.63s/it]

 68%|██████▊   | 16388/24156 [1:48:20<31:16:43, 14.50s/it]

 68%|██████▊   | 16389/24156 [1:48:40<34:19:11, 15.91s/it]

 68%|██████▊   | 16390/24156 [1:49:00<37:04:43, 17.19s/it]

 68%|██████▊   | 16391/24156 [1:49:18<37:31:58, 17.40s/it]

 68%|██████▊   | 16392/24156 [1:49:36<37:50:00, 17.54s/it]

 68%|██████▊   | 16393/24156 [1:49:55<38:53:20, 18.03s/it]

 68%|██████▊   | 16394/24156 [1:50:14<39:53:26, 18.50s/it]

 68%|██████▊   | 16395/24156 [1:50:32<39:09:39, 18.17s/it]

 68%|██████▊   | 16396/24156 [1:50:53<41:21:27, 19.19s/it]
[2024-04-05 16:55:50,718] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16397/24156 [1:51:04<36:12:47, 16.80s/it]
{'loss': 0.5072, 'learning_rate': 1.0951724137931033e-06, 'rewards/chosen': -1.8190975189208984, 'rewards/rejected': -2.6725220680236816, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8534247279167175, 'policy_logps/rejected': -327.4117736816406, 'policy_logps/chosen': -291.12982177734375, 'referece_logps/rejected': -300.68658447265625, 'referece_logps/chosen': -272.9388732910156, 'logits/rejected': 0.06035010516643524, 'logits/chosen': 0.07666759192943573, 'epoch': 6.11}
[2024-04-05 16:56:20,172] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 16399/24156 [1:51:39<36:42:30, 17.04s/it]

 68%|██████▊   | 16400/24156 [1:51:52<33:49:26, 15.70s/it]
{'loss': 0.6428, 'learning_rate': 1.1034482758620688e-06, 'rewards/chosen': -1.233425498008728, 'rewards/rejected': -1.9448251724243164, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7113995552062988, 'policy_logps/rejected': -397.87188720703125, 'policy_logps/chosen': -291.19384765625, 'referece_logps/rejected': -378.42364501953125, 'referece_logps/chosen': -278.859619140625, 'logits/rejected': -0.8172556161880493, 'logits/chosen': -0.8030933141708374, 'epoch': 6.11}


 68%|██████▊   | 16402/24156 [1:52:24<33:51:59, 15.72s/it]

 68%|██████▊   | 16403/24156 [1:52:35<31:00:59, 14.40s/it]
{'loss': 0.3829, 'learning_rate': 1.1117241379310346e-06, 'rewards/chosen': -1.4488822221755981, 'rewards/rejected': -3.384744882583618, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9358627796173096, 'policy_logps/rejected': -451.9565124511719, 'policy_logps/chosen': -420.79803466796875, 'referece_logps/rejected': -418.10906982421875, 'referece_logps/chosen': -406.3092041015625, 'logits/rejected': -0.7505602240562439, 'logits/chosen': -0.6601776480674744, 'epoch': 6.11}


 68%|██████▊   | 16405/24156 [1:53:09<33:47:03, 15.69s/it]

 68%|██████▊   | 16406/24156 [1:53:32<38:19:03, 17.80s/it]
{'loss': 0.3181, 'learning_rate': 1.12e-06, 'rewards/chosen': -1.108988642692566, 'rewards/rejected': -3.1629648208618164, 'rewards/accuracies': 1.0, 'rewards/margins': 2.05397629737854, 'policy_logps/rejected': -379.9979248046875, 'policy_logps/chosen': -394.0074157714844, 'referece_logps/rejected': -348.3682861328125, 'referece_logps/chosen': -382.9175109863281, 'logits/rejected': -0.637660026550293, 'logits/chosen': -0.6019558310508728, 'epoch': 6.11}

 68%|██████▊   | 16407/24156 [1:53:47<36:27:12, 16.94s/it]
[2024-04-05 16:59:06,287] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 16409/24156 [1:54:24<37:30:49, 17.43s/it]

 68%|██████▊   | 16410/24156 [1:54:44<39:35:03, 18.40s/it]

 68%|██████▊   | 16411/24156 [1:55:00<37:26:03, 17.40s/it]

 68%|██████▊   | 16412/24156 [1:55:10<33:06:12, 15.39s/it]

 68%|██████▊   | 16413/24156 [1:55:21<30:07:20, 14.00s/it]

 68%|██████▊   | 16414/24156 [1:55:41<34:04:13, 15.84s/it]
[2024-04-05 17:00:38,633] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16415/24156 [1:55:56<33:41:42, 15.67s/it]

 68%|██████▊   | 16416/24156 [1:56:19<38:01:08, 17.68s/it]

 68%|██████▊   | 16417/24156 [1:56:40<40:15:29, 18.73s/it]
{'loss': 0.3123, 'learning_rate': 1.1503448275862068e-06, 'rewards/chosen': -1.2516396045684814, 'rewards/rejected': -2.714050769805908, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4624109268188477, 'policy_logps/rejected': -340.4202575683594, 'policy_logps/chosen': -547.6027221679688, 'referece_logps/rejected': -313.27978515625, 'referece_logps/chosen': -535.0863037109375, 'logits/rejected': 0.34839385747909546, 'logits/chosen': 0.2937832176685333, 'epoch': 6.12}

 68%|██████▊   | 16418/24156 [1:57:01<41:49:12, 19.46s/it]


 68%|██████▊   | 16420/24156 [1:57:37<39:54:20, 18.57s/it]
{'loss': 0.3036, 'learning_rate': 1.1586206896551724e-06, 'rewards/chosen': -0.8075580596923828, 'rewards/rejected': -2.850419521331787, 'rewards/accuracies': 1.0, 'rewards/margins': 2.042861223220825, 'policy_logps/rejected': -441.4786071777344, 'policy_logps/chosen': -481.0353088378906, 'referece_logps/rejected': -412.97442626953125, 'referece_logps/chosen': -472.959716796875, 'logits/rejected': -0.06802916526794434, 'logits/chosen': -0.052392806857824326, 'epoch': 6.12}


 68%|██████▊   | 16422/24156 [1:58:06<34:47:13, 16.19s/it]

 68%|██████▊   | 16423/24156 [1:58:17<31:11:39, 14.52s/it]
{'loss': 0.4589, 'learning_rate': 1.1668965517241379e-06, 'rewards/chosen': -1.5192638635635376, 'rewards/rejected': -2.2527642250061035, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7335004806518555, 'policy_logps/rejected': -483.53021240234375, 'policy_logps/chosen': -524.3350830078125, 'referece_logps/rejected': -461.0025634765625, 'referece_logps/chosen': -509.1424560546875, 'logits/rejected': -0.15866544842720032, 'logits/chosen': -0.3486216366291046, 'epoch': 6.12}

 68%|██████▊   | 16424/24156 [1:58:27<28:43:27, 13.37s/it]


 68%|██████▊   | 16426/24156 [1:59:04<34:48:04, 16.21s/it]
{'loss': 0.2554, 'learning_rate': 1.1751724137931034e-06, 'rewards/chosen': -1.575869083404541, 'rewards/rejected': -3.731058359146118, 'rewards/accuracies': 1.0, 'rewards/margins': 2.155189275741577, 'policy_logps/rejected': -263.0540466308594, 'policy_logps/chosen': -243.45872497558594, 'referece_logps/rejected': -225.7434539794922, 'referece_logps/chosen': -227.70001220703125, 'logits/rejected': -0.5666698217391968, 'logits/chosen': -0.6365767121315002, 'epoch': 6.12}


 68%|██████▊   | 16428/24156 [1:59:47<40:55:55, 19.07s/it]
[2024-04-05 17:04:44,232] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3002, 'learning_rate': 1.1806896551724138e-06, 'rewards/chosen': -1.2246549129486084, 'rewards/rejected': -3.9909350872039795, 'rewards/accuracies': 0.875, 'rewards/margins': 2.766279935836792, 'policy_logps/rejected': -361.2477722167969, 'policy_logps/chosen': -461.42266845703125, 'referece_logps/rejected': -321.3384094238281, 'referece_logps/chosen': -449.1761169433594, 'logits/rejected': 0.27780401706695557, 'logits/chosen': 0.32257696986198425, 'epoch': 6.12}
[2024-04-05 17:04:56,649] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 16430/24156 [2:00:19<38:16:45, 17.84s/it]

 68%|██████▊   | 16431/24156 [2:00:34<36:54:14, 17.20s/it]

 68%|██████▊   | 16432/24156 [2:00:52<37:06:55, 17.30s/it]

 68%|██████▊   | 16433/24156 [2:01:11<38:13:12, 17.82s/it]

 68%|██████▊   | 16434/24156 [2:01:30<39:01:33, 18.19s/it]
{'loss': 0.4161, 'learning_rate': 1.1972413793103448e-06, 'rewards/chosen': -0.9036520719528198, 'rewards/rejected': -3.092545986175537, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1888937950134277, 'policy_logps/rejected': -524.0083618164062, 'policy_logps/chosen': -529.2349853515625, 'referece_logps/rejected': -493.0828857421875, 'referece_logps/chosen': -520.198486328125, 'logits/rejected': 0.49955832958221436, 'logits/chosen': 0.45875003933906555, 'epoch': 6.12}


 68%|██████▊   | 16436/24156 [2:02:05<38:16:40, 17.85s/it]

 68%|██████▊   | 16437/24156 [2:02:21<37:00:12, 17.26s/it]

 68%|██████▊   | 16438/24156 [2:02:40<38:09:11, 17.80s/it]
[2024-04-05 17:07:37,459] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4923, 'learning_rate': 1.2082758620689655e-06, 'rewards/chosen': -1.0801260471343994, 'rewards/rejected': -2.193126678466797, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1130008697509766, 'policy_logps/rejected': -400.2851257324219, 'policy_logps/chosen': -336.64520263671875, 'referece_logps/rejected': -378.3538818359375, 'referece_logps/chosen': -325.84393310546875, 'logits/rejected': -0.3999220132827759, 'logits/chosen': -0.5409958362579346, 'epoch': 6.12}


 68%|██████▊   | 16440/24156 [2:03:14<37:49:48, 17.65s/it]

 68%|██████▊   | 16441/24156 [2:03:35<39:44:58, 18.55s/it]
{'loss': 0.3742, 'learning_rate': 1.216551724137931e-06, 'rewards/chosen': -1.5515064001083374, 'rewards/rejected': -2.883453845977783, 'rewards/accuracies': 0.75, 'rewards/margins': 1.331947684288025, 'policy_logps/rejected': -453.4352111816406, 'policy_logps/chosen': -393.1806335449219, 'referece_logps/rejected': -424.60064697265625, 'referece_logps/chosen': -377.66558837890625, 'logits/rejected': 0.7040971517562866, 'logits/chosen': 0.7263847589492798, 'epoch': 6.13}


 68%|██████▊   | 16443/24156 [2:04:15<41:37:13, 19.43s/it]
[2024-04-05 17:09:12,579] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16444/24156 [2:04:31<39:03:12, 18.23s/it]
[2024-04-05 17:09:28,020] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.315, 'learning_rate': 1.2248275862068965e-06, 'rewards/chosen': -0.746380627155304, 'rewards/rejected': -3.5230813026428223, 'rewards/accuracies': 0.875, 'rewards/margins': 2.776700735092163, 'policy_logps/rejected': -404.35321044921875, 'policy_logps/chosen': -440.70751953125, 'referece_logps/rejected': -369.12237548828125, 'referece_logps/chosen': -433.2436828613281, 'logits/rejected': 0.11924625933170319, 'logits/chosen': 0.0029717236757278442, 'epoch': 6.13}


 68%|██████▊   | 16446/24156 [2:05:01<35:35:14, 16.62s/it]

 68%|██████▊   | 16447/24156 [2:05:13<32:24:33, 15.13s/it]

 68%|██████▊   | 16448/24156 [2:05:24<29:59:30, 14.01s/it]
{'loss': 0.3393, 'learning_rate': 1.2358620689655173e-06, 'rewards/chosen': -0.9946900010108948, 'rewards/rejected': -2.8598792552948, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8651891946792603, 'policy_logps/rejected': -275.8892822265625, 'policy_logps/chosen': -278.88299560546875, 'referece_logps/rejected': -247.29046630859375, 'referece_logps/chosen': -268.9360656738281, 'logits/rejected': -0.49872520565986633, 'logits/chosen': -0.448178231716156, 'epoch': 6.13}


 68%|██████▊   | 16450/24156 [2:05:55<31:19:30, 14.63s/it]

 68%|██████▊   | 16451/24156 [2:06:07<29:54:48, 13.98s/it]

 68%|██████▊   | 16452/24156 [2:06:21<29:53:39, 13.97s/it]

 68%|██████▊   | 16453/24156 [2:06:41<33:30:16, 15.66s/it]

 68%|██████▊   | 16454/24156 [2:06:57<33:36:15, 15.71s/it]

 68%|██████▊   | 16455/24156 [2:07:13<34:10:30, 15.98s/it]

 68%|██████▊   | 16456/24156 [2:07:35<37:43:54, 17.64s/it]

 68%|██████▊   | 16457/24156 [2:07:51<36:49:52, 17.22s/it]

 68%|██████▊   | 16458/24156 [2:08:07<35:50:14, 16.76s/it]

 68%|██████▊   | 16459/24156 [2:08:21<34:18:12, 16.04s/it]

 68%|██████▊   | 16460/24156 [2:08:43<38:12:55, 17.88s/it]

 68%|██████▊   | 16461/24156 [2:09:05<40:56:56, 19.16s/it]

 68%|██████▊   | 16462/24156 [2:09:25<41:25:31, 19.38s/it]

 68%|██████▊   | 16463/24156 [2:09:43<40:20:55, 18.88s/it]

 68%|██████▊   | 16464/24156 [2:10:01<39:57:54, 18.70s/it]
{'loss': 0.3349, 'learning_rate': 1.28e-06, 'rewards/chosen': -1.6109539270401, 'rewards/rejected': -3.6832196712493896, 'rewards/accuracies': 1.0, 'rewards/margins': 2.072266101837158, 'policy_logps/rejected': -392.0534362792969, 'policy_logps/chosen': -313.9323425292969, 'referece_logps/rejected': -355.2212219238281, 'referece_logps/chosen': -297.8227844238281, 'logits/rejected': -0.41444969177246094, 'logits/chosen': -0.37069007754325867, 'epoch': 6.13}

 68%|██████▊   | 16465/24156 [2:10:24<42:27:46, 19.88s/it]

 68%|██████▊   | 16466/24156 [2:10:38<38:58:31, 18.25s/it]
[2024-04-05 17:15:57,364] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 16468/24156 [2:11:15<38:15:21, 17.91s/it]
[2024-04-05 17:16:12,273] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4091, 'learning_rate': 1.2910344827586206e-06, 'rewards/chosen': -1.296036958694458, 'rewards/rejected': -2.723714590072632, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4276773929595947, 'policy_logps/rejected': -215.6597442626953, 'policy_logps/chosen': -220.87535095214844, 'referece_logps/rejected': -188.422607421875, 'referece_logps/chosen': -207.91497802734375, 'logits/rejected': -0.39594659209251404, 'logits/chosen': -0.4853031635284424, 'epoch': 6.14}


 68%|██████▊   | 16470/24156 [2:11:49<36:58:34, 17.32s/it]

 68%|██████▊   | 16471/24156 [2:12:04<35:12:17, 16.49s/it]

 68%|██████▊   | 16472/24156 [2:12:18<33:33:52, 15.73s/it]

 68%|██████▊   | 16473/24156 [2:12:36<35:12:28, 16.50s/it]
{'loss': 0.454, 'learning_rate': 1.3048275862068966e-06, 'rewards/chosen': -1.0049885511398315, 'rewards/rejected': -2.9648804664611816, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9598920345306396, 'policy_logps/rejected': -451.7550964355469, 'policy_logps/chosen': -566.1605834960938, 'referece_logps/rejected': -422.1062927246094, 'referece_logps/chosen': -556.1106567382812, 'logits/rejected': -0.37837928533554077, 'logits/chosen': -0.32045778632164, 'epoch': 6.14}


 68%|██████▊   | 16475/24156 [2:13:15<38:15:52, 17.93s/it]

 68%|██████▊   | 16476/24156 [2:13:37<40:32:57, 19.01s/it]

 68%|██████▊   | 16477/24156 [2:13:56<40:20:42, 18.91s/it]

 68%|██████▊   | 16478/24156 [2:14:10<37:08:33, 17.42s/it]

 68%|██████▊   | 16479/24156 [2:14:24<35:05:44, 16.46s/it]
{'loss': 0.3669, 'learning_rate': 1.3213793103448276e-06, 'rewards/chosen': -1.8739793300628662, 'rewards/rejected': -3.5981483459472656, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7241687774658203, 'policy_logps/rejected': -283.345458984375, 'policy_logps/chosen': -316.404541015625, 'referece_logps/rejected': -247.364013671875, 'referece_logps/chosen': -297.66473388671875, 'logits/rejected': -1.192088007926941, 'logits/chosen': -1.2281333208084106, 'epoch': 6.14}

 68%|██████▊   | 16480/24156 [2:14:45<37:48:08, 17.73s/it]


 68%|██████▊   | 16482/24156 [2:15:17<36:27:58, 17.11s/it]

 68%|██████▊   | 16483/24156 [2:15:36<37:09:18, 17.43s/it]

 68%|██████▊   | 16484/24156 [2:15:52<36:16:38, 17.02s/it]
{'loss': 0.4979, 'learning_rate': 1.3351724137931033e-06, 'rewards/chosen': -1.706749677658081, 'rewards/rejected': -2.5447795391082764, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8380299210548401, 'policy_logps/rejected': -513.7850341796875, 'policy_logps/chosen': -350.26300048828125, 'referece_logps/rejected': -488.3372497558594, 'referece_logps/chosen': -333.1955261230469, 'logits/rejected': -0.5273194313049316, 'logits/chosen': -0.2475782334804535, 'epoch': 6.14}


 68%|██████▊   | 16486/24156 [2:16:23<34:53:57, 16.38s/it]

 68%|██████▊   | 16487/24156 [2:16:44<37:50:24, 17.76s/it]
{'loss': 0.4143, 'learning_rate': 1.3434482758620689e-06, 'rewards/chosen': -1.3241956233978271, 'rewards/rejected': -3.753857374191284, 'rewards/accuracies': 0.75, 'rewards/margins': 2.429661750793457, 'policy_logps/rejected': -313.737060546875, 'policy_logps/chosen': -436.5562744140625, 'referece_logps/rejected': -276.198486328125, 'referece_logps/chosen': -423.3143310546875, 'logits/rejected': 0.6679955124855042, 'logits/chosen': 0.48828238248825073, 'epoch': 6.14}


 68%|██████▊   | 16489/24156 [2:17:18<36:21:24, 17.07s/it]

 68%|██████▊   | 16490/24156 [2:17:34<35:42:23, 16.77s/it]

 68%|██████▊   | 16491/24156 [2:17:50<35:08:58, 16.51s/it]

 68%|██████▊   | 16492/24156 [2:18:08<36:01:29, 16.92s/it]

 68%|██████▊   | 16493/24156 [2:18:29<38:55:45, 18.29s/it]

 68%|██████▊   | 16494/24156 [2:18:48<39:01:27, 18.34s/it]

 68%|██████▊   | 16495/24156 [2:18:59<34:49:32, 16.36s/it]

 68%|██████▊   | 16496/24156 [2:19:10<31:13:20, 14.67s/it]

 68%|██████▊   | 16497/24156 [2:19:23<30:03:52, 14.13s/it]

 68%|██████▊   | 16498/24156 [2:19:40<31:54:38, 15.00s/it]
{'loss': 0.4005, 'learning_rate': 1.3737931034482756e-06, 'rewards/chosen': -1.9879875183105469, 'rewards/rejected': -2.5473101139068604, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5593227744102478, 'policy_logps/rejected': -282.2206726074219, 'policy_logps/chosen': -284.60760498046875, 'referece_logps/rejected': -256.7475891113281, 'referece_logps/chosen': -264.7276916503906, 'logits/rejected': -0.6947582364082336, 'logits/chosen': -0.6751932501792908, 'epoch': 6.15}


 68%|██████▊   | 16500/24156 [2:20:12<33:04:25, 15.55s/it]
[2024-04-05 17:25:09,221] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 68%|██████▊   | 16500/24156 [2:20:12<33:04:25, 15.55s/it]/mnt/petrelfs/songmingyang/anaconda3/envs/vcd_origin/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.2726, 'learning_rate': 1.3820689655172413e-06, 'rewards/chosen': -1.7033178806304932, 'rewards/rejected': -3.480527877807617, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7772098779678345, 'policy_logps/rejected': -318.94476318359375, 'policy_logps/chosen': -403.8304748535156, 'referece_logps/rejected': -284.1395263671875, 'referece_logps/chosen': -386.79730224609375, 'logits/rejected': -0.11214077472686768, 'logits/chosen': -0.09828578680753708, 'epoch': 6.15}

 68%|██████▊   | 16502/24156 [2:21:06<43:45:09, 20.58s/it]
{'loss': 0.3191, 'learning_rate': 1.3848275862068965e-06, 'rewards/chosen': -1.6869884729385376, 'rewards/rejected': -2.9461023807525635, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2591139078140259, 'policy_logps/rejected': -412.96209716796875, 'policy_logps/chosen': -397.81427001953125, 'referece_logps/rejected': -383.5010681152344, 'referece_logps/chosen': -380.94439697265625, 'logits/rejected': -0.8573998212814331, 'logits/chosen': -0.7818175554275513, 'epoch': 6.15}


 68%|██████▊   | 16504/24156 [2:21:46<43:09:38, 20.31s/it]
{'loss': 0.3026, 'learning_rate': 1.3903448275862069e-06, 'rewards/chosen': -1.4033037424087524, 'rewards/rejected': -3.4192986488342285, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0159950256347656, 'policy_logps/rejected': -358.38104248046875, 'policy_logps/chosen': -323.47503662109375, 'referece_logps/rejected': -324.18804931640625, 'referece_logps/chosen': -309.44195556640625, 'logits/rejected': -0.9740282893180847, 'logits/chosen': -0.9630001783370972, 'epoch': 6.15}
[2024-04-05 17:27:06,367] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 16506/24156 [2:22:24<40:43:10, 19.16s/it]

 68%|██████▊   | 16507/24156 [2:22:42<40:01:05, 18.83s/it]
{'loss': 0.3034, 'learning_rate': 1.3986206896551724e-06, 'rewards/chosen': -1.265751600265503, 'rewards/rejected': -3.051579713821411, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7858283519744873, 'policy_logps/rejected': -243.17166137695312, 'policy_logps/chosen': -506.7321472167969, 'referece_logps/rejected': -212.65586853027344, 'referece_logps/chosen': -494.07464599609375, 'logits/rejected': -0.10465767979621887, 'logits/chosen': -0.34716325998306274, 'epoch': 6.15}
[2024-04-05 17:28:02,330] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 16509/24156 [2:23:26<43:35:45, 20.52s/it]
{'loss': 0.3731, 'learning_rate': 1.4041379310344828e-06, 'rewards/chosen': -1.453542709350586, 'rewards/rejected': -3.4263787269592285, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9728360176086426, 'policy_logps/rejected': -262.099365234375, 'policy_logps/chosen': -374.66943359375, 'referece_logps/rejected': -227.8355712890625, 'referece_logps/chosen': -360.134033203125, 'logits/rejected': -0.6359689235687256, 'logits/chosen': -0.6755629181861877, 'epoch': 6.15}


 68%|██████▊   | 16511/24156 [2:24:08<43:19:19, 20.40s/it]

 68%|██████▊   | 16512/24156 [2:24:22<39:27:08, 18.58s/it]

 68%|██████▊   | 16513/24156 [2:24:44<41:25:07, 19.51s/it]

 68%|██████▊   | 16514/24156 [2:25:02<40:29:36, 19.08s/it]

 68%|██████▊   | 16515/24156 [2:25:21<40:26:00, 19.05s/it]
{'loss': 0.3614, 'learning_rate': 1.4206896551724136e-06, 'rewards/chosen': -1.7097587585449219, 'rewards/rejected': -3.3506486415863037, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6408897638320923, 'policy_logps/rejected': -407.2664489746094, 'policy_logps/chosen': -442.2648010253906, 'referece_logps/rejected': -373.75994873046875, 'referece_logps/chosen': -425.167236328125, 'logits/rejected': -0.12605077028274536, 'logits/chosen': -0.1698782742023468, 'epoch': 6.15}
[2024-04-05 17:30:40,807] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16516/24156 [2:25:43<42:42:25, 20.12s/it]

 68%|██████▊   | 16517/24156 [2:25:59<39:49:18, 18.77s/it]


 68%|██████▊   | 16519/24156 [2:26:28<35:56:13, 16.94s/it]

 68%|██████▊   | 16520/24156 [2:26:52<40:14:40, 18.97s/it]
[2024-04-05 17:31:49,640] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3813, 'learning_rate': 1.4344827586206896e-06, 'rewards/chosen': -1.826798439025879, 'rewards/rejected': -4.632759094238281, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8059606552124023, 'policy_logps/rejected': -635.9495239257812, 'policy_logps/chosen': -534.9705200195312, 'referece_logps/rejected': -589.6218872070312, 'referece_logps/chosen': -516.7025146484375, 'logits/rejected': 0.4786365032196045, 'logits/chosen': 0.48718196153640747, 'epoch': 6.15}

 68%|██████▊   | 16521/24156 [2:27:03<34:59:01, 16.50s/it]


 68%|██████▊   | 16523/24156 [2:27:38<36:06:31, 17.03s/it]
{'loss': 0.6263, 'learning_rate': 1.442758620689655e-06, 'rewards/chosen': -1.7583050727844238, 'rewards/rejected': -1.6469628810882568, 'rewards/accuracies': 0.5, 'rewards/margins': -0.11134228110313416, 'policy_logps/rejected': -315.13677978515625, 'policy_logps/chosen': -358.6697998046875, 'referece_logps/rejected': -298.66717529296875, 'referece_logps/chosen': -341.08673095703125, 'logits/rejected': -1.2755036354064941, 'logits/chosen': -1.31207275390625, 'epoch': 6.16}

 68%|██████▊   | 16524/24156 [2:27:55<36:19:50, 17.14s/it]
[2024-04-05 17:33:06,833] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 16525/24156 [2:28:09<34:31:53, 16.29s/it]

 68%|██████▊   | 16526/24156 [2:28:26<34:30:02, 16.28s/it]


 68%|██████▊   | 16528/24156 [2:28:57<34:06:06, 16.09s/it]

 68%|██████▊   | 16529/24156 [2:29:14<34:50:40, 16.45s/it]

 68%|██████▊   | 16530/24156 [2:29:34<37:20:53, 17.63s/it]
[2024-04-05 17:34:31,924] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.295, 'learning_rate': 1.4620689655172414e-06, 'rewards/chosen': -2.3499467372894287, 'rewards/rejected': -4.0993194580078125, 'rewards/accuracies': 0.875, 'rewards/margins': 1.749373197555542, 'policy_logps/rejected': -567.8683471679688, 'policy_logps/chosen': -517.0711669921875, 'referece_logps/rejected': -526.8751831054688, 'referece_logps/chosen': -493.5716857910156, 'logits/rejected': -0.5185922384262085, 'logits/chosen': -0.45226386189460754, 'epoch': 6.16}

 68%|██████▊   | 16531/24156 [2:29:56<39:34:56, 18.69s/it]


 68%|██████▊   | 16533/24156 [2:30:36<41:13:28, 19.47s/it]

 68%|██████▊   | 16534/24156 [2:30:48<36:27:36, 17.22s/it]

 68%|██████▊   | 16535/24156 [2:31:08<38:29:45, 18.18s/it]

 68%|██████▊   | 16536/24156 [2:31:29<39:58:35, 18.89s/it]

 68%|██████▊   | 16537/24156 [2:31:51<41:44:46, 19.73s/it]
{'loss': 0.2661, 'learning_rate': 1.4813793103448276e-06, 'rewards/chosen': -1.5623369216918945, 'rewards/rejected': -4.524109363555908, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9617726802825928, 'policy_logps/rejected': -579.52685546875, 'policy_logps/chosen': -409.6275634765625, 'referece_logps/rejected': -534.2857055664062, 'referece_logps/chosen': -394.00421142578125, 'logits/rejected': -0.2812047600746155, 'logits/chosen': -0.10645988583564758, 'epoch': 6.16}


 68%|██████▊   | 16539/24156 [2:32:27<39:52:29, 18.85s/it]

 68%|██████▊   | 16540/24156 [2:32:47<40:44:25, 19.26s/it]

 68%|██████▊   | 16541/24156 [2:33:07<41:01:32, 19.39s/it]

 68%|██████▊   | 16542/24156 [2:33:29<42:46:40, 20.23s/it]
{'loss': 0.4993, 'learning_rate': 1.4951724137931035e-06, 'rewards/chosen': -1.7297511100769043, 'rewards/rejected': -2.8329687118530273, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1032177209854126, 'policy_logps/rejected': -426.464599609375, 'policy_logps/chosen': -360.5901794433594, 'referece_logps/rejected': -398.1349182128906, 'referece_logps/chosen': -343.29266357421875, 'logits/rejected': -0.4535704255104065, 'logits/chosen': -0.41802480816841125, 'epoch': 6.16}


 68%|██████▊   | 16544/24156 [2:34:05<40:19:29, 19.07s/it]
{'loss': 0.3402, 'learning_rate': 1.5006896551724137e-06, 'rewards/chosen': -2.1919405460357666, 'rewards/rejected': -2.8539326190948486, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6619921326637268, 'policy_logps/rejected': -338.26934814453125, 'policy_logps/chosen': -322.08026123046875, 'referece_logps/rejected': -309.72998046875, 'referece_logps/chosen': -300.1608581542969, 'logits/rejected': -0.4693962633609772, 'logits/chosen': -0.4209686815738678, 'epoch': 6.16}


 68%|██████▊   | 16546/24156 [2:34:31<34:35:08, 16.36s/it]
{'loss': 0.4129, 'learning_rate': 1.5062068965517241e-06, 'rewards/chosen': -1.5880088806152344, 'rewards/rejected': -2.4228427410125732, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8348336219787598, 'policy_logps/rejected': -475.6402587890625, 'policy_logps/chosen': -430.3392333984375, 'referece_logps/rejected': -451.41180419921875, 'referece_logps/chosen': -414.45916748046875, 'logits/rejected': -1.5244154930114746, 'logits/chosen': -1.3311080932617188, 'epoch': 6.16}

 69%|██████▊   | 16547/24156 [2:34:44<31:59:47, 15.14s/it]


 69%|██████▊   | 16549/24156 [2:35:07<27:58:51, 13.24s/it]
{'loss': 0.4385, 'learning_rate': 1.5144827586206896e-06, 'rewards/chosen': -1.7418408393859863, 'rewards/rejected': -3.0977628231048584, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3559218645095825, 'policy_logps/rejected': -280.3765869140625, 'policy_logps/chosen': -252.7289581298828, 'referece_logps/rejected': -249.39892578125, 'referece_logps/chosen': -235.31053161621094, 'logits/rejected': -0.9134039282798767, 'logits/chosen': -0.8175273537635803, 'epoch': 6.17}

 69%|██████▊   | 16550/24156 [2:35:17<26:20:36, 12.47s/it]

 69%|██████▊   | 16551/24156 [2:35:28<25:13:30, 11.94s/it]

 69%|██████▊   | 16552/24156 [2:35:42<26:08:27, 12.38s/it]

 69%|██████▊   | 16553/24156 [2:36:04<32:37:43, 15.45s/it]


 69%|██████▊   | 16555/24156 [2:36:33<32:11:15, 15.24s/it]

 69%|██████▊   | 16556/24156 [2:36:49<32:18:58, 15.31s/it]

 69%|██████▊   | 16557/24156 [2:37:09<35:19:45, 16.74s/it]
{'loss': 0.3805, 'learning_rate': 1.5365517241379309e-06, 'rewards/chosen': -2.010434865951538, 'rewards/rejected': -3.168046236038208, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1576114892959595, 'policy_logps/rejected': -413.2271423339844, 'policy_logps/chosen': -384.9013977050781, 'referece_logps/rejected': -381.5467224121094, 'referece_logps/chosen': -364.7970886230469, 'logits/rejected': 0.5309600234031677, 'logits/chosen': 0.4843079447746277, 'epoch': 6.17}

 69%|██████▊   | 16558/24156 [2:37:26<35:28:23, 16.81s/it]

 69%|██████▊   | 16559/24156 [2:37:44<36:21:52, 17.23s/it]


 69%|██████▊   | 16561/24156 [2:38:11<32:07:50, 15.23s/it]

 69%|██████▊   | 16562/24156 [2:38:31<35:06:51, 16.65s/it]

 69%|██████▊   | 16563/24156 [2:38:47<34:26:52, 16.33s/it]
{'loss': 0.4973, 'learning_rate': 1.5531034482758621e-06, 'rewards/chosen': -1.257859706878662, 'rewards/rejected': -3.4888172149658203, 'rewards/accuracies': 0.875, 'rewards/margins': 2.230957508087158, 'policy_logps/rejected': -407.85968017578125, 'policy_logps/chosen': -350.6966247558594, 'referece_logps/rejected': -372.97149658203125, 'referece_logps/chosen': -338.1180419921875, 'logits/rejected': -0.009350977838039398, 'logits/chosen': 0.05987437069416046, 'epoch': 6.17}

 69%|██████▊   | 16564/24156 [2:39:06<36:20:44, 17.23s/it]

 69%|██████▊   | 16565/24156 [2:39:28<39:10:41, 18.58s/it]


 69%|██████▊   | 16567/24156 [2:40:05<39:21:25, 18.67s/it]
{'loss': 0.3718, 'learning_rate': 1.5641379310344828e-06, 'rewards/chosen': -1.202789545059204, 'rewards/rejected': -2.2451159954071045, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0423264503479004, 'policy_logps/rejected': -318.8979797363281, 'policy_logps/chosen': -342.92230224609375, 'referece_logps/rejected': -296.4468688964844, 'referece_logps/chosen': -330.8943786621094, 'logits/rejected': -0.22457414865493774, 'logits/chosen': -0.27552467584609985, 'epoch': 6.17}


 69%|██████▊   | 16569/24156 [2:40:45<40:58:47, 19.44s/it]

 69%|██████▊   | 16570/24156 [2:41:04<40:18:12, 19.13s/it]

 69%|██████▊   | 16571/24156 [2:41:22<39:35:34, 18.79s/it]
{'loss': 0.3438, 'learning_rate': 1.5751724137931034e-06, 'rewards/chosen': -0.9879365563392639, 'rewards/rejected': -3.0845682621002197, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0966315269470215, 'policy_logps/rejected': -436.2364501953125, 'policy_logps/chosen': -366.3480224609375, 'referece_logps/rejected': -405.3907775878906, 'referece_logps/chosen': -356.4686584472656, 'logits/rejected': -0.7607898712158203, 'logits/chosen': -0.7586909532546997, 'epoch': 6.17}

 69%|██████▊   | 16572/24156 [2:41:38<38:22:30, 18.22s/it]


 69%|██████▊   | 16574/24156 [2:42:10<36:14:03, 17.20s/it]

 69%|██████▊   | 16575/24156 [2:42:23<33:58:39, 16.13s/it]
{'loss': 0.3189, 'learning_rate': 1.5862068965517242e-06, 'rewards/chosen': -1.1048626899719238, 'rewards/rejected': -3.9328348636627197, 'rewards/accuracies': 0.875, 'rewards/margins': 2.827972173690796, 'policy_logps/rejected': -385.30584716796875, 'policy_logps/chosen': -358.593994140625, 'referece_logps/rejected': -345.97747802734375, 'referece_logps/chosen': -347.54534912109375, 'logits/rejected': -0.7729511857032776, 'logits/chosen': -0.6444894671440125, 'epoch': 6.18}


 69%|██████▊   | 16577/24156 [2:42:52<31:23:54, 14.91s/it]

 69%|██████▊   | 16578/24156 [2:43:09<33:08:58, 15.75s/it]
{'loss': 0.4323, 'learning_rate': 1.5944827586206895e-06, 'rewards/chosen': -0.8045344948768616, 'rewards/rejected': -2.9809911251068115, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1764564514160156, 'policy_logps/rejected': -502.6621398925781, 'policy_logps/chosen': -482.80865478515625, 'referece_logps/rejected': -472.8522033691406, 'referece_logps/chosen': -474.7633056640625, 'logits/rejected': -0.34506645798683167, 'logits/chosen': -0.5298116207122803, 'epoch': 6.18}

 69%|██████▊   | 16579/24156 [2:43:22<31:29:34, 14.96s/it]

 69%|██████▊   | 16580/24156 [2:43:39<32:15:11, 15.33s/it]


 69%|██████▊   | 16582/24156 [2:44:10<33:19:38, 15.84s/it]

 69%|██████▊   | 16583/24156 [2:44:27<34:26:05, 16.37s/it]
{'loss': 0.3747, 'learning_rate': 1.6082758620689654e-06, 'rewards/chosen': -1.897566795349121, 'rewards/rejected': -2.42252516746521, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5249583125114441, 'policy_logps/rejected': -587.3890991210938, 'policy_logps/chosen': -600.2965087890625, 'referece_logps/rejected': -563.163818359375, 'referece_logps/chosen': -581.3209228515625, 'logits/rejected': -0.5378286242485046, 'logits/chosen': -0.6380215287208557, 'epoch': 6.18}

 69%|██████▊   | 16584/24156 [2:44:49<37:33:35, 17.86s/it]


 69%|██████▊   | 16586/24156 [2:45:26<37:39:26, 17.91s/it]

 69%|██████▊   | 16587/24156 [2:45:42<36:31:16, 17.37s/it]
{'loss': 0.4092, 'learning_rate': 1.6193103448275863e-06, 'rewards/chosen': -1.8540350198745728, 'rewards/rejected': -3.9444997310638428, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0904648303985596, 'policy_logps/rejected': -336.9285583496094, 'policy_logps/chosen': -351.910888671875, 'referece_logps/rejected': -297.48358154296875, 'referece_logps/chosen': -333.3704833984375, 'logits/rejected': -1.1247572898864746, 'logits/chosen': -1.0563178062438965, 'epoch': 6.18}

 69%|██████▊   | 16588/24156 [2:45:59<36:01:15, 17.13s/it]

 69%|██████▊   | 16589/24156 [2:46:21<39:23:15, 18.74s/it]

 69%|██████▊   | 16590/24156 [2:46:43<41:06:40, 19.56s/it]

 69%|██████▊   | 16591/24156 [2:47:02<41:08:13, 19.58s/it]


 69%|██████▊   | 16593/24156 [2:47:40<39:49:19, 18.96s/it]
{'loss': 0.3388, 'learning_rate': 1.635862068965517e-06, 'rewards/chosen': -2.635298490524292, 'rewards/rejected': -5.38840913772583, 'rewards/accuracies': 1.0, 'rewards/margins': 2.753110885620117, 'policy_logps/rejected': -327.3570251464844, 'policy_logps/chosen': -451.1129150390625, 'referece_logps/rejected': -273.4729309082031, 'referece_logps/chosen': -424.7599182128906, 'logits/rejected': 0.24805393815040588, 'logits/chosen': 0.17009270191192627, 'epoch': 6.18}

 69%|██████▊   | 16594/24156 [2:47:51<35:16:40, 16.79s/it]

 69%|██████▊   | 16595/24156 [2:48:13<38:04:25, 18.13s/it]

 69%|██████▊   | 16596/24156 [2:48:33<39:25:27, 18.77s/it]


 69%|██████▊   | 16598/24156 [2:49:10<39:16:15, 18.71s/it]

 69%|██████▊   | 16599/24156 [2:49:30<40:09:28, 19.13s/it]

 69%|██████▊   | 16600/24156 [2:49:50<40:31:02, 19.30s/it]

 69%|██████▊   | 16601/24156 [2:50:10<41:02:22, 19.56s/it]
{'loss': 0.3398, 'learning_rate': 1.6579310344827586e-06, 'rewards/chosen': -1.8378115892410278, 'rewards/rejected': -3.5966379642486572, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7588262557983398, 'policy_logps/rejected': -280.474853515625, 'policy_logps/chosen': -399.248046875, 'referece_logps/rejected': -244.5084991455078, 'referece_logps/chosen': -380.8699645996094, 'logits/rejected': -0.32758763432502747, 'logits/chosen': -0.4678643047809601, 'epoch': 6.19}


 69%|██████▊   | 16603/24156 [2:50:52<42:38:45, 20.33s/it]

 69%|██████▊   | 16604/24156 [2:51:04<37:20:33, 17.80s/it]
{'loss': 0.2944, 'learning_rate': 1.666206896551724e-06, 'rewards/chosen': -1.211461067199707, 'rewards/rejected': -2.575361967086792, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3639007806777954, 'policy_logps/rejected': -364.109619140625, 'policy_logps/chosen': -442.316162109375, 'referece_logps/rejected': -338.3559875488281, 'referece_logps/chosen': -430.2015380859375, 'logits/rejected': 0.7082569599151611, 'logits/chosen': 0.7785700559616089, 'epoch': 6.19}

 69%|██████▊   | 16605/24156 [2:51:25<39:36:20, 18.88s/it]

 69%|██████▊   | 16606/24156 [2:51:46<40:25:48, 19.28s/it]


 69%|██████▉   | 16608/24156 [2:52:20<37:43:32, 17.99s/it]

 69%|██████▉   | 16609/24156 [2:52:36<36:24:48, 17.37s/it]

 69%|██████▉   | 16610/24156 [2:52:48<32:55:35, 15.71s/it]
{'loss': 0.3991, 'learning_rate': 1.6827586206896551e-06, 'rewards/chosen': -1.141841173171997, 'rewards/rejected': -3.1662073135375977, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0243663787841797, 'policy_logps/rejected': -332.2220458984375, 'policy_logps/chosen': -388.0537109375, 'referece_logps/rejected': -300.55999755859375, 'referece_logps/chosen': -376.6352844238281, 'logits/rejected': -0.35987234115600586, 'logits/chosen': -0.22179722785949707, 'epoch': 6.19}


 69%|██████▉   | 16612/24156 [2:53:18<32:22:44, 15.45s/it]
{'loss': 0.4553, 'learning_rate': 1.6882758620689655e-06, 'rewards/chosen': -1.1422500610351562, 'rewards/rejected': -2.632101535797119, 'rewards/accuracies': 0.875, 'rewards/margins': 1.489851474761963, 'policy_logps/rejected': -301.36468505859375, 'policy_logps/chosen': -332.29364013671875, 'referece_logps/rejected': -275.04364013671875, 'referece_logps/chosen': -320.8711242675781, 'logits/rejected': -1.0725103616714478, 'logits/chosen': -0.9776108264923096, 'epoch': 6.19}

 69%|██████▉   | 16613/24156 [2:53:36<33:51:08, 16.16s/it]

 69%|██████▉   | 16614/24156 [2:53:47<30:44:51, 14.68s/it]


 69%|██████▉   | 16616/24156 [2:54:26<35:59:36, 17.19s/it]

 69%|██████▉   | 16617/24156 [2:54:40<33:53:36, 16.18s/it]

 69%|██████▉   | 16618/24156 [2:54:54<32:24:42, 15.48s/it]

 69%|██████▉   | 16619/24156 [2:55:14<35:29:29, 16.95s/it]
{'loss': 0.3381, 'learning_rate': 1.7075862068965517e-06, 'rewards/chosen': -1.3645378351211548, 'rewards/rejected': -3.6858832836151123, 'rewards/accuracies': 0.75, 'rewards/margins': 2.321345329284668, 'policy_logps/rejected': -395.7196960449219, 'policy_logps/chosen': -482.23748779296875, 'referece_logps/rejected': -358.8608703613281, 'referece_logps/chosen': -468.5920715332031, 'logits/rejected': -0.7335770130157471, 'logits/chosen': -0.6644808053970337, 'epoch': 6.19}


 69%|██████▉   | 16621/24156 [2:55:52<37:48:56, 18.07s/it]

 69%|██████▉   | 16622/24156 [2:56:10<37:40:02, 18.00s/it]
{'loss': 0.3592, 'learning_rate': 1.7158620689655172e-06, 'rewards/chosen': -1.2983670234680176, 'rewards/rejected': -2.595653772354126, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2972866296768188, 'policy_logps/rejected': -327.3457336425781, 'policy_logps/chosen': -315.6800231933594, 'referece_logps/rejected': -301.3891906738281, 'referece_logps/chosen': -302.69635009765625, 'logits/rejected': -0.3246905207633972, 'logits/chosen': -0.2546464204788208, 'epoch': 6.19}


 69%|██████▉   | 16624/24156 [2:56:40<35:24:02, 16.92s/it]
{'loss': 0.3632, 'learning_rate': 1.7213793103448276e-06, 'rewards/chosen': -1.5103765726089478, 'rewards/rejected': -2.907593250274658, 'rewards/accuracies': 0.625, 'rewards/margins': 1.397216558456421, 'policy_logps/rejected': -229.14805603027344, 'policy_logps/chosen': -394.7256164550781, 'referece_logps/rejected': -200.0721435546875, 'referece_logps/chosen': -379.6218566894531, 'logits/rejected': -0.669819712638855, 'logits/chosen': -0.8979969620704651, 'epoch': 6.19}

 69%|██████▉   | 16625/24156 [2:56:59<36:35:59, 17.50s/it]

 69%|██████▉   | 16626/24156 [2:57:19<38:16:31, 18.30s/it]

 69%|██████▉   | 16627/24156 [2:57:42<40:40:32, 19.45s/it]

 69%|██████▉   | 16628/24156 [2:58:01<40:50:02, 19.53s/it]

 69%|██████▉   | 16629/24156 [2:58:15<37:26:47, 17.91s/it]


 69%|██████▉   | 16631/24156 [2:58:45<33:05:28, 15.83s/it]
{'loss': 0.4802, 'learning_rate': 1.7406896551724137e-06, 'rewards/chosen': -1.367225170135498, 'rewards/rejected': -2.7425429821014404, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3753178119659424, 'policy_logps/rejected': -435.2860412597656, 'policy_logps/chosen': -401.781494140625, 'referece_logps/rejected': -407.8606262207031, 'referece_logps/chosen': -388.1092529296875, 'logits/rejected': 0.12716427445411682, 'logits/chosen': 0.3433113098144531, 'epoch': 6.2}


 69%|██████▉   | 16633/24156 [2:59:13<30:36:56, 14.65s/it]

 69%|██████▉   | 16634/24156 [2:59:24<28:40:05, 13.72s/it]

 69%|██████▉   | 16635/24156 [2:59:46<33:58:21, 16.26s/it]
{'loss': 0.4597, 'learning_rate': 1.7517241379310344e-06, 'rewards/chosen': -1.501805305480957, 'rewards/rejected': -2.4397714138031006, 'rewards/accuracies': 0.75, 'rewards/margins': 0.937965989112854, 'policy_logps/rejected': -330.29193115234375, 'policy_logps/chosen': -372.8445739746094, 'referece_logps/rejected': -305.894287109375, 'referece_logps/chosen': -357.8265380859375, 'logits/rejected': -1.0553085803985596, 'logits/chosen': -1.1295812129974365, 'epoch': 6.2}

 69%|██████▉   | 16636/24156 [3:00:01<32:53:24, 15.75s/it]


 69%|██████▉   | 16638/24156 [3:00:31<32:20:35, 15.49s/it]

 69%|██████▉   | 16639/24156 [3:00:49<34:04:45, 16.32s/it]
{'loss': 0.435, 'learning_rate': 1.7627586206896552e-06, 'rewards/chosen': -1.2381566762924194, 'rewards/rejected': -1.8817389011383057, 'rewards/accuracies': 0.75, 'rewards/margins': 0.643582284450531, 'policy_logps/rejected': -375.71649169921875, 'policy_logps/chosen': -414.1068115234375, 'referece_logps/rejected': -356.89910888671875, 'referece_logps/chosen': -401.7252502441406, 'logits/rejected': -0.8041895627975464, 'logits/chosen': -0.7756122350692749, 'epoch': 6.2}

 69%|██████▉   | 16640/24156 [3:01:10<36:47:36, 17.62s/it]

 69%|██████▉   | 16641/24156 [3:01:29<37:58:49, 18.19s/it]

 69%|██████▉   | 16642/24156 [3:01:47<37:53:25, 18.15s/it]


 69%|██████▉   | 16644/24156 [3:02:25<37:58:54, 18.20s/it]
{'loss': 0.3541, 'learning_rate': 1.776551724137931e-06, 'rewards/chosen': -1.0632266998291016, 'rewards/rejected': -2.293938636779785, 'rewards/accuracies': 0.875, 'rewards/margins': 1.230711817741394, 'policy_logps/rejected': -384.91522216796875, 'policy_logps/chosen': -413.32501220703125, 'referece_logps/rejected': -361.9758605957031, 'referece_logps/chosen': -402.6927795410156, 'logits/rejected': -0.7373006939888, 'logits/chosen': -0.9822409152984619, 'epoch': 6.2}
[2024-04-05 18:07:42,912] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16645/24156 [3:02:45<39:23:31, 18.88s/it]


 69%|██████▉   | 16647/24156 [3:03:23<39:09:34, 18.77s/it]
{'loss': 0.3472, 'learning_rate': 1.7848275862068964e-06, 'rewards/chosen': -1.9163413047790527, 'rewards/rejected': -3.6262354850769043, 'rewards/accuracies': 1.0, 'rewards/margins': 1.709894061088562, 'policy_logps/rejected': -414.141357421875, 'policy_logps/chosen': -328.735595703125, 'referece_logps/rejected': -377.87896728515625, 'referece_logps/chosen': -309.57220458984375, 'logits/rejected': -0.5457642674446106, 'logits/chosen': -0.48584628105163574, 'epoch': 6.2}

 69%|██████▉   | 16648/24156 [3:03:39<37:10:54, 17.83s/it]

 69%|██████▉   | 16649/24156 [3:03:56<36:33:52, 17.53s/it]

 69%|██████▉   | 16650/24156 [3:04:17<38:49:31, 18.62s/it]

 69%|██████▉   | 16651/24156 [3:04:34<38:14:19, 18.34s/it]


 69%|██████▉   | 16653/24156 [3:05:09<37:50:17, 18.16s/it]
{'loss': 0.2881, 'learning_rate': 1.8013793103448277e-06, 'rewards/chosen': -1.728829026222229, 'rewards/rejected': -3.574556350708008, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8457274436950684, 'policy_logps/rejected': -389.6512451171875, 'policy_logps/chosen': -331.2375793457031, 'referece_logps/rejected': -353.90570068359375, 'referece_logps/chosen': -313.9493103027344, 'logits/rejected': -0.3353210389614105, 'logits/chosen': -0.19750049710273743, 'epoch': 6.2}

 69%|██████▉   | 16654/24156 [3:05:26<37:08:48, 17.83s/it]

 69%|██████▉   | 16655/24156 [3:05:41<35:04:50, 16.84s/it]

 69%|██████▉   | 16656/24156 [3:06:03<38:33:16, 18.51s/it]

 69%|██████▉   | 16657/24156 [3:06:18<36:17:32, 17.42s/it]

 69%|██████▉   | 16658/24156 [3:06:36<36:31:16, 17.53s/it]


 69%|██████▉   | 16660/24156 [3:07:21<42:12:36, 20.27s/it]

 69%|██████▉   | 16661/24156 [3:07:37<39:37:04, 19.03s/it]
{'loss': 0.461, 'learning_rate': 1.823448275862069e-06, 'rewards/chosen': -1.5540310144424438, 'rewards/rejected': -2.4762918949127197, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9222607612609863, 'policy_logps/rejected': -351.9623718261719, 'policy_logps/chosen': -424.09002685546875, 'referece_logps/rejected': -327.1994934082031, 'referece_logps/chosen': -408.5496826171875, 'logits/rejected': -0.02700820192694664, 'logits/chosen': -0.11307947337627411, 'epoch': 6.21}

 69%|██████▉   | 16662/24156 [3:07:55<38:51:57, 18.67s/it]

 69%|██████▉   | 16663/24156 [3:08:09<35:40:29, 17.14s/it]

 69%|██████▉   | 16664/24156 [3:08:28<37:12:41, 17.88s/it]

 69%|██████▉   | 16665/24156 [3:08:39<32:40:18, 15.70s/it]

 69%|██████▉   | 16666/24156 [3:08:56<33:43:33, 16.21s/it]

 69%|██████▉   | 16667/24156 [3:09:18<37:17:22, 17.93s/it]

 69%|██████▉   | 16668/24156 [3:09:33<34:57:10, 16.80s/it]

 69%|██████▉   | 16669/24156 [3:09:47<33:29:48, 16.11s/it]

 69%|██████▉   | 16670/24156 [3:09:58<30:13:00, 14.53s/it]

 69%|██████▉   | 16671/24156 [3:10:14<31:30:44, 15.16s/it]


 69%|██████▉   | 16673/24156 [3:10:45<32:01:39, 15.41s/it]
{'loss': 0.3196, 'learning_rate': 1.856551724137931e-06, 'rewards/chosen': -0.8567878603935242, 'rewards/rejected': -2.5452957153320312, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6885076761245728, 'policy_logps/rejected': -694.3661499023438, 'policy_logps/chosen': -326.56744384765625, 'referece_logps/rejected': -668.9130859375, 'referece_logps/chosen': -317.99957275390625, 'logits/rejected': -0.9016704559326172, 'logits/chosen': -0.8196703195571899, 'epoch': 6.21}


 69%|██████▉   | 16675/24156 [3:11:15<31:24:49, 15.12s/it]
{'loss': 0.3895, 'learning_rate': 1.8620689655172412e-06, 'rewards/chosen': -1.0659785270690918, 'rewards/rejected': -2.961909532546997, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8959310054779053, 'policy_logps/rejected': -268.42474365234375, 'policy_logps/chosen': -361.7686462402344, 'referece_logps/rejected': -238.80564880371094, 'referece_logps/chosen': -351.10888671875, 'logits/rejected': -0.7545909881591797, 'logits/chosen': -0.7773506045341492, 'epoch': 6.21}
[2024-04-05 18:16:26,655] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16676/24156 [3:11:29<30:31:22, 14.69s/it]

 69%|██████▉   | 16677/24156 [3:11:51<34:53:35, 16.80s/it]

 69%|██████▉   | 16678/24156 [3:12:05<32:56:54, 15.86s/it]

 69%|██████▉   | 16679/24156 [3:12:22<33:46:35, 16.26s/it]

 69%|██████▉   | 16680/24156 [3:12:37<33:10:22, 15.97s/it]


 69%|██████▉   | 16682/24156 [3:13:16<36:47:34, 17.72s/it]
[2024-04-05 18:18:13,152] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4604, 'learning_rate': 1.8813793103448273e-06, 'rewards/chosen': -1.0297060012817383, 'rewards/rejected': -2.9704031944274902, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9406969547271729, 'policy_logps/rejected': -318.9419860839844, 'policy_logps/chosen': -336.1485595703125, 'referece_logps/rejected': -289.23797607421875, 'referece_logps/chosen': -325.8514709472656, 'logits/rejected': -0.2910672426223755, 'logits/chosen': -0.22184187173843384, 'epoch': 6.22}

 69%|██████▉   | 16683/24156 [3:13:37<38:56:08, 18.76s/it]

 69%|██████▉   | 16684/24156 [3:13:53<37:22:23, 18.01s/it]

 69%|██████▉   | 16685/24156 [3:14:11<37:00:29, 17.83s/it]

 69%|██████▉   | 16686/24156 [3:14:28<36:57:10, 17.81s/it]

 69%|██████▉   | 16687/24156 [3:14:44<35:33:31, 17.14s/it]


 69%|██████▉   | 16689/24156 [3:15:16<34:20:03, 16.55s/it]
{'loss': 0.3764, 'learning_rate': 1.9006896551724137e-06, 'rewards/chosen': -1.3053771257400513, 'rewards/rejected': -3.411653757095337, 'rewards/accuracies': 1.0, 'rewards/margins': 2.106276512145996, 'policy_logps/rejected': -309.2666320800781, 'policy_logps/chosen': -326.72723388671875, 'referece_logps/rejected': -275.15008544921875, 'referece_logps/chosen': -313.6734924316406, 'logits/rejected': -0.2135336995124817, 'logits/chosen': -0.26610222458839417, 'epoch': 6.22}

 69%|██████▉   | 16690/24156 [3:15:36<36:53:29, 17.79s/it]

 69%|██████▉   | 16691/24156 [3:15:52<35:47:55, 17.26s/it]


 69%|██████▉   | 16693/24156 [3:16:32<38:52:51, 18.76s/it]
[2024-04-05 18:21:29,332] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3911, 'learning_rate': 1.9117241379310345e-06, 'rewards/chosen': -2.155215263366699, 'rewards/rejected': -3.9434425830841064, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7882274389266968, 'policy_logps/rejected': -458.12298583984375, 'policy_logps/chosen': -493.5760498046875, 'referece_logps/rejected': -418.6885681152344, 'referece_logps/chosen': -472.02392578125, 'logits/rejected': -0.28942686319351196, 'logits/chosen': -0.4150986969470978, 'epoch': 6.22}

 69%|██████▉   | 16694/24156 [3:16:52<39:36:18, 19.11s/it]

 69%|██████▉   | 16695/24156 [3:17:11<39:45:57, 19.19s/it]

 69%|██████▉   | 16696/24156 [3:17:28<38:21:45, 18.51s/it]

 69%|██████▉   | 16697/24156 [3:17:51<41:18:45, 19.94s/it]

 69%|██████▉   | 16698/24156 [3:18:10<40:39:08, 19.62s/it]


 69%|██████▉   | 16700/24156 [3:18:52<42:21:35, 20.45s/it]
{'loss': 0.2948, 'learning_rate': 1.9310344827586207e-06, 'rewards/chosen': -1.925734043121338, 'rewards/rejected': -4.660456657409668, 'rewards/accuracies': 1.0, 'rewards/margins': 2.734722375869751, 'policy_logps/rejected': -485.050537109375, 'policy_logps/chosen': -409.8017883300781, 'referece_logps/rejected': -438.44598388671875, 'referece_logps/chosen': -390.5444641113281, 'logits/rejected': 0.17388781905174255, 'logits/chosen': 0.11347407102584839, 'epoch': 6.22}

 69%|██████▉   | 16701/24156 [3:19:03<36:22:25, 17.56s/it]

 69%|██████▉   | 16702/24156 [3:19:18<35:01:10, 16.91s/it]


 69%|██████▉   | 16704/24156 [3:19:48<33:10:58, 16.03s/it]
{'loss': 0.2918, 'learning_rate': 1.9420689655172415e-06, 'rewards/chosen': -1.8959304094314575, 'rewards/rejected': -4.1609697341918945, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2650392055511475, 'policy_logps/rejected': -339.5968933105469, 'policy_logps/chosen': -457.7733154296875, 'referece_logps/rejected': -297.9872131347656, 'referece_logps/chosen': -438.8139953613281, 'logits/rejected': -0.412691593170166, 'logits/chosen': -0.3727925419807434, 'epoch': 6.22}

 69%|██████▉   | 16705/24156 [3:20:06<34:00:51, 16.43s/it]

 69%|██████▉   | 16706/24156 [3:20:22<34:01:35, 16.44s/it]

 69%|██████▉   | 16707/24156 [3:20:42<36:07:42, 17.46s/it]

 69%|██████▉   | 16708/24156 [3:20:55<33:28:32, 16.18s/it]
[2024-04-05 18:26:14,117] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16709/24156 [3:21:17<36:51:57, 17.82s/it]

 69%|██████▉   | 16710/24156 [3:21:38<39:19:55, 19.02s/it]
[2024-04-05 18:26:55,485] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16711/24156 [3:21:58<39:40:01, 19.18s/it]

 69%|██████▉   | 16712/24156 [3:22:20<41:29:54, 20.07s/it]

 69%|██████▉   | 16713/24156 [3:22:37<39:20:50, 19.03s/it]

 69%|██████▉   | 16714/24156 [3:22:54<38:21:21, 18.55s/it]

 69%|██████▉   | 16715/24156 [3:23:06<34:08:26, 16.52s/it]

 69%|██████▉   | 16716/24156 [3:23:25<35:28:06, 17.16s/it]

 69%|██████▉   | 16717/24156 [3:23:45<37:41:43, 18.24s/it]

 69%|██████▉   | 16718/24156 [3:24:05<38:31:02, 18.64s/it]

 69%|██████▉   | 16719/24156 [3:24:28<41:21:14, 20.02s/it]

 69%|██████▉   | 16720/24156 [3:24:46<39:57:01, 19.34s/it]

 69%|██████▉   | 16721/24156 [3:25:08<41:29:34, 20.09s/it]

 69%|██████▉   | 16722/24156 [3:25:20<36:47:46, 17.82s/it]

 69%|██████▉   | 16723/24156 [3:25:39<37:24:18, 18.12s/it]

 69%|██████▉   | 16724/24156 [3:25:57<37:13:30, 18.03s/it]

 69%|██████▉   | 16725/24156 [3:26:11<34:45:48, 16.84s/it]

 69%|██████▉   | 16726/24156 [3:26:31<36:45:19, 17.81s/it]

 69%|██████▉   | 16727/24156 [3:26:47<35:46:20, 17.33s/it]

 69%|██████▉   | 16728/24156 [3:26:59<32:11:27, 15.60s/it]

 69%|██████▉   | 16729/24156 [3:27:16<33:11:39, 16.09s/it]

 69%|██████▉   | 16730/24156 [3:27:27<29:55:02, 14.50s/it]
[2024-04-05 18:32:46,516] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16731/24156 [3:27:49<34:37:53, 16.79s/it]

 69%|██████▉   | 16732/24156 [3:28:09<36:53:20, 17.89s/it]

 69%|██████▉   | 16733/24156 [3:28:22<33:29:36, 16.24s/it]

 69%|██████▉   | 16734/24156 [3:28:40<34:43:44, 16.85s/it]

 69%|██████▉   | 16735/24156 [3:28:55<33:35:18, 16.29s/it]

 69%|██████▉   | 16736/24156 [3:29:08<31:14:36, 15.16s/it]

 69%|██████▉   | 16737/24156 [3:29:29<35:13:15, 17.09s/it]

 69%|██████▉   | 16738/24156 [3:29:41<32:05:20, 15.57s/it]

 69%|██████▉   | 16739/24156 [3:29:53<30:00:24, 14.56s/it]


 69%|██████▉   | 16741/24156 [3:30:33<35:17:37, 17.14s/it]
{'loss': 0.333, 'learning_rate': 1.9999976989402254e-06, 'rewards/chosen': -1.7030657529830933, 'rewards/rejected': -3.9589805603027344, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2559149265289307, 'policy_logps/rejected': -294.8009033203125, 'policy_logps/chosen': -345.12786865234375, 'referece_logps/rejected': -255.21109008789062, 'referece_logps/chosen': -328.09722900390625, 'logits/rejected': -0.5711635947227478, 'logits/chosen': -0.5623902082443237, 'epoch': 6.24}

 69%|██████▉   | 16742/24156 [3:30:55<38:09:28, 18.53s/it]

 69%|██████▉   | 16743/24156 [3:31:15<39:29:48, 19.18s/it]

 69%|██████▉   | 16744/24156 [3:31:30<36:51:33, 17.90s/it]

 69%|██████▉   | 16745/24156 [3:31:47<35:59:18, 17.48s/it]


 69%|██████▉   | 16747/24156 [3:32:25<37:51:48, 18.40s/it]
{'loss': 0.4487, 'learning_rate': 1.99999564956035e-06, 'rewards/chosen': -1.813410997390747, 'rewards/rejected': -2.8722941875457764, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0588833093643188, 'policy_logps/rejected': -268.2236633300781, 'policy_logps/chosen': -350.4327087402344, 'referece_logps/rejected': -239.50070190429688, 'referece_logps/chosen': -332.29864501953125, 'logits/rejected': -1.6150436401367188, 'logits/chosen': -1.8454480171203613, 'epoch': 6.24}

 69%|██████▉   | 16748/24156 [3:32:41<36:34:39, 17.78s/it]

 69%|██████▉   | 16749/24156 [3:32:53<33:02:01, 16.06s/it]

 69%|██████▉   | 16750/24156 [3:33:16<37:16:14, 18.12s/it]

 69%|██████▉   | 16751/24156 [3:33:28<33:42:48, 16.39s/it]

 69%|██████▉   | 16752/24156 [3:33:41<31:34:13, 15.35s/it]

 69%|██████▉   | 16753/24156 [3:33:53<29:21:53, 14.28s/it]

 69%|██████▉   | 16754/24156 [3:34:14<33:10:04, 16.13s/it]

 69%|██████▉   | 16755/24156 [3:34:30<33:32:08, 16.31s/it]

 69%|██████▉   | 16756/24156 [3:34:52<36:46:38, 17.89s/it]

 69%|██████▉   | 16757/24156 [3:35:07<34:56:37, 17.00s/it]

 69%|██████▉   | 16758/24156 [3:35:21<33:27:28, 16.28s/it]

 69%|██████▉   | 16759/24156 [3:35:41<35:37:01, 17.33s/it]

 69%|██████▉   | 16760/24156 [3:36:02<37:50:42, 18.42s/it]

 69%|██████▉   | 16761/24156 [3:36:20<37:31:02, 18.26s/it]

 69%|██████▉   | 16762/24156 [3:36:37<36:44:45, 17.89s/it]

 69%|██████▉   | 16763/24156 [3:36:52<34:50:53, 16.97s/it]

 69%|██████▉   | 16764/24156 [3:37:12<36:43:37, 17.89s/it]
[2024-04-05 18:42:30,685] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16765/24156 [3:37:33<38:45:57, 18.88s/it]
[2024-04-05 18:42:49,058] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 69%|██████▉   | 16767/24156 [3:38:09<37:47:25, 18.41s/it]

 69%|██████▉   | 16768/24156 [3:38:24<35:43:59, 17.41s/it]

 69%|██████▉   | 16769/24156 [3:38:46<38:12:49, 18.62s/it]

 69%|██████▉   | 16770/24156 [3:39:03<37:08:22, 18.10s/it]
{'loss': 0.4968, 'learning_rate': 1.999981798305881e-06, 'rewards/chosen': -2.2033376693725586, 'rewards/rejected': -3.6613357067108154, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4579977989196777, 'policy_logps/rejected': -408.20965576171875, 'policy_logps/chosen': -401.6418762207031, 'referece_logps/rejected': -371.5962829589844, 'referece_logps/chosen': -379.6085205078125, 'logits/rejected': -0.9805111885070801, 'logits/chosen': -1.0783988237380981, 'epoch': 6.25}


 69%|██████▉   | 16772/24156 [3:39:34<34:59:53, 17.06s/it]

 69%|██████▉   | 16773/24156 [3:39:55<37:21:22, 18.22s/it]
[2024-04-05 18:44:52,289] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16774/24156 [3:40:09<34:36:33, 16.88s/it]
[2024-04-05 18:45:06,047] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16775/24156 [3:40:29<36:40:42, 17.89s/it]

 69%|██████▉   | 16776/24156 [3:40:44<34:59:22, 17.07s/it]

 69%|██████▉   | 16777/24156 [3:40:56<32:04:34, 15.65s/it]

 69%|██████▉   | 16778/24156 [3:41:07<29:07:53, 14.21s/it]

 69%|██████▉   | 16779/24156 [3:41:19<27:42:43, 13.52s/it]

 69%|██████▉   | 16780/24156 [3:41:39<31:37:09, 15.43s/it]
[2024-04-05 18:46:36,451] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 16781/24156 [3:41:58<33:55:10, 16.56s/it]

 69%|██████▉   | 16782/24156 [3:42:09<30:17:08, 14.79s/it]

 69%|██████▉   | 16783/24156 [3:42:22<29:23:24, 14.35s/it]

 69%|██████▉   | 16784/24156 [3:42:42<33:04:55, 16.16s/it]

 69%|██████▉   | 16785/24156 [3:42:55<30:50:45, 15.07s/it]

 69%|██████▉   | 16786/24156 [3:43:09<30:07:18, 14.71s/it]

 69%|██████▉   | 16787/24156 [3:43:23<29:27:47, 14.39s/it]

 69%|██████▉   | 16788/24156 [3:43:42<32:35:58, 15.93s/it]

 70%|██████▉   | 16789/24156 [3:43:58<32:39:36, 15.96s/it]

 70%|██████▉   | 16790/24156 [3:44:21<37:02:34, 18.10s/it]

 70%|██████▉   | 16791/24156 [3:44:40<37:25:39, 18.29s/it]

 70%|██████▉   | 16792/24156 [3:44:55<35:36:48, 17.41s/it]

 70%|██████▉   | 16793/24156 [3:45:06<31:47:46, 15.55s/it]

 70%|██████▉   | 16794/24156 [3:45:24<33:08:58, 16.21s/it]

 70%|██████▉   | 16795/24156 [3:45:42<33:48:08, 16.53s/it]

 70%|██████▉   | 16796/24156 [3:45:59<34:13:41, 16.74s/it]

 70%|██████▉   | 16797/24156 [3:46:18<36:02:03, 17.63s/it]

 70%|██████▉   | 16798/24156 [3:46:34<34:44:48, 17.00s/it]

 70%|██████▉   | 16799/24156 [3:46:53<36:01:48, 17.63s/it]

 70%|██████▉   | 16800/24156 [3:47:11<36:29:42, 17.86s/it]

 70%|██████▉   | 16801/24156 [3:47:31<37:11:43, 18.21s/it]

 70%|██████▉   | 16802/24156 [3:47:47<36:19:08, 17.78s/it]

 70%|██████▉   | 16803/24156 [3:48:04<35:56:01, 17.59s/it]

 70%|██████▉   | 16804/24156 [3:48:22<35:57:35, 17.61s/it]

 70%|██████▉   | 16805/24156 [3:48:45<39:18:45, 19.25s/it]

 70%|██████▉   | 16806/24156 [3:49:01<37:17:52, 18.27s/it]

 70%|██████▉   | 16807/24156 [3:49:23<39:21:32, 19.28s/it]

 70%|██████▉   | 16808/24156 [3:49:38<37:04:36, 18.17s/it]

 70%|██████▉   | 16809/24156 [3:49:52<34:03:13, 16.69s/it]

 70%|██████▉   | 16810/24156 [3:50:04<31:32:22, 15.46s/it]

 70%|██████▉   | 16811/24156 [3:50:18<30:32:35, 14.97s/it]

 70%|██████▉   | 16812/24156 [3:50:36<32:35:21, 15.98s/it]
[2024-04-05 18:55:33,822] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 16813/24156 [3:50:51<31:46:45, 15.58s/it]

 70%|██████▉   | 16814/24156 [3:51:09<33:18:29, 16.33s/it]

 70%|██████▉   | 16815/24156 [3:51:26<33:50:44, 16.60s/it]

 70%|██████▉   | 16816/24156 [3:51:46<35:52:02, 17.59s/it]

 70%|██████▉   | 16817/24156 [3:52:07<37:52:53, 18.58s/it]

 70%|██████▉   | 16818/24156 [3:52:24<36:36:07, 17.96s/it]

 70%|██████▉   | 16819/24156 [3:52:40<35:29:06, 17.41s/it]

 70%|██████▉   | 16820/24156 [3:52:50<31:24:28, 15.41s/it]

 70%|██████▉   | 16821/24156 [3:53:11<34:38:22, 17.00s/it]

 70%|██████▉   | 16822/24156 [3:53:27<33:51:44, 16.62s/it]

 70%|██████▉   | 16823/24156 [3:53:42<32:41:36, 16.05s/it]

 70%|██████▉   | 16824/24156 [3:54:02<35:16:58, 17.32s/it]

 70%|██████▉   | 16825/24156 [3:54:18<34:17:20, 16.84s/it]

 70%|██████▉   | 16826/24156 [3:54:35<34:20:26, 16.87s/it]

 70%|██████▉   | 16827/24156 [3:54:51<34:11:56, 16.80s/it]

 70%|██████▉   | 16828/24156 [3:55:09<34:53:36, 17.14s/it]

 70%|██████▉   | 16829/24156 [3:55:24<33:37:11, 16.52s/it]

 70%|██████▉   | 16830/24156 [3:55:39<32:32:09, 15.99s/it]

 70%|██████▉   | 16831/24156 [3:55:59<35:09:39, 17.28s/it]

 70%|██████▉   | 16832/24156 [3:56:15<34:05:14, 16.76s/it]

 70%|██████▉   | 16833/24156 [3:56:27<31:29:13, 15.48s/it]

 70%|██████▉   | 16834/24156 [3:56:43<31:34:13, 15.52s/it]

 70%|██████▉   | 16835/24156 [3:57:05<35:38:14, 17.52s/it]

 70%|██████▉   | 16836/24156 [3:57:25<37:09:16, 18.27s/it]

 70%|██████▉   | 16837/24156 [3:57:36<32:48:28, 16.14s/it]

 70%|██████▉   | 16838/24156 [3:57:53<32:55:11, 16.19s/it]

 70%|██████▉   | 16839/24156 [3:58:14<36:07:04, 17.77s/it]

 70%|██████▉   | 16840/24156 [3:58:25<32:04:05, 15.78s/it]

 70%|██████▉   | 16841/24156 [3:58:36<29:16:34, 14.41s/it]

 70%|██████▉   | 16842/24156 [3:58:48<27:26:34, 13.51s/it]

 70%|██████▉   | 16843/24156 [3:59:01<27:20:31, 13.46s/it]

 70%|██████▉   | 16844/24156 [3:59:12<25:37:23, 12.62s/it]

 70%|██████▉   | 16845/24156 [3:59:30<29:02:10, 14.30s/it]

 70%|██████▉   | 16846/24156 [3:59:50<32:17:35, 15.90s/it]

 70%|██████▉   | 16847/24156 [4:00:06<32:37:31, 16.07s/it]

 70%|██████▉   | 16848/24156 [4:00:24<33:45:49, 16.63s/it]

 70%|██████▉   | 16849/24156 [4:00:35<30:30:33, 15.03s/it]

 70%|██████▉   | 16850/24156 [4:00:47<28:20:52, 13.97s/it]

 70%|██████▉   | 16851/24156 [4:01:09<33:14:23, 16.38s/it]

 70%|██████▉   | 16852/24156 [4:01:25<32:56:36, 16.24s/it]

 70%|██████▉   | 16853/24156 [4:01:39<31:55:35, 15.74s/it]

 70%|██████▉   | 16854/24156 [4:01:51<29:32:58, 14.57s/it]

 70%|██████▉   | 16855/24156 [4:02:13<33:41:46, 16.62s/it]

 70%|██████▉   | 16856/24156 [4:02:24<30:37:07, 15.10s/it]

 70%|██████▉   | 16857/24156 [4:02:43<33:03:44, 16.31s/it]
{'loss': 0.5038, 'learning_rate': 1.999843388147098e-06, 'rewards/chosen': -1.7045859098434448, 'rewards/rejected': -2.9008312225341797, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1962449550628662, 'policy_logps/rejected': -534.0191040039062, 'policy_logps/chosen': -382.9418029785156, 'referece_logps/rejected': -505.01080322265625, 'referece_logps/chosen': -365.89593505859375, 'logits/rejected': -0.4459671378135681, 'logits/chosen': -0.39541977643966675, 'epoch': 6.28}

 70%|██████▉   | 16858/24156 [4:02:53<29:15:10, 14.43s/it]


 70%|██████▉   | 16860/24156 [4:03:22<28:25:10, 14.02s/it]

 70%|██████▉   | 16861/24156 [4:03:38<29:31:42, 14.57s/it]

 70%|██████▉   | 16862/24156 [4:03:58<32:40:53, 16.13s/it]

 70%|██████▉   | 16863/24156 [4:04:14<32:58:12, 16.27s/it]

 70%|██████▉   | 16864/24156 [4:04:26<30:16:04, 14.94s/it]

 70%|██████▉   | 16865/24156 [4:04:45<32:58:03, 16.28s/it]

 70%|██████▉   | 16866/24156 [4:05:02<32:56:24, 16.27s/it]

 70%|██████▉   | 16867/24156 [4:05:25<37:07:03, 18.33s/it]
[2024-04-05 19:10:22,251] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 16868/24156 [4:05:37<33:32:00, 16.56s/it]

 70%|██████▉   | 16869/24156 [4:05:56<34:40:54, 17.13s/it]

 70%|██████▉   | 16870/24156 [4:06:16<36:35:40, 18.08s/it]

 70%|██████▉   | 16871/24156 [4:06:37<38:24:00, 18.98s/it]
[2024-04-05 19:11:34,510] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 16872/24156 [4:06:58<39:53:33, 19.72s/it]
[2024-04-05 19:11:55,954] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 16873/24156 [4:07:17<39:00:10, 19.28s/it]
{'loss': 0.4336, 'learning_rate': 1.999803121958055e-06, 'rewards/chosen': -2.088416814804077, 'rewards/rejected': -2.628967046737671, 'rewards/accuracies': 0.5, 'rewards/margins': 0.540550172328949, 'policy_logps/rejected': -283.1585388183594, 'policy_logps/chosen': -518.1861572265625, 'referece_logps/rejected': -256.8688659667969, 'referece_logps/chosen': -497.302001953125, 'logits/rejected': 0.05547672510147095, 'logits/chosen': 0.08362576365470886, 'epoch': 6.29}

 70%|██████▉   | 16874/24156 [4:07:29<35:02:13, 17.32s/it]


 70%|██████▉   | 16876/24156 [4:08:08<36:39:35, 18.13s/it]
{'loss': 0.3429, 'learning_rate': 1.999795059797202e-06, 'rewards/chosen': -2.0422513484954834, 'rewards/rejected': -4.061371803283691, 'rewards/accuracies': 0.75, 'rewards/margins': 2.019120693206787, 'policy_logps/rejected': -469.1758117675781, 'policy_logps/chosen': -352.46221923828125, 'referece_logps/rejected': -428.56207275390625, 'referece_logps/chosen': -332.03973388671875, 'logits/rejected': -0.1887614130973816, 'logits/chosen': -0.02809910476207733, 'epoch': 6.29}
[2024-04-05 19:13:23,600] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|██████▉   | 16878/24156 [4:08:47<38:00:27, 18.80s/it]
{'loss': 0.2793, 'learning_rate': 1.999789595156459e-06, 'rewards/chosen': -1.3374420404434204, 'rewards/rejected': -3.6186578273773193, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2812156677246094, 'policy_logps/rejected': -394.68682861328125, 'policy_logps/chosen': -440.4197692871094, 'referece_logps/rejected': -358.500244140625, 'referece_logps/chosen': -427.04534912109375, 'logits/rejected': -0.3540568947792053, 'logits/chosen': -0.3733959496021271, 'epoch': 6.29}


 70%|██████▉   | 16880/24156 [4:09:23<37:38:19, 18.62s/it]

 70%|██████▉   | 16881/24156 [4:09:43<38:44:06, 19.17s/it]

 70%|██████▉   | 16882/24156 [4:10:01<37:41:34, 18.65s/it]

 70%|██████▉   | 16883/24156 [4:10:16<35:43:48, 17.69s/it]
{'loss': 0.3854, 'learning_rate': 1.9997756190235315e-06, 'rewards/chosen': -1.563952922821045, 'rewards/rejected': -3.3991217613220215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8351688385009766, 'policy_logps/rejected': -385.4773864746094, 'policy_logps/chosen': -410.31683349609375, 'referece_logps/rejected': -351.4862060546875, 'referece_logps/chosen': -394.67730712890625, 'logits/rejected': -0.9224693775177002, 'logits/chosen': -0.9881858229637146, 'epoch': 6.29}


 70%|██████▉   | 16885/24156 [4:10:47<33:04:32, 16.38s/it]
{'loss': 0.362, 'learning_rate': 1.999769902758988e-06, 'rewards/chosen': -1.4472675323486328, 'rewards/rejected': -2.754436731338501, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3071695566177368, 'policy_logps/rejected': -350.0186462402344, 'policy_logps/chosen': -446.2814025878906, 'referece_logps/rejected': -322.47430419921875, 'referece_logps/chosen': -431.8087463378906, 'logits/rejected': -0.6490849256515503, 'logits/chosen': -0.8583645820617676, 'epoch': 6.29}

 70%|██████▉   | 16886/24156 [4:11:06<34:51:39, 17.26s/it]

 70%|██████▉   | 16887/24156 [4:11:20<32:53:18, 16.29s/it]


 70%|██████▉   | 16889/24156 [4:11:43<27:45:03, 13.75s/it]
{'loss': 0.4162, 'learning_rate': 1.9997582545555197e-06, 'rewards/chosen': -1.4402393102645874, 'rewards/rejected': -2.520005464553833, 'rewards/accuracies': 0.75, 'rewards/margins': 1.079766035079956, 'policy_logps/rejected': -434.0430603027344, 'policy_logps/chosen': -396.4765319824219, 'referece_logps/rejected': -408.8430480957031, 'referece_logps/chosen': -382.0741271972656, 'logits/rejected': -0.23805725574493408, 'logits/chosen': -0.18770937621593475, 'epoch': 6.29}


 70%|██████▉   | 16891/24156 [4:12:15<29:21:58, 14.55s/it]
{'loss': 0.4163, 'learning_rate': 1.9997523226174323e-06, 'rewards/chosen': -1.536131739616394, 'rewards/rejected': -3.89621901512146, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3600869178771973, 'policy_logps/rejected': -432.2574768066406, 'policy_logps/chosen': -476.1527099609375, 'referece_logps/rejected': -393.2953186035156, 'referece_logps/chosen': -460.7914123535156, 'logits/rejected': -0.3835631310939789, 'logits/chosen': -0.6054995059967041, 'epoch': 6.29}


 70%|██████▉   | 16893/24156 [4:12:45<29:46:34, 14.76s/it]
{'loss': 0.3083, 'learning_rate': 1.9997463187890097e-06, 'rewards/chosen': -1.2620022296905518, 'rewards/rejected': -2.547524929046631, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2855230569839478, 'policy_logps/rejected': -259.2821350097656, 'policy_logps/chosen': -447.2842102050781, 'referece_logps/rejected': -233.806884765625, 'referece_logps/chosen': -434.6641845703125, 'logits/rejected': -0.3816671371459961, 'logits/chosen': -0.46953389048576355, 'epoch': 6.29}
[2024-04-05 19:18:05,511] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|██████▉   | 16895/24156 [4:13:22<32:28:58, 16.10s/it]
{'loss': 0.4189, 'learning_rate': 1.999740243070684e-06, 'rewards/chosen': -1.1867560148239136, 'rewards/rejected': -2.5832138061523438, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3964577913284302, 'policy_logps/rejected': -481.2250061035156, 'policy_logps/chosen': -569.2803344726562, 'referece_logps/rejected': -455.3929138183594, 'referece_logps/chosen': -557.4127807617188, 'logits/rejected': 0.303764283657074, 'logits/chosen': 0.2920125126838684, 'epoch': 6.29}


 70%|██████▉   | 16897/24156 [4:13:54<31:33:29, 15.65s/it]
{'loss': 0.4298, 'learning_rate': 1.999734095462892e-06, 'rewards/chosen': -0.9459838271141052, 'rewards/rejected': -2.5538992881774902, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6079155206680298, 'policy_logps/rejected': -577.427490234375, 'policy_logps/chosen': -322.4122009277344, 'referece_logps/rejected': -551.8885498046875, 'referece_logps/chosen': -312.9523620605469, 'logits/rejected': -1.0718010663986206, 'logits/chosen': -0.8030674457550049, 'epoch': 6.3}


 70%|██████▉   | 16899/24156 [4:14:20<29:08:55, 14.46s/it]

 70%|██████▉   | 16900/24156 [4:14:34<28:51:30, 14.32s/it]
{'loss': 0.3447, 'learning_rate': 1.999724739259422e-06, 'rewards/chosen': -1.595961093902588, 'rewards/rejected': -3.6424946784973145, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0465338230133057, 'policy_logps/rejected': -451.79425048828125, 'policy_logps/chosen': -464.20550537109375, 'referece_logps/rejected': -415.3692932128906, 'referece_logps/chosen': -448.2459411621094, 'logits/rejected': -0.2161794900894165, 'logits/chosen': -0.2605511546134949, 'epoch': 6.3}


 70%|██████▉   | 16902/24156 [4:15:03<28:51:25, 14.32s/it]

 70%|██████▉   | 16903/24156 [4:15:21<30:59:05, 15.38s/it]

 70%|██████▉   | 16904/24156 [4:15:36<30:35:24, 15.19s/it]

 70%|██████▉   | 16905/24156 [4:15:57<34:18:22, 17.03s/it]
{'loss': 0.4087, 'learning_rate': 1.9997087861459772e-06, 'rewards/chosen': -1.9156932830810547, 'rewards/rejected': -4.620774745941162, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7050814628601074, 'policy_logps/rejected': -369.8164978027344, 'policy_logps/chosen': -405.5862121582031, 'referece_logps/rejected': -323.6087951660156, 'referece_logps/chosen': -386.4292907714844, 'logits/rejected': -0.9205530881881714, 'logits/chosen': -1.0105931758880615, 'epoch': 6.3}


 70%|██████▉   | 16907/24156 [4:16:24<30:53:20, 15.34s/it]

 70%|██████▉   | 16908/24156 [4:16:36<29:04:15, 14.44s/it]

 70%|██████▉   | 16909/24156 [4:16:48<27:24:24, 13.61s/it]
{'loss': 0.3745, 'learning_rate': 1.9996957001624602e-06, 'rewards/chosen': -1.3147788047790527, 'rewards/rejected': -3.7957420349121094, 'rewards/accuracies': 0.875, 'rewards/margins': 2.480963706970215, 'policy_logps/rejected': -487.23980712890625, 'policy_logps/chosen': -400.01885986328125, 'referece_logps/rejected': -449.2823486328125, 'referece_logps/chosen': -386.87103271484375, 'logits/rejected': -0.304639995098114, 'logits/chosen': -0.2837190628051758, 'epoch': 6.3}


 70%|███████   | 16911/24156 [4:17:10<24:39:54, 12.26s/it]

 70%|███████   | 16912/24156 [4:17:27<27:53:29, 13.86s/it]

 70%|███████   | 16913/24156 [4:17:47<31:45:53, 15.79s/it]

 70%|███████   | 16914/24156 [4:18:04<32:19:22, 16.07s/it]

 70%|███████   | 16915/24156 [4:18:20<31:54:58, 15.87s/it]
{'loss': 0.351, 'learning_rate': 1.9996755320414172e-06, 'rewards/chosen': -1.8762750625610352, 'rewards/rejected': -3.526822805404663, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6505478620529175, 'policy_logps/rejected': -397.177001953125, 'policy_logps/chosen': -385.06982421875, 'referece_logps/rejected': -361.90875244140625, 'referece_logps/chosen': -366.30706787109375, 'logits/rejected': -0.6406176686286926, 'logits/chosen': -0.5600139498710632, 'epoch': 6.3}
[2024-04-05 19:23:38,158] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|███████   | 16917/24156 [4:19:04<38:21:22, 19.07s/it]

 70%|███████   | 16918/24156 [4:19:17<35:02:36, 17.43s/it]
[2024-04-05 19:24:14,672] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|███████   | 16919/24156 [4:19:34<34:36:28, 17.22s/it]

 70%|███████   | 16920/24156 [4:19:55<37:03:42, 18.44s/it]

 70%|███████   | 16921/24156 [4:20:10<34:50:16, 17.33s/it]

 70%|███████   | 16922/24156 [4:20:27<34:55:56, 17.38s/it]

 70%|███████   | 16923/24156 [4:20:40<31:53:31, 15.87s/it]
{'loss': 0.3859, 'learning_rate': 1.99964763482834e-06, 'rewards/chosen': -1.7939579486846924, 'rewards/rejected': -2.6086151599884033, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8146568536758423, 'policy_logps/rejected': -246.75131225585938, 'policy_logps/chosen': -371.9373779296875, 'referece_logps/rejected': -220.66514587402344, 'referece_logps/chosen': -353.997802734375, 'logits/rejected': 0.2143389880657196, 'logits/chosen': 0.161597341299057, 'epoch': 6.31}


 70%|███████   | 16925/24156 [4:21:11<31:55:46, 15.90s/it]
{'loss': 0.3375, 'learning_rate': 1.999640480816786e-06, 'rewards/chosen': -2.0355939865112305, 'rewards/rejected': -2.762127637863159, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7265333533287048, 'policy_logps/rejected': -456.8248596191406, 'policy_logps/chosen': -454.7457275390625, 'referece_logps/rejected': -429.20361328125, 'referece_logps/chosen': -434.3897705078125, 'logits/rejected': 0.31230199337005615, 'logits/chosen': 0.2933529019355774, 'epoch': 6.31}
[2024-04-05 19:26:24,290] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|███████   | 16927/24156 [4:21:46<33:54:49, 16.89s/it]

 70%|███████   | 16928/24156 [4:22:05<35:10:14, 17.52s/it]

 70%|███████   | 16929/24156 [4:22:24<36:02:28, 17.95s/it]

 70%|███████   | 16930/24156 [4:22:37<32:37:11, 16.25s/it]

 70%|███████   | 16931/24156 [4:22:56<34:42:09, 17.29s/it]
{'loss': 0.4792, 'learning_rate': 1.9996185874904516e-06, 'rewards/chosen': -1.479356288909912, 'rewards/rejected': -3.0229597091674805, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5436034202575684, 'policy_logps/rejected': -261.59869384765625, 'policy_logps/chosen': -359.45416259765625, 'referece_logps/rejected': -231.36911010742188, 'referece_logps/chosen': -344.6606140136719, 'logits/rejected': -1.0117838382720947, 'logits/chosen': -1.0122127532958984, 'epoch': 6.31}


 70%|███████   | 16933/24156 [4:23:23<30:33:07, 15.23s/it]

 70%|███████   | 16934/24156 [4:23:34<27:47:17, 13.85s/it]
{'loss': 0.4895, 'learning_rate': 1.9996073982289693e-06, 'rewards/chosen': -1.4723947048187256, 'rewards/rejected': -2.780545949935913, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3081510066986084, 'policy_logps/rejected': -507.54638671875, 'policy_logps/chosen': -532.3709716796875, 'referece_logps/rejected': -479.74090576171875, 'referece_logps/chosen': -517.6470336914062, 'logits/rejected': -0.30976617336273193, 'logits/chosen': -0.35921066999435425, 'epoch': 6.31}

 70%|███████   | 16935/24156 [4:23:45<25:57:10, 12.94s/it]


 70%|███████   | 16937/24156 [4:24:18<30:19:59, 15.13s/it]

 70%|███████   | 16938/24156 [4:24:35<31:53:11, 15.90s/it]
{'loss': 0.4203, 'learning_rate': 1.9995922276342796e-06, 'rewards/chosen': -1.2669514417648315, 'rewards/rejected': -1.7405439615249634, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4735925793647766, 'policy_logps/rejected': -403.02911376953125, 'policy_logps/chosen': -437.6286926269531, 'referece_logps/rejected': -385.6236877441406, 'referece_logps/chosen': -424.9591979980469, 'logits/rejected': -0.4273843765258789, 'logits/chosen': -0.4401770234107971, 'epoch': 6.31}


 70%|███████   | 16940/24156 [4:25:06<31:24:52, 15.67s/it]

 70%|███████   | 16941/24156 [4:25:26<34:10:19, 17.05s/it]

 70%|███████   | 16942/24156 [4:25:44<34:28:15, 17.20s/it]

 70%|███████   | 16943/24156 [4:26:02<35:17:01, 17.61s/it]
[2024-04-05 19:30:59,629] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3585, 'learning_rate': 1.999572860073065e-06, 'rewards/chosen': -1.6784439086914062, 'rewards/rejected': -4.310684680938721, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6322405338287354, 'policy_logps/rejected': -379.495361328125, 'policy_logps/chosen': -497.75823974609375, 'referece_logps/rejected': -336.38848876953125, 'referece_logps/chosen': -480.9737854003906, 'logits/rejected': -0.27296629548072815, 'logits/chosen': -0.3473302125930786, 'epoch': 6.31}


 70%|███████   | 16945/24156 [4:26:34<32:37:46, 16.29s/it]

 70%|███████   | 16946/24156 [4:26:48<31:04:09, 15.51s/it]
{'loss': 0.3511, 'learning_rate': 1.999561023903487e-06, 'rewards/chosen': -1.6299229860305786, 'rewards/rejected': -3.3015296459198, 'rewards/accuracies': 0.75, 'rewards/margins': 1.671606421470642, 'policy_logps/rejected': -565.0736694335938, 'policy_logps/chosen': -351.1192932128906, 'referece_logps/rejected': -532.0584106445312, 'referece_logps/chosen': -334.820068359375, 'logits/rejected': -1.0063449144363403, 'logits/chosen': -0.8162037134170532, 'epoch': 6.31}

 70%|███████   | 16947/24156 [4:27:01<30:01:17, 14.99s/it]


 70%|███████   | 16949/24156 [4:27:42<35:41:02, 17.82s/it]

 70%|███████   | 16950/24156 [4:28:05<38:22:41, 19.17s/it]

 70%|███████   | 16951/24156 [4:28:25<38:55:18, 19.45s/it]
{'loss': 0.3597, 'learning_rate': 1.9995409375722544e-06, 'rewards/chosen': -1.6334198713302612, 'rewards/rejected': -1.9002426862716675, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2668226957321167, 'policy_logps/rejected': -369.80987548828125, 'policy_logps/chosen': -373.216552734375, 'referece_logps/rejected': -350.8074645996094, 'referece_logps/chosen': -356.88238525390625, 'logits/rejected': -0.34647712111473083, 'logits/chosen': -0.42616498470306396, 'epoch': 6.32}


 70%|███████   | 16953/24156 [4:28:56<34:28:42, 17.23s/it]

 70%|███████   | 16954/24156 [4:29:13<33:55:05, 16.95s/it]
{'loss': 0.34, 'learning_rate': 1.9995286701475312e-06, 'rewards/chosen': -1.6038470268249512, 'rewards/rejected': -3.111689329147339, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5078425407409668, 'policy_logps/rejected': -330.65423583984375, 'policy_logps/chosen': -382.3084411621094, 'referece_logps/rejected': -299.537353515625, 'referece_logps/chosen': -366.2699890136719, 'logits/rejected': -1.0270880460739136, 'logits/chosen': -1.1020323038101196, 'epoch': 6.32}

 70%|███████   | 16955/24156 [4:29:29<33:44:02, 16.86s/it]

 70%|███████   | 16956/24156 [4:29:45<33:08:42, 16.57s/it]

 70%|███████   | 16957/24156 [4:30:04<34:09:52, 17.08s/it]


 70%|███████   | 16959/24156 [4:30:26<27:57:34, 13.99s/it]

 70%|███████   | 16960/24156 [4:30:39<27:29:38, 13.75s/it]

 70%|███████   | 16961/24156 [4:30:55<28:38:27, 14.33s/it]
{'loss': 0.3997, 'learning_rate': 1.9994994172602847e-06, 'rewards/chosen': -1.928407907485962, 'rewards/rejected': -3.338085889816284, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4096779823303223, 'policy_logps/rejected': -481.0405578613281, 'policy_logps/chosen': -371.7552490234375, 'referece_logps/rejected': -447.65972900390625, 'referece_logps/chosen': -352.4711608886719, 'logits/rejected': -0.612117350101471, 'logits/chosen': -0.6811087727546692, 'epoch': 6.32}


 70%|███████   | 16963/24156 [4:31:24<29:38:38, 14.84s/it]

 70%|███████   | 16964/24156 [4:31:35<27:09:13, 13.59s/it]

 70%|███████   | 16965/24156 [4:31:51<28:43:47, 14.38s/it]

 70%|███████   | 16966/24156 [4:32:10<31:46:37, 15.91s/it]

 70%|███████   | 16967/24156 [4:32:21<28:39:57, 14.35s/it]

 70%|███████   | 16968/24156 [4:32:37<29:38:58, 14.85s/it]

 70%|███████   | 16969/24156 [4:32:50<28:29:53, 14.27s/it]

 70%|███████   | 16970/24156 [4:33:03<27:44:52, 13.90s/it]
{'loss': 0.4006, 'learning_rate': 1.999460512710351e-06, 'rewards/chosen': -1.4011750221252441, 'rewards/rejected': -2.6736507415771484, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2724757194519043, 'policy_logps/rejected': -496.80755615234375, 'policy_logps/chosen': -372.7863464355469, 'referece_logps/rejected': -470.071044921875, 'referece_logps/chosen': -358.7746276855469, 'logits/rejected': 0.05636981129646301, 'logits/chosen': 0.26046550273895264, 'epoch': 6.32}


 70%|███████   | 16972/24156 [4:33:31<27:15:46, 13.66s/it]

 70%|███████   | 16973/24156 [4:33:44<27:03:37, 13.56s/it]

 70%|███████   | 16974/24156 [4:34:04<31:03:49, 15.57s/it]

 70%|███████   | 16975/24156 [4:34:17<29:24:04, 14.74s/it]

 70%|███████   | 16976/24156 [4:34:32<29:40:24, 14.88s/it]

 70%|███████   | 16977/24156 [4:34:53<33:17:52, 16.70s/it]

 70%|███████   | 16978/24156 [4:35:10<33:37:04, 16.86s/it]
{'loss': 0.3916, 'learning_rate': 1.999424709107543e-06, 'rewards/chosen': -0.8719136714935303, 'rewards/rejected': -2.331787109375, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4598735570907593, 'policy_logps/rejected': -367.1839904785156, 'policy_logps/chosen': -289.1134338378906, 'referece_logps/rejected': -343.86614990234375, 'referece_logps/chosen': -280.394287109375, 'logits/rejected': -0.062434934079647064, 'logits/chosen': -0.13233524560928345, 'epoch': 6.33}


 70%|███████   | 16980/24156 [4:35:35<29:03:42, 14.58s/it]
{'loss': 0.3647, 'learning_rate': 1.999415578538277e-06, 'rewards/chosen': -1.3167297840118408, 'rewards/rejected': -2.6697654724121094, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3530356884002686, 'policy_logps/rejected': -397.78167724609375, 'policy_logps/chosen': -395.06488037109375, 'referece_logps/rejected': -371.083984375, 'referece_logps/chosen': -381.8976135253906, 'logits/rejected': -0.04477448761463165, 'logits/chosen': 0.18485084176063538, 'epoch': 6.33}


 70%|███████   | 16982/24156 [4:36:16<35:16:50, 17.70s/it]
{'loss': 0.3111, 'learning_rate': 1.99940637610289e-06, 'rewards/chosen': -1.843977689743042, 'rewards/rejected': -4.286874771118164, 'rewards/accuracies': 1.0, 'rewards/margins': 2.442896604537964, 'policy_logps/rejected': -421.8054504394531, 'policy_logps/chosen': -427.0589294433594, 'referece_logps/rejected': -378.9367370605469, 'referece_logps/chosen': -408.6191711425781, 'logits/rejected': -0.2964647710323334, 'logits/chosen': -0.29836052656173706, 'epoch': 6.33}


 70%|███████   | 16984/24156 [4:36:58<38:04:25, 19.11s/it]
{'loss': 0.2614, 'learning_rate': 1.9993971018020447e-06, 'rewards/chosen': -1.4377421140670776, 'rewards/rejected': -3.60536789894104, 'rewards/accuracies': 1.0, 'rewards/margins': 2.167625665664673, 'policy_logps/rejected': -190.8854522705078, 'policy_logps/chosen': -305.0472412109375, 'referece_logps/rejected': -154.83177185058594, 'referece_logps/chosen': -290.6698303222656, 'logits/rejected': -0.37542372941970825, 'logits/chosen': -0.29453837871551514, 'epoch': 6.33}

 70%|███████   | 16985/24156 [4:37:10<33:32:55, 16.84s/it]

 70%|███████   | 16986/24156 [4:37:24<31:58:30, 16.05s/it]


 70%|███████   | 16988/24156 [4:38:03<35:23:08, 17.77s/it]
{'loss': 0.407, 'learning_rate': 1.99937833760665e-06, 'rewards/chosen': -1.3564422130584717, 'rewards/rejected': -2.931874990463257, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5754332542419434, 'policy_logps/rejected': -313.01641845703125, 'policy_logps/chosen': -389.02154541015625, 'referece_logps/rejected': -283.6976623535156, 'referece_logps/chosen': -375.4571228027344, 'logits/rejected': -0.43513768911361694, 'logits/chosen': -0.37286117672920227, 'epoch': 6.33}


 70%|███████   | 16990/24156 [4:38:31<31:50:10, 15.99s/it]

 70%|███████   | 16991/24156 [4:38:43<29:43:33, 14.94s/it]
{'loss': 0.3333, 'learning_rate': 1.999364075818273e-06, 'rewards/chosen': -1.6885420083999634, 'rewards/rejected': -3.6684460639953613, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9799039363861084, 'policy_logps/rejected': -452.6611328125, 'policy_logps/chosen': -336.6621398925781, 'referece_logps/rejected': -415.9766845703125, 'referece_logps/chosen': -319.7767333984375, 'logits/rejected': 0.1730731874704361, 'logits/chosen': 0.15820086002349854, 'epoch': 6.33}


 70%|███████   | 16993/24156 [4:39:13<30:42:14, 15.43s/it]
[2024-04-05 19:44:10,837] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3494, 'learning_rate': 1.9993544781311903e-06, 'rewards/chosen': -2.9692676067352295, 'rewards/rejected': -3.9400174617767334, 'rewards/accuracies': 0.875, 'rewards/margins': 0.970750093460083, 'policy_logps/rejected': -662.139404296875, 'policy_logps/chosen': -678.9356689453125, 'referece_logps/rejected': -622.7391967773438, 'referece_logps/chosen': -649.2430419921875, 'logits/rejected': -0.1809278279542923, 'logits/chosen': -0.08497927337884903, 'epoch': 6.33}


 70%|███████   | 16995/24156 [4:39:37<26:55:50, 13.54s/it]

 70%|███████   | 16996/24156 [4:39:49<25:52:34, 13.01s/it]
{'loss': 0.3753, 'learning_rate': 1.9993399468600463e-06, 'rewards/chosen': -1.4531782865524292, 'rewards/rejected': -2.7355527877807617, 'rewards/accuracies': 0.75, 'rewards/margins': 1.282374620437622, 'policy_logps/rejected': -281.24456787109375, 'policy_logps/chosen': -504.41455078125, 'referece_logps/rejected': -253.8890380859375, 'referece_logps/chosen': -489.8827819824219, 'logits/rejected': -0.624718189239502, 'logits/chosen': -0.7720552682876587, 'epoch': 6.33}

 70%|███████   | 16997/24156 [4:40:02<26:12:56, 13.18s/it]

 70%|███████   | 16998/24156 [4:40:20<28:55:31, 14.55s/it]

 70%|███████   | 16999/24156 [4:40:32<27:24:45, 13.79s/it]

 70%|███████   | 17000/24156 [4:40:48<28:46:48, 14.48s/it]


 70%|███████   | 17002/24156 [4:41:41<39:47:41, 20.03s/it]

 70%|███████   | 17003/24156 [4:42:03<40:56:59, 20.61s/it]

 70%|███████   | 17004/24156 [4:42:17<37:17:49, 18.77s/it]
{'loss': 0.5268, 'learning_rate': 1.999300406342636e-06, 'rewards/chosen': -1.7276086807250977, 'rewards/rejected': -3.0875275135040283, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3599185943603516, 'policy_logps/rejected': -357.302001953125, 'policy_logps/chosen': -523.2418212890625, 'referece_logps/rejected': -326.4267578125, 'referece_logps/chosen': -505.96575927734375, 'logits/rejected': -0.19750207662582397, 'logits/chosen': -0.21542613208293915, 'epoch': 6.34}


 70%|███████   | 17006/24156 [4:42:56<38:02:26, 19.15s/it]
{'loss': 0.3766, 'learning_rate': 1.9992903415668968e-06, 'rewards/chosen': -1.536172866821289, 'rewards/rejected': -4.0471343994140625, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5109612941741943, 'policy_logps/rejected': -344.66705322265625, 'policy_logps/chosen': -345.0091857910156, 'referece_logps/rejected': -304.1956787109375, 'referece_logps/chosen': -329.6474609375, 'logits/rejected': -0.01749470829963684, 'logits/chosen': -0.11448335647583008, 'epoch': 6.34}

 70%|███████   | 17007/24156 [4:43:07<33:22:46, 16.81s/it]

 70%|███████   | 17008/24156 [4:43:23<32:44:03, 16.49s/it]

 70%|███████   | 17009/24156 [4:43:44<35:39:06, 17.96s/it]

 70%|███████   | 17010/24156 [4:44:04<36:59:08, 18.63s/it]

 70%|███████   | 17011/24156 [4:44:20<35:30:32, 17.89s/it]

 70%|███████   | 17012/24156 [4:44:38<35:16:39, 17.78s/it]

 70%|███████   | 17013/24156 [4:44:59<36:57:49, 18.63s/it]

 70%|███████   | 17014/24156 [4:45:17<36:41:48, 18.50s/it]


 70%|███████   | 17016/24156 [4:45:58<38:59:18, 19.66s/it]

 70%|███████   | 17017/24156 [4:46:10<34:25:22, 17.36s/it]
{'loss': 0.4723, 'learning_rate': 1.999233700874068e-06, 'rewards/chosen': -1.9230464696884155, 'rewards/rejected': -2.830636739730835, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9075905084609985, 'policy_logps/rejected': -465.3333740234375, 'policy_logps/chosen': -615.2374267578125, 'referece_logps/rejected': -437.02703857421875, 'referece_logps/chosen': -596.0069580078125, 'logits/rejected': -0.3449171483516693, 'logits/chosen': -0.4571094512939453, 'epoch': 6.34}

 70%|███████   | 17018/24156 [4:46:21<30:29:15, 15.38s/it]


 70%|███████   | 17020/24156 [4:46:45<27:21:10, 13.80s/it]
{'loss': 0.5233, 'learning_rate': 1.9992178761786866e-06, 'rewards/chosen': -1.7650883197784424, 'rewards/rejected': -2.199172258377075, 'rewards/accuracies': 0.5, 'rewards/margins': 0.43408411741256714, 'policy_logps/rejected': -321.41754150390625, 'policy_logps/chosen': -394.6194763183594, 'referece_logps/rejected': -299.42578125, 'referece_logps/chosen': -376.9685974121094, 'logits/rejected': -0.9006483554840088, 'logits/chosen': -1.0167664289474487, 'epoch': 6.34}

 70%|███████   | 17021/24156 [4:47:01<28:16:32, 14.27s/it]

 70%|███████   | 17022/24156 [4:47:17<29:27:53, 14.87s/it]


 70%|███████   | 17024/24156 [4:47:46<29:20:34, 14.81s/it]

 70%|███████   | 17025/24156 [4:48:04<31:13:28, 15.76s/it]
{'loss': 0.3274, 'learning_rate': 1.999191142428143e-06, 'rewards/chosen': -2.295030117034912, 'rewards/rejected': -3.991361618041992, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6963316202163696, 'policy_logps/rejected': -487.11871337890625, 'policy_logps/chosen': -481.13250732421875, 'referece_logps/rejected': -447.205078125, 'referece_logps/chosen': -458.1821594238281, 'logits/rejected': 0.7111725211143494, 'logits/chosen': 0.6064594984054565, 'epoch': 6.34}
[2024-04-05 19:53:22,731] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|███████   | 17026/24156 [4:48:25<34:41:30, 17.52s/it]
[2024-04-05 19:53:42,003] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|███████   | 17027/24156 [4:48:45<35:43:48, 18.04s/it]


 70%|███████   | 17029/24156 [4:49:22<36:14:08, 18.30s/it]
{'loss': 0.4949, 'learning_rate': 1.9991694321022177e-06, 'rewards/chosen': -1.330464482307434, 'rewards/rejected': -2.8501408100128174, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5196762084960938, 'policy_logps/rejected': -564.1820678710938, 'policy_logps/chosen': -465.2644958496094, 'referece_logps/rejected': -535.6806640625, 'referece_logps/chosen': -451.9598693847656, 'logits/rejected': -0.066700279712677, 'logits/chosen': 0.0020219087600708008, 'epoch': 6.34}


 71%|███████   | 17031/24156 [4:49:58<36:24:50, 18.40s/it]
[2024-04-05 19:54:55,440] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4852, 'learning_rate': 1.9991584691662326e-06, 'rewards/chosen': -2.0539326667785645, 'rewards/rejected': -3.7160449028015137, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6621121168136597, 'policy_logps/rejected': -369.9185791015625, 'policy_logps/chosen': -499.31439208984375, 'referece_logps/rejected': -332.7581481933594, 'referece_logps/chosen': -478.7750549316406, 'logits/rejected': -0.1041041910648346, 'logits/chosen': -0.27899888157844543, 'epoch': 6.35}

 71%|███████   | 17032/24156 [4:50:19<37:44:38, 19.07s/it]
[2024-04-05 19:55:38,640] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17033/24156 [4:50:41<39:48:13, 20.12s/it]

 71%|███████   | 17034/24156 [4:50:54<35:44:25, 18.07s/it]


 71%|███████   | 17036/24156 [4:51:30<35:28:01, 17.93s/it]
{'loss': 0.3848, 'learning_rate': 1.9991307474946163e-06, 'rewards/chosen': -1.705809235572815, 'rewards/rejected': -2.9111990928649902, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2053896188735962, 'policy_logps/rejected': -327.152587890625, 'policy_logps/chosen': -478.6568908691406, 'referece_logps/rejected': -298.04058837890625, 'referece_logps/chosen': -461.5988464355469, 'logits/rejected': 0.19060872495174408, 'logits/chosen': 0.21104207634925842, 'epoch': 6.35}


 71%|███████   | 17038/24156 [4:52:00<32:17:38, 16.33s/it]

 71%|███████   | 17039/24156 [4:52:12<29:55:00, 15.13s/it]
{'loss': 0.326, 'learning_rate': 1.9991138989539313e-06, 'rewards/chosen': -1.9849178791046143, 'rewards/rejected': -3.235515832901001, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2505978345870972, 'policy_logps/rejected': -270.2495422363281, 'policy_logps/chosen': -466.0302734375, 'referece_logps/rejected': -237.8944091796875, 'referece_logps/chosen': -446.18109130859375, 'logits/rejected': -0.03817085921764374, 'logits/chosen': -0.10541385412216187, 'epoch': 6.35}

 71%|███████   | 17040/24156 [4:52:23<27:11:38, 13.76s/it]
[2024-04-05 19:57:40,050] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17041/24156 [4:52:43<30:38:45, 15.51s/it]


 71%|███████   | 17043/24156 [4:53:06<27:06:02, 13.72s/it]
{'loss': 0.3298, 'learning_rate': 1.9990911827779994e-06, 'rewards/chosen': -2.275862693786621, 'rewards/rejected': -3.850534200668335, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5746715068817139, 'policy_logps/rejected': -374.8359680175781, 'policy_logps/chosen': -392.3858337402344, 'referece_logps/rejected': -336.33062744140625, 'referece_logps/chosen': -369.62725830078125, 'logits/rejected': -0.5603861808776855, 'logits/chosen': -0.6860164999961853, 'epoch': 6.35}

 71%|███████   | 17044/24156 [4:53:20<26:59:51, 13.67s/it]
[2024-04-05 19:58:38,353] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17045/24156 [4:53:41<31:27:32, 15.93s/it]
[2024-04-05 19:58:56,458] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 71%|███████   | 17047/24156 [4:54:16<33:01:57, 16.73s/it]

 71%|███████   | 17048/24156 [4:54:36<35:13:50, 17.84s/it]

 71%|███████   | 17049/24156 [4:54:48<31:28:57, 15.95s/it]
{'loss': 0.4278, 'learning_rate': 1.999056569695223e-06, 'rewards/chosen': -1.303877830505371, 'rewards/rejected': -3.5313596725463867, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2274816036224365, 'policy_logps/rejected': -367.4820556640625, 'policy_logps/chosen': -417.3597412109375, 'referece_logps/rejected': -332.16845703125, 'referece_logps/chosen': -404.32098388671875, 'logits/rejected': -0.5725035071372986, 'logits/chosen': -0.4311373829841614, 'epoch': 6.35}

 71%|███████   | 17050/24156 [4:55:02<30:01:41, 15.21s/it]


 71%|███████   | 17052/24156 [4:55:30<28:42:49, 14.55s/it]

 71%|███████   | 17053/24156 [4:55:48<31:00:57, 15.72s/it]
{'loss': 0.3976, 'learning_rate': 1.9990331351037987e-06, 'rewards/chosen': -1.8853647708892822, 'rewards/rejected': -2.307554006576538, 'rewards/accuracies': 0.625, 'rewards/margins': 0.42218923568725586, 'policy_logps/rejected': -314.6083984375, 'policy_logps/chosen': -421.21734619140625, 'referece_logps/rejected': -291.5328369140625, 'referece_logps/chosen': -402.3636474609375, 'logits/rejected': -0.33751434087753296, 'logits/chosen': -0.5027405023574829, 'epoch': 6.35}

 71%|███████   | 17054/24156 [4:55:59<27:58:38, 14.18s/it]


 71%|███████   | 17056/24156 [4:56:34<32:03:02, 16.25s/it]

 71%|███████   | 17057/24156 [4:56:55<34:26:54, 17.47s/it]
{'loss': 0.3711, 'learning_rate': 1.999009413157901e-06, 'rewards/chosen': -2.1163055896759033, 'rewards/rejected': -3.540571928024292, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4242664575576782, 'policy_logps/rejected': -313.5862121582031, 'policy_logps/chosen': -377.91839599609375, 'referece_logps/rejected': -278.18048095703125, 'referece_logps/chosen': -356.7553405761719, 'logits/rejected': -0.5743144154548645, 'logits/chosen': -0.6143609881401062, 'epoch': 6.36}

 71%|███████   | 17058/24156 [4:57:09<32:46:54, 16.63s/it]


 71%|███████   | 17060/24156 [4:57:42<33:18:28, 16.90s/it]

 71%|███████   | 17061/24156 [4:57:58<32:47:32, 16.64s/it]

 71%|███████   | 17062/24156 [4:58:14<32:30:05, 16.49s/it]
{'loss': 0.256, 'learning_rate': 1.998979356643703e-06, 'rewards/chosen': -1.6496249437332153, 'rewards/rejected': -4.176904201507568, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5272791385650635, 'policy_logps/rejected': -425.877685546875, 'policy_logps/chosen': -303.825927734375, 'referece_logps/rejected': -384.1086120605469, 'referece_logps/chosen': -287.32965087890625, 'logits/rejected': -1.5369579792022705, 'logits/chosen': -1.3607168197631836, 'epoch': 6.36}

 71%|███████   | 17063/24156 [4:58:32<33:03:34, 16.78s/it]


 71%|███████   | 17065/24156 [4:59:12<36:33:48, 18.56s/it]
{'loss': 0.349, 'learning_rate': 1.998961107230061e-06, 'rewards/chosen': -1.4506902694702148, 'rewards/rejected': -4.219906806945801, 'rewards/accuracies': 0.875, 'rewards/margins': 2.769216537475586, 'policy_logps/rejected': -394.43963623046875, 'policy_logps/chosen': -322.74090576171875, 'referece_logps/rejected': -352.2406005859375, 'referece_logps/chosen': -308.2340087890625, 'logits/rejected': -0.4759458601474762, 'logits/chosen': -0.4808274507522583, 'epoch': 6.36}


 71%|███████   | 17067/24156 [4:59:40<32:15:02, 16.38s/it]

 71%|███████   | 17068/24156 [5:00:01<34:32:50, 17.55s/it]

 71%|███████   | 17069/24156 [5:00:14<32:20:39, 16.43s/it]

 71%|███████   | 17070/24156 [5:00:33<33:21:26, 16.95s/it]

 71%|███████   | 17071/24156 [5:00:53<35:12:21, 17.89s/it]

 71%|███████   | 17072/24156 [5:01:11<35:21:56, 17.97s/it]
{'loss': 0.4398, 'learning_rate': 1.9989178967274507e-06, 'rewards/chosen': -1.1749041080474854, 'rewards/rejected': -2.2209205627441406, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0460164546966553, 'policy_logps/rejected': -473.4999084472656, 'policy_logps/chosen': -401.1626281738281, 'referece_logps/rejected': -451.2906799316406, 'referece_logps/chosen': -389.41363525390625, 'logits/rejected': -0.4563412070274353, 'logits/chosen': -0.48343151807785034, 'epoch': 6.36}


 71%|███████   | 17074/24156 [5:01:39<31:26:55, 15.99s/it]
{'loss': 0.4051, 'learning_rate': 1.9989053892496392e-06, 'rewards/chosen': -1.009202241897583, 'rewards/rejected': -2.2184622287750244, 'rewards/accuracies': 1.0, 'rewards/margins': 1.209260106086731, 'policy_logps/rejected': -374.04736328125, 'policy_logps/chosen': -382.91986083984375, 'referece_logps/rejected': -351.86273193359375, 'referece_logps/chosen': -372.8278503417969, 'logits/rejected': -0.8330383896827698, 'logits/chosen': -0.7751163840293884, 'epoch': 6.36}

 71%|███████   | 17075/24156 [5:02:00<34:44:16, 17.66s/it]

 71%|███████   | 17076/24156 [5:02:12<31:18:43, 15.92s/it]
[2024-04-05 20:07:26,642] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17077/24156 [5:02:29<32:01:57, 16.29s/it]


 71%|███████   | 17079/24156 [5:03:11<36:41:09, 18.66s/it]
[2024-04-05 20:08:08,364] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17080/24156 [5:03:31<37:33:29, 19.11s/it]
{'loss': 0.3963, 'learning_rate': 1.9988674358432274e-06, 'rewards/chosen': -1.6448819637298584, 'rewards/rejected': -2.8589425086975098, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2140604257583618, 'policy_logps/rejected': -639.6018676757812, 'policy_logps/chosen': -406.7778015136719, 'referece_logps/rejected': -611.0123901367188, 'referece_logps/chosen': -390.3289794921875, 'logits/rejected': -0.38591068983078003, 'logits/chosen': -0.5423150062561035, 'epoch': 6.36}


 71%|███████   | 17082/24156 [5:04:09<36:57:25, 18.81s/it]
{'loss': 0.3679, 'learning_rate': 1.9988546410531294e-06, 'rewards/chosen': -1.1305979490280151, 'rewards/rejected': -3.227313280105591, 'rewards/accuracies': 1.0, 'rewards/margins': 2.096715211868286, 'policy_logps/rejected': -349.6553955078125, 'policy_logps/chosen': -441.76409912109375, 'referece_logps/rejected': -317.38226318359375, 'referece_logps/chosen': -430.4581298828125, 'logits/rejected': -0.7827816009521484, 'logits/chosen': -0.7536447048187256, 'epoch': 6.36}
[2024-04-05 20:09:27,349] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17083/24156 [5:04:30<38:23:55, 19.54s/it]
[2024-04-05 20:09:39,604] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17084/24156 [5:04:42<34:05:53, 17.36s/it]


 71%|███████   | 17086/24156 [5:05:07<29:10:42, 14.86s/it]
{'loss': 0.3851, 'learning_rate': 1.998828835996506e-06, 'rewards/chosen': -1.6466948986053467, 'rewards/rejected': -4.03447961807251, 'rewards/accuracies': 0.875, 'rewards/margins': 2.387784957885742, 'policy_logps/rejected': -387.9912414550781, 'policy_logps/chosen': -570.677978515625, 'referece_logps/rejected': -347.64642333984375, 'referece_logps/chosen': -554.2109375, 'logits/rejected': 0.4573065936565399, 'logits/chosen': 0.47090816497802734, 'epoch': 6.37}


 71%|███████   | 17088/24156 [5:05:33<27:21:46, 13.94s/it]
{'loss': 0.3946, 'learning_rate': 1.9988158257318356e-06, 'rewards/chosen': -1.4695007801055908, 'rewards/rejected': -3.2549350261688232, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7854340076446533, 'policy_logps/rejected': -289.385009765625, 'policy_logps/chosen': -443.60687255859375, 'referece_logps/rejected': -256.8356628417969, 'referece_logps/chosen': -428.911865234375, 'logits/rejected': -0.6811941266059875, 'logits/chosen': -0.707859218120575, 'epoch': 6.37}


 71%|███████   | 17090/24156 [5:05:59<26:01:59, 13.26s/it]
{'loss': 0.4043, 'learning_rate': 1.998802743644172e-06, 'rewards/chosen': -1.3779085874557495, 'rewards/rejected': -2.9961516857147217, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6182432174682617, 'policy_logps/rejected': -334.8667907714844, 'policy_logps/chosen': -335.6805725097656, 'referece_logps/rejected': -304.9052734375, 'referece_logps/chosen': -321.9014892578125, 'logits/rejected': -0.8736730813980103, 'logits/chosen': -0.968194305896759, 'epoch': 6.37}

 71%|███████   | 17091/24156 [5:06:20<30:34:01, 15.58s/it]


 71%|███████   | 17093/24156 [5:06:51<29:31:02, 15.04s/it]
{'loss': 0.4893, 'learning_rate': 1.9987829858466232e-06, 'rewards/chosen': -1.416893482208252, 'rewards/rejected': -1.8783854246139526, 'rewards/accuracies': 0.5, 'rewards/margins': 0.46149197220802307, 'policy_logps/rejected': -458.42877197265625, 'policy_logps/chosen': -493.8512268066406, 'referece_logps/rejected': -439.6448974609375, 'referece_logps/chosen': -479.6822814941406, 'logits/rejected': -0.46012479066848755, 'logits/chosen': -0.34470391273498535, 'epoch': 6.37}

 71%|███████   | 17094/24156 [5:07:01<26:56:52, 13.74s/it]


 71%|███████   | 17096/24156 [5:07:23<23:51:03, 12.16s/it]
{'loss': 0.4851, 'learning_rate': 1.998763066452654e-06, 'rewards/chosen': -1.3657054901123047, 'rewards/rejected': -1.6444857120513916, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2787801921367645, 'policy_logps/rejected': -353.384033203125, 'policy_logps/chosen': -326.6281433105469, 'referece_logps/rejected': -336.939208984375, 'referece_logps/chosen': -312.9710998535156, 'logits/rejected': 0.16853275895118713, 'logits/chosen': 0.2016642540693283, 'epoch': 6.37}


 71%|███████   | 17098/24156 [5:07:49<25:08:42, 12.83s/it]

 71%|███████   | 17099/24156 [5:08:01<24:53:02, 12.69s/it]
{'loss': 0.4436, 'learning_rate': 1.998742985465487e-06, 'rewards/chosen': -1.6887216567993164, 'rewards/rejected': -3.4685161113739014, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7797942161560059, 'policy_logps/rejected': -427.7931213378906, 'policy_logps/chosen': -542.9625854492188, 'referece_logps/rejected': -393.1080017089844, 'referece_logps/chosen': -526.0753173828125, 'logits/rejected': 0.045135729014873505, 'logits/chosen': -0.12040115147829056, 'epoch': 6.37}

 71%|███████   | 17100/24156 [5:08:14<24:52:25, 12.69s/it]

 71%|███████   | 17101/24156 [5:08:36<30:11:42, 15.41s/it]


 71%|███████   | 17103/24156 [5:09:19<36:11:18, 18.47s/it]

 71%|███████   | 17104/24156 [5:09:39<37:25:28, 19.11s/it]

 71%|███████   | 17105/24156 [5:10:01<39:10:28, 20.00s/it]
{'loss': 0.5083, 'learning_rate': 1.9987023387245823e-06, 'rewards/chosen': -1.625354290008545, 'rewards/rejected': -3.324936628341675, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6995819807052612, 'policy_logps/rejected': -389.6454772949219, 'policy_logps/chosen': -380.31036376953125, 'referece_logps/rejected': -356.39605712890625, 'referece_logps/chosen': -364.05682373046875, 'logits/rejected': -0.3005479872226715, 'logits/chosen': -0.1505560427904129, 'epoch': 6.37}

 71%|███████   | 17106/24156 [5:10:13<34:08:12, 17.43s/it]

 71%|███████   | 17107/24156 [5:10:28<32:55:18, 16.81s/it]

 71%|███████   | 17108/24156 [5:10:46<33:18:29, 17.01s/it]

 71%|███████   | 17109/24156 [5:10:58<30:41:33, 15.68s/it]


 71%|███████   | 17111/24156 [5:11:36<34:13:30, 17.49s/it]
[2024-04-05 20:16:33,141] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6231, 'learning_rate': 1.9986610456502143e-06, 'rewards/chosen': -2.6032826900482178, 'rewards/rejected': -4.523992538452148, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9207097291946411, 'policy_logps/rejected': -498.1222229003906, 'policy_logps/chosen': -468.6568603515625, 'referece_logps/rejected': -452.8822937011719, 'referece_logps/chosen': -442.6240234375, 'logits/rejected': -0.06850448995828629, 'logits/chosen': -0.07895120978355408, 'epoch': 6.38}
[2024-04-05 20:16:44,499] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 71%|███████   | 17113/24156 [5:12:08<33:28:48, 17.11s/it]
{'loss': 0.3717, 'learning_rate': 1.99864713766704e-06, 'rewards/chosen': -1.502479076385498, 'rewards/rejected': -2.837653636932373, 'rewards/accuracies': 0.875, 'rewards/margins': 1.335174560546875, 'policy_logps/rejected': -274.0062561035156, 'policy_logps/chosen': -367.83209228515625, 'referece_logps/rejected': -245.6297607421875, 'referece_logps/chosen': -352.80731201171875, 'logits/rejected': -0.6540884971618652, 'logits/chosen': -0.7419230341911316, 'epoch': 6.38}

 71%|███████   | 17114/24156 [5:12:21<31:20:25, 16.02s/it]

 71%|███████   | 17115/24156 [5:12:42<34:11:25, 17.48s/it]

 71%|███████   | 17116/24156 [5:13:00<34:33:46, 17.67s/it]
[2024-04-05 20:18:19,284] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17117/24156 [5:13:22<36:57:39, 18.90s/it]
[2024-04-05 20:18:42,005] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17118/24156 [5:13:45<39:11:42, 20.05s/it]


 71%|███████   | 17120/24156 [5:14:24<38:43:23, 19.81s/it]
{'loss': 0.3656, 'learning_rate': 1.9985978942219917e-06, 'rewards/chosen': -1.1512740850448608, 'rewards/rejected': -2.579547882080078, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4282736778259277, 'policy_logps/rejected': -299.8144836425781, 'policy_logps/chosen': -348.805908203125, 'referece_logps/rejected': -274.0190124511719, 'referece_logps/chosen': -337.29315185546875, 'logits/rejected': 0.4290297329425812, 'logits/chosen': 0.5280022621154785, 'epoch': 6.38}

 71%|███████   | 17121/24156 [5:14:34<33:22:18, 17.08s/it]

 71%|███████   | 17122/24156 [5:14:50<32:38:18, 16.70s/it]


 71%|███████   | 17124/24156 [5:15:16<28:21:24, 14.52s/it]
{'loss': 0.4113, 'learning_rate': 1.9985693601674754e-06, 'rewards/chosen': -1.3611738681793213, 'rewards/rejected': -2.594543695449829, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2333698272705078, 'policy_logps/rejected': -335.2627868652344, 'policy_logps/chosen': -353.2054443359375, 'referece_logps/rejected': -309.31732177734375, 'referece_logps/chosen': -339.5937194824219, 'logits/rejected': -1.078477144241333, 'logits/chosen': -1.2975587844848633, 'epoch': 6.38}
[2024-04-05 20:20:32,178] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17125/24156 [5:15:35<30:51:46, 15.80s/it]


 71%|███████   | 17127/24156 [5:16:12<33:58:54, 17.40s/it]
{'loss': 0.4544, 'learning_rate': 1.998547771137305e-06, 'rewards/chosen': -1.9740232229232788, 'rewards/rejected': -2.720043420791626, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7460201978683472, 'policy_logps/rejected': -323.6046142578125, 'policy_logps/chosen': -410.4714050292969, 'referece_logps/rejected': -296.4041748046875, 'referece_logps/chosen': -390.7311706542969, 'logits/rejected': -0.11740852892398834, 'logits/chosen': 0.0854581892490387, 'epoch': 6.38}

 71%|███████   | 17128/24156 [5:16:26<31:56:15, 16.36s/it]

 71%|███████   | 17129/24156 [5:16:46<34:22:40, 17.61s/it]

 71%|███████   | 17130/24156 [5:17:03<33:42:52, 17.27s/it]

 71%|███████   | 17131/24156 [5:17:24<36:22:04, 18.64s/it]


 71%|███████   | 17133/24156 [5:17:58<35:08:27, 18.01s/it]
{'loss': 0.4055, 'learning_rate': 1.99850410840539e-06, 'rewards/chosen': -1.295012354850769, 'rewards/rejected': -3.2674522399902344, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9724397659301758, 'policy_logps/rejected': -438.5455017089844, 'policy_logps/chosen': -291.351318359375, 'referece_logps/rejected': -405.8710021972656, 'referece_logps/chosen': -278.40118408203125, 'logits/rejected': -0.7320331335067749, 'logits/chosen': -0.7698920369148254, 'epoch': 6.38}

 71%|███████   | 17134/24156 [5:18:15<34:48:24, 17.84s/it]

 71%|███████   | 17135/24156 [5:18:35<35:58:51, 18.45s/it]

 71%|███████   | 17136/24156 [5:18:55<36:44:05, 18.84s/it]

 71%|███████   | 17137/24156 [5:19:11<35:18:28, 18.11s/it]

 71%|███████   | 17138/24156 [5:19:34<37:48:16, 19.39s/it]
[2024-04-05 20:24:51,860] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17139/24156 [5:19:54<38:28:25, 19.74s/it]

 71%|███████   | 17140/24156 [5:20:10<36:17:28, 18.62s/it]


 71%|███████   | 17142/24156 [5:20:40<32:53:17, 16.88s/it]

 71%|███████   | 17143/24156 [5:20:54<31:14:06, 16.03s/it]
{'loss': 0.3923, 'learning_rate': 1.998429901188178e-06, 'rewards/chosen': -1.6775317192077637, 'rewards/rejected': -3.09606671333313, 'rewards/accuracies': 0.625, 'rewards/margins': 1.418534755706787, 'policy_logps/rejected': -586.2938232421875, 'policy_logps/chosen': -363.09271240234375, 'referece_logps/rejected': -555.3331909179688, 'referece_logps/chosen': -346.3174133300781, 'logits/rejected': -0.7909828424453735, 'logits/chosen': -0.5362411141395569, 'epoch': 6.39}

 71%|███████   | 17144/24156 [5:21:12<32:15:58, 16.57s/it]

 71%|███████   | 17145/24156 [5:21:31<33:37:39, 17.27s/it]

 71%|███████   | 17146/24156 [5:21:51<35:20:28, 18.15s/it]

 71%|███████   | 17147/24156 [5:22:14<37:57:02, 19.49s/it]

 71%|███████   | 17148/24156 [5:22:31<36:32:26, 18.77s/it]

 71%|███████   | 17149/24156 [5:22:47<34:46:44, 17.87s/it]

 71%|███████   | 17150/24156 [5:23:07<36:06:18, 18.55s/it]

 71%|███████   | 17151/24156 [5:23:22<33:59:49, 17.47s/it]

 71%|███████   | 17152/24156 [5:23:38<33:04:53, 17.00s/it]

 71%|███████   | 17153/24156 [5:23:50<30:14:07, 15.54s/it]

 71%|███████   | 17154/24156 [5:24:03<29:10:41, 15.00s/it]


 71%|███████   | 17156/24156 [5:24:38<31:24:57, 16.16s/it]
{'loss': 0.4333, 'learning_rate': 1.9983307484793526e-06, 'rewards/chosen': -1.895760178565979, 'rewards/rejected': -3.1856253147125244, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2898651361465454, 'policy_logps/rejected': -264.5693359375, 'policy_logps/chosen': -223.6404266357422, 'referece_logps/rejected': -232.7130889892578, 'referece_logps/chosen': -204.682861328125, 'logits/rejected': -0.6049971580505371, 'logits/chosen': -0.5091865062713623, 'epoch': 6.39}

 71%|███████   | 17157/24156 [5:24:59<34:11:42, 17.59s/it]

 71%|███████   | 17158/24156 [5:25:19<35:42:49, 18.37s/it]

 71%|███████   | 17159/24156 [5:25:33<32:43:15, 16.84s/it]

 71%|███████   | 17160/24156 [5:25:52<33:59:48, 17.49s/it]

 71%|███████   | 17161/24156 [5:26:08<33:30:34, 17.25s/it]

 71%|███████   | 17162/24156 [5:26:27<34:30:53, 17.77s/it]

 71%|███████   | 17163/24156 [5:26:48<36:08:37, 18.61s/it]

 71%|███████   | 17164/24156 [5:27:01<33:11:33, 17.09s/it]

 71%|███████   | 17165/24156 [5:27:20<34:15:09, 17.64s/it]


 71%|███████   | 17167/24156 [5:27:50<30:57:45, 15.95s/it]

 71%|███████   | 17168/24156 [5:28:04<30:07:21, 15.52s/it]

 71%|███████   | 17169/24156 [5:28:21<30:47:27, 15.86s/it]

 71%|███████   | 17170/24156 [5:28:42<33:56:00, 17.49s/it]

 71%|███████   | 17171/24156 [5:29:04<36:22:33, 18.75s/it]

 71%|███████   | 17172/24156 [5:29:21<35:17:10, 18.19s/it]

 71%|███████   | 17173/24156 [5:29:42<37:02:47, 19.10s/it]

 71%|███████   | 17174/24156 [5:30:03<38:11:06, 19.69s/it]
[2024-04-05 20:35:00,639] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████   | 17175/24156 [5:30:15<33:20:14, 17.19s/it]

 71%|███████   | 17176/24156 [5:30:25<29:36:15, 15.27s/it]

 71%|███████   | 17177/24156 [5:30:43<30:50:01, 15.91s/it]

 71%|███████   | 17178/24156 [5:30:54<27:59:10, 14.44s/it]

 71%|███████   | 17179/24156 [5:31:14<31:22:15, 16.19s/it]

 71%|███████   | 17180/24156 [5:31:26<28:43:59, 14.83s/it]

 71%|███████   | 17181/24156 [5:31:36<26:21:29, 13.60s/it]

 71%|███████   | 17182/24156 [5:31:56<29:50:56, 15.41s/it]

 71%|███████   | 17183/24156 [5:32:16<32:13:46, 16.64s/it]

 71%|███████   | 17184/24156 [5:32:26<28:49:04, 14.88s/it]

 71%|███████   | 17185/24156 [5:32:49<33:23:41, 17.25s/it]

 71%|███████   | 17186/24156 [5:33:05<32:29:47, 16.78s/it]

 71%|███████   | 17187/24156 [5:33:27<35:24:48, 18.29s/it]

 71%|███████   | 17188/24156 [5:33:44<34:57:59, 18.07s/it]

 71%|███████   | 17189/24156 [5:33:57<32:06:04, 16.59s/it]

 71%|███████   | 17190/24156 [5:34:09<29:33:04, 15.27s/it]

 71%|███████   | 17191/24156 [5:34:25<29:35:13, 15.29s/it]

 71%|███████   | 17192/24156 [5:34:38<28:37:17, 14.80s/it]

 71%|███████   | 17193/24156 [5:34:54<29:15:19, 15.13s/it]

 71%|███████   | 17194/24156 [5:35:11<29:55:22, 15.47s/it]

 71%|███████   | 17195/24156 [5:35:23<27:51:58, 14.41s/it]

 71%|███████   | 17196/24156 [5:35:34<26:12:30, 13.56s/it]

 71%|███████   | 17197/24156 [5:35:51<28:03:02, 14.51s/it]

 71%|███████   | 17198/24156 [5:36:08<29:22:35, 15.20s/it]

 71%|███████   | 17199/24156 [5:36:27<31:56:08, 16.53s/it]

 71%|███████   | 17200/24156 [5:36:45<32:22:41, 16.76s/it]

 71%|███████   | 17201/24156 [5:37:05<34:41:10, 17.95s/it]

 71%|███████   | 17202/24156 [5:37:22<34:09:32, 17.68s/it]

 71%|███████   | 17203/24156 [5:37:35<31:23:39, 16.25s/it]

 71%|███████   | 17204/24156 [5:37:48<29:36:48, 15.33s/it]

 71%|███████   | 17205/24156 [5:38:08<32:03:18, 16.60s/it]

 71%|███████   | 17206/24156 [5:38:20<29:09:11, 15.10s/it]

 71%|███████   | 17207/24156 [5:38:38<30:49:22, 15.97s/it]

 71%|███████   | 17208/24156 [5:38:54<31:07:18, 16.13s/it]

 71%|███████   | 17209/24156 [5:39:06<28:53:16, 14.97s/it]

 71%|███████   | 17210/24156 [5:39:26<31:31:32, 16.34s/it]

 71%|███████   | 17211/24156 [5:39:43<31:47:26, 16.48s/it]

 71%|███████▏  | 17212/24156 [5:39:54<28:30:36, 14.78s/it]

 71%|███████▏  | 17213/24156 [5:40:11<29:57:49, 15.54s/it]

 71%|███████▏  | 17214/24156 [5:40:30<32:12:36, 16.70s/it]

 71%|███████▏  | 17215/24156 [5:40:51<34:43:50, 18.01s/it]

 71%|███████▏  | 17216/24156 [5:41:06<33:00:57, 17.13s/it]

 71%|███████▏  | 17217/24156 [5:41:18<29:32:39, 15.33s/it]

 71%|███████▏  | 17218/24156 [5:41:34<30:18:14, 15.72s/it]

 71%|███████▏  | 17219/24156 [5:41:46<27:56:34, 14.50s/it]

 71%|███████▏  | 17220/24156 [5:42:07<31:56:04, 16.58s/it]

 71%|███████▏  | 17221/24156 [5:42:30<35:39:46, 18.51s/it]
{'loss': 0.3644, 'learning_rate': 1.997789495577544e-06, 'rewards/chosen': -1.3808159828186035, 'rewards/rejected': -2.2136342525482178, 'rewards/accuracies': 0.625, 'rewards/margins': 0.832818329334259, 'policy_logps/rejected': -493.25830078125, 'policy_logps/chosen': -406.07275390625, 'referece_logps/rejected': -471.1219787597656, 'referece_logps/chosen': -392.26458740234375, 'logits/rejected': -1.1082799434661865, 'logits/chosen': -0.949659526348114, 'epoch': 6.42}


 71%|███████▏  | 17223/24156 [5:43:01<31:34:57, 16.40s/it]
{'loss': 0.414, 'learning_rate': 1.9977716396251466e-06, 'rewards/chosen': -2.453704595565796, 'rewards/rejected': -3.0546047687530518, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6009001135826111, 'policy_logps/rejected': -344.1955261230469, 'policy_logps/chosen': -340.27227783203125, 'referece_logps/rejected': -313.64947509765625, 'referece_logps/chosen': -315.7352294921875, 'logits/rejected': -0.7442597150802612, 'logits/chosen': -0.6832171082496643, 'epoch': 6.42}


 71%|███████▏  | 17225/24156 [5:43:36<32:41:38, 16.98s/it]

 71%|███████▏  | 17226/24156 [5:43:55<33:59:01, 17.65s/it]

 71%|███████▏  | 17227/24156 [5:44:06<30:09:28, 15.67s/it]

 71%|███████▏  | 17228/24156 [5:44:24<31:16:03, 16.25s/it]

 71%|███████▏  | 17229/24156 [5:44:43<33:01:10, 17.16s/it]
{'loss': 0.3314, 'learning_rate': 1.9977176412856685e-06, 'rewards/chosen': -1.7880806922912598, 'rewards/rejected': -3.139590263366699, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3515095710754395, 'policy_logps/rejected': -360.0946044921875, 'policy_logps/chosen': -416.369384765625, 'referece_logps/rejected': -328.69866943359375, 'referece_logps/chosen': -398.48858642578125, 'logits/rejected': -0.7246090173721313, 'logits/chosen': -0.8436284065246582, 'epoch': 6.42}

 71%|███████▏  | 17230/24156 [5:45:02<33:45:26, 17.55s/it]


 71%|███████▏  | 17232/24156 [5:45:40<35:16:57, 18.34s/it]

 71%|███████▏  | 17233/24156 [5:45:52<31:51:36, 16.57s/it]
{'loss': 0.4434, 'learning_rate': 1.997681283670402e-06, 'rewards/chosen': -1.172234296798706, 'rewards/rejected': -3.137331962585449, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9650976657867432, 'policy_logps/rejected': -449.53240966796875, 'policy_logps/chosen': -425.6149597167969, 'referece_logps/rejected': -418.15911865234375, 'referece_logps/chosen': -413.8927001953125, 'logits/rejected': -0.5480618476867676, 'logits/chosen': -0.4501717984676361, 'epoch': 6.42}


 71%|███████▏  | 17235/24156 [5:46:26<31:53:11, 16.59s/it]

 71%|███████▏  | 17236/24156 [5:46:36<28:28:21, 14.81s/it]

 71%|███████▏  | 17237/24156 [5:46:47<26:05:33, 13.58s/it]

 71%|███████▏  | 17238/24156 [5:47:04<27:49:52, 14.48s/it]

 71%|███████▏  | 17239/24156 [5:47:22<30:18:34, 15.77s/it]

 71%|███████▏  | 17240/24156 [5:47:33<27:22:10, 14.25s/it]
{'loss': 0.5506, 'learning_rate': 1.9976169673389584e-06, 'rewards/chosen': -1.4618638753890991, 'rewards/rejected': -1.6951515674591064, 'rewards/accuracies': 0.5, 'rewards/margins': 0.23328782618045807, 'policy_logps/rejected': -476.802001953125, 'policy_logps/chosen': -512.1093139648438, 'referece_logps/rejected': -459.8504943847656, 'referece_logps/chosen': -497.49066162109375, 'logits/rejected': -0.45156240463256836, 'logits/chosen': -0.2727290689945221, 'epoch': 6.42}


 71%|███████▏  | 17242/24156 [5:48:06<28:54:44, 15.05s/it]
{'loss': 0.3685, 'learning_rate': 1.9975984298340067e-06, 'rewards/chosen': -1.5680286884307861, 'rewards/rejected': -2.049954891204834, 'rewards/accuracies': 0.625, 'rewards/margins': 0.48192623257637024, 'policy_logps/rejected': -559.2926635742188, 'policy_logps/chosen': -452.5257263183594, 'referece_logps/rejected': -538.7930908203125, 'referece_logps/chosen': -436.845458984375, 'logits/rejected': -0.8874258399009705, 'logits/chosen': -0.8460738658905029, 'epoch': 6.42}


 71%|███████▏  | 17244/24156 [5:48:31<25:50:32, 13.46s/it]

 71%|███████▏  | 17245/24156 [5:48:47<27:26:09, 14.29s/it]

 71%|███████▏  | 17246/24156 [5:48:58<25:35:08, 13.33s/it]

 71%|███████▏  | 17247/24156 [5:49:18<29:12:34, 15.22s/it]

 71%|███████▏  | 17248/24156 [5:49:37<31:43:18, 16.53s/it]

 71%|███████▏  | 17249/24156 [5:49:49<28:54:00, 15.06s/it]

 71%|███████▏  | 17250/24156 [5:50:09<31:48:58, 16.59s/it]

 71%|███████▏  | 17251/24156 [5:50:27<32:20:35, 16.86s/it]

 71%|███████▏  | 17252/24156 [5:50:45<33:31:45, 17.48s/it]
{'loss': 0.4569, 'learning_rate': 1.997504666304298e-06, 'rewards/chosen': -1.7440210580825806, 'rewards/rejected': -2.7121834754943848, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9681624174118042, 'policy_logps/rejected': -361.6283874511719, 'policy_logps/chosen': -367.3117370605469, 'referece_logps/rejected': -334.506591796875, 'referece_logps/chosen': -349.87152099609375, 'logits/rejected': -0.23819975554943085, 'logits/chosen': -0.22329659759998322, 'epoch': 6.43}


 71%|███████▏  | 17254/24156 [5:51:19<32:27:54, 16.93s/it]
{'loss': 0.4148, 'learning_rate': 1.9974856984068057e-06, 'rewards/chosen': -2.1421890258789062, 'rewards/rejected': -2.7391252517700195, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5969361066818237, 'policy_logps/rejected': -353.79345703125, 'policy_logps/chosen': -355.56109619140625, 'referece_logps/rejected': -326.4021911621094, 'referece_logps/chosen': -334.13922119140625, 'logits/rejected': -0.9960537552833557, 'logits/chosen': -1.0053139925003052, 'epoch': 6.43}


 71%|███████▏  | 17256/24156 [5:51:52<32:34:14, 16.99s/it]

 71%|███████▏  | 17257/24156 [5:52:10<33:05:32, 17.27s/it]

 71%|███████▏  | 17258/24156 [5:52:29<33:56:32, 17.71s/it]
{'loss': 0.4023, 'learning_rate': 1.9974475474311516e-06, 'rewards/chosen': -2.4242236614227295, 'rewards/rejected': -4.083880424499512, 'rewards/accuracies': 0.5, 'rewards/margins': 1.6596570014953613, 'policy_logps/rejected': -371.1585388183594, 'policy_logps/chosen': -394.7537841796875, 'referece_logps/rejected': -330.3197326660156, 'referece_logps/chosen': -370.51153564453125, 'logits/rejected': 0.039122726768255234, 'logits/chosen': 0.12015745043754578, 'epoch': 6.43}


 71%|███████▏  | 17260/24156 [5:52:53<28:23:55, 14.83s/it]

 71%|███████▏  | 17261/24156 [5:53:12<31:04:43, 16.23s/it]
[2024-04-05 20:58:09,640] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 17262/24156 [5:53:31<32:40:26, 17.06s/it]
{'loss': 0.3418, 'learning_rate': 1.9974091095570906e-06, 'rewards/chosen': -2.0645992755889893, 'rewards/rejected': -4.850255966186523, 'rewards/accuracies': 1.0, 'rewards/margins': 2.785656452178955, 'policy_logps/rejected': -333.18389892578125, 'policy_logps/chosen': -348.6978454589844, 'referece_logps/rejected': -284.68133544921875, 'referece_logps/chosen': -328.0518798828125, 'logits/rejected': 0.07757581770420074, 'logits/chosen': -0.11779242753982544, 'epoch': 6.43}


 71%|███████▏  | 17264/24156 [5:54:03<32:24:25, 16.93s/it]
[2024-04-05 20:59:00,601] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3628, 'learning_rate': 1.9973897830366092e-06, 'rewards/chosen': -2.641594409942627, 'rewards/rejected': -4.257979393005371, 'rewards/accuracies': 0.75, 'rewards/margins': 1.616384744644165, 'policy_logps/rejected': -413.66766357421875, 'policy_logps/chosen': -419.736328125, 'referece_logps/rejected': -371.087890625, 'referece_logps/chosen': -393.3203430175781, 'logits/rejected': -0.8046172261238098, 'logits/chosen': -0.8938906192779541, 'epoch': 6.43}

 71%|███████▏  | 17265/24156 [5:54:21<32:46:25, 17.12s/it]
{'loss': 0.3988, 'learning_rate': 1.9973800928811132e-06, 'rewards/chosen': -1.4231947660446167, 'rewards/rejected': -1.9234212636947632, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5002264380455017, 'policy_logps/rejected': -346.03985595703125, 'policy_logps/chosen': -343.85772705078125, 'referece_logps/rejected': -326.8056640625, 'referece_logps/chosen': -329.62579345703125, 'logits/rejected': -0.7851871848106384, 'logits/chosen': -0.8548896312713623, 'epoch': 6.43}


 71%|███████▏  | 17267/24156 [5:54:47<28:40:17, 14.98s/it]
{'loss': 0.3779, 'learning_rate': 1.9973606587804807e-06, 'rewards/chosen': -1.5298964977264404, 'rewards/rejected': -2.0908889770507812, 'rewards/accuracies': 0.625, 'rewards/margins': 0.560992419719696, 'policy_logps/rejected': -469.6214599609375, 'policy_logps/chosen': -460.0653991699219, 'referece_logps/rejected': -448.71258544921875, 'referece_logps/chosen': -444.76641845703125, 'logits/rejected': -0.4713244140148163, 'logits/chosen': -0.5833261013031006, 'epoch': 6.43}


 71%|███████▏  | 17269/24156 [5:55:23<31:46:38, 16.61s/it]
[2024-04-05 21:00:20,383] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 71%|███████▏  | 17270/24156 [5:55:37<30:35:13, 15.99s/it]
{'loss': 0.3632, 'learning_rate': 1.997331373158055e-06, 'rewards/chosen': -1.4558809995651245, 'rewards/rejected': -2.934061288833618, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4781804084777832, 'policy_logps/rejected': -352.2928466796875, 'policy_logps/chosen': -450.18914794921875, 'referece_logps/rejected': -322.9522399902344, 'referece_logps/chosen': -435.6304016113281, 'logits/rejected': -0.4387815296649933, 'logits/chosen': -0.41520488262176514, 'epoch': 6.43}


 72%|███████▏  | 17272/24156 [5:56:14<33:56:37, 17.75s/it]

 72%|███████▏  | 17273/24156 [5:56:36<36:04:35, 18.87s/it]

 72%|███████▏  | 17274/24156 [5:56:56<36:54:58, 19.31s/it]

 72%|███████▏  | 17275/24156 [5:57:13<35:31:25, 18.59s/it]
{'loss': 0.4758, 'learning_rate': 1.997282205208454e-06, 'rewards/chosen': -1.5264902114868164, 'rewards/rejected': -4.106065273284912, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5795750617980957, 'policy_logps/rejected': -458.43817138671875, 'policy_logps/chosen': -595.3387451171875, 'referece_logps/rejected': -417.37750244140625, 'referece_logps/chosen': -580.0738525390625, 'logits/rejected': -0.7993483543395996, 'logits/chosen': -0.8030323386192322, 'epoch': 6.44}


 72%|███████▏  | 17277/24156 [5:57:50<34:55:42, 18.28s/it]

 72%|███████▏  | 17278/24156 [5:58:05<33:01:39, 17.29s/it]

 72%|███████▏  | 17279/24156 [5:58:20<31:51:45, 16.68s/it]

 72%|███████▏  | 17280/24156 [5:58:36<31:28:54, 16.48s/it]
{'loss': 0.4418, 'learning_rate': 1.9972325890544075e-06, 'rewards/chosen': -1.7594106197357178, 'rewards/rejected': -4.22776985168457, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4683592319488525, 'policy_logps/rejected': -392.125, 'policy_logps/chosen': -365.35833740234375, 'referece_logps/rejected': -349.8473205566406, 'referece_logps/chosen': -347.76422119140625, 'logits/rejected': -0.49366745352745056, 'logits/chosen': -0.34023356437683105, 'epoch': 6.44}

 72%|███████▏  | 17281/24156 [5:58:55<32:49:57, 17.19s/it]


 72%|███████▏  | 17283/24156 [5:59:22<29:14:01, 15.31s/it]

 72%|███████▏  | 17284/24156 [5:59:32<26:29:35, 13.88s/it]

 72%|███████▏  | 17285/24156 [5:59:48<27:19:00, 14.31s/it]

 72%|███████▏  | 17286/24156 [6:00:00<26:24:14, 13.84s/it]

 72%|███████▏  | 17287/24156 [6:00:11<24:42:15, 12.95s/it]

 72%|███████▏  | 17288/24156 [6:00:23<23:43:17, 12.43s/it]

 72%|███████▏  | 17289/24156 [6:00:43<28:05:21, 14.73s/it]
[2024-04-05 21:05:40,141] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17290/24156 [6:01:02<30:57:20, 16.23s/it]

 72%|███████▏  | 17291/24156 [6:01:19<31:22:33, 16.45s/it]

 72%|███████▏  | 17292/24156 [6:01:30<28:05:28, 14.73s/it]

 72%|███████▏  | 17293/24156 [6:01:51<31:26:39, 16.49s/it]

 72%|███████▏  | 17294/24156 [6:02:07<31:27:05, 16.50s/it]

 72%|███████▏  | 17295/24156 [6:02:20<29:12:16, 15.32s/it]

 72%|███████▏  | 17296/24156 [6:02:30<26:33:34, 13.94s/it]

 72%|███████▏  | 17297/24156 [6:02:47<28:18:05, 14.85s/it]

 72%|███████▏  | 17298/24156 [6:03:12<33:40:31, 17.68s/it]

 72%|███████▏  | 17299/24156 [6:03:30<33:49:07, 17.76s/it]

 72%|███████▏  | 17300/24156 [6:03:50<35:06:35, 18.44s/it]
{'loss': 0.4066, 'learning_rate': 1.9970296428427663e-06, 'rewards/chosen': -1.8514490127563477, 'rewards/rejected': -4.125691890716553, 'rewards/accuracies': 0.75, 'rewards/margins': 2.274242877960205, 'policy_logps/rejected': -299.4150695800781, 'policy_logps/chosen': -330.95782470703125, 'referece_logps/rejected': -258.1581726074219, 'referece_logps/chosen': -312.443359375, 'logits/rejected': -0.715435266494751, 'logits/chosen': -0.7233571410179138, 'epoch': 6.45}
[2024-04-05 21:09:08,511] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 72%|███████▏  | 17302/24156 [6:04:27<34:33:59, 18.16s/it]

 72%|███████▏  | 17303/24156 [6:04:41<32:12:49, 16.92s/it]
[2024-04-05 21:09:38,041] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17304/24156 [6:04:52<28:57:14, 15.21s/it]
{'loss': 0.4021, 'learning_rate': 1.9969881932187153e-06, 'rewards/chosen': -1.9642417430877686, 'rewards/rejected': -3.110300064086914, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1460585594177246, 'policy_logps/rejected': -412.0121154785156, 'policy_logps/chosen': -420.75152587890625, 'referece_logps/rejected': -380.90911865234375, 'referece_logps/chosen': -401.109130859375, 'logits/rejected': 0.5624552965164185, 'logits/chosen': 0.3744671642780304, 'epoch': 6.45}


 72%|███████▏  | 17306/24156 [6:05:32<33:48:21, 17.77s/it]
[2024-04-05 21:10:29,463] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3641, 'learning_rate': 1.9969673608685856e-06, 'rewards/chosen': -2.1595351696014404, 'rewards/rejected': -3.7593977451324463, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5998624563217163, 'policy_logps/rejected': -494.9033203125, 'policy_logps/chosen': -474.6536865234375, 'referece_logps/rejected': -457.309326171875, 'referece_logps/chosen': -453.0583801269531, 'logits/rejected': -0.30723869800567627, 'logits/chosen': -0.13514190912246704, 'epoch': 6.45}


 72%|███████▏  | 17308/24156 [6:06:08<33:52:34, 17.81s/it]
{'loss': 0.3175, 'learning_rate': 1.996946456828383e-06, 'rewards/chosen': -1.338608741760254, 'rewards/rejected': -3.5821516513824463, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2435426712036133, 'policy_logps/rejected': -361.35125732421875, 'policy_logps/chosen': -363.5166931152344, 'referece_logps/rejected': -325.52972412109375, 'referece_logps/chosen': -350.130615234375, 'logits/rejected': -0.38399815559387207, 'logits/chosen': -0.27990907430648804, 'epoch': 6.45}


 72%|███████▏  | 17310/24156 [6:06:31<27:39:27, 14.54s/it]

 72%|███████▏  | 17311/24156 [6:06:42<25:34:19, 13.45s/it]
{'loss': 0.3563, 'learning_rate': 1.9969149663524794e-06, 'rewards/chosen': -1.0138859748840332, 'rewards/rejected': -3.436432361602783, 'rewards/accuracies': 0.875, 'rewards/margins': 2.422546863555908, 'policy_logps/rejected': -500.6280212402344, 'policy_logps/chosen': -519.0361938476562, 'referece_logps/rejected': -466.263671875, 'referece_logps/chosen': -508.8973388671875, 'logits/rejected': -0.6758185625076294, 'logits/chosen': -0.5907547473907471, 'epoch': 6.45}


 72%|███████▏  | 17313/24156 [6:07:07<24:43:30, 13.01s/it]

 72%|███████▏  | 17314/24156 [6:07:21<25:17:37, 13.31s/it]

 72%|███████▏  | 17315/24156 [6:07:42<30:07:19, 15.85s/it]
{'loss': 0.3772, 'learning_rate': 1.9968727281500905e-06, 'rewards/chosen': -1.8875195980072021, 'rewards/rejected': -4.442912578582764, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5553929805755615, 'policy_logps/rejected': -330.1741943359375, 'policy_logps/chosen': -408.2058410644531, 'referece_logps/rejected': -285.7450866699219, 'referece_logps/chosen': -389.33062744140625, 'logits/rejected': 0.04107050597667694, 'logits/chosen': 0.0569864958524704, 'epoch': 6.45}

 72%|███████▏  | 17316/24156 [6:08:03<32:59:53, 17.37s/it]
[2024-04-05 21:13:22,873] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17317/24156 [6:08:25<35:40:42, 18.78s/it]


 72%|███████▏  | 17319/24156 [6:08:54<30:57:42, 16.30s/it]

 72%|███████▏  | 17320/24156 [6:09:13<32:06:55, 16.91s/it]
{'loss': 0.3986, 'learning_rate': 1.996819527180156e-06, 'rewards/chosen': -1.345890998840332, 'rewards/rejected': -3.343435764312744, 'rewards/accuracies': 0.75, 'rewards/margins': 1.997544765472412, 'policy_logps/rejected': -243.3157501220703, 'policy_logps/chosen': -331.2480773925781, 'referece_logps/rejected': -209.8813934326172, 'referece_logps/chosen': -317.7891845703125, 'logits/rejected': -0.6668840646743774, 'logits/chosen': -0.7279936671257019, 'epoch': 6.45}


 72%|███████▏  | 17322/24156 [6:09:34<26:07:01, 13.76s/it]

 72%|███████▏  | 17323/24156 [6:09:45<24:21:13, 12.83s/it]
{'loss': 0.3052, 'learning_rate': 1.996787391558334e-06, 'rewards/chosen': -1.5574449300765991, 'rewards/rejected': -2.764479398727417, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2070343494415283, 'policy_logps/rejected': -430.1084289550781, 'policy_logps/chosen': -390.27508544921875, 'referece_logps/rejected': -402.46368408203125, 'referece_logps/chosen': -374.70062255859375, 'logits/rejected': -1.2173209190368652, 'logits/chosen': -1.2551213502883911, 'epoch': 6.45}

 72%|███████▏  | 17324/24156 [6:09:56<23:09:54, 12.21s/it]


 72%|███████▏  | 17326/24156 [6:10:24<24:37:50, 12.98s/it]
{'loss': 0.5565, 'learning_rate': 1.996755094662965e-06, 'rewards/chosen': -1.3538626432418823, 'rewards/rejected': -2.1572203636169434, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8033578991889954, 'policy_logps/rejected': -298.1413269042969, 'policy_logps/chosen': -421.29669189453125, 'referece_logps/rejected': -276.569091796875, 'referece_logps/chosen': -407.758056640625, 'logits/rejected': -0.5062280893325806, 'logits/chosen': -0.5106600522994995, 'epoch': 6.46}


 72%|███████▏  | 17328/24156 [6:10:51<25:00:38, 13.19s/it]

 72%|███████▏  | 17329/24156 [6:11:13<29:53:46, 15.76s/it]
[2024-04-05 21:16:10,521] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4228, 'learning_rate': 1.9967226364992753e-06, 'rewards/chosen': -2.2160398960113525, 'rewards/rejected': -3.895528793334961, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6794887781143188, 'policy_logps/rejected': -506.240966796875, 'policy_logps/chosen': -497.91888427734375, 'referece_logps/rejected': -467.28564453125, 'referece_logps/chosen': -475.7584228515625, 'logits/rejected': -0.04563853144645691, 'logits/chosen': -0.09876591712236404, 'epoch': 6.46}
[2024-04-05 21:16:23,097] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 72%|███████▏  | 17331/24156 [6:11:43<29:44:44, 15.69s/it]
{'loss': 0.4059, 'learning_rate': 1.9967009081325626e-06, 'rewards/chosen': -1.3201532363891602, 'rewards/rejected': -2.4768145084381104, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1566612720489502, 'policy_logps/rejected': -423.709716796875, 'policy_logps/chosen': -363.5707092285156, 'referece_logps/rejected': -398.9415283203125, 'referece_logps/chosen': -350.3691711425781, 'logits/rejected': -0.47578686475753784, 'logits/chosen': -0.3832097053527832, 'epoch': 6.46}


 72%|███████▏  | 17333/24156 [6:12:22<33:39:26, 17.76s/it]

 72%|███████▏  | 17334/24156 [6:12:39<32:59:29, 17.41s/it]

 72%|███████▏  | 17335/24156 [6:13:00<35:00:24, 18.48s/it]
{'loss': 0.3314, 'learning_rate': 1.996657236387964e-06, 'rewards/chosen': -1.8696703910827637, 'rewards/rejected': -2.538673162460327, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6690026521682739, 'policy_logps/rejected': -244.3441619873047, 'policy_logps/chosen': -385.0142822265625, 'referece_logps/rejected': -218.95742797851562, 'referece_logps/chosen': -366.3175964355469, 'logits/rejected': -1.0392491817474365, 'logits/chosen': -1.1384423971176147, 'epoch': 6.46}


 72%|███████▏  | 17337/24156 [6:13:35<33:56:52, 17.92s/it]

 72%|███████▏  | 17338/24156 [6:13:55<34:40:36, 18.31s/it]

 72%|███████▏  | 17339/24156 [6:14:16<36:07:46, 19.08s/it]
[2024-04-05 21:19:12,990] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17340/24156 [6:14:36<36:38:59, 19.36s/it]
{'loss': 0.3375, 'learning_rate': 1.996602243577479e-06, 'rewards/chosen': -0.9806541800498962, 'rewards/rejected': -3.5562503337860107, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5755960941314697, 'policy_logps/rejected': -375.303466796875, 'policy_logps/chosen': -455.6194763183594, 'referece_logps/rejected': -339.74102783203125, 'referece_logps/chosen': -445.8128967285156, 'logits/rejected': -1.080937385559082, 'logits/chosen': -0.9699577689170837, 'epoch': 6.46}


 72%|███████▏  | 17342/24156 [6:15:07<32:46:19, 17.31s/it]

 72%|███████▏  | 17343/24156 [6:15:22<31:04:33, 16.42s/it]

 72%|███████▏  | 17344/24156 [6:15:43<33:51:56, 17.90s/it]
{'loss': 0.3967, 'learning_rate': 1.996557926840724e-06, 'rewards/chosen': -1.0813905000686646, 'rewards/rejected': -4.225111961364746, 'rewards/accuracies': 0.875, 'rewards/margins': 3.143721580505371, 'policy_logps/rejected': -378.07025146484375, 'policy_logps/chosen': -479.4351501464844, 'referece_logps/rejected': -335.8191223144531, 'referece_logps/chosen': -468.6212463378906, 'logits/rejected': 0.06652046740055084, 'logits/chosen': 0.005011171102523804, 'epoch': 6.46}


 72%|███████▏  | 17346/24156 [6:16:21<34:36:03, 18.29s/it]
{'loss': 0.4944, 'learning_rate': 1.9965356609806e-06, 'rewards/chosen': -1.6273739337921143, 'rewards/rejected': -3.2521238327026367, 'rewards/accuracies': 0.75, 'rewards/margins': 1.624750018119812, 'policy_logps/rejected': -417.5274353027344, 'policy_logps/chosen': -452.24609375, 'referece_logps/rejected': -385.0061950683594, 'referece_logps/chosen': -435.972412109375, 'logits/rejected': -0.26312941312789917, 'logits/chosen': -0.2767788767814636, 'epoch': 6.46}


 72%|███████▏  | 17348/24156 [6:16:53<32:19:06, 17.09s/it]

 72%|███████▏  | 17349/24156 [6:17:13<33:49:08, 17.89s/it]
{'loss': 0.433, 'learning_rate': 1.9965021278302334e-06, 'rewards/chosen': -1.6446545124053955, 'rewards/rejected': -3.5813539028167725, 'rewards/accuracies': 0.875, 'rewards/margins': 1.936699390411377, 'policy_logps/rejected': -449.829345703125, 'policy_logps/chosen': -475.37054443359375, 'referece_logps/rejected': -414.0157775878906, 'referece_logps/chosen': -458.9240417480469, 'logits/rejected': -0.6146775484085083, 'logits/chosen': -0.40183037519454956, 'epoch': 6.46}


 72%|███████▏  | 17351/24156 [6:17:43<30:18:50, 16.04s/it]

 72%|███████▏  | 17352/24156 [6:17:54<27:18:37, 14.45s/it]
{'loss': 0.5241, 'learning_rate': 1.996468433452474e-06, 'rewards/chosen': -1.2891697883605957, 'rewards/rejected': -2.5378122329711914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2486425638198853, 'policy_logps/rejected': -672.5128784179688, 'policy_logps/chosen': -516.2403564453125, 'referece_logps/rejected': -647.134765625, 'referece_logps/chosen': -503.3486328125, 'logits/rejected': -1.1532427072525024, 'logits/chosen': -1.0556062459945679, 'epoch': 6.46}


 72%|███████▏  | 17354/24156 [6:18:25<28:56:29, 15.32s/it]

 72%|███████▏  | 17355/24156 [6:18:48<32:47:25, 17.36s/it]
[2024-04-05 21:23:45,102] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17356/24156 [6:19:05<32:54:58, 17.43s/it]
{'loss': 0.4256, 'learning_rate': 1.996423256826721e-06, 'rewards/chosen': -2.2136423587799072, 'rewards/rejected': -3.3351328372955322, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1214903593063354, 'policy_logps/rejected': -323.9154052734375, 'policy_logps/chosen': -310.89678955078125, 'referece_logps/rejected': -290.5640869140625, 'referece_logps/chosen': -288.7603759765625, 'logits/rejected': -0.6875106692314148, 'logits/chosen': -0.7141430974006653, 'epoch': 6.47}

 72%|███████▏  | 17357/24156 [6:19:22<32:44:59, 17.34s/it]


 72%|███████▏  | 17359/24156 [6:20:00<34:12:03, 18.11s/it]

 72%|███████▏  | 17360/24156 [6:20:19<35:02:08, 18.56s/it]

 72%|███████▏  | 17361/24156 [6:20:33<32:22:38, 17.15s/it]
{'loss': 0.4776, 'learning_rate': 1.996366383009484e-06, 'rewards/chosen': -1.6544950008392334, 'rewards/rejected': -2.406264305114746, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7517694234848022, 'policy_logps/rejected': -326.0673828125, 'policy_logps/chosen': -271.0755310058594, 'referece_logps/rejected': -302.0047302246094, 'referece_logps/chosen': -254.5305938720703, 'logits/rejected': -0.5807973146438599, 'logits/chosen': -0.7532422542572021, 'epoch': 6.47}

 72%|███████▏  | 17362/24156 [6:20:55<34:52:33, 18.48s/it]

 72%|███████▏  | 17363/24156 [6:21:10<33:13:09, 17.60s/it]


 72%|███████▏  | 17365/24156 [6:21:38<29:09:10, 15.45s/it]

 72%|███████▏  | 17366/24156 [6:21:50<26:59:43, 14.31s/it]

 72%|███████▏  | 17367/24156 [6:22:10<30:16:27, 16.05s/it]
[2024-04-05 21:27:07,184] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17368/24156 [6:22:24<29:04:17, 15.42s/it]

 72%|███████▏  | 17369/24156 [6:22:45<32:38:04, 17.31s/it]

 72%|███████▏  | 17370/24156 [6:22:57<29:30:32, 15.65s/it]
{'loss': 0.5409, 'learning_rate': 1.996262881717778e-06, 'rewards/chosen': -1.0656771659851074, 'rewards/rejected': -1.6440508365631104, 'rewards/accuracies': 0.375, 'rewards/margins': 0.5783735513687134, 'policy_logps/rejected': -424.2823791503906, 'policy_logps/chosen': -432.1240234375, 'referece_logps/rejected': -407.84185791015625, 'referece_logps/chosen': -421.46722412109375, 'logits/rejected': -1.33689546585083, 'logits/chosen': -1.3353627920150757, 'epoch': 6.47}

 72%|███████▏  | 17371/24156 [6:23:19<32:54:32, 17.46s/it]

 72%|███████▏  | 17372/24156 [6:23:31<29:51:27, 15.84s/it]


 72%|███████▏  | 17374/24156 [6:24:01<29:03:29, 15.42s/it]

 72%|███████▏  | 17375/24156 [6:24:22<32:01:48, 17.00s/it]
{'loss': 0.3003, 'learning_rate': 1.9962047541456127e-06, 'rewards/chosen': -1.6704097986221313, 'rewards/rejected': -1.9539884328842163, 'rewards/accuracies': 0.5, 'rewards/margins': 0.28357863426208496, 'policy_logps/rejected': -370.3687744140625, 'policy_logps/chosen': -419.09124755859375, 'referece_logps/rejected': -350.82891845703125, 'referece_logps/chosen': -402.38714599609375, 'logits/rejected': 0.20119085907936096, 'logits/chosen': 0.14207947254180908, 'epoch': 6.47}


 72%|███████▏  | 17377/24156 [6:24:58<32:42:21, 17.37s/it]

 72%|███████▏  | 17378/24156 [6:25:12<30:29:52, 16.20s/it]

 72%|███████▏  | 17379/24156 [6:25:31<32:30:32, 17.27s/it]

 72%|███████▏  | 17380/24156 [6:25:53<35:11:46, 18.70s/it]
{'loss': 0.3918, 'learning_rate': 1.996146178853237e-06, 'rewards/chosen': -0.6884049773216248, 'rewards/rejected': -2.796520948410034, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1081161499023438, 'policy_logps/rejected': -351.21002197265625, 'policy_logps/chosen': -485.6781311035156, 'referece_logps/rejected': -323.24481201171875, 'referece_logps/chosen': -478.7940673828125, 'logits/rejected': -0.6216517686843872, 'logits/chosen': -0.4879114627838135, 'epoch': 6.48}

 72%|███████▏  | 17381/24156 [6:26:11<34:31:51, 18.35s/it]


 72%|███████▏  | 17383/24156 [6:26:44<32:07:36, 17.08s/it]

 72%|███████▏  | 17384/24156 [6:27:02<32:41:29, 17.38s/it]
{'loss': 0.2937, 'learning_rate': 1.996098996278468e-06, 'rewards/chosen': -1.1520839929580688, 'rewards/rejected': -4.23314094543457, 'rewards/accuracies': 0.875, 'rewards/margins': 3.081057071685791, 'policy_logps/rejected': -445.5451354980469, 'policy_logps/chosen': -467.5589599609375, 'referece_logps/rejected': -403.2137451171875, 'referece_logps/chosen': -456.0381164550781, 'logits/rejected': -0.8804660439491272, 'logits/chosen': -0.9983227849006653, 'epoch': 6.48}


 72%|███████▏  | 17386/24156 [6:27:38<32:40:33, 17.38s/it]
{'loss': 0.4172, 'learning_rate': 1.9960752975487875e-06, 'rewards/chosen': -1.009303331375122, 'rewards/rejected': -2.4997119903564453, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4904087781906128, 'policy_logps/rejected': -632.404052734375, 'policy_logps/chosen': -627.6915283203125, 'referece_logps/rejected': -607.4069213867188, 'referece_logps/chosen': -617.598388671875, 'logits/rejected': 0.21376700699329376, 'logits/chosen': 0.40362417697906494, 'epoch': 6.48}


 72%|███████▏  | 17388/24156 [6:28:02<27:45:33, 14.77s/it]
{'loss': 0.3277, 'learning_rate': 1.9960515271931797e-06, 'rewards/chosen': -1.3566652536392212, 'rewards/rejected': -3.3957083225250244, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0390431880950928, 'policy_logps/rejected': -286.44189453125, 'policy_logps/chosen': -391.665283203125, 'referece_logps/rejected': -252.4848175048828, 'referece_logps/chosen': -378.0986328125, 'logits/rejected': -0.7488019466400146, 'logits/chosen': -0.6446788311004639, 'epoch': 6.48}

 72%|███████▏  | 17389/24156 [6:28:23<31:12:44, 16.60s/it]


 72%|███████▏  | 17391/24156 [6:28:52<29:28:28, 15.68s/it]

 72%|███████▏  | 17392/24156 [6:29:12<31:29:17, 16.76s/it]
{'loss': 0.5001, 'learning_rate': 1.9960037716110257e-06, 'rewards/chosen': -1.7613283395767212, 'rewards/rejected': -5.162750244140625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4014220237731934, 'policy_logps/rejected': -490.773681640625, 'policy_logps/chosen': -452.7514343261719, 'referece_logps/rejected': -439.1461486816406, 'referece_logps/chosen': -435.1382141113281, 'logits/rejected': 0.06994965672492981, 'logits/chosen': 0.06733148545026779, 'epoch': 6.48}

 72%|███████▏  | 17393/24156 [6:29:27<30:31:13, 16.25s/it]


 72%|███████▏  | 17395/24156 [6:30:08<34:45:04, 18.50s/it]
{'loss': 0.3991, 'learning_rate': 1.995967766919102e-06, 'rewards/chosen': -1.710548758506775, 'rewards/rejected': -2.4085185527801514, 'rewards/accuracies': 0.875, 'rewards/margins': 0.697969913482666, 'policy_logps/rejected': -364.84173583984375, 'policy_logps/chosen': -269.8793029785156, 'referece_logps/rejected': -340.7565612792969, 'referece_logps/chosen': -252.7738037109375, 'logits/rejected': -0.7061337232589722, 'logits/chosen': -0.5858563184738159, 'epoch': 6.48}

 72%|███████▏  | 17396/24156 [6:30:23<32:44:20, 17.43s/it]


 72%|███████▏  | 17398/24156 [6:31:00<34:13:44, 18.23s/it]
[2024-04-05 21:35:57,966] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17399/24156 [6:31:21<35:18:34, 18.81s/it]
[2024-04-05 21:36:18,127] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4316, 'learning_rate': 1.9959195100005345e-06, 'rewards/chosen': -1.4411263465881348, 'rewards/rejected': -3.5547780990600586, 'rewards/accuracies': 0.625, 'rewards/margins': 2.113651752471924, 'policy_logps/rejected': -371.1148376464844, 'policy_logps/chosen': -429.276123046875, 'referece_logps/rejected': -335.5670471191406, 'referece_logps/chosen': -414.8648376464844, 'logits/rejected': -0.16921041905879974, 'logits/chosen': -0.19104871153831482, 'epoch': 6.48}

 72%|███████▏  | 17400/24156 [6:31:39<35:16:25, 18.80s/it]


 72%|███████▏  | 17402/24156 [6:32:10<32:17:04, 17.21s/it]

 72%|███████▏  | 17403/24156 [6:32:28<32:46:03, 17.47s/it]
{'loss': 0.336, 'learning_rate': 1.9958709666230738e-06, 'rewards/chosen': -1.3132437467575073, 'rewards/rejected': -3.1995315551757812, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8862881660461426, 'policy_logps/rejected': -284.9620666503906, 'policy_logps/chosen': -478.5448303222656, 'referece_logps/rejected': -252.96676635742188, 'referece_logps/chosen': -465.4124450683594, 'logits/rejected': -0.535779595375061, 'logits/chosen': -0.5281718969345093, 'epoch': 6.48}


 72%|███████▏  | 17405/24156 [6:33:02<32:02:47, 17.09s/it]

 72%|███████▏  | 17406/24156 [6:33:14<29:14:26, 15.60s/it]

 72%|███████▏  | 17407/24156 [6:33:34<31:30:04, 16.80s/it]
{'loss': 0.3421, 'learning_rate': 1.9958221368006827e-06, 'rewards/chosen': -1.6212611198425293, 'rewards/rejected': -2.934771776199341, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3135106563568115, 'policy_logps/rejected': -315.0702209472656, 'policy_logps/chosen': -478.0831298828125, 'referece_logps/rejected': -285.7225036621094, 'referece_logps/chosen': -461.87054443359375, 'logits/rejected': -0.3965737223625183, 'logits/chosen': -0.37407365441322327, 'epoch': 6.49}


 72%|███████▏  | 17409/24156 [6:34:09<31:00:21, 16.54s/it]

 72%|███████▏  | 17410/24156 [6:34:29<33:02:26, 17.63s/it]
{'loss': 0.4707, 'learning_rate': 1.9957853264628492e-06, 'rewards/chosen': -1.676047682762146, 'rewards/rejected': -3.808225631713867, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1321780681610107, 'policy_logps/rejected': -351.9771728515625, 'policy_logps/chosen': -438.086669921875, 'referece_logps/rejected': -313.89495849609375, 'referece_logps/chosen': -421.326171875, 'logits/rejected': -0.1878688633441925, 'logits/chosen': -0.2570934593677521, 'epoch': 6.49}

 72%|███████▏  | 17411/24156 [6:34:42<30:20:04, 16.19s/it]
[2024-04-05 21:40:02,829] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 72%|███████▏  | 17413/24156 [6:35:18<31:17:53, 16.71s/it]
{'loss': 0.3718, 'learning_rate': 1.9957483550135965e-06, 'rewards/chosen': -1.5559980869293213, 'rewards/rejected': -2.829986095428467, 'rewards/accuracies': 0.875, 'rewards/margins': 1.273988127708435, 'policy_logps/rejected': -478.31427001953125, 'policy_logps/chosen': -442.4617614746094, 'referece_logps/rejected': -450.0144348144531, 'referece_logps/chosen': -426.9017028808594, 'logits/rejected': 0.3438229560852051, 'logits/chosen': 0.32667914032936096, 'epoch': 6.49}

 72%|███████▏  | 17414/24156 [6:35:37<32:51:31, 17.55s/it]


 72%|███████▏  | 17416/24156 [6:36:09<30:59:44, 16.56s/it]

 72%|███████▏  | 17417/24156 [6:36:20<28:04:54, 15.00s/it]
{'loss': 0.5564, 'learning_rate': 1.995698809140506e-06, 'rewards/chosen': -1.9205939769744873, 'rewards/rejected': -3.17181658744812, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2512227296829224, 'policy_logps/rejected': -341.91009521484375, 'policy_logps/chosen': -313.4974060058594, 'referece_logps/rejected': -310.19189453125, 'referece_logps/chosen': -294.29144287109375, 'logits/rejected': -1.2714165449142456, 'logits/chosen': -1.2739086151123047, 'epoch': 6.49}

 72%|███████▏  | 17418/24156 [6:36:35<28:08:01, 15.03s/it]

 72%|███████▏  | 17419/24156 [6:36:56<31:17:47, 16.72s/it]


 72%|███████▏  | 17421/24156 [6:37:26<30:29:31, 16.30s/it]

 72%|███████▏  | 17422/24156 [6:37:46<32:26:00, 17.34s/it]
{'loss': 0.3977, 'learning_rate': 1.9956364740572712e-06, 'rewards/chosen': -1.548113465309143, 'rewards/rejected': -2.1799495220184326, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6318360567092896, 'policy_logps/rejected': -321.4346923828125, 'policy_logps/chosen': -297.0206298828125, 'referece_logps/rejected': -299.63519287109375, 'referece_logps/chosen': -281.53948974609375, 'logits/rejected': 0.30591121315956116, 'logits/chosen': 0.3180556297302246, 'epoch': 6.49}

 72%|███████▏  | 17423/24156 [6:38:02<31:27:15, 16.82s/it]

 72%|███████▏  | 17424/24156 [6:38:16<29:45:41, 15.92s/it]

 72%|███████▏  | 17425/24156 [6:38:30<28:47:41, 15.40s/it]


 72%|███████▏  | 17427/24156 [6:38:59<27:15:29, 14.58s/it]
{'loss': 0.3597, 'learning_rate': 1.9955736915092256e-06, 'rewards/chosen': -1.448133945465088, 'rewards/rejected': -2.3432207107543945, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8950867056846619, 'policy_logps/rejected': -394.51995849609375, 'policy_logps/chosen': -434.61212158203125, 'referece_logps/rejected': -371.0877380371094, 'referece_logps/chosen': -420.13079833984375, 'logits/rejected': -0.18719543516635895, 'logits/chosen': -0.19957634806632996, 'epoch': 6.49}

 72%|███████▏  | 17428/24156 [6:39:13<27:25:24, 14.67s/it]

 72%|███████▏  | 17429/24156 [6:39:27<27:04:53, 14.49s/it]


 72%|███████▏  | 17431/24156 [6:40:11<33:42:39, 18.05s/it]

 72%|███████▏  | 17432/24156 [6:40:30<34:47:18, 18.63s/it]

 72%|███████▏  | 17433/24156 [6:40:43<31:10:53, 16.70s/it]

 72%|███████▏  | 17434/24156 [6:40:55<28:28:11, 15.25s/it]

 72%|███████▏  | 17435/24156 [6:41:11<28:55:57, 15.50s/it]

 72%|███████▏  | 17436/24156 [6:41:27<29:20:24, 15.72s/it]

 72%|███████▏  | 17437/24156 [6:41:47<31:42:45, 16.99s/it]
{'loss': 0.4167, 'learning_rate': 1.9954467841317664e-06, 'rewards/chosen': -1.068427562713623, 'rewards/rejected': -2.241741180419922, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1733134984970093, 'policy_logps/rejected': -328.7442626953125, 'policy_logps/chosen': -374.3238525390625, 'referece_logps/rejected': -306.3268127441406, 'referece_logps/chosen': -363.6395568847656, 'logits/rejected': -1.029617428779602, 'logits/chosen': -1.0947906970977783, 'epoch': 6.5}


 72%|███████▏  | 17439/24156 [6:42:17<30:25:28, 16.31s/it]
{'loss': 0.3636, 'learning_rate': 1.995421187906753e-06, 'rewards/chosen': -1.2561031579971313, 'rewards/rejected': -3.6978085041046143, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4417054653167725, 'policy_logps/rejected': -449.99456787109375, 'policy_logps/chosen': -468.6524658203125, 'referece_logps/rejected': -413.01654052734375, 'referece_logps/chosen': -456.0914306640625, 'logits/rejected': 0.3972494304180145, 'logits/chosen': 0.37103286385536194, 'epoch': 6.5}


 72%|███████▏  | 17441/24156 [6:42:55<32:37:49, 17.49s/it]

 72%|███████▏  | 17442/24156 [6:43:15<33:59:39, 18.23s/it]
{'loss': 0.3313, 'learning_rate': 1.995382659359388e-06, 'rewards/chosen': -0.3867110311985016, 'rewards/rejected': -1.974351406097412, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5876405239105225, 'policy_logps/rejected': -412.09326171875, 'policy_logps/chosen': -451.5107421875, 'referece_logps/rejected': -392.3497619628906, 'referece_logps/chosen': -447.6436462402344, 'logits/rejected': 0.16107045114040375, 'logits/chosen': 0.14606022834777832, 'epoch': 6.5}


 72%|███████▏  | 17444/24156 [6:43:43<29:44:30, 15.95s/it]
{'loss': 0.3897, 'learning_rate': 1.995356884190609e-06, 'rewards/chosen': -2.2999954223632812, 'rewards/rejected': -3.8475518226623535, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5475561618804932, 'policy_logps/rejected': -328.19366455078125, 'policy_logps/chosen': -370.71002197265625, 'referece_logps/rejected': -289.7181701660156, 'referece_logps/chosen': -347.7100524902344, 'logits/rejected': -1.0753389596939087, 'logits/chosen': -1.0277773141860962, 'epoch': 6.5}

 72%|███████▏  | 17445/24156 [6:44:02<31:47:44, 17.06s/it]

 72%|███████▏  | 17446/24156 [6:44:19<31:57:02, 17.14s/it]

 72%|███████▏  | 17447/24156 [6:44:38<32:50:20, 17.62s/it]


 72%|███████▏  | 17449/24156 [6:45:19<35:19:11, 18.96s/it]
{'loss': 0.4266, 'learning_rate': 1.9952921331353088e-06, 'rewards/chosen': -1.3376463651657104, 'rewards/rejected': -2.4802942276000977, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1426479816436768, 'policy_logps/rejected': -463.6505432128906, 'policy_logps/chosen': -555.7084350585938, 'referece_logps/rejected': -438.84759521484375, 'referece_logps/chosen': -542.33203125, 'logits/rejected': -0.7722780704498291, 'logits/chosen': -0.9287622570991516, 'epoch': 6.5}

 72%|███████▏  | 17450/24156 [6:45:40<36:56:02, 19.83s/it]

 72%|███████▏  | 17451/24156 [6:45:59<36:09:19, 19.41s/it]

 72%|███████▏  | 17452/24156 [6:46:14<33:40:21, 18.08s/it]

 72%|███████▏  | 17453/24156 [6:46:24<29:30:18, 15.85s/it]

 72%|███████▏  | 17454/24156 [6:46:37<27:41:14, 14.87s/it]


 72%|███████▏  | 17456/24156 [6:47:07<27:11:30, 14.61s/it]
{'loss': 0.422, 'learning_rate': 1.9952007301835523e-06, 'rewards/chosen': -1.4660449028015137, 'rewards/rejected': -3.0157546997070312, 'rewards/accuracies': 0.75, 'rewards/margins': 1.549709677696228, 'policy_logps/rejected': -576.8573608398438, 'policy_logps/chosen': -485.8365478515625, 'referece_logps/rejected': -546.6998291015625, 'referece_logps/chosen': -471.1761474609375, 'logits/rejected': -0.26034948229789734, 'logits/chosen': -0.1788770705461502, 'epoch': 6.5}

 72%|███████▏  | 17457/24156 [6:47:23<27:40:58, 14.88s/it]


 72%|███████▏  | 17459/24156 [6:48:03<32:36:32, 17.53s/it]

 72%|███████▏  | 17460/24156 [6:48:22<32:57:33, 17.72s/it]
{'loss': 0.4755, 'learning_rate': 1.9951481063235246e-06, 'rewards/chosen': -1.7203916311264038, 'rewards/rejected': -2.103438138961792, 'rewards/accuracies': 0.625, 'rewards/margins': 0.38304662704467773, 'policy_logps/rejected': -329.4685974121094, 'policy_logps/chosen': -439.98468017578125, 'referece_logps/rejected': -308.4342346191406, 'referece_logps/chosen': -422.78070068359375, 'logits/rejected': -0.4920459985733032, 'logits/chosen': -0.4757052958011627, 'epoch': 6.51}

 72%|███████▏  | 17461/24156 [6:48:41<33:35:59, 18.07s/it]


 72%|███████▏  | 17463/24156 [6:49:15<32:43:49, 17.60s/it]
{'loss': 0.3591, 'learning_rate': 1.9951084505846337e-06, 'rewards/chosen': -1.5820374488830566, 'rewards/rejected': -3.070049524307251, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4880120754241943, 'policy_logps/rejected': -479.7552490234375, 'policy_logps/chosen': -501.8216247558594, 'referece_logps/rejected': -449.0547180175781, 'referece_logps/chosen': -486.00128173828125, 'logits/rejected': 1.0267610549926758, 'logits/chosen': 0.8847119808197021, 'epoch': 6.51}


 72%|███████▏  | 17465/24156 [6:49:52<32:54:00, 17.70s/it]
{'loss': 0.3325, 'learning_rate': 1.995081923979473e-06, 'rewards/chosen': -1.7477688789367676, 'rewards/rejected': -3.5606913566589355, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8129225969314575, 'policy_logps/rejected': -461.74951171875, 'policy_logps/chosen': -390.6337890625, 'referece_logps/rejected': -426.1426086425781, 'referece_logps/chosen': -373.15606689453125, 'logits/rejected': -0.48696309328079224, 'logits/chosen': -0.4915081262588501, 'epoch': 6.51}


 72%|███████▏  | 17467/24156 [6:50:26<32:34:52, 17.54s/it]
{'loss': 0.413, 'learning_rate': 1.9950553258198167e-06, 'rewards/chosen': -1.318441390991211, 'rewards/rejected': -1.8669931888580322, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5485518574714661, 'policy_logps/rejected': -405.4938049316406, 'policy_logps/chosen': -369.0521545410156, 'referece_logps/rejected': -386.8238525390625, 'referece_logps/chosen': -355.8677062988281, 'logits/rejected': -0.8143125176429749, 'logits/chosen': -0.7471656799316406, 'epoch': 6.51}


 72%|███████▏  | 17469/24156 [6:51:04<34:11:43, 18.41s/it]
{'loss': 0.2472, 'learning_rate': 1.9950286561075776e-06, 'rewards/chosen': -2.5085060596466064, 'rewards/rejected': -5.625748157501221, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1172423362731934, 'policy_logps/rejected': -602.7609252929688, 'policy_logps/chosen': -467.8855285644531, 'referece_logps/rejected': -546.5034790039062, 'referece_logps/chosen': -442.80047607421875, 'logits/rejected': -0.036701053380966187, 'logits/chosen': 0.1610901653766632, 'epoch': 6.51}

 72%|███████▏  | 17470/24156 [6:51:23<34:39:31, 18.66s/it]

 72%|███████▏  | 17471/24156 [6:51:41<34:22:47, 18.51s/it]

 72%|███████▏  | 17472/24156 [6:51:55<31:36:25, 17.02s/it]


 72%|███████▏  | 17474/24156 [6:52:30<32:45:22, 17.65s/it]

 72%|███████▏  | 17475/24156 [6:52:50<33:54:02, 18.27s/it]

 72%|███████▏  | 17476/24156 [6:53:06<32:46:20, 17.66s/it]

 72%|███████▏  | 17477/24156 [6:53:24<32:56:39, 17.76s/it]
{'loss': 0.3361, 'learning_rate': 1.9949212617712274e-06, 'rewards/chosen': -1.627691388130188, 'rewards/rejected': -3.6301188468933105, 'rewards/accuracies': 0.875, 'rewards/margins': 2.002427577972412, 'policy_logps/rejected': -575.90380859375, 'policy_logps/chosen': -485.7340087890625, 'referece_logps/rejected': -539.6025390625, 'referece_logps/chosen': -469.45709228515625, 'logits/rejected': -1.16411554813385, 'logits/chosen': -1.1411755084991455, 'epoch': 6.51}


 72%|███████▏  | 17479/24156 [6:54:02<34:17:09, 18.49s/it]
{'loss': 0.3942, 'learning_rate': 1.994894234324944e-06, 'rewards/chosen': -1.1264747381210327, 'rewards/rejected': -2.5826313495635986, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4561564922332764, 'policy_logps/rejected': -322.8650817871094, 'policy_logps/chosen': -329.72857666015625, 'referece_logps/rejected': -297.03875732421875, 'referece_logps/chosen': -318.4638366699219, 'logits/rejected': -0.303741991519928, 'logits/chosen': -0.2947203814983368, 'epoch': 6.51}

 72%|███████▏  | 17480/24156 [6:54:22<34:54:00, 18.82s/it]


 72%|███████▏  | 17482/24156 [6:55:00<35:04:13, 18.92s/it]
{'loss': 0.2911, 'learning_rate': 1.994853559016755e-06, 'rewards/chosen': -2.5439999103546143, 'rewards/rejected': -3.679378032684326, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1353780031204224, 'policy_logps/rejected': -499.12774658203125, 'policy_logps/chosen': -546.767578125, 'referece_logps/rejected': -462.33392333984375, 'referece_logps/chosen': -521.3275146484375, 'logits/rejected': -0.07347553968429565, 'logits/chosen': -0.20356965065002441, 'epoch': 6.51}

 72%|███████▏  | 17483/24156 [6:55:21<36:03:27, 19.45s/it]

 72%|███████▏  | 17484/24156 [6:55:43<37:42:01, 20.34s/it]

 72%|███████▏  | 17485/24156 [6:55:54<32:18:55, 17.44s/it]

 72%|███████▏  | 17486/24156 [6:56:12<32:33:20, 17.57s/it]


 72%|███████▏  | 17488/24156 [6:56:48<32:55:27, 17.78s/it]
{'loss': 0.2774, 'learning_rate': 1.994771725524988e-06, 'rewards/chosen': -2.0520994663238525, 'rewards/rejected': -4.42759370803833, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3754940032958984, 'policy_logps/rejected': -452.3045654296875, 'policy_logps/chosen': -453.3587646484375, 'referece_logps/rejected': -408.0286865234375, 'referece_logps/chosen': -432.8377685546875, 'logits/rejected': 1.2380402088165283, 'logits/chosen': 1.2506388425827026, 'epoch': 6.52}

 72%|███████▏  | 17489/24156 [6:57:07<33:54:35, 18.31s/it]


 72%|███████▏  | 17491/24156 [6:57:38<31:28:33, 17.00s/it]

 72%|███████▏  | 17492/24156 [6:58:02<35:15:04, 19.04s/it]
[2024-04-05 22:02:59,484] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17493/24156 [6:58:22<35:46:08, 19.33s/it]
[2024-04-05 22:03:19,470] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2761, 'learning_rate': 1.9947030391624747e-06, 'rewards/chosen': -2.181370735168457, 'rewards/rejected': -4.823635101318359, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6422643661499023, 'policy_logps/rejected': -515.2883911132812, 'policy_logps/chosen': -415.3958435058594, 'referece_logps/rejected': -467.0520935058594, 'referece_logps/chosen': -393.5821533203125, 'logits/rejected': -0.12414988875389099, 'logits/chosen': -0.12114106118679047, 'epoch': 6.52}

 72%|███████▏  | 17494/24156 [6:58:38<34:11:18, 18.47s/it]

 72%|███████▏  | 17495/24156 [6:58:53<31:53:00, 17.23s/it]

 72%|███████▏  | 17496/24156 [6:59:13<33:30:49, 18.12s/it]

 72%|███████▏  | 17497/24156 [6:59:24<29:26:14, 15.91s/it]

 72%|███████▏  | 17498/24156 [6:59:35<26:49:35, 14.51s/it]

 72%|███████▏  | 17499/24156 [6:59:56<30:12:22, 16.34s/it]

 72%|███████▏  | 17500/24156 [7:00:12<30:05:56, 16.28s/it]
[2024-04-05 22:05:47,334] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17501/24156 [7:00:50<42:12:23, 22.83s/it]

 72%|███████▏  | 17502/24156 [7:01:04<37:14:29, 20.15s/it]


 72%|███████▏  | 17504/24156 [7:01:36<34:22:17, 18.60s/it]
{'loss': 0.3836, 'learning_rate': 1.9945503556092474e-06, 'rewards/chosen': -2.667860507965088, 'rewards/rejected': -3.7811758518218994, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1133155822753906, 'policy_logps/rejected': -382.65496826171875, 'policy_logps/chosen': -417.7413635253906, 'referece_logps/rejected': -344.84326171875, 'referece_logps/chosen': -391.0627746582031, 'logits/rejected': -1.4309781789779663, 'logits/chosen': -1.2797499895095825, 'epoch': 6.52}
[2024-04-05 22:06:50,871] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 17505/24156 [7:01:53<33:34:50, 18.18s/it]

 72%|███████▏  | 17506/24156 [7:02:09<31:59:34, 17.32s/it]

 72%|███████▏  | 17507/24156 [7:02:28<32:54:53, 17.82s/it]

 72%|███████▏  | 17508/24156 [7:02:46<33:06:46, 17.93s/it]


 72%|███████▏  | 17510/24156 [7:03:26<35:12:25, 19.07s/it]
{'loss': 0.2572, 'learning_rate': 1.9944661618174233e-06, 'rewards/chosen': -1.0047290325164795, 'rewards/rejected': -5.057435035705566, 'rewards/accuracies': 1.0, 'rewards/margins': 4.052706241607666, 'policy_logps/rejected': -238.22268676757812, 'policy_logps/chosen': -410.91455078125, 'referece_logps/rejected': -187.64833068847656, 'referece_logps/chosen': -400.8672790527344, 'logits/rejected': -0.20957745611667633, 'logits/chosen': -0.3007870316505432, 'epoch': 6.52}

 72%|███████▏  | 17511/24156 [7:03:44<34:16:20, 18.57s/it]

 72%|███████▏  | 17512/24156 [7:04:02<34:10:48, 18.52s/it]

 72%|███████▏  | 17513/24156 [7:04:20<33:37:31, 18.22s/it]

 73%|███████▎  | 17514/24156 [7:04:34<31:09:14, 16.89s/it]

 73%|███████▎  | 17515/24156 [7:04:49<30:29:11, 16.53s/it]

 73%|███████▎  | 17516/24156 [7:05:10<32:40:40, 17.72s/it]


 73%|███████▎  | 17518/24156 [7:05:43<32:03:01, 17.38s/it]
{'loss': 0.4344, 'learning_rate': 1.9943529022948115e-06, 'rewards/chosen': -1.6045383214950562, 'rewards/rejected': -4.344053268432617, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7395148277282715, 'policy_logps/rejected': -410.2382507324219, 'policy_logps/chosen': -283.082275390625, 'referece_logps/rejected': -366.79766845703125, 'referece_logps/chosen': -267.0368957519531, 'logits/rejected': -0.9170367121696472, 'logits/chosen': -0.5158298015594482, 'epoch': 6.53}

 73%|███████▎  | 17519/24156 [7:06:00<32:07:28, 17.42s/it]

 73%|███████▎  | 17520/24156 [7:06:20<33:21:38, 18.10s/it]

 73%|███████▎  | 17521/24156 [7:06:34<31:10:11, 16.91s/it]

 73%|███████▎  | 17522/24156 [7:06:45<27:54:02, 15.14s/it]

 73%|███████▎  | 17523/24156 [7:06:58<26:40:55, 14.48s/it]
[2024-04-05 22:12:16,814] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 73%|███████▎  | 17525/24156 [7:07:41<33:10:24, 18.01s/it]
[2024-04-05 22:12:38,264] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4929, 'learning_rate': 1.994252861743419e-06, 'rewards/chosen': -1.7342042922973633, 'rewards/rejected': -1.4202924966812134, 'rewards/accuracies': 0.375, 'rewards/margins': -0.31391167640686035, 'policy_logps/rejected': -322.9908447265625, 'policy_logps/chosen': -483.97467041015625, 'referece_logps/rejected': -308.7879333496094, 'referece_logps/chosen': -466.6326599121094, 'logits/rejected': -0.27766692638397217, 'logits/chosen': -0.2586735486984253, 'epoch': 6.53}

 73%|███████▎  | 17526/24156 [7:08:02<34:52:57, 18.94s/it]

 73%|███████▎  | 17527/24156 [7:08:19<33:57:13, 18.44s/it]

 73%|███████▎  | 17528/24156 [7:08:39<34:48:44, 18.91s/it]

 73%|███████▎  | 17529/24156 [7:09:00<35:59:35, 19.55s/it]

 73%|███████▎  | 17530/24156 [7:09:15<33:37:08, 18.27s/it]

 73%|███████▎  | 17531/24156 [7:09:33<33:18:47, 18.10s/it]

 73%|███████▎  | 17532/24156 [7:09:53<34:00:10, 18.48s/it]
[2024-04-05 22:15:07,832] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17533/24156 [7:10:10<33:36:47, 18.27s/it]

 73%|███████▎  | 17534/24156 [7:10:27<32:48:04, 17.83s/it]

 73%|███████▎  | 17535/24156 [7:10:44<32:10:28, 17.49s/it]

 73%|███████▎  | 17536/24156 [7:11:04<33:26:19, 18.18s/it]

 73%|███████▎  | 17537/24156 [7:11:20<32:16:05, 17.55s/it]

 73%|███████▎  | 17538/24156 [7:11:38<32:28:51, 17.67s/it]

 73%|███████▎  | 17539/24156 [7:11:56<32:58:22, 17.94s/it]

 73%|███████▎  | 17540/24156 [7:12:15<33:41:24, 18.33s/it]

 73%|███████▎  | 17541/24156 [7:12:35<34:09:43, 18.59s/it]


 73%|███████▎  | 17543/24156 [7:13:01<29:00:22, 15.79s/it]
{'loss': 0.3788, 'learning_rate': 1.993991593238671e-06, 'rewards/chosen': -1.5497616529464722, 'rewards/rejected': -4.68701171875, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1372499465942383, 'policy_logps/rejected': -383.412353515625, 'policy_logps/chosen': -427.0087585449219, 'referece_logps/rejected': -336.542236328125, 'referece_logps/chosen': -411.5111389160156, 'logits/rejected': -0.3876117467880249, 'logits/chosen': -0.27027255296707153, 'epoch': 6.54}

 73%|███████▎  | 17544/24156 [7:13:22<32:05:49, 17.48s/it]

 73%|███████▎  | 17545/24156 [7:13:45<34:39:19, 18.87s/it]


 73%|███████▎  | 17547/24156 [7:14:17<31:25:33, 17.12s/it]

{'loss': 0.3427, 'learning_rate': 1.9939327472800795e-06, 'rewards/chosen': -0.8417961001396179, 'rewards/rejected': -3.581839084625244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7400429248809814, 'policy_logps/rejected': -344.0550231933594, 'policy_logps/chosen': -416.9520568847656, 'referece_logps/rejected': -308.23663330078125, 'referece_logps/chosen': -408.5340881347656, 'logits/rejected': 0.05597085505723953, 'logits/chosen': 0.0806719958782196, 'epoch': 6.54}
{'loss': 0.4118, 'learning_rate': 1.9939179911198567e-06, 'rewards/chosen': -1.494097113609314, 'rewards/rejected': -2.1477653980255127, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6536681652069092, 'policy_logps/rejected': -560.1675415039062, 'policy_logps/chosen': -462.7834167480469, 'referece_logps/rejected': -538.68994140625, 'referece_logps/chosen': -447.84246826171875, 'logits/rejected': 0.1728236824274063, 'logits/chosen': 0.08275540173053741, 'epoch': 6.54}
 73%|███████▎  | 17548/24156 [7:14:31<29:48:13, 16.24s/it]

 73%|███████▎  | 17549/24156 [7:14:51<32:04:37, 17.48s/it]


 73%|███████▎  | 17551/24156 [7:15:17<27:46:47, 15.14s/it]
{'loss': 0.3823, 'learning_rate': 1.993873615434052e-06, 'rewards/chosen': -1.3927196264266968, 'rewards/rejected': -3.915304183959961, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5225844383239746, 'policy_logps/rejected': -393.40325927734375, 'policy_logps/chosen': -382.1866760253906, 'referece_logps/rejected': -354.2502136230469, 'referece_logps/chosen': -368.259521484375, 'logits/rejected': 0.5124931335449219, 'logits/chosen': 0.4849884808063507, 'epoch': 6.54}

 73%|███████▎  | 17552/24156 [7:15:38<30:48:42, 16.80s/it]

 73%|███████▎  | 17553/24156 [7:16:01<34:20:57, 18.73s/it]

 73%|███████▎  | 17554/24156 [7:16:19<33:43:33, 18.39s/it]

 73%|███████▎  | 17555/24156 [7:16:33<31:22:08, 17.11s/it]
[2024-04-05 22:21:51,350] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17556/24156 [7:16:54<33:28:50, 18.26s/it]

 73%|███████▎  | 17557/24156 [7:17:11<32:35:52, 17.78s/it]

 73%|███████▎  | 17558/24156 [7:17:29<33:02:03, 18.02s/it]

 73%|███████▎  | 17559/24156 [7:17:51<35:12:29, 19.21s/it]


 73%|███████▎  | 17561/24156 [7:18:21<31:42:32, 17.31s/it]

 73%|███████▎  | 17562/24156 [7:18:33<28:45:54, 15.70s/it]

 73%|███████▎  | 17563/24156 [7:18:46<26:56:51, 14.71s/it]

 73%|███████▎  | 17564/24156 [7:19:01<27:05:56, 14.80s/it]

 73%|███████▎  | 17565/24156 [7:19:20<29:38:56, 16.19s/it]

 73%|███████▎  | 17566/24156 [7:19:37<30:08:18, 16.46s/it]

 73%|███████▎  | 17567/24156 [7:19:53<29:43:45, 16.24s/it]

 73%|███████▎  | 17568/24156 [7:20:08<28:52:04, 15.77s/it]

 73%|███████▎  | 17569/24156 [7:20:23<28:29:12, 15.57s/it]

 73%|███████▎  | 17570/24156 [7:20:39<28:52:14, 15.78s/it]

 73%|███████▎  | 17571/24156 [7:21:01<32:20:29, 17.68s/it]
[2024-04-05 22:25:58,772] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17572/24156 [7:21:18<31:35:23, 17.27s/it]
[2024-04-05 22:26:15,093] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17573/24156 [7:21:30<28:39:11, 15.67s/it]

 73%|███████▎  | 17574/24156 [7:21:42<27:09:22, 14.85s/it]

 73%|███████▎  | 17575/24156 [7:22:01<29:12:43, 15.98s/it]
{'loss': 0.2922, 'learning_rate': 1.993512821679954e-06, 'rewards/chosen': -1.4558483362197876, 'rewards/rejected': -3.486818552017212, 'rewards/accuracies': 0.75, 'rewards/margins': 2.030970335006714, 'policy_logps/rejected': -366.32965087890625, 'policy_logps/chosen': -407.4760437011719, 'referece_logps/rejected': -331.46148681640625, 'referece_logps/chosen': -392.91754150390625, 'logits/rejected': -0.6395900845527649, 'logits/chosen': -0.5883535742759705, 'epoch': 6.55}


 73%|███████▎  | 17577/24156 [7:22:23<24:21:10, 13.33s/it]

 73%|███████▎  | 17578/24156 [7:22:35<23:59:20, 13.13s/it]

 73%|███████▎  | 17579/24156 [7:22:49<24:01:56, 13.15s/it]

 73%|███████▎  | 17580/24156 [7:23:02<24:20:28, 13.33s/it]

 73%|███████▎  | 17581/24156 [7:23:14<23:35:44, 12.92s/it]

 73%|███████▎  | 17582/24156 [7:23:25<22:24:20, 12.27s/it]

 73%|███████▎  | 17583/24156 [7:23:39<23:16:59, 12.75s/it]
{'loss': 0.4328, 'learning_rate': 1.9933902707764017e-06, 'rewards/chosen': -1.607909083366394, 'rewards/rejected': -3.73553466796875, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1276254653930664, 'policy_logps/rejected': -236.97509765625, 'policy_logps/chosen': -370.18548583984375, 'referece_logps/rejected': -199.61976623535156, 'referece_logps/chosen': -354.1064147949219, 'logits/rejected': -1.0164051055908203, 'logits/chosen': -0.779873788356781, 'epoch': 6.55}


 73%|███████▎  | 17585/24156 [7:24:10<26:03:22, 14.28s/it]

 73%|███████▎  | 17586/24156 [7:24:31<30:03:11, 16.47s/it]

 73%|███████▎  | 17587/24156 [7:24:42<27:15:30, 14.94s/it]

 73%|███████▎  | 17588/24156 [7:25:00<28:52:51, 15.83s/it]
{'loss': 0.4372, 'learning_rate': 1.9933130960608086e-06, 'rewards/chosen': -1.258885145187378, 'rewards/rejected': -2.7896478176116943, 'rewards/accuracies': 1.0, 'rewards/margins': 1.530762791633606, 'policy_logps/rejected': -382.9477844238281, 'policy_logps/chosen': -311.5148620605469, 'referece_logps/rejected': -355.0513000488281, 'referece_logps/chosen': -298.926025390625, 'logits/rejected': -0.36401936411857605, 'logits/chosen': -0.29345017671585083, 'epoch': 6.55}


 73%|███████▎  | 17590/24156 [7:25:36<30:36:51, 16.79s/it]

 73%|███████▎  | 17591/24156 [7:25:48<27:41:18, 15.18s/it]
{'loss': 0.4103, 'learning_rate': 1.9932665769473245e-06, 'rewards/chosen': -2.003969669342041, 'rewards/rejected': -3.013681650161743, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0097122192382812, 'policy_logps/rejected': -336.27178955078125, 'policy_logps/chosen': -342.5621337890625, 'referece_logps/rejected': -306.1349792480469, 'referece_logps/chosen': -322.5224609375, 'logits/rejected': -0.4678432047367096, 'logits/chosen': -0.405490905046463, 'epoch': 6.55}


 73%|███████▎  | 17593/24156 [7:26:17<26:48:43, 14.71s/it]

 73%|███████▎  | 17594/24156 [7:26:39<30:36:08, 16.79s/it]

 73%|███████▎  | 17595/24156 [7:26:57<31:15:53, 17.15s/it]

 73%|███████▎  | 17596/24156 [7:27:14<31:14:24, 17.14s/it]

 73%|███████▎  | 17597/24156 [7:27:31<31:04:06, 17.05s/it]
[2024-04-05 22:32:28,050] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17598/24156 [7:27:50<32:16:17, 17.72s/it]

 73%|███████▎  | 17599/24156 [7:28:11<34:21:22, 18.86s/it]

 73%|███████▎  | 17600/24156 [7:28:33<35:47:00, 19.65s/it]

 73%|███████▎  | 17601/24156 [7:28:50<34:17:34, 18.83s/it]

 73%|███████▎  | 17602/24156 [7:29:08<34:05:52, 18.73s/it]

 73%|███████▎  | 17603/24156 [7:29:26<33:48:34, 18.57s/it]

 73%|███████▎  | 17604/24156 [7:29:47<34:43:17, 19.08s/it]

 73%|███████▎  | 17605/24156 [7:30:02<32:54:06, 18.08s/it]

 73%|███████▎  | 17606/24156 [7:30:16<30:13:35, 16.61s/it]

 73%|███████▎  | 17607/24156 [7:30:26<26:58:50, 14.83s/it]

 73%|███████▎  | 17608/24156 [7:30:41<26:43:36, 14.69s/it]

 73%|███████▎  | 17609/24156 [7:30:56<27:09:10, 14.93s/it]

 73%|███████▎  | 17610/24156 [7:31:15<29:28:41, 16.21s/it]

 73%|███████▎  | 17611/24156 [7:31:31<29:01:11, 15.96s/it]

 73%|███████▎  | 17612/24156 [7:31:43<27:05:29, 14.90s/it]

 73%|███████▎  | 17613/24156 [7:32:03<29:35:00, 16.28s/it]

 73%|███████▎  | 17614/24156 [7:32:24<32:13:53, 17.74s/it]
[2024-04-05 22:37:21,330] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17615/24156 [7:32:47<35:26:06, 19.50s/it]

 73%|███████▎  | 17616/24156 [7:33:02<32:41:02, 17.99s/it]
[2024-04-05 22:37:59,417] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17617/24156 [7:33:15<30:07:20, 16.58s/it]

 73%|███████▎  | 17618/24156 [7:33:35<31:45:45, 17.49s/it]

 73%|███████▎  | 17619/24156 [7:33:48<29:28:51, 16.24s/it]

 73%|███████▎  | 17620/24156 [7:34:04<29:03:17, 16.00s/it]

 73%|███████▎  | 17621/24156 [7:34:16<27:01:52, 14.89s/it]

 73%|███████▎  | 17622/24156 [7:34:30<26:37:19, 14.67s/it]

 73%|███████▎  | 17623/24156 [7:34:50<29:15:25, 16.12s/it]
{'loss': 0.4136, 'learning_rate': 1.992760375241759e-06, 'rewards/chosen': -1.4715367555618286, 'rewards/rejected': -3.6790261268615723, 'rewards/accuracies': 1.0, 'rewards/margins': 2.207489490509033, 'policy_logps/rejected': -465.82904052734375, 'policy_logps/chosen': -515.55810546875, 'referece_logps/rejected': -429.038818359375, 'referece_logps/chosen': -500.84271240234375, 'logits/rejected': -0.7611079812049866, 'logits/chosen': -0.728909969329834, 'epoch': 6.57}


 73%|███████▎  | 17625/24156 [7:35:24<30:32:43, 16.84s/it]

 73%|███████▎  | 17626/24156 [7:35:39<29:20:56, 16.18s/it]

 73%|███████▎  | 17627/24156 [7:35:55<29:33:50, 16.30s/it]

 73%|███████▎  | 17628/24156 [7:36:14<31:02:36, 17.12s/it]

 73%|███████▎  | 17629/24156 [7:36:32<31:12:53, 17.22s/it]

 73%|███████▎  | 17630/24156 [7:36:47<30:03:52, 16.58s/it]

 73%|███████▎  | 17631/24156 [7:37:07<31:56:58, 17.63s/it]

 73%|███████▎  | 17632/24156 [7:37:25<32:14:58, 17.80s/it]

 73%|███████▎  | 17633/24156 [7:37:42<32:04:22, 17.70s/it]

 73%|███████▎  | 17634/24156 [7:37:59<31:40:45, 17.49s/it]

 73%|███████▎  | 17635/24156 [7:38:13<29:20:58, 16.20s/it]
{'loss': 0.4605, 'learning_rate': 1.9925658375193725e-06, 'rewards/chosen': -2.4809212684631348, 'rewards/rejected': -3.929105758666992, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4481844902038574, 'policy_logps/rejected': -464.1504821777344, 'policy_logps/chosen': -361.466552734375, 'referece_logps/rejected': -424.8594055175781, 'referece_logps/chosen': -336.6573791503906, 'logits/rejected': -0.8210446834564209, 'logits/chosen': -0.7884881496429443, 'epoch': 6.57}


 73%|███████▎  | 17637/24156 [7:38:41<27:39:47, 15.28s/it]

 73%|███████▎  | 17638/24156 [7:38:57<27:46:43, 15.34s/it]

 73%|███████▎  | 17639/24156 [7:39:16<29:34:12, 16.33s/it]

 73%|███████▎  | 17640/24156 [7:39:37<32:05:44, 17.73s/it]
[2024-04-05 22:44:33,999] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17641/24156 [7:39:54<31:53:16, 17.62s/it]
[2024-04-05 22:44:51,358] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17642/24156 [7:40:09<30:35:18, 16.90s/it]

 73%|███████▎  | 17643/24156 [7:40:30<32:29:41, 17.96s/it]

 73%|███████▎  | 17644/24156 [7:40:49<33:26:03, 18.48s/it]
[2024-04-05 22:45:46,721] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17645/24156 [7:41:11<35:06:34, 19.41s/it]
[2024-04-05 22:46:08,302] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17646/24156 [7:41:32<36:16:22, 20.06s/it]
[2024-04-05 22:46:29,868] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17647/24156 [7:41:45<32:11:33, 17.81s/it]
[2024-04-05 22:46:42,415] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17648/24156 [7:41:58<29:25:02, 16.27s/it]

 73%|███████▎  | 17649/24156 [7:42:13<29:07:22, 16.11s/it]

 73%|███████▎  | 17650/24156 [7:42:35<32:07:46, 17.78s/it]
[2024-04-05 22:47:32,516] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17651/24156 [7:42:48<29:26:33, 16.29s/it]

 73%|███████▎  | 17652/24156 [7:43:07<30:52:10, 17.09s/it]
[2024-04-05 22:48:04,282] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17653/24156 [7:43:23<30:20:46, 16.80s/it]

 73%|███████▎  | 17654/24156 [7:43:34<27:08:31, 15.03s/it]
{'loss': 0.451, 'learning_rate': 1.9922525647836993e-06, 'rewards/chosen': -1.5802531242370605, 'rewards/rejected': -2.4115805625915527, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8313271999359131, 'policy_logps/rejected': -287.7678527832031, 'policy_logps/chosen': -409.8020324707031, 'referece_logps/rejected': -263.65203857421875, 'referece_logps/chosen': -393.9995422363281, 'logits/rejected': 0.36196959018707275, 'logits/chosen': 0.48372867703437805, 'epoch': 6.58}


 73%|███████▎  | 17656/24156 [7:44:01<26:02:55, 14.43s/it]

 73%|███████▎  | 17657/24156 [7:44:15<25:45:38, 14.27s/it]

 73%|███████▎  | 17658/24156 [7:44:31<26:57:30, 14.94s/it]
[2024-04-05 22:49:28,699] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17659/24156 [7:44:52<30:22:50, 16.83s/it]
{'loss': 0.3589, 'learning_rate': 1.9921690542410935e-06, 'rewards/chosen': -1.5941336154937744, 'rewards/rejected': -4.404879570007324, 'rewards/accuracies': 1.0, 'rewards/margins': 2.810746192932129, 'policy_logps/rejected': -396.46429443359375, 'policy_logps/chosen': -309.70477294921875, 'referece_logps/rejected': -352.41552734375, 'referece_logps/chosen': -293.7634582519531, 'logits/rejected': -1.2448217868804932, 'logits/chosen': -1.178138256072998, 'epoch': 6.58}


 73%|███████▎  | 17661/24156 [7:45:20<27:06:22, 15.02s/it]

 73%|███████▎  | 17662/24156 [7:45:44<31:44:14, 17.59s/it]

 73%|███████▎  | 17663/24156 [7:45:58<29:48:46, 16.53s/it]

 73%|███████▎  | 17664/24156 [7:46:11<28:08:07, 15.60s/it]
{'loss': 0.4656, 'learning_rate': 1.992085097792025e-06, 'rewards/chosen': -1.7740683555603027, 'rewards/rejected': -3.062965154647827, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2888967990875244, 'policy_logps/rejected': -325.1539306640625, 'policy_logps/chosen': -429.5741882324219, 'referece_logps/rejected': -294.5243225097656, 'referece_logps/chosen': -411.83349609375, 'logits/rejected': -0.793506383895874, 'logits/chosen': -0.8653650879859924, 'epoch': 6.58}


 73%|███████▎  | 17666/24156 [7:46:45<29:13:18, 16.21s/it]

 73%|███████▎  | 17667/24156 [7:47:07<31:57:56, 17.73s/it]

 73%|███████▎  | 17668/24156 [7:47:22<30:55:45, 17.16s/it]

 73%|███████▎  | 17669/24156 [7:47:41<31:34:09, 17.52s/it]

 73%|███████▎  | 17670/24156 [7:47:59<32:03:22, 17.79s/it]

 73%|███████▎  | 17671/24156 [7:48:11<28:50:31, 16.01s/it]

 73%|███████▎  | 17672/24156 [7:48:23<26:42:10, 14.83s/it]

 73%|███████▎  | 17673/24156 [7:48:37<26:08:55, 14.52s/it]

 73%|███████▎  | 17674/24156 [7:48:49<24:40:00, 13.70s/it]

 73%|███████▎  | 17675/24156 [7:49:06<26:42:43, 14.84s/it]
{'loss': 0.4064, 'learning_rate': 1.991898824199565e-06, 'rewards/chosen': -1.3466057777404785, 'rewards/rejected': -2.154613733291626, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8080081939697266, 'policy_logps/rejected': -414.4464416503906, 'policy_logps/chosen': -304.69927978515625, 'referece_logps/rejected': -392.9002990722656, 'referece_logps/chosen': -291.23321533203125, 'logits/rejected': -0.838890552520752, 'logits/chosen': -0.5606110095977783, 'epoch': 6.59}


 73%|███████▎  | 17677/24156 [7:49:41<28:28:38, 15.82s/it]

 73%|███████▎  | 17678/24156 [7:49:57<28:56:41, 16.09s/it]
{'loss': 0.4315, 'learning_rate': 1.991847647834194e-06, 'rewards/chosen': -1.8430052995681763, 'rewards/rejected': -3.2036476135253906, 'rewards/accuracies': 0.75, 'rewards/margins': 1.360642671585083, 'policy_logps/rejected': -438.3421325683594, 'policy_logps/chosen': -378.9931945800781, 'referece_logps/rejected': -406.3056335449219, 'referece_logps/chosen': -360.56317138671875, 'logits/rejected': -1.2652547359466553, 'logits/chosen': -1.2101036310195923, 'epoch': 6.59}

 73%|███████▎  | 17679/24156 [7:50:10<27:01:12, 15.02s/it]

 73%|███████▎  | 17680/24156 [7:50:26<27:31:44, 15.30s/it]


 73%|███████▎  | 17682/24156 [7:50:50<24:48:44, 13.80s/it]
{'loss': 0.3853, 'learning_rate': 1.9917791630550674e-06, 'rewards/chosen': -1.8437861204147339, 'rewards/rejected': -2.058403491973877, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2146175503730774, 'policy_logps/rejected': -530.0054321289062, 'policy_logps/chosen': -540.7413330078125, 'referece_logps/rejected': -509.42138671875, 'referece_logps/chosen': -522.3035278320312, 'logits/rejected': 0.4670080542564392, 'logits/chosen': 0.5011465549468994, 'epoch': 6.59}
[2024-04-05 22:56:07,393] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 73%|███████▎  | 17684/24156 [7:51:31<31:10:05, 17.34s/it]

 73%|███████▎  | 17685/24156 [7:51:53<33:11:22, 18.46s/it]
{'loss': 0.2915, 'learning_rate': 1.9917276122625198e-06, 'rewards/chosen': -1.7959809303283691, 'rewards/rejected': -2.6843013763427734, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8883206844329834, 'policy_logps/rejected': -420.026123046875, 'policy_logps/chosen': -351.5404052734375, 'referece_logps/rejected': -393.18310546875, 'referece_logps/chosen': -333.58056640625, 'logits/rejected': 0.5304765701293945, 'logits/chosen': 0.7400975227355957, 'epoch': 6.59}


 73%|███████▎  | 17687/24156 [7:52:33<34:24:07, 19.14s/it]

 73%|███████▎  | 17688/24156 [7:52:51<33:49:32, 18.83s/it]

 73%|███████▎  | 17689/24156 [7:53:09<33:34:19, 18.69s/it]
{'loss': 0.4018, 'learning_rate': 1.9916586282773785e-06, 'rewards/chosen': -1.3776428699493408, 'rewards/rejected': -2.355987310409546, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9783442616462708, 'policy_logps/rejected': -442.5546569824219, 'policy_logps/chosen': -341.534912109375, 'referece_logps/rejected': -418.99481201171875, 'referece_logps/chosen': -327.7584533691406, 'logits/rejected': -0.7406242489814758, 'logits/chosen': -0.8999420404434204, 'epoch': 6.59}


 73%|███████▎  | 17691/24156 [7:53:47<32:55:53, 18.34s/it]

 73%|███████▎  | 17692/24156 [7:54:03<31:52:45, 17.75s/it]
{'loss': 0.4424, 'learning_rate': 1.991606703103065e-06, 'rewards/chosen': -1.244360089302063, 'rewards/rejected': -2.588853359222412, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3444932699203491, 'policy_logps/rejected': -279.70556640625, 'policy_logps/chosen': -278.1691589355469, 'referece_logps/rejected': -253.81703186035156, 'referece_logps/chosen': -265.72552490234375, 'logits/rejected': -0.2835454046726227, 'logits/chosen': -0.34401148557662964, 'epoch': 6.59}


 73%|███████▎  | 17694/24156 [7:54:43<33:35:28, 18.71s/it]

 73%|███████▎  | 17695/24156 [7:55:03<34:10:56, 19.05s/it]
{'loss': 0.3706, 'learning_rate': 1.9915546174934053e-06, 'rewards/chosen': -1.8500722646713257, 'rewards/rejected': -2.1663739681243896, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3163016438484192, 'policy_logps/rejected': -611.13232421875, 'policy_logps/chosen': -493.36639404296875, 'referece_logps/rejected': -589.468505859375, 'referece_logps/chosen': -474.8656005859375, 'logits/rejected': 0.26335057616233826, 'logits/chosen': 0.1539241224527359, 'epoch': 6.59}

 73%|███████▎  | 17696/24156 [7:55:22<34:33:27, 19.26s/it]

 73%|███████▎  | 17697/24156 [7:55:40<33:39:46, 18.76s/it]


 73%|███████▎  | 17699/24156 [7:56:07<28:17:13, 15.77s/it]
{'loss': 0.3741, 'learning_rate': 1.9914849204623346e-06, 'rewards/chosen': -1.2199528217315674, 'rewards/rejected': -2.226156234741211, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0062031745910645, 'policy_logps/rejected': -598.4321899414062, 'policy_logps/chosen': -489.0712585449219, 'referece_logps/rejected': -576.1705932617188, 'referece_logps/chosen': -476.8717041015625, 'logits/rejected': 0.12695397436618805, 'logits/chosen': 0.1422821581363678, 'epoch': 6.59}


 73%|███████▎  | 17701/24156 [7:56:43<30:07:26, 16.80s/it]

 73%|███████▎  | 17702/24156 [7:57:01<30:42:47, 17.13s/it]

 73%|███████▎  | 17703/24156 [7:57:14<28:15:15, 15.76s/it]
{'loss': 0.4966, 'learning_rate': 1.991414938247903e-06, 'rewards/chosen': -1.5808329582214355, 'rewards/rejected': -2.5329251289367676, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9520920515060425, 'policy_logps/rejected': -391.9645690917969, 'policy_logps/chosen': -339.7907409667969, 'referece_logps/rejected': -366.6353454589844, 'referece_logps/chosen': -323.982421875, 'logits/rejected': 0.07809282839298248, 'logits/chosen': 0.07995175570249557, 'epoch': 6.6}
[2024-04-05 23:02:34,109] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 73%|███████▎  | 17705/24156 [7:57:52<30:33:51, 17.06s/it]
{'loss': 0.3134, 'learning_rate': 1.9913798402032145e-06, 'rewards/chosen': -1.3765660524368286, 'rewards/rejected': -1.8459961414337158, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4694300889968872, 'policy_logps/rejected': -309.278564453125, 'policy_logps/chosen': -319.0478210449219, 'referece_logps/rejected': -290.818603515625, 'referece_logps/chosen': -305.28216552734375, 'logits/rejected': -0.16437315940856934, 'logits/chosen': -0.18015387654304504, 'epoch': 6.6}

 73%|███████▎  | 17706/24156 [7:58:05<28:17:39, 15.79s/it]


 73%|███████▎  | 17708/24156 [7:58:34<27:40:48, 15.45s/it]

 73%|███████▎  | 17709/24156 [7:58:46<25:45:29, 14.38s/it]

 73%|███████▎  | 17710/24156 [7:59:00<25:23:09, 14.18s/it]
{'loss': 0.4872, 'learning_rate': 1.9912917832107768e-06, 'rewards/chosen': -1.5464942455291748, 'rewards/rejected': -2.500635862350464, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9541417360305786, 'policy_logps/rejected': -435.80023193359375, 'policy_logps/chosen': -309.0805358886719, 'referece_logps/rejected': -410.7938537597656, 'referece_logps/chosen': -293.6155700683594, 'logits/rejected': -0.22699564695358276, 'logits/chosen': 0.04643837362527847, 'epoch': 6.6}


 73%|███████▎  | 17712/24156 [7:59:24<23:55:32, 13.37s/it]
{'loss': 0.4674, 'learning_rate': 1.9912564356681645e-06, 'rewards/chosen': -1.2574036121368408, 'rewards/rejected': -2.817836046218872, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5604323148727417, 'policy_logps/rejected': -327.03857421875, 'policy_logps/chosen': -280.9616394042969, 'referece_logps/rejected': -298.8601989746094, 'referece_logps/chosen': -268.3876037597656, 'logits/rejected': -0.8366636037826538, 'logits/chosen': -0.6272109746932983, 'epoch': 6.6}

 73%|███████▎  | 17713/24156 [7:59:41<25:39:41, 14.34s/it]


 73%|███████▎  | 17715/24156 [8:00:15<28:10:26, 15.75s/it]
{'loss': 0.3388, 'learning_rate': 1.991203280706145e-06, 'rewards/chosen': -1.820910930633545, 'rewards/rejected': -4.565317153930664, 'rewards/accuracies': 1.0, 'rewards/margins': 2.744405746459961, 'policy_logps/rejected': -471.63677978515625, 'policy_logps/chosen': -431.2897033691406, 'referece_logps/rejected': -425.983642578125, 'referece_logps/chosen': -413.0805969238281, 'logits/rejected': -1.1778837442398071, 'logits/chosen': -1.1472259759902954, 'epoch': 6.6}


 73%|███████▎  | 17717/24156 [8:00:49<29:32:51, 16.52s/it]

 73%|███████▎  | 17718/24156 [8:01:11<32:30:32, 18.18s/it]
[2024-04-05 23:06:08,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2901, 'learning_rate': 1.9911499653740503e-06, 'rewards/chosen': -1.5187164545059204, 'rewards/rejected': -3.0241613388061523, 'rewards/accuracies': 0.75, 'rewards/margins': 1.505444884300232, 'policy_logps/rejected': -334.6486511230469, 'policy_logps/chosen': -346.07452392578125, 'referece_logps/rejected': -304.40704345703125, 'referece_logps/chosen': -330.8873291015625, 'logits/rejected': -0.7676679491996765, 'logits/chosen': -0.7741873860359192, 'epoch': 6.6}
[2024-04-05 23:06:32,025] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17719/24156 [8:01:35<35:14:49, 19.71s/it]


 73%|███████▎  | 17721/24156 [8:02:18<37:08:55, 20.78s/it]

 73%|███████▎  | 17722/24156 [8:02:38<37:04:23, 20.74s/it]
{'loss': 0.3461, 'learning_rate': 1.9910786288149434e-06, 'rewards/chosen': -1.923715591430664, 'rewards/rejected': -3.692425012588501, 'rewards/accuracies': 0.875, 'rewards/margins': 1.768709659576416, 'policy_logps/rejected': -302.31201171875, 'policy_logps/chosen': -383.2692565917969, 'referece_logps/rejected': -265.38775634765625, 'referece_logps/chosen': -364.03204345703125, 'logits/rejected': -0.7839134931564331, 'logits/chosen': -0.698815643787384, 'epoch': 6.6}


 73%|███████▎  | 17724/24156 [8:03:21<37:35:14, 21.04s/it]

 73%|███████▎  | 17725/24156 [8:03:40<36:17:41, 20.32s/it]
{'loss': 0.4765, 'learning_rate': 1.9910249393195987e-06, 'rewards/chosen': -1.7339842319488525, 'rewards/rejected': -2.4396910667419434, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7057069540023804, 'policy_logps/rejected': -412.0166320800781, 'policy_logps/chosen': -491.62451171875, 'referece_logps/rejected': -387.61968994140625, 'referece_logps/chosen': -474.28466796875, 'logits/rejected': -0.48873600363731384, 'logits/chosen': -0.45516595244407654, 'epoch': 6.6}


 73%|███████▎  | 17727/24156 [8:04:14<33:50:12, 18.95s/it]
{'loss': 0.4419, 'learning_rate': 1.9909890572437093e-06, 'rewards/chosen': -2.1181282997131348, 'rewards/rejected': -3.916287660598755, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7981595993041992, 'policy_logps/rejected': -311.81243896484375, 'policy_logps/chosen': -447.9427795410156, 'referece_logps/rejected': -272.6495666503906, 'referece_logps/chosen': -426.76141357421875, 'logits/rejected': -0.7955936193466187, 'logits/chosen': -1.0039551258087158, 'epoch': 6.6}


 73%|███████▎  | 17729/24156 [8:04:40<28:20:47, 15.88s/it]
{'loss': 0.5838, 'learning_rate': 1.9909531039076347e-06, 'rewards/chosen': -1.5377357006072998, 'rewards/rejected': -2.613924026489258, 'rewards/accuracies': 0.75, 'rewards/margins': 1.076188087463379, 'policy_logps/rejected': -399.7754821777344, 'policy_logps/chosen': -303.9915771484375, 'referece_logps/rejected': -373.63623046875, 'referece_logps/chosen': -288.61419677734375, 'logits/rejected': -0.1262061446905136, 'logits/chosen': 0.00019432883709669113, 'epoch': 6.61}
[2024-04-05 23:09:58,612] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 73%|███████▎  | 17731/24156 [8:05:14<28:57:18, 16.22s/it]

 73%|███████▎  | 17732/24156 [8:05:28<27:37:34, 15.48s/it]

 73%|███████▎  | 17733/24156 [8:05:46<29:02:06, 16.27s/it]
{'loss': 0.4682, 'learning_rate': 1.9908809834652773e-06, 'rewards/chosen': -1.8698208332061768, 'rewards/rejected': -1.7952141761779785, 'rewards/accuracies': 0.375, 'rewards/margins': -0.07460669428110123, 'policy_logps/rejected': -422.36187744140625, 'policy_logps/chosen': -569.4747314453125, 'referece_logps/rejected': -404.40972900390625, 'referece_logps/chosen': -550.776611328125, 'logits/rejected': -1.2311967611312866, 'logits/chosen': -1.3328404426574707, 'epoch': 6.61}

 73%|███████▎  | 17734/24156 [8:06:01<28:08:13, 15.77s/it]


 73%|███████▎  | 17736/24156 [8:06:34<27:56:16, 15.67s/it]
{'loss': 0.5598, 'learning_rate': 1.9908267060947895e-06, 'rewards/chosen': -1.0049197673797607, 'rewards/rejected': -1.6982301473617554, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6933103203773499, 'policy_logps/rejected': -275.85833740234375, 'policy_logps/chosen': -303.45672607421875, 'referece_logps/rejected': -258.8760681152344, 'referece_logps/chosen': -293.4075622558594, 'logits/rejected': -0.36385008692741394, 'logits/chosen': -0.24087777733802795, 'epoch': 6.61}


 73%|███████▎  | 17738/24156 [8:07:06<28:09:58, 15.80s/it]

 73%|███████▎  | 17739/24156 [8:07:26<30:27:37, 17.09s/it]
{'loss': 0.4199, 'learning_rate': 1.9907722684151544e-06, 'rewards/chosen': -2.172908067703247, 'rewards/rejected': -3.004528522491455, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8316206336021423, 'policy_logps/rejected': -452.1094970703125, 'policy_logps/chosen': -563.858642578125, 'referece_logps/rejected': -422.064208984375, 'referece_logps/chosen': -542.1295166015625, 'logits/rejected': 0.13846208155155182, 'logits/chosen': 0.13454362750053406, 'epoch': 6.61}


 73%|███████▎  | 17741/24156 [8:07:56<29:08:11, 16.35s/it]

 73%|███████▎  | 17742/24156 [8:08:18<31:56:32, 17.93s/it]
[2024-04-05 23:13:15,140] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 73%|███████▎  | 17743/24156 [8:08:38<33:22:50, 18.74s/it]

 73%|███████▎  | 17744/24156 [8:08:58<34:00:37, 19.10s/it]
[2024-04-05 23:13:55,695] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4554, 'learning_rate': 1.9906811827304795e-06, 'rewards/chosen': -1.0092391967773438, 'rewards/rejected': -2.193085193634033, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1838459968566895, 'policy_logps/rejected': -409.4559631347656, 'policy_logps/chosen': -335.20440673828125, 'referece_logps/rejected': -387.525146484375, 'referece_logps/chosen': -325.1119689941406, 'logits/rejected': -1.2305139303207397, 'logits/chosen': -1.184417724609375, 'epoch': 6.61}


 73%|███████▎  | 17746/24156 [8:09:33<32:13:41, 18.10s/it]
{'loss': 0.3967, 'learning_rate': 1.9906446237877333e-06, 'rewards/chosen': -1.1570134162902832, 'rewards/rejected': -2.9933996200561523, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8363863229751587, 'policy_logps/rejected': -450.519775390625, 'policy_logps/chosen': -516.998779296875, 'referece_logps/rejected': -420.58575439453125, 'referece_logps/chosen': -505.42864990234375, 'logits/rejected': -0.6009138822555542, 'logits/chosen': -0.6723689436912537, 'epoch': 6.61}
[2024-04-05 23:14:48,852] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 73%|███████▎  | 17748/24156 [8:10:11<33:05:07, 18.59s/it]
{'loss': 0.4507, 'learning_rate': 1.99060799360957e-06, 'rewards/chosen': -1.3846441507339478, 'rewards/rejected': -2.071368455886841, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6867243051528931, 'policy_logps/rejected': -556.193115234375, 'policy_logps/chosen': -439.867431640625, 'referece_logps/rejected': -535.4794921875, 'referece_logps/chosen': -426.02099609375, 'logits/rejected': -0.8360676169395447, 'logits/chosen': -0.7047717571258545, 'epoch': 6.61}


 73%|███████▎  | 17750/24156 [8:10:46<32:36:00, 18.32s/it]

 73%|███████▎  | 17751/24156 [8:11:00<30:13:23, 16.99s/it]

 73%|███████▎  | 17752/24156 [8:11:20<31:53:56, 17.93s/it]
[2024-04-05 23:16:17,775] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3508, 'learning_rate': 1.9905345195575317e-06, 'rewards/chosen': -1.557785987854004, 'rewards/rejected': -4.209494590759277, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6517086029052734, 'policy_logps/rejected': -415.9434509277344, 'policy_logps/chosen': -408.11236572265625, 'referece_logps/rejected': -373.8484802246094, 'referece_logps/chosen': -392.5345458984375, 'logits/rejected': -0.4484107494354248, 'logits/chosen': -0.42687898874282837, 'epoch': 6.61}


 73%|███████▎  | 17754/24156 [8:11:51<29:06:04, 16.36s/it]

 74%|███████▎  | 17755/24156 [8:12:10<30:53:17, 17.37s/it]
{'loss': 0.3571, 'learning_rate': 1.990479227045161e-06, 'rewards/chosen': -2.0881292819976807, 'rewards/rejected': -3.642610788345337, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5544812679290771, 'policy_logps/rejected': -366.5582580566406, 'policy_logps/chosen': -329.59686279296875, 'referece_logps/rejected': -330.13214111328125, 'referece_logps/chosen': -308.715576171875, 'logits/rejected': -0.24401041865348816, 'logits/chosen': -0.18732646107673645, 'epoch': 6.62}


 74%|███████▎  | 17757/24156 [8:12:46<31:04:50, 17.49s/it]

 74%|███████▎  | 17758/24156 [8:13:08<33:31:45, 18.87s/it]
[2024-04-05 23:18:05,594] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▎  | 17759/24156 [8:13:26<33:11:38, 18.68s/it]
[2024-04-05 23:18:23,841] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3501, 'learning_rate': 1.9904052544145505e-06, 'rewards/chosen': -1.887603759765625, 'rewards/rejected': -3.6094424724578857, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7218390703201294, 'policy_logps/rejected': -345.8543701171875, 'policy_logps/chosen': -481.12200927734375, 'referece_logps/rejected': -309.75994873046875, 'referece_logps/chosen': -462.2459716796875, 'logits/rejected': 0.06633076071739197, 'logits/chosen': -0.017892345786094666, 'epoch': 6.62}


 74%|███████▎  | 17761/24156 [8:13:54<28:26:37, 16.01s/it]
{'loss': 0.3544, 'learning_rate': 1.990368161270607e-06, 'rewards/chosen': -2.1129751205444336, 'rewards/rejected': -3.6868667602539062, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5738919973373413, 'policy_logps/rejected': -706.763916015625, 'policy_logps/chosen': -494.5173645019531, 'referece_logps/rejected': -669.895263671875, 'referece_logps/chosen': -473.3875732421875, 'logits/rejected': -1.3510602712631226, 'logits/chosen': -1.2053214311599731, 'epoch': 6.62}
[2024-04-05 23:19:09,322] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▎  | 17762/24156 [8:14:12<29:17:13, 16.49s/it]

 74%|███████▎  | 17763/24156 [8:14:31<30:46:37, 17.33s/it]

 74%|███████▎  | 17764/24156 [8:14:45<29:06:37, 16.40s/it]


 74%|███████▎  | 17766/24156 [8:15:16<28:23:30, 16.00s/it]
{'loss': 0.3718, 'learning_rate': 1.9902751168486196e-06, 'rewards/chosen': -1.4750556945800781, 'rewards/rejected': -4.06086540222168, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5858094692230225, 'policy_logps/rejected': -315.45953369140625, 'policy_logps/chosen': -449.4705810546875, 'referece_logps/rejected': -274.85089111328125, 'referece_logps/chosen': -434.72003173828125, 'logits/rejected': -0.7074651122093201, 'logits/chosen': -0.7362377047538757, 'epoch': 6.62}


 74%|███████▎  | 17768/24156 [8:15:39<24:09:24, 13.61s/it]
{'loss': 0.4994, 'learning_rate': 1.9902377744619985e-06, 'rewards/chosen': -1.4156404733657837, 'rewards/rejected': -2.0038270950317383, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5881864428520203, 'policy_logps/rejected': -285.61669921875, 'policy_logps/chosen': -304.5472106933594, 'referece_logps/rejected': -265.5784606933594, 'referece_logps/chosen': -290.3907775878906, 'logits/rejected': -0.5432929992675781, 'logits/chosen': -0.5771840214729309, 'epoch': 6.62}

 74%|███████▎  | 17769/24156 [8:15:57<26:40:57, 15.04s/it]

 74%|███████▎  | 17770/24156 [8:16:15<28:16:32, 15.94s/it]


 74%|███████▎  | 17772/24156 [8:16:47<27:31:44, 15.52s/it]
{'loss': 0.4199, 'learning_rate': 1.9901628760729613e-06, 'rewards/chosen': -2.0600571632385254, 'rewards/rejected': -4.484251022338867, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4241943359375, 'policy_logps/rejected': -315.55047607421875, 'policy_logps/chosen': -338.456298828125, 'referece_logps/rejected': -270.7079772949219, 'referece_logps/chosen': -317.8557434082031, 'logits/rejected': -0.6000313758850098, 'logits/chosen': -0.7514929175376892, 'epoch': 6.62}
[2024-04-05 23:22:07,434] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 74%|███████▎  | 17774/24156 [8:17:24<29:34:28, 16.68s/it]
{'loss': 0.43, 'learning_rate': 1.990125320075931e-06, 'rewards/chosen': -2.2040584087371826, 'rewards/rejected': -4.297337532043457, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0932791233062744, 'policy_logps/rejected': -567.904296875, 'policy_logps/chosen': -539.3619384765625, 'referece_logps/rejected': -524.930908203125, 'referece_logps/chosen': -517.3213500976562, 'logits/rejected': -1.011168360710144, 'logits/chosen': -1.10912024974823, 'epoch': 6.62}

 74%|███████▎  | 17775/24156 [8:17:40<29:08:00, 16.44s/it]


 74%|███████▎  | 17777/24156 [8:18:15<30:28:26, 17.20s/it]

 74%|███████▎  | 17778/24156 [8:18:34<31:40:53, 17.88s/it]
[2024-04-05 23:23:31,719] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▎  | 17779/24156 [8:18:51<31:12:44, 17.62s/it]

 74%|███████▎  | 17780/24156 [8:19:03<28:13:06, 15.93s/it]
{'loss': 0.5646, 'learning_rate': 1.9900122249072188e-06, 'rewards/chosen': -1.6197525262832642, 'rewards/rejected': -2.4333267211914062, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8135740756988525, 'policy_logps/rejected': -387.5071716308594, 'policy_logps/chosen': -322.942138671875, 'referece_logps/rejected': -363.17388916015625, 'referece_logps/chosen': -306.74462890625, 'logits/rejected': -1.3133437633514404, 'logits/chosen': -1.206652283668518, 'epoch': 6.62}


 74%|███████▎  | 17782/24156 [8:19:25<23:26:58, 13.24s/it]

 74%|███████▎  | 17783/24156 [8:19:35<22:08:01, 12.50s/it]
{'loss': 0.5311, 'learning_rate': 1.9899554370522264e-06, 'rewards/chosen': -1.66204035282135, 'rewards/rejected': -4.486806869506836, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8247666358947754, 'policy_logps/rejected': -291.1571044921875, 'policy_logps/chosen': -381.1599426269531, 'referece_logps/rejected': -246.28903198242188, 'referece_logps/chosen': -364.53948974609375, 'logits/rejected': 0.32734647393226624, 'logits/chosen': 0.2890186607837677, 'epoch': 6.63}


 74%|███████▎  | 17785/24156 [8:20:00<21:51:50, 12.35s/it]

 74%|███████▎  | 17786/24156 [8:20:11<20:58:18, 11.85s/it]
{'loss': 0.3966, 'learning_rate': 1.989898489029052e-06, 'rewards/chosen': -1.457371473312378, 'rewards/rejected': -1.9845001697540283, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5271288156509399, 'policy_logps/rejected': -386.5225524902344, 'policy_logps/chosen': -351.353271484375, 'referece_logps/rejected': -366.67755126953125, 'referece_logps/chosen': -336.779541015625, 'logits/rejected': -0.6657248139381409, 'logits/chosen': -0.5757891535758972, 'epoch': 6.63}


 74%|███████▎  | 17788/24156 [8:20:35<21:09:42, 11.96s/it]
{'loss': 0.4148, 'learning_rate': 1.989860434702495e-06, 'rewards/chosen': -1.4245346784591675, 'rewards/rejected': -3.6643073558807373, 'rewards/accuracies': 0.75, 'rewards/margins': 2.239772319793701, 'policy_logps/rejected': -581.0013427734375, 'policy_logps/chosen': -469.570068359375, 'referece_logps/rejected': -544.3582763671875, 'referece_logps/chosen': -455.32476806640625, 'logits/rejected': -0.47844448685646057, 'logits/chosen': -0.34781402349472046, 'epoch': 6.63}

 74%|███████▎  | 17789/24156 [8:20:54<24:32:41, 13.88s/it]


 74%|███████▎  | 17791/24156 [8:21:23<24:43:55, 13.99s/it]

 74%|███████▎  | 17792/24156 [8:21:35<24:06:12, 13.63s/it]

 74%|███████▎  | 17793/24156 [8:21:49<24:09:00, 13.66s/it]
{'loss': 0.3743, 'learning_rate': 1.989764987483854e-06, 'rewards/chosen': -1.49565589427948, 'rewards/rejected': -3.221524953842163, 'rewards/accuracies': 0.75, 'rewards/margins': 1.725869059562683, 'policy_logps/rejected': -390.3555603027344, 'policy_logps/chosen': -324.2707214355469, 'referece_logps/rejected': -358.14031982421875, 'referece_logps/chosen': -309.3141784667969, 'logits/rejected': -0.38628166913986206, 'logits/chosen': -0.42887774109840393, 'epoch': 6.63}


 74%|███████▎  | 17795/24156 [8:22:24<27:43:12, 15.69s/it]

 74%|███████▎  | 17796/24156 [8:22:45<30:32:35, 17.29s/it]

 74%|███████▎  | 17797/24156 [8:23:03<31:09:19, 17.64s/it]

 74%|███████▎  | 17798/24156 [8:23:24<32:42:37, 18.52s/it]
{'loss': 0.415, 'learning_rate': 1.9896690954392e-06, 'rewards/chosen': -1.5198016166687012, 'rewards/rejected': -2.5529940128326416, 'rewards/accuracies': 0.75, 'rewards/margins': 1.033192753791809, 'policy_logps/rejected': -401.0145263671875, 'policy_logps/chosen': -325.61920166015625, 'referece_logps/rejected': -375.4845886230469, 'referece_logps/chosen': -310.42120361328125, 'logits/rejected': -0.8794682025909424, 'logits/chosen': -0.805574893951416, 'epoch': 6.63}

 74%|███████▎  | 17799/24156 [8:23:46<34:36:15, 19.60s/it]


 74%|███████▎  | 17801/24156 [8:24:21<32:37:22, 18.48s/it]

 74%|███████▎  | 17802/24156 [8:24:45<35:10:40, 19.93s/it]
[2024-04-05 23:29:42,301] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4089, 'learning_rate': 1.9895920615577016e-06, 'rewards/chosen': -1.2630068063735962, 'rewards/rejected': -3.9733500480651855, 'rewards/accuracies': 0.875, 'rewards/margins': 2.710343837738037, 'policy_logps/rejected': -338.41839599609375, 'policy_logps/chosen': -404.0543518066406, 'referece_logps/rejected': -298.68487548828125, 'referece_logps/chosen': -391.42431640625, 'logits/rejected': -0.3091542422771454, 'logits/chosen': -0.3572990596294403, 'epoch': 6.63}

 74%|███████▎  | 17803/24156 [8:25:00<32:41:43, 18.53s/it]


 74%|███████▎  | 17805/24156 [8:25:37<33:12:26, 18.82s/it]

 74%|███████▎  | 17806/24156 [8:25:53<31:42:09, 17.97s/it]
[2024-04-05 23:30:50,882] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3731, 'learning_rate': 1.9895147430372898e-06, 'rewards/chosen': -1.5083707571029663, 'rewards/rejected': -2.9842031002044678, 'rewards/accuracies': 0.75, 'rewards/margins': 1.475832462310791, 'policy_logps/rejected': -377.6633605957031, 'policy_logps/chosen': -375.58758544921875, 'referece_logps/rejected': -347.82135009765625, 'referece_logps/chosen': -360.50384521484375, 'logits/rejected': -0.3994072377681732, 'logits/chosen': -0.23205499351024628, 'epoch': 6.63}

 74%|███████▎  | 17807/24156 [8:26:11<31:30:13, 17.86s/it]

 74%|███████▎  | 17808/24156 [8:26:22<27:52:59, 15.81s/it]


 74%|███████▎  | 17810/24156 [8:26:51<26:35:38, 15.09s/it]
{'loss': 0.4602, 'learning_rate': 1.9894371399002047e-06, 'rewards/chosen': -1.0146842002868652, 'rewards/rejected': -3.3277101516723633, 'rewards/accuracies': 1.0, 'rewards/margins': 2.313025712966919, 'policy_logps/rejected': -302.62762451171875, 'policy_logps/chosen': -328.860595703125, 'referece_logps/rejected': -269.3504943847656, 'referece_logps/chosen': -318.7137451171875, 'logits/rejected': -0.291098028421402, 'logits/chosen': -0.12554192543029785, 'epoch': 6.64}


 74%|███████▎  | 17812/24156 [8:27:25<29:00:15, 16.46s/it]
{'loss': 0.4709, 'learning_rate': 1.9893982316073815e-06, 'rewards/chosen': -2.1478500366210938, 'rewards/rejected': -3.274523973464966, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1266740560531616, 'policy_logps/rejected': -542.340087890625, 'policy_logps/chosen': -457.8568115234375, 'referece_logps/rejected': -509.5948486328125, 'referece_logps/chosen': -436.3783264160156, 'logits/rejected': 0.17033860087394714, 'logits/chosen': 0.28377002477645874, 'epoch': 6.64}

 74%|███████▎  | 17813/24156 [8:27:46<31:09:54, 17.69s/it]
[2024-04-05 23:33:04,725] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▎  | 17814/24156 [8:28:07<33:02:10, 18.75s/it]

 74%|███████▎  | 17815/24156 [8:28:26<33:08:09, 18.81s/it]

 74%|███████▍  | 17816/24156 [8:28:40<30:37:42, 17.39s/it]


 74%|███████▍  | 17818/24156 [8:29:10<28:59:03, 16.46s/it]
{'loss': 0.2863, 'learning_rate': 1.989281079865379e-06, 'rewards/chosen': -1.9160445928573608, 'rewards/rejected': -3.1616556644439697, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2456114292144775, 'policy_logps/rejected': -309.3536376953125, 'policy_logps/chosen': -392.90911865234375, 'referece_logps/rejected': -277.737060546875, 'referece_logps/chosen': -373.7486572265625, 'logits/rejected': -0.6168558597564697, 'logits/chosen': -0.6378884315490723, 'epoch': 6.64}

 74%|███████▍  | 17819/24156 [8:29:23<27:04:10, 15.38s/it]

 74%|███████▍  | 17820/24156 [8:29:37<26:31:36, 15.07s/it]


 74%|███████▍  | 17822/24156 [8:30:12<28:05:31, 15.97s/it]
{'loss': 0.3613, 'learning_rate': 1.989202623012527e-06, 'rewards/chosen': -1.8158786296844482, 'rewards/rejected': -4.790661334991455, 'rewards/accuracies': 0.875, 'rewards/margins': 2.974782705307007, 'policy_logps/rejected': -351.7312927246094, 'policy_logps/chosen': -346.5025939941406, 'referece_logps/rejected': -303.82464599609375, 'referece_logps/chosen': -328.3438415527344, 'logits/rejected': -0.44502392411231995, 'logits/chosen': -0.4194168448448181, 'epoch': 6.64}


 74%|███████▍  | 17824/24156 [8:30:44<28:03:08, 15.95s/it]

 74%|███████▍  | 17825/24156 [8:31:04<30:11:20, 17.17s/it]

 74%|███████▍  | 17826/24156 [8:31:16<27:24:57, 15.59s/it]
{'loss': 0.3891, 'learning_rate': 1.9891238816327775e-06, 'rewards/chosen': -1.6086499691009521, 'rewards/rejected': -2.5107262134552, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9020760655403137, 'policy_logps/rejected': -369.29632568359375, 'policy_logps/chosen': -370.6935119628906, 'referece_logps/rejected': -344.18902587890625, 'referece_logps/chosen': -354.60699462890625, 'logits/rejected': 0.44343802332878113, 'logits/chosen': 0.46319347620010376, 'epoch': 6.64}

 74%|███████▍  | 17827/24156 [8:31:35<29:06:54, 16.56s/it]

 74%|███████▍  | 17828/24156 [8:31:47<27:04:30, 15.40s/it]

 74%|███████▍  | 17829/24156 [8:32:07<29:14:05, 16.63s/it]

 74%|███████▍  | 17830/24156 [8:32:18<26:26:25, 15.05s/it]

 74%|███████▍  | 17831/24156 [8:32:31<25:06:56, 14.30s/it]


 74%|███████▍  | 17833/24156 [8:33:12<30:33:09, 17.40s/it]
{'loss': 0.366, 'learning_rate': 1.988985399643536e-06, 'rewards/chosen': -1.3818175792694092, 'rewards/rejected': -3.630676746368408, 'rewards/accuracies': 0.875, 'rewards/margins': 2.24885892868042, 'policy_logps/rejected': -202.30905151367188, 'policy_logps/chosen': -394.8173828125, 'referece_logps/rejected': -166.00228881835938, 'referece_logps/chosen': -380.99920654296875, 'logits/rejected': -1.101637840270996, 'logits/chosen': -1.4491881132125854, 'epoch': 6.64}

 74%|███████▍  | 17834/24156 [8:33:33<32:26:26, 18.47s/it]

 74%|███████▍  | 17835/24156 [8:33:45<29:07:50, 16.59s/it]

 74%|███████▍  | 17836/24156 [8:34:05<30:52:07, 17.58s/it]


 74%|███████▍  | 17838/24156 [8:34:32<26:50:06, 15.29s/it]

 74%|███████▍  | 17839/24156 [8:34:46<25:56:58, 14.79s/it]
{'loss': 0.5064, 'learning_rate': 1.9888660074089875e-06, 'rewards/chosen': -1.6359041929244995, 'rewards/rejected': -4.653110980987549, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0172066688537598, 'policy_logps/rejected': -273.43798828125, 'policy_logps/chosen': -293.5051574707031, 'referece_logps/rejected': -226.9068603515625, 'referece_logps/chosen': -277.1461486816406, 'logits/rejected': -1.4311211109161377, 'logits/chosen': -1.353256344795227, 'epoch': 6.65}

 74%|███████▍  | 17840/24156 [8:35:01<26:12:34, 14.94s/it]

 74%|███████▍  | 17841/24156 [8:35:15<25:24:27, 14.48s/it]

 74%|███████▍  | 17842/24156 [8:35:28<24:41:38, 14.08s/it]

 74%|███████▍  | 17843/24156 [8:35:44<25:41:40, 14.65s/it]


 74%|███████▍  | 17845/24156 [8:36:08<23:34:30, 13.45s/it]
{'loss': 0.4654, 'learning_rate': 1.988745975206787e-06, 'rewards/chosen': -1.2010999917984009, 'rewards/rejected': -2.4558064937591553, 'rewards/accuracies': 0.875, 'rewards/margins': 1.254706621170044, 'policy_logps/rejected': -290.89605712890625, 'policy_logps/chosen': -431.5303039550781, 'referece_logps/rejected': -266.3379821777344, 'referece_logps/chosen': -419.519287109375, 'logits/rejected': -0.2580091953277588, 'logits/chosen': -0.4141322374343872, 'epoch': 6.65}

 74%|███████▍  | 17846/24156 [8:36:23<24:25:08, 13.93s/it]

 74%|███████▍  | 17847/24156 [8:36:34<22:42:05, 12.95s/it]

 74%|███████▍  | 17848/24156 [8:36:45<21:31:02, 12.28s/it]

 74%|███████▍  | 17849/24156 [8:37:07<26:37:39, 15.20s/it]

 74%|███████▍  | 17850/24156 [8:37:29<30:22:36, 17.34s/it]


 74%|███████▍  | 17852/24156 [8:38:08<32:55:41, 18.80s/it]
{'loss': 0.3111, 'learning_rate': 1.9886051288932063e-06, 'rewards/chosen': -1.336907982826233, 'rewards/rejected': -3.652951240539551, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3160431385040283, 'policy_logps/rejected': -445.5409851074219, 'policy_logps/chosen': -365.9287414550781, 'referece_logps/rejected': -409.011474609375, 'referece_logps/chosen': -352.5596923828125, 'logits/rejected': -0.7480883598327637, 'logits/chosen': -0.6923024654388428, 'epoch': 6.65}

 74%|███████▍  | 17853/24156 [8:38:26<32:19:55, 18.47s/it]


 74%|███████▍  | 17855/24156 [8:39:00<31:22:19, 17.92s/it]

 74%|███████▍  | 17856/24156 [8:39:14<29:20:39, 16.77s/it]

 74%|███████▍  | 17857/24156 [8:39:30<28:53:34, 16.51s/it]

 74%|███████▍  | 17858/24156 [8:39:45<27:44:18, 15.86s/it]
{'loss': 0.406, 'learning_rate': 1.9884837103614693e-06, 'rewards/chosen': -1.4477155208587646, 'rewards/rejected': -2.833015203475952, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3852996826171875, 'policy_logps/rejected': -371.7230529785156, 'policy_logps/chosen': -415.5135803222656, 'referece_logps/rejected': -343.3929138183594, 'referece_logps/chosen': -401.03643798828125, 'logits/rejected': -0.4771817624568939, 'logits/chosen': -0.5853789448738098, 'epoch': 6.65}


 74%|███████▍  | 17860/24156 [8:40:13<25:56:29, 14.83s/it]
{'loss': 0.4505, 'learning_rate': 1.9884430953536113e-06, 'rewards/chosen': -1.482283353805542, 'rewards/rejected': -3.169194221496582, 'rewards/accuracies': 0.875, 'rewards/margins': 1.686910629272461, 'policy_logps/rejected': -352.1860046386719, 'policy_logps/chosen': -310.8956604003906, 'referece_logps/rejected': -320.4941101074219, 'referece_logps/chosen': -296.07281494140625, 'logits/rejected': -0.621773362159729, 'logits/chosen': -0.5149489641189575, 'epoch': 6.65}

 74%|███████▍  | 17861/24156 [8:40:29<26:49:14, 15.34s/it]


 74%|███████▍  | 17863/24156 [8:41:10<31:29:34, 18.02s/it]
{'loss': 0.461, 'learning_rate': 1.9883820395731576e-06, 'rewards/chosen': -1.5683655738830566, 'rewards/rejected': -3.0657286643981934, 'rewards/accuracies': 0.75, 'rewards/margins': 1.497363567352295, 'policy_logps/rejected': -363.20245361328125, 'policy_logps/chosen': -311.8125305175781, 'referece_logps/rejected': -332.5451965332031, 'referece_logps/chosen': -296.12884521484375, 'logits/rejected': -0.7475964426994324, 'logits/chosen': -0.7204889059066772, 'epoch': 6.66}

 74%|███████▍  | 17864/24156 [8:41:28<31:07:46, 17.81s/it]

 74%|███████▍  | 17865/24156 [8:41:50<33:18:24, 19.06s/it]

 74%|███████▍  | 17866/24156 [8:42:08<32:52:56, 18.82s/it]


 74%|███████▍  | 17868/24156 [8:42:51<35:06:35, 20.10s/it]
{'loss': 0.3787, 'learning_rate': 1.988279924580366e-06, 'rewards/chosen': -1.4542536735534668, 'rewards/rejected': -3.482513904571533, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0282602310180664, 'policy_logps/rejected': -335.99798583984375, 'policy_logps/chosen': -362.99578857421875, 'referece_logps/rejected': -301.1728820800781, 'referece_logps/chosen': -348.45318603515625, 'logits/rejected': -0.26995909214019775, 'logits/chosen': -0.25988972187042236, 'epoch': 6.66}

 74%|███████▍  | 17869/24156 [8:43:07<33:20:02, 19.09s/it]


 74%|███████▍  | 17871/24156 [8:43:43<32:15:56, 18.48s/it]
{'loss': 0.2988, 'learning_rate': 1.9882184423856232e-06, 'rewards/chosen': -0.9276666641235352, 'rewards/rejected': -3.5684123039245605, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6407456398010254, 'policy_logps/rejected': -283.2786560058594, 'policy_logps/chosen': -488.04193115234375, 'referece_logps/rejected': -247.59454345703125, 'referece_logps/chosen': -478.7652587890625, 'logits/rejected': -0.042562685906887054, 'logits/chosen': -0.12515570223331451, 'epoch': 6.66}


 74%|███████▍  | 17873/24156 [8:44:21<32:40:57, 18.73s/it]

 74%|███████▍  | 17874/24156 [8:44:36<31:00:59, 17.77s/it]
{'loss': 0.4056, 'learning_rate': 1.988156800303733e-06, 'rewards/chosen': -2.089613437652588, 'rewards/rejected': -4.8873772621154785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7977638244628906, 'policy_logps/rejected': -442.4965515136719, 'policy_logps/chosen': -331.4367370605469, 'referece_logps/rejected': -393.622802734375, 'referece_logps/chosen': -310.5405578613281, 'logits/rejected': -0.38358205556869507, 'logits/chosen': -0.10042187571525574, 'epoch': 6.66}

 74%|███████▍  | 17875/24156 [8:44:53<30:32:32, 17.51s/it]

 74%|███████▍  | 17876/24156 [8:45:05<27:33:16, 15.80s/it]

 74%|███████▍  | 17877/24156 [8:45:20<27:03:44, 15.52s/it]

 74%|███████▍  | 17878/24156 [8:45:40<29:32:11, 16.94s/it]

 74%|███████▍  | 17879/24156 [8:45:54<27:55:26, 16.02s/it]

 74%|███████▍  | 17880/24156 [8:46:05<25:19:06, 14.52s/it]

 74%|███████▍  | 17881/24156 [8:46:28<29:27:41, 16.90s/it]
[2024-04-05 23:51:45,787] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 74%|███████▍  | 17883/24156 [8:47:11<33:47:00, 19.39s/it]
{'loss': 0.3401, 'learning_rate': 1.987970914835037e-06, 'rewards/chosen': -1.1903053522109985, 'rewards/rejected': -3.4853837490081787, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2950782775878906, 'policy_logps/rejected': -345.6488342285156, 'policy_logps/chosen': -383.69232177734375, 'referece_logps/rejected': -310.7950134277344, 'referece_logps/chosen': -371.78924560546875, 'logits/rejected': -0.1278906762599945, 'logits/chosen': -0.13352182507514954, 'epoch': 6.66}

 74%|███████▍  | 17884/24156 [8:47:32<34:33:29, 19.84s/it]

 74%|███████▍  | 17885/24156 [8:47:52<34:32:28, 19.83s/it]

 74%|███████▍  | 17886/24156 [8:48:05<31:20:10, 17.99s/it]

 74%|███████▍  | 17887/24156 [8:48:28<33:41:42, 19.35s/it]

 74%|███████▍  | 17888/24156 [8:48:48<33:57:32, 19.50s/it]

 74%|███████▍  | 17889/24156 [8:49:06<33:27:23, 19.22s/it]


 74%|███████▍  | 17891/24156 [8:49:37<30:16:20, 17.40s/it]
{'loss': 0.4622, 'learning_rate': 1.9878044755653327e-06, 'rewards/chosen': -1.2893911600112915, 'rewards/rejected': -3.327547311782837, 'rewards/accuracies': 0.75, 'rewards/margins': 2.038156032562256, 'policy_logps/rejected': -279.341552734375, 'policy_logps/chosen': -479.2933349609375, 'referece_logps/rejected': -246.06607055664062, 'referece_logps/chosen': -466.3994445800781, 'logits/rejected': -0.025849122554063797, 'logits/chosen': -0.14138999581336975, 'epoch': 6.67}

 74%|███████▍  | 17892/24156 [8:49:51<28:18:08, 16.27s/it]
[2024-04-05 23:55:01,118] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 74%|███████▍  | 17894/24156 [8:50:25<29:41:38, 17.07s/it]
{'loss': 0.4894, 'learning_rate': 1.9877417678252443e-06, 'rewards/chosen': -1.356483817100525, 'rewards/rejected': -2.323213815689087, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9667301774024963, 'policy_logps/rejected': -361.4591979980469, 'policy_logps/chosen': -300.95880126953125, 'referece_logps/rejected': -338.22705078125, 'referece_logps/chosen': -287.3939514160156, 'logits/rejected': -0.75055330991745, 'logits/chosen': -0.8102719187736511, 'epoch': 6.67}


 74%|███████▍  | 17896/24156 [8:50:59<30:14:40, 17.39s/it]
{'loss': 0.3994, 'learning_rate': 1.987699873881211e-06, 'rewards/chosen': -0.9176298379898071, 'rewards/rejected': -2.49548077583313, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5778509378433228, 'policy_logps/rejected': -456.90484619140625, 'policy_logps/chosen': -377.0555419921875, 'referece_logps/rejected': -431.95001220703125, 'referece_logps/chosen': -367.8792724609375, 'logits/rejected': -0.042382530868053436, 'logits/chosen': 0.03105458989739418, 'epoch': 6.67}

 74%|███████▍  | 17897/24156 [8:51:15<29:18:30, 16.86s/it]
[2024-04-05 23:56:25,997] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17898/24156 [8:51:29<27:45:51, 15.97s/it]

 74%|███████▍  | 17899/24156 [8:51:40<25:40:18, 14.77s/it]


 74%|███████▍  | 17901/24156 [8:52:11<26:25:46, 15.21s/it]
{'loss': 0.39, 'learning_rate': 1.9875948282991916e-06, 'rewards/chosen': -1.1564397811889648, 'rewards/rejected': -2.950756549835205, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7943170070648193, 'policy_logps/rejected': -367.822021484375, 'policy_logps/chosen': -304.6673583984375, 'referece_logps/rejected': -338.3144836425781, 'referece_logps/chosen': -293.1029357910156, 'logits/rejected': -0.46613502502441406, 'logits/chosen': -0.6281719207763672, 'epoch': 6.67}


 74%|███████▍  | 17903/24156 [8:52:45<28:02:19, 16.14s/it]

 74%|███████▍  | 17904/24156 [8:53:01<27:57:27, 16.10s/it]
{'loss': 0.4723, 'learning_rate': 1.9875315878986206e-06, 'rewards/chosen': -1.2307171821594238, 'rewards/rejected': -2.253861427307129, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0231441259384155, 'policy_logps/rejected': -321.9967956542969, 'policy_logps/chosen': -310.97625732421875, 'referece_logps/rejected': -299.458251953125, 'referece_logps/chosen': -298.6690979003906, 'logits/rejected': -0.852957010269165, 'logits/chosen': -0.8832834959030151, 'epoch': 6.67}

 74%|███████▍  | 17905/24156 [8:53:19<28:38:38, 16.50s/it]

 74%|███████▍  | 17906/24156 [8:53:30<26:09:17, 15.07s/it]

 74%|███████▍  | 17907/24156 [8:53:51<29:01:20, 16.72s/it]

 74%|███████▍  | 17908/24156 [8:54:10<30:05:34, 17.34s/it]

 74%|███████▍  | 17909/24156 [8:54:23<27:44:25, 15.99s/it]

 74%|███████▍  | 17910/24156 [8:54:38<27:38:50, 15.94s/it]

 74%|███████▍  | 17911/24156 [8:54:50<25:33:56, 14.74s/it]

 74%|███████▍  | 17912/24156 [8:55:03<24:15:23, 13.99s/it]

 74%|███████▍  | 17913/24156 [8:55:23<27:25:45, 15.82s/it]


 74%|███████▍  | 17915/24156 [8:55:52<26:23:28, 15.22s/it]

 74%|███████▍  | 17916/24156 [8:56:05<25:38:38, 14.79s/it]
{'loss': 0.2976, 'learning_rate': 1.987277028638848e-06, 'rewards/chosen': -1.1867684125900269, 'rewards/rejected': -3.456841230392456, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2700726985931396, 'policy_logps/rejected': -323.60601806640625, 'policy_logps/chosen': -410.3359375, 'referece_logps/rejected': -289.03759765625, 'referece_logps/chosen': -398.46826171875, 'logits/rejected': -0.8324392437934875, 'logits/chosen': -0.8963978290557861, 'epoch': 6.68}


 74%|███████▍  | 17918/24156 [8:56:39<28:10:08, 16.26s/it]
{'loss': 0.3063, 'learning_rate': 1.987234353601302e-06, 'rewards/chosen': -1.4040241241455078, 'rewards/rejected': -3.4806573390960693, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0766332149505615, 'policy_logps/rejected': -331.994140625, 'policy_logps/chosen': -392.98388671875, 'referece_logps/rejected': -297.1875, 'referece_logps/chosen': -378.94366455078125, 'logits/rejected': -0.6956519484519958, 'logits/chosen': -0.9701008796691895, 'epoch': 6.68}

 74%|███████▍  | 17919/24156 [8:56:55<27:50:24, 16.07s/it]

 74%|███████▍  | 17920/24156 [8:57:14<29:21:49, 16.95s/it]

 74%|███████▍  | 17921/24156 [8:57:29<28:27:39, 16.43s/it]
[2024-04-06 00:02:47,357] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17922/24156 [8:57:50<30:39:28, 17.70s/it]

 74%|███████▍  | 17923/24156 [8:58:04<28:46:29, 16.62s/it]

 74%|███████▍  | 17924/24156 [8:58:15<25:55:22, 14.97s/it]

 74%|███████▍  | 17925/24156 [8:58:30<26:06:55, 15.09s/it]

 74%|███████▍  | 17926/24156 [8:58:50<28:31:18, 16.48s/it]

 74%|███████▍  | 17927/24156 [8:59:06<28:02:45, 16.21s/it]

 74%|███████▍  | 17928/24156 [8:59:18<26:03:23, 15.06s/it]

 74%|███████▍  | 17929/24156 [8:59:30<24:26:30, 14.13s/it]

 74%|███████▍  | 17930/24156 [8:59:42<23:26:41, 13.56s/it]

 74%|███████▍  | 17931/24156 [9:00:03<26:59:14, 15.61s/it]

 74%|███████▍  | 17932/24156 [9:00:19<27:12:57, 15.74s/it]

 74%|███████▍  | 17933/24156 [9:00:40<30:05:56, 17.41s/it]

 74%|███████▍  | 17934/24156 [9:00:57<29:58:27, 17.34s/it]

 74%|███████▍  | 17935/24156 [9:01:17<31:04:35, 17.98s/it]

 74%|███████▍  | 17936/24156 [9:01:38<32:36:13, 18.87s/it]

 74%|███████▍  | 17937/24156 [9:01:56<32:21:42, 18.73s/it]

 74%|███████▍  | 17938/24156 [9:02:14<32:05:48, 18.58s/it]

 74%|███████▍  | 17939/24156 [9:02:34<32:27:32, 18.80s/it]
[2024-04-06 00:07:54,893] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17940/24156 [9:02:57<35:02:44, 20.30s/it]
[2024-04-06 00:08:08,617] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17941/24156 [9:03:11<31:38:08, 18.32s/it]

 74%|███████▍  | 17942/24156 [9:03:30<31:51:35, 18.46s/it]

 74%|███████▍  | 17943/24156 [9:03:48<31:35:58, 18.31s/it]

 74%|███████▍  | 17944/24156 [9:04:03<30:09:13, 17.47s/it]

 74%|███████▍  | 17945/24156 [9:04:16<27:23:46, 15.88s/it]

 74%|███████▍  | 17946/24156 [9:04:27<25:16:37, 14.65s/it]

 74%|███████▍  | 17947/24156 [9:04:45<27:04:10, 15.69s/it]

 74%|███████▍  | 17948/24156 [9:04:57<25:04:12, 14.54s/it]

 74%|███████▍  | 17949/24156 [9:05:17<27:39:20, 16.04s/it]

 74%|███████▍  | 17950/24156 [9:05:38<30:12:59, 17.53s/it]

 74%|███████▍  | 17951/24156 [9:05:57<31:18:09, 18.16s/it]

 74%|███████▍  | 17952/24156 [9:06:09<27:56:48, 16.22s/it]
[2024-04-06 00:11:24,701] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17953/24156 [9:06:27<28:53:21, 16.77s/it]

 74%|███████▍  | 17954/24156 [9:06:48<31:04:37, 18.04s/it]

 74%|███████▍  | 17955/24156 [9:07:04<29:49:30, 17.31s/it]

 74%|███████▍  | 17956/24156 [9:07:17<27:37:27, 16.04s/it]

 74%|███████▍  | 17957/24156 [9:07:31<26:23:39, 15.33s/it]

 74%|███████▍  | 17958/24156 [9:07:50<28:36:28, 16.62s/it]
[2024-04-06 00:13:08,107] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17959/24156 [9:08:11<30:33:57, 17.76s/it]

 74%|███████▍  | 17960/24156 [9:08:31<32:02:15, 18.61s/it]

 74%|███████▍  | 17961/24156 [9:08:49<31:33:03, 18.33s/it]

 74%|███████▍  | 17962/24156 [9:09:04<29:47:21, 17.31s/it]

 74%|███████▍  | 17963/24156 [9:09:18<28:02:20, 16.30s/it]

 74%|███████▍  | 17964/24156 [9:09:30<26:07:56, 15.19s/it]

 74%|███████▍  | 17965/24156 [9:09:53<29:41:54, 17.27s/it]

 74%|███████▍  | 17966/24156 [9:10:11<30:04:31, 17.49s/it]
[2024-04-06 00:15:29,921] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17967/24156 [9:10:32<32:21:11, 18.82s/it]
[2024-04-06 00:15:45,476] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 74%|███████▍  | 17969/24156 [9:11:08<31:46:26, 18.49s/it]

 74%|███████▍  | 17970/24156 [9:11:28<32:41:08, 19.02s/it]

 74%|███████▍  | 17971/24156 [9:11:48<33:06:17, 19.27s/it]

 74%|███████▍  | 17972/24156 [9:12:06<32:34:08, 18.96s/it]

 74%|███████▍  | 17973/24156 [9:12:26<32:54:30, 19.16s/it]

 74%|███████▍  | 17974/24156 [9:12:40<30:02:36, 17.50s/it]

 74%|███████▍  | 17975/24156 [9:12:54<28:41:08, 16.71s/it]

 74%|███████▍  | 17976/24156 [9:13:14<30:21:21, 17.68s/it]

 74%|███████▍  | 17977/24156 [9:13:32<30:17:44, 17.65s/it]
[2024-04-06 00:18:29,471] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17978/24156 [9:13:48<29:35:30, 17.24s/it]

 74%|███████▍  | 17979/24156 [9:14:03<28:14:30, 16.46s/it]

 74%|███████▍  | 17980/24156 [9:14:25<30:54:04, 18.01s/it]

 74%|███████▍  | 17981/24156 [9:14:37<28:09:30, 16.42s/it]

 74%|███████▍  | 17982/24156 [9:14:52<27:28:35, 16.02s/it]

 74%|███████▍  | 17983/24156 [9:15:06<26:08:53, 15.25s/it]

 74%|███████▍  | 17984/24156 [9:15:28<29:44:24, 17.35s/it]

 74%|███████▍  | 17985/24156 [9:15:48<31:13:27, 18.22s/it]

 74%|███████▍  | 17986/24156 [9:16:08<32:02:04, 18.69s/it]
[2024-04-06 00:21:05,554] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 74%|███████▍  | 17987/24156 [9:16:29<33:10:04, 19.36s/it]

 74%|███████▍  | 17988/24156 [9:16:47<32:21:55, 18.89s/it]

 74%|███████▍  | 17989/24156 [9:17:06<32:44:04, 19.11s/it]

 74%|███████▍  | 17990/24156 [9:17:19<29:22:52, 17.15s/it]

 74%|███████▍  | 17991/24156 [9:17:32<27:04:07, 15.81s/it]

 74%|███████▍  | 17992/24156 [9:17:49<27:55:24, 16.31s/it]

 74%|███████▍  | 17993/24156 [9:18:00<25:02:10, 14.62s/it]

 74%|███████▍  | 17994/24156 [9:18:22<28:43:16, 16.78s/it]

 74%|███████▍  | 17995/24156 [9:18:40<29:21:46, 17.16s/it]

 74%|███████▍  | 17996/24156 [9:18:54<27:47:41, 16.24s/it]

 75%|███████▍  | 17997/24156 [9:19:05<25:13:52, 14.75s/it]

 75%|███████▍  | 17998/24156 [9:19:27<28:45:44, 16.81s/it]
[2024-04-06 00:24:24,167] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 17999/24156 [9:19:48<30:53:51, 18.07s/it]
[2024-04-06 00:24:45,153] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 18000/24156 [9:20:08<31:51:25, 18.63s/it]

 75%|███████▍  | 18001/24156 [9:20:36<36:40:56, 21.46s/it]
[2024-04-06 00:25:33,146] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 18002/24156 [9:20:56<36:03:12, 21.09s/it]

 75%|███████▍  | 18003/24156 [9:21:16<35:21:12, 20.68s/it]

 75%|███████▍  | 18004/24156 [9:21:36<35:15:23, 20.63s/it]

 75%|███████▍  | 18005/24156 [9:21:53<33:06:12, 19.37s/it]

 75%|███████▍  | 18006/24156 [9:22:13<33:40:46, 19.71s/it]
[2024-04-06 00:27:10,581] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 18007/24156 [9:22:26<30:07:37, 17.64s/it]

 75%|███████▍  | 18008/24156 [9:22:39<27:47:30, 16.27s/it]

 75%|███████▍  | 18009/24156 [9:22:59<29:29:52, 17.28s/it]

 75%|███████▍  | 18010/24156 [9:23:10<26:38:56, 15.61s/it]

 75%|███████▍  | 18011/24156 [9:23:30<28:58:55, 16.98s/it]

 75%|███████▍  | 18012/24156 [9:23:50<30:18:55, 17.76s/it]

 75%|███████▍  | 18013/24156 [9:24:07<29:41:58, 17.40s/it]

 75%|███████▍  | 18014/24156 [9:24:25<29:55:56, 17.54s/it]

 75%|███████▍  | 18015/24156 [9:24:42<29:49:49, 17.49s/it]

 75%|███████▍  | 18016/24156 [9:24:57<28:32:53, 16.74s/it]

 75%|███████▍  | 18017/24156 [9:25:08<25:44:02, 15.09s/it]

 75%|███████▍  | 18018/24156 [9:25:28<28:01:06, 16.43s/it]

 75%|███████▍  | 18019/24156 [9:25:41<26:16:04, 15.41s/it]

 75%|███████▍  | 18020/24156 [9:25:59<27:34:28, 16.18s/it]
{'loss': 0.3832, 'learning_rate': 1.984963863005064e-06, 'rewards/chosen': -2.2264349460601807, 'rewards/rejected': -2.8187503814697266, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5923151969909668, 'policy_logps/rejected': -327.298828125, 'policy_logps/chosen': -405.21258544921875, 'referece_logps/rejected': -299.11126708984375, 'referece_logps/chosen': -382.9482421875, 'logits/rejected': -0.8893924355506897, 'logits/chosen': -0.8428429961204529, 'epoch': 6.71}
[2024-04-06 00:31:17,137] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▍  | 18022/24156 [9:26:40<31:39:48, 18.58s/it]
[2024-04-06 00:31:37,968] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 18023/24156 [9:26:59<31:23:22, 18.43s/it]

 75%|███████▍  | 18024/24156 [9:27:15<30:14:10, 17.75s/it]

 75%|███████▍  | 18025/24156 [9:27:29<28:19:23, 16.63s/it]

 75%|███████▍  | 18026/24156 [9:27:45<28:17:03, 16.61s/it]

 75%|███████▍  | 18027/24156 [9:28:05<29:48:06, 17.50s/it]

 75%|███████▍  | 18028/24156 [9:28:27<32:02:03, 18.82s/it]

 75%|███████▍  | 18029/24156 [9:28:48<33:12:28, 19.51s/it]

 75%|███████▍  | 18030/24156 [9:29:06<32:37:21, 19.17s/it]

 75%|███████▍  | 18031/24156 [9:29:21<30:34:41, 17.97s/it]
{'loss': 0.4124, 'learning_rate': 1.9847079937474705e-06, 'rewards/chosen': -1.4769715070724487, 'rewards/rejected': -2.9570677280426025, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4800963401794434, 'policy_logps/rejected': -384.86993408203125, 'policy_logps/chosen': -458.86102294921875, 'referece_logps/rejected': -355.29925537109375, 'referece_logps/chosen': -444.09124755859375, 'logits/rejected': -0.6465961933135986, 'logits/chosen': -0.5164041519165039, 'epoch': 6.72}


 75%|███████▍  | 18033/24156 [9:29:58<30:55:45, 18.18s/it]

 75%|███████▍  | 18034/24156 [9:30:10<27:44:04, 16.31s/it]

 75%|███████▍  | 18035/24156 [9:30:21<25:27:27, 14.97s/it]

 75%|███████▍  | 18036/24156 [9:30:35<24:44:23, 14.55s/it]

 75%|███████▍  | 18037/24156 [9:30:46<23:01:39, 13.55s/it]
{'loss': 0.4244, 'learning_rate': 1.984567525853602e-06, 'rewards/chosen': -1.8333336114883423, 'rewards/rejected': -3.1883881092071533, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3550546169281006, 'policy_logps/rejected': -539.358154296875, 'policy_logps/chosen': -413.80157470703125, 'referece_logps/rejected': -507.47430419921875, 'referece_logps/chosen': -395.46826171875, 'logits/rejected': -0.8178521990776062, 'logits/chosen': -0.8897266983985901, 'epoch': 6.72}


 75%|███████▍  | 18039/24156 [9:31:15<24:00:36, 14.13s/it]

 75%|███████▍  | 18040/24156 [9:31:32<25:12:40, 14.84s/it]

 75%|███████▍  | 18041/24156 [9:31:53<28:16:09, 16.64s/it]
[2024-04-06 00:36:50,132] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4417, 'learning_rate': 1.9844735265932887e-06, 'rewards/chosen': -1.611542820930481, 'rewards/rejected': -2.5165059566497803, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9049632549285889, 'policy_logps/rejected': -381.6187438964844, 'policy_logps/chosen': -479.7947998046875, 'referece_logps/rejected': -356.4537048339844, 'referece_logps/chosen': -463.6793212890625, 'logits/rejected': -0.10192237794399261, 'logits/chosen': -0.35657238960266113, 'epoch': 6.72}

 75%|███████▍  | 18042/24156 [9:32:10<28:45:54, 16.94s/it]


 75%|███████▍  | 18044/24156 [9:32:45<28:35:20, 16.84s/it]

 75%|███████▍  | 18045/24156 [9:33:03<29:23:46, 17.32s/it]

 75%|███████▍  | 18046/24156 [9:33:16<27:00:59, 15.92s/it]

 75%|███████▍  | 18047/24156 [9:33:31<26:45:00, 15.76s/it]

 75%|███████▍  | 18048/24156 [9:33:49<27:40:27, 16.31s/it]

 75%|███████▍  | 18049/24156 [9:34:05<27:33:30, 16.25s/it]

 75%|███████▍  | 18050/24156 [9:34:21<27:36:44, 16.28s/it]

 75%|███████▍  | 18051/24156 [9:34:43<30:36:52, 18.05s/it]
{'loss': 0.3916, 'learning_rate': 1.9842372896477317e-06, 'rewards/chosen': -2.1625750064849854, 'rewards/rejected': -3.2647616863250732, 'rewards/accuracies': 0.625, 'rewards/margins': 1.102186679840088, 'policy_logps/rejected': -379.39892578125, 'policy_logps/chosen': -307.9287109375, 'referece_logps/rejected': -346.7513122558594, 'referece_logps/chosen': -286.30291748046875, 'logits/rejected': -1.1499927043914795, 'logits/chosen': -0.9786328673362732, 'epoch': 6.73}


 75%|███████▍  | 18053/24156 [9:35:19<30:36:00, 18.05s/it]
{'loss': 0.4147, 'learning_rate': 1.9841898299209716e-06, 'rewards/chosen': -1.3763563632965088, 'rewards/rejected': -3.724173069000244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3478167057037354, 'policy_logps/rejected': -390.64080810546875, 'policy_logps/chosen': -368.50909423828125, 'referece_logps/rejected': -353.39910888671875, 'referece_logps/chosen': -354.74554443359375, 'logits/rejected': -0.6890839338302612, 'logits/chosen': -0.4382115602493286, 'epoch': 6.73}


 75%|███████▍  | 18055/24156 [9:35:46<26:48:58, 15.82s/it]

 75%|███████▍  | 18056/24156 [9:36:05<28:48:35, 17.00s/it]

 75%|███████▍  | 18057/24156 [9:36:20<27:35:07, 16.28s/it]

 75%|███████▍  | 18058/24156 [9:36:35<26:51:19, 15.85s/it]

 75%|███████▍  | 18059/24156 [9:36:52<27:24:05, 16.18s/it]

 75%|███████▍  | 18060/24156 [9:37:12<29:10:33, 17.23s/it]

 75%|███████▍  | 18061/24156 [9:37:31<30:30:22, 18.02s/it]
[2024-04-06 00:42:28,853] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 18062/24156 [9:37:52<31:54:18, 18.85s/it]

 75%|███████▍  | 18063/24156 [9:38:11<32:06:05, 18.97s/it]

 75%|███████▍  | 18064/24156 [9:38:23<28:31:07, 16.85s/it]

 75%|███████▍  | 18065/24156 [9:38:42<29:34:20, 17.48s/it]

 75%|███████▍  | 18066/24156 [9:38:57<28:16:02, 16.71s/it]

 75%|███████▍  | 18067/24156 [9:39:15<28:51:57, 17.07s/it]

 75%|███████▍  | 18068/24156 [9:39:35<30:26:48, 18.00s/it]

 75%|███████▍  | 18069/24156 [9:39:46<26:57:04, 15.94s/it]
{'loss': 0.507, 'learning_rate': 1.9838076046290855e-06, 'rewards/chosen': -1.7864625453948975, 'rewards/rejected': -3.205463171005249, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4190008640289307, 'policy_logps/rejected': -288.29803466796875, 'policy_logps/chosen': -337.9129943847656, 'referece_logps/rejected': -256.243408203125, 'referece_logps/chosen': -320.04833984375, 'logits/rejected': -0.5744627714157104, 'logits/chosen': -0.5626176595687866, 'epoch': 6.73}


 75%|███████▍  | 18071/24156 [9:40:13<25:16:47, 14.96s/it]

 75%|███████▍  | 18072/24156 [9:40:24<23:10:43, 13.72s/it]

 75%|███████▍  | 18073/24156 [9:40:40<24:24:10, 14.44s/it]

 75%|███████▍  | 18074/24156 [9:40:54<24:05:31, 14.26s/it]
{'loss': 0.4664, 'learning_rate': 1.9836872306303727e-06, 'rewards/chosen': -2.1285154819488525, 'rewards/rejected': -3.596149206161499, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4676337242126465, 'policy_logps/rejected': -252.67312622070312, 'policy_logps/chosen': -419.135498046875, 'referece_logps/rejected': -216.7116241455078, 'referece_logps/chosen': -397.8503112792969, 'logits/rejected': 0.052014291286468506, 'logits/chosen': 0.028367280960083008, 'epoch': 6.73}


 75%|███████▍  | 18076/24156 [9:41:15<20:57:25, 12.41s/it]

 75%|███████▍  | 18077/24156 [9:41:28<20:59:25, 12.43s/it]
{'loss': 0.3449, 'learning_rate': 1.983614794022309e-06, 'rewards/chosen': -1.9582809209823608, 'rewards/rejected': -4.2993693351745605, 'rewards/accuracies': 0.875, 'rewards/margins': 2.34108829498291, 'policy_logps/rejected': -316.8994140625, 'policy_logps/chosen': -338.88372802734375, 'referece_logps/rejected': -273.9057312011719, 'referece_logps/chosen': -319.3009338378906, 'logits/rejected': -0.07737962156534195, 'logits/chosen': -0.11660037189722061, 'epoch': 6.74}

 75%|███████▍  | 18078/24156 [9:41:39<20:09:36, 11.94s/it]


 75%|███████▍  | 18080/24156 [9:42:12<24:22:55, 14.45s/it]
{'loss': 0.3181, 'learning_rate': 1.9835421982719375e-06, 'rewards/chosen': -2.1467790603637695, 'rewards/rejected': -4.624666213989258, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4778873920440674, 'policy_logps/rejected': -243.9351348876953, 'policy_logps/chosen': -385.9288635253906, 'referece_logps/rejected': -197.68846130371094, 'referece_logps/chosen': -364.4610595703125, 'logits/rejected': -0.3217472732067108, 'logits/chosen': -0.4389263987541199, 'epoch': 6.74}
[2024-04-06 00:47:30,509] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▍  | 18082/24156 [9:42:44<24:52:12, 14.74s/it]
{'loss': 0.2841, 'learning_rate': 1.9834937126984284e-06, 'rewards/chosen': -1.5907931327819824, 'rewards/rejected': -3.623349666595459, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0325562953948975, 'policy_logps/rejected': -355.5623474121094, 'policy_logps/chosen': -522.5755615234375, 'referece_logps/rejected': -319.3288879394531, 'referece_logps/chosen': -506.6676025390625, 'logits/rejected': -0.4745018184185028, 'logits/chosen': -0.5398184657096863, 'epoch': 6.74}

 75%|███████▍  | 18083/24156 [9:42:57<23:56:42, 14.19s/it]
[2024-04-06 00:48:10,745] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▍  | 18085/24156 [9:43:26<24:07:46, 14.31s/it]
{'loss': 0.3941, 'learning_rate': 1.9834208517369897e-06, 'rewards/chosen': -2.3465287685394287, 'rewards/rejected': -4.205439567565918, 'rewards/accuracies': 0.75, 'rewards/margins': 1.858910322189331, 'policy_logps/rejected': -262.37799072265625, 'policy_logps/chosen': -386.53363037109375, 'referece_logps/rejected': -220.32362365722656, 'referece_logps/chosen': -363.068359375, 'logits/rejected': -0.665088951587677, 'logits/chosen': -0.5982721447944641, 'epoch': 6.74}


 75%|███████▍  | 18087/24156 [9:43:59<26:12:04, 15.54s/it]

 75%|███████▍  | 18088/24156 [9:44:17<27:28:22, 16.30s/it]
[2024-04-06 00:49:14,148] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3054, 'learning_rate': 1.983347831664621e-06, 'rewards/chosen': -1.477713942527771, 'rewards/rejected': -4.184151649475098, 'rewards/accuracies': 0.875, 'rewards/margins': 2.706437587738037, 'policy_logps/rejected': -248.4091033935547, 'policy_logps/chosen': -388.7993469238281, 'referece_logps/rejected': -206.5675811767578, 'referece_logps/chosen': -374.0221862792969, 'logits/rejected': -0.1805853694677353, 'logits/chosen': -0.2532953917980194, 'epoch': 6.74}


 75%|███████▍  | 18090/24156 [9:44:52<28:37:09, 16.98s/it]

 75%|███████▍  | 18091/24156 [9:45:12<30:00:57, 17.82s/it]

 75%|███████▍  | 18092/24156 [9:45:32<31:01:13, 18.42s/it]
[2024-04-06 00:50:29,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3567, 'learning_rate': 1.9832502240826628e-06, 'rewards/chosen': -2.076730251312256, 'rewards/rejected': -3.4219672679901123, 'rewards/accuracies': 0.625, 'rewards/margins': 1.345237374305725, 'policy_logps/rejected': -427.6258544921875, 'policy_logps/chosen': -483.3941650390625, 'referece_logps/rejected': -393.4062194824219, 'referece_logps/chosen': -462.6268310546875, 'logits/rejected': 0.17693263292312622, 'logits/chosen': 0.11590602993965149, 'epoch': 6.74}


 75%|███████▍  | 18094/24156 [9:46:04<29:20:30, 17.43s/it]

 75%|███████▍  | 18095/24156 [9:46:24<30:20:59, 18.03s/it]

 75%|███████▍  | 18096/24156 [9:46:40<29:35:55, 17.58s/it]

 75%|███████▍  | 18097/24156 [9:47:00<30:36:52, 18.19s/it]
{'loss': 0.4084, 'learning_rate': 1.983127816900208e-06, 'rewards/chosen': -2.328418254852295, 'rewards/rejected': -3.610835313796997, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2824171781539917, 'policy_logps/rejected': -271.1933288574219, 'policy_logps/chosen': -335.59515380859375, 'referece_logps/rejected': -235.0850067138672, 'referece_logps/chosen': -312.3109436035156, 'logits/rejected': -0.3790980577468872, 'logits/chosen': -0.5432552099227905, 'epoch': 6.74}


 75%|███████▍  | 18099/24156 [9:47:29<27:15:18, 16.20s/it]
{'loss': 0.4096, 'learning_rate': 1.983078730308072e-06, 'rewards/chosen': -1.9105592966079712, 'rewards/rejected': -3.8032262325286865, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8926671743392944, 'policy_logps/rejected': -488.420166015625, 'policy_logps/chosen': -365.40106201171875, 'referece_logps/rejected': -450.3879089355469, 'referece_logps/chosen': -346.2954406738281, 'logits/rejected': -0.5102437138557434, 'logits/chosen': -0.3416709303855896, 'epoch': 6.74}


 75%|███████▍  | 18101/24156 [9:48:07<29:42:07, 17.66s/it]

 75%|███████▍  | 18102/24156 [9:48:21<27:59:46, 16.65s/it]

 75%|███████▍  | 18103/24156 [9:48:44<31:21:14, 18.65s/it]

 75%|███████▍  | 18104/24156 [9:49:04<31:59:13, 19.03s/it]

 75%|███████▍  | 18105/24156 [9:49:15<27:44:31, 16.51s/it]
{'loss': 0.3198, 'learning_rate': 1.9829310463976e-06, 'rewards/chosen': -1.021571159362793, 'rewards/rejected': -2.101212739944458, 'rewards/accuracies': 0.75, 'rewards/margins': 1.079641580581665, 'policy_logps/rejected': -499.3405456542969, 'policy_logps/chosen': -292.2772521972656, 'referece_logps/rejected': -478.3283996582031, 'referece_logps/chosen': -282.0615234375, 'logits/rejected': -0.9626370668411255, 'logits/chosen': -0.7713159322738647, 'epoch': 6.75}


 75%|███████▍  | 18107/24156 [9:49:44<26:37:50, 15.85s/it]

 75%|███████▍  | 18108/24156 [9:49:55<24:02:44, 14.31s/it]
{'loss': 0.4015, 'learning_rate': 1.98285696588886e-06, 'rewards/chosen': -2.2436423301696777, 'rewards/rejected': -3.859502077102661, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6158599853515625, 'policy_logps/rejected': -362.3019714355469, 'policy_logps/chosen': -382.260009765625, 'referece_logps/rejected': -323.7069091796875, 'referece_logps/chosen': -359.8236083984375, 'logits/rejected': 0.0075174737721681595, 'logits/chosen': -0.07451412826776505, 'epoch': 6.75}


 75%|███████▍  | 18110/24156 [9:50:16<20:51:12, 12.42s/it]
{'loss': 0.4236, 'learning_rate': 1.9828074905380164e-06, 'rewards/chosen': -1.4027858972549438, 'rewards/rejected': -3.0820159912109375, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6792300939559937, 'policy_logps/rejected': -342.0942687988281, 'policy_logps/chosen': -351.4896545410156, 'referece_logps/rejected': -311.274169921875, 'referece_logps/chosen': -337.4618225097656, 'logits/rejected': -0.44402390718460083, 'logits/chosen': -0.3934321403503418, 'epoch': 6.75}


 75%|███████▍  | 18112/24156 [9:50:45<23:07:16, 13.77s/it]

 75%|███████▍  | 18113/24156 [9:50:56<21:43:11, 12.94s/it]

 75%|███████▍  | 18114/24156 [9:51:11<22:41:06, 13.52s/it]

 75%|███████▍  | 18115/24156 [9:51:28<24:45:30, 14.75s/it]
{'loss': 0.5062, 'learning_rate': 1.982683492979299e-06, 'rewards/chosen': -1.4993572235107422, 'rewards/rejected': -3.0440287590026855, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5446714162826538, 'policy_logps/rejected': -442.6921691894531, 'policy_logps/chosen': -385.1739501953125, 'referece_logps/rejected': -412.25189208984375, 'referece_logps/chosen': -370.180419921875, 'logits/rejected': -0.05374757945537567, 'logits/chosen': 0.06014862656593323, 'epoch': 6.75}


 75%|███████▌  | 18117/24156 [9:51:55<23:13:10, 13.84s/it]
{'loss': 0.4357, 'learning_rate': 1.9826337702925315e-06, 'rewards/chosen': -1.3212268352508545, 'rewards/rejected': -2.407228708267212, 'rewards/accuracies': 0.75, 'rewards/margins': 1.086001992225647, 'policy_logps/rejected': -552.2670288085938, 'policy_logps/chosen': -634.7882080078125, 'referece_logps/rejected': -528.1947631835938, 'referece_logps/chosen': -621.575927734375, 'logits/rejected': -0.09418174624443054, 'logits/chosen': -0.11233930289745331, 'epoch': 6.75}

 75%|███████▌  | 18118/24156 [9:52:06<21:32:52, 12.85s/it]


 75%|███████▌  | 18120/24156 [9:52:28<20:11:45, 12.05s/it]
{'loss': 0.4387, 'learning_rate': 1.9825590537771763e-06, 'rewards/chosen': -1.682839035987854, 'rewards/rejected': -2.4691030979156494, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7862640619277954, 'policy_logps/rejected': -518.3753662109375, 'policy_logps/chosen': -419.7412109375, 'referece_logps/rejected': -493.6843566894531, 'referece_logps/chosen': -402.9128723144531, 'logits/rejected': -0.4978961944580078, 'logits/chosen': -0.4841364920139313, 'epoch': 6.75}

 75%|███████▌  | 18121/24156 [9:52:48<23:55:47, 14.27s/it]


 75%|███████▌  | 18123/24156 [9:53:23<27:05:34, 16.17s/it]

 75%|███████▌  | 18124/24156 [9:53:44<29:48:47, 17.79s/it]
{'loss': 0.4221, 'learning_rate': 1.9824591844698007e-06, 'rewards/chosen': -1.7315866947174072, 'rewards/rejected': -2.340059280395508, 'rewards/accuracies': 0.375, 'rewards/margins': 0.6084725856781006, 'policy_logps/rejected': -293.642578125, 'policy_logps/chosen': -347.85113525390625, 'referece_logps/rejected': -270.2419738769531, 'referece_logps/chosen': -330.5353088378906, 'logits/rejected': -0.9783558249473572, 'logits/chosen': -1.257947325706482, 'epoch': 6.75}


 75%|███████▌  | 18126/24156 [9:54:09<25:14:05, 15.07s/it]

 75%|███████▌  | 18127/24156 [9:54:29<27:40:31, 16.53s/it]

 75%|███████▌  | 18128/24156 [9:54:50<30:14:15, 18.06s/it]
[2024-04-06 00:59:47,826] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18129/24156 [9:55:09<30:28:38, 18.20s/it]
[2024-04-06 01:00:06,370] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3665, 'learning_rate': 1.982333950450614e-06, 'rewards/chosen': -1.4370938539505005, 'rewards/rejected': -2.2735824584960938, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8364884853363037, 'policy_logps/rejected': -391.47113037109375, 'policy_logps/chosen': -326.119873046875, 'referece_logps/rejected': -368.73529052734375, 'referece_logps/chosen': -311.7489318847656, 'logits/rejected': -0.8718203902244568, 'logits/chosen': -0.7334209680557251, 'epoch': 6.75}


 75%|███████▌  | 18131/24156 [9:55:39<28:22:16, 16.95s/it]

 75%|███████▌  | 18132/24156 [9:55:55<28:01:17, 16.75s/it]

 75%|███████▌  | 18133/24156 [9:56:17<30:37:27, 18.30s/it]
{'loss': 0.3853, 'learning_rate': 1.982233445362411e-06, 'rewards/chosen': -2.4849915504455566, 'rewards/rejected': -5.155490398406982, 'rewards/accuracies': 0.875, 'rewards/margins': 2.670499086380005, 'policy_logps/rejected': -381.4918212890625, 'policy_logps/chosen': -431.8403015136719, 'referece_logps/rejected': -329.9369201660156, 'referece_logps/chosen': -406.9903564453125, 'logits/rejected': -0.2458413541316986, 'logits/chosen': -0.10444606840610504, 'epoch': 6.76}

 75%|███████▌  | 18134/24156 [9:56:30<27:43:25, 16.57s/it]


 75%|███████▌  | 18136/24156 [9:57:12<31:45:16, 18.99s/it]

 75%|███████▌  | 18137/24156 [9:57:29<30:45:52, 18.40s/it]

 75%|███████▌  | 18138/24156 [9:57:51<32:22:11, 19.36s/it]

 75%|███████▌  | 18139/24156 [9:58:11<32:24:24, 19.39s/it]
{'loss': 0.2909, 'learning_rate': 1.9820821580097844e-06, 'rewards/chosen': -2.071308135986328, 'rewards/rejected': -4.3295159339904785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2582075595855713, 'policy_logps/rejected': -333.4042053222656, 'policy_logps/chosen': -413.0447082519531, 'referece_logps/rejected': -290.10906982421875, 'referece_logps/chosen': -392.33160400390625, 'logits/rejected': -0.6505311727523804, 'logits/chosen': -0.7606498003005981, 'epoch': 6.76}


 75%|███████▌  | 18141/24156 [9:58:42<29:14:57, 17.51s/it]

 75%|███████▌  | 18142/24156 [9:59:01<29:53:54, 17.90s/it]
{'loss': 0.2887, 'learning_rate': 1.9820062759858386e-06, 'rewards/chosen': -1.2808518409729004, 'rewards/rejected': -2.8605244159698486, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5796723365783691, 'policy_logps/rejected': -321.7154541015625, 'policy_logps/chosen': -362.85205078125, 'referece_logps/rejected': -293.11016845703125, 'referece_logps/chosen': -350.0435485839844, 'logits/rejected': 0.0012309849262237549, 'logits/chosen': -0.11402414739131927, 'epoch': 6.76}


 75%|███████▌  | 18144/24156 [9:59:33<28:52:10, 17.29s/it]

 75%|███████▌  | 18145/24156 [9:59:49<28:02:04, 16.79s/it]

 75%|███████▌  | 18146/24156 [10:00:11<30:49:43, 18.47s/it]
[2024-04-06 01:05:08,912] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18147/24156 [10:00:27<29:07:50, 17.45s/it]
[2024-04-06 01:05:23,998] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18148/24156 [10:00:40<27:02:57, 16.21s/it]

 75%|███████▌  | 18149/24156 [10:00:57<27:21:28, 16.40s/it]

 75%|███████▌  | 18150/24156 [10:01:17<29:09:26, 17.48s/it]

 75%|███████▌  | 18151/24156 [10:01:37<30:39:54, 18.38s/it]

 75%|███████▌  | 18152/24156 [10:01:57<31:29:22, 18.88s/it]
{'loss': 0.4537, 'learning_rate': 1.981752188493625e-06, 'rewards/chosen': -1.643456220626831, 'rewards/rejected': -3.051715850830078, 'rewards/accuracies': 0.75, 'rewards/margins': 1.408259630203247, 'policy_logps/rejected': -430.21307373046875, 'policy_logps/chosen': -396.33245849609375, 'referece_logps/rejected': -399.6959228515625, 'referece_logps/chosen': -379.89788818359375, 'logits/rejected': -0.49984118342399597, 'logits/chosen': -0.5151215195655823, 'epoch': 6.76}


 75%|███████▌  | 18154/24156 [10:02:35<31:20:00, 18.79s/it]

 75%|███████▌  | 18155/24156 [10:02:51<29:54:36, 17.94s/it]
{'loss': 0.314, 'learning_rate': 1.9816756180697475e-06, 'rewards/chosen': -0.9547069072723389, 'rewards/rejected': -4.5313591957092285, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5766525268554688, 'policy_logps/rejected': -293.86041259765625, 'policy_logps/chosen': -408.6187744140625, 'referece_logps/rejected': -248.54684448242188, 'referece_logps/chosen': -399.07171630859375, 'logits/rejected': -0.2127620130777359, 'logits/chosen': -0.34893718361854553, 'epoch': 6.76}


 75%|███████▌  | 18157/24156 [10:03:24<28:09:27, 16.90s/it]
{'loss': 0.3539, 'learning_rate': 1.9816244828816396e-06, 'rewards/chosen': -1.7423557043075562, 'rewards/rejected': -4.005204677581787, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2628488540649414, 'policy_logps/rejected': -370.96282958984375, 'policy_logps/chosen': -353.1142578125, 'referece_logps/rejected': -330.9107666015625, 'referece_logps/chosen': -335.6907043457031, 'logits/rejected': -0.4921267628669739, 'logits/chosen': -0.43748557567596436, 'epoch': 6.76}

 75%|███████▌  | 18158/24156 [10:03:37<26:10:51, 15.71s/it]


 75%|███████▌  | 18160/24156 [10:04:16<30:01:41, 18.03s/it]
{'loss': 0.2709, 'learning_rate': 1.9815476477503862e-06, 'rewards/chosen': -0.7450336813926697, 'rewards/rejected': -3.8164546489715576, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0714211463928223, 'policy_logps/rejected': -428.4377136230469, 'policy_logps/chosen': -517.0392456054688, 'referece_logps/rejected': -390.27313232421875, 'referece_logps/chosen': -509.58892822265625, 'logits/rejected': 0.3887398838996887, 'logits/chosen': 0.548076868057251, 'epoch': 6.77}

 75%|███████▌  | 18161/24156 [10:04:34<30:13:11, 18.15s/it]

 75%|███████▌  | 18162/24156 [10:04:52<30:08:59, 18.11s/it]

 75%|███████▌  | 18163/24156 [10:05:05<27:22:02, 16.44s/it]

 75%|███████▌  | 18164/24156 [10:05:17<25:04:17, 15.06s/it]

 75%|███████▌  | 18165/24156 [10:05:27<22:49:34, 13.72s/it]
[2024-04-06 01:10:44,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18166/24156 [10:05:47<25:59:56, 15.63s/it]

 75%|███████▌  | 18167/24156 [10:06:03<26:08:32, 15.71s/it]
[2024-04-06 01:11:22,130] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18168/24156 [10:06:25<29:00:44, 17.44s/it]
[2024-04-06 01:11:36,548] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18169/24156 [10:06:39<27:29:54, 16.53s/it]


 75%|███████▌  | 18171/24156 [10:07:12<27:59:21, 16.84s/it]
{'loss': 0.439, 'learning_rate': 1.981264560341158e-06, 'rewards/chosen': -2.703150510787964, 'rewards/rejected': -4.9515767097473145, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2484264373779297, 'policy_logps/rejected': -351.8488464355469, 'policy_logps/chosen': -303.302978515625, 'referece_logps/rejected': -302.33306884765625, 'referece_logps/chosen': -276.2715148925781, 'logits/rejected': 0.48638591170310974, 'logits/chosen': 0.6513234376907349, 'epoch': 6.77}

 75%|███████▌  | 18172/24156 [10:07:23<24:54:56, 14.99s/it]


 75%|███████▌  | 18174/24156 [10:08:04<29:45:13, 17.91s/it]

 75%|███████▌  | 18175/24156 [10:08:18<27:52:21, 16.78s/it]

 75%|███████▌  | 18176/24156 [10:08:40<30:20:11, 18.26s/it]

 75%|███████▌  | 18177/24156 [10:09:00<31:16:00, 18.83s/it]
{'loss': 0.376, 'learning_rate': 1.9811092493360188e-06, 'rewards/chosen': -1.548295021057129, 'rewards/rejected': -3.6152660846710205, 'rewards/accuracies': 0.875, 'rewards/margins': 2.06697154045105, 'policy_logps/rejected': -366.93304443359375, 'policy_logps/chosen': -266.7153625488281, 'referece_logps/rejected': -330.78033447265625, 'referece_logps/chosen': -251.23240661621094, 'logits/rejected': -0.8464666604995728, 'logits/chosen': -0.7338865995407104, 'epoch': 6.77}

 75%|███████▌  | 18178/24156 [10:09:20<32:00:58, 19.28s/it]
[2024-04-06 01:14:36,611] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18179/24156 [10:09:39<31:43:43, 19.11s/it]

 75%|███████▌  | 18180/24156 [10:09:55<30:11:18, 18.19s/it]


 75%|███████▌  | 18182/24156 [10:10:32<30:23:05, 18.31s/it]
{'loss': 0.3402, 'learning_rate': 1.9809793384646433e-06, 'rewards/chosen': -1.7346577644348145, 'rewards/rejected': -2.863614559173584, 'rewards/accuracies': 0.875, 'rewards/margins': 1.128956913948059, 'policy_logps/rejected': -348.889892578125, 'policy_logps/chosen': -328.84991455078125, 'referece_logps/rejected': -320.25372314453125, 'referece_logps/chosen': -311.50335693359375, 'logits/rejected': -0.8600894808769226, 'logits/chosen': -0.8542919158935547, 'epoch': 6.77}

 75%|███████▌  | 18183/24156 [10:10:47<28:38:14, 17.26s/it]

 75%|███████▌  | 18184/24156 [10:11:05<28:53:16, 17.41s/it]

 75%|███████▌  | 18185/24156 [10:11:25<29:58:30, 18.07s/it]


 75%|███████▌  | 18187/24156 [10:12:00<30:15:53, 18.25s/it]
[2024-04-06 01:16:57,602] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18188/24156 [10:12:12<27:07:45, 16.36s/it]
{'loss': 0.3341, 'learning_rate': 1.9808228634658243e-06, 'rewards/chosen': -1.6591689586639404, 'rewards/rejected': -3.253659725189209, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5944904088974, 'policy_logps/rejected': -314.90234375, 'policy_logps/chosen': -268.5016174316406, 'referece_logps/rejected': -282.36578369140625, 'referece_logps/chosen': -251.90992736816406, 'logits/rejected': -0.9131357669830322, 'logits/chosen': -0.9828699827194214, 'epoch': 6.78}
[2024-04-06 01:17:24,471] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18189/24156 [10:12:27<26:24:03, 15.93s/it]

 75%|███████▌  | 18190/24156 [10:12:41<25:21:32, 15.30s/it]


 75%|███████▌  | 18192/24156 [10:13:16<26:26:20, 15.96s/it]
{'loss': 0.3885, 'learning_rate': 1.9807181941479306e-06, 'rewards/chosen': -1.1531180143356323, 'rewards/rejected': -3.225144624710083, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0720267295837402, 'policy_logps/rejected': -367.4773864746094, 'policy_logps/chosen': -267.967529296875, 'referece_logps/rejected': -335.2259521484375, 'referece_logps/chosen': -256.43634033203125, 'logits/rejected': -0.5198783278465271, 'logits/chosen': -0.40318363904953003, 'epoch': 6.78}


 75%|███████▌  | 18194/24156 [10:13:56<30:09:32, 18.21s/it]
{'loss': 0.428, 'learning_rate': 1.9806657537046612e-06, 'rewards/chosen': -1.7143628597259521, 'rewards/rejected': -2.6132278442382812, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8988648653030396, 'policy_logps/rejected': -326.172119140625, 'policy_logps/chosen': -452.65167236328125, 'referece_logps/rejected': -300.039794921875, 'referece_logps/chosen': -435.508056640625, 'logits/rejected': -0.46310505270957947, 'logits/chosen': -0.3726809322834015, 'epoch': 6.78}


 75%|███████▌  | 18196/24156 [10:14:36<31:12:34, 18.85s/it]
{'loss': 0.3708, 'learning_rate': 1.9806132427435366e-06, 'rewards/chosen': -1.4468185901641846, 'rewards/rejected': -2.554882764816284, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1080642938613892, 'policy_logps/rejected': -440.4439697265625, 'policy_logps/chosen': -504.9849853515625, 'referece_logps/rejected': -414.8951416015625, 'referece_logps/chosen': -490.5168151855469, 'logits/rejected': -0.2114340364933014, 'logits/chosen': -0.31606411933898926, 'epoch': 6.78}

 75%|███████▌  | 18197/24156 [10:14:56<31:46:24, 19.20s/it]


 75%|███████▌  | 18199/24156 [10:15:29<30:03:36, 18.17s/it]

 75%|███████▌  | 18200/24156 [10:15:45<28:59:03, 17.52s/it]
{'loss': 0.4335, 'learning_rate': 1.9805080092828307e-06, 'rewards/chosen': -1.7701599597930908, 'rewards/rejected': -2.0700879096984863, 'rewards/accuracies': 0.75, 'rewards/margins': 0.29992789030075073, 'policy_logps/rejected': -411.2336120605469, 'policy_logps/chosen': -447.7258605957031, 'referece_logps/rejected': -390.5327453613281, 'referece_logps/chosen': -430.0242919921875, 'logits/rejected': -1.2141104936599731, 'logits/chosen': -1.0950827598571777, 'epoch': 6.78}

 75%|███████▌  | 18201/24156 [10:15:59<27:25:10, 16.58s/it]


 75%|███████▌  | 18203/24156 [10:16:35<28:40:41, 17.34s/it]

 75%|███████▌  | 18204/24156 [10:16:52<28:45:27, 17.39s/it]

 75%|███████▌  | 18205/24156 [10:17:11<29:24:50, 17.79s/it]

 75%|███████▌  | 18206/24156 [10:17:26<28:19:48, 17.14s/it]
{'loss': 0.4259, 'learning_rate': 1.9803496303024206e-06, 'rewards/chosen': -2.033393383026123, 'rewards/rejected': -3.690619707107544, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6572262048721313, 'policy_logps/rejected': -315.0687255859375, 'policy_logps/chosen': -248.37722778320312, 'referece_logps/rejected': -278.1625061035156, 'referece_logps/chosen': -228.04330444335938, 'logits/rejected': -0.27917709946632385, 'logits/chosen': -0.23054713010787964, 'epoch': 6.78}
[2024-04-06 01:22:43,480] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▌  | 18208/24156 [10:17:58<26:47:00, 16.21s/it]
{'loss': 0.2902, 'learning_rate': 1.980296696313637e-06, 'rewards/chosen': -1.4213054180145264, 'rewards/rejected': -4.076717376708984, 'rewards/accuracies': 1.0, 'rewards/margins': 2.655411720275879, 'policy_logps/rejected': -380.28057861328125, 'policy_logps/chosen': -327.3386535644531, 'referece_logps/rejected': -339.5134582519531, 'referece_logps/chosen': -313.1255798339844, 'logits/rejected': -0.46587198972702026, 'logits/chosen': -0.4907759428024292, 'epoch': 6.78}

 75%|███████▌  | 18209/24156 [10:18:16<27:21:13, 16.56s/it]

 75%|███████▌  | 18210/24156 [10:18:37<29:50:41, 18.07s/it]


 75%|███████▌  | 18212/24156 [10:19:16<31:08:31, 18.86s/it]

 75%|███████▌  | 18213/24156 [10:19:34<30:40:39, 18.58s/it]
{'loss': 0.5243, 'learning_rate': 1.9801640529505045e-06, 'rewards/chosen': -1.2834423780441284, 'rewards/rejected': -3.329909324645996, 'rewards/accuracies': 0.875, 'rewards/margins': 2.046466827392578, 'policy_logps/rejected': -319.91729736328125, 'policy_logps/chosen': -396.54608154296875, 'referece_logps/rejected': -286.6181945800781, 'referece_logps/chosen': -383.7116394042969, 'logits/rejected': -0.06633196771144867, 'logits/chosen': -0.27068349719047546, 'epoch': 6.79}


 75%|███████▌  | 18215/24156 [10:20:12<30:58:04, 18.77s/it]
{'loss': 0.3829, 'learning_rate': 1.9801108722587973e-06, 'rewards/chosen': -1.5056681632995605, 'rewards/rejected': -3.1639187335968018, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6582510471343994, 'policy_logps/rejected': -298.317138671875, 'policy_logps/chosen': -310.8733215332031, 'referece_logps/rejected': -266.67791748046875, 'referece_logps/chosen': -295.816650390625, 'logits/rejected': 0.30706292390823364, 'logits/chosen': 0.3731400966644287, 'epoch': 6.79}


 75%|███████▌  | 18217/24156 [10:20:41<27:23:51, 16.61s/it]
{'loss': 0.5261, 'learning_rate': 1.9800576210891356e-06, 'rewards/chosen': -1.5359294414520264, 'rewards/rejected': -3.9207277297973633, 'rewards/accuracies': 1.0, 'rewards/margins': 2.384798049926758, 'policy_logps/rejected': -318.5166931152344, 'policy_logps/chosen': -316.0264587402344, 'referece_logps/rejected': -279.30938720703125, 'referece_logps/chosen': -300.6671142578125, 'logits/rejected': -0.1899167150259018, 'logits/chosen': -0.1561242789030075, 'epoch': 6.79}


 75%|███████▌  | 18219/24156 [10:21:19<29:26:16, 17.85s/it]
{'loss': 0.4439, 'learning_rate': 1.9800042994453476e-06, 'rewards/chosen': -1.6639891862869263, 'rewards/rejected': -3.0990912914276123, 'rewards/accuracies': 0.75, 'rewards/margins': 1.435102105140686, 'policy_logps/rejected': -287.8184814453125, 'policy_logps/chosen': -247.6879119873047, 'referece_logps/rejected': -256.82757568359375, 'referece_logps/chosen': -231.04800415039062, 'logits/rejected': -0.5878751277923584, 'logits/chosen': -0.6003658771514893, 'epoch': 6.79}

 75%|███████▌  | 18220/24156 [10:21:36<29:05:27, 17.64s/it]

 75%|███████▌  | 18221/24156 [10:21:52<28:26:08, 17.25s/it]

 75%|███████▌  | 18222/24156 [10:22:12<29:32:19, 17.92s/it]


 75%|███████▌  | 18224/24156 [10:22:49<30:02:59, 18.24s/it]

 75%|███████▌  | 18225/24156 [10:23:09<30:59:21, 18.81s/it]
[2024-04-06 01:28:06,411] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 18226/24156 [10:23:29<31:43:17, 19.26s/it]
{'loss': 0.2969, 'learning_rate': 1.979817118763755e-06, 'rewards/chosen': -1.7413032054901123, 'rewards/rejected': -4.578812122344971, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8375086784362793, 'policy_logps/rejected': -239.74539184570312, 'policy_logps/chosen': -353.4537353515625, 'referece_logps/rejected': -193.95730590820312, 'referece_logps/chosen': -336.0407409667969, 'logits/rejected': -0.7979355454444885, 'logits/chosen': -0.8818005919456482, 'epoch': 6.79}


 75%|███████▌  | 18228/24156 [10:24:03<29:15:26, 17.77s/it]

 75%|███████▌  | 18229/24156 [10:24:23<30:25:30, 18.48s/it]
[2024-04-06 01:29:20,647] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.468, 'learning_rate': 1.979736634248902e-06, 'rewards/chosen': -1.0083189010620117, 'rewards/rejected': -3.461984157562256, 'rewards/accuracies': 1.0, 'rewards/margins': 2.453665256500244, 'policy_logps/rejected': -426.9996643066406, 'policy_logps/chosen': -396.14483642578125, 'referece_logps/rejected': -392.37982177734375, 'referece_logps/chosen': -386.0616455078125, 'logits/rejected': 0.250896692276001, 'logits/chosen': 0.3492249548435211, 'epoch': 6.79}

 75%|███████▌  | 18230/24156 [10:24:40<29:26:34, 17.89s/it]


 75%|███████▌  | 18232/24156 [10:25:11<26:47:45, 16.28s/it]

 75%|███████▌  | 18233/24156 [10:25:27<26:39:17, 16.20s/it]

 75%|███████▌  | 18234/24156 [10:25:47<28:37:33, 17.40s/it]
[2024-04-06 01:30:44,554] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4143, 'learning_rate': 1.9796021411420386e-06, 'rewards/chosen': -1.8552672863006592, 'rewards/rejected': -4.301125526428223, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4458584785461426, 'policy_logps/rejected': -449.97686767578125, 'policy_logps/chosen': -494.9096984863281, 'referece_logps/rejected': -406.9656066894531, 'referece_logps/chosen': -476.3570251464844, 'logits/rejected': -0.09405013918876648, 'logits/chosen': -0.03548073768615723, 'epoch': 6.79}
[2024-04-06 01:31:07,061] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▌  | 18236/24156 [10:26:31<32:13:57, 19.60s/it]
[2024-04-06 01:31:28,220] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4071, 'learning_rate': 1.979548220623503e-06, 'rewards/chosen': -1.3697948455810547, 'rewards/rejected': -2.068472385406494, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6986774802207947, 'policy_logps/rejected': -412.8065185546875, 'policy_logps/chosen': -480.19915771484375, 'referece_logps/rejected': -392.12176513671875, 'referece_logps/chosen': -466.501220703125, 'logits/rejected': 1.2043136358261108, 'logits/chosen': 1.129359245300293, 'epoch': 6.79}

 75%|███████▌  | 18237/24156 [10:26:41<27:51:47, 16.95s/it]

 76%|███████▌  | 18238/24156 [10:26:52<24:45:33, 15.06s/it]


 76%|███████▌  | 18240/24156 [10:27:23<24:30:43, 14.92s/it]

 76%|███████▌  | 18241/24156 [10:27:39<25:03:58, 15.26s/it]
{'loss': 0.4766, 'learning_rate': 1.9794131111716147e-06, 'rewards/chosen': -1.3516291379928589, 'rewards/rejected': -2.405149459838867, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0535200834274292, 'policy_logps/rejected': -373.4836730957031, 'policy_logps/chosen': -357.08660888671875, 'referece_logps/rejected': -349.4321594238281, 'referece_logps/chosen': -343.5702819824219, 'logits/rejected': -0.16245588660240173, 'logits/chosen': -0.10942476987838745, 'epoch': 6.8}


 76%|███████▌  | 18243/24156 [10:28:09<24:51:11, 15.13s/it]

 76%|███████▌  | 18244/24156 [10:28:25<25:18:17, 15.41s/it]
{'loss': 0.4004, 'learning_rate': 1.9793318342132536e-06, 'rewards/chosen': -1.4341627359390259, 'rewards/rejected': -3.2300407886505127, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7958779335021973, 'policy_logps/rejected': -479.6392517089844, 'policy_logps/chosen': -427.0845031738281, 'referece_logps/rejected': -447.33880615234375, 'referece_logps/chosen': -412.74285888671875, 'logits/rejected': 0.15592975914478302, 'logits/chosen': 0.21151703596115112, 'epoch': 6.8}


 76%|███████▌  | 18246/24156 [10:28:58<26:32:52, 16.17s/it]
{'loss': 0.4882, 'learning_rate': 1.9792775615461144e-06, 'rewards/chosen': -1.5946168899536133, 'rewards/rejected': -3.3623764514923096, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7677596807479858, 'policy_logps/rejected': -451.9053039550781, 'policy_logps/chosen': -494.0282897949219, 'referece_logps/rejected': -418.281494140625, 'referece_logps/chosen': -478.0821228027344, 'logits/rejected': -0.4906655550003052, 'logits/chosen': -0.3698147237300873, 'epoch': 6.8}

 76%|███████▌  | 18247/24156 [10:29:11<25:04:15, 15.27s/it]


 76%|███████▌  | 18249/24156 [10:29:49<28:35:31, 17.43s/it]
[2024-04-06 01:34:46,608] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4006, 'learning_rate': 1.9791960205128153e-06, 'rewards/chosen': -2.0781185626983643, 'rewards/rejected': -3.556581974029541, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4784635305404663, 'policy_logps/rejected': -608.352783203125, 'policy_logps/chosen': -537.5301513671875, 'referece_logps/rejected': -572.7869873046875, 'referece_logps/chosen': -516.7489624023438, 'logits/rejected': -0.1592012345790863, 'logits/chosen': -0.13277050852775574, 'epoch': 6.8}
[2024-04-06 01:35:05,392] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 18250/24156 [10:30:08<29:15:20, 17.83s/it]

 76%|███████▌  | 18251/24156 [10:30:21<26:44:05, 16.30s/it]

 76%|███████▌  | 18252/24156 [10:30:38<27:18:55, 16.66s/it]

 76%|███████▌  | 18253/24156 [10:30:54<26:59:45, 16.46s/it]


 76%|███████▌  | 18255/24156 [10:31:33<29:28:45, 17.98s/it]
[2024-04-06 01:36:30,651] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.314, 'learning_rate': 1.9790324631772943e-06, 'rewards/chosen': -1.9483722448349, 'rewards/rejected': -4.528153896331787, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5797817707061768, 'policy_logps/rejected': -327.7306213378906, 'policy_logps/chosen': -365.6954040527344, 'referece_logps/rejected': -282.4490661621094, 'referece_logps/chosen': -346.21173095703125, 'logits/rejected': -0.7132203578948975, 'logits/chosen': -0.7813093066215515, 'epoch': 6.8}

 76%|███████▌  | 18256/24156 [10:31:52<30:04:30, 18.35s/it]

 76%|███████▌  | 18257/24156 [10:32:10<29:45:23, 18.16s/it]

 76%|███████▌  | 18258/24156 [10:32:32<31:44:13, 19.37s/it]

 76%|███████▌  | 18259/24156 [10:32:47<29:23:01, 17.94s/it]


 76%|███████▌  | 18261/24156 [10:33:29<31:59:31, 19.54s/it]
{'loss': 0.3631, 'learning_rate': 1.9788682722381284e-06, 'rewards/chosen': -2.016303539276123, 'rewards/rejected': -3.5280232429504395, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5117197036743164, 'policy_logps/rejected': -282.9112854003906, 'policy_logps/chosen': -482.456787109375, 'referece_logps/rejected': -247.63104248046875, 'referece_logps/chosen': -462.2937316894531, 'logits/rejected': 0.19394390285015106, 'logits/chosen': 0.22691576182842255, 'epoch': 6.8}

 76%|███████▌  | 18262/24156 [10:33:46<30:46:03, 18.79s/it]

 76%|███████▌  | 18263/24156 [10:34:03<29:53:28, 18.26s/it]

 76%|███████▌  | 18264/24156 [10:34:19<28:46:43, 17.58s/it]

 76%|███████▌  | 18265/24156 [10:34:31<25:55:01, 15.84s/it]

 76%|███████▌  | 18266/24156 [10:34:55<29:52:45, 18.26s/it]

 76%|███████▌  | 18267/24156 [10:35:15<30:38:19, 18.73s/it]

 76%|███████▌  | 18268/24156 [10:35:31<29:21:34, 17.95s/it]

 76%|███████▌  | 18269/24156 [10:35:49<29:38:22, 18.13s/it]
[2024-04-06 01:41:06,423] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 18270/24156 [10:36:09<30:20:34, 18.56s/it]

 76%|███████▌  | 18271/24156 [10:36:25<29:06:23, 17.81s/it]


 76%|███████▌  | 18273/24156 [10:37:08<32:14:15, 19.73s/it]
{'loss': 0.3366, 'learning_rate': 1.9785379899743092e-06, 'rewards/chosen': -2.145927906036377, 'rewards/rejected': -5.163005352020264, 'rewards/accuracies': 1.0, 'rewards/margins': 3.017077922821045, 'policy_logps/rejected': -381.336669921875, 'policy_logps/chosen': -453.2449951171875, 'referece_logps/rejected': -329.7066345214844, 'referece_logps/chosen': -431.78570556640625, 'logits/rejected': -0.8062202334403992, 'logits/chosen': -0.9008238315582275, 'epoch': 6.81}

 76%|███████▌  | 18274/24156 [10:37:28<32:30:14, 19.89s/it]

 76%|███████▌  | 18275/24156 [10:37:42<29:33:16, 18.09s/it]

 76%|███████▌  | 18276/24156 [10:38:03<30:42:29, 18.80s/it]
[2024-04-06 01:43:22,338] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▌  | 18278/24156 [10:38:38<29:08:52, 17.85s/it]
{'loss': 0.2979, 'learning_rate': 1.9783996246886974e-06, 'rewards/chosen': -1.3089135885238647, 'rewards/rejected': -3.4612505435943604, 'rewards/accuracies': 1.0, 'rewards/margins': 2.152336835861206, 'policy_logps/rejected': -347.0401916503906, 'policy_logps/chosen': -437.13409423828125, 'referece_logps/rejected': -312.42767333984375, 'referece_logps/chosen': -424.04498291015625, 'logits/rejected': -0.6745585799217224, 'logits/chosen': -0.719342052936554, 'epoch': 6.81}

 76%|███████▌  | 18279/24156 [10:38:57<29:41:35, 18.19s/it]

 76%|███████▌  | 18280/24156 [10:39:19<31:42:17, 19.42s/it]


 76%|███████▌  | 18282/24156 [10:39:58<31:26:31, 19.27s/it]

 76%|███████▌  | 18283/24156 [10:40:14<29:51:31, 18.30s/it]
{'loss': 0.2644, 'learning_rate': 1.978260819684961e-06, 'rewards/chosen': -1.7366057634353638, 'rewards/rejected': -3.8090717792510986, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0724658966064453, 'policy_logps/rejected': -264.1873474121094, 'policy_logps/chosen': -318.1529541015625, 'referece_logps/rejected': -226.0966339111328, 'referece_logps/chosen': -300.786865234375, 'logits/rejected': -0.3438071608543396, 'logits/chosen': -0.45663905143737793, 'epoch': 6.81}

 76%|███████▌  | 18284/24156 [10:40:33<29:56:38, 18.36s/it]

 76%|███████▌  | 18285/24156 [10:40:47<27:46:53, 17.04s/it]

 76%|███████▌  | 18286/24156 [10:41:03<27:27:57, 16.84s/it]

 76%|███████▌  | 18287/24156 [10:41:18<26:49:07, 16.45s/it]

 76%|███████▌  | 18288/24156 [10:41:33<25:50:12, 15.85s/it]

 76%|███████▌  | 18289/24156 [10:41:50<26:10:51, 16.06s/it]

 76%|███████▌  | 18290/24156 [10:42:09<27:57:58, 17.16s/it]

 76%|███████▌  | 18291/24156 [10:42:29<29:21:20, 18.02s/it]

 76%|███████▌  | 18292/24156 [10:42:50<30:29:42, 18.72s/it]


 76%|███████▌  | 18294/24156 [10:43:20<27:37:11, 16.96s/it]
{'loss': 0.2916, 'learning_rate': 1.977953901176655e-06, 'rewards/chosen': -1.370181918144226, 'rewards/rejected': -2.482468605041504, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1122865676879883, 'policy_logps/rejected': -422.3795166015625, 'policy_logps/chosen': -376.211669921875, 'referece_logps/rejected': -397.5548095703125, 'referece_logps/chosen': -362.5098571777344, 'logits/rejected': -0.42839112877845764, 'logits/chosen': -0.46556058526039124, 'epoch': 6.82}
[2024-04-06 01:48:38,678] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 18295/24156 [10:43:41<29:26:36, 18.09s/it]

 76%|███████▌  | 18296/24156 [10:43:57<28:23:59, 17.45s/it]

 76%|███████▌  | 18297/24156 [10:44:19<30:31:09, 18.75s/it]
[2024-04-06 01:49:37,255] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▌  | 18299/24156 [10:44:59<31:12:24, 19.18s/it]
{'loss': 0.3681, 'learning_rate': 1.977813689495104e-06, 'rewards/chosen': -2.3970353603363037, 'rewards/rejected': -4.033120155334473, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6360845565795898, 'policy_logps/rejected': -501.84698486328125, 'policy_logps/chosen': -435.1343078613281, 'referece_logps/rejected': -461.51580810546875, 'referece_logps/chosen': -411.1639709472656, 'logits/rejected': -0.46158188581466675, 'logits/chosen': -0.45887476205825806, 'epoch': 6.82}

 76%|███████▌  | 18300/24156 [10:45:18<31:16:25, 19.23s/it]

 76%|███████▌  | 18301/24156 [10:45:39<32:09:47, 19.78s/it]

 76%|███████▌  | 18302/24156 [10:45:55<30:16:29, 18.62s/it]

 76%|███████▌  | 18303/24156 [10:46:14<30:23:51, 18.70s/it]

 76%|███████▌  | 18304/24156 [10:46:34<31:05:37, 19.13s/it]

 76%|███████▌  | 18305/24156 [10:46:51<30:04:50, 18.51s/it]


 76%|███████▌  | 18307/24156 [10:47:25<29:02:19, 17.87s/it]
{'loss': 0.4817, 'learning_rate': 1.977588436764946e-06, 'rewards/chosen': -1.1049842834472656, 'rewards/rejected': -3.7355756759643555, 'rewards/accuracies': 0.875, 'rewards/margins': 2.63059139251709, 'policy_logps/rejected': -246.91110229492188, 'policy_logps/chosen': -434.18487548828125, 'referece_logps/rejected': -209.5553436279297, 'referece_logps/chosen': -423.135009765625, 'logits/rejected': -1.1664246320724487, 'logits/chosen': -1.3399087190628052, 'epoch': 6.82}

 76%|███████▌  | 18308/24156 [10:47:45<30:17:20, 18.65s/it]

 76%|███████▌  | 18309/24156 [10:47:56<26:29:16, 16.31s/it]

 76%|███████▌  | 18310/24156 [10:48:15<27:36:10, 17.00s/it]

 76%|███████▌  | 18311/24156 [10:48:26<24:57:54, 15.38s/it]

 76%|███████▌  | 18312/24156 [10:48:48<27:51:52, 17.17s/it]

 76%|███████▌  | 18313/24156 [10:49:02<26:39:13, 16.42s/it]

 76%|███████▌  | 18314/24156 [10:49:19<26:56:57, 16.61s/it]


 76%|███████▌  | 18316/24156 [10:50:01<30:08:04, 18.58s/it]
{'loss': 0.4702, 'learning_rate': 1.977333683034674e-06, 'rewards/chosen': -1.9243264198303223, 'rewards/rejected': -2.699328660964966, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7750023007392883, 'policy_logps/rejected': -497.6163330078125, 'policy_logps/chosen': -569.1555786132812, 'referece_logps/rejected': -470.623046875, 'referece_logps/chosen': -549.912353515625, 'logits/rejected': -1.0975842475891113, 'logits/chosen': -1.0451024770736694, 'epoch': 6.82}

 76%|███████▌  | 18317/24156 [10:50:17<29:02:25, 17.90s/it]

 76%|███████▌  | 18318/24156 [10:50:36<29:22:17, 18.11s/it]

 76%|███████▌  | 18319/24156 [10:50:58<31:23:20, 19.36s/it]

 76%|███████▌  | 18320/24156 [10:51:18<31:39:35, 19.53s/it]
[2024-04-06 01:56:36,612] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 18321/24156 [10:51:39<32:29:51, 20.05s/it]

 76%|███████▌  | 18322/24156 [10:52:01<33:07:30, 20.44s/it]

 76%|███████▌  | 18323/24156 [10:52:20<32:49:58, 20.26s/it]

 76%|███████▌  | 18324/24156 [10:52:32<28:40:31, 17.70s/it]

 76%|███████▌  | 18325/24156 [10:52:52<29:52:29, 18.44s/it]

 76%|███████▌  | 18326/24156 [10:53:11<30:06:58, 18.60s/it]

 76%|███████▌  | 18327/24156 [10:53:28<29:21:38, 18.13s/it]

 76%|███████▌  | 18328/24156 [10:53:48<29:58:59, 18.52s/it]

 76%|███████▌  | 18329/24156 [10:54:08<30:48:43, 19.04s/it]

 76%|███████▌  | 18330/24156 [10:54:25<30:00:20, 18.54s/it]

 76%|███████▌  | 18331/24156 [10:54:44<30:13:36, 18.68s/it]

 76%|███████▌  | 18332/24156 [10:55:02<29:59:08, 18.54s/it]

 76%|███████▌  | 18333/24156 [10:55:15<27:08:37, 16.78s/it]


 76%|███████▌  | 18335/24156 [10:55:57<30:38:57, 18.96s/it]
{'loss': 0.4121, 'learning_rate': 1.9767911965546394e-06, 'rewards/chosen': -1.9477230310440063, 'rewards/rejected': -2.271212100982666, 'rewards/accuracies': 0.375, 'rewards/margins': 0.3234890103340149, 'policy_logps/rejected': -406.18170166015625, 'policy_logps/chosen': -635.5147094726562, 'referece_logps/rejected': -383.4696044921875, 'referece_logps/chosen': -616.0375366210938, 'logits/rejected': -1.3038702011108398, 'logits/chosen': -1.4456813335418701, 'epoch': 6.83}

 76%|███████▌  | 18336/24156 [10:56:15<29:54:44, 18.50s/it]

 76%|███████▌  | 18337/24156 [10:56:29<27:53:35, 17.26s/it]

 76%|███████▌  | 18338/24156 [10:56:43<26:22:35, 16.32s/it]

 76%|███████▌  | 18339/24156 [10:56:57<25:00:38, 15.48s/it]

 76%|███████▌  | 18340/24156 [10:57:11<24:21:11, 15.07s/it]

 76%|███████▌  | 18341/24156 [10:57:22<22:41:45, 14.05s/it]

 76%|███████▌  | 18342/24156 [10:57:42<25:27:59, 15.77s/it]


 76%|███████▌  | 18344/24156 [10:58:17<26:43:40, 16.56s/it]

 76%|███████▌  | 18345/24156 [10:58:38<28:48:10, 17.84s/it]

 76%|███████▌  | 18346/24156 [10:58:55<28:34:54, 17.71s/it]

 76%|███████▌  | 18347/24156 [10:59:12<27:51:40, 17.27s/it]

 76%|███████▌  | 18348/24156 [10:59:23<25:13:11, 15.63s/it]

 76%|███████▌  | 18349/24156 [10:59:37<24:11:18, 15.00s/it]

 76%|███████▌  | 18350/24156 [10:59:58<27:22:05, 16.97s/it]

 76%|███████▌  | 18351/24156 [11:00:13<26:17:34, 16.31s/it]

 76%|███████▌  | 18352/24156 [11:00:24<23:51:09, 14.79s/it]

 76%|███████▌  | 18353/24156 [11:00:44<25:59:22, 16.12s/it]

 76%|███████▌  | 18354/24156 [11:01:04<28:11:51, 17.50s/it]

 76%|███████▌  | 18355/24156 [11:01:18<26:11:56, 16.26s/it]

 76%|███████▌  | 18356/24156 [11:01:28<23:26:35, 14.55s/it]

 76%|███████▌  | 18357/24156 [11:01:40<22:00:16, 13.66s/it]

 76%|███████▌  | 18358/24156 [11:02:00<25:13:41, 15.66s/it]

 76%|███████▌  | 18359/24156 [11:02:18<26:14:23, 16.30s/it]

 76%|███████▌  | 18360/24156 [11:02:40<28:54:48, 17.96s/it]

 76%|███████▌  | 18361/24156 [11:02:55<27:18:37, 16.97s/it]

 76%|███████▌  | 18362/24156 [11:03:08<25:33:50, 15.88s/it]

 76%|███████▌  | 18363/24156 [11:03:31<28:53:54, 17.96s/it]

 76%|███████▌  | 18364/24156 [11:03:50<29:28:58, 18.32s/it]

 76%|███████▌  | 18365/24156 [11:04:06<28:26:34, 17.68s/it]

 76%|███████▌  | 18366/24156 [11:04:29<30:45:12, 19.12s/it]

 76%|███████▌  | 18367/24156 [11:04:49<31:13:16, 19.42s/it]

 76%|███████▌  | 18368/24156 [11:04:59<27:02:03, 16.81s/it]

 76%|███████▌  | 18369/24156 [11:05:19<28:16:49, 17.59s/it]

 76%|███████▌  | 18370/24156 [11:05:39<29:24:57, 18.30s/it]

 76%|███████▌  | 18371/24156 [11:05:57<29:14:11, 18.19s/it]
{'loss': 0.3406, 'learning_rate': 1.975745946143463e-06, 'rewards/chosen': -2.3519527912139893, 'rewards/rejected': -3.8491592407226562, 'rewards/accuracies': 1.0, 'rewards/margins': 1.497206687927246, 'policy_logps/rejected': -408.9855651855469, 'policy_logps/chosen': -504.189208984375, 'referece_logps/rejected': -370.4939880371094, 'referece_logps/chosen': -480.6697082519531, 'logits/rejected': -0.20235028862953186, 'logits/chosen': -0.27601146697998047, 'epoch': 6.84}


 76%|███████▌  | 18373/24156 [11:06:37<30:37:28, 19.06s/it]

 76%|███████▌  | 18374/24156 [11:06:53<29:22:03, 18.28s/it]
{'loss': 0.5294, 'learning_rate': 1.975657815455146e-06, 'rewards/chosen': -2.0186100006103516, 'rewards/rejected': -3.364030361175537, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3454201221466064, 'policy_logps/rejected': -304.09405517578125, 'policy_logps/chosen': -330.7270812988281, 'referece_logps/rejected': -270.4537353515625, 'referece_logps/chosen': -310.5409851074219, 'logits/rejected': -0.25704047083854675, 'logits/chosen': -0.5032919049263, 'epoch': 6.85}


 76%|███████▌  | 18376/24156 [11:07:24<26:29:21, 16.50s/it]

 76%|███████▌  | 18377/24156 [11:07:46<29:15:37, 18.23s/it]
{'loss': 0.3418, 'learning_rate': 1.9755695269119068e-06, 'rewards/chosen': -1.5094774961471558, 'rewards/rejected': -3.0536978244781494, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5442203283309937, 'policy_logps/rejected': -338.42340087890625, 'policy_logps/chosen': -384.3605651855469, 'referece_logps/rejected': -307.88641357421875, 'referece_logps/chosen': -369.2657775878906, 'logits/rejected': -0.2637713551521301, 'logits/chosen': -0.0783303827047348, 'epoch': 6.85}


 76%|███████▌  | 18379/24156 [11:08:23<29:42:48, 18.52s/it]

 76%|███████▌  | 18380/24156 [11:08:39<28:48:32, 17.96s/it]
{'loss': 0.2533, 'learning_rate': 1.97548108052803e-06, 'rewards/chosen': -1.9936726093292236, 'rewards/rejected': -4.2527570724487305, 'rewards/accuracies': 0.875, 'rewards/margins': 2.259084463119507, 'policy_logps/rejected': -371.6048583984375, 'policy_logps/chosen': -348.9886169433594, 'referece_logps/rejected': -329.0773010253906, 'referece_logps/chosen': -329.0518798828125, 'logits/rejected': 0.041018396615982056, 'logits/chosen': 0.026130512356758118, 'epoch': 6.85}


 76%|███████▌  | 18382/24156 [11:09:13<27:22:15, 17.07s/it]

 76%|███████▌  | 18383/24156 [11:09:32<28:28:43, 17.76s/it]

 76%|███████▌  | 18384/24156 [11:09:52<29:21:40, 18.31s/it]

 76%|███████▌  | 18385/24156 [11:10:12<30:11:20, 18.83s/it]

 76%|███████▌  | 18386/24156 [11:10:31<30:33:46, 19.07s/it]

 76%|███████▌  | 18387/24156 [11:10:45<28:02:36, 17.50s/it]

 76%|███████▌  | 18388/24156 [11:11:04<28:30:18, 17.79s/it]

 76%|███████▌  | 18389/24156 [11:11:23<29:27:15, 18.39s/it]

 76%|███████▌  | 18390/24156 [11:11:43<30:10:43, 18.84s/it]

 76%|███████▌  | 18391/24156 [11:12:02<30:16:36, 18.91s/it]

 76%|███████▌  | 18392/24156 [11:12:22<30:48:22, 19.24s/it]

 76%|███████▌  | 18393/24156 [11:12:36<28:07:53, 17.57s/it]

 76%|███████▌  | 18394/24156 [11:12:54<28:26:06, 17.77s/it]
{'loss': 0.3784, 'learning_rate': 1.975066244153772e-06, 'rewards/chosen': -2.3160197734832764, 'rewards/rejected': -3.694227933883667, 'rewards/accuracies': 0.75, 'rewards/margins': 1.378207802772522, 'policy_logps/rejected': -296.0540771484375, 'policy_logps/chosen': -319.3113708496094, 'referece_logps/rejected': -259.11181640625, 'referece_logps/chosen': -296.15118408203125, 'logits/rejected': -0.3392346203327179, 'logits/chosen': -0.2904333174228668, 'epoch': 6.85}


 76%|███████▌  | 18396/24156 [11:13:33<30:25:22, 19.01s/it]

 76%|███████▌  | 18397/24156 [11:13:52<30:07:10, 18.83s/it]

 76%|███████▌  | 18398/24156 [11:14:08<28:39:01, 17.91s/it]

 76%|███████▌  | 18399/24156 [11:14:22<27:04:40, 16.93s/it]

 76%|███████▌  | 18400/24156 [11:14:44<29:07:41, 18.22s/it]

 76%|███████▌  | 18401/24156 [11:15:04<30:05:23, 18.82s/it]

 76%|███████▌  | 18402/24156 [11:15:22<29:37:09, 18.53s/it]

 76%|███████▌  | 18403/24156 [11:15:40<29:28:16, 18.44s/it]

 76%|███████▌  | 18404/24156 [11:15:56<28:15:55, 17.69s/it]

 76%|███████▌  | 18405/24156 [11:16:10<26:36:44, 16.66s/it]
{'loss': 0.3962, 'learning_rate': 1.97473789100514e-06, 'rewards/chosen': -1.7523287534713745, 'rewards/rejected': -2.8781440258026123, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1258156299591064, 'policy_logps/rejected': -437.61151123046875, 'policy_logps/chosen': -382.1116943359375, 'referece_logps/rejected': -408.830078125, 'referece_logps/chosen': -364.58837890625, 'logits/rejected': 0.11933489143848419, 'logits/chosen': -0.02335488051176071, 'epoch': 6.86}


 76%|███████▌  | 18407/24156 [11:16:39<25:05:09, 15.71s/it]
{'loss': 0.4028, 'learning_rate': 1.9746779626140203e-06, 'rewards/chosen': -2.2025279998779297, 'rewards/rejected': -4.0003132820129395, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7977854013442993, 'policy_logps/rejected': -343.15496826171875, 'policy_logps/chosen': -394.58343505859375, 'referece_logps/rejected': -303.15185546875, 'referece_logps/chosen': -372.5581359863281, 'logits/rejected': -1.1300036907196045, 'logits/chosen': -1.0084081888198853, 'epoch': 6.86}


 76%|███████▌  | 18409/24156 [11:17:13<26:05:34, 16.34s/it]

 76%|███████▌  | 18410/24156 [11:17:31<26:56:17, 16.88s/it]

 76%|███████▌  | 18411/24156 [11:17:44<24:53:45, 15.60s/it]
{'loss': 0.3348, 'learning_rate': 1.9745578955742413e-06, 'rewards/chosen': -2.024894952774048, 'rewards/rejected': -4.443459510803223, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4185643196105957, 'policy_logps/rejected': -413.77508544921875, 'policy_logps/chosen': -304.578369140625, 'referece_logps/rejected': -369.34051513671875, 'referece_logps/chosen': -284.32940673828125, 'logits/rejected': -0.7947510480880737, 'logits/chosen': -0.6788790225982666, 'epoch': 6.86}


 76%|███████▌  | 18413/24156 [11:18:19<26:56:40, 16.89s/it]

 76%|███████▌  | 18414/24156 [11:18:32<25:03:00, 15.71s/it]
{'loss': 0.4322, 'learning_rate': 1.9744676613360614e-06, 'rewards/chosen': -2.1172025203704834, 'rewards/rejected': -3.3398165702819824, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2226142883300781, 'policy_logps/rejected': -319.6104736328125, 'policy_logps/chosen': -408.756591796875, 'referece_logps/rejected': -286.2123107910156, 'referece_logps/chosen': -387.5845642089844, 'logits/rejected': -0.4565606713294983, 'logits/chosen': -0.4033626914024353, 'epoch': 6.86}

 76%|███████▌  | 18415/24156 [11:18:53<27:25:26, 17.20s/it]

 76%|███████▌  | 18416/24156 [11:19:05<25:04:15, 15.72s/it]


 76%|███████▌  | 18418/24156 [11:19:36<25:12:15, 15.81s/it]

 76%|███████▋  | 18419/24156 [11:19:55<26:23:38, 16.56s/it]

 76%|███████▋  | 18420/24156 [11:20:10<25:38:39, 16.09s/it]

 76%|███████▋  | 18421/24156 [11:20:26<25:48:31, 16.20s/it]

 76%|███████▋  | 18422/24156 [11:20:46<27:30:42, 17.27s/it]

 76%|███████▋  | 18423/24156 [11:21:07<29:35:09, 18.58s/it]

 76%|███████▋  | 18424/24156 [11:21:23<27:57:49, 17.56s/it]

 76%|███████▋  | 18425/24156 [11:21:38<26:53:35, 16.89s/it]
{'loss': 0.3218, 'learning_rate': 1.9741354536849282e-06, 'rewards/chosen': -2.0345962047576904, 'rewards/rejected': -3.31246018409729, 'rewards/accuracies': 0.75, 'rewards/margins': 1.277863621711731, 'policy_logps/rejected': -343.565673828125, 'policy_logps/chosen': -503.7390441894531, 'referece_logps/rejected': -310.4410400390625, 'referece_logps/chosen': -483.39306640625, 'logits/rejected': 0.19449186325073242, 'logits/chosen': 0.16293595731258392, 'epoch': 6.86}

 76%|███████▋  | 18426/24156 [11:21:55<27:00:44, 16.97s/it]


 76%|███████▋  | 18428/24156 [11:22:30<27:49:13, 17.48s/it]

 76%|███████▋  | 18429/24156 [11:22:43<25:33:49, 16.07s/it]

 76%|███████▋  | 18430/24156 [11:23:02<27:12:23, 17.10s/it]
{'loss': 0.4257, 'learning_rate': 1.973983749680924e-06, 'rewards/chosen': -1.353938341140747, 'rewards/rejected': -3.2101995944976807, 'rewards/accuracies': 0.875, 'rewards/margins': 1.856261134147644, 'policy_logps/rejected': -303.48822021484375, 'policy_logps/chosen': -433.2908630371094, 'referece_logps/rejected': -271.3861999511719, 'referece_logps/chosen': -419.75152587890625, 'logits/rejected': -0.2283032238483429, 'logits/chosen': -0.250640869140625, 'epoch': 6.87}


 76%|███████▋  | 18432/24156 [11:23:28<23:27:46, 14.76s/it]

 76%|███████▋  | 18433/24156 [11:23:48<25:52:54, 16.28s/it]
{'loss': 0.4331, 'learning_rate': 1.973892517162062e-06, 'rewards/chosen': -2.2046029567718506, 'rewards/rejected': -4.095252990722656, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8906502723693848, 'policy_logps/rejected': -460.38677978515625, 'policy_logps/chosen': -414.630126953125, 'referece_logps/rejected': -419.4342956542969, 'referece_logps/chosen': -392.5841369628906, 'logits/rejected': -0.6228336691856384, 'logits/chosen': -0.5809287428855896, 'epoch': 6.87}


 76%|███████▋  | 18435/24156 [11:24:21<26:12:57, 16.50s/it]

 76%|███████▋  | 18436/24156 [11:24:43<28:45:01, 18.09s/it]

 76%|███████▋  | 18437/24156 [11:25:04<30:21:18, 19.11s/it]

 76%|███████▋  | 18438/24156 [11:25:18<28:04:55, 17.68s/it]

 76%|███████▋  | 18439/24156 [11:25:38<29:10:31, 18.37s/it]

 76%|███████▋  | 18440/24156 [11:26:00<30:54:28, 19.47s/it]
{'loss': 0.546, 'learning_rate': 1.9736790285407437e-06, 'rewards/chosen': -2.197369337081909, 'rewards/rejected': -3.76084566116333, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5634760856628418, 'policy_logps/rejected': -544.12158203125, 'policy_logps/chosen': -435.86651611328125, 'referece_logps/rejected': -506.51312255859375, 'referece_logps/chosen': -413.892822265625, 'logits/rejected': -0.7698570489883423, 'logits/chosen': -0.7837738394737244, 'epoch': 6.87}


 76%|███████▋  | 18442/24156 [11:26:36<29:28:03, 18.57s/it]

 76%|███████▋  | 18443/24156 [11:26:55<29:27:02, 18.56s/it]

 76%|███████▋  | 18444/24156 [11:27:14<29:39:48, 18.70s/it]
{'loss': 0.4719, 'learning_rate': 1.97355664994579e-06, 'rewards/chosen': -1.4422351121902466, 'rewards/rejected': -3.835664749145508, 'rewards/accuracies': 1.0, 'rewards/margins': 2.393429756164551, 'policy_logps/rejected': -304.2033996582031, 'policy_logps/chosen': -353.22015380859375, 'referece_logps/rejected': -265.84674072265625, 'referece_logps/chosen': -338.79779052734375, 'logits/rejected': 0.287960946559906, 'logits/chosen': 0.2447974979877472, 'epoch': 6.87}


 76%|███████▋  | 18446/24156 [11:27:55<31:09:57, 19.65s/it]

 76%|███████▋  | 18447/24156 [11:28:06<27:02:40, 17.05s/it]
{'loss': 0.4346, 'learning_rate': 1.9734646822301857e-06, 'rewards/chosen': -1.9111738204956055, 'rewards/rejected': -3.134758949279785, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2235852479934692, 'policy_logps/rejected': -402.6004943847656, 'policy_logps/chosen': -324.1200256347656, 'referece_logps/rejected': -371.25286865234375, 'referece_logps/chosen': -305.0082702636719, 'logits/rejected': -1.3005884885787964, 'logits/chosen': -1.1539806127548218, 'epoch': 6.87}


 76%|███████▋  | 18449/24156 [11:28:28<22:05:17, 13.93s/it]

 76%|███████▋  | 18450/24156 [11:28:43<22:18:18, 14.07s/it]
{'loss': 0.468, 'learning_rate': 1.973372557014494e-06, 'rewards/chosen': -1.2336105108261108, 'rewards/rejected': -2.071063995361328, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8374534249305725, 'policy_logps/rejected': -399.86334228515625, 'policy_logps/chosen': -373.32763671875, 'referece_logps/rejected': -379.1526794433594, 'referece_logps/chosen': -360.9915466308594, 'logits/rejected': -0.8852816820144653, 'logits/chosen': -0.7986551523208618, 'epoch': 6.87}


 76%|███████▋  | 18452/24156 [11:29:19<26:01:48, 16.43s/it]

 76%|███████▋  | 18453/24156 [11:29:33<24:54:06, 15.72s/it]
{'loss': 0.321, 'learning_rate': 1.973280274313619e-06, 'rewards/chosen': -1.5011775493621826, 'rewards/rejected': -3.279013156890869, 'rewards/accuracies': 0.75, 'rewards/margins': 1.777835488319397, 'policy_logps/rejected': -420.14556884765625, 'policy_logps/chosen': -429.70953369140625, 'referece_logps/rejected': -387.35546875, 'referece_logps/chosen': -414.6977233886719, 'logits/rejected': -1.0192861557006836, 'logits/chosen': -0.8514831066131592, 'epoch': 6.88}

 76%|███████▋  | 18454/24156 [11:29:52<26:16:41, 16.59s/it]


 76%|███████▋  | 18456/24156 [11:30:28<27:29:08, 17.36s/it]
{'loss': 0.3658, 'learning_rate': 1.9731878341424927e-06, 'rewards/chosen': -1.4999099969863892, 'rewards/rejected': -4.014040946960449, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5141305923461914, 'policy_logps/rejected': -468.6172790527344, 'policy_logps/chosen': -432.40277099609375, 'referece_logps/rejected': -428.4768981933594, 'referece_logps/chosen': -417.4037170410156, 'logits/rejected': -0.3643619120121002, 'logits/chosen': -0.3610183894634247, 'epoch': 6.88}


 76%|███████▋  | 18458/24156 [11:30:55<24:17:59, 15.35s/it]

 76%|███████▋  | 18459/24156 [11:31:12<25:12:51, 15.93s/it]

 76%|███████▋  | 18460/24156 [11:31:32<26:56:44, 17.03s/it]

 76%|███████▋  | 18461/24156 [11:31:47<26:09:43, 16.54s/it]
{'loss': 0.3145, 'learning_rate': 1.9730334172973547e-06, 'rewards/chosen': -1.2694435119628906, 'rewards/rejected': -3.732715368270874, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4632718563079834, 'policy_logps/rejected': -419.76031494140625, 'policy_logps/chosen': -317.6014404296875, 'referece_logps/rejected': -382.43316650390625, 'referece_logps/chosen': -304.90704345703125, 'logits/rejected': -0.6448463797569275, 'logits/chosen': -0.7453805804252625, 'epoch': 6.88}

 76%|███████▋  | 18462/24156 [11:32:06<27:03:18, 17.11s/it]

 76%|███████▋  | 18463/24156 [11:32:24<27:20:39, 17.29s/it]
[2024-04-06 02:37:41,455] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▋  | 18464/24156 [11:32:44<28:48:41, 18.22s/it]
[2024-04-06 02:38:03,118] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▋  | 18466/24156 [11:33:20<28:20:15, 17.93s/it]

 76%|███████▋  | 18467/24156 [11:33:39<28:34:36, 18.08s/it]

 76%|███████▋  | 18468/24156 [11:33:59<29:39:32, 18.77s/it]

 76%|███████▋  | 18469/24156 [11:34:16<28:51:08, 18.26s/it]

 76%|███████▋  | 18470/24156 [11:34:27<25:21:29, 16.06s/it]

 76%|███████▋  | 18471/24156 [11:34:39<23:13:26, 14.71s/it]

 76%|███████▋  | 18472/24156 [11:34:49<21:18:02, 13.49s/it]

 76%|███████▋  | 18473/24156 [11:35:01<20:30:04, 12.99s/it]
{'loss': 0.506, 'learning_rate': 1.97266103279141e-06, 'rewards/chosen': -1.199205756187439, 'rewards/rejected': -2.7805349826812744, 'rewards/accuracies': 1.0, 'rewards/margins': 1.581329107284546, 'policy_logps/rejected': -520.7302856445312, 'policy_logps/chosen': -466.76239013671875, 'referece_logps/rejected': -492.9249267578125, 'referece_logps/chosen': -454.7702941894531, 'logits/rejected': -0.6095113158226013, 'logits/chosen': -0.5634573698043823, 'epoch': 6.88}

 76%|███████▋  | 18474/24156 [11:35:14<20:15:55, 12.84s/it]


 76%|███████▋  | 18476/24156 [11:35:43<21:31:52, 13.65s/it]
{'loss': 0.5483, 'learning_rate': 1.9725675432020475e-06, 'rewards/chosen': -1.8210445642471313, 'rewards/rejected': -3.0818824768066406, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2608380317687988, 'policy_logps/rejected': -361.9904479980469, 'policy_logps/chosen': -326.49346923828125, 'referece_logps/rejected': -331.1716003417969, 'referece_logps/chosen': -308.28302001953125, 'logits/rejected': -0.578294038772583, 'logits/chosen': -0.5420660972595215, 'epoch': 6.88}


 76%|███████▋  | 18478/24156 [11:36:09<21:02:34, 13.34s/it]

 76%|███████▋  | 18479/24156 [11:36:21<20:37:05, 13.07s/it]

 77%|███████▋  | 18480/24156 [11:36:33<20:10:33, 12.80s/it]
{'loss': 0.4411, 'learning_rate': 1.972442645644502e-06, 'rewards/chosen': -1.1609909534454346, 'rewards/rejected': -1.8980779647827148, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7370868921279907, 'policy_logps/rejected': -367.4823913574219, 'policy_logps/chosen': -390.9632873535156, 'referece_logps/rejected': -348.5015563964844, 'referece_logps/chosen': -379.3533630371094, 'logits/rejected': 0.5039167404174805, 'logits/chosen': 0.40749305486679077, 'epoch': 6.89}

 77%|███████▋  | 18481/24156 [11:36:49<21:24:39, 13.58s/it]


 77%|███████▋  | 18483/24156 [11:37:17<21:36:03, 13.71s/it]

 77%|███████▋  | 18484/24156 [11:37:29<20:41:27, 13.13s/it]
{'loss': 0.5216, 'learning_rate': 1.9723174683807743e-06, 'rewards/chosen': -2.5024595260620117, 'rewards/rejected': -2.701483964920044, 'rewards/accuracies': 0.5, 'rewards/margins': 0.19902455806732178, 'policy_logps/rejected': -422.135009765625, 'policy_logps/chosen': -308.7578125, 'referece_logps/rejected': -395.12017822265625, 'referece_logps/chosen': -283.73321533203125, 'logits/rejected': -0.37717753648757935, 'logits/chosen': -0.44077327847480774, 'epoch': 6.89}


 77%|███████▋  | 18486/24156 [11:37:58<21:10:58, 13.45s/it]
{'loss': 0.42, 'learning_rate': 1.9722547748703405e-06, 'rewards/chosen': -1.8874706029891968, 'rewards/rejected': -1.9259185791015625, 'rewards/accuracies': 0.375, 'rewards/margins': 0.03844797611236572, 'policy_logps/rejected': -365.7625427246094, 'policy_logps/chosen': -507.5933837890625, 'referece_logps/rejected': -346.5033264160156, 'referece_logps/chosen': -488.71868896484375, 'logits/rejected': -0.5363279581069946, 'logits/chosen': -0.6899033188819885, 'epoch': 6.89}

 77%|███████▋  | 18487/24156 [11:38:10<20:47:00, 13.20s/it]


 77%|███████▋  | 18489/24156 [11:38:46<24:16:56, 15.43s/it]
{'loss': 0.3224, 'learning_rate': 1.9721606035191552e-06, 'rewards/chosen': -2.2380166053771973, 'rewards/rejected': -2.899198293685913, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6611817479133606, 'policy_logps/rejected': -270.984130859375, 'policy_logps/chosen': -287.39080810546875, 'referece_logps/rejected': -241.9921417236328, 'referece_logps/chosen': -265.0106506347656, 'logits/rejected': -0.24125905334949493, 'logits/chosen': -0.17363271117210388, 'epoch': 6.89}

 77%|███████▋  | 18490/24156 [11:39:06<26:53:03, 17.08s/it]


 77%|███████▋  | 18492/24156 [11:39:43<27:45:53, 17.65s/it]

 77%|███████▋  | 18493/24156 [11:39:57<25:58:52, 16.52s/it]
{'loss': 0.274, 'learning_rate': 1.9720347970482833e-06, 'rewards/chosen': -2.439812660217285, 'rewards/rejected': -4.065017223358154, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6252045631408691, 'policy_logps/rejected': -344.15948486328125, 'policy_logps/chosen': -389.5028076171875, 'referece_logps/rejected': -303.5093078613281, 'referece_logps/chosen': -365.1047058105469, 'logits/rejected': -1.1298433542251587, 'logits/chosen': -1.2697339057922363, 'epoch': 6.89}


 77%|███████▋  | 18495/24156 [11:40:26<24:20:47, 15.48s/it]

 77%|███████▋  | 18496/24156 [11:40:42<24:26:10, 15.54s/it]

 77%|███████▋  | 18497/24156 [11:41:01<26:17:07, 16.72s/it]
{'loss': 0.2805, 'learning_rate': 1.971908710988539e-06, 'rewards/chosen': -1.5159912109375, 'rewards/rejected': -2.8536007404327393, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3376095294952393, 'policy_logps/rejected': -392.094482421875, 'policy_logps/chosen': -390.3782653808594, 'referece_logps/rejected': -363.5584716796875, 'referece_logps/chosen': -375.21832275390625, 'logits/rejected': -1.3874359130859375, 'logits/chosen': -1.4017038345336914, 'epoch': 6.89}


 77%|███████▋  | 18499/24156 [11:41:29<24:11:07, 15.39s/it]
{'loss': 0.3137, 'learning_rate': 1.9718455631241705e-06, 'rewards/chosen': -1.8617126941680908, 'rewards/rejected': -2.3774187564849854, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5157061219215393, 'policy_logps/rejected': -255.17828369140625, 'policy_logps/chosen': -280.15423583984375, 'referece_logps/rejected': -231.40411376953125, 'referece_logps/chosen': -261.537109375, 'logits/rejected': -0.6181406378746033, 'logits/chosen': -0.6727495789527893, 'epoch': 6.89}
[2024-04-06 02:46:47,854] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 77%|███████▋  | 18501/24156 [11:42:24<34:27:35, 21.94s/it]
[2024-04-06 02:47:21,006] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18502/24156 [11:42:41<32:25:54, 20.65s/it]

 77%|███████▋  | 18503/24156 [11:43:03<33:12:02, 21.14s/it]
[2024-04-06 02:48:00,946] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18504/24156 [11:43:21<31:38:14, 20.15s/it]

 77%|███████▋  | 18505/24156 [11:43:33<27:45:55, 17.69s/it]
{'loss': 0.3775, 'learning_rate': 1.971655700247582e-06, 'rewards/chosen': -1.1891885995864868, 'rewards/rejected': -3.3819501399993896, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1927614212036133, 'policy_logps/rejected': -305.0143127441406, 'policy_logps/chosen': -436.8272399902344, 'referece_logps/rejected': -271.19482421875, 'referece_logps/chosen': -424.9353332519531, 'logits/rejected': -0.5015345215797424, 'logits/chosen': -0.5996930003166199, 'epoch': 6.89}

 77%|███████▋  | 18506/24156 [11:43:55<29:35:47, 18.86s/it]


 77%|███████▋  | 18508/24156 [11:44:32<29:02:59, 18.52s/it]
{'loss': 0.4244, 'learning_rate': 1.971560532990492e-06, 'rewards/chosen': -1.7617824077606201, 'rewards/rejected': -5.479722499847412, 'rewards/accuracies': 1.0, 'rewards/margins': 3.71794056892395, 'policy_logps/rejected': -321.13134765625, 'policy_logps/chosen': -367.9604797363281, 'referece_logps/rejected': -266.3341369628906, 'referece_logps/chosen': -350.3426513671875, 'logits/rejected': -0.7944626808166504, 'logits/chosen': -0.9543452262878418, 'epoch': 6.9}

 77%|███████▋  | 18509/24156 [11:44:53<30:21:51, 19.36s/it]


 77%|███████▋  | 18511/24156 [11:45:22<26:09:20, 16.68s/it]
{'loss': 0.4143, 'learning_rate': 1.9714652085413922e-06, 'rewards/chosen': -2.2357590198516846, 'rewards/rejected': -3.4278194904327393, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1920604705810547, 'policy_logps/rejected': -520.4424438476562, 'policy_logps/chosen': -337.4541015625, 'referece_logps/rejected': -486.1642761230469, 'referece_logps/chosen': -315.0965576171875, 'logits/rejected': 0.5373637676239014, 'logits/chosen': 0.8226039409637451, 'epoch': 6.9}


 77%|███████▋  | 18513/24156 [11:45:54<25:59:07, 16.58s/it]

 77%|███████▋  | 18514/24156 [11:46:14<27:34:52, 17.60s/it]

 77%|███████▋  | 18515/24156 [11:46:32<27:57:02, 17.84s/it]

 77%|███████▋  | 18516/24156 [11:46:52<28:42:57, 18.33s/it]

 77%|███████▋  | 18517/24156 [11:47:08<27:39:45, 17.66s/it]

 77%|███████▋  | 18518/24156 [11:47:22<26:01:18, 16.62s/it]
{'loss': 0.377, 'learning_rate': 1.97124217361125e-06, 'rewards/chosen': -1.8259599208831787, 'rewards/rejected': -4.453738212585449, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6277785301208496, 'policy_logps/rejected': -452.22857666015625, 'policy_logps/chosen': -359.2486267089844, 'referece_logps/rejected': -407.6911926269531, 'referece_logps/chosen': -340.989013671875, 'logits/rejected': -0.5245598554611206, 'logits/chosen': -0.3839033842086792, 'epoch': 6.9}


 77%|███████▋  | 18520/24156 [11:47:54<25:40:48, 16.40s/it]

 77%|███████▋  | 18521/24156 [11:48:08<24:19:03, 15.54s/it]
{'loss': 0.4549, 'learning_rate': 1.9711463253003167e-06, 'rewards/chosen': -1.1681740283966064, 'rewards/rejected': -2.271540403366089, 'rewards/accuracies': 1.0, 'rewards/margins': 1.103366494178772, 'policy_logps/rejected': -486.5423583984375, 'policy_logps/chosen': -510.88525390625, 'referece_logps/rejected': -463.82696533203125, 'referece_logps/chosen': -499.2035827636719, 'logits/rejected': 0.3441324234008789, 'logits/chosen': 0.20517267286777496, 'epoch': 6.9}

 77%|███████▋  | 18522/24156 [11:48:21<23:08:43, 14.79s/it]


 77%|███████▋  | 18524/24156 [11:48:48<21:43:50, 13.89s/it]
{'loss': 0.4312, 'learning_rate': 1.9710503198643894e-06, 'rewards/chosen': -1.662247657775879, 'rewards/rejected': -2.902186632156372, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2399389743804932, 'policy_logps/rejected': -565.9505615234375, 'policy_logps/chosen': -295.46136474609375, 'referece_logps/rejected': -536.9286499023438, 'referece_logps/chosen': -278.8388671875, 'logits/rejected': -1.4972460269927979, 'logits/chosen': -1.2633707523345947, 'epoch': 6.9}

 77%|███████▋  | 18525/24156 [11:48:59<20:32:59, 13.14s/it]

 77%|███████▋  | 18526/24156 [11:49:17<22:42:41, 14.52s/it]

 77%|███████▋  | 18527/24156 [11:49:29<21:20:05, 13.64s/it]


 77%|███████▋  | 18529/24156 [11:49:50<18:50:33, 12.05s/it]

 77%|███████▋  | 18530/24156 [11:50:01<18:12:16, 11.65s/it]

 77%|███████▋  | 18531/24156 [11:50:22<22:45:50, 14.57s/it]
[2024-04-06 02:55:19,412] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18532/24156 [11:50:36<22:40:54, 14.52s/it]
[2024-04-06 02:55:33,816] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3969, 'learning_rate': 1.9707935373201585e-06, 'rewards/chosen': -1.6235260963439941, 'rewards/rejected': -1.7523589134216309, 'rewards/accuracies': 0.5, 'rewards/margins': 0.12883299589157104, 'policy_logps/rejected': -555.0575561523438, 'policy_logps/chosen': -490.36199951171875, 'referece_logps/rejected': -537.533935546875, 'referece_logps/chosen': -474.126708984375, 'logits/rejected': -0.3998069167137146, 'logits/chosen': -0.3207339942455292, 'epoch': 6.9}


 77%|███████▋  | 18534/24156 [11:51:03<21:48:35, 13.97s/it]

 77%|███████▋  | 18535/24156 [11:51:16<21:32:08, 13.79s/it]
{'loss': 0.4768, 'learning_rate': 1.9706969558923434e-06, 'rewards/chosen': -1.6456308364868164, 'rewards/rejected': -4.0595879554748535, 'rewards/accuracies': 0.75, 'rewards/margins': 2.413957118988037, 'policy_logps/rejected': -492.3570556640625, 'policy_logps/chosen': -436.94561767578125, 'referece_logps/rejected': -451.7611999511719, 'referece_logps/chosen': -420.4892883300781, 'logits/rejected': -0.555603563785553, 'logits/chosen': -0.6434929966926575, 'epoch': 6.91}


 77%|███████▋  | 18537/24156 [11:51:56<27:07:25, 17.38s/it]
{'loss': 0.4618, 'learning_rate': 1.970632481021563e-06, 'rewards/chosen': -1.8576149940490723, 'rewards/rejected': -3.074673652648926, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2170586585998535, 'policy_logps/rejected': -260.1810607910156, 'policy_logps/chosen': -366.91796875, 'referece_logps/rejected': -229.43429565429688, 'referece_logps/chosen': -348.341796875, 'logits/rejected': -0.5880455374717712, 'logits/chosen': -0.6497238874435425, 'epoch': 6.91}

 77%|███████▋  | 18538/24156 [11:52:16<27:55:58, 17.90s/it]
[2024-04-06 02:57:30,857] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18539/24156 [11:52:33<27:54:27, 17.89s/it]


 77%|███████▋  | 18541/24156 [11:53:11<28:25:29, 18.22s/it]
{'loss': 0.3497, 'learning_rate': 1.9705033218954994e-06, 'rewards/chosen': -2.4828696250915527, 'rewards/rejected': -3.4705400466918945, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9876704216003418, 'policy_logps/rejected': -377.16278076171875, 'policy_logps/chosen': -345.7161560058594, 'referece_logps/rejected': -342.4573669433594, 'referece_logps/chosen': -320.887451171875, 'logits/rejected': -0.9323986172676086, 'logits/chosen': -0.8656587600708008, 'epoch': 6.91}


 77%|███████▋  | 18543/24156 [11:53:44<27:17:14, 17.50s/it]
{'loss': 0.3656, 'learning_rate': 1.970438637649504e-06, 'rewards/chosen': -1.2654305696487427, 'rewards/rejected': -3.387589931488037, 'rewards/accuracies': 0.875, 'rewards/margins': 2.122159242630005, 'policy_logps/rejected': -210.96682739257812, 'policy_logps/chosen': -299.53509521484375, 'referece_logps/rejected': -177.09092712402344, 'referece_logps/chosen': -286.88079833984375, 'logits/rejected': -0.14865601062774658, 'logits/chosen': -0.060674965381622314, 'epoch': 6.91}

 77%|███████▋  | 18544/24156 [11:54:06<29:06:16, 18.67s/it]


 77%|███████▋  | 18546/24156 [11:54:46<30:19:25, 19.46s/it]
[2024-04-06 02:59:43,576] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.399, 'learning_rate': 1.9703414804398868e-06, 'rewards/chosen': -1.8974578380584717, 'rewards/rejected': -3.497215986251831, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5997581481933594, 'policy_logps/rejected': -445.6573486328125, 'policy_logps/chosen': -449.03277587890625, 'referece_logps/rejected': -410.68511962890625, 'referece_logps/chosen': -430.05816650390625, 'logits/rejected': -0.5523032546043396, 'logits/chosen': -0.5607649087905884, 'epoch': 6.91}


 77%|███████▋  | 18548/24156 [11:55:20<27:43:44, 17.80s/it]

 77%|███████▋  | 18549/24156 [11:55:33<25:09:36, 16.15s/it]
{'loss': 0.4111, 'learning_rate': 1.9702441662354943e-06, 'rewards/chosen': -1.6692835092544556, 'rewards/rejected': -3.58836030960083, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9190768003463745, 'policy_logps/rejected': -307.2015380859375, 'policy_logps/chosen': -332.07183837890625, 'referece_logps/rejected': -271.31793212890625, 'referece_logps/chosen': -315.37896728515625, 'logits/rejected': -0.2721034288406372, 'logits/chosen': -0.2920260727405548, 'epoch': 6.91}


 77%|███████▋  | 18551/24156 [11:55:54<20:54:27, 13.43s/it]
{'loss': 0.361, 'learning_rate': 1.9701792028876875e-06, 'rewards/chosen': -1.460313081741333, 'rewards/rejected': -4.432981967926025, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9726686477661133, 'policy_logps/rejected': -321.27117919921875, 'policy_logps/chosen': -388.3553161621094, 'referece_logps/rejected': -276.94140625, 'referece_logps/chosen': -373.752197265625, 'logits/rejected': -0.7035340666770935, 'logits/chosen': -0.7026057839393616, 'epoch': 6.91}


 77%|███████▋  | 18553/24156 [11:56:31<24:36:14, 15.81s/it]

 77%|███████▋  | 18554/24156 [11:56:43<22:55:24, 14.73s/it]
{'loss': 0.4195, 'learning_rate': 1.9700816270603383e-06, 'rewards/chosen': -1.6494202613830566, 'rewards/rejected': -4.906520366668701, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2571001052856445, 'policy_logps/rejected': -444.07489013671875, 'policy_logps/chosen': -454.8467712402344, 'referece_logps/rejected': -395.00970458984375, 'referece_logps/chosen': -438.35260009765625, 'logits/rejected': 0.18475471436977386, 'logits/chosen': 0.36790627241134644, 'epoch': 6.91}

 77%|███████▋  | 18555/24156 [11:56:58<22:53:33, 14.71s/it]

 77%|███████▋  | 18556/24156 [11:57:17<25:13:11, 16.21s/it]


 77%|███████▋  | 18558/24156 [11:57:47<24:47:55, 15.95s/it]
{'loss': 0.3482, 'learning_rate': 1.9699512818112437e-06, 'rewards/chosen': -1.5659481287002563, 'rewards/rejected': -3.205897808074951, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6399495601654053, 'policy_logps/rejected': -339.8384704589844, 'policy_logps/chosen': -256.2259826660156, 'referece_logps/rejected': -307.779541015625, 'referece_logps/chosen': -240.5664825439453, 'logits/rejected': -0.8151039481163025, 'logits/chosen': -0.8204789161682129, 'epoch': 6.91}


 77%|███████▋  | 18560/24156 [11:58:12<21:51:07, 14.06s/it]

 77%|███████▋  | 18561/24156 [11:58:31<24:03:56, 15.48s/it]
{'loss': 0.4433, 'learning_rate': 1.969853339785454e-06, 'rewards/chosen': -1.1035171747207642, 'rewards/rejected': -2.452084541320801, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3485671281814575, 'policy_logps/rejected': -313.6021728515625, 'policy_logps/chosen': -281.50579833984375, 'referece_logps/rejected': -289.08135986328125, 'referece_logps/chosen': -270.47064208984375, 'logits/rejected': -0.17574718594551086, 'logits/chosen': -0.19471526145935059, 'epoch': 6.92}


 77%|███████▋  | 18563/24156 [11:59:02<24:07:00, 15.52s/it]
{'loss': 0.407, 'learning_rate': 1.9697879579251726e-06, 'rewards/chosen': -2.086294412612915, 'rewards/rejected': -4.291801452636719, 'rewards/accuracies': 0.875, 'rewards/margins': 2.205507278442383, 'policy_logps/rejected': -381.89251708984375, 'policy_logps/chosen': -365.7102966308594, 'referece_logps/rejected': -338.9744873046875, 'referece_logps/chosen': -344.84735107421875, 'logits/rejected': -0.7916046380996704, 'logits/chosen': -0.7341802716255188, 'epoch': 6.92}
[2024-04-06 03:04:21,746] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18564/24156 [11:59:24<27:01:56, 17.40s/it]


 77%|███████▋  | 18566/24156 [11:59:53<24:09:45, 15.56s/it]
{'loss': 0.4228, 'learning_rate': 1.969689754381871e-06, 'rewards/chosen': -1.2976396083831787, 'rewards/rejected': -3.0489742755889893, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7513344287872314, 'policy_logps/rejected': -301.4618835449219, 'policy_logps/chosen': -402.234375, 'referece_logps/rejected': -270.9721374511719, 'referece_logps/chosen': -389.25799560546875, 'logits/rejected': -0.6259623169898987, 'logits/chosen': -0.5385916829109192, 'epoch': 6.92}


 77%|███████▋  | 18568/24156 [12:00:15<20:23:48, 13.14s/it]
{'loss': 0.437, 'learning_rate': 1.969624198191283e-06, 'rewards/chosen': -2.160229206085205, 'rewards/rejected': -2.9365735054016113, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7763442397117615, 'policy_logps/rejected': -381.7734680175781, 'policy_logps/chosen': -416.1305236816406, 'referece_logps/rejected': -352.4077453613281, 'referece_logps/chosen': -394.52825927734375, 'logits/rejected': -0.5607825517654419, 'logits/chosen': -0.6242055296897888, 'epoch': 6.92}


 77%|███████▋  | 18570/24156 [12:00:37<18:57:15, 12.22s/it]

 77%|███████▋  | 18571/24156 [12:00:57<22:26:57, 14.47s/it]
{'loss': 0.4333, 'learning_rate': 1.9695257331746044e-06, 'rewards/chosen': -1.5519944429397583, 'rewards/rejected': -3.2023470401763916, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6503527164459229, 'policy_logps/rejected': -298.46453857421875, 'policy_logps/chosen': -355.57073974609375, 'referece_logps/rejected': -266.44110107421875, 'referece_logps/chosen': -340.05078125, 'logits/rejected': 0.5622315406799316, 'logits/chosen': 0.5372574329376221, 'epoch': 6.92}


 77%|███████▋  | 18573/24156 [12:01:25<22:19:13, 14.39s/it]
{'loss': 0.3796, 'learning_rate': 1.9694600026831722e-06, 'rewards/chosen': -2.0226941108703613, 'rewards/rejected': -2.8531036376953125, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8304097652435303, 'policy_logps/rejected': -436.37005615234375, 'policy_logps/chosen': -256.8949279785156, 'referece_logps/rejected': -407.8389892578125, 'referece_logps/chosen': -236.6680145263672, 'logits/rejected': -0.8075057864189148, 'logits/chosen': -0.7976831793785095, 'epoch': 6.92}

 77%|███████▋  | 18574/24156 [12:01:42<23:29:11, 15.15s/it]


 77%|███████▋  | 18576/24156 [12:02:16<24:31:48, 15.83s/it]

 77%|███████▋  | 18577/24156 [12:02:35<25:37:19, 16.53s/it]

 77%|███████▋  | 18578/24156 [12:02:51<25:30:04, 16.46s/it]
{'loss': 0.4034, 'learning_rate': 1.9692953714746343e-06, 'rewards/chosen': -1.2210636138916016, 'rewards/rejected': -2.420171022415161, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1991074085235596, 'policy_logps/rejected': -393.7886657714844, 'policy_logps/chosen': -564.7356567382812, 'referece_logps/rejected': -369.58697509765625, 'referece_logps/chosen': -552.5250244140625, 'logits/rejected': -0.7850330471992493, 'logits/chosen': -0.7993927597999573, 'epoch': 6.92}

 77%|███████▋  | 18579/24156 [12:03:12<27:27:17, 17.72s/it]

 77%|███████▋  | 18580/24156 [12:03:32<28:38:23, 18.49s/it]

 77%|███████▋  | 18581/24156 [12:03:50<28:28:11, 18.38s/it]

 77%|███████▋  | 18582/24156 [12:04:06<27:18:53, 17.64s/it]

 77%|███████▋  | 18583/24156 [12:04:20<25:37:30, 16.55s/it]


 77%|███████▋  | 18585/24156 [12:04:53<25:05:19, 16.21s/it]
{'loss': 0.3434, 'learning_rate': 1.9690641559468774e-06, 'rewards/chosen': -1.2027199268341064, 'rewards/rejected': -3.177762508392334, 'rewards/accuracies': 0.625, 'rewards/margins': 1.975042462348938, 'policy_logps/rejected': -386.0989685058594, 'policy_logps/chosen': -449.40228271484375, 'referece_logps/rejected': -354.32135009765625, 'referece_logps/chosen': -437.37506103515625, 'logits/rejected': -0.3575500249862671, 'logits/chosen': -0.4082554578781128, 'epoch': 6.92}


 77%|███████▋  | 18587/24156 [12:05:33<28:05:08, 18.16s/it]
{'loss': 0.3782, 'learning_rate': 1.9689979375704896e-06, 'rewards/chosen': -1.968092918395996, 'rewards/rejected': -4.078492164611816, 'rewards/accuracies': 0.875, 'rewards/margins': 2.110399007797241, 'policy_logps/rejected': -373.53204345703125, 'policy_logps/chosen': -369.5776062011719, 'referece_logps/rejected': -332.7471008300781, 'referece_logps/chosen': -349.8966979980469, 'logits/rejected': -0.3459177613258362, 'logits/chosen': -0.44460901618003845, 'epoch': 6.93}

 77%|███████▋  | 18588/24156 [12:05:46<26:02:55, 16.84s/it]

 77%|███████▋  | 18589/24156 [12:05:58<23:47:32, 15.39s/it]

 77%|███████▋  | 18590/24156 [12:06:18<25:43:36, 16.64s/it]

 77%|███████▋  | 18591/24156 [12:06:30<23:31:52, 15.22s/it]

 77%|███████▋  | 18592/24156 [12:06:48<24:57:30, 16.15s/it]


 77%|███████▋  | 18594/24156 [12:07:16<22:46:53, 14.75s/it]

 77%|███████▋  | 18595/24156 [12:07:31<23:01:44, 14.91s/it]

 77%|███████▋  | 18596/24156 [12:07:51<25:18:37, 16.39s/it]
{'loss': 0.4058, 'learning_rate': 1.9686990926699107e-06, 'rewards/chosen': -1.8249297142028809, 'rewards/rejected': -3.5819664001464844, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7570369243621826, 'policy_logps/rejected': -343.2344055175781, 'policy_logps/chosen': -362.6422119140625, 'referece_logps/rejected': -307.41473388671875, 'referece_logps/chosen': -344.3929138183594, 'logits/rejected': -0.914750874042511, 'logits/chosen': -0.8085582256317139, 'epoch': 6.93}


 77%|███████▋  | 18598/24156 [12:08:31<28:11:59, 18.27s/it]
{'loss': 0.346, 'learning_rate': 1.968632491118939e-06, 'rewards/chosen': -1.080373764038086, 'rewards/rejected': -3.582913398742676, 'rewards/accuracies': 1.0, 'rewards/margins': 2.502540111541748, 'policy_logps/rejected': -366.6401672363281, 'policy_logps/chosen': -690.8184814453125, 'referece_logps/rejected': -330.81097412109375, 'referece_logps/chosen': -680.0147705078125, 'logits/rejected': 0.004013001918792725, 'logits/chosen': 0.014090120792388916, 'epoch': 6.93}


 77%|███████▋  | 18600/24156 [12:09:02<26:16:23, 17.02s/it]

 77%|███████▋  | 18601/24156 [12:09:13<23:46:59, 15.41s/it]
{'loss': 0.4612, 'learning_rate': 1.968532458195419e-06, 'rewards/chosen': -1.640012502670288, 'rewards/rejected': -3.51461124420166, 'rewards/accuracies': 0.5, 'rewards/margins': 1.8745988607406616, 'policy_logps/rejected': -335.8101806640625, 'policy_logps/chosen': -296.47320556640625, 'referece_logps/rejected': -300.6640319824219, 'referece_logps/chosen': -280.07305908203125, 'logits/rejected': 0.017934486269950867, 'logits/chosen': 0.18664313852787018, 'epoch': 6.93}

 77%|███████▋  | 18602/24156 [12:09:26<22:39:11, 14.68s/it]


 77%|███████▋  | 18604/24156 [12:10:00<24:38:33, 15.98s/it]
{'loss': 0.5601, 'learning_rate': 1.968432268569812e-06, 'rewards/chosen': -2.0479888916015625, 'rewards/rejected': -3.008685827255249, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9606969356536865, 'policy_logps/rejected': -437.588134765625, 'policy_logps/chosen': -531.304443359375, 'referece_logps/rejected': -407.50128173828125, 'referece_logps/chosen': -510.82452392578125, 'logits/rejected': 0.8279541730880737, 'logits/chosen': 0.749043345451355, 'epoch': 6.93}

 77%|███████▋  | 18605/24156 [12:10:19<26:18:23, 17.06s/it]

 77%|███████▋  | 18606/24156 [12:10:39<27:34:31, 17.89s/it]


 77%|███████▋  | 18608/24156 [12:11:18<28:20:31, 18.39s/it]
{'loss': 0.4076, 'learning_rate': 1.9682984386715547e-06, 'rewards/chosen': -1.9958341121673584, 'rewards/rejected': -3.273467540740967, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2776331901550293, 'policy_logps/rejected': -400.89849853515625, 'policy_logps/chosen': -376.368408203125, 'referece_logps/rejected': -368.163818359375, 'referece_logps/chosen': -356.41009521484375, 'logits/rejected': -0.8749186992645264, 'logits/chosen': -0.792171061038971, 'epoch': 6.93}

 77%|███████▋  | 18609/24156 [12:11:35<27:55:20, 18.12s/it]

 77%|███████▋  | 18610/24156 [12:11:47<25:01:10, 16.24s/it]

 77%|███████▋  | 18611/24156 [12:12:08<27:17:52, 17.72s/it]
[2024-04-06 03:17:28,725] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 77%|███████▋  | 18613/24156 [12:12:44<26:35:31, 17.27s/it]
{'loss': 0.3795, 'learning_rate': 1.9681307596426953e-06, 'rewards/chosen': -1.9644107818603516, 'rewards/rejected': -3.225770950317383, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2613601684570312, 'policy_logps/rejected': -440.2667541503906, 'policy_logps/chosen': -394.3972473144531, 'referece_logps/rejected': -408.009033203125, 'referece_logps/chosen': -374.753173828125, 'logits/rejected': 0.5360063314437866, 'logits/chosen': 0.473968505859375, 'epoch': 6.93}

 77%|███████▋  | 18614/24156 [12:12:54<23:33:17, 15.30s/it]

 77%|███████▋  | 18615/24156 [12:13:16<26:36:03, 17.28s/it]

 77%|███████▋  | 18616/24156 [12:13:37<28:12:45, 18.33s/it]

 77%|███████▋  | 18617/24156 [12:13:50<25:47:34, 16.76s/it]

 77%|███████▋  | 18618/24156 [12:14:01<23:00:10, 14.95s/it]


 77%|███████▋  | 18620/24156 [12:14:36<25:11:03, 16.38s/it]
{'loss': 0.3831, 'learning_rate': 1.967895278046118e-06, 'rewards/chosen': -0.8807647824287415, 'rewards/rejected': -3.563993453979492, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6832287311553955, 'policy_logps/rejected': -441.1810302734375, 'policy_logps/chosen': -405.35992431640625, 'referece_logps/rejected': -405.5411376953125, 'referece_logps/chosen': -396.5522766113281, 'logits/rejected': -0.03929653763771057, 'logits/chosen': -0.24552127718925476, 'epoch': 6.94}


 77%|███████▋  | 18622/24156 [12:15:06<23:49:09, 15.50s/it]
{'loss': 0.3888, 'learning_rate': 1.9678278409818766e-06, 'rewards/chosen': -1.6379435062408447, 'rewards/rejected': -1.558235764503479, 'rewards/accuracies': 0.5, 'rewards/margins': -0.07970774173736572, 'policy_logps/rejected': -434.13067626953125, 'policy_logps/chosen': -469.9042663574219, 'referece_logps/rejected': -418.54833984375, 'referece_logps/chosen': -453.5248107910156, 'logits/rejected': 0.9815112352371216, 'logits/chosen': 0.8917920589447021, 'epoch': 6.94}

 77%|███████▋  | 18623/24156 [12:15:23<24:13:50, 15.77s/it]

 77%|███████▋  | 18624/24156 [12:15:38<23:49:37, 15.51s/it]


 77%|███████▋  | 18626/24156 [12:16:08<23:11:50, 15.10s/it]
{'loss': 0.4949, 'learning_rate': 1.967692758074134e-06, 'rewards/chosen': -1.972643494606018, 'rewards/rejected': -4.709271430969238, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7366275787353516, 'policy_logps/rejected': -454.1268310546875, 'policy_logps/chosen': -382.83953857421875, 'referece_logps/rejected': -407.0340881347656, 'referece_logps/chosen': -363.1130676269531, 'logits/rejected': -1.147026538848877, 'logits/chosen': -1.0723063945770264, 'epoch': 6.94}


 77%|███████▋  | 18628/24156 [12:16:44<25:22:09, 16.52s/it]

 77%|███████▋  | 18629/24156 [12:17:04<26:50:02, 17.48s/it]
{'loss': 0.4342, 'learning_rate': 1.9675912632306007e-06, 'rewards/chosen': -0.981035590171814, 'rewards/rejected': -3.1511361598968506, 'rewards/accuracies': 1.0, 'rewards/margins': 2.170100450515747, 'policy_logps/rejected': -471.79681396484375, 'policy_logps/chosen': -551.10986328125, 'referece_logps/rejected': -440.2854919433594, 'referece_logps/chosen': -541.2994995117188, 'logits/rejected': 0.2906535267829895, 'logits/chosen': 0.25213637948036194, 'epoch': 6.94}

 77%|███████▋  | 18630/24156 [12:17:16<24:14:50, 15.80s/it]

 77%|███████▋  | 18631/24156 [12:17:27<21:53:14, 14.26s/it]

 77%|███████▋  | 18632/24156 [12:17:39<21:14:34, 13.84s/it]

 77%|███████▋  | 18633/24156 [12:17:53<21:00:02, 13.69s/it]


 77%|███████▋  | 18635/24156 [12:18:18<20:36:03, 13.43s/it]
{'loss': 0.4028, 'learning_rate': 1.967387803910556e-06, 'rewards/chosen': -1.7689969539642334, 'rewards/rejected': -4.312481880187988, 'rewards/accuracies': 1.0, 'rewards/margins': 2.543484687805176, 'policy_logps/rejected': -435.0394287109375, 'policy_logps/chosen': -286.50469970703125, 'referece_logps/rejected': -391.9146728515625, 'referece_logps/chosen': -268.814697265625, 'logits/rejected': -0.5410026907920837, 'logits/chosen': -0.4620610177516937, 'epoch': 6.94}


 77%|███████▋  | 18637/24156 [12:18:42<19:39:06, 12.82s/it]
{'loss': 0.4299, 'learning_rate': 1.9673198450045758e-06, 'rewards/chosen': -2.073814630508423, 'rewards/rejected': -4.433618545532227, 'rewards/accuracies': 0.625, 'rewards/margins': 2.3598036766052246, 'policy_logps/rejected': -536.7396240234375, 'policy_logps/chosen': -399.1814880371094, 'referece_logps/rejected': -492.4034118652344, 'referece_logps/chosen': -378.4433288574219, 'logits/rejected': -0.7956299781799316, 'logits/chosen': -0.6878353357315063, 'epoch': 6.94}

 77%|███████▋  | 18638/24156 [12:18:59<21:38:56, 14.12s/it]

 77%|███████▋  | 18639/24156 [12:19:19<24:13:58, 15.81s/it]

 77%|███████▋  | 18640/24156 [12:19:31<22:19:07, 14.57s/it]

 77%|███████▋  | 18641/24156 [12:19:50<24:15:03, 15.83s/it]


 77%|███████▋  | 18643/24156 [12:20:22<24:49:14, 16.21s/it]
{'loss': 0.2847, 'learning_rate': 1.9671155509571515e-06, 'rewards/chosen': -2.4417989253997803, 'rewards/rejected': -3.7721002101898193, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3303015232086182, 'policy_logps/rejected': -303.03936767578125, 'policy_logps/chosen': -471.1812744140625, 'referece_logps/rejected': -265.318359375, 'referece_logps/chosen': -446.7632751464844, 'logits/rejected': -0.1992490142583847, 'logits/chosen': -0.3725099265575409, 'epoch': 6.95}

 77%|███████▋  | 18644/24156 [12:20:42<26:21:41, 17.22s/it]

 77%|███████▋  | 18645/24156 [12:21:01<27:02:35, 17.67s/it]

 77%|███████▋  | 18646/24156 [12:21:20<27:48:49, 18.17s/it]


 77%|███████▋  | 18648/24156 [12:22:03<30:04:06, 19.65s/it]
{'loss': 0.3203, 'learning_rate': 1.9669448278005692e-06, 'rewards/chosen': -1.5722447633743286, 'rewards/rejected': -2.8379716873168945, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2657268047332764, 'policy_logps/rejected': -300.5585632324219, 'policy_logps/chosen': -291.8733215332031, 'referece_logps/rejected': -272.1788635253906, 'referece_logps/chosen': -276.15087890625, 'logits/rejected': -0.876632809638977, 'logits/chosen': -0.8630953431129456, 'epoch': 6.95}

 77%|███████▋  | 18649/24156 [12:22:24<30:53:23, 20.19s/it]


 77%|███████▋  | 18651/24156 [12:22:57<27:25:59, 17.94s/it]
{'loss': 0.3487, 'learning_rate': 1.9668421853080807e-06, 'rewards/chosen': -1.2030739784240723, 'rewards/rejected': -3.64070463180542, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4376304149627686, 'policy_logps/rejected': -521.5480346679688, 'policy_logps/chosen': -412.320556640625, 'referece_logps/rejected': -485.1409912109375, 'referece_logps/chosen': -400.28985595703125, 'logits/rejected': -0.5404378175735474, 'logits/chosen': -0.5819272398948669, 'epoch': 6.95}


 77%|███████▋  | 18653/24156 [12:23:30<26:16:10, 17.19s/it]
{'loss': 0.4346, 'learning_rate': 1.9667736700739447e-06, 'rewards/chosen': -1.407721757888794, 'rewards/rejected': -2.6024601459503174, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1947382688522339, 'policy_logps/rejected': -328.1026916503906, 'policy_logps/chosen': -388.7885437011719, 'referece_logps/rejected': -302.0780944824219, 'referece_logps/chosen': -374.7113037109375, 'logits/rejected': -0.8717713356018066, 'logits/chosen': -0.8255781531333923, 'epoch': 6.95}

 77%|███████▋  | 18654/24156 [12:23:51<27:45:43, 18.16s/it]

 77%|███████▋  | 18655/24156 [12:24:08<27:26:58, 17.96s/it]

 77%|███████▋  | 18656/24156 [12:24:30<28:56:19, 18.94s/it]


 77%|███████▋  | 18658/24156 [12:25:05<28:35:05, 18.72s/it]
{'loss': 0.2713, 'learning_rate': 1.9666020778542e-06, 'rewards/chosen': -1.5936554670333862, 'rewards/rejected': -3.8204667568206787, 'rewards/accuracies': 1.0, 'rewards/margins': 2.226811408996582, 'policy_logps/rejected': -341.3478088378906, 'policy_logps/chosen': -326.50421142578125, 'referece_logps/rejected': -303.14312744140625, 'referece_logps/chosen': -310.5676574707031, 'logits/rejected': -0.8439074754714966, 'logits/chosen': -0.9519321918487549, 'epoch': 6.95}
[2024-04-06 03:30:24,608] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18659/24156 [12:25:27<30:21:31, 19.88s/it]
[2024-04-06 03:30:42,907] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18660/24156 [12:25:45<29:37:41, 19.41s/it]


 77%|███████▋  | 18662/24156 [12:26:19<28:01:16, 18.36s/it]
[2024-04-06 03:31:16,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18663/24156 [12:26:39<28:54:58, 18.95s/it]

 77%|███████▋  | 18664/24156 [12:26:59<29:22:12, 19.25s/it]
[2024-04-06 03:31:56,395] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3624, 'learning_rate': 1.9663955937681847e-06, 'rewards/chosen': -1.851638674736023, 'rewards/rejected': -3.423666000366211, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5720272064208984, 'policy_logps/rejected': -473.4530334472656, 'policy_logps/chosen': -505.3031311035156, 'referece_logps/rejected': -439.21636962890625, 'referece_logps/chosen': -486.78668212890625, 'logits/rejected': -0.06529714167118073, 'logits/chosen': 0.04669215530157089, 'epoch': 6.95}

 77%|███████▋  | 18665/24156 [12:27:20<30:06:10, 19.74s/it]


 77%|███████▋  | 18667/24156 [12:27:55<28:59:50, 19.02s/it]
{'loss': 0.389, 'learning_rate': 1.966292117182284e-06, 'rewards/chosen': -1.775092363357544, 'rewards/rejected': -4.076952934265137, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3018603324890137, 'policy_logps/rejected': -320.7447509765625, 'policy_logps/chosen': -529.799072265625, 'referece_logps/rejected': -279.9752197265625, 'referece_logps/chosen': -512.048095703125, 'logits/rejected': -0.6932210922241211, 'logits/chosen': -0.7916659116744995, 'epoch': 6.95}

 77%|███████▋  | 18668/24156 [12:28:14<29:07:19, 19.10s/it]


 77%|███████▋  | 18670/24156 [12:28:45<25:30:50, 16.74s/it]
{'loss': 0.3826, 'learning_rate': 1.9661884842567684e-06, 'rewards/chosen': -2.338575839996338, 'rewards/rejected': -3.6583054065704346, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3197295665740967, 'policy_logps/rejected': -337.4030456542969, 'policy_logps/chosen': -362.9075927734375, 'referece_logps/rejected': -300.82000732421875, 'referece_logps/chosen': -339.5218505859375, 'logits/rejected': -0.861718475818634, 'logits/chosen': -0.927069365978241, 'epoch': 6.96}

 77%|███████▋  | 18671/24156 [12:28:59<24:28:05, 16.06s/it]

 77%|███████▋  | 18672/24156 [12:29:17<25:00:20, 16.42s/it]

 77%|███████▋  | 18673/24156 [12:29:35<26:06:02, 17.14s/it]

 77%|███████▋  | 18674/24156 [12:29:52<25:54:50, 17.02s/it]

 77%|███████▋  | 18675/24156 [12:30:06<24:36:18, 16.16s/it]

 77%|███████▋  | 18676/24156 [12:30:19<23:11:43, 15.24s/it]

 77%|███████▋  | 18677/24156 [12:30:42<26:34:51, 17.47s/it]
[2024-04-06 03:36:02,271] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18678/24156 [12:31:05<28:58:02, 19.04s/it]

 77%|███████▋  | 18679/24156 [12:31:18<26:12:33, 17.23s/it]
[2024-04-06 03:36:36,973] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18680/24156 [12:31:39<28:14:38, 18.57s/it]
[2024-04-06 03:36:56,946] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18681/24156 [12:31:59<28:52:47, 18.99s/it]

 77%|███████▋  | 18682/24156 [12:32:16<27:49:00, 18.29s/it]

 77%|███████▋  | 18683/24156 [12:32:34<27:25:24, 18.04s/it]

 77%|███████▋  | 18684/24156 [12:32:44<24:04:03, 15.83s/it]


 77%|███████▋  | 18686/24156 [12:33:25<27:39:20, 18.20s/it]
{'loss': 0.4718, 'learning_rate': 1.965633135611645e-06, 'rewards/chosen': -1.5338448286056519, 'rewards/rejected': -3.2184085845947266, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6845641136169434, 'policy_logps/rejected': -419.4241638183594, 'policy_logps/chosen': -359.0529479980469, 'referece_logps/rejected': -387.24005126953125, 'referece_logps/chosen': -343.7145080566406, 'logits/rejected': -0.39856284856796265, 'logits/chosen': -0.34823912382125854, 'epoch': 6.96}

 77%|███████▋  | 18687/24156 [12:33:40<26:16:44, 17.30s/it]


 77%|███████▋  | 18689/24156 [12:34:09<24:29:41, 16.13s/it]
{'loss': 0.3404, 'learning_rate': 1.965528512925649e-06, 'rewards/chosen': -1.7107303142547607, 'rewards/rejected': -3.2394025325775146, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5286720991134644, 'policy_logps/rejected': -278.3966369628906, 'policy_logps/chosen': -260.9453430175781, 'referece_logps/rejected': -246.00262451171875, 'referece_logps/chosen': -243.83802795410156, 'logits/rejected': -0.6839525699615479, 'logits/chosen': -0.6590191721916199, 'epoch': 6.96}

 77%|███████▋  | 18690/24156 [12:34:28<25:29:18, 16.79s/it]

 77%|███████▋  | 18691/24156 [12:34:47<26:30:29, 17.46s/it]

 77%|███████▋  | 18692/24156 [12:35:08<28:13:54, 18.60s/it]

 77%|███████▋  | 18693/24156 [12:35:20<25:20:42, 16.70s/it]


 77%|███████▋  | 18695/24156 [12:35:53<25:54:28, 17.08s/it]
{'loss': 0.4024, 'learning_rate': 1.965318798922404e-06, 'rewards/chosen': -1.9224499464035034, 'rewards/rejected': -4.208003044128418, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2855536937713623, 'policy_logps/rejected': -423.68670654296875, 'policy_logps/chosen': -418.31292724609375, 'referece_logps/rejected': -381.6066589355469, 'referece_logps/chosen': -399.08837890625, 'logits/rejected': -0.7552425861358643, 'logits/chosen': -0.7185496687889099, 'epoch': 6.97}

 77%|███████▋  | 18696/24156 [12:36:09<25:19:59, 16.70s/it]

 77%|███████▋  | 18697/24156 [12:36:21<23:04:52, 15.22s/it]
[2024-04-06 03:41:40,586] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18698/24156 [12:36:43<26:15:35, 17.32s/it]

 77%|███████▋  | 18699/24156 [12:37:00<26:03:54, 17.20s/it]

 77%|███████▋  | 18700/24156 [12:37:17<25:57:44, 17.13s/it]


 77%|███████▋  | 18702/24156 [12:37:58<28:30:10, 18.81s/it]

 77%|███████▋  | 18703/24156 [12:38:15<27:54:02, 18.42s/it]
{'loss': 0.3012, 'learning_rate': 1.965038208474994e-06, 'rewards/chosen': -2.0131640434265137, 'rewards/rejected': -5.266606330871582, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2534422874450684, 'policy_logps/rejected': -396.32391357421875, 'policy_logps/chosen': -364.12591552734375, 'referece_logps/rejected': -343.6578369140625, 'referece_logps/chosen': -343.9942626953125, 'logits/rejected': -1.1818656921386719, 'logits/chosen': -1.3090139627456665, 'epoch': 6.97}

 77%|███████▋  | 18704/24156 [12:38:28<25:21:24, 16.74s/it]

 77%|███████▋  | 18705/24156 [12:38:41<23:35:18, 15.58s/it]

 77%|███████▋  | 18706/24156 [12:38:55<23:04:21, 15.24s/it]

 77%|███████▋  | 18707/24156 [12:39:06<21:06:06, 13.94s/it]

 77%|███████▋  | 18708/24156 [12:39:17<19:45:50, 13.06s/it]

 77%|███████▋  | 18709/24156 [12:39:32<20:43:04, 13.69s/it]


 77%|███████▋  | 18711/24156 [12:40:00<20:16:34, 13.41s/it]

 77%|███████▋  | 18712/24156 [12:40:14<20:36:16, 13.63s/it]
{'loss': 0.487, 'learning_rate': 1.9647212170755988e-06, 'rewards/chosen': -1.685436487197876, 'rewards/rejected': -2.0031967163085938, 'rewards/accuracies': 0.5, 'rewards/margins': 0.31776049733161926, 'policy_logps/rejected': -420.35919189453125, 'policy_logps/chosen': -437.33636474609375, 'referece_logps/rejected': -400.3272399902344, 'referece_logps/chosen': -420.48199462890625, 'logits/rejected': -0.11685668677091599, 'logits/chosen': -0.08922995626926422, 'epoch': 6.97}

 77%|███████▋  | 18713/24156 [12:40:35<23:55:22, 15.82s/it]

 77%|███████▋  | 18714/24156 [12:40:54<25:29:06, 16.86s/it]
[2024-04-06 03:46:13,760] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 18715/24156 [12:41:16<27:57:01, 18.49s/it]

 77%|███████▋  | 18716/24156 [12:41:28<24:55:35, 16.50s/it]

 77%|███████▋  | 18717/24156 [12:41:44<24:29:35, 16.21s/it]

 77%|███████▋  | 18718/24156 [12:42:05<26:39:18, 17.65s/it]

 77%|███████▋  | 18719/24156 [12:42:18<24:49:55, 16.44s/it]

 77%|███████▋  | 18720/24156 [12:42:35<24:50:30, 16.45s/it]

 78%|███████▊  | 18721/24156 [12:42:46<22:34:11, 14.95s/it]
[2024-04-06 03:47:58,772] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 18722/24156 [12:43:01<22:37:34, 14.99s/it]

 78%|███████▊  | 18723/24156 [12:43:21<24:51:13, 16.47s/it]

 78%|███████▊  | 18724/24156 [12:43:36<24:06:37, 15.98s/it]

 78%|███████▊  | 18725/24156 [12:43:48<22:13:40, 14.73s/it]

 78%|███████▊  | 18726/24156 [12:44:00<20:56:56, 13.89s/it]

 78%|███████▊  | 18727/24156 [12:44:21<24:17:13, 16.10s/it]

 78%|███████▊  | 18728/24156 [12:44:37<24:11:01, 16.04s/it]

 78%|███████▊  | 18729/24156 [12:44:49<22:13:43, 14.75s/it]

 78%|███████▊  | 18730/24156 [12:45:03<21:54:31, 14.54s/it]

 78%|███████▊  | 18731/24156 [12:45:17<21:42:25, 14.40s/it]

 78%|███████▊  | 18732/24156 [12:45:28<20:15:10, 13.44s/it]

 78%|███████▊  | 18733/24156 [12:45:48<23:12:08, 15.40s/it]

 78%|███████▊  | 18734/24156 [12:46:04<23:21:31, 15.51s/it]

 78%|███████▊  | 18735/24156 [12:46:21<23:55:26, 15.89s/it]

 78%|███████▊  | 18736/24156 [12:46:36<23:39:36, 15.72s/it]

 78%|███████▊  | 18737/24156 [12:46:52<23:38:37, 15.71s/it]

 78%|███████▊  | 18738/24156 [12:47:02<21:23:03, 14.21s/it]

 78%|███████▊  | 18739/24156 [12:47:19<22:28:43, 14.94s/it]

 78%|███████▊  | 18740/24156 [12:47:39<24:44:21, 16.44s/it]

 78%|███████▊  | 18741/24156 [12:47:59<26:20:24, 17.51s/it]

 78%|███████▊  | 18742/24156 [12:48:22<29:04:34, 19.33s/it]

 78%|███████▊  | 18743/24156 [12:48:36<26:28:41, 17.61s/it]

 78%|███████▊  | 18744/24156 [12:48:49<24:31:12, 16.31s/it]

 78%|███████▊  | 18745/24156 [12:49:10<26:40:37, 17.75s/it]

 78%|███████▊  | 18746/24156 [12:49:31<28:07:09, 18.71s/it]


 78%|███████▊  | 18748/24156 [12:50:05<26:28:41, 17.63s/it]

 78%|███████▊  | 18749/24156 [12:50:17<23:59:37, 15.98s/it]

 78%|███████▊  | 18750/24156 [12:50:32<23:29:17, 15.64s/it]

 78%|███████▊  | 18751/24156 [12:50:42<21:18:09, 14.19s/it]

 78%|███████▊  | 18752/24156 [12:51:02<23:46:02, 15.83s/it]

 78%|███████▊  | 18753/24156 [12:51:22<25:26:13, 16.95s/it]

 78%|███████▊  | 18754/24156 [12:51:41<26:25:28, 17.61s/it]

 78%|███████▊  | 18755/24156 [12:51:56<25:30:38, 17.00s/it]

 78%|███████▊  | 18756/24156 [12:52:18<27:26:57, 18.30s/it]

 78%|███████▊  | 18757/24156 [12:52:33<26:08:01, 17.43s/it]

 78%|███████▊  | 18758/24156 [12:52:53<27:10:54, 18.13s/it]

 78%|███████▊  | 18759/24156 [12:53:12<27:50:40, 18.57s/it]

 78%|███████▊  | 18760/24156 [12:53:24<24:37:14, 16.43s/it]

 78%|███████▊  | 18761/24156 [12:53:38<23:41:27, 15.81s/it]

 78%|███████▊  | 18762/24156 [12:53:56<24:38:50, 16.45s/it]
[2024-04-06 03:58:53,710] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 18763/24156 [12:54:18<27:04:03, 18.07s/it]
[2024-04-06 03:59:15,555] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 18764/24156 [12:54:36<26:54:32, 17.97s/it]

 78%|███████▊  | 18765/24156 [12:54:48<24:10:08, 16.14s/it]

 78%|███████▊  | 18766/24156 [12:55:02<23:29:30, 15.69s/it]

 78%|███████▊  | 18767/24156 [12:55:15<22:12:53, 14.84s/it]

 78%|███████▊  | 18768/24156 [12:55:36<24:43:43, 16.52s/it]
{'loss': 0.3881, 'learning_rate': 1.9627172804204513e-06, 'rewards/chosen': -2.8406426906585693, 'rewards/rejected': -3.556795597076416, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7161529064178467, 'policy_logps/rejected': -409.0847473144531, 'policy_logps/chosen': -536.4463500976562, 'referece_logps/rejected': -373.5168151855469, 'referece_logps/chosen': -508.0399475097656, 'logits/rejected': -0.5852975845336914, 'logits/chosen': -0.6334980130195618, 'epoch': 6.99}


 78%|███████▊  | 18770/24156 [12:56:05<23:03:49, 15.42s/it]
{'loss': 0.3945, 'learning_rate': 1.9626447067834367e-06, 'rewards/chosen': -1.7514101266860962, 'rewards/rejected': -3.21616268157959, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4647527933120728, 'policy_logps/rejected': -314.0437927246094, 'policy_logps/chosen': -401.7545471191406, 'referece_logps/rejected': -281.8821716308594, 'referece_logps/chosen': -384.240478515625, 'logits/rejected': -0.25274115800857544, 'logits/chosen': -0.3124080002307892, 'epoch': 6.99}


 78%|███████▊  | 18772/24156 [12:56:28<20:19:33, 13.59s/it]

 78%|███████▊  | 18773/24156 [12:56:46<22:00:43, 14.72s/it]

 78%|███████▊  | 18774/24156 [12:57:06<24:23:14, 16.31s/it]

 78%|███████▊  | 18775/24156 [12:57:30<27:50:46, 18.63s/it]

 78%|███████▊  | 18776/24156 [12:57:42<25:07:02, 16.81s/it]

 78%|███████▊  | 18777/24156 [12:57:56<23:42:16, 15.86s/it]

 78%|███████▊  | 18778/24156 [12:58:17<26:09:07, 17.51s/it]

 78%|███████▊  | 18779/24156 [12:58:38<27:30:29, 18.42s/it]

 78%|███████▊  | 18780/24156 [12:58:49<24:02:11, 16.10s/it]

 78%|███████▊  | 18781/24156 [12:59:10<26:11:24, 17.54s/it]

 78%|███████▊  | 18782/24156 [12:59:26<25:31:44, 17.10s/it]

 78%|███████▊  | 18783/24156 [12:59:39<23:50:14, 15.97s/it]

 78%|███████▊  | 18784/24156 [12:59:52<22:31:16, 15.09s/it]
{'loss': 0.3068, 'learning_rate': 1.96213475340134e-06, 'rewards/chosen': -2.070965528488159, 'rewards/rejected': -4.545783042907715, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4748172760009766, 'policy_logps/rejected': -270.3020935058594, 'policy_logps/chosen': -218.6664276123047, 'referece_logps/rejected': -224.84426879882812, 'referece_logps/chosen': -197.956787109375, 'logits/rejected': -0.6653772592544556, 'logits/chosen': -0.6836436986923218, 'epoch': 7.0}

 78%|███████▊  | 18785/24156 [13:00:07<22:35:01, 15.14s/it]


 78%|███████▊  | 18787/24156 [13:00:40<23:14:46, 15.59s/it]

 78%|███████▊  | 18788/24156 [13:01:01<25:39:20, 17.21s/it]

 78%|███████▊  | 18789/24156 [13:01:14<23:58:58, 16.09s/it]

 78%|███████▊  | 18790/24156 [13:01:28<22:47:53, 15.30s/it]

 78%|███████▊  | 18791/24156 [13:01:47<24:30:18, 16.44s/it]

 78%|███████▊  | 18792/24156 [13:02:07<26:00:45, 17.46s/it]

 78%|███████▊  | 18793/24156 [13:02:22<25:13:19, 16.93s/it]

 78%|███████▊  | 18794/24156 [13:02:43<26:44:34, 17.95s/it]

 78%|███████▊  | 18795/24156 [13:03:02<27:09:59, 18.24s/it]

 78%|███████▊  | 18796/24156 [13:03:14<24:19:07, 16.33s/it]

 78%|███████▊  | 18797/24156 [13:03:24<21:48:43, 14.65s/it]

 78%|███████▊  | 18798/24156 [13:03:41<22:33:15, 15.15s/it]

 78%|███████▊  | 18799/24156 [13:03:58<23:42:17, 15.93s/it]
{'loss': 0.3411, 'learning_rate': 1.9615846128743918e-06, 'rewards/chosen': -1.8130238056182861, 'rewards/rejected': -2.767071485519409, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9540480375289917, 'policy_logps/rejected': -481.3117980957031, 'policy_logps/chosen': -611.8273315429688, 'referece_logps/rejected': -453.6410827636719, 'referece_logps/chosen': -593.6970825195312, 'logits/rejected': -0.08206754922866821, 'logits/chosen': 0.007528245449066162, 'epoch': 7.0}


 78%|███████▊  | 18801/24156 [13:04:28<22:56:11, 15.42s/it]

 78%|███████▊  | 18802/24156 [13:04:46<24:14:09, 16.30s/it]

 78%|███████▊  | 18803/24156 [13:04:58<22:30:21, 15.14s/it]

 78%|███████▊  | 18804/24156 [13:05:11<21:29:38, 14.46s/it]

 78%|███████▊  | 18805/24156 [13:05:25<21:20:32, 14.36s/it]

 78%|███████▊  | 18806/24156 [13:05:41<21:57:10, 14.77s/it]

 78%|███████▊  | 18807/24156 [13:05:57<22:37:42, 15.23s/it]

 78%|███████▊  | 18808/24156 [13:06:14<23:18:29, 15.69s/it]
{'loss': 0.3128, 'learning_rate': 1.961252661480094e-06, 'rewards/chosen': -1.2875980138778687, 'rewards/rejected': -3.420639991760254, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1330418586730957, 'policy_logps/rejected': -288.5628967285156, 'policy_logps/chosen': -319.64984130859375, 'referece_logps/rejected': -254.3565216064453, 'referece_logps/chosen': -306.77386474609375, 'logits/rejected': 0.1596994400024414, 'logits/chosen': 0.22445860505104065, 'epoch': 7.01}


 78%|███████▊  | 18810/24156 [13:06:48<24:42:01, 16.63s/it]

 78%|███████▊  | 18811/24156 [13:07:09<26:48:17, 18.05s/it]

 78%|███████▊  | 18812/24156 [13:07:28<26:56:34, 18.15s/it]

 78%|███████▊  | 18813/24156 [13:07:40<24:21:57, 16.42s/it]

 78%|███████▊  | 18814/24156 [13:07:52<22:20:05, 15.05s/it]

 78%|███████▊  | 18815/24156 [13:08:03<20:23:25, 13.74s/it]

 78%|███████▊  | 18816/24156 [13:08:13<19:03:50, 12.85s/it]

 78%|███████▊  | 18817/24156 [13:08:27<19:22:03, 13.06s/it]

 78%|███████▊  | 18818/24156 [13:08:43<20:31:18, 13.84s/it]

 78%|███████▊  | 18819/24156 [13:08:54<19:23:42, 13.08s/it]

 78%|███████▊  | 18820/24156 [13:09:07<19:30:16, 13.16s/it]

 78%|███████▊  | 18821/24156 [13:09:27<22:28:47, 15.17s/it]

 78%|███████▊  | 18822/24156 [13:09:39<21:10:34, 14.29s/it]

 78%|███████▊  | 18823/24156 [13:09:59<23:30:21, 15.87s/it]

 78%|███████▊  | 18824/24156 [13:10:16<23:59:48, 16.20s/it]

 78%|███████▊  | 18825/24156 [13:10:32<24:01:36, 16.23s/it]

 78%|███████▊  | 18826/24156 [13:10:52<25:23:55, 17.15s/it]
{'loss': 0.3614, 'learning_rate': 1.960584560022215e-06, 'rewards/chosen': -1.4534258842468262, 'rewards/rejected': -4.047461986541748, 'rewards/accuracies': 1.0, 'rewards/margins': 2.594036102294922, 'policy_logps/rejected': -360.522705078125, 'policy_logps/chosen': -491.1612243652344, 'referece_logps/rejected': -320.048095703125, 'referece_logps/chosen': -476.62701416015625, 'logits/rejected': -0.23240545392036438, 'logits/chosen': -0.35757771134376526, 'epoch': 7.01}


 78%|███████▊  | 18828/24156 [13:11:17<21:58:25, 14.85s/it]
{'loss': 0.3947, 'learning_rate': 1.960509981086303e-06, 'rewards/chosen': -1.9811184406280518, 'rewards/rejected': -3.9951436519622803, 'rewards/accuracies': 0.625, 'rewards/margins': 2.0140254497528076, 'policy_logps/rejected': -616.7080078125, 'policy_logps/chosen': -463.96435546875, 'referece_logps/rejected': -576.7565307617188, 'referece_logps/chosen': -444.15313720703125, 'logits/rejected': -0.23255908489227295, 'logits/chosen': -0.2785414159297943, 'epoch': 7.01}

 78%|███████▊  | 18829/24156 [13:11:38<24:49:27, 16.78s/it]


 78%|███████▊  | 18831/24156 [13:12:09<23:38:31, 15.98s/it]

 78%|███████▊  | 18832/24156 [13:12:31<25:58:38, 17.57s/it]

 78%|███████▊  | 18833/24156 [13:12:43<23:35:10, 15.95s/it]
{'loss': 0.427, 'learning_rate': 1.9603232315836186e-06, 'rewards/chosen': -2.226344347000122, 'rewards/rejected': -2.715385913848877, 'rewards/accuracies': 0.625, 'rewards/margins': 0.489041805267334, 'policy_logps/rejected': -561.0706787109375, 'policy_logps/chosen': -304.095458984375, 'referece_logps/rejected': -533.9168701171875, 'referece_logps/chosen': -281.83203125, 'logits/rejected': -0.1474991738796234, 'logits/chosen': -0.10297145694494247, 'epoch': 7.02}


 78%|███████▊  | 18835/24156 [13:13:13<23:01:31, 15.58s/it]
{'loss': 0.352, 'learning_rate': 1.960248410931483e-06, 'rewards/chosen': -1.813187837600708, 'rewards/rejected': -3.960308074951172, 'rewards/accuracies': 0.875, 'rewards/margins': 2.147120475769043, 'policy_logps/rejected': -541.7785034179688, 'policy_logps/chosen': -364.03662109375, 'referece_logps/rejected': -502.1754150390625, 'referece_logps/chosen': -345.90472412109375, 'logits/rejected': -0.7094558477401733, 'logits/chosen': -0.3313712477684021, 'epoch': 7.02}


 78%|███████▊  | 18837/24156 [13:13:39<20:35:01, 13.93s/it]

 78%|███████▊  | 18838/24156 [13:13:49<19:07:23, 12.95s/it]

 78%|███████▊  | 18839/24156 [13:14:00<18:01:23, 12.20s/it]
{'loss': 0.3738, 'learning_rate': 1.960098562483551e-06, 'rewards/chosen': -1.4687098264694214, 'rewards/rejected': -2.760056257247925, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2913464307785034, 'policy_logps/rejected': -339.7402038574219, 'policy_logps/chosen': -379.9740295410156, 'referece_logps/rejected': -312.1396484375, 'referece_logps/chosen': -365.28692626953125, 'logits/rejected': -0.3600057065486908, 'logits/chosen': -0.3128699064254761, 'epoch': 7.02}


 78%|███████▊  | 18841/24156 [13:14:24<17:58:58, 12.18s/it]
{'loss': 0.4872, 'learning_rate': 1.9600235346985298e-06, 'rewards/chosen': -1.9981589317321777, 'rewards/rejected': -3.215095043182373, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2169361114501953, 'policy_logps/rejected': -395.07330322265625, 'policy_logps/chosen': -404.1309814453125, 'referece_logps/rejected': -362.92236328125, 'referece_logps/chosen': -384.1493835449219, 'logits/rejected': 0.33091676235198975, 'logits/chosen': 0.3156355321407318, 'epoch': 7.02}


 78%|███████▊  | 18843/24156 [13:14:56<21:41:45, 14.70s/it]

 78%|███████▊  | 18844/24156 [13:15:09<20:55:47, 14.18s/it]
{'loss': 0.3317, 'learning_rate': 1.959910863584851e-06, 'rewards/chosen': -1.9965155124664307, 'rewards/rejected': -3.817545175552368, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8210299015045166, 'policy_logps/rejected': -497.49981689453125, 'policy_logps/chosen': -427.94818115234375, 'referece_logps/rejected': -459.3243713378906, 'referece_logps/chosen': -407.9830017089844, 'logits/rejected': -1.2905008792877197, 'logits/chosen': -1.1421681642532349, 'epoch': 7.02}

 78%|███████▊  | 18845/24156 [13:15:29<23:21:41, 15.84s/it]


 78%|███████▊  | 18847/24156 [13:16:06<25:08:10, 17.04s/it]

 78%|███████▊  | 18848/24156 [13:16:24<25:35:40, 17.36s/it]

 78%|███████▊  | 18849/24156 [13:16:44<26:40:33, 18.10s/it]

 78%|███████▊  | 18850/24156 [13:17:05<28:10:04, 19.11s/it]
[2024-04-06 04:22:02,486] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3035, 'learning_rate': 1.959685055454236e-06, 'rewards/chosen': -1.820127010345459, 'rewards/rejected': -4.165355682373047, 'rewards/accuracies': 0.875, 'rewards/margins': 2.345228433609009, 'policy_logps/rejected': -293.5632019042969, 'policy_logps/chosen': -298.90167236328125, 'referece_logps/rejected': -251.90965270996094, 'referece_logps/chosen': -280.7004089355469, 'logits/rejected': -0.30186739563941956, 'logits/chosen': -0.3720860481262207, 'epoch': 7.02}

 78%|███████▊  | 18851/24156 [13:17:25<28:30:42, 19.35s/it]
{'loss': 0.3778, 'learning_rate': 1.9596473603788254e-06, 'rewards/chosen': -2.0411853790283203, 'rewards/rejected': -2.732267141342163, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6910814642906189, 'policy_logps/rejected': -400.7181701660156, 'policy_logps/chosen': -469.01678466796875, 'referece_logps/rejected': -373.3955078125, 'referece_logps/chosen': -448.6049499511719, 'logits/rejected': -1.0364794731140137, 'logits/chosen': -1.0555673837661743, 'epoch': 7.02}


 78%|███████▊  | 18853/24156 [13:17:55<24:48:44, 16.84s/it]

 78%|███████▊  | 18854/24156 [13:18:06<22:05:50, 15.00s/it]
{'loss': 0.3408, 'learning_rate': 1.959534171645611e-06, 'rewards/chosen': -1.494827389717102, 'rewards/rejected': -3.995145797729492, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5003180503845215, 'policy_logps/rejected': -716.96923828125, 'policy_logps/chosen': -474.8955078125, 'referece_logps/rejected': -677.0177612304688, 'referece_logps/chosen': -459.947265625, 'logits/rejected': -0.8089119791984558, 'logits/chosen': -0.45353296399116516, 'epoch': 7.02}


 78%|███████▊  | 18856/24156 [13:18:27<18:49:13, 12.78s/it]

 78%|███████▊  | 18857/24156 [13:18:44<20:26:00, 13.88s/it]

 78%|███████▊  | 18858/24156 [13:19:05<23:49:15, 16.19s/it]
{'loss': 0.3677, 'learning_rate': 1.9593830118437013e-06, 'rewards/chosen': -1.5590771436691284, 'rewards/rejected': -3.150165557861328, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5910882949829102, 'policy_logps/rejected': -229.95887756347656, 'policy_logps/chosen': -210.0750732421875, 'referece_logps/rejected': -198.45721435546875, 'referece_logps/chosen': -194.4842987060547, 'logits/rejected': -0.4352462887763977, 'logits/chosen': -0.42085954546928406, 'epoch': 7.03}
[2024-04-06 04:24:24,504] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 78%|███████▊  | 18860/24156 [13:19:39<23:47:54, 16.18s/it]
{'loss': 0.3977, 'learning_rate': 1.9593073284588487e-06, 'rewards/chosen': -1.4788981676101685, 'rewards/rejected': -3.6173946857452393, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1384968757629395, 'policy_logps/rejected': -284.7451171875, 'policy_logps/chosen': -415.8890075683594, 'referece_logps/rejected': -248.5711212158203, 'referece_logps/chosen': -401.0999755859375, 'logits/rejected': -1.1585530042648315, 'logits/chosen': -1.180538296699524, 'epoch': 7.03}


 78%|███████▊  | 18862/24156 [13:20:20<26:50:46, 18.26s/it]
{'loss': 0.3427, 'learning_rate': 1.9592315760919852e-06, 'rewards/chosen': -1.4320745468139648, 'rewards/rejected': -4.144793510437012, 'rewards/accuracies': 0.75, 'rewards/margins': 2.712718963623047, 'policy_logps/rejected': -268.197998046875, 'policy_logps/chosen': -309.3651123046875, 'referece_logps/rejected': -226.7500457763672, 'referece_logps/chosen': -295.0443420410156, 'logits/rejected': -0.6017154455184937, 'logits/chosen': -0.6454256176948547, 'epoch': 7.03}


 78%|███████▊  | 18864/24156 [13:20:56<26:15:40, 17.86s/it]

 78%|███████▊  | 18865/24156 [13:21:16<27:16:27, 18.56s/it]

 78%|███████▊  | 18866/24156 [13:21:37<28:25:03, 19.34s/it]
[2024-04-06 04:26:34,926] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 18867/24156 [13:21:52<26:19:08, 17.91s/it]
{'loss': 0.2951, 'learning_rate': 1.95904189341429e-06, 'rewards/chosen': -2.6883668899536133, 'rewards/rejected': -3.505345344543457, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8169784545898438, 'policy_logps/rejected': -197.42515563964844, 'policy_logps/chosen': -322.5895690917969, 'referece_logps/rejected': -162.37168884277344, 'referece_logps/chosen': -295.7059020996094, 'logits/rejected': -0.8889995217323303, 'logits/chosen': -1.0011074542999268, 'epoch': 7.03}

 78%|███████▊  | 18868/24156 [13:22:03<23:19:59, 15.89s/it]

 78%|███████▊  | 18869/24156 [13:22:23<24:54:18, 16.96s/it]


 78%|███████▊  | 18871/24156 [13:22:59<26:15:18, 17.88s/it]

 78%|███████▊  | 18872/24156 [13:23:13<24:32:54, 16.72s/it]

 78%|███████▊  | 18873/24156 [13:23:34<26:00:54, 17.73s/it]

 78%|███████▊  | 18874/24156 [13:23:44<23:00:24, 15.68s/it]

 78%|███████▊  | 18875/24156 [13:23:59<22:40:43, 15.46s/it]

 78%|███████▊  | 18876/24156 [13:24:10<20:33:24, 14.02s/it]

 78%|███████▊  | 18877/24156 [13:24:26<21:36:50, 14.74s/it]

 78%|███████▊  | 18878/24156 [13:24:46<23:48:14, 16.24s/it]
{'loss': 0.5115, 'learning_rate': 1.958623074459405e-06, 'rewards/chosen': -1.9335837364196777, 'rewards/rejected': -3.607819080352783, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6742353439331055, 'policy_logps/rejected': -346.15399169921875, 'policy_logps/chosen': -302.7398986816406, 'referece_logps/rejected': -310.0758056640625, 'referece_logps/chosen': -283.4040832519531, 'logits/rejected': -0.5742252469062805, 'logits/chosen': -0.5365057587623596, 'epoch': 7.03}


 78%|███████▊  | 18880/24156 [13:25:20<23:55:28, 16.32s/it]
{'loss': 0.4589, 'learning_rate': 1.9585467015001665e-06, 'rewards/chosen': -1.8025988340377808, 'rewards/rejected': -2.575270652770996, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7726719379425049, 'policy_logps/rejected': -341.5904846191406, 'policy_logps/chosen': -308.4075927734375, 'referece_logps/rejected': -315.8377685546875, 'referece_logps/chosen': -290.3815612792969, 'logits/rejected': 0.28430065512657166, 'logits/chosen': 0.3552108108997345, 'epoch': 7.03}


 78%|███████▊  | 18882/24156 [13:25:57<25:46:07, 17.59s/it]
{'loss': 0.393, 'learning_rate': 1.9584702596136125e-06, 'rewards/chosen': -2.146867036819458, 'rewards/rejected': -2.8143417835235596, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6674750447273254, 'policy_logps/rejected': -257.51593017578125, 'policy_logps/chosen': -299.59747314453125, 'referece_logps/rejected': -229.37249755859375, 'referece_logps/chosen': -278.1287841796875, 'logits/rejected': -0.9484527707099915, 'logits/chosen': -1.0196435451507568, 'epoch': 7.04}


 78%|███████▊  | 18884/24156 [13:26:32<25:32:14, 17.44s/it]

 78%|███████▊  | 18885/24156 [13:26:50<25:36:48, 17.49s/it]

 78%|███████▊  | 18886/24156 [13:27:01<22:40:13, 15.49s/it]

 78%|███████▊  | 18887/24156 [13:27:14<21:45:18, 14.86s/it]
{'loss': 0.3566, 'learning_rate': 1.9582788533763077e-06, 'rewards/chosen': -1.5704073905944824, 'rewards/rejected': -3.206063747406006, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6356563568115234, 'policy_logps/rejected': -294.8684997558594, 'policy_logps/chosen': -296.2060241699219, 'referece_logps/rejected': -262.807861328125, 'referece_logps/chosen': -280.501953125, 'logits/rejected': -0.16948069632053375, 'logits/chosen': -0.12230034172534943, 'epoch': 7.04}

 78%|███████▊  | 18888/24156 [13:27:28<21:08:10, 14.44s/it]


 78%|███████▊  | 18890/24156 [13:27:56<21:05:19, 14.42s/it]

 78%|███████▊  | 18891/24156 [13:28:10<21:00:37, 14.37s/it]
{'loss': 0.4318, 'learning_rate': 1.958125418296092e-06, 'rewards/chosen': -1.0481314659118652, 'rewards/rejected': -2.494870901107788, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4467394351959229, 'policy_logps/rejected': -211.37960815429688, 'policy_logps/chosen': -264.9225769042969, 'referece_logps/rejected': -186.43089294433594, 'referece_logps/chosen': -254.44125366210938, 'logits/rejected': -0.8558302521705627, 'logits/chosen': -0.8607798218727112, 'epoch': 7.04}


 78%|███████▊  | 18893/24156 [13:28:44<22:56:10, 15.69s/it]

 78%|███████▊  | 18894/24156 [13:29:00<23:12:49, 15.88s/it]

 78%|███████▊  | 18895/24156 [13:29:15<22:30:50, 15.41s/it]

 78%|███████▊  | 18896/24156 [13:29:31<22:41:33, 15.53s/it]

 78%|███████▊  | 18897/24156 [13:29:44<21:52:50, 14.98s/it]
{'loss': 0.304, 'learning_rate': 1.957894748961928e-06, 'rewards/chosen': -2.3179500102996826, 'rewards/rejected': -3.6753082275390625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3573578596115112, 'policy_logps/rejected': -441.0712890625, 'policy_logps/chosen': -341.5655517578125, 'referece_logps/rejected': -404.3182067871094, 'referece_logps/chosen': -318.3860778808594, 'logits/rejected': -0.8103979229927063, 'logits/chosen': -1.0628008842468262, 'epoch': 7.04}

 78%|███████▊  | 18898/24156 [13:30:04<23:48:04, 16.30s/it]


 78%|███████▊  | 18900/24156 [13:30:45<26:51:41, 18.40s/it]

 78%|███████▊  | 18901/24156 [13:30:57<23:55:35, 16.39s/it]
{'loss': 0.3742, 'learning_rate': 1.9577406249944305e-06, 'rewards/chosen': -1.7889009714126587, 'rewards/rejected': -3.317573070526123, 'rewards/accuracies': 0.5, 'rewards/margins': 1.5286720991134644, 'policy_logps/rejected': -376.8544006347656, 'policy_logps/chosen': -343.66070556640625, 'referece_logps/rejected': -343.67864990234375, 'referece_logps/chosen': -325.77166748046875, 'logits/rejected': -0.2601674795150757, 'logits/chosen': -0.12034988403320312, 'epoch': 7.04}


 78%|███████▊  | 18903/24156 [13:31:29<23:51:18, 16.35s/it]

 78%|███████▊  | 18904/24156 [13:31:41<21:58:03, 15.06s/it]

 78%|███████▊  | 18905/24156 [13:32:01<24:08:15, 16.55s/it]

 78%|███████▊  | 18906/24156 [13:32:20<25:21:48, 17.39s/it]
{'loss': 0.4078, 'learning_rate': 1.957547582650165e-06, 'rewards/chosen': -1.3980265855789185, 'rewards/rejected': -3.859830856323242, 'rewards/accuracies': 0.625, 'rewards/margins': 2.461803674697876, 'policy_logps/rejected': -252.4687042236328, 'policy_logps/chosen': -384.804931640625, 'referece_logps/rejected': -213.87037658691406, 'referece_logps/chosen': -370.8246765136719, 'logits/rejected': -0.7025502324104309, 'logits/chosen': -0.975452721118927, 'epoch': 7.04}

 78%|███████▊  | 18907/24156 [13:32:38<25:27:59, 17.47s/it]


 78%|███████▊  | 18909/24156 [13:33:24<29:45:05, 20.41s/it]

 78%|███████▊  | 18910/24156 [13:33:44<29:27:27, 20.22s/it]

 78%|███████▊  | 18911/24156 [13:34:02<28:32:33, 19.59s/it]

 78%|███████▊  | 18912/24156 [13:34:21<27:58:18, 19.20s/it]
{'loss': 0.3365, 'learning_rate': 1.95731536378708e-06, 'rewards/chosen': -1.6031252145767212, 'rewards/rejected': -2.7725703716278076, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1694450378417969, 'policy_logps/rejected': -303.6329345703125, 'policy_logps/chosen': -306.8702392578125, 'referece_logps/rejected': -275.9072265625, 'referece_logps/chosen': -290.8389892578125, 'logits/rejected': -0.24165228009223938, 'logits/chosen': -0.15450982749462128, 'epoch': 7.05}
[2024-04-06 04:39:39,201] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 78%|███████▊  | 18914/24156 [13:34:53<25:10:32, 17.29s/it]

 78%|███████▊  | 18915/24156 [13:35:13<26:17:14, 18.06s/it]

 78%|███████▊  | 18916/24156 [13:35:29<25:18:17, 17.39s/it]

 78%|███████▊  | 18917/24156 [13:35:51<27:10:15, 18.67s/it]

 78%|███████▊  | 18918/24156 [13:36:10<27:41:48, 19.04s/it]

 78%|███████▊  | 18919/24156 [13:36:32<28:57:00, 19.90s/it]

 78%|███████▊  | 18920/24156 [13:36:50<28:01:03, 19.26s/it]

 78%|███████▊  | 18921/24156 [13:37:10<28:21:26, 19.50s/it]

 78%|███████▊  | 18922/24156 [13:37:26<26:52:36, 18.49s/it]
{'loss': 0.3485, 'learning_rate': 1.9569269556476273e-06, 'rewards/chosen': -1.6755915880203247, 'rewards/rejected': -3.581634283065796, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9060430526733398, 'policy_logps/rejected': -577.9945678710938, 'policy_logps/chosen': -477.4261169433594, 'referece_logps/rejected': -542.17822265625, 'referece_logps/chosen': -460.670166015625, 'logits/rejected': 0.30751845240592957, 'logits/chosen': 0.29831328988075256, 'epoch': 7.05}


 78%|███████▊  | 18924/24156 [13:38:07<28:15:58, 19.45s/it]

 78%|███████▊  | 18925/24156 [13:38:25<27:30:51, 18.94s/it]

 78%|███████▊  | 18926/24156 [13:38:39<25:21:17, 17.45s/it]

 78%|███████▊  | 18927/24156 [13:38:59<26:13:14, 18.05s/it]
{'loss': 0.3415, 'learning_rate': 1.956732106432585e-06, 'rewards/chosen': -1.4306354522705078, 'rewards/rejected': -2.870389461517334, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4397541284561157, 'policy_logps/rejected': -397.93988037109375, 'policy_logps/chosen': -384.55816650390625, 'referece_logps/rejected': -369.2359619140625, 'referece_logps/chosen': -370.2518005371094, 'logits/rejected': 0.47339096665382385, 'logits/chosen': 0.39571046829223633, 'epoch': 7.05}


 78%|███████▊  | 18929/24156 [13:39:35<26:09:58, 18.02s/it]

 78%|███████▊  | 18930/24156 [13:39:55<27:11:35, 18.73s/it]

 78%|███████▊  | 18931/24156 [13:40:14<26:57:19, 18.57s/it]

 78%|███████▊  | 18932/24156 [13:40:24<23:37:15, 16.28s/it]

 78%|███████▊  | 18933/24156 [13:40:41<23:47:00, 16.39s/it]

 78%|███████▊  | 18934/24156 [13:40:57<23:22:51, 16.12s/it]
{'loss': 0.4093, 'learning_rate': 1.956458595184475e-06, 'rewards/chosen': -2.1549484729766846, 'rewards/rejected': -3.895425319671631, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7404773235321045, 'policy_logps/rejected': -475.1825256347656, 'policy_logps/chosen': -386.15069580078125, 'referece_logps/rejected': -436.228271484375, 'referece_logps/chosen': -364.6011962890625, 'logits/rejected': 0.148091122508049, 'logits/chosen': 0.199102520942688, 'epoch': 7.05}
[2024-04-06 04:46:15,717] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 78%|███████▊  | 18935/24156 [13:41:18<25:47:18, 17.78s/it]


 78%|███████▊  | 18937/24156 [13:41:49<24:36:17, 16.97s/it]

 78%|███████▊  | 18938/24156 [13:42:05<23:58:36, 16.54s/it]

 78%|███████▊  | 18939/24156 [13:42:25<25:24:09, 17.53s/it]
{'loss': 0.2395, 'learning_rate': 1.9562627141644843e-06, 'rewards/chosen': -1.091728687286377, 'rewards/rejected': -4.622742652893066, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5310139656066895, 'policy_logps/rejected': -410.622802734375, 'policy_logps/chosen': -293.9208679199219, 'referece_logps/rejected': -364.3953857421875, 'referece_logps/chosen': -283.0035705566406, 'logits/rejected': -0.5507810711860657, 'logits/chosen': -0.2532026171684265, 'epoch': 7.06}

 78%|███████▊  | 18940/24156 [13:42:44<26:11:52, 18.08s/it]

 78%|███████▊  | 18941/24156 [13:42:56<23:30:15, 16.23s/it]


 78%|███████▊  | 18943/24156 [13:43:28<23:40:51, 16.35s/it]

 78%|███████▊  | 18944/24156 [13:43:49<25:54:08, 17.89s/it]
{'loss': 0.4104, 'learning_rate': 1.95606640337527e-06, 'rewards/chosen': -1.5483119487762451, 'rewards/rejected': -3.0526137351989746, 'rewards/accuracies': 0.875, 'rewards/margins': 1.504301905632019, 'policy_logps/rejected': -414.66217041015625, 'policy_logps/chosen': -463.2762145996094, 'referece_logps/rejected': -384.1361083984375, 'referece_logps/chosen': -447.7931213378906, 'logits/rejected': -0.6727451086044312, 'logits/chosen': -0.7960425615310669, 'epoch': 7.06}


 78%|███████▊  | 18946/24156 [13:44:23<25:15:41, 17.46s/it]

 78%|███████▊  | 18947/24156 [13:44:38<24:00:53, 16.60s/it]

 78%|███████▊  | 18948/24156 [13:44:49<21:49:14, 15.08s/it]
{'loss': 0.38, 'learning_rate': 1.9559090453693393e-06, 'rewards/chosen': -1.7475676536560059, 'rewards/rejected': -2.2776715755462646, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5301040410995483, 'policy_logps/rejected': -291.15911865234375, 'policy_logps/chosen': -320.322509765625, 'referece_logps/rejected': -268.3824157714844, 'referece_logps/chosen': -302.8468017578125, 'logits/rejected': -0.547695517539978, 'logits/chosen': -0.46986761689186096, 'epoch': 7.06}


 78%|███████▊  | 18950/24156 [13:45:20<22:32:35, 15.59s/it]
{'loss': 0.4969, 'learning_rate': 1.9558302632570747e-06, 'rewards/chosen': -1.5820647478103638, 'rewards/rejected': -2.9820337295532227, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3999689817428589, 'policy_logps/rejected': -539.2708740234375, 'policy_logps/chosen': -536.3155517578125, 'referece_logps/rejected': -509.4505615234375, 'referece_logps/chosen': -520.494873046875, 'logits/rejected': -0.745486855506897, 'logits/chosen': -0.6214393377304077, 'epoch': 7.06}


 78%|███████▊  | 18952/24156 [13:45:58<24:23:00, 16.87s/it]
{'loss': 0.4835, 'learning_rate': 1.9557514124128285e-06, 'rewards/chosen': -1.8745791912078857, 'rewards/rejected': -3.84816837310791, 'rewards/accuracies': 1.0, 'rewards/margins': 1.973589301109314, 'policy_logps/rejected': -305.6995849609375, 'policy_logps/chosen': -274.94488525390625, 'referece_logps/rejected': -267.2178955078125, 'referece_logps/chosen': -256.1990966796875, 'logits/rejected': 0.0007692873477935791, 'logits/chosen': 0.04131251573562622, 'epoch': 7.06}


 78%|███████▊  | 18954/24156 [13:46:31<24:08:45, 16.71s/it]
{'loss': 0.4119, 'learning_rate': 1.9556724928422713e-06, 'rewards/chosen': -2.017970085144043, 'rewards/rejected': -2.8657360076904297, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8477659821510315, 'policy_logps/rejected': -337.3341369628906, 'policy_logps/chosen': -442.16192626953125, 'referece_logps/rejected': -308.6767578125, 'referece_logps/chosen': -421.9822082519531, 'logits/rejected': -0.9515089392662048, 'logits/chosen': -0.7318381667137146, 'epoch': 7.06}

 78%|███████▊  | 18955/24156 [13:46:48<24:08:32, 16.71s/it]

 78%|███████▊  | 18956/24156 [13:47:06<24:44:03, 17.12s/it]


 78%|███████▊  | 18958/24156 [13:47:38<23:05:01, 15.99s/it]
{'loss': 0.4502, 'learning_rate': 1.955514447544928e-06, 'rewards/chosen': -1.8658196926116943, 'rewards/rejected': -2.216970920562744, 'rewards/accuracies': 0.625, 'rewards/margins': 0.3511514365673065, 'policy_logps/rejected': -306.9504089355469, 'policy_logps/chosen': -406.8197937011719, 'referece_logps/rejected': -284.7806701660156, 'referece_logps/chosen': -388.1615905761719, 'logits/rejected': -0.1710326224565506, 'logits/chosen': -0.2802608013153076, 'epoch': 7.06}

 78%|███████▊  | 18959/24156 [13:47:51<21:48:02, 15.10s/it]


 78%|███████▊  | 18961/24156 [13:48:18<21:04:58, 14.61s/it]
{'loss': 0.4849, 'learning_rate': 1.9553957332075964e-06, 'rewards/chosen': -1.269779086112976, 'rewards/rejected': -2.7654833793640137, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4957042932510376, 'policy_logps/rejected': -416.35369873046875, 'policy_logps/chosen': -394.2710266113281, 'referece_logps/rejected': -388.6988525390625, 'referece_logps/chosen': -381.5732727050781, 'logits/rejected': 0.3450797200202942, 'logits/chosen': 0.48799818754196167, 'epoch': 7.06}

 78%|███████▊  | 18962/24156 [13:48:29<19:24:09, 13.45s/it]

 79%|███████▊  | 18963/24156 [13:48:45<20:30:41, 14.22s/it]


 79%|███████▊  | 18965/24156 [13:49:08<18:24:28, 12.77s/it]

 79%|███████▊  | 18966/24156 [13:49:24<19:45:49, 13.71s/it]
{'loss': 0.4091, 'learning_rate': 1.955197532484535e-06, 'rewards/chosen': -1.7991429567337036, 'rewards/rejected': -2.371553421020508, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5724105834960938, 'policy_logps/rejected': -396.4349365234375, 'policy_logps/chosen': -373.0851745605469, 'referece_logps/rejected': -372.71942138671875, 'referece_logps/chosen': -355.0937194824219, 'logits/rejected': 0.4510834515094757, 'logits/chosen': 0.4652192294597626, 'epoch': 7.07}

 79%|███████▊  | 18967/24156 [13:49:35<18:39:22, 12.94s/it]

 79%|███████▊  | 18968/24156 [13:49:56<22:18:18, 15.48s/it]

 79%|███████▊  | 18969/24156 [13:50:12<22:36:23, 15.69s/it]

 79%|███████▊  | 18970/24156 [13:50:30<23:12:49, 16.11s/it]

 79%|███████▊  | 18971/24156 [13:50:45<22:52:48, 15.89s/it]

 79%|███████▊  | 18972/24156 [13:51:05<24:32:58, 17.05s/it]

 79%|███████▊  | 18973/24156 [13:51:21<24:21:53, 16.92s/it]


 79%|███████▊  | 18975/24156 [13:51:54<24:06:39, 16.75s/it]
{'loss': 0.4052, 'learning_rate': 1.9548396894309375e-06, 'rewards/chosen': -0.9444388151168823, 'rewards/rejected': -3.9580650329589844, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0136260986328125, 'policy_logps/rejected': -395.77008056640625, 'policy_logps/chosen': -316.67919921875, 'referece_logps/rejected': -356.1894226074219, 'referece_logps/chosen': -307.23480224609375, 'logits/rejected': -0.5541244149208069, 'logits/chosen': -0.47776007652282715, 'epoch': 7.07}


 79%|███████▊  | 18977/24156 [13:52:26<23:08:47, 16.09s/it]

 79%|███████▊  | 18978/24156 [13:52:36<20:47:40, 14.46s/it]
{'loss': 0.4492, 'learning_rate': 1.954720099413938e-06, 'rewards/chosen': -1.7778537273406982, 'rewards/rejected': -3.9830892086029053, 'rewards/accuracies': 0.875, 'rewards/margins': 2.205235719680786, 'policy_logps/rejected': -200.73486328125, 'policy_logps/chosen': -384.622314453125, 'referece_logps/rejected': -160.90396118164062, 'referece_logps/chosen': -366.84381103515625, 'logits/rejected': -0.4873703420162201, 'logits/chosen': -0.6860949993133545, 'epoch': 7.07}


 79%|███████▊  | 18980/24156 [13:53:02<19:49:19, 13.79s/it]

 79%|███████▊  | 18981/24156 [13:53:18<20:54:16, 14.54s/it]

 79%|███████▊  | 18982/24156 [13:53:30<19:49:51, 13.80s/it]

 79%|███████▊  | 18983/24156 [13:53:48<21:41:01, 15.09s/it]
{'loss': 0.5106, 'learning_rate': 1.95452043946775e-06, 'rewards/chosen': -2.639805555343628, 'rewards/rejected': -2.8478140830993652, 'rewards/accuracies': 0.625, 'rewards/margins': 0.20800822973251343, 'policy_logps/rejected': -465.5368957519531, 'policy_logps/chosen': -536.9097900390625, 'referece_logps/rejected': -437.05877685546875, 'referece_logps/chosen': -510.5117492675781, 'logits/rejected': -0.5967587828636169, 'logits/chosen': -0.6998594403266907, 'epoch': 7.07}

 79%|███████▊  | 18984/24156 [13:54:10<24:17:54, 16.91s/it]

 79%|███████▊  | 18985/24156 [13:54:27<24:24:05, 16.99s/it]


 79%|███████▊  | 18987/24156 [13:54:56<22:27:00, 15.64s/it]

 79%|███████▊  | 18988/24156 [13:55:16<24:26:31, 17.03s/it]
{'loss': 0.4928, 'learning_rate': 1.954320350535362e-06, 'rewards/chosen': -2.0736160278320312, 'rewards/rejected': -2.3696792125701904, 'rewards/accuracies': 0.75, 'rewards/margins': 0.29606306552886963, 'policy_logps/rejected': -475.30853271484375, 'policy_logps/chosen': -377.60955810546875, 'referece_logps/rejected': -451.6116943359375, 'referece_logps/chosen': -356.8733825683594, 'logits/rejected': -1.289506435394287, 'logits/chosen': -1.2753407955169678, 'epoch': 7.07}

 79%|███████▊  | 18989/24156 [13:55:31<23:21:20, 16.27s/it]


 79%|███████▊  | 18991/24156 [13:56:02<23:16:02, 16.22s/it]
{'loss': 0.4139, 'learning_rate': 1.9542000912999537e-06, 'rewards/chosen': -1.5630667209625244, 'rewards/rejected': -3.365570068359375, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8025035858154297, 'policy_logps/rejected': -485.57745361328125, 'policy_logps/chosen': -462.8292236328125, 'referece_logps/rejected': -451.9217224121094, 'referece_logps/chosen': -447.1985778808594, 'logits/rejected': 0.11941753327846527, 'logits/chosen': 0.4660179018974304, 'epoch': 7.08}


 79%|███████▊  | 18993/24156 [13:56:26<20:22:44, 14.21s/it]

 79%|███████▊  | 18994/24156 [13:56:42<21:07:08, 14.73s/it]

 79%|███████▊  | 18995/24156 [13:56:58<21:48:47, 15.22s/it]
{'loss': 0.3821, 'learning_rate': 1.954039505504456e-06, 'rewards/chosen': -1.8959933519363403, 'rewards/rejected': -2.9789628982543945, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0829696655273438, 'policy_logps/rejected': -262.52593994140625, 'policy_logps/chosen': -332.89599609375, 'referece_logps/rejected': -232.73631286621094, 'referece_logps/chosen': -313.93603515625, 'logits/rejected': -0.8322595357894897, 'logits/chosen': -0.7427626252174377, 'epoch': 7.08}

 79%|███████▊  | 18996/24156 [13:57:12<20:54:00, 14.58s/it]

 79%|███████▊  | 18997/24156 [13:57:24<19:54:16, 13.89s/it]

 79%|███████▊  | 18998/24156 [13:57:45<22:55:35, 16.00s/it]

 79%|███████▊  | 18999/24156 [13:57:55<20:37:39, 14.40s/it]

 79%|███████▊  | 19000/24156 [13:58:11<21:19:52, 14.89s/it]


 79%|███████▊  | 19002/24156 [13:58:58<26:22:27, 18.42s/it]

 79%|███████▊  | 19003/24156 [13:59:16<26:16:31, 18.36s/it]

 79%|███████▊  | 19004/24156 [13:59:36<26:58:01, 18.84s/it]
{'loss': 0.3403, 'learning_rate': 1.9536771842131354e-06, 'rewards/chosen': -1.7283222675323486, 'rewards/rejected': -3.202810764312744, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4744884967803955, 'policy_logps/rejected': -424.22265625, 'policy_logps/chosen': -488.4181823730469, 'referece_logps/rejected': -392.1945495605469, 'referece_logps/chosen': -471.1349182128906, 'logits/rejected': -0.9968221187591553, 'logits/chosen': -0.7629165053367615, 'epoch': 7.08}

 79%|███████▊  | 19005/24156 [13:59:51<25:11:10, 17.60s/it]

 79%|███████▊  | 19006/24156 [14:00:02<22:12:19, 15.52s/it]

 79%|███████▊  | 19007/24156 [14:00:15<21:13:20, 14.84s/it]

 79%|███████▊  | 19008/24156 [14:00:27<19:56:13, 13.94s/it]


 79%|███████▊  | 19010/24156 [14:01:01<22:34:13, 15.79s/it]
{'loss': 0.3623, 'learning_rate': 1.953434865160006e-06, 'rewards/chosen': -1.7852020263671875, 'rewards/rejected': -2.754481792449951, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9692798256874084, 'policy_logps/rejected': -308.7373046875, 'policy_logps/chosen': -306.38983154296875, 'referece_logps/rejected': -281.1925048828125, 'referece_logps/chosen': -288.53778076171875, 'logits/rejected': -0.5265828371047974, 'logits/chosen': -0.5605119466781616, 'epoch': 7.08}


 79%|███████▊  | 19012/24156 [14:01:29<20:49:24, 14.57s/it]

 79%|███████▊  | 19013/24156 [14:01:43<20:36:01, 14.42s/it]
{'loss': 0.5059, 'learning_rate': 1.9533134742345383e-06, 'rewards/chosen': -2.008415460586548, 'rewards/rejected': -3.628197431564331, 'rewards/accuracies': 0.75, 'rewards/margins': 1.619781732559204, 'policy_logps/rejected': -401.42730712890625, 'policy_logps/chosen': -403.0281066894531, 'referece_logps/rejected': -365.14532470703125, 'referece_logps/chosen': -382.9440002441406, 'logits/rejected': -0.2905064821243286, 'logits/chosen': -0.3090408146381378, 'epoch': 7.08}


 79%|███████▊  | 19015/24156 [14:02:05<18:09:04, 12.71s/it]
{'loss': 0.4194, 'learning_rate': 1.953232461260926e-06, 'rewards/chosen': -2.220921516418457, 'rewards/rejected': -3.8929965496063232, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6720749139785767, 'policy_logps/rejected': -476.47052001953125, 'policy_logps/chosen': -368.6064453125, 'referece_logps/rejected': -437.5405578613281, 'referece_logps/chosen': -346.3972473144531, 'logits/rejected': -0.3266436755657196, 'logits/chosen': -0.3104386031627655, 'epoch': 7.08}

 79%|███████▊  | 19016/24156 [14:02:16<17:34:37, 12.31s/it]

 79%|███████▊  | 19017/24156 [14:02:32<19:02:30, 13.34s/it]


 79%|███████▊  | 19019/24156 [14:03:13<23:57:54, 16.79s/it]
{'loss': 0.3568, 'learning_rate': 1.9530702296839968e-06, 'rewards/chosen': -2.3371808528900146, 'rewards/rejected': -4.166804790496826, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8296246528625488, 'policy_logps/rejected': -489.1001892089844, 'policy_logps/chosen': -505.41400146484375, 'referece_logps/rejected': -447.4320983886719, 'referece_logps/chosen': -482.04217529296875, 'logits/rejected': -0.4225877523422241, 'logits/chosen': -0.5413426160812378, 'epoch': 7.09}

 79%|███████▊  | 19020/24156 [14:03:26<22:33:09, 15.81s/it]


 79%|███████▊  | 19022/24156 [14:04:09<26:22:03, 18.49s/it]

 79%|███████▉  | 19023/24156 [14:04:25<25:28:10, 17.86s/it]
{'loss': 0.2848, 'learning_rate': 1.9529077239730233e-06, 'rewards/chosen': -1.5421791076660156, 'rewards/rejected': -3.5121428966522217, 'rewards/accuracies': 0.875, 'rewards/margins': 1.969963788986206, 'policy_logps/rejected': -389.38751220703125, 'policy_logps/chosen': -296.7930603027344, 'referece_logps/rejected': -354.2660827636719, 'referece_logps/chosen': -281.3712463378906, 'logits/rejected': -0.3863397240638733, 'logits/chosen': -0.2850257456302643, 'epoch': 7.09}


 79%|███████▉  | 19025/24156 [14:05:01<25:15:29, 17.72s/it]

 79%|███████▉  | 19026/24156 [14:05:21<26:06:35, 18.32s/it]
{'loss': 0.3358, 'learning_rate': 1.9527856648174413e-06, 'rewards/chosen': -1.352971076965332, 'rewards/rejected': -3.3728113174438477, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0198400020599365, 'policy_logps/rejected': -261.9781188964844, 'policy_logps/chosen': -396.636962890625, 'referece_logps/rejected': -228.25, 'referece_logps/chosen': -383.10723876953125, 'logits/rejected': -0.7827810049057007, 'logits/chosen': -0.8863734006881714, 'epoch': 7.09}

 79%|███████▉  | 19027/24156 [14:05:40<26:34:19, 18.65s/it]

 79%|███████▉  | 19028/24156 [14:06:00<27:03:51, 19.00s/it]

 79%|███████▉  | 19029/24156 [14:06:18<26:49:52, 18.84s/it]


 79%|███████▉  | 19031/24156 [14:06:57<26:57:23, 18.94s/it]
{'loss': 0.2694, 'learning_rate': 1.952581890335989e-06, 'rewards/chosen': -2.2372851371765137, 'rewards/rejected': -4.865830421447754, 'rewards/accuracies': 0.875, 'rewards/margins': 2.628544807434082, 'policy_logps/rejected': -359.82421875, 'policy_logps/chosen': -409.3233642578125, 'referece_logps/rejected': -311.1659240722656, 'referece_logps/chosen': -386.95050048828125, 'logits/rejected': -0.6944836378097534, 'logits/chosen': -0.7270575761795044, 'epoch': 7.09}

 79%|███████▉  | 19032/24156 [14:07:14<26:23:08, 18.54s/it]

 79%|███████▉  | 19033/24156 [14:07:26<23:28:31, 16.50s/it]

 79%|███████▉  | 19034/24156 [14:07:42<23:05:34, 16.23s/it]

 79%|███████▉  | 19035/24156 [14:07:59<23:42:40, 16.67s/it]


 79%|███████▉  | 19037/24156 [14:08:21<19:24:42, 13.65s/it]
{'loss': 0.3265, 'learning_rate': 1.952336795854563e-06, 'rewards/chosen': -1.4117438793182373, 'rewards/rejected': -3.8121752738952637, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4004313945770264, 'policy_logps/rejected': -257.00213623046875, 'policy_logps/chosen': -340.7528381347656, 'referece_logps/rejected': -218.88038635253906, 'referece_logps/chosen': -326.6353759765625, 'logits/rejected': -0.6528711915016174, 'logits/chosen': -0.6592770218849182, 'epoch': 7.09}

 79%|███████▉  | 19038/24156 [14:08:32<18:09:14, 12.77s/it]


 79%|███████▉  | 19040/24156 [14:09:01<19:10:55, 13.50s/it]
{'loss': 0.2687, 'learning_rate': 1.952214017481325e-06, 'rewards/chosen': -1.8598167896270752, 'rewards/rejected': -4.282286167144775, 'rewards/accuracies': 0.75, 'rewards/margins': 2.422468900680542, 'policy_logps/rejected': -250.7353973388672, 'policy_logps/chosen': -414.92236328125, 'referece_logps/rejected': -207.91253662109375, 'referece_logps/chosen': -396.32415771484375, 'logits/rejected': -0.2742029130458832, 'logits/chosen': -0.2304706573486328, 'epoch': 7.09}


 79%|███████▉  | 19042/24156 [14:09:35<21:35:53, 15.20s/it]
{'loss': 0.2893, 'learning_rate': 1.952132079641343e-06, 'rewards/chosen': -1.4950225353240967, 'rewards/rejected': -4.034250259399414, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5392279624938965, 'policy_logps/rejected': -578.92529296875, 'policy_logps/chosen': -369.038818359375, 'referece_logps/rejected': -538.582763671875, 'referece_logps/chosen': -354.088623046875, 'logits/rejected': -1.0217339992523193, 'logits/chosen': -0.9173179268836975, 'epoch': 7.09}

 79%|███████▉  | 19043/24156 [14:09:52<22:19:06, 15.71s/it]

 79%|███████▉  | 19044/24156 [14:10:02<20:10:30, 14.21s/it]
[2024-04-06 05:15:21,412] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19045/24156 [14:10:24<23:16:58, 16.40s/it]

 79%|███████▉  | 19046/24156 [14:10:44<24:47:11, 17.46s/it]
[2024-04-06 05:16:01,283] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 19048/24156 [14:11:27<28:08:55, 19.84s/it]

 79%|███████▉  | 19049/24156 [14:11:47<28:04:47, 19.79s/it]
{'loss': 0.3035, 'learning_rate': 1.9518447580699603e-06, 'rewards/chosen': -1.9065890312194824, 'rewards/rejected': -4.866020679473877, 'rewards/accuracies': 1.0, 'rewards/margins': 2.959432363510132, 'policy_logps/rejected': -287.11749267578125, 'policy_logps/chosen': -266.0904541015625, 'referece_logps/rejected': -238.457275390625, 'referece_logps/chosen': -247.02456665039062, 'logits/rejected': -0.8333411812782288, 'logits/chosen': -0.8859646320343018, 'epoch': 7.1}

 79%|███████▉  | 19050/24156 [14:12:00<25:16:43, 17.82s/it]
[2024-04-06 05:17:15,763] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19051/24156 [14:12:18<25:18:39, 17.85s/it]


 79%|███████▉  | 19053/24156 [14:12:47<23:14:58, 16.40s/it]
{'loss': 0.4902, 'learning_rate': 1.9516801978489955e-06, 'rewards/chosen': -2.768679141998291, 'rewards/rejected': -3.8267300128936768, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0580511093139648, 'policy_logps/rejected': -307.6258239746094, 'policy_logps/chosen': -421.1453857421875, 'referece_logps/rejected': -269.3585205078125, 'referece_logps/chosen': -393.4585876464844, 'logits/rejected': -0.4308750629425049, 'logits/chosen': -0.5608786940574646, 'epoch': 7.1}
[2024-04-06 05:18:06,205] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 19055/24156 [14:13:21<23:01:20, 16.25s/it]
{'loss': 0.4589, 'learning_rate': 1.951597815085217e-06, 'rewards/chosen': -2.0320115089416504, 'rewards/rejected': -4.360626697540283, 'rewards/accuracies': 0.875, 'rewards/margins': 2.328615427017212, 'policy_logps/rejected': -453.337158203125, 'policy_logps/chosen': -401.8875732421875, 'referece_logps/rejected': -409.73089599609375, 'referece_logps/chosen': -381.56744384765625, 'logits/rejected': -0.2467602640390396, 'logits/chosen': -0.320090115070343, 'epoch': 7.1}

 79%|███████▉  | 19056/24156 [14:13:43<25:24:00, 17.93s/it]


 79%|███████▉  | 19058/24156 [14:14:20<25:47:39, 18.21s/it]
{'loss': 0.274, 'learning_rate': 1.951474112639588e-06, 'rewards/chosen': -1.3213521242141724, 'rewards/rejected': -2.5296928882598877, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2083407640457153, 'policy_logps/rejected': -252.7424774169922, 'policy_logps/chosen': -343.3074035644531, 'referece_logps/rejected': -227.445556640625, 'referece_logps/chosen': -330.0938720703125, 'logits/rejected': 0.2597598135471344, 'logits/chosen': 0.1835412234067917, 'epoch': 7.1}

 79%|███████▉  | 19059/24156 [14:14:36<25:09:15, 17.77s/it]


 79%|███████▉  | 19061/24156 [14:15:13<25:31:54, 18.04s/it]
{'loss': 0.4031, 'learning_rate': 1.951350256251799e-06, 'rewards/chosen': -1.8269128799438477, 'rewards/rejected': -2.836419105529785, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0095059871673584, 'policy_logps/rejected': -421.6566162109375, 'policy_logps/chosen': -453.56591796875, 'referece_logps/rejected': -393.2924499511719, 'referece_logps/chosen': -435.2967834472656, 'logits/rejected': 0.38035109639167786, 'logits/chosen': 0.4468908905982971, 'epoch': 7.1}


 79%|███████▉  | 19063/24156 [14:15:48<24:54:34, 17.61s/it]
{'loss': 0.3711, 'learning_rate': 1.951267599813079e-06, 'rewards/chosen': -1.8781754970550537, 'rewards/rejected': -3.655078887939453, 'rewards/accuracies': 0.875, 'rewards/margins': 1.776903510093689, 'policy_logps/rejected': -323.83721923828125, 'policy_logps/chosen': -376.3892517089844, 'referece_logps/rejected': -287.2864074707031, 'referece_logps/chosen': -357.6074523925781, 'logits/rejected': -0.725718080997467, 'logits/chosen': -0.7362667918205261, 'epoch': 7.1}

 79%|███████▉  | 19064/24156 [14:16:02<23:36:41, 16.69s/it]

 79%|███████▉  | 19065/24156 [14:16:23<25:11:12, 17.81s/it]

 79%|███████▉  | 19066/24156 [14:16:42<25:47:05, 18.24s/it]

 79%|███████▉  | 19067/24156 [14:16:59<25:29:06, 18.03s/it]

 79%|███████▉  | 19068/24156 [14:17:15<24:39:30, 17.45s/it]

 79%|███████▉  | 19069/24156 [14:17:36<26:01:55, 18.42s/it]

 79%|███████▉  | 19070/24156 [14:17:55<26:08:03, 18.50s/it]

 79%|███████▉  | 19071/24156 [14:18:11<25:20:16, 17.94s/it]

 79%|███████▉  | 19072/24156 [14:18:24<23:07:00, 16.37s/it]

 79%|███████▉  | 19073/24156 [14:18:43<24:16:04, 17.19s/it]
[2024-04-06 05:24:02,846] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19074/24156 [14:19:05<26:22:23, 18.68s/it]
[2024-04-06 05:24:21,351] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19075/24156 [14:19:24<26:17:34, 18.63s/it]

 79%|███████▉  | 19076/24156 [14:19:46<27:37:49, 19.58s/it]

 79%|███████▉  | 19077/24156 [14:20:06<27:44:13, 19.66s/it]

 79%|███████▉  | 19078/24156 [14:20:26<28:08:10, 19.95s/it]

 79%|███████▉  | 19079/24156 [14:20:46<28:11:36, 19.99s/it]

 79%|███████▉  | 19080/24156 [14:21:03<26:45:51, 18.98s/it]

 79%|███████▉  | 19081/24156 [14:21:18<25:03:48, 17.78s/it]

 79%|███████▉  | 19082/24156 [14:21:38<26:06:30, 18.52s/it]

 79%|███████▉  | 19083/24156 [14:21:58<26:38:30, 18.91s/it]

 79%|███████▉  | 19084/24156 [14:22:09<23:22:03, 16.59s/it]

 79%|███████▉  | 19085/24156 [14:22:20<21:05:22, 14.97s/it]

 79%|███████▉  | 19086/24156 [14:22:36<21:28:09, 15.24s/it]

 79%|███████▉  | 19087/24156 [14:22:49<20:23:09, 14.48s/it]

 79%|███████▉  | 19088/24156 [14:23:07<22:04:54, 15.69s/it]

 79%|███████▉  | 19089/24156 [14:23:31<25:15:50, 17.95s/it]

 79%|███████▉  | 19090/24156 [14:23:48<25:11:55, 17.91s/it]
[2024-04-06 05:29:08,074] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19091/24156 [14:24:11<27:00:44, 19.20s/it]

 79%|███████▉  | 19092/24156 [14:24:30<27:06:12, 19.27s/it]

 79%|███████▉  | 19093/24156 [14:24:48<26:29:33, 18.84s/it]

 79%|███████▉  | 19094/24156 [14:25:02<24:41:54, 17.57s/it]

 79%|███████▉  | 19095/24156 [14:25:24<26:10:08, 18.61s/it]

 79%|███████▉  | 19096/24156 [14:25:37<24:00:35, 17.08s/it]

 79%|███████▉  | 19097/24156 [14:25:55<24:27:34, 17.41s/it]

 79%|███████▉  | 19098/24156 [14:26:11<23:40:27, 16.85s/it]

 79%|███████▉  | 19099/24156 [14:26:31<24:57:46, 17.77s/it]

 79%|███████▉  | 19100/24156 [14:26:42<22:03:11, 15.70s/it]

 79%|███████▉  | 19101/24156 [14:27:01<23:31:13, 16.75s/it]

 79%|███████▉  | 19102/24156 [14:27:20<24:29:34, 17.45s/it]

 79%|███████▉  | 19103/24156 [14:27:38<24:36:42, 17.53s/it]

 79%|███████▉  | 19104/24156 [14:27:54<24:09:22, 17.21s/it]

 79%|███████▉  | 19105/24156 [14:28:11<23:53:28, 17.03s/it]

 79%|███████▉  | 19106/24156 [14:28:21<21:12:04, 15.11s/it]

 79%|███████▉  | 19107/24156 [14:28:32<19:33:31, 13.95s/it]

 79%|███████▉  | 19108/24156 [14:28:50<20:55:14, 14.92s/it]


 79%|███████▉  | 19110/24156 [14:29:23<21:39:21, 15.45s/it]
{'loss': 0.282, 'learning_rate': 1.949305494634155e-06, 'rewards/chosen': -2.172529458999634, 'rewards/rejected': -4.600896835327148, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4283673763275146, 'policy_logps/rejected': -449.777587890625, 'policy_logps/chosen': -260.927978515625, 'referece_logps/rejected': -403.7685546875, 'referece_logps/chosen': -239.20265197753906, 'logits/rejected': 0.04432370513677597, 'logits/chosen': -0.10638723522424698, 'epoch': 7.12}

 79%|███████▉  | 19111/24156 [14:29:42<23:23:46, 16.70s/it]

 79%|███████▉  | 19112/24156 [14:30:04<25:29:27, 18.19s/it]

 79%|███████▉  | 19113/24156 [14:30:22<25:31:28, 18.22s/it]

 79%|███████▉  | 19114/24156 [14:30:39<24:55:20, 17.79s/it]

 79%|███████▉  | 19115/24156 [14:30:52<23:05:40, 16.49s/it]

 79%|███████▉  | 19116/24156 [14:31:10<23:29:39, 16.78s/it]

 79%|███████▉  | 19117/24156 [14:31:22<21:24:07, 15.29s/it]

 79%|███████▉  | 19118/24156 [14:31:36<20:54:20, 14.94s/it]

 79%|███████▉  | 19119/24156 [14:31:58<23:57:14, 17.12s/it]

 79%|███████▉  | 19120/24156 [14:32:18<24:59:35, 17.87s/it]

 79%|███████▉  | 19121/24156 [14:32:39<26:18:52, 18.81s/it]

 79%|███████▉  | 19122/24156 [14:32:54<24:58:03, 17.86s/it]

 79%|███████▉  | 19123/24156 [14:33:07<22:36:17, 16.17s/it]

 79%|███████▉  | 19124/24156 [14:33:17<20:14:08, 14.48s/it]

 79%|███████▉  | 19125/24156 [14:33:36<22:06:50, 15.82s/it]

 79%|███████▉  | 19126/24156 [14:33:48<20:26:58, 14.64s/it]

 79%|███████▉  | 19127/24156 [14:34:09<23:14:11, 16.63s/it]

 79%|███████▉  | 19128/24156 [14:34:27<23:35:50, 16.90s/it]

 79%|███████▉  | 19129/24156 [14:34:45<24:04:27, 17.24s/it]

 79%|███████▉  | 19130/24156 [14:35:05<25:09:40, 18.02s/it]

 79%|███████▉  | 19131/24156 [14:35:20<24:12:39, 17.35s/it]

 79%|███████▉  | 19132/24156 [14:35:42<25:53:46, 18.56s/it]

 79%|███████▉  | 19133/24156 [14:35:57<24:26:59, 17.52s/it]

 79%|███████▉  | 19134/24156 [14:36:09<21:59:45, 15.77s/it]

 79%|███████▉  | 19135/24156 [14:36:19<19:52:03, 14.24s/it]

 79%|███████▉  | 19136/24156 [14:36:38<21:40:38, 15.55s/it]

 79%|███████▉  | 19137/24156 [14:36:53<21:42:55, 15.58s/it]

 79%|███████▉  | 19138/24156 [14:37:08<21:10:23, 15.19s/it]

 79%|███████▉  | 19139/24156 [14:37:24<21:25:12, 15.37s/it]

 79%|███████▉  | 19140/24156 [14:37:40<21:41:38, 15.57s/it]

 79%|███████▉  | 19141/24156 [14:37:57<22:21:44, 16.05s/it]

 79%|███████▉  | 19142/24156 [14:38:14<22:59:00, 16.50s/it]

 79%|███████▉  | 19143/24156 [14:38:34<24:26:10, 17.55s/it]

 79%|███████▉  | 19144/24156 [14:38:48<22:50:17, 16.40s/it]

 79%|███████▉  | 19145/24156 [14:39:10<24:58:28, 17.94s/it]

 79%|███████▉  | 19146/24156 [14:39:29<25:46:17, 18.52s/it]

 79%|███████▉  | 19147/24156 [14:39:49<26:08:17, 18.79s/it]


 79%|███████▉  | 19149/24156 [14:40:23<24:47:39, 17.83s/it]

 79%|███████▉  | 19150/24156 [14:40:37<23:15:04, 16.72s/it]

 79%|███████▉  | 19151/24156 [14:40:57<24:29:45, 17.62s/it]

 79%|███████▉  | 19152/24156 [14:41:14<24:16:37, 17.47s/it]

 79%|███████▉  | 19153/24156 [14:41:33<25:09:54, 18.11s/it]

 79%|███████▉  | 19154/24156 [14:41:50<24:26:26, 17.59s/it]

 79%|███████▉  | 19155/24156 [14:42:05<23:18:46, 16.78s/it]

 79%|███████▉  | 19156/24156 [14:42:22<23:24:40, 16.86s/it]

 79%|███████▉  | 19157/24156 [14:42:40<23:58:12, 17.26s/it]

 79%|███████▉  | 19158/24156 [14:42:53<22:24:45, 16.14s/it]

 79%|███████▉  | 19159/24156 [14:43:14<24:16:13, 17.49s/it]

 79%|███████▉  | 19160/24156 [14:43:36<26:04:40, 18.79s/it]
[2024-04-06 05:48:33,352] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19161/24156 [14:43:51<24:36:29, 17.74s/it]
[2024-04-06 05:48:48,625] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19162/24156 [14:44:07<23:52:15, 17.21s/it]

 79%|███████▉  | 19163/24156 [14:44:28<25:12:51, 18.18s/it]

 79%|███████▉  | 19164/24156 [14:44:45<24:48:27, 17.89s/it]

 79%|███████▉  | 19165/24156 [14:45:03<24:59:00, 18.02s/it]

 79%|███████▉  | 19166/24156 [14:45:22<25:20:53, 18.29s/it]
[2024-04-06 05:50:19,497] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19167/24156 [14:45:39<24:39:05, 17.79s/it]

 79%|███████▉  | 19168/24156 [14:45:56<24:39:54, 17.80s/it]

 79%|███████▉  | 19169/24156 [14:46:14<24:43:20, 17.85s/it]
[2024-04-06 05:51:11,905] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19170/24156 [14:46:37<26:29:51, 19.13s/it]

 79%|███████▉  | 19171/24156 [14:46:57<27:07:35, 19.59s/it]

 79%|███████▉  | 19172/24156 [14:47:18<27:31:31, 19.88s/it]

 79%|███████▉  | 19173/24156 [14:47:36<27:01:39, 19.53s/it]

 79%|███████▉  | 19174/24156 [14:47:49<24:04:56, 17.40s/it]

 79%|███████▉  | 19175/24156 [14:48:00<21:30:49, 15.55s/it]

 79%|███████▉  | 19176/24156 [14:48:13<20:15:25, 14.64s/it]

 79%|███████▉  | 19177/24156 [14:48:33<22:44:11, 16.44s/it]
[2024-04-06 05:53:30,786] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 19178/24156 [14:48:45<20:36:29, 14.90s/it]

 79%|███████▉  | 19179/24156 [14:49:02<21:47:57, 15.77s/it]

 79%|███████▉  | 19180/24156 [14:49:15<20:23:05, 14.75s/it]

 79%|███████▉  | 19181/24156 [14:49:37<23:25:40, 16.95s/it]

 79%|███████▉  | 19182/24156 [14:49:58<24:59:11, 18.08s/it]

 79%|███████▉  | 19183/24156 [14:50:10<22:34:45, 16.35s/it]

 79%|███████▉  | 19184/24156 [14:50:28<23:08:02, 16.75s/it]
{'loss': 0.302, 'learning_rate': 1.946139850525381e-06, 'rewards/chosen': -1.490188479423523, 'rewards/rejected': -3.7476446628570557, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2574563026428223, 'policy_logps/rejected': -282.42095947265625, 'policy_logps/chosen': -280.0159912109375, 'referece_logps/rejected': -244.94451904296875, 'referece_logps/chosen': -265.1141052246094, 'logits/rejected': -0.9958626627922058, 'logits/chosen': -0.9533416032791138, 'epoch': 7.15}


 79%|███████▉  | 19186/24156 [14:51:00<23:08:54, 16.77s/it]

 79%|███████▉  | 19187/24156 [14:51:13<21:38:22, 15.68s/it]

 79%|███████▉  | 19188/24156 [14:51:30<21:57:58, 15.92s/it]
{'loss': 0.1987, 'learning_rate': 1.9459660784364274e-06, 'rewards/chosen': -2.2627079486846924, 'rewards/rejected': -4.549084186553955, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2863762378692627, 'policy_logps/rejected': -296.42333984375, 'policy_logps/chosen': -404.24383544921875, 'referece_logps/rejected': -250.93251037597656, 'referece_logps/chosen': -381.6167907714844, 'logits/rejected': -0.9174646139144897, 'logits/chosen': -0.7994422912597656, 'epoch': 7.15}


 79%|███████▉  | 19190/24156 [14:51:58<20:29:58, 14.86s/it]

 79%|███████▉  | 19191/24156 [14:52:16<21:42:53, 15.74s/it]

 79%|███████▉  | 19192/24156 [14:52:33<22:16:00, 16.15s/it]
{'loss': 0.3412, 'learning_rate': 1.9457920342568146e-06, 'rewards/chosen': -2.179922580718994, 'rewards/rejected': -3.6352198123931885, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4552972316741943, 'policy_logps/rejected': -344.48040771484375, 'policy_logps/chosen': -317.21484375, 'referece_logps/rejected': -308.12823486328125, 'referece_logps/chosen': -295.4156494140625, 'logits/rejected': -0.9153730869293213, 'logits/chosen': -0.8738422393798828, 'epoch': 7.15}


 79%|███████▉  | 19194/24156 [14:53:09<23:18:13, 16.91s/it]

 79%|███████▉  | 19195/24156 [14:53:31<25:08:38, 18.25s/it]

 79%|███████▉  | 19196/24156 [14:53:49<25:13:03, 18.30s/it]

 79%|███████▉  | 19197/24156 [14:54:09<25:59:25, 18.87s/it]

 79%|███████▉  | 19198/24156 [14:54:29<26:16:08, 19.07s/it]

 79%|███████▉  | 19199/24156 [14:54:50<27:06:12, 19.68s/it]
{'loss': 0.3542, 'learning_rate': 1.945486802374961e-06, 'rewards/chosen': -1.7629947662353516, 'rewards/rejected': -4.9079060554504395, 'rewards/accuracies': 0.875, 'rewards/margins': 3.144911289215088, 'policy_logps/rejected': -422.775146484375, 'policy_logps/chosen': -320.85137939453125, 'referece_logps/rejected': -373.6961364746094, 'referece_logps/chosen': -303.221435546875, 'logits/rejected': -0.881388783454895, 'logits/chosen': -0.9096680879592896, 'epoch': 7.15}
[2024-04-06 06:00:09,952] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 19201/24156 [14:55:32<27:52:40, 20.25s/it]

 79%|███████▉  | 19202/24156 [14:55:52<27:49:34, 20.22s/it]

 79%|███████▉  | 19203/24156 [14:56:13<28:03:00, 20.39s/it]
{'loss': 0.2832, 'learning_rate': 1.945312010204397e-06, 'rewards/chosen': -1.614868402481079, 'rewards/rejected': -4.639109134674072, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0242409706115723, 'policy_logps/rejected': -376.92095947265625, 'policy_logps/chosen': -366.2643127441406, 'referece_logps/rejected': -330.52984619140625, 'referece_logps/chosen': -350.1156311035156, 'logits/rejected': -0.13168847560882568, 'logits/chosen': -0.21867969632148743, 'epoch': 7.15}


 80%|███████▉  | 19205/24156 [14:56:42<23:37:31, 17.18s/it]

 80%|███████▉  | 19206/24156 [14:56:53<21:00:49, 15.28s/it]

 80%|███████▉  | 19207/24156 [14:57:04<19:04:22, 13.87s/it]

 80%|███████▉  | 19208/24156 [14:57:16<18:26:07, 13.41s/it]

 80%|███████▉  | 19209/24156 [14:57:36<21:01:13, 15.30s/it]

 80%|███████▉  | 19210/24156 [14:57:58<23:55:37, 17.42s/it]
{'loss': 0.3553, 'learning_rate': 1.9450054696707402e-06, 'rewards/chosen': -1.1756999492645264, 'rewards/rejected': -3.482858657836914, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3071582317352295, 'policy_logps/rejected': -251.4114990234375, 'policy_logps/chosen': -378.02130126953125, 'referece_logps/rejected': -216.5829315185547, 'referece_logps/chosen': -366.2642822265625, 'logits/rejected': 0.23631352186203003, 'logits/chosen': 0.09044039249420166, 'epoch': 7.16}
[2024-04-06 06:03:16,087] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 19212/24156 [14:58:34<24:04:00, 17.52s/it]
{'loss': 0.2904, 'learning_rate': 1.9449177337537285e-06, 'rewards/chosen': -2.246671438217163, 'rewards/rejected': -5.084702014923096, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8380305767059326, 'policy_logps/rejected': -455.26300048828125, 'policy_logps/chosen': -375.0746765136719, 'referece_logps/rejected': -404.416015625, 'referece_logps/chosen': -352.6079406738281, 'logits/rejected': -0.5885992646217346, 'logits/chosen': -0.4992704689502716, 'epoch': 7.16}

 80%|███████▉  | 19213/24156 [14:58:47<22:00:42, 16.03s/it]

 80%|███████▉  | 19214/24156 [14:59:09<24:27:17, 17.81s/it]

 80%|███████▉  | 19215/24156 [14:59:21<22:05:59, 16.10s/it]


 80%|███████▉  | 19217/24156 [14:59:57<23:42:16, 17.28s/it]
{'loss': 0.3693, 'learning_rate': 1.9446980967056564e-06, 'rewards/chosen': -1.9445163011550903, 'rewards/rejected': -3.445394277572632, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5008782148361206, 'policy_logps/rejected': -398.2784118652344, 'policy_logps/chosen': -305.3081359863281, 'referece_logps/rejected': -363.8244934082031, 'referece_logps/chosen': -285.86297607421875, 'logits/rejected': -0.43388232588768005, 'logits/chosen': -0.33562570810317993, 'epoch': 7.16}

 80%|███████▉  | 19218/24156 [15:00:13<22:57:43, 16.74s/it]


 80%|███████▉  | 19220/24156 [15:00:51<24:37:50, 17.96s/it]

 80%|███████▉  | 19221/24156 [15:01:04<22:35:21, 16.48s/it]

 80%|███████▉  | 19222/24156 [15:01:16<20:38:49, 15.06s/it]

 80%|███████▉  | 19223/24156 [15:01:30<19:55:48, 14.54s/it]

 80%|███████▉  | 19224/24156 [15:01:49<22:02:33, 16.09s/it]

 80%|███████▉  | 19225/24156 [15:02:12<24:41:42, 18.03s/it]
[2024-04-06 06:07:09,291] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 19226/24156 [15:02:34<26:12:52, 19.14s/it]
[2024-04-06 06:07:31,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3316, 'learning_rate': 1.9443016801646826e-06, 'rewards/chosen': -1.5738295316696167, 'rewards/rejected': -3.000279188156128, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4264497756958008, 'policy_logps/rejected': -440.63934326171875, 'policy_logps/chosen': -323.84478759765625, 'referece_logps/rejected': -410.63653564453125, 'referece_logps/chosen': -308.1064758300781, 'logits/rejected': -0.6850733160972595, 'logits/chosen': -0.5696589946746826, 'epoch': 7.16}


 80%|███████▉  | 19228/24156 [15:03:07<24:47:16, 18.11s/it]

 80%|███████▉  | 19229/24156 [15:03:18<21:45:45, 15.90s/it]

 80%|███████▉  | 19230/24156 [15:03:33<21:46:35, 15.91s/it]

 80%|███████▉  | 19231/24156 [15:03:48<21:02:43, 15.38s/it]

 80%|███████▉  | 19232/24156 [15:03:58<19:05:58, 13.96s/it]
{'loss': 0.3605, 'learning_rate': 1.944036638526524e-06, 'rewards/chosen': -1.3949711322784424, 'rewards/rejected': -2.456382989883423, 'rewards/accuracies': 0.75, 'rewards/margins': 1.061411738395691, 'policy_logps/rejected': -515.8948974609375, 'policy_logps/chosen': -522.6671142578125, 'referece_logps/rejected': -491.3310241699219, 'referece_logps/chosen': -508.71746826171875, 'logits/rejected': 0.14851446449756622, 'logits/chosen': 0.20361606776714325, 'epoch': 7.17}


 80%|███████▉  | 19234/24156 [15:04:20<16:50:05, 12.31s/it]

 80%|███████▉  | 19235/24156 [15:04:31<16:12:46, 11.86s/it]

 80%|███████▉  | 19236/24156 [15:04:42<15:50:05, 11.59s/it]

 80%|███████▉  | 19237/24156 [15:04:53<15:40:35, 11.47s/it]
{'loss': 0.3887, 'learning_rate': 1.943815303785481e-06, 'rewards/chosen': -1.793681263923645, 'rewards/rejected': -4.223514556884766, 'rewards/accuracies': 1.0, 'rewards/margins': 2.42983341217041, 'policy_logps/rejected': -435.3526306152344, 'policy_logps/chosen': -343.747802734375, 'referece_logps/rejected': -393.1174621582031, 'referece_logps/chosen': -325.8110046386719, 'logits/rejected': -0.5592344403266907, 'logits/chosen': -0.1725434958934784, 'epoch': 7.17}

 80%|███████▉  | 19238/24156 [15:05:03<15:19:46, 11.22s/it]

 80%|███████▉  | 19239/24156 [15:05:23<18:47:14, 13.76s/it]


 80%|███████▉  | 19241/24156 [15:05:58<21:20:19, 15.63s/it]

 80%|███████▉  | 19242/24156 [15:06:16<22:37:25, 16.57s/it]
[2024-04-06 06:11:13,845] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3381, 'learning_rate': 1.943593544869403e-06, 'rewards/chosen': -3.342207193374634, 'rewards/rejected': -4.817405700683594, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4751986265182495, 'policy_logps/rejected': -568.2395629882812, 'policy_logps/chosen': -519.70703125, 'referece_logps/rejected': -520.0654296875, 'referece_logps/chosen': -486.28497314453125, 'logits/rejected': 0.10760192573070526, 'logits/chosen': 0.0629606693983078, 'epoch': 7.17}

 80%|███████▉  | 19243/24156 [15:06:37<24:24:55, 17.89s/it]

 80%|███████▉  | 19244/24156 [15:06:57<25:10:30, 18.45s/it]


 80%|███████▉  | 19246/24156 [15:07:34<25:16:45, 18.53s/it]

 80%|███████▉  | 19247/24156 [15:07:53<25:13:58, 18.50s/it]

 80%|███████▉  | 19248/24156 [15:08:09<24:16:03, 17.80s/it]

 80%|███████▉  | 19249/24156 [15:08:29<25:01:46, 18.36s/it]
{'loss': 0.3202, 'learning_rate': 1.943282369962631e-06, 'rewards/chosen': -2.590775489807129, 'rewards/rejected': -4.345162868499756, 'rewards/accuracies': 0.875, 'rewards/margins': 1.754387617111206, 'policy_logps/rejected': -420.916259765625, 'policy_logps/chosen': -516.4259643554688, 'referece_logps/rejected': -377.46466064453125, 'referece_logps/chosen': -490.5181579589844, 'logits/rejected': -0.19630733132362366, 'logits/chosen': -0.2563731074333191, 'epoch': 7.17}

 80%|███████▉  | 19250/24156 [15:08:43<23:32:57, 17.28s/it]


 80%|███████▉  | 19252/24156 [15:09:21<24:37:20, 18.08s/it]

 80%|███████▉  | 19253/24156 [15:09:32<21:44:09, 15.96s/it]
{'loss': 0.4613, 'learning_rate': 1.9431041826493336e-06, 'rewards/chosen': -1.5918598175048828, 'rewards/rejected': -2.511583089828491, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9197233319282532, 'policy_logps/rejected': -371.93756103515625, 'policy_logps/chosen': -401.4783935546875, 'referece_logps/rejected': -346.82177734375, 'referece_logps/chosen': -385.559814453125, 'logits/rejected': -0.9205724000930786, 'logits/chosen': -0.9075171947479248, 'epoch': 7.17}

 80%|███████▉  | 19254/24156 [15:09:51<23:12:16, 17.04s/it]


 80%|███████▉  | 19256/24156 [15:10:14<19:08:58, 14.07s/it]
{'loss': 0.2977, 'learning_rate': 1.9429703641422676e-06, 'rewards/chosen': -1.9055756330490112, 'rewards/rejected': -4.238223075866699, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3326473236083984, 'policy_logps/rejected': -538.9217529296875, 'policy_logps/chosen': -545.78076171875, 'referece_logps/rejected': -496.53955078125, 'referece_logps/chosen': -526.7250366210938, 'logits/rejected': -0.4649357199668884, 'logits/chosen': -0.35441750288009644, 'epoch': 7.17}


 80%|███████▉  | 19258/24156 [15:10:52<22:26:08, 16.49s/it]

 80%|███████▉  | 19259/24156 [15:11:11<23:15:22, 17.10s/it]

 80%|███████▉  | 19260/24156 [15:11:31<24:22:36, 17.92s/it]
{'loss': 0.4394, 'learning_rate': 1.9427917021445534e-06, 'rewards/chosen': -1.7513644695281982, 'rewards/rejected': -3.037997245788574, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2866325378417969, 'policy_logps/rejected': -386.9354248046875, 'policy_logps/chosen': -325.015869140625, 'referece_logps/rejected': -356.5555114746094, 'referece_logps/chosen': -307.5022277832031, 'logits/rejected': -0.4133652150630951, 'logits/chosen': -0.40897732973098755, 'epoch': 7.18}


 80%|███████▉  | 19262/24156 [15:12:07<24:18:34, 17.88s/it]

 80%|███████▉  | 19263/24156 [15:12:24<23:58:41, 17.64s/it]

 80%|███████▉  | 19264/24156 [15:12:41<23:28:27, 17.27s/it]

 80%|███████▉  | 19265/24156 [15:12:59<23:51:48, 17.56s/it]
{'loss': 0.3313, 'learning_rate': 1.9425679933099324e-06, 'rewards/chosen': -2.0365724563598633, 'rewards/rejected': -3.1471428871154785, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1105706691741943, 'policy_logps/rejected': -371.4645690917969, 'policy_logps/chosen': -380.18377685546875, 'referece_logps/rejected': -339.9931640625, 'referece_logps/chosen': -359.8180236816406, 'logits/rejected': 0.04815392941236496, 'logits/chosen': 0.09859626740217209, 'epoch': 7.18}


 80%|███████▉  | 19267/24156 [15:13:38<25:27:16, 18.74s/it]

 80%|███████▉  | 19268/24156 [15:13:54<24:21:27, 17.94s/it]

 80%|███████▉  | 19269/24156 [15:14:09<23:08:15, 17.04s/it]

 80%|███████▉  | 19270/24156 [15:14:21<21:00:30, 15.48s/it]

 80%|███████▉  | 19271/24156 [15:14:33<19:33:54, 14.42s/it]
{'loss': 0.3406, 'learning_rate': 1.9422989835461586e-06, 'rewards/chosen': -1.3074227571487427, 'rewards/rejected': -3.179558277130127, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8721355199813843, 'policy_logps/rejected': -392.2313537597656, 'policy_logps/chosen': -494.01776123046875, 'referece_logps/rejected': -360.4357604980469, 'referece_logps/chosen': -480.9434814453125, 'logits/rejected': 0.13465365767478943, 'logits/chosen': 0.23478423058986664, 'epoch': 7.18}


 80%|███████▉  | 19273/24156 [15:15:10<21:57:00, 16.18s/it]
{'loss': 0.4422, 'learning_rate': 1.9422091780983544e-06, 'rewards/chosen': -1.843043327331543, 'rewards/rejected': -3.50809383392334, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6650502681732178, 'policy_logps/rejected': -446.47930908203125, 'policy_logps/chosen': -456.3894348144531, 'referece_logps/rejected': -411.3983459472656, 'referece_logps/chosen': -437.9590148925781, 'logits/rejected': -0.35604044795036316, 'logits/chosen': -0.4417920708656311, 'epoch': 7.18}

 80%|███████▉  | 19274/24156 [15:15:26<21:37:25, 15.95s/it]


 80%|███████▉  | 19276/24156 [15:15:54<20:10:31, 14.88s/it]

 80%|███████▉  | 19277/24156 [15:16:10<20:55:57, 15.45s/it]
{'loss': 0.3026, 'learning_rate': 1.9420293639516658e-06, 'rewards/chosen': -1.8005061149597168, 'rewards/rejected': -4.457509994506836, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6570043563842773, 'policy_logps/rejected': -278.74853515625, 'policy_logps/chosen': -242.69583129882812, 'referece_logps/rejected': -234.17343139648438, 'referece_logps/chosen': -224.69076538085938, 'logits/rejected': -0.5630474090576172, 'logits/chosen': -0.5947475433349609, 'epoch': 7.18}
[2024-04-06 06:21:29,694] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 19279/24156 [15:16:53<24:53:41, 18.38s/it]
[2024-04-06 06:21:50,364] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3043, 'learning_rate': 1.9419393552657114e-06, 'rewards/chosen': -1.8475819826126099, 'rewards/rejected': -4.615478992462158, 'rewards/accuracies': 1.0, 'rewards/margins': 2.767897367477417, 'policy_logps/rejected': -440.2877197265625, 'policy_logps/chosen': -440.44696044921875, 'referece_logps/rejected': -394.1329650878906, 'referece_logps/chosen': -421.9711608886719, 'logits/rejected': 0.7015938758850098, 'logits/chosen': 0.6485123634338379, 'epoch': 7.18}


 80%|███████▉  | 19281/24156 [15:17:25<23:17:58, 17.21s/it]

 80%|███████▉  | 19282/24156 [15:17:43<23:38:46, 17.47s/it]
[2024-04-06 06:22:40,254] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 19283/24156 [15:18:02<24:30:29, 18.11s/it]

 80%|███████▉  | 19284/24156 [15:18:17<23:08:43, 17.10s/it]

 80%|███████▉  | 19285/24156 [15:18:39<25:15:24, 18.67s/it]

 80%|███████▉  | 19286/24156 [15:18:52<22:34:48, 16.69s/it]

 80%|███████▉  | 19287/24156 [15:19:13<24:21:18, 18.01s/it]
[2024-04-06 06:24:10,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 19288/24156 [15:19:29<23:35:50, 17.45s/it]

 80%|███████▉  | 19289/24156 [15:19:51<25:36:19, 18.94s/it]

 80%|███████▉  | 19290/24156 [15:20:09<25:03:16, 18.54s/it]

 80%|███████▉  | 19291/24156 [15:20:30<25:59:01, 19.23s/it]
{'loss': 0.3561, 'learning_rate': 1.941397880981511e-06, 'rewards/chosen': -1.5489445924758911, 'rewards/rejected': -3.1130471229553223, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5641025304794312, 'policy_logps/rejected': -332.8891906738281, 'policy_logps/chosen': -351.7352600097656, 'referece_logps/rejected': -301.7586975097656, 'referece_logps/chosen': -336.24578857421875, 'logits/rejected': -0.04640796035528183, 'logits/chosen': 0.02847592532634735, 'epoch': 7.19}

 80%|███████▉  | 19292/24156 [15:20:42<23:18:02, 17.25s/it]

 80%|███████▉  | 19293/24156 [15:21:05<25:20:31, 18.76s/it]

 80%|███████▉  | 19294/24156 [15:21:19<23:33:01, 17.44s/it]

 80%|███████▉  | 19295/24156 [15:21:32<21:48:58, 16.16s/it]

 80%|███████▉  | 19296/24156 [15:21:54<24:20:24, 18.03s/it]

 80%|███████▉  | 19297/24156 [15:22:07<21:56:29, 16.26s/it]


 80%|███████▉  | 19299/24156 [15:22:42<22:33:31, 16.72s/it]
{'loss': 0.2478, 'learning_rate': 1.9410355441553678e-06, 'rewards/chosen': -2.0167083740234375, 'rewards/rejected': -4.012850761413574, 'rewards/accuracies': 0.875, 'rewards/margins': 1.996142029762268, 'policy_logps/rejected': -340.97686767578125, 'policy_logps/chosen': -341.41070556640625, 'referece_logps/rejected': -300.8483581542969, 'referece_logps/chosen': -321.24359130859375, 'logits/rejected': -0.8281529545783997, 'logits/chosen': -0.8395206332206726, 'epoch': 7.19}

 80%|███████▉  | 19300/24156 [15:23:02<24:06:03, 17.87s/it]

 80%|███████▉  | 19301/24156 [15:23:22<24:56:18, 18.49s/it]

 80%|███████▉  | 19302/24156 [15:23:41<25:08:34, 18.65s/it]


 80%|███████▉  | 19304/24156 [15:24:19<25:32:57, 18.96s/it]

 80%|███████▉  | 19305/24156 [15:24:30<22:09:43, 16.45s/it]

 80%|███████▉  | 19306/24156 [15:24:52<24:15:16, 18.00s/it]

 80%|███████▉  | 19307/24156 [15:25:12<25:11:11, 18.70s/it]

 80%|███████▉  | 19308/24156 [15:25:28<24:07:29, 17.91s/it]
{'loss': 0.3567, 'learning_rate': 1.9406266210940804e-06, 'rewards/chosen': -1.893202781677246, 'rewards/rejected': -4.129308223724365, 'rewards/accuracies': 0.875, 'rewards/margins': 2.23610520362854, 'policy_logps/rejected': -409.529052734375, 'policy_logps/chosen': -352.7160339355469, 'referece_logps/rejected': -368.2359924316406, 'referece_logps/chosen': -333.78399658203125, 'logits/rejected': -0.4716169536113739, 'logits/chosen': -0.4674290418624878, 'epoch': 7.19}

 80%|███████▉  | 19309/24156 [15:25:39<21:26:37, 15.93s/it]

 80%|███████▉  | 19310/24156 [15:25:54<21:09:16, 15.72s/it]

 80%|███████▉  | 19311/24156 [15:26:07<19:45:55, 14.69s/it]

 80%|███████▉  | 19312/24156 [15:26:23<20:32:30, 15.27s/it]

 80%|███████▉  | 19313/24156 [15:26:35<19:12:31, 14.28s/it]


 80%|███████▉  | 19315/24156 [15:27:20<24:33:56, 18.27s/it]

 80%|███████▉  | 19316/24156 [15:27:33<22:42:17, 16.89s/it]
{'loss': 0.4702, 'learning_rate': 1.940261984051901e-06, 'rewards/chosen': -2.477562427520752, 'rewards/rejected': -3.319394111633301, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8418318629264832, 'policy_logps/rejected': -446.4613037109375, 'policy_logps/chosen': -463.31781005859375, 'referece_logps/rejected': -413.26739501953125, 'referece_logps/chosen': -438.5421447753906, 'logits/rejected': -0.5355398058891296, 'logits/chosen': -0.6438961029052734, 'epoch': 7.2}

 80%|███████▉  | 19317/24156 [15:27:47<21:22:38, 15.90s/it]

 80%|███████▉  | 19318/24156 [15:27:59<19:41:44, 14.66s/it]

 80%|███████▉  | 19319/24156 [15:28:19<22:01:39, 16.39s/it]


 80%|███████▉  | 19321/24156 [15:29:04<26:13:40, 19.53s/it]
{'loss': 0.4323, 'learning_rate': 1.9400335365223836e-06, 'rewards/chosen': -2.022463083267212, 'rewards/rejected': -4.235108375549316, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2126455307006836, 'policy_logps/rejected': -342.6755676269531, 'policy_logps/chosen': -348.21185302734375, 'referece_logps/rejected': -300.3244934082031, 'referece_logps/chosen': -327.9872131347656, 'logits/rejected': -0.696842610836029, 'logits/chosen': -0.7473999261856079, 'epoch': 7.2}

 80%|███████▉  | 19322/24156 [15:29:25<26:36:47, 19.82s/it]


 80%|███████▉  | 19324/24156 [15:30:02<25:56:28, 19.33s/it]

 80%|████████  | 19325/24156 [15:30:22<26:13:18, 19.54s/it]
{'loss': 0.3041, 'learning_rate': 1.9398504743115383e-06, 'rewards/chosen': -2.0031089782714844, 'rewards/rejected': -4.13419246673584, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1310834884643555, 'policy_logps/rejected': -354.79620361328125, 'policy_logps/chosen': -246.42054748535156, 'referece_logps/rejected': -313.45428466796875, 'referece_logps/chosen': -226.38946533203125, 'logits/rejected': -0.27694931626319885, 'logits/chosen': -0.2437279373407364, 'epoch': 7.2}

 80%|████████  | 19326/24156 [15:30:42<26:09:18, 19.49s/it]

 80%|████████  | 19327/24156 [15:31:04<27:10:14, 20.26s/it]

 80%|████████  | 19328/24156 [15:31:27<28:29:35, 21.25s/it]


 80%|████████  | 19330/24156 [15:31:52<22:16:44, 16.62s/it]

 80%|████████  | 19331/24156 [15:32:12<23:45:41, 17.73s/it]
{'loss': 0.3051, 'learning_rate': 1.9395753741399737e-06, 'rewards/chosen': -2.0950510501861572, 'rewards/rejected': -5.281303882598877, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1862523555755615, 'policy_logps/rejected': -380.2100830078125, 'policy_logps/chosen': -311.0507507324219, 'referece_logps/rejected': -327.39703369140625, 'referece_logps/chosen': -290.1002502441406, 'logits/rejected': -0.2506129741668701, 'logits/chosen': -0.4139567017555237, 'epoch': 7.2}

 80%|████████  | 19332/24156 [15:32:23<20:55:56, 15.62s/it]

 80%|████████  | 19333/24156 [15:32:42<22:12:10, 16.57s/it]


 80%|████████  | 19335/24156 [15:33:14<21:57:28, 16.40s/it]
{'loss': 0.2708, 'learning_rate': 1.9393916361989957e-06, 'rewards/chosen': -2.2234601974487305, 'rewards/rejected': -3.6642560958862305, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4407954216003418, 'policy_logps/rejected': -310.1278381347656, 'policy_logps/chosen': -381.8502197265625, 'referece_logps/rejected': -273.48529052734375, 'referece_logps/chosen': -359.6156005859375, 'logits/rejected': -0.7897167205810547, 'logits/chosen': -0.7430583238601685, 'epoch': 7.2}

 80%|████████  | 19336/24156 [15:33:31<22:20:21, 16.69s/it]

 80%|████████  | 19337/24156 [15:33:51<23:38:25, 17.66s/it]

 80%|████████  | 19338/24156 [15:34:07<22:53:21, 17.10s/it]


 80%|████████  | 19340/24156 [15:34:44<23:57:31, 17.91s/it]
{'loss': 0.3331, 'learning_rate': 1.9391615838107364e-06, 'rewards/chosen': -1.1729568243026733, 'rewards/rejected': -3.094156503677368, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9211996793746948, 'policy_logps/rejected': -361.03448486328125, 'policy_logps/chosen': -245.4955291748047, 'referece_logps/rejected': -330.0929260253906, 'referece_logps/chosen': -233.76596069335938, 'logits/rejected': -0.9061698913574219, 'logits/chosen': -0.7832253575325012, 'epoch': 7.21}

 80%|████████  | 19341/24156 [15:34:59<22:42:22, 16.98s/it]

 80%|████████  | 19342/24156 [15:35:17<23:11:37, 17.34s/it]

 80%|████████  | 19343/24156 [15:35:35<23:33:40, 17.62s/it]


 80%|████████  | 19345/24156 [15:36:10<23:34:09, 17.64s/it]

 80%|████████  | 19346/24156 [15:36:29<23:55:00, 17.90s/it]
{'loss': 0.3678, 'learning_rate': 1.9388849638036733e-06, 'rewards/chosen': -1.810408115386963, 'rewards/rejected': -4.876992702484131, 'rewards/accuracies': 0.875, 'rewards/margins': 3.066584825515747, 'policy_logps/rejected': -664.6538696289062, 'policy_logps/chosen': -511.8136901855469, 'referece_logps/rejected': -615.884033203125, 'referece_logps/chosen': -493.7096252441406, 'logits/rejected': -0.7332494258880615, 'logits/chosen': -0.6061723232269287, 'epoch': 7.21}


 80%|████████  | 19348/24156 [15:37:02<22:57:31, 17.19s/it]
{'loss': 0.3388, 'learning_rate': 1.9387926220988544e-06, 'rewards/chosen': -2.153803825378418, 'rewards/rejected': -4.3869404792785645, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2331368923187256, 'policy_logps/rejected': -339.9896545410156, 'policy_logps/chosen': -348.3360290527344, 'referece_logps/rejected': -296.1202392578125, 'referece_logps/chosen': -326.79803466796875, 'logits/rejected': -0.018880553543567657, 'logits/chosen': -0.13072393834590912, 'epoch': 7.21}

 80%|████████  | 19349/24156 [15:37:19<22:43:32, 17.02s/it]

 80%|████████  | 19350/24156 [15:37:38<23:32:13, 17.63s/it]


 80%|████████  | 19352/24156 [15:38:23<26:53:24, 20.15s/it]
{'loss': 0.4079, 'learning_rate': 1.9386077361753537e-06, 'rewards/chosen': -1.6635804176330566, 'rewards/rejected': -3.0075628757476807, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3439826965332031, 'policy_logps/rejected': -465.4389343261719, 'policy_logps/chosen': -522.1753540039062, 'referece_logps/rejected': -435.36334228515625, 'referece_logps/chosen': -505.53955078125, 'logits/rejected': 0.28065991401672363, 'logits/chosen': 0.034637004137039185, 'epoch': 7.21}


 80%|████████  | 19354/24156 [15:38:52<22:53:55, 17.17s/it]

 80%|████████  | 19355/24156 [15:39:13<24:16:15, 18.20s/it]
{'loss': 0.3652, 'learning_rate': 1.938468894559274e-06, 'rewards/chosen': -1.816825032234192, 'rewards/rejected': -3.5386528968811035, 'rewards/accuracies': 0.875, 'rewards/margins': 1.721827745437622, 'policy_logps/rejected': -570.693603515625, 'policy_logps/chosen': -340.1971130371094, 'referece_logps/rejected': -535.3070678710938, 'referece_logps/chosen': -322.02886962890625, 'logits/rejected': -0.25640156865119934, 'logits/chosen': -0.14298486709594727, 'epoch': 7.21}

 80%|████████  | 19356/24156 [15:39:25<22:02:17, 16.53s/it]


 80%|████████  | 19358/24156 [15:39:53<19:51:34, 14.90s/it]
{'loss': 0.3587, 'learning_rate': 1.9383299011051914e-06, 'rewards/chosen': -1.0248757600784302, 'rewards/rejected': -3.1401500701904297, 'rewards/accuracies': 0.875, 'rewards/margins': 2.115274429321289, 'policy_logps/rejected': -327.80609130859375, 'policy_logps/chosen': -645.2429809570312, 'referece_logps/rejected': -296.40460205078125, 'referece_logps/chosen': -634.9942626953125, 'logits/rejected': 0.2869895398616791, 'logits/chosen': -0.045311786234378815, 'epoch': 7.21}


 80%|████████  | 19360/24156 [15:40:31<22:31:05, 16.90s/it]

 80%|████████  | 19361/24156 [15:40:47<22:18:11, 16.74s/it]
{'loss': 0.3147, 'learning_rate': 1.938190755835594e-06, 'rewards/chosen': -2.480043649673462, 'rewards/rejected': -5.19447660446167, 'rewards/accuracies': 0.875, 'rewards/margins': 2.714432716369629, 'policy_logps/rejected': -372.447509765625, 'policy_logps/chosen': -401.088134765625, 'referece_logps/rejected': -320.50274658203125, 'referece_logps/chosen': -376.28765869140625, 'logits/rejected': -0.3038414716720581, 'logits/chosen': -0.35816240310668945, 'epoch': 7.21}

 80%|████████  | 19362/24156 [15:41:05<23:00:31, 17.28s/it]

 80%|████████  | 19363/24156 [15:41:24<23:21:14, 17.54s/it]


 80%|████████  | 19365/24156 [15:42:03<25:01:30, 18.80s/it]
{'loss': 0.4306, 'learning_rate': 1.938004992690911e-06, 'rewards/chosen': -1.9366793632507324, 'rewards/rejected': -3.0530495643615723, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1163705587387085, 'policy_logps/rejected': -421.7345275878906, 'policy_logps/chosen': -353.54364013671875, 'referece_logps/rejected': -391.2040100097656, 'referece_logps/chosen': -334.17681884765625, 'logits/rejected': -0.43221426010131836, 'logits/chosen': -0.3916366398334503, 'epoch': 7.21}

 80%|████████  | 19366/24156 [15:42:18<23:29:03, 17.65s/it]


 80%|████████  | 19368/24156 [15:42:53<22:57:20, 17.26s/it]

 80%|████████  | 19369/24156 [15:43:05<21:00:02, 15.79s/it]
{'loss': 0.2807, 'learning_rate': 1.937818959745436e-06, 'rewards/chosen': -1.4646925926208496, 'rewards/rejected': -3.282461643218994, 'rewards/accuracies': 0.875, 'rewards/margins': 1.817768931388855, 'policy_logps/rejected': -300.42840576171875, 'policy_logps/chosen': -399.296875, 'referece_logps/rejected': -267.6037902832031, 'referece_logps/chosen': -384.64996337890625, 'logits/rejected': 0.20139259099960327, 'logits/chosen': 0.29483091831207275, 'epoch': 7.22}

 80%|████████  | 19370/24156 [15:43:24<22:14:26, 16.73s/it]

 80%|████████  | 19371/24156 [15:43:38<20:59:16, 15.79s/it]


 80%|████████  | 19373/24156 [15:44:09<20:38:07, 15.53s/it]
{'loss': 0.3039, 'learning_rate': 1.937632657052679e-06, 'rewards/chosen': -1.5481641292572021, 'rewards/rejected': -2.9073543548583984, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3591903448104858, 'policy_logps/rejected': -328.3735656738281, 'policy_logps/chosen': -409.34759521484375, 'referece_logps/rejected': -299.3000183105469, 'referece_logps/chosen': -393.86590576171875, 'logits/rejected': -0.9099345207214355, 'logits/chosen': -0.944897472858429, 'epoch': 7.22}

 80%|████████  | 19374/24156 [15:44:30<22:48:18, 17.17s/it]

 80%|████████  | 19375/24156 [15:44:52<24:35:44, 18.52s/it]

 80%|████████  | 19376/24156 [15:45:10<24:36:32, 18.53s/it]


 80%|████████  | 19378/24156 [15:45:45<23:23:23, 17.62s/it]

 80%|████████  | 19379/24156 [15:45:57<21:12:14, 15.98s/it]
{'loss': 0.308, 'learning_rate': 1.9373526973546308e-06, 'rewards/chosen': -1.9779791831970215, 'rewards/rejected': -3.3346893787384033, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3567105531692505, 'policy_logps/rejected': -295.04052734375, 'policy_logps/chosen': -251.81167602539062, 'referece_logps/rejected': -261.6936340332031, 'referece_logps/chosen': -232.03187561035156, 'logits/rejected': 0.12608836591243744, 'logits/chosen': 0.10738788545131683, 'epoch': 7.22}

 80%|████████  | 19380/24156 [15:46:12<20:47:49, 15.68s/it]
[2024-04-06 06:51:33,679] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|████████  | 19382/24156 [15:46:57<25:09:11, 18.97s/it]
[2024-04-06 06:51:54,312] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 19383/24156 [15:47:13<24:06:25, 18.18s/it]

 80%|████████  | 19384/24156 [15:47:35<25:43:44, 19.41s/it]

 80%|████████  | 19385/24156 [15:47:47<22:37:09, 17.07s/it]
{'loss': 0.2848, 'learning_rate': 1.937072131026968e-06, 'rewards/chosen': -2.2120656967163086, 'rewards/rejected': -4.120304584503174, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9082388877868652, 'policy_logps/rejected': -347.6674499511719, 'policy_logps/chosen': -554.1170043945312, 'referece_logps/rejected': -306.46441650390625, 'referece_logps/chosen': -531.996337890625, 'logits/rejected': -0.66202712059021, 'logits/chosen': -0.8878998756408691, 'epoch': 7.22}


 80%|████████  | 19387/24156 [15:48:17<21:43:13, 16.40s/it]
{'loss': 0.3326, 'learning_rate': 1.9369784741425377e-06, 'rewards/chosen': -1.7185086011886597, 'rewards/rejected': -2.949469566345215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2309610843658447, 'policy_logps/rejected': -625.3006591796875, 'policy_logps/chosen': -593.0807495117188, 'referece_logps/rejected': -595.8059692382812, 'referece_logps/chosen': -575.8956909179688, 'logits/rejected': 0.7711583375930786, 'logits/chosen': 0.685814380645752, 'epoch': 7.22}

 80%|████████  | 19388/24156 [15:48:31<20:38:00, 15.58s/it]

 80%|████████  | 19389/24156 [15:48:52<22:53:41, 17.29s/it]


 80%|████████  | 19391/24156 [15:49:23<22:19:11, 16.86s/it]
{'loss': 0.2612, 'learning_rate': 1.936790958251265e-06, 'rewards/chosen': -1.6390914916992188, 'rewards/rejected': -3.4234437942504883, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7843520641326904, 'policy_logps/rejected': -498.8611755371094, 'policy_logps/chosen': -493.0282287597656, 'referece_logps/rejected': -464.6267395019531, 'referece_logps/chosen': -476.6372985839844, 'logits/rejected': 0.21182985603809357, 'logits/chosen': 0.28218191862106323, 'epoch': 7.22}

 80%|████████  | 19392/24156 [15:49:43<23:23:45, 17.68s/it]


 80%|████████  | 19394/24156 [15:50:11<21:13:00, 16.04s/it]
{'loss': 0.3111, 'learning_rate': 1.9366501445022486e-06, 'rewards/chosen': -1.50058913230896, 'rewards/rejected': -3.7188687324523926, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2182791233062744, 'policy_logps/rejected': -386.4908752441406, 'policy_logps/chosen': -465.05621337890625, 'referece_logps/rejected': -349.3022155761719, 'referece_logps/chosen': -450.05029296875, 'logits/rejected': -0.0833110511302948, 'logits/chosen': -0.13943713903427124, 'epoch': 7.23}

 80%|████████  | 19395/24156 [15:50:33<23:21:17, 17.66s/it]

 80%|████████  | 19396/24156 [15:50:53<24:16:35, 18.36s/it]


 80%|████████  | 19398/24156 [15:51:35<26:02:36, 19.70s/it]
{'loss': 0.2658, 'learning_rate': 1.936462157106125e-06, 'rewards/chosen': -2.0773541927337646, 'rewards/rejected': -3.532097578048706, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4547433853149414, 'policy_logps/rejected': -273.1530456542969, 'policy_logps/chosen': -337.33807373046875, 'referece_logps/rejected': -237.83206176757812, 'referece_logps/chosen': -316.56451416015625, 'logits/rejected': 0.13481618463993073, 'logits/chosen': -0.051394134759902954, 'epoch': 7.23}


 80%|████████  | 19400/24156 [15:52:09<24:10:40, 18.30s/it]
{'loss': 0.299, 'learning_rate': 1.9363680623957975e-06, 'rewards/chosen': -1.0087167024612427, 'rewards/rejected': -3.5572049617767334, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5484886169433594, 'policy_logps/rejected': -364.47589111328125, 'policy_logps/chosen': -377.0019226074219, 'referece_logps/rejected': -328.9038391113281, 'referece_logps/chosen': -366.9147644042969, 'logits/rejected': -0.38769564032554626, 'logits/chosen': -0.48229047656059265, 'epoch': 7.23}


 80%|████████  | 19402/24156 [15:52:48<24:37:33, 18.65s/it]
{'loss': 0.2618, 'learning_rate': 1.9362739003529796e-06, 'rewards/chosen': -1.9659080505371094, 'rewards/rejected': -4.669382095336914, 'rewards/accuracies': 1.0, 'rewards/margins': 2.703474283218384, 'policy_logps/rejected': -384.26580810546875, 'policy_logps/chosen': -266.46990966796875, 'referece_logps/rejected': -337.5719909667969, 'referece_logps/chosen': -246.8108367919922, 'logits/rejected': -0.6326426863670349, 'logits/chosen': -0.5237028002738953, 'epoch': 7.23}

 80%|████████  | 19403/24156 [15:53:09<25:34:50, 19.38s/it]

 80%|████████  | 19404/24156 [15:53:26<24:51:13, 18.83s/it]

 80%|████████  | 19405/24156 [15:53:47<25:36:40, 19.41s/it]

 80%|████████  | 19406/24156 [15:54:05<24:59:43, 18.94s/it]

 80%|████████  | 19407/24156 [15:54:21<23:37:55, 17.91s/it]

 80%|████████  | 19408/24156 [15:54:36<22:27:53, 17.03s/it]

 80%|████████  | 19409/24156 [15:54:47<20:17:05, 15.38s/it]

 80%|████████  | 19410/24156 [15:55:08<22:34:43, 17.13s/it]


 80%|████████  | 19412/24156 [15:55:48<24:23:07, 18.50s/it]
{'loss': 0.379, 'learning_rate': 1.9358020803886883e-06, 'rewards/chosen': -2.121105432510376, 'rewards/rejected': -3.8318467140197754, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7107412815093994, 'policy_logps/rejected': -367.4637145996094, 'policy_logps/chosen': -392.8462219238281, 'referece_logps/rejected': -329.1452331542969, 'referece_logps/chosen': -371.6351623535156, 'logits/rejected': -0.1752457469701767, 'logits/chosen': -0.15759237110614777, 'epoch': 7.23}

 80%|████████  | 19413/24156 [15:56:08<24:46:25, 18.80s/it]
[2024-04-06 07:01:23,982] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 19414/24156 [15:56:26<24:50:03, 18.85s/it]

 80%|████████  | 19415/24156 [15:56:37<21:35:13, 16.39s/it]

 80%|████████  | 19416/24156 [15:56:56<22:26:09, 17.04s/it]

 80%|████████  | 19417/24156 [15:57:13<22:30:54, 17.10s/it]

 80%|████████  | 19418/24156 [15:57:27<21:10:32, 16.09s/it]

 80%|████████  | 19419/24156 [15:57:38<19:12:41, 14.60s/it]

 80%|████████  | 19420/24156 [15:57:53<19:30:47, 14.83s/it]

 80%|████████  | 19421/24156 [15:58:04<18:06:11, 13.76s/it]

 80%|████████  | 19422/24156 [15:58:22<19:45:47, 15.03s/it]

 80%|████████  | 19423/24156 [15:58:42<21:40:45, 16.49s/it]

 80%|████████  | 19424/24156 [15:59:01<22:43:02, 17.28s/it]

 80%|████████  | 19425/24156 [15:59:23<24:20:25, 18.52s/it]

 80%|████████  | 19426/24156 [15:59:40<23:44:53, 18.07s/it]

 80%|████████  | 19427/24156 [15:59:52<21:17:27, 16.21s/it]

 80%|████████  | 19428/24156 [16:00:08<21:08:42, 16.10s/it]

 80%|████████  | 19429/24156 [16:00:28<22:41:13, 17.28s/it]


 80%|████████  | 19431/24156 [16:01:00<22:03:19, 16.80s/it]
{'loss': 0.3578, 'learning_rate': 1.9349009884385103e-06, 'rewards/chosen': -2.4616782665252686, 'rewards/rejected': -4.388726234436035, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9270482063293457, 'policy_logps/rejected': -392.18896484375, 'policy_logps/chosen': -456.8558654785156, 'referece_logps/rejected': -348.3016662597656, 'referece_logps/chosen': -432.2390441894531, 'logits/rejected': -0.9764575362205505, 'logits/chosen': -1.0755544900894165, 'epoch': 7.24}

 80%|████████  | 19432/24156 [16:01:18<22:17:42, 16.99s/it]


 80%|████████  | 19434/24156 [16:01:50<21:59:20, 16.76s/it]
{'loss': 0.3785, 'learning_rate': 1.934758155989148e-06, 'rewards/chosen': -0.7967569231987, 'rewards/rejected': -3.255223274230957, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4584667682647705, 'policy_logps/rejected': -287.1702575683594, 'policy_logps/chosen': -460.5944519042969, 'referece_logps/rejected': -254.61802673339844, 'referece_logps/chosen': -452.6268310546875, 'logits/rejected': 0.09886196255683899, 'logits/chosen': 0.17009101808071136, 'epoch': 7.24}

 80%|████████  | 19435/24156 [16:02:05<21:03:42, 16.06s/it]

 80%|████████  | 19436/24156 [16:02:26<22:59:52, 17.54s/it]

 80%|████████  | 19437/24156 [16:02:37<20:34:16, 15.69s/it]

 80%|████████  | 19438/24156 [16:02:57<22:01:54, 16.81s/it]

 80%|████████  | 19439/24156 [16:03:13<21:59:19, 16.78s/it]

 80%|████████  | 19440/24156 [16:03:33<23:18:05, 17.79s/it]

 80%|████████  | 19441/24156 [16:03:46<21:18:33, 16.27s/it]

 80%|████████  | 19442/24156 [16:04:02<21:10:45, 16.17s/it]
[2024-04-06 07:09:23,363] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 19443/24156 [16:04:26<24:10:03, 18.46s/it]
[2024-04-06 07:09:44,702] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|████████  | 19445/24156 [16:05:09<26:03:29, 19.91s/it]
[2024-04-06 07:10:05,988] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2585, 'learning_rate': 1.934233143262213e-06, 'rewards/chosen': -1.4406237602233887, 'rewards/rejected': -2.929774284362793, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4891502857208252, 'policy_logps/rejected': -338.4014892578125, 'policy_logps/chosen': -366.39617919921875, 'referece_logps/rejected': -309.1037292480469, 'referece_logps/chosen': -351.98992919921875, 'logits/rejected': 0.3831852674484253, 'logits/chosen': 0.33957409858703613, 'epoch': 7.24}

 81%|████████  | 19446/24156 [16:05:19<22:27:47, 17.17s/it]


 81%|████████  | 19448/24156 [16:05:51<21:27:28, 16.41s/it]
{'loss': 0.3018, 'learning_rate': 1.9340896052353508e-06, 'rewards/chosen': -1.9568012952804565, 'rewards/rejected': -4.388784408569336, 'rewards/accuracies': 0.875, 'rewards/margins': 2.431983232498169, 'policy_logps/rejected': -317.8638000488281, 'policy_logps/chosen': -543.8258666992188, 'referece_logps/rejected': -273.9759521484375, 'referece_logps/chosen': -524.2578735351562, 'logits/rejected': -0.7864897847175598, 'logits/chosen': -0.5806145071983337, 'epoch': 7.25}

 81%|████████  | 19449/24156 [16:06:08<21:52:59, 16.74s/it]
[2024-04-06 07:11:27,289] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19450/24156 [16:06:30<23:50:10, 18.23s/it]

 81%|████████  | 19451/24156 [16:06:45<22:36:06, 17.29s/it]

 81%|████████  | 19452/24156 [16:07:02<22:24:22, 17.15s/it]

 81%|████████  | 19453/24156 [16:07:12<19:51:33, 15.20s/it]

 81%|████████  | 19454/24156 [16:07:28<19:50:46, 15.19s/it]

 81%|████████  | 19455/24156 [16:07:39<18:13:46, 13.96s/it]

 81%|████████  | 19456/24156 [16:07:54<18:44:49, 14.36s/it]

 81%|████████  | 19457/24156 [16:08:13<20:39:53, 15.83s/it]

 81%|████████  | 19458/24156 [16:08:34<22:42:16, 17.40s/it]

 81%|████████  | 19459/24156 [16:08:47<21:00:45, 16.11s/it]

 81%|████████  | 19460/24156 [16:09:02<20:34:11, 15.77s/it]
[2024-04-06 07:14:13,435] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19461/24156 [16:09:16<19:43:52, 15.13s/it]

 81%|████████  | 19462/24156 [16:09:35<21:22:04, 16.39s/it]

 81%|████████  | 19463/24156 [16:09:50<20:33:18, 15.77s/it]
[2024-04-06 07:15:09,824] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19464/24156 [16:10:12<23:16:42, 17.86s/it]

 81%|████████  | 19465/24156 [16:10:29<22:56:01, 17.60s/it]
[2024-04-06 07:15:48,516] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19466/24156 [16:10:51<24:31:51, 18.83s/it]

 81%|████████  | 19467/24156 [16:11:05<22:28:25, 17.25s/it]

 81%|████████  | 19468/24156 [16:11:28<24:46:55, 19.03s/it]

 81%|████████  | 19469/24156 [16:11:43<23:28:30, 18.03s/it]

 81%|████████  | 19470/24156 [16:11:59<22:35:09, 17.35s/it]

 81%|████████  | 19471/24156 [16:12:13<21:00:03, 16.14s/it]

 81%|████████  | 19472/24156 [16:12:34<22:57:53, 17.65s/it]

 81%|████████  | 19473/24156 [16:12:54<24:04:47, 18.51s/it]


 81%|████████  | 19475/24156 [16:13:25<21:35:29, 16.61s/it]
{'loss': 0.399, 'learning_rate': 1.93279096496263e-06, 'rewards/chosen': -1.8425483703613281, 'rewards/rejected': -3.7505440711975098, 'rewards/accuracies': 0.875, 'rewards/margins': 1.907995581626892, 'policy_logps/rejected': -255.83685302734375, 'policy_logps/chosen': -312.59375, 'referece_logps/rejected': -218.3314208984375, 'referece_logps/chosen': -294.16827392578125, 'logits/rejected': -0.6495558023452759, 'logits/chosen': -0.6902021765708923, 'epoch': 7.26}

 81%|████████  | 19476/24156 [16:13:46<23:11:01, 17.83s/it]


 81%|████████  | 19478/24156 [16:14:27<25:08:48, 19.35s/it]
{'loss': 0.2828, 'learning_rate': 1.932645916690225e-06, 'rewards/chosen': -2.382779836654663, 'rewards/rejected': -4.695173740386963, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3123939037323, 'policy_logps/rejected': -411.5414123535156, 'policy_logps/chosen': -353.91070556640625, 'referece_logps/rejected': -364.5896911621094, 'referece_logps/chosen': -330.0828857421875, 'logits/rejected': -0.14001567661762238, 'logits/chosen': -0.17567336559295654, 'epoch': 7.26}
[2024-04-06 07:19:42,533] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19479/24156 [16:14:45<24:34:45, 18.92s/it]

 81%|████████  | 19480/24156 [16:15:05<25:08:52, 19.36s/it]

 81%|████████  | 19481/24156 [16:15:17<22:13:30, 17.11s/it]

 81%|████████  | 19482/24156 [16:15:39<23:59:54, 18.48s/it]

 81%|████████  | 19483/24156 [16:15:50<21:03:58, 16.23s/it]
[2024-04-06 07:20:58,906] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19484/24156 [16:16:01<19:12:18, 14.80s/it]

 81%|████████  | 19485/24156 [16:16:20<20:28:58, 15.79s/it]

 81%|████████  | 19486/24156 [16:16:38<21:31:16, 16.59s/it]

 81%|████████  | 19487/24156 [16:16:51<20:12:27, 15.58s/it]

 81%|████████  | 19488/24156 [16:17:12<22:20:24, 17.23s/it]

 81%|████████  | 19489/24156 [16:17:25<20:29:04, 15.80s/it]

 81%|████████  | 19490/24156 [16:17:40<20:25:21, 15.76s/it]

 81%|████████  | 19491/24156 [16:17:54<19:38:54, 15.16s/it]

 81%|████████  | 19492/24156 [16:18:09<19:37:30, 15.15s/it]

 81%|████████  | 19493/24156 [16:18:22<18:35:45, 14.36s/it]

 81%|████████  | 19494/24156 [16:18:34<17:42:53, 13.68s/it]

 81%|████████  | 19495/24156 [16:18:51<19:05:53, 14.75s/it]

 81%|████████  | 19496/24156 [16:19:02<17:33:49, 13.57s/it]

 81%|████████  | 19497/24156 [16:19:18<18:41:04, 14.44s/it]

 81%|████████  | 19498/24156 [16:19:34<19:14:24, 14.87s/it]

 81%|████████  | 19499/24156 [16:19:46<17:51:55, 13.81s/it]
[2024-04-06 07:24:59,762] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19500/24156 [16:20:02<18:57:23, 14.66s/it]

 81%|████████  | 19501/24156 [16:20:33<25:04:52, 19.40s/it]

 81%|████████  | 19502/24156 [16:20:53<25:18:39, 19.58s/it]
[2024-04-06 07:26:10,455] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 81%|████████  | 19504/24156 [16:21:34<25:54:29, 20.05s/it]
{'loss': 0.2562, 'learning_rate': 1.93138251332098e-06, 'rewards/chosen': -1.9646751880645752, 'rewards/rejected': -4.519275188446045, 'rewards/accuracies': 0.875, 'rewards/margins': 2.554600238800049, 'policy_logps/rejected': -294.7752685546875, 'policy_logps/chosen': -308.10546875, 'referece_logps/rejected': -249.58248901367188, 'referece_logps/chosen': -288.4587097167969, 'logits/rejected': -0.6662206649780273, 'logits/chosen': -0.7333237528800964, 'epoch': 7.27}

 81%|████████  | 19505/24156 [16:21:49<24:05:33, 18.65s/it]

 81%|████████  | 19506/24156 [16:22:06<23:32:13, 18.22s/it]

 81%|████████  | 19507/24156 [16:22:25<23:50:20, 18.46s/it]

 81%|████████  | 19508/24156 [16:22:43<23:38:12, 18.31s/it]

 81%|████████  | 19509/24156 [16:23:00<23:11:11, 17.96s/it]

 81%|████████  | 19510/24156 [16:23:22<24:42:24, 19.14s/it]

 81%|████████  | 19511/24156 [16:23:34<21:52:19, 16.95s/it]

 81%|████████  | 19512/24156 [16:23:48<20:45:10, 16.09s/it]

 81%|████████  | 19513/24156 [16:24:04<20:43:41, 16.07s/it]

 81%|████████  | 19514/24156 [16:24:23<21:55:42, 17.01s/it]

 81%|████████  | 19515/24156 [16:24:39<21:13:40, 16.47s/it]

 81%|████████  | 19516/24156 [16:24:51<19:33:43, 15.18s/it]

 81%|████████  | 19517/24156 [16:25:09<20:42:38, 16.07s/it]

 81%|████████  | 19518/24156 [16:25:28<21:45:49, 16.89s/it]


 81%|████████  | 19520/24156 [16:26:04<21:56:46, 17.04s/it]
{'loss': 0.406, 'learning_rate': 1.930599407531011e-06, 'rewards/chosen': -1.5701851844787598, 'rewards/rejected': -3.3686463832855225, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7984611988067627, 'policy_logps/rejected': -586.39599609375, 'policy_logps/chosen': -500.11920166015625, 'referece_logps/rejected': -552.7095336914062, 'referece_logps/chosen': -484.4173583984375, 'logits/rejected': -0.761236310005188, 'logits/chosen': -0.5704358220100403, 'epoch': 7.27}

 81%|████████  | 19521/24156 [16:26:20<21:25:45, 16.64s/it]

 81%|████████  | 19522/24156 [16:26:31<19:34:00, 15.20s/it]

 81%|████████  | 19523/24156 [16:26:51<21:17:01, 16.54s/it]

 81%|████████  | 19524/24156 [16:27:09<21:55:57, 17.05s/it]

 81%|████████  | 19525/24156 [16:27:23<20:39:11, 16.06s/it]

 81%|████████  | 19526/24156 [16:27:40<20:57:51, 16.30s/it]

 81%|████████  | 19527/24156 [16:27:52<19:25:16, 15.10s/it]

 81%|████████  | 19528/24156 [16:28:12<21:06:32, 16.42s/it]

 81%|████████  | 19529/24156 [16:28:32<22:37:54, 17.61s/it]

 81%|████████  | 19530/24156 [16:28:53<23:55:51, 18.62s/it]

 81%|████████  | 19531/24156 [16:29:12<23:57:31, 18.65s/it]

 81%|████████  | 19532/24156 [16:29:32<24:20:20, 18.95s/it]


 81%|████████  | 19534/24156 [16:30:08<23:38:35, 18.42s/it]

 81%|████████  | 19535/24156 [16:30:29<24:40:49, 19.23s/it]

 81%|████████  | 19536/24156 [16:30:47<24:09:53, 18.83s/it]

 81%|████████  | 19537/24156 [16:31:01<22:33:38, 17.58s/it]
{'loss': 0.5051, 'learning_rate': 1.929762665111438e-06, 'rewards/chosen': -2.5754783153533936, 'rewards/rejected': -3.881448984146118, 'rewards/accuracies': 1.0, 'rewards/margins': 1.305970549583435, 'policy_logps/rejected': -541.4129028320312, 'policy_logps/chosen': -408.0735778808594, 'referece_logps/rejected': -502.59844970703125, 'referece_logps/chosen': -382.3188171386719, 'logits/rejected': -0.42441654205322266, 'logits/chosen': -0.30694976449012756, 'epoch': 7.28}


 81%|████████  | 19539/24156 [16:31:32<20:27:45, 15.96s/it]

 81%|████████  | 19540/24156 [16:31:46<19:49:51, 15.47s/it]

 81%|████████  | 19541/24156 [16:31:58<18:27:55, 14.40s/it]

 81%|████████  | 19542/24156 [16:32:15<19:40:52, 15.36s/it]

 81%|████████  | 19543/24156 [16:32:28<18:26:33, 14.39s/it]
{'loss': 0.4519, 'learning_rate': 1.9294661907413168e-06, 'rewards/chosen': -2.3487515449523926, 'rewards/rejected': -2.9714956283569336, 'rewards/accuracies': 0.625, 'rewards/margins': 0.6227438449859619, 'policy_logps/rejected': -306.74981689453125, 'policy_logps/chosen': -405.90631103515625, 'referece_logps/rejected': -277.0348205566406, 'referece_logps/chosen': -382.41876220703125, 'logits/rejected': -0.8324337005615234, 'logits/chosen': -0.9036121368408203, 'epoch': 7.28}


 81%|████████  | 19545/24156 [16:33:04<21:10:52, 16.54s/it]

 81%|████████  | 19546/24156 [16:33:20<21:03:11, 16.44s/it]

 81%|████████  | 19547/24156 [16:33:42<23:11:13, 18.11s/it]

 81%|████████  | 19548/24156 [16:34:04<24:36:40, 19.23s/it]

 81%|████████  | 19549/24156 [16:34:19<23:12:38, 18.14s/it]

 81%|████████  | 19550/24156 [16:34:34<21:42:17, 16.96s/it]

 81%|████████  | 19551/24156 [16:34:49<21:07:52, 16.52s/it]

 81%|████████  | 19552/24156 [16:35:08<22:06:36, 17.29s/it]

 81%|████████  | 19553/24156 [16:35:22<20:48:40, 16.28s/it]

 81%|████████  | 19554/24156 [16:35:41<21:40:06, 16.95s/it]

 81%|████████  | 19555/24156 [16:35:58<21:42:40, 16.99s/it]

 81%|████████  | 19556/24156 [16:36:15<22:00:45, 17.23s/it]

 81%|████████  | 19557/24156 [16:36:27<19:56:26, 15.61s/it]

 81%|████████  | 19558/24156 [16:36:46<21:09:36, 16.57s/it]

 81%|████████  | 19559/24156 [16:37:03<21:25:30, 16.78s/it]
{'loss': 0.3905, 'learning_rate': 1.92867265215115e-06, 'rewards/chosen': -2.374032497406006, 'rewards/rejected': -2.4368274211883545, 'rewards/accuracies': 0.5, 'rewards/margins': 0.06279480457305908, 'policy_logps/rejected': -342.5436706542969, 'policy_logps/chosen': -429.55609130859375, 'referece_logps/rejected': -318.1754150390625, 'referece_logps/chosen': -405.8157653808594, 'logits/rejected': -0.600503146648407, 'logits/chosen': -0.5308349132537842, 'epoch': 7.29}


 81%|████████  | 19561/24156 [16:37:42<23:16:54, 18.24s/it]
[2024-04-06 07:42:39,451] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19562/24156 [16:38:04<24:48:48, 19.44s/it]

 81%|████████  | 19563/24156 [16:38:16<21:58:08, 17.22s/it]

 81%|████████  | 19564/24156 [16:38:28<19:55:37, 15.62s/it]

 81%|████████  | 19565/24156 [16:38:46<20:57:41, 16.44s/it]
{'loss': 0.3366, 'learning_rate': 1.9283739731278668e-06, 'rewards/chosen': -2.122593402862549, 'rewards/rejected': -3.7052230834960938, 'rewards/accuracies': 0.75, 'rewards/margins': 1.582629919052124, 'policy_logps/rejected': -342.0326232910156, 'policy_logps/chosen': -334.6070861816406, 'referece_logps/rejected': -304.9803771972656, 'referece_logps/chosen': -313.38116455078125, 'logits/rejected': -0.39644157886505127, 'logits/chosen': -0.37821853160858154, 'epoch': 7.29}


 81%|████████  | 19567/24156 [16:39:19<21:00:19, 16.48s/it]
{'loss': 0.3516, 'learning_rate': 1.9282742799285876e-06, 'rewards/chosen': -2.5619773864746094, 'rewards/rejected': -4.317599773406982, 'rewards/accuracies': 0.875, 'rewards/margins': 1.755622386932373, 'policy_logps/rejected': -499.2184143066406, 'policy_logps/chosen': -336.49456787109375, 'referece_logps/rejected': -456.0423889160156, 'referece_logps/chosen': -310.8747863769531, 'logits/rejected': -0.9410311579704285, 'logits/chosen': -0.9319037199020386, 'epoch': 7.29}


 81%|████████  | 19569/24156 [16:39:56<22:01:29, 17.29s/it]

 81%|████████  | 19570/24156 [16:40:18<23:37:38, 18.55s/it]
[2024-04-06 07:45:14,992] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19571/24156 [16:40:32<22:06:39, 17.36s/it]
[2024-04-06 07:45:29,585] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19572/24156 [16:40:50<22:20:21, 17.54s/it]
{'loss': 0.3736, 'learning_rate': 1.9280247549127274e-06, 'rewards/chosen': -2.4957165718078613, 'rewards/rejected': -3.2688772678375244, 'rewards/accuracies': 0.75, 'rewards/margins': 0.773160457611084, 'policy_logps/rejected': -303.8034362792969, 'policy_logps/chosen': -340.2013854980469, 'referece_logps/rejected': -271.11468505859375, 'referece_logps/chosen': -315.24420166015625, 'logits/rejected': -0.19599352777004242, 'logits/chosen': -0.34723734855651855, 'epoch': 7.29}

 81%|████████  | 19573/24156 [16:41:07<22:11:32, 17.43s/it]


 81%|████████  | 19575/24156 [16:41:44<22:06:45, 17.38s/it]
{'loss': 0.3836, 'learning_rate': 1.927874839698416e-06, 'rewards/chosen': -1.998726725578308, 'rewards/rejected': -4.008152484893799, 'rewards/accuracies': 0.875, 'rewards/margins': 2.009425640106201, 'policy_logps/rejected': -448.4056701660156, 'policy_logps/chosen': -357.80303955078125, 'referece_logps/rejected': -408.3241271972656, 'referece_logps/chosen': -337.8157958984375, 'logits/rejected': -0.6768680214881897, 'logits/chosen': -0.7159780263900757, 'epoch': 7.29}


 81%|████████  | 19577/24156 [16:42:10<19:05:58, 15.02s/it]

 81%|████████  | 19578/24156 [16:42:21<17:23:37, 13.68s/it]

 81%|████████  | 19579/24156 [16:42:34<17:10:31, 13.51s/it]

 81%|████████  | 19580/24156 [16:42:55<20:03:13, 15.78s/it]

 81%|████████  | 19581/24156 [16:43:09<19:16:14, 15.16s/it]

 81%|████████  | 19582/24156 [16:43:28<20:48:41, 16.38s/it]

 81%|████████  | 19583/24156 [16:43:39<18:46:58, 14.79s/it]

 81%|████████  | 19584/24156 [16:43:51<17:48:01, 14.02s/it]

 81%|████████  | 19585/24156 [16:44:11<19:53:59, 15.67s/it]

 81%|████████  | 19586/24156 [16:44:28<20:27:18, 16.11s/it]

 81%|████████  | 19587/24156 [16:44:49<22:16:28, 17.55s/it]

 81%|████████  | 19588/24156 [16:45:05<21:40:27, 17.08s/it]
{'loss': 0.359, 'learning_rate': 1.927223472649277e-06, 'rewards/chosen': -2.0823817253112793, 'rewards/rejected': -5.638588905334473, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5562074184417725, 'policy_logps/rejected': -520.8161010742188, 'policy_logps/chosen': -318.1408996582031, 'referece_logps/rejected': -464.4302062988281, 'referece_logps/chosen': -297.31707763671875, 'logits/rejected': -0.4781253933906555, 'logits/chosen': -0.17076517641544342, 'epoch': 7.3}


 81%|████████  | 19590/24156 [16:45:35<20:49:59, 16.43s/it]

 81%|████████  | 19591/24156 [16:45:56<22:49:41, 18.00s/it]
{'loss': 0.3768, 'learning_rate': 1.9270727570547445e-06, 'rewards/chosen': -2.155064344406128, 'rewards/rejected': -3.4783270359039307, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3232629299163818, 'policy_logps/rejected': -463.0503234863281, 'policy_logps/chosen': -518.6490478515625, 'referece_logps/rejected': -428.2670593261719, 'referece_logps/chosen': -497.098388671875, 'logits/rejected': 0.04037332534790039, 'logits/chosen': 0.16521933674812317, 'epoch': 7.3}
[2024-04-06 07:51:12,714] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19592/24156 [16:46:15<23:09:33, 18.27s/it]


 81%|████████  | 19594/24156 [16:46:38<18:41:39, 14.75s/it]

 81%|████████  | 19595/24156 [16:46:55<19:20:40, 15.27s/it]

 81%|████████  | 19596/24156 [16:47:13<20:20:17, 16.06s/it]
{'loss': 0.3493, 'learning_rate': 1.926821231088834e-06, 'rewards/chosen': -2.106588125228882, 'rewards/rejected': -5.544668197631836, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4380807876586914, 'policy_logps/rejected': -590.1434936523438, 'policy_logps/chosen': -563.6088256835938, 'referece_logps/rejected': -534.6968383789062, 'referece_logps/chosen': -542.5429077148438, 'logits/rejected': -0.8058205842971802, 'logits/chosen': -0.6497516632080078, 'epoch': 7.3}


 81%|████████  | 19598/24156 [16:47:45<19:52:57, 15.70s/it]

 81%|████████  | 19599/24156 [16:48:06<21:57:32, 17.35s/it]
[2024-04-06 07:53:03,639] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2686, 'learning_rate': 1.9266701155640645e-06, 'rewards/chosen': -2.0543410778045654, 'rewards/rejected': -4.7103071212768555, 'rewards/accuracies': 0.875, 'rewards/margins': 2.65596604347229, 'policy_logps/rejected': -376.9735107421875, 'policy_logps/chosen': -398.43695068359375, 'referece_logps/rejected': -329.87042236328125, 'referece_logps/chosen': -377.8935241699219, 'logits/rejected': -0.21654215455055237, 'logits/chosen': -0.19839587807655334, 'epoch': 7.3}
[2024-04-06 07:53:15,132] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 81%|████████  | 19601/24156 [16:48:39<22:01:17, 17.40s/it]

 81%|████████  | 19602/24156 [16:48:59<22:55:28, 18.12s/it]

 81%|████████  | 19603/24156 [16:49:20<24:02:42, 19.01s/it]
[2024-04-06 07:54:17,653] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19604/24156 [16:49:31<20:55:37, 16.55s/it]
[2024-04-06 07:54:28,460] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 19605/24156 [16:49:46<20:22:37, 16.12s/it]

 81%|████████  | 19606/24156 [16:50:02<20:26:35, 16.17s/it]
{'loss': 0.3685, 'learning_rate': 1.9263169296578642e-06, 'rewards/chosen': -2.4254231452941895, 'rewards/rejected': -4.239462852478027, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8140400648117065, 'policy_logps/rejected': -384.2659912109375, 'policy_logps/chosen': -401.9889831542969, 'referece_logps/rejected': -341.871337890625, 'referece_logps/chosen': -377.7347412109375, 'logits/rejected': -0.8144984841346741, 'logits/chosen': -0.840009331703186, 'epoch': 7.3}


 81%|████████  | 19608/24156 [16:50:37<20:40:25, 16.36s/it]

 81%|████████  | 19609/24156 [16:50:52<20:20:52, 16.11s/it]
{'loss': 0.3783, 'learning_rate': 1.926165314464848e-06, 'rewards/chosen': -1.948526382446289, 'rewards/rejected': -4.4973015785217285, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5487751960754395, 'policy_logps/rejected': -306.42156982421875, 'policy_logps/chosen': -325.07196044921875, 'referece_logps/rejected': -261.4486083984375, 'referece_logps/chosen': -305.58673095703125, 'logits/rejected': -0.5995103716850281, 'logits/chosen': -0.7191798686981201, 'epoch': 7.31}


 81%|████████  | 19611/24156 [16:51:21<19:16:45, 15.27s/it]
{'loss': 0.4542, 'learning_rate': 1.926064154419452e-06, 'rewards/chosen': -3.0602383613586426, 'rewards/rejected': -3.486359119415283, 'rewards/accuracies': 0.625, 'rewards/margins': 0.4261211156845093, 'policy_logps/rejected': -306.5048522949219, 'policy_logps/chosen': -456.25250244140625, 'referece_logps/rejected': -271.6412658691406, 'referece_logps/chosen': -425.65008544921875, 'logits/rejected': -0.7798279523849487, 'logits/chosen': -1.0580344200134277, 'epoch': 7.31}

 81%|████████  | 19612/24156 [16:51:36<19:08:08, 15.16s/it]


 81%|████████  | 19614/24156 [16:52:03<18:16:01, 14.48s/it]
{'loss': 0.4121, 'learning_rate': 1.9259122894944653e-06, 'rewards/chosen': -1.444002389907837, 'rewards/rejected': -2.9565558433532715, 'rewards/accuracies': 0.875, 'rewards/margins': 1.512553334236145, 'policy_logps/rejected': -332.468994140625, 'policy_logps/chosen': -316.69024658203125, 'referece_logps/rejected': -302.9034729003906, 'referece_logps/chosen': -302.25018310546875, 'logits/rejected': -0.3766616880893707, 'logits/chosen': -0.42809635400772095, 'epoch': 7.31}


 81%|████████  | 19616/24156 [16:52:31<17:33:08, 13.92s/it]

 81%|████████  | 19617/24156 [16:52:42<16:20:44, 12.96s/it]

 81%|████████  | 19618/24156 [16:52:52<15:30:44, 12.31s/it]

 81%|████████  | 19619/24156 [16:53:11<17:47:07, 14.11s/it]

 81%|████████  | 19620/24156 [16:53:27<18:28:11, 14.66s/it]

 81%|████████  | 19621/24156 [16:53:43<19:07:58, 15.19s/it]

 81%|████████  | 19622/24156 [16:54:03<21:03:52, 16.73s/it]

 81%|████████  | 19623/24156 [16:54:15<19:10:35, 15.23s/it]

 81%|████████  | 19624/24156 [16:54:28<18:30:13, 14.70s/it]
{'loss': 0.3591, 'learning_rate': 1.925404991280732e-06, 'rewards/chosen': -1.7271056175231934, 'rewards/rejected': -2.7912447452545166, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0641392469406128, 'policy_logps/rejected': -327.44586181640625, 'policy_logps/chosen': -335.8477478027344, 'referece_logps/rejected': -299.5334167480469, 'referece_logps/chosen': -318.5766906738281, 'logits/rejected': -0.4901416599750519, 'logits/chosen': -0.6708403825759888, 'epoch': 7.31}


 81%|████████  | 19626/24156 [16:55:10<22:23:25, 17.79s/it]

 81%|████████▏ | 19627/24156 [16:55:30<23:11:06, 18.43s/it]

 81%|████████▏ | 19628/24156 [16:55:48<23:16:31, 18.51s/it]

 81%|████████▏ | 19629/24156 [16:56:01<20:53:38, 16.62s/it]

 81%|████████▏ | 19630/24156 [16:56:11<18:44:25, 14.91s/it]

 81%|████████▏ | 19631/24156 [16:56:31<20:32:32, 16.34s/it]

 81%|████████▏ | 19632/24156 [16:56:50<21:19:08, 16.96s/it]

 81%|████████▏ | 19633/24156 [16:57:06<21:05:03, 16.78s/it]

 81%|████████▏ | 19634/24156 [16:57:23<21:11:27, 16.87s/it]

 81%|████████▏ | 19635/24156 [16:57:44<22:55:31, 18.26s/it]

 81%|████████▏ | 19636/24156 [16:58:05<23:42:55, 18.89s/it]
{'loss': 0.4097, 'learning_rate': 1.9247940375478703e-06, 'rewards/chosen': -2.6072044372558594, 'rewards/rejected': -4.353888988494873, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7466844320297241, 'policy_logps/rejected': -387.021728515625, 'policy_logps/chosen': -383.2874755859375, 'referece_logps/rejected': -343.48284912109375, 'referece_logps/chosen': -357.21539306640625, 'logits/rejected': 0.07026243209838867, 'logits/chosen': 0.16218698024749756, 'epoch': 7.32}


 81%|████████▏ | 19638/24156 [16:58:33<20:55:00, 16.67s/it]

 81%|████████▏ | 19639/24156 [16:58:45<19:14:02, 15.33s/it]

 81%|████████▏ | 19640/24156 [16:59:05<20:50:16, 16.61s/it]

 81%|████████▏ | 19641/24156 [16:59:20<20:20:27, 16.22s/it]

 81%|████████▏ | 19642/24156 [16:59:31<18:14:28, 14.55s/it]
{'loss': 0.3141, 'learning_rate': 1.9244876628295158e-06, 'rewards/chosen': -1.6357134580612183, 'rewards/rejected': -2.905067205429077, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2693537473678589, 'policy_logps/rejected': -414.8048095703125, 'policy_logps/chosen': -560.3397216796875, 'referece_logps/rejected': -385.7541198730469, 'referece_logps/chosen': -543.9826049804688, 'logits/rejected': 0.5892001390457153, 'logits/chosen': 0.4183254837989807, 'epoch': 7.32}

 81%|████████▏ | 19643/24156 [16:59:52<20:57:44, 16.72s/it]

 81%|████████▏ | 19644/24156 [17:00:10<21:25:14, 17.09s/it]

 81%|████████▏ | 19645/24156 [17:00:23<19:36:42, 15.65s/it]

 81%|████████▏ | 19646/24156 [17:00:39<19:42:07, 15.73s/it]


 81%|████████▏ | 19648/24156 [17:01:12<20:09:11, 16.09s/it]

 81%|████████▏ | 19649/24156 [17:01:32<21:38:18, 17.28s/it]
{'loss': 0.3609, 'learning_rate': 1.9241294694826372e-06, 'rewards/chosen': -2.3581905364990234, 'rewards/rejected': -3.734002113342285, 'rewards/accuracies': 0.625, 'rewards/margins': 1.375811219215393, 'policy_logps/rejected': -305.5502624511719, 'policy_logps/chosen': -284.29547119140625, 'referece_logps/rejected': -268.21026611328125, 'referece_logps/chosen': -260.71356201171875, 'logits/rejected': -0.5333616733551025, 'logits/chosen': -0.5400381088256836, 'epoch': 7.32}


 81%|████████▏ | 19651/24156 [17:02:02<19:42:51, 15.75s/it]
{'loss': 0.3731, 'learning_rate': 1.9240269789945987e-06, 'rewards/chosen': -1.4416509866714478, 'rewards/rejected': -3.037508010864258, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5958566665649414, 'policy_logps/rejected': -367.4552307128906, 'policy_logps/chosen': -343.3686828613281, 'referece_logps/rejected': -337.08013916015625, 'referece_logps/chosen': -328.9521484375, 'logits/rejected': -0.10473577678203583, 'logits/chosen': -0.027373000979423523, 'epoch': 7.32}


 81%|████████▏ | 19653/24156 [17:02:42<22:21:48, 17.88s/it]

 81%|████████▏ | 19654/24156 [17:03:04<23:56:07, 19.14s/it]
{'loss': 0.2687, 'learning_rate': 1.923873118680347e-06, 'rewards/chosen': -1.6221930980682373, 'rewards/rejected': -4.351024150848389, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7288312911987305, 'policy_logps/rejected': -352.3964538574219, 'policy_logps/chosen': -329.6498107910156, 'referece_logps/rejected': -308.88623046875, 'referece_logps/chosen': -313.4278869628906, 'logits/rejected': -1.1862343549728394, 'logits/chosen': -0.9307035207748413, 'epoch': 7.32}

 81%|████████▏ | 19655/24156 [17:03:19<22:13:35, 17.78s/it]

 81%|████████▏ | 19656/24156 [17:03:33<21:00:44, 16.81s/it]


 81%|████████▏ | 19658/24156 [17:03:58<17:59:27, 14.40s/it]

 81%|████████▏ | 19659/24156 [17:04:16<19:29:30, 15.60s/it]

 81%|████████▏ | 19660/24156 [17:04:29<18:34:08, 14.87s/it]
{'loss': 0.4125, 'learning_rate': 1.923564949647249e-06, 'rewards/chosen': -1.1070235967636108, 'rewards/rejected': -2.816006898880005, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7089834213256836, 'policy_logps/rejected': -391.97222900390625, 'policy_logps/chosen': -355.80340576171875, 'referece_logps/rejected': -363.8121337890625, 'referece_logps/chosen': -344.7331848144531, 'logits/rejected': -1.2194772958755493, 'logits/chosen': -1.2236882448196411, 'epoch': 7.32}


 81%|████████▏ | 19662/24156 [17:05:02<19:12:12, 15.38s/it]

 81%|████████▏ | 19663/24156 [17:05:24<21:25:52, 17.17s/it]
{'loss': 0.3354, 'learning_rate': 1.9234106409782626e-06, 'rewards/chosen': -1.8598979711532593, 'rewards/rejected': -4.295417785644531, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4355196952819824, 'policy_logps/rejected': -585.1456298828125, 'policy_logps/chosen': -503.3576965332031, 'referece_logps/rejected': -542.19140625, 'referece_logps/chosen': -484.75872802734375, 'logits/rejected': 0.049285609275102615, 'logits/chosen': 0.15419554710388184, 'epoch': 7.33}

 81%|████████▏ | 19664/24156 [17:05:37<19:58:25, 16.01s/it]

 81%|████████▏ | 19665/24156 [17:05:51<19:16:55, 15.46s/it]

 81%|████████▏ | 19666/24156 [17:06:07<19:17:22, 15.47s/it]


 81%|████████▏ | 19668/24156 [17:06:44<21:31:29, 17.27s/it]

 81%|████████▏ | 19669/24156 [17:06:56<19:41:13, 15.80s/it]
{'loss': 0.3922, 'learning_rate': 1.9231015754602457e-06, 'rewards/chosen': -1.7440909147262573, 'rewards/rejected': -1.993456482887268, 'rewards/accuracies': 0.375, 'rewards/margins': 0.24936561286449432, 'policy_logps/rejected': -415.9501953125, 'policy_logps/chosen': -486.53411865234375, 'referece_logps/rejected': -396.015625, 'referece_logps/chosen': -469.0931701660156, 'logits/rejected': -0.5925723314285278, 'logits/chosen': -0.5212472081184387, 'epoch': 7.33}


 81%|████████▏ | 19671/24156 [17:07:20<17:21:50, 13.94s/it]

 81%|████████▏ | 19672/24156 [17:07:31<16:07:00, 12.94s/it]
{'loss': 0.4177, 'learning_rate': 1.92294681866122e-06, 'rewards/chosen': -2.199389934539795, 'rewards/rejected': -4.254101276397705, 'rewards/accuracies': 1.0, 'rewards/margins': 2.05471134185791, 'policy_logps/rejected': -425.22186279296875, 'policy_logps/chosen': -369.6883544921875, 'referece_logps/rejected': -382.68084716796875, 'referece_logps/chosen': -347.6944274902344, 'logits/rejected': 0.28025251626968384, 'logits/chosen': 0.38573208451271057, 'epoch': 7.33}

 81%|████████▏ | 19673/24156 [17:07:51<19:04:53, 15.32s/it]


 81%|████████▏ | 19675/24156 [17:08:18<17:33:15, 14.10s/it]
{'loss': 0.3897, 'learning_rate': 1.9227919125355585e-06, 'rewards/chosen': -1.5131114721298218, 'rewards/rejected': -2.8810641765594482, 'rewards/accuracies': 1.0, 'rewards/margins': 1.367952585220337, 'policy_logps/rejected': -300.95172119140625, 'policy_logps/chosen': -266.0489807128906, 'referece_logps/rejected': -272.1410827636719, 'referece_logps/chosen': -250.9178466796875, 'logits/rejected': -1.3087674379348755, 'logits/chosen': -1.2634766101837158, 'epoch': 7.33}

 81%|████████▏ | 19676/24156 [17:08:29<16:17:12, 13.09s/it]

 81%|████████▏ | 19677/24156 [17:08:40<15:21:33, 12.34s/it]


 81%|████████▏ | 19679/24156 [17:09:16<19:26:39, 15.64s/it]

 81%|████████▏ | 19680/24156 [17:09:34<20:13:22, 16.27s/it]

 81%|████████▏ | 19681/24156 [17:09:55<21:50:24, 17.57s/it]

 81%|████████▏ | 19682/24156 [17:10:13<22:00:52, 17.71s/it]
{'loss': 0.3843, 'learning_rate': 1.922429884335156e-06, 'rewards/chosen': -2.005854368209839, 'rewards/rejected': -5.0056633949279785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9998092651367188, 'policy_logps/rejected': -424.600341796875, 'policy_logps/chosen': -334.8874816894531, 'referece_logps/rejected': -374.5436706542969, 'referece_logps/chosen': -314.82891845703125, 'logits/rejected': -0.08275303244590759, 'logits/chosen': 0.01331784576177597, 'epoch': 7.33}

 81%|████████▏ | 19683/24156 [17:10:33<22:57:15, 18.47s/it]


 81%|████████▏ | 19685/24156 [17:11:00<19:47:09, 15.93s/it]

 81%|████████▏ | 19686/24156 [17:11:16<19:44:06, 15.89s/it]

 81%|████████▏ | 19687/24156 [17:11:33<19:58:56, 16.10s/it]
{'loss': 0.3888, 'learning_rate': 1.922170795268186e-06, 'rewards/chosen': -1.412989616394043, 'rewards/rejected': -3.121340274810791, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7083503007888794, 'policy_logps/rejected': -422.7876281738281, 'policy_logps/chosen': -422.3534851074219, 'referece_logps/rejected': -391.57421875, 'referece_logps/chosen': -408.2235412597656, 'logits/rejected': -0.11520730704069138, 'logits/chosen': -0.09718115627765656, 'epoch': 7.33}

 82%|████████▏ | 19688/24156 [17:11:47<19:27:47, 15.68s/it]


 82%|████████▏ | 19690/24156 [17:12:15<17:56:01, 14.46s/it]

 82%|████████▏ | 19691/24156 [17:12:37<20:42:35, 16.70s/it]
{'loss': 0.2517, 'learning_rate': 1.921963225606862e-06, 'rewards/chosen': -1.4737426042556763, 'rewards/rejected': -3.072852373123169, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5991098880767822, 'policy_logps/rejected': -323.02276611328125, 'policy_logps/chosen': -349.3629150390625, 'referece_logps/rejected': -292.2942199707031, 'referece_logps/chosen': -334.6254577636719, 'logits/rejected': -0.8282384872436523, 'logits/chosen': -0.873428225517273, 'epoch': 7.34}

 82%|████████▏ | 19692/24156 [17:13:00<22:57:37, 18.52s/it]
[2024-04-06 08:18:11,253] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19693/24156 [17:13:14<21:21:32, 17.23s/it]

 82%|████████▏ | 19694/24156 [17:13:30<20:57:21, 16.91s/it]

 82%|████████▏ | 19695/24156 [17:13:41<18:51:27, 15.22s/it]


 82%|████████▏ | 19697/24156 [17:14:17<20:14:07, 16.34s/it]
{'loss': 0.2867, 'learning_rate': 1.9216513739085736e-06, 'rewards/chosen': -1.5970494747161865, 'rewards/rejected': -3.7447116374969482, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1476621627807617, 'policy_logps/rejected': -307.9149169921875, 'policy_logps/chosen': -373.8232727050781, 'referece_logps/rejected': -270.4678039550781, 'referece_logps/chosen': -357.852783203125, 'logits/rejected': -0.6280422806739807, 'logits/chosen': -0.8125288486480713, 'epoch': 7.34}


 82%|████████▏ | 19699/24156 [17:14:44<18:26:18, 14.89s/it]
{'loss': 0.412, 'learning_rate': 1.9215472907840247e-06, 'rewards/chosen': -1.8653324842453003, 'rewards/rejected': -4.202242851257324, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3369107246398926, 'policy_logps/rejected': -276.297607421875, 'policy_logps/chosen': -458.35882568359375, 'referece_logps/rejected': -234.27516174316406, 'referece_logps/chosen': -439.70550537109375, 'logits/rejected': -1.0755187273025513, 'logits/chosen': -1.1376073360443115, 'epoch': 7.34}

 82%|████████▏ | 19700/24156 [17:15:01<19:20:29, 15.63s/it]

 82%|████████▏ | 19701/24156 [17:15:17<19:29:02, 15.74s/it]

 82%|████████▏ | 19702/24156 [17:15:32<18:55:48, 15.30s/it]


 82%|████████▏ | 19704/24156 [17:16:03<19:14:45, 15.56s/it]
{'loss': 0.569, 'learning_rate': 1.9212867930719775e-06, 'rewards/chosen': -1.5037003755569458, 'rewards/rejected': -2.437427520751953, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9337273240089417, 'policy_logps/rejected': -528.6322021484375, 'policy_logps/chosen': -352.73773193359375, 'referece_logps/rejected': -504.2579650878906, 'referece_logps/chosen': -337.70074462890625, 'logits/rejected': -0.5975086092948914, 'logits/chosen': -0.49420973658561707, 'epoch': 7.34}


 82%|████████▏ | 19706/24156 [17:16:35<19:03:39, 15.42s/it]

 82%|████████▏ | 19707/24156 [17:16:47<17:45:59, 14.38s/it]

 82%|████████▏ | 19708/24156 [17:17:01<17:31:05, 14.18s/it]

 82%|████████▏ | 19709/24156 [17:17:23<20:22:34, 16.50s/it]
[2024-04-06 08:22:20,103] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4002, 'learning_rate': 1.921025881309791e-06, 'rewards/chosen': -1.7476228475570679, 'rewards/rejected': -3.162374258041382, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4147515296936035, 'policy_logps/rejected': -404.1307678222656, 'policy_logps/chosen': -419.228515625, 'referece_logps/rejected': -372.50701904296875, 'referece_logps/chosen': -401.7523193359375, 'logits/rejected': 0.725080132484436, 'logits/chosen': 0.6942126750946045, 'epoch': 7.34}

 82%|████████▏ | 19710/24156 [17:17:42<21:20:04, 17.28s/it]


 82%|████████▏ | 19712/24156 [17:18:17<21:33:46, 17.47s/it]
{'loss': 0.456, 'learning_rate': 1.9208691355571847e-06, 'rewards/chosen': -1.2876625061035156, 'rewards/rejected': -3.8694205284118652, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5817580223083496, 'policy_logps/rejected': -454.5118713378906, 'policy_logps/chosen': -345.5083312988281, 'referece_logps/rejected': -415.8177185058594, 'referece_logps/chosen': -332.6317138671875, 'logits/rejected': -0.9313734769821167, 'logits/chosen': -0.7506470084190369, 'epoch': 7.34}
[2024-04-06 08:23:35,479] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19713/24156 [17:18:38<23:01:03, 18.65s/it]
[2024-04-06 08:23:58,927] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19714/24156 [17:19:01<24:47:17, 20.09s/it]


 82%|████████▏ | 19716/24156 [17:19:33<21:47:29, 17.67s/it]
{'loss': 0.3909, 'learning_rate': 1.920659909461795e-06, 'rewards/chosen': -1.7121491432189941, 'rewards/rejected': -2.976855754852295, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2647064924240112, 'policy_logps/rejected': -476.7038269042969, 'policy_logps/chosen': -429.70703125, 'referece_logps/rejected': -446.935302734375, 'referece_logps/chosen': -412.5855712890625, 'logits/rejected': -0.16082102060317993, 'logits/chosen': 0.0057436153292655945, 'epoch': 7.35}

 82%|████████▏ | 19717/24156 [17:19:50<21:32:15, 17.47s/it]

 82%|████████▏ | 19718/24156 [17:20:10<22:27:40, 18.22s/it]


 82%|████████▏ | 19720/24156 [17:20:47<22:25:20, 18.20s/it]
{'loss': 0.2997, 'learning_rate': 1.9204504185546245e-06, 'rewards/chosen': -2.2499611377716064, 'rewards/rejected': -3.529827833175659, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2798665761947632, 'policy_logps/rejected': -438.73162841796875, 'policy_logps/chosen': -301.752685546875, 'referece_logps/rejected': -403.4333801269531, 'referece_logps/chosen': -279.2530517578125, 'logits/rejected': -0.6562103629112244, 'logits/chosen': -0.515373945236206, 'epoch': 7.35}

 82%|████████▏ | 19721/24156 [17:20:57<19:36:51, 15.92s/it]

 82%|████████▏ | 19722/24156 [17:21:12<19:16:28, 15.65s/it]


 82%|████████▏ | 19724/24156 [17:21:41<18:13:22, 14.80s/it]
{'loss': 0.2931, 'learning_rate': 1.92024066289593e-06, 'rewards/chosen': -0.9613297581672668, 'rewards/rejected': -3.4638662338256836, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5025367736816406, 'policy_logps/rejected': -385.8110656738281, 'policy_logps/chosen': -235.4054412841797, 'referece_logps/rejected': -351.17242431640625, 'referece_logps/chosen': -225.7921142578125, 'logits/rejected': -0.6988784074783325, 'logits/chosen': -0.6434546709060669, 'epoch': 7.35}

 82%|████████▏ | 19725/24156 [17:22:00<19:40:07, 15.98s/it]


 82%|████████▏ | 19727/24156 [17:22:29<18:23:56, 14.96s/it]
{'loss': 0.2946, 'learning_rate': 1.920083172445012e-06, 'rewards/chosen': -1.9868431091308594, 'rewards/rejected': -3.9263243675231934, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9394807815551758, 'policy_logps/rejected': -432.7575988769531, 'policy_logps/chosen': -572.4705200195312, 'referece_logps/rejected': -393.49432373046875, 'referece_logps/chosen': -552.6021118164062, 'logits/rejected': 0.3465220332145691, 'logits/chosen': 0.10499891638755798, 'epoch': 7.35}

 82%|████████▏ | 19728/24156 [17:22:40<16:50:49, 13.70s/it]

 82%|████████▏ | 19729/24156 [17:22:59<18:44:35, 15.24s/it]


 82%|████████▏ | 19731/24156 [17:23:33<20:22:34, 16.58s/it]

 82%|████████▏ | 19732/24156 [17:23:47<19:29:43, 15.86s/it]
{'loss': 0.2846, 'learning_rate': 1.919820357565374e-06, 'rewards/chosen': -1.6968353986740112, 'rewards/rejected': -3.992732048034668, 'rewards/accuracies': 1.0, 'rewards/margins': 2.295896530151367, 'policy_logps/rejected': -425.791748046875, 'policy_logps/chosen': -320.85687255859375, 'referece_logps/rejected': -385.8643798828125, 'referece_logps/chosen': -303.8885192871094, 'logits/rejected': -0.4243021309375763, 'logits/chosen': -0.4116421937942505, 'epoch': 7.35}


 82%|████████▏ | 19734/24156 [17:24:15<18:33:56, 15.11s/it]

 82%|████████▏ | 19735/24156 [17:24:31<18:55:35, 15.41s/it]
{'loss': 0.4251, 'learning_rate': 1.9196624702023033e-06, 'rewards/chosen': -1.7224459648132324, 'rewards/rejected': -3.060365676879883, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3379197120666504, 'policy_logps/rejected': -473.82684326171875, 'policy_logps/chosen': -597.193115234375, 'referece_logps/rejected': -443.22320556640625, 'referece_logps/chosen': -579.9686279296875, 'logits/rejected': -0.29807353019714355, 'logits/chosen': -0.3559257984161377, 'epoch': 7.35}

 82%|████████▏ | 19736/24156 [17:24:52<21:09:21, 17.23s/it]

 82%|████████▏ | 19737/24156 [17:25:12<22:03:57, 17.98s/it]

 82%|████████▏ | 19738/24156 [17:25:30<22:03:20, 17.97s/it]

 82%|████████▏ | 19739/24156 [17:25:48<21:56:33, 17.88s/it]

 82%|████████▏ | 19740/24156 [17:26:04<21:19:38, 17.39s/it]

 82%|████████▏ | 19741/24156 [17:26:20<20:52:56, 17.03s/it]


 82%|████████▏ | 19743/24156 [17:27:01<22:55:07, 18.70s/it]
{'loss': 0.3927, 'learning_rate': 1.9192407098601327e-06, 'rewards/chosen': -1.6324725151062012, 'rewards/rejected': -3.162142515182495, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5296696424484253, 'policy_logps/rejected': -489.9855651855469, 'policy_logps/chosen': -502.8486633300781, 'referece_logps/rejected': -458.3641357421875, 'referece_logps/chosen': -486.52398681640625, 'logits/rejected': -0.23380152881145477, 'logits/chosen': -0.18372997641563416, 'epoch': 7.36}
[2024-04-06 08:32:20,212] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19744/24156 [17:27:23<23:51:36, 19.47s/it]
[2024-04-06 08:32:39,561] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19745/24156 [17:27:42<23:48:39, 19.43s/it]

 82%|████████▏ | 19746/24156 [17:27:55<21:18:20, 17.39s/it]

 82%|████████▏ | 19747/24156 [17:28:17<23:02:59, 18.82s/it]


 82%|████████▏ | 19749/24156 [17:28:49<21:55:31, 17.91s/it]
[2024-04-06 08:33:46,658] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3831, 'learning_rate': 1.9189236955175935e-06, 'rewards/chosen': -1.6533938646316528, 'rewards/rejected': -3.3083317279815674, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6549381017684937, 'policy_logps/rejected': -352.5327453613281, 'policy_logps/chosen': -411.9120788574219, 'referece_logps/rejected': -319.44940185546875, 'referece_logps/chosen': -395.3781433105469, 'logits/rejected': 0.19775390625, 'logits/chosen': -0.1315724104642868, 'epoch': 7.36}

 82%|████████▏ | 19750/24156 [17:29:07<21:45:56, 17.78s/it]

 82%|████████▏ | 19751/24156 [17:29:28<23:11:32, 18.95s/it]

 82%|████████▏ | 19752/24156 [17:29:46<22:43:59, 18.58s/it]

 82%|████████▏ | 19753/24156 [17:30:01<21:15:00, 17.37s/it]

 82%|████████▏ | 19754/24156 [17:30:17<21:02:59, 17.21s/it]

 82%|████████▏ | 19755/24156 [17:30:36<21:21:41, 17.47s/it]

 82%|████████▏ | 19756/24156 [17:30:57<22:46:18, 18.63s/it]

 82%|████████▏ | 19757/24156 [17:31:13<21:41:58, 17.76s/it]

 82%|████████▏ | 19758/24156 [17:31:29<21:17:50, 17.43s/it]

 82%|████████▏ | 19759/24156 [17:31:43<19:48:00, 16.21s/it]

 82%|████████▏ | 19760/24156 [17:32:03<21:15:59, 17.42s/it]


 82%|████████▏ | 19762/24156 [17:32:30<18:57:05, 15.53s/it]
{'loss': 0.356, 'learning_rate': 1.9182347912218115e-06, 'rewards/chosen': -1.9832528829574585, 'rewards/rejected': -2.9497475624084473, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9664944410324097, 'policy_logps/rejected': -612.344970703125, 'policy_logps/chosen': -380.9845886230469, 'referece_logps/rejected': -582.8474731445312, 'referece_logps/chosen': -361.1520690917969, 'logits/rejected': -0.6650514006614685, 'logits/chosen': -0.4351147711277008, 'epoch': 7.36}

 82%|████████▏ | 19763/24156 [17:32:44<18:34:12, 15.22s/it]


 82%|████████▏ | 19765/24156 [17:33:16<18:50:40, 15.45s/it]
{'loss': 0.3505, 'learning_rate': 1.9180754170597505e-06, 'rewards/chosen': -1.160852074623108, 'rewards/rejected': -4.369042873382568, 'rewards/accuracies': 1.0, 'rewards/margins': 3.208190441131592, 'policy_logps/rejected': -381.0333557128906, 'policy_logps/chosen': -475.3900451660156, 'referece_logps/rejected': -337.34295654296875, 'referece_logps/chosen': -463.78155517578125, 'logits/rejected': -0.3347407281398773, 'logits/chosen': -0.39722999930381775, 'epoch': 7.36}


 82%|████████▏ | 19767/24156 [17:33:52<20:48:14, 17.06s/it]

 82%|████████▏ | 19768/24156 [17:34:04<18:56:49, 15.54s/it]
{'loss': 0.262, 'learning_rate': 1.917915894359215e-06, 'rewards/chosen': -1.5622038841247559, 'rewards/rejected': -2.778930187225342, 'rewards/accuracies': 1.0, 'rewards/margins': 1.216726303100586, 'policy_logps/rejected': -395.0875244140625, 'policy_logps/chosen': -532.427734375, 'referece_logps/rejected': -367.29827880859375, 'referece_logps/chosen': -516.8056640625, 'logits/rejected': -0.07047998905181885, 'logits/chosen': -0.018913306295871735, 'epoch': 7.37}

 82%|████████▏ | 19769/24156 [17:34:21<19:31:29, 16.02s/it]


 82%|████████▏ | 19771/24156 [17:34:50<18:10:39, 14.92s/it]
{'loss': 0.3681, 'learning_rate': 1.9177562231460147e-06, 'rewards/chosen': -1.8421566486358643, 'rewards/rejected': -3.0894205570220947, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2472639083862305, 'policy_logps/rejected': -320.9571228027344, 'policy_logps/chosen': -378.0035095214844, 'referece_logps/rejected': -290.0628967285156, 'referece_logps/chosen': -359.5819396972656, 'logits/rejected': -0.3109952211380005, 'logits/chosen': -0.39013659954071045, 'epoch': 7.37}

 82%|████████▏ | 19772/24156 [17:35:10<19:50:15, 16.29s/it]

 82%|████████▏ | 19773/24156 [17:35:22<18:28:41, 15.18s/it]

 82%|████████▏ | 19774/24156 [17:35:39<19:08:26, 15.72s/it]

 82%|████████▏ | 19775/24156 [17:35:57<19:44:52, 16.23s/it]


 82%|████████▏ | 19777/24156 [17:36:28<18:55:44, 15.56s/it]
{'loss': 0.4632, 'learning_rate': 1.917436435284978e-06, 'rewards/chosen': -1.9692363739013672, 'rewards/rejected': -3.2517199516296387, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2824832201004028, 'policy_logps/rejected': -312.9658508300781, 'policy_logps/chosen': -443.61273193359375, 'referece_logps/rejected': -280.4486389160156, 'referece_logps/chosen': -423.9203796386719, 'logits/rejected': -0.010205402038991451, 'logits/chosen': -0.04818776249885559, 'epoch': 7.37}

 82%|████████▏ | 19778/24156 [17:36:39<17:09:34, 14.11s/it]
[2024-04-06 08:41:48,394] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 82%|████████▏ | 19780/24156 [17:37:02<15:31:29, 12.77s/it]
{'loss': 0.4315, 'learning_rate': 1.9172763186888814e-06, 'rewards/chosen': -1.3494925498962402, 'rewards/rejected': -2.413933038711548, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0644406080245972, 'policy_logps/rejected': -356.86700439453125, 'policy_logps/chosen': -537.9826049804688, 'referece_logps/rejected': -332.7276611328125, 'referece_logps/chosen': -524.4876708984375, 'logits/rejected': 0.09490178525447845, 'logits/chosen': -0.012188822031021118, 'epoch': 7.37}

 82%|████████▏ | 19781/24156 [17:37:25<19:04:33, 15.70s/it]


 82%|████████▏ | 19783/24156 [17:37:50<17:28:56, 14.39s/it]
{'loss': 0.5402, 'learning_rate': 1.9171160536835996e-06, 'rewards/chosen': -2.3500728607177734, 'rewards/rejected': -3.0665221214294434, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7164491415023804, 'policy_logps/rejected': -501.0263671875, 'policy_logps/chosen': -460.2790832519531, 'referece_logps/rejected': -470.3611145019531, 'referece_logps/chosen': -436.7783203125, 'logits/rejected': -0.9079169631004333, 'logits/chosen': -0.931145191192627, 'epoch': 7.37}

 82%|████████▏ | 19784/24156 [17:38:12<20:01:57, 16.50s/it]

 82%|████████▏ | 19785/24156 [17:38:34<22:09:47, 18.25s/it]


 82%|████████▏ | 19787/24156 [17:39:03<19:15:38, 15.87s/it]
{'loss': 0.3575, 'learning_rate': 1.916902136195977e-06, 'rewards/chosen': -1.2171810865402222, 'rewards/rejected': -2.677503824234009, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4603230953216553, 'policy_logps/rejected': -360.48345947265625, 'policy_logps/chosen': -438.1624450683594, 'referece_logps/rejected': -333.7084045410156, 'referece_logps/chosen': -425.99066162109375, 'logits/rejected': -0.6722837686538696, 'logits/chosen': -0.7909334301948547, 'epoch': 7.37}

 82%|████████▏ | 19788/24156 [17:39:17<18:53:55, 15.58s/it]

 82%|████████▏ | 19789/24156 [17:39:34<19:15:48, 15.88s/it]

 82%|████████▏ | 19790/24156 [17:39:57<21:46:26, 17.95s/it]

 82%|████████▏ | 19791/24156 [17:40:07<19:07:30, 15.77s/it]

 82%|████████▏ | 19792/24156 [17:40:18<17:17:05, 14.26s/it]


 82%|████████▏ | 19794/24156 [17:40:45<16:52:23, 13.93s/it]
{'loss': 0.4117, 'learning_rate': 1.916527146027161e-06, 'rewards/chosen': -1.1371116638183594, 'rewards/rejected': -2.4774534702301025, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3403419256210327, 'policy_logps/rejected': -396.0526123046875, 'policy_logps/chosen': -471.0604248046875, 'referece_logps/rejected': -371.278076171875, 'referece_logps/chosen': -459.6893615722656, 'logits/rejected': -0.2043624222278595, 'logits/chosen': -0.214026540517807, 'epoch': 7.37}

 82%|████████▏ | 19795/24156 [17:41:01<17:57:12, 14.82s/it]

 82%|████████▏ | 19796/24156 [17:41:12<16:27:31, 13.59s/it]

 82%|████████▏ | 19797/24156 [17:41:31<18:26:02, 15.22s/it]

 82%|████████▏ | 19798/24156 [17:41:43<17:17:09, 14.28s/it]

 82%|████████▏ | 19799/24156 [17:42:03<19:09:40, 15.83s/it]
[2024-04-06 08:47:21,066] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 82%|████████▏ | 19801/24156 [17:42:41<20:54:27, 17.28s/it]
{'loss': 0.294, 'learning_rate': 1.9161513485127544e-06, 'rewards/chosen': -1.8314902782440186, 'rewards/rejected': -3.5084667205810547, 'rewards/accuracies': 0.875, 'rewards/margins': 1.676975965499878, 'policy_logps/rejected': -372.1054382324219, 'policy_logps/chosen': -507.93426513671875, 'referece_logps/rejected': -337.020751953125, 'referece_logps/chosen': -489.6193542480469, 'logits/rejected': 0.8165339231491089, 'logits/chosen': 0.7974433302879333, 'epoch': 7.38}

 82%|████████▏ | 19802/24156 [17:42:59<21:11:37, 17.52s/it]

 82%|████████▏ | 19803/24156 [17:43:21<22:48:27, 18.86s/it]

 82%|████████▏ | 19804/24156 [17:43:35<21:14:19, 17.57s/it]

 82%|████████▏ | 19805/24156 [17:43:54<21:40:13, 17.93s/it]

 82%|████████▏ | 19806/24156 [17:44:13<21:59:48, 18.20s/it]


 82%|████████▏ | 19808/24156 [17:44:41<19:00:43, 15.74s/it]
{'loss': 0.4286, 'learning_rate': 1.9157747439837867e-06, 'rewards/chosen': -1.9524335861206055, 'rewards/rejected': -3.3181049823760986, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3656710386276245, 'policy_logps/rejected': -372.8750305175781, 'policy_logps/chosen': -396.48199462890625, 'referece_logps/rejected': -339.6939697265625, 'referece_logps/chosen': -376.9576110839844, 'logits/rejected': -1.071941614151001, 'logits/chosen': -1.0544869899749756, 'epoch': 7.38}

 82%|████████▏ | 19809/24156 [17:44:54<18:02:43, 14.94s/it]

 82%|████████▏ | 19810/24156 [17:45:08<17:34:58, 14.56s/it]

 82%|████████▏ | 19811/24156 [17:45:18<16:10:45, 13.41s/it]

 82%|████████▏ | 19812/24156 [17:45:30<15:32:34, 12.88s/it]
[2024-04-06 08:50:47,681] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19813/24156 [17:45:50<18:11:27, 15.08s/it]

 82%|████████▏ | 19814/24156 [17:46:10<19:48:50, 16.43s/it]

 82%|████████▏ | 19815/24156 [17:46:22<18:24:48, 15.27s/it]

 82%|████████▏ | 19816/24156 [17:46:42<20:09:02, 16.71s/it]


 82%|████████▏ | 19818/24156 [17:47:13<19:40:25, 16.33s/it]
{'loss': 0.3465, 'learning_rate': 1.915235338248446e-06, 'rewards/chosen': -2.0641045570373535, 'rewards/rejected': -2.891469955444336, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8273653984069824, 'policy_logps/rejected': -310.3686828613281, 'policy_logps/chosen': -283.8122863769531, 'referece_logps/rejected': -281.4539794921875, 'referece_logps/chosen': -263.1712646484375, 'logits/rejected': -0.41522130370140076, 'logits/chosen': -0.3414784073829651, 'epoch': 7.38}


 82%|████████▏ | 19820/24156 [17:47:55<22:36:57, 18.78s/it]
{'loss': 0.4124, 'learning_rate': 1.9151272596316977e-06, 'rewards/chosen': -1.963141679763794, 'rewards/rejected': -3.615501880645752, 'rewards/accuracies': 0.875, 'rewards/margins': 1.652360200881958, 'policy_logps/rejected': -526.2606811523438, 'policy_logps/chosen': -459.09332275390625, 'referece_logps/rejected': -490.10565185546875, 'referece_logps/chosen': -439.4618835449219, 'logits/rejected': -0.7424172759056091, 'logits/chosen': -0.6463592052459717, 'epoch': 7.38}
[2024-04-06 08:53:13,270] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19821/24156 [17:48:16<23:17:57, 19.35s/it]


 82%|████████▏ | 19823/24156 [17:48:51<21:58:56, 18.26s/it]
{'loss': 0.4347, 'learning_rate': 1.9149650183244355e-06, 'rewards/chosen': -2.0698983669281006, 'rewards/rejected': -2.6627724170684814, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5928740501403809, 'policy_logps/rejected': -390.576171875, 'policy_logps/chosen': -524.1401977539062, 'referece_logps/rejected': -363.94842529296875, 'referece_logps/chosen': -503.441162109375, 'logits/rejected': -0.0036106258630752563, 'logits/chosen': 0.008139356970787048, 'epoch': 7.39}
[2024-04-06 08:54:10,232] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19824/24156 [17:49:13<23:14:31, 19.31s/it]
[2024-04-06 08:54:30,789] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19825/24156 [17:49:33<23:41:05, 19.69s/it]


 82%|████████▏ | 19827/24156 [17:50:03<20:45:30, 17.26s/it]
{'loss': 0.4056, 'learning_rate': 1.914748466308931e-06, 'rewards/chosen': -2.252438545227051, 'rewards/rejected': -3.8054113388061523, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5529727935791016, 'policy_logps/rejected': -414.1288757324219, 'policy_logps/chosen': -258.14691162109375, 'referece_logps/rejected': -376.0747375488281, 'referece_logps/chosen': -235.62249755859375, 'logits/rejected': -1.0602259635925293, 'logits/chosen': -0.6923940777778625, 'epoch': 7.39}


 82%|████████▏ | 19829/24156 [17:50:43<22:37:07, 18.82s/it]
[2024-04-06 08:55:40,663] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3574, 'learning_rate': 1.914640091630486e-06, 'rewards/chosen': -1.625139832496643, 'rewards/rejected': -4.274433135986328, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6492931842803955, 'policy_logps/rejected': -435.55389404296875, 'policy_logps/chosen': -375.06439208984375, 'referece_logps/rejected': -392.80950927734375, 'referece_logps/chosen': -358.8130187988281, 'logits/rejected': -0.27407869696617126, 'logits/chosen': -0.2890763282775879, 'epoch': 7.39}

 82%|████████▏ | 19830/24156 [17:51:02<22:45:34, 18.94s/it]

 82%|████████▏ | 19831/24156 [17:51:21<22:29:12, 18.72s/it]

 82%|████████▏ | 19832/24156 [17:51:39<22:18:51, 18.58s/it]

 82%|████████▏ | 19833/24156 [17:51:51<19:53:42, 16.57s/it]

 82%|████████▏ | 19834/24156 [17:52:05<18:55:29, 15.76s/it]

 82%|████████▏ | 19835/24156 [17:52:27<21:16:02, 17.72s/it]

 82%|████████▏ | 19836/24156 [17:52:48<22:36:04, 18.83s/it]

 82%|████████▏ | 19837/24156 [17:53:09<23:25:25, 19.52s/it]

 82%|████████▏ | 19838/24156 [17:53:28<23:01:06, 19.19s/it]

 82%|████████▏ | 19839/24156 [17:53:51<24:21:38, 20.31s/it]

 82%|████████▏ | 19840/24156 [17:54:02<20:53:27, 17.43s/it]

 82%|████████▏ | 19841/24156 [17:54:16<19:53:28, 16.60s/it]

 82%|████████▏ | 19842/24156 [17:54:32<19:42:12, 16.44s/it]

 82%|████████▏ | 19843/24156 [17:54:49<19:39:19, 16.41s/it]

 82%|████████▏ | 19844/24156 [17:55:09<20:55:17, 17.47s/it]

 82%|████████▏ | 19845/24156 [17:55:20<18:53:37, 15.78s/it]


 82%|████████▏ | 19847/24156 [17:55:57<19:59:18, 16.70s/it]
{'loss': 0.2991, 'learning_rate': 1.9136617608079485e-06, 'rewards/chosen': -1.7094477415084839, 'rewards/rejected': -4.305164337158203, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5957164764404297, 'policy_logps/rejected': -339.2586364746094, 'policy_logps/chosen': -366.40869140625, 'referece_logps/rejected': -296.2070007324219, 'referece_logps/chosen': -349.3141784667969, 'logits/rejected': 0.43032947182655334, 'logits/chosen': 0.40095436573028564, 'epoch': 7.39}

 82%|████████▏ | 19848/24156 [17:56:16<20:42:19, 17.30s/it]

 82%|████████▏ | 19849/24156 [17:56:38<22:22:59, 18.71s/it]

 82%|████████▏ | 19850/24156 [17:56:56<22:02:48, 18.43s/it]


 82%|████████▏ | 19852/24156 [17:57:31<21:19:09, 17.83s/it]
{'loss': 0.3648, 'learning_rate': 1.913389057568864e-06, 'rewards/chosen': -2.501962661743164, 'rewards/rejected': -3.6746363639831543, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1726737022399902, 'policy_logps/rejected': -357.2529602050781, 'policy_logps/chosen': -338.69500732421875, 'referece_logps/rejected': -320.506591796875, 'referece_logps/chosen': -313.6753845214844, 'logits/rejected': -0.3559664487838745, 'logits/chosen': -0.12578371167182922, 'epoch': 7.4}

 82%|████████▏ | 19853/24156 [17:57:51<21:58:13, 18.38s/it]

 82%|████████▏ | 19854/24156 [17:58:13<23:13:10, 19.43s/it]

 82%|████████▏ | 19855/24156 [17:58:31<22:32:52, 18.87s/it]

 82%|████████▏ | 19856/24156 [17:58:49<22:14:12, 18.62s/it]

 82%|████████▏ | 19857/24156 [17:59:07<22:00:12, 18.43s/it]

 82%|████████▏ | 19858/24156 [17:59:17<19:17:08, 16.15s/it]

 82%|████████▏ | 19859/24156 [17:59:36<20:18:35, 17.02s/it]

 82%|████████▏ | 19860/24156 [17:59:52<19:55:18, 16.69s/it]

 82%|████████▏ | 19861/24156 [18:00:10<20:20:51, 17.06s/it]

 82%|████████▏ | 19862/24156 [18:00:30<21:13:20, 17.79s/it]

 82%|████████▏ | 19863/24156 [18:00:46<20:39:24, 17.32s/it]

 82%|████████▏ | 19864/24156 [18:01:06<21:30:12, 18.04s/it]

 82%|████████▏ | 19865/24156 [18:01:18<19:20:46, 16.23s/it]

 82%|████████▏ | 19866/24156 [18:01:36<20:07:13, 16.88s/it]

 82%|████████▏ | 19867/24156 [18:01:52<19:49:05, 16.63s/it]

 82%|████████▏ | 19868/24156 [18:02:07<19:13:25, 16.14s/it]

 82%|████████▏ | 19869/24156 [18:02:27<20:21:46, 17.10s/it]

 82%|████████▏ | 19870/24156 [18:02:40<19:07:13, 16.06s/it]

 82%|████████▏ | 19871/24156 [18:02:55<18:51:16, 15.84s/it]

 82%|████████▏ | 19872/24156 [18:03:18<21:08:22, 17.76s/it]

 82%|████████▏ | 19873/24156 [18:03:36<21:24:51, 18.00s/it]
[2024-04-06 09:08:54,225] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19874/24156 [18:03:57<22:17:18, 18.74s/it]
[2024-04-06 09:09:08,102] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19875/24156 [18:04:11<20:32:57, 17.28s/it]

 82%|████████▏ | 19876/24156 [18:04:28<20:37:02, 17.34s/it]

 82%|████████▏ | 19877/24156 [18:04:48<21:24:42, 18.01s/it]

 82%|████████▏ | 19878/24156 [18:05:09<22:29:19, 18.92s/it]

 82%|████████▏ | 19879/24156 [18:05:20<19:52:05, 16.72s/it]

 82%|████████▏ | 19880/24156 [18:05:38<20:19:30, 17.11s/it]

 82%|████████▏ | 19881/24156 [18:05:59<21:27:26, 18.07s/it]

 82%|████████▏ | 19882/24156 [18:06:09<18:52:03, 15.89s/it]

 82%|████████▏ | 19883/24156 [18:06:22<17:39:29, 14.88s/it]

 82%|████████▏ | 19884/24156 [18:06:38<17:56:32, 15.12s/it]
[2024-04-06 09:11:53,561] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 19885/24156 [18:06:56<19:06:52, 16.11s/it]

 82%|████████▏ | 19886/24156 [18:07:13<19:32:15, 16.47s/it]

 82%|████████▏ | 19887/24156 [18:07:33<20:35:32, 17.37s/it]

 82%|████████▏ | 19888/24156 [18:07:52<21:23:10, 18.04s/it]

 82%|████████▏ | 19889/24156 [18:08:03<18:45:47, 15.83s/it]

 82%|████████▏ | 19890/24156 [18:08:17<18:14:25, 15.39s/it]

 82%|████████▏ | 19891/24156 [18:08:29<16:41:17, 14.09s/it]

 82%|████████▏ | 19892/24156 [18:08:43<16:45:23, 14.15s/it]

 82%|████████▏ | 19893/24156 [18:09:02<18:41:27, 15.78s/it]

 82%|████████▏ | 19894/24156 [18:09:18<18:36:23, 15.72s/it]

 82%|████████▏ | 19895/24156 [18:09:39<20:20:12, 17.18s/it]

 82%|████████▏ | 19896/24156 [18:09:57<20:43:39, 17.52s/it]

 82%|████████▏ | 19897/24156 [18:10:13<20:16:52, 17.14s/it]

 82%|████████▏ | 19898/24156 [18:10:29<19:47:37, 16.73s/it]

 82%|████████▏ | 19899/24156 [18:10:45<19:36:00, 16.58s/it]

 82%|████████▏ | 19900/24156 [18:11:08<21:42:08, 18.36s/it]

 82%|████████▏ | 19901/24156 [18:11:19<19:15:51, 16.30s/it]

 82%|████████▏ | 19902/24156 [18:11:31<17:39:47, 14.95s/it]

 82%|████████▏ | 19903/24156 [18:11:47<18:02:20, 15.27s/it]

 82%|████████▏ | 19904/24156 [18:11:59<16:57:10, 14.35s/it]

 82%|████████▏ | 19905/24156 [18:12:10<15:39:12, 13.26s/it]

 82%|████████▏ | 19906/24156 [18:12:27<16:57:13, 14.36s/it]

 82%|████████▏ | 19907/24156 [18:12:38<15:39:20, 13.26s/it]

 82%|████████▏ | 19908/24156 [18:12:49<14:55:37, 12.65s/it]

 82%|████████▏ | 19909/24156 [18:13:00<14:28:26, 12.27s/it]

 82%|████████▏ | 19910/24156 [18:13:13<14:46:14, 12.52s/it]

 82%|████████▏ | 19911/24156 [18:13:32<16:59:42, 14.41s/it]

 82%|████████▏ | 19912/24156 [18:13:50<18:16:38, 15.50s/it]

 82%|████████▏ | 19913/24156 [18:14:07<18:50:36, 15.99s/it]

 82%|████████▏ | 19914/24156 [18:14:23<18:47:56, 15.95s/it]

 82%|████████▏ | 19915/24156 [18:14:43<20:07:43, 17.09s/it]

 82%|████████▏ | 19916/24156 [18:15:00<20:13:58, 17.18s/it]

 82%|████████▏ | 19917/24156 [18:15:20<21:17:02, 18.08s/it]

 82%|████████▏ | 19918/24156 [18:15:34<19:42:18, 16.74s/it]

 82%|████████▏ | 19919/24156 [18:15:56<21:35:10, 18.34s/it]

 82%|████████▏ | 19920/24156 [18:16:16<22:00:21, 18.70s/it]

 82%|████████▏ | 19921/24156 [18:16:26<19:12:24, 16.33s/it]

 82%|████████▏ | 19922/24156 [18:16:37<17:11:13, 14.61s/it]

 82%|████████▏ | 19923/24156 [18:16:56<18:40:46, 15.89s/it]


 82%|████████▏ | 19925/24156 [18:17:21<16:41:48, 14.21s/it]

 82%|████████▏ | 19926/24156 [18:17:31<15:26:55, 13.15s/it]

 82%|████████▏ | 19927/24156 [18:17:42<14:34:09, 12.40s/it]

 82%|████████▏ | 19928/24156 [18:18:00<16:20:56, 13.92s/it]

 83%|████████▎ | 19929/24156 [18:18:20<18:32:50, 15.80s/it]

 83%|████████▎ | 19930/24156 [18:18:39<19:52:19, 16.93s/it]
[2024-04-06 09:23:36,808] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 19931/24156 [18:18:53<18:38:55, 15.89s/it]

 83%|████████▎ | 19932/24156 [18:19:10<19:03:57, 16.25s/it]

 83%|████████▎ | 19933/24156 [18:19:27<19:20:56, 16.49s/it]

 83%|████████▎ | 19934/24156 [18:19:48<21:04:35, 17.97s/it]

 83%|████████▎ | 19935/24156 [18:20:05<20:45:07, 17.70s/it]

 83%|████████▎ | 19936/24156 [18:20:18<18:49:17, 16.06s/it]

 83%|████████▎ | 19937/24156 [18:20:35<19:06:44, 16.31s/it]

 83%|████████▎ | 19938/24156 [18:20:45<17:07:58, 14.62s/it]

 83%|████████▎ | 19939/24156 [18:21:01<17:27:09, 14.90s/it]

 83%|████████▎ | 19940/24156 [18:21:12<16:19:42, 13.94s/it]
{'loss': 0.3145, 'learning_rate': 1.908522401627609e-06, 'rewards/chosen': -1.7602393627166748, 'rewards/rejected': -3.5179269313812256, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7576873302459717, 'policy_logps/rejected': -445.25604248046875, 'policy_logps/chosen': -435.0777893066406, 'referece_logps/rejected': -410.0767822265625, 'referece_logps/chosen': -417.4753723144531, 'logits/rejected': 0.3231499195098877, 'logits/chosen': 0.32770460844039917, 'epoch': 7.43}
[2024-04-06 09:26:32,793] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 19942/24156 [18:21:55<20:29:43, 17.51s/it]
[2024-04-06 09:26:52,411] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 19943/24156 [18:22:07<18:30:45, 15.82s/it]

 83%|████████▎ | 19944/24156 [18:22:19<17:19:22, 14.81s/it]
{'loss': 0.3539, 'learning_rate': 1.9082981796511878e-06, 'rewards/chosen': -1.3384456634521484, 'rewards/rejected': -3.354323625564575, 'rewards/accuracies': 1.0, 'rewards/margins': 2.015878438949585, 'policy_logps/rejected': -183.85562133789062, 'policy_logps/chosen': -206.2904510498047, 'referece_logps/rejected': -150.3123779296875, 'referece_logps/chosen': -192.906005859375, 'logits/rejected': -0.13078851997852325, 'logits/chosen': -0.11448302865028381, 'epoch': 7.43}


 83%|████████▎ | 19946/24156 [18:22:50<18:00:51, 15.40s/it]

 83%|████████▎ | 19947/24156 [18:23:02<16:55:52, 14.48s/it]

 83%|████████▎ | 19948/24156 [18:23:23<19:05:57, 16.34s/it]

 83%|████████▎ | 19949/24156 [18:23:36<17:53:23, 15.31s/it]

 83%|████████▎ | 19950/24156 [18:23:48<16:51:17, 14.43s/it]

 83%|████████▎ | 19951/24156 [18:24:08<18:40:01, 15.98s/it]

 83%|████████▎ | 19952/24156 [18:24:26<19:30:15, 16.70s/it]
{'loss': 0.3522, 'learning_rate': 1.9078489519944815e-06, 'rewards/chosen': -1.7539584636688232, 'rewards/rejected': -3.1231837272644043, 'rewards/accuracies': 0.875, 'rewards/margins': 1.369225263595581, 'policy_logps/rejected': -290.7247314453125, 'policy_logps/chosen': -282.804931640625, 'referece_logps/rejected': -259.492919921875, 'referece_logps/chosen': -265.2653503417969, 'logits/rejected': -0.5237577557563782, 'logits/chosen': -0.5115134716033936, 'epoch': 7.43}


 83%|████████▎ | 19954/24156 [18:24:55<18:17:51, 15.68s/it]

 83%|████████▎ | 19955/24156 [18:25:05<16:33:17, 14.19s/it]

 83%|████████▎ | 19956/24156 [18:25:20<16:36:16, 14.23s/it]

 83%|████████▎ | 19957/24156 [18:25:36<17:27:38, 14.97s/it]

 83%|████████▎ | 19958/24156 [18:25:47<16:02:26, 13.76s/it]

 83%|████████▎ | 19959/24156 [18:26:07<18:05:11, 15.51s/it]

 83%|████████▎ | 19960/24156 [18:26:27<19:46:49, 16.97s/it]

 83%|████████▎ | 19961/24156 [18:26:39<17:59:55, 15.45s/it]
{'loss': 0.3445, 'learning_rate': 1.9073423223934215e-06, 'rewards/chosen': -1.874204158782959, 'rewards/rejected': -2.5147652626037598, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6405611634254456, 'policy_logps/rejected': -614.0350952148438, 'policy_logps/chosen': -391.2677001953125, 'referece_logps/rejected': -588.8873901367188, 'referece_logps/chosen': -372.525634765625, 'logits/rejected': -2.1355631351470947, 'logits/chosen': -1.9438481330871582, 'epoch': 7.44}


 83%|████████▎ | 19963/24156 [18:27:13<19:08:52, 16.44s/it]

 83%|████████▎ | 19964/24156 [18:27:34<20:55:46, 17.97s/it]

 83%|████████▎ | 19965/24156 [18:27:48<19:30:09, 16.75s/it]

 83%|████████▎ | 19966/24156 [18:28:04<19:05:50, 16.41s/it]

 83%|████████▎ | 19967/24156 [18:28:27<21:42:32, 18.66s/it]

 83%|████████▎ | 19968/24156 [18:28:44<21:04:30, 18.12s/it]

 83%|████████▎ | 19969/24156 [18:28:55<18:36:28, 16.00s/it]

 83%|████████▎ | 19970/24156 [18:29:15<19:46:37, 17.01s/it]
{'loss': 0.4186, 'learning_rate': 1.906834371575121e-06, 'rewards/chosen': -2.227947950363159, 'rewards/rejected': -4.212075710296631, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9841281175613403, 'policy_logps/rejected': -485.19775390625, 'policy_logps/chosen': -528.94140625, 'referece_logps/rejected': -443.0770263671875, 'referece_logps/chosen': -506.6619873046875, 'logits/rejected': 0.6855452060699463, 'logits/chosen': 0.42875707149505615, 'epoch': 7.44}

 83%|████████▎ | 19971/24156 [18:29:32<19:47:19, 17.02s/it]


 83%|████████▎ | 19973/24156 [18:29:59<17:20:43, 14.93s/it]

 83%|████████▎ | 19974/24156 [18:30:18<18:59:22, 16.35s/it]

 83%|████████▎ | 19975/24156 [18:30:31<17:49:24, 15.35s/it]

 83%|████████▎ | 19976/24156 [18:30:45<17:17:39, 14.89s/it]

 83%|████████▎ | 19977/24156 [18:31:06<19:21:45, 16.68s/it]
{'loss': 0.3796, 'learning_rate': 1.9064383857557148e-06, 'rewards/chosen': -1.9934475421905518, 'rewards/rejected': -4.478857517242432, 'rewards/accuracies': 0.75, 'rewards/margins': 2.48540997505188, 'policy_logps/rejected': -492.8511962890625, 'policy_logps/chosen': -479.2505187988281, 'referece_logps/rejected': -448.0626220703125, 'referece_logps/chosen': -459.3160400390625, 'logits/rejected': -0.19829687476158142, 'logits/chosen': -0.22920109331607819, 'epoch': 7.44}


 83%|████████▎ | 19979/24156 [18:31:40<20:00:37, 17.25s/it]
[2024-04-06 09:36:37,295] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 19980/24156 [18:32:01<21:21:05, 18.41s/it]
[2024-04-06 09:36:58,409] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 19981/24156 [18:32:17<20:25:27, 17.61s/it]

 83%|████████▎ | 19982/24156 [18:32:29<18:35:14, 16.03s/it]

 83%|████████▎ | 19983/24156 [18:32:50<20:14:47, 17.47s/it]

 83%|████████▎ | 19984/24156 [18:33:13<22:18:16, 19.25s/it]
[2024-04-06 09:38:10,724] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 19985/24156 [18:33:35<23:19:46, 20.14s/it]
{'loss': 0.3397, 'learning_rate': 1.9059848528494294e-06, 'rewards/chosen': -1.7840532064437866, 'rewards/rejected': -3.0530731678009033, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2690200805664062, 'policy_logps/rejected': -505.6604919433594, 'policy_logps/chosen': -269.5550231933594, 'referece_logps/rejected': -475.12969970703125, 'referece_logps/chosen': -251.71450805664062, 'logits/rejected': -0.050584543496370316, 'logits/chosen': 0.1265489012002945, 'epoch': 7.45}


 83%|████████▎ | 19987/24156 [18:34:12<22:18:45, 19.27s/it]
[2024-04-06 09:39:09,396] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3639, 'learning_rate': 1.9058713067332378e-06, 'rewards/chosen': -1.0035148859024048, 'rewards/rejected': -3.4765191078186035, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4730043411254883, 'policy_logps/rejected': -311.159912109375, 'policy_logps/chosen': -358.1322021484375, 'referece_logps/rejected': -276.39471435546875, 'referece_logps/chosen': -348.0970458984375, 'logits/rejected': -0.6327813863754272, 'logits/chosen': -0.730079710483551, 'epoch': 7.45}

 83%|████████▎ | 19988/24156 [18:34:30<22:02:23, 19.04s/it]

 83%|████████▎ | 19989/24156 [18:34:42<19:34:38, 16.91s/it]


 83%|████████▎ | 19991/24156 [18:35:20<20:35:13, 17.79s/it]
{'loss': 0.3491, 'learning_rate': 1.905644019090448e-06, 'rewards/chosen': -1.44911789894104, 'rewards/rejected': -2.795438051223755, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3463200330734253, 'policy_logps/rejected': -355.1489562988281, 'policy_logps/chosen': -332.86627197265625, 'referece_logps/rejected': -327.1946105957031, 'referece_logps/chosen': -318.37506103515625, 'logits/rejected': -0.024284591898322105, 'logits/chosen': 0.0033211037516593933, 'epoch': 7.45}

 83%|████████▎ | 19992/24156 [18:35:36<20:05:19, 17.37s/it]


 83%|████████▎ | 19994/24156 [18:36:03<17:37:35, 15.25s/it]

 83%|████████▎ | 19995/24156 [18:36:16<16:46:22, 14.51s/it]
{'loss': 0.2892, 'learning_rate': 1.9054164709549365e-06, 'rewards/chosen': -1.9203840494155884, 'rewards/rejected': -4.221357345581055, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3009729385375977, 'policy_logps/rejected': -397.5953063964844, 'policy_logps/chosen': -336.0107421875, 'referece_logps/rejected': -355.3816833496094, 'referece_logps/chosen': -316.8069152832031, 'logits/rejected': -0.2630700469017029, 'logits/chosen': -0.19693124294281006, 'epoch': 7.45}


 83%|████████▎ | 19997/24156 [18:36:47<17:44:23, 15.36s/it]

 83%|████████▎ | 19998/24156 [18:37:00<16:41:23, 14.45s/it]

 83%|████████▎ | 19999/24156 [18:37:19<18:28:48, 16.00s/it]
{'loss': 0.311, 'learning_rate': 1.9051886623921542e-06, 'rewards/chosen': -1.7012012004852295, 'rewards/rejected': -3.5913610458374023, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8901598453521729, 'policy_logps/rejected': -366.6678466796875, 'policy_logps/chosen': -410.31005859375, 'referece_logps/rejected': -330.7542419433594, 'referece_logps/chosen': -393.29803466796875, 'logits/rejected': -1.120381236076355, 'logits/chosen': -1.1158186197280884, 'epoch': 7.45}


 83%|████████▎ | 20001/24156 [18:38:09<24:33:33, 21.28s/it]

 83%|████████▎ | 20002/24156 [18:38:28<24:00:33, 20.81s/it]

 83%|████████▎ | 20003/24156 [18:38:52<24:52:02, 21.56s/it]
[2024-04-06 09:43:49,115] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20004/24156 [18:39:11<24:14:01, 21.01s/it]

 83%|████████▎ | 20005/24156 [18:39:26<22:04:24, 19.14s/it]
{'loss': 0.342, 'learning_rate': 1.9048464613902038e-06, 'rewards/chosen': -1.6831942796707153, 'rewards/rejected': -3.9079906940460205, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2247962951660156, 'policy_logps/rejected': -314.2059631347656, 'policy_logps/chosen': -307.1295166015625, 'referece_logps/rejected': -275.12603759765625, 'referece_logps/chosen': -290.297607421875, 'logits/rejected': -0.9998540878295898, 'logits/chosen': -1.0546855926513672, 'epoch': 7.45}


 83%|████████▎ | 20007/24156 [18:40:06<22:35:51, 19.61s/it]

 83%|████████▎ | 20008/24156 [18:40:17<19:31:49, 16.95s/it]

 83%|████████▎ | 20009/24156 [18:40:31<18:23:40, 15.97s/it]
{'loss': 0.3994, 'learning_rate': 1.9046180020460799e-06, 'rewards/chosen': -1.8670963048934937, 'rewards/rejected': -3.393714427947998, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5266181230545044, 'policy_logps/rejected': -417.43023681640625, 'policy_logps/chosen': -501.5138854980469, 'referece_logps/rejected': -383.49310302734375, 'referece_logps/chosen': -482.8429870605469, 'logits/rejected': -0.855412483215332, 'logits/chosen': -0.8071476817131042, 'epoch': 7.45}


 83%|████████▎ | 20011/24156 [18:40:56<16:21:57, 14.21s/it]

 83%|████████▎ | 20012/24156 [18:41:13<17:09:48, 14.91s/it]

 83%|████████▎ | 20013/24156 [18:41:33<19:04:48, 16.58s/it]

 83%|████████▎ | 20014/24156 [18:41:44<17:03:24, 14.82s/it]

 83%|████████▎ | 20015/24156 [18:42:07<19:51:41, 17.27s/it]
[2024-04-06 09:47:04,073] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20016/24156 [18:42:26<20:44:09, 18.03s/it]

 83%|████████▎ | 20017/24156 [18:42:44<20:30:29, 17.84s/it]
{'loss': 0.2804, 'learning_rate': 1.9041603028308026e-06, 'rewards/chosen': -1.5020400285720825, 'rewards/rejected': -4.820638656616211, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3185982704162598, 'policy_logps/rejected': -528.8505859375, 'policy_logps/chosen': -356.0984802246094, 'referece_logps/rejected': -480.64422607421875, 'referece_logps/chosen': -341.0780944824219, 'logits/rejected': -0.6139928698539734, 'logits/chosen': -0.5478467345237732, 'epoch': 7.46}


 83%|████████▎ | 20019/24156 [18:43:11<17:55:25, 15.60s/it]
{'loss': 0.4326, 'learning_rate': 1.904045715465176e-06, 'rewards/chosen': -1.450427770614624, 'rewards/rejected': -3.677997350692749, 'rewards/accuracies': 0.875, 'rewards/margins': 2.227569341659546, 'policy_logps/rejected': -225.9248504638672, 'policy_logps/chosen': -384.1117248535156, 'referece_logps/rejected': -189.14488220214844, 'referece_logps/chosen': -369.607421875, 'logits/rejected': 0.24690166115760803, 'logits/chosen': 0.15248483419418335, 'epoch': 7.46}


 83%|████████▎ | 20021/24156 [18:43:41<17:50:55, 15.54s/it]

 83%|████████▎ | 20022/24156 [18:44:02<19:46:31, 17.22s/it]
{'loss': 0.3752, 'learning_rate': 1.9038737125288425e-06, 'rewards/chosen': -2.0067946910858154, 'rewards/rejected': -3.9965131282806396, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9897184371948242, 'policy_logps/rejected': -539.21875, 'policy_logps/chosen': -460.7919006347656, 'referece_logps/rejected': -499.25360107421875, 'referece_logps/chosen': -440.72393798828125, 'logits/rejected': -0.25174954533576965, 'logits/chosen': -0.166562020778656, 'epoch': 7.46}

 83%|████████▎ | 20023/24156 [18:44:13<17:46:56, 15.49s/it]

 83%|████████▎ | 20024/24156 [18:44:33<19:17:18, 16.81s/it]


 83%|████████▎ | 20026/24156 [18:45:14<21:21:43, 18.62s/it]
{'loss': 0.3185, 'learning_rate': 1.9036441477996252e-06, 'rewards/chosen': -1.739273190498352, 'rewards/rejected': -2.7548727989196777, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0155998468399048, 'policy_logps/rejected': -498.9745178222656, 'policy_logps/chosen': -415.3429260253906, 'referece_logps/rejected': -471.42578125, 'referece_logps/chosen': -397.9502258300781, 'logits/rejected': 0.060100674629211426, 'logits/chosen': 0.11582747846841812, 'epoch': 7.46}
[2024-04-06 09:50:34,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 20028/24156 [18:45:57<22:52:46, 19.95s/it]
[2024-04-06 09:50:54,450] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20029/24156 [18:46:17<22:57:33, 20.03s/it]

 83%|████████▎ | 20030/24156 [18:46:37<22:47:03, 19.88s/it]

 83%|████████▎ | 20031/24156 [18:46:55<22:04:20, 19.26s/it]

 83%|████████▎ | 20032/24156 [18:47:16<22:47:31, 19.90s/it]

 83%|████████▎ | 20033/24156 [18:47:31<21:14:31, 18.55s/it]

 83%|████████▎ | 20034/24156 [18:47:50<21:16:55, 18.59s/it]
{'loss': 0.3713, 'learning_rate': 1.9031842386548162e-06, 'rewards/chosen': -1.361518383026123, 'rewards/rejected': -3.0502724647521973, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6887538433074951, 'policy_logps/rejected': -345.5328674316406, 'policy_logps/chosen': -384.99554443359375, 'referece_logps/rejected': -315.0301513671875, 'referece_logps/chosen': -371.38037109375, 'logits/rejected': 0.3428613245487213, 'logits/chosen': 0.24495822191238403, 'epoch': 7.46}


 83%|████████▎ | 20036/24156 [18:48:21<19:38:53, 17.17s/it]
[2024-04-06 09:53:18,419] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4514, 'learning_rate': 1.9030690989821747e-06, 'rewards/chosen': -1.9490686655044556, 'rewards/rejected': -2.4593584537506104, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5102897882461548, 'policy_logps/rejected': -405.4971923828125, 'policy_logps/chosen': -513.8701782226562, 'referece_logps/rejected': -380.903564453125, 'referece_logps/chosen': -494.3795166015625, 'logits/rejected': 0.6876985430717468, 'logits/chosen': 0.7311019897460938, 'epoch': 7.46}


 83%|████████▎ | 20038/24156 [18:48:54<19:45:00, 17.27s/it]
[2024-04-06 09:53:51,701] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20039/24156 [18:49:06<17:50:47, 15.61s/it]
{'loss': 0.2911, 'learning_rate': 1.9028962677170063e-06, 'rewards/chosen': -1.58210027217865, 'rewards/rejected': -3.5203633308410645, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9382632970809937, 'policy_logps/rejected': -474.2154846191406, 'policy_logps/chosen': -430.1490478515625, 'referece_logps/rejected': -439.0118713378906, 'referece_logps/chosen': -414.3280334472656, 'logits/rejected': -0.08987052738666534, 'logits/chosen': -0.17357294261455536, 'epoch': 7.47}


 83%|████████▎ | 20041/24156 [18:49:45<19:57:41, 17.46s/it]

 83%|████████▎ | 20042/24156 [18:49:58<18:43:36, 16.39s/it]

 83%|████████▎ | 20043/24156 [18:50:17<19:25:51, 17.01s/it]
{'loss': 0.2379, 'learning_rate': 1.902665598795368e-06, 'rewards/chosen': -1.4410475492477417, 'rewards/rejected': -3.3251521587371826, 'rewards/accuracies': 1.0, 'rewards/margins': 1.88410484790802, 'policy_logps/rejected': -245.63714599609375, 'policy_logps/chosen': -293.7491760253906, 'referece_logps/rejected': -212.3856201171875, 'referece_logps/chosen': -279.33868408203125, 'logits/rejected': -0.09618278592824936, 'logits/chosen': -0.15336205065250397, 'epoch': 7.47}
[2024-04-06 09:55:29,276] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 20045/24156 [18:50:49<19:09:41, 16.78s/it]

 83%|████████▎ | 20046/24156 [18:51:09<20:05:09, 17.59s/it]

 83%|████████▎ | 20047/24156 [18:51:26<20:00:17, 17.53s/it]
{'loss': 0.2627, 'learning_rate': 1.9024346702376988e-06, 'rewards/chosen': -2.100278854370117, 'rewards/rejected': -5.130466938018799, 'rewards/accuracies': 1.0, 'rewards/margins': 3.03018856048584, 'policy_logps/rejected': -483.39007568359375, 'policy_logps/chosen': -378.7539978027344, 'referece_logps/rejected': -432.08544921875, 'referece_logps/chosen': -357.751220703125, 'logits/rejected': -0.4107012152671814, 'logits/chosen': -0.2707611322402954, 'epoch': 7.47}

 83%|████████▎ | 20048/24156 [18:51:48<21:18:25, 18.67s/it]


 83%|████████▎ | 20050/24156 [18:52:14<18:07:30, 15.89s/it]

 83%|████████▎ | 20051/24156 [18:52:27<16:52:35, 14.80s/it]

 83%|████████▎ | 20052/24156 [18:52:48<19:10:49, 16.82s/it]

 83%|████████▎ | 20053/24156 [18:53:10<20:44:36, 18.20s/it]
[2024-04-06 09:58:07,117] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20054/24156 [18:53:25<19:50:57, 17.42s/it]

 83%|████████▎ | 20055/24156 [18:53:45<20:34:00, 18.05s/it]

 83%|████████▎ | 20056/24156 [18:54:04<21:03:56, 18.50s/it]

 83%|████████▎ | 20057/24156 [18:54:18<19:17:04, 16.94s/it]

 83%|████████▎ | 20058/24156 [18:54:39<20:40:45, 18.17s/it]

 83%|████████▎ | 20059/24156 [18:54:59<21:22:01, 18.78s/it]

 83%|████████▎ | 20060/24156 [18:55:11<19:13:55, 16.90s/it]

 83%|████████▎ | 20061/24156 [18:55:24<17:41:52, 15.56s/it]

 83%|████████▎ | 20062/24156 [18:55:43<18:52:15, 16.59s/it]

 83%|████████▎ | 20063/24156 [18:56:00<18:55:02, 16.64s/it]
[2024-04-06 10:00:57,017] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20064/24156 [18:56:21<20:40:52, 18.19s/it]

 83%|████████▎ | 20065/24156 [18:56:38<20:05:49, 17.69s/it]

 83%|████████▎ | 20066/24156 [18:56:59<21:09:48, 18.63s/it]
[2024-04-06 10:01:56,166] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20067/24156 [18:57:20<22:01:40, 19.39s/it]

 83%|████████▎ | 20068/24156 [18:57:34<20:10:26, 17.77s/it]

 83%|████████▎ | 20069/24156 [18:57:44<17:45:04, 15.64s/it]

 83%|████████▎ | 20070/24156 [18:57:56<16:13:13, 14.29s/it]
{'loss': 0.3354, 'learning_rate': 1.9011017958037745e-06, 'rewards/chosen': -1.882473111152649, 'rewards/rejected': -4.5291852951049805, 'rewards/accuracies': 0.625, 'rewards/margins': 2.646712064743042, 'policy_logps/rejected': -353.83056640625, 'policy_logps/chosen': -364.8977966308594, 'referece_logps/rejected': -308.5386962890625, 'referece_logps/chosen': -346.07305908203125, 'logits/rejected': -0.4099782407283783, 'logits/chosen': -0.27508631348609924, 'epoch': 7.48}


 83%|████████▎ | 20072/24156 [18:58:17<14:09:15, 12.48s/it]
{'loss': 0.3638, 'learning_rate': 1.9009854885179875e-06, 'rewards/chosen': -1.8442326784133911, 'rewards/rejected': -3.331435441970825, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4872030019760132, 'policy_logps/rejected': -348.77813720703125, 'policy_logps/chosen': -388.07196044921875, 'referece_logps/rejected': -315.4637756347656, 'referece_logps/chosen': -369.629638671875, 'logits/rejected': 0.20346255600452423, 'logits/chosen': 0.32059839367866516, 'epoch': 7.48}

 83%|████████▎ | 20073/24156 [18:58:34<15:41:12, 13.83s/it]


 83%|████████▎ | 20075/24156 [18:59:07<17:04:05, 15.06s/it]

 83%|████████▎ | 20076/24156 [18:59:19<16:07:17, 14.22s/it]

 83%|████████▎ | 20077/24156 [18:59:36<16:46:48, 14.81s/it]

 83%|████████▎ | 20078/24156 [18:59:55<18:23:42, 16.24s/it]
{'loss': 0.4083, 'learning_rate': 1.900636177964932e-06, 'rewards/chosen': -2.212524175643921, 'rewards/rejected': -3.9229962825775146, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7104722261428833, 'policy_logps/rejected': -344.039306640625, 'policy_logps/chosen': -322.04052734375, 'referece_logps/rejected': -304.8092956542969, 'referece_logps/chosen': -299.9152526855469, 'logits/rejected': 0.05915093794465065, 'logits/chosen': 0.10593074560165405, 'epoch': 7.48}


 83%|████████▎ | 20080/24156 [19:00:23<16:52:26, 14.90s/it]

 83%|████████▎ | 20081/24156 [19:00:38<16:55:21, 14.95s/it]
{'loss': 0.3344, 'learning_rate': 1.9004613040988863e-06, 'rewards/chosen': -2.599058151245117, 'rewards/rejected': -4.372481346130371, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7734230756759644, 'policy_logps/rejected': -320.7069091796875, 'policy_logps/chosen': -467.77960205078125, 'referece_logps/rejected': -276.98211669921875, 'referece_logps/chosen': -441.78900146484375, 'logits/rejected': 0.03931637108325958, 'logits/chosen': 0.01832537353038788, 'epoch': 7.48}


 83%|████████▎ | 20083/24156 [19:01:01<14:59:03, 13.24s/it]
{'loss': 0.3368, 'learning_rate': 1.9003446405816482e-06, 'rewards/chosen': -1.0682332515716553, 'rewards/rejected': -3.176151990890503, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1079187393188477, 'policy_logps/rejected': -389.3397521972656, 'policy_logps/chosen': -389.7021789550781, 'referece_logps/rejected': -357.5782165527344, 'referece_logps/chosen': -379.0198669433594, 'logits/rejected': -0.7893050909042358, 'logits/chosen': -0.7452678084373474, 'epoch': 7.48}


 83%|████████▎ | 20085/24156 [19:01:31<16:20:11, 14.45s/it]

 83%|████████▎ | 20086/24156 [19:01:46<16:29:38, 14.59s/it]

 83%|████████▎ | 20087/24156 [19:02:05<18:03:04, 15.97s/it]

 83%|████████▎ | 20088/24156 [19:02:16<16:20:25, 14.46s/it]
{'loss': 0.3437, 'learning_rate': 1.9000526985601702e-06, 'rewards/chosen': -1.347772479057312, 'rewards/rejected': -3.62805438041687, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2802820205688477, 'policy_logps/rejected': -478.43560791015625, 'policy_logps/chosen': -366.6250305175781, 'referece_logps/rejected': -442.1549987792969, 'referece_logps/chosen': -353.1473388671875, 'logits/rejected': -0.747266411781311, 'logits/chosen': -0.7898787260055542, 'epoch': 7.48}


 83%|████████▎ | 20090/24156 [19:02:54<19:11:30, 16.99s/it]

 83%|████████▎ | 20091/24156 [19:03:06<17:22:41, 15.39s/it]

 83%|████████▎ | 20092/24156 [19:03:20<17:02:36, 15.10s/it]

 83%|████████▎ | 20093/24156 [19:03:31<15:39:26, 13.87s/it]

 83%|████████▎ | 20094/24156 [19:03:52<18:03:49, 16.01s/it]
{'loss': 0.4221, 'learning_rate': 1.8997018341967324e-06, 'rewards/chosen': -2.2789316177368164, 'rewards/rejected': -4.217578887939453, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9386472702026367, 'policy_logps/rejected': -437.58447265625, 'policy_logps/chosen': -400.90521240234375, 'referece_logps/rejected': -395.40869140625, 'referece_logps/chosen': -378.11590576171875, 'logits/rejected': -0.55903160572052, 'logits/chosen': -0.7510738968849182, 'epoch': 7.49}


 83%|████████▎ | 20096/24156 [19:04:17<16:04:47, 14.26s/it]

 83%|████████▎ | 20097/24156 [19:04:28<14:52:05, 13.19s/it]

 83%|████████▎ | 20098/24156 [19:04:50<17:42:57, 15.72s/it]
[2024-04-06 10:09:47,094] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3109, 'learning_rate': 1.8994676011276401e-06, 'rewards/chosen': -2.25480580329895, 'rewards/rejected': -5.818966388702393, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5641603469848633, 'policy_logps/rejected': -629.0219116210938, 'policy_logps/chosen': -500.9031982421875, 'referece_logps/rejected': -570.832275390625, 'referece_logps/chosen': -478.3551330566406, 'logits/rejected': 0.047113582491874695, 'logits/chosen': 0.180148184299469, 'epoch': 7.49}
[2024-04-06 10:10:02,126] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 20100/24156 [19:05:28<20:09:28, 17.89s/it]

 83%|████████▎ | 20101/24156 [19:05:48<20:47:59, 18.47s/it]

 83%|████████▎ | 20102/24156 [19:06:04<19:54:02, 17.67s/it]
{'loss': 0.4817, 'learning_rate': 1.8992331093423655e-06, 'rewards/chosen': -2.667604446411133, 'rewards/rejected': -4.002065181732178, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3344603776931763, 'policy_logps/rejected': -483.82977294921875, 'policy_logps/chosen': -401.28997802734375, 'referece_logps/rejected': -443.80914306640625, 'referece_logps/chosen': -374.6138916015625, 'logits/rejected': -1.025722861289978, 'logits/chosen': -0.9809713959693909, 'epoch': 7.49}


 83%|████████▎ | 20104/24156 [19:06:40<20:25:15, 18.14s/it]

 83%|████████▎ | 20105/24156 [19:06:54<19:04:55, 16.96s/it]
{'loss': 0.3796, 'learning_rate': 1.8990570707614859e-06, 'rewards/chosen': -2.2843070030212402, 'rewards/rejected': -3.779696464538574, 'rewards/accuracies': 0.875, 'rewards/margins': 1.495389461517334, 'policy_logps/rejected': -301.8477478027344, 'policy_logps/chosen': -314.61834716796875, 'referece_logps/rejected': -264.05078125, 'referece_logps/chosen': -291.7752685546875, 'logits/rejected': -0.21465754508972168, 'logits/chosen': -0.20038354396820068, 'epoch': 7.49}


 83%|████████▎ | 20107/24156 [19:07:30<19:34:47, 17.41s/it]

 83%|████████▎ | 20108/24156 [19:07:46<18:55:53, 16.84s/it]
{'loss': 0.3122, 'learning_rate': 1.8988808867191732e-06, 'rewards/chosen': -2.0075583457946777, 'rewards/rejected': -4.161262035369873, 'rewards/accuracies': 0.875, 'rewards/margins': 2.153703451156616, 'policy_logps/rejected': -407.87689208984375, 'policy_logps/chosen': -396.9646301269531, 'referece_logps/rejected': -366.2643127441406, 'referece_logps/chosen': -376.8890380859375, 'logits/rejected': -0.2350400686264038, 'logits/chosen': -0.2914559841156006, 'epoch': 7.49}


 83%|████████▎ | 20110/24156 [19:08:19<18:24:32, 16.38s/it]

 83%|████████▎ | 20111/24156 [19:08:38<19:31:20, 17.37s/it]
{'loss': 0.3217, 'learning_rate': 1.898704557243933e-06, 'rewards/chosen': -1.492992639541626, 'rewards/rejected': -5.048018932342529, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5550262928009033, 'policy_logps/rejected': -280.0250244140625, 'policy_logps/chosen': -333.29547119140625, 'referece_logps/rejected': -229.54486083984375, 'referece_logps/chosen': -318.36553955078125, 'logits/rejected': -1.1559536457061768, 'logits/chosen': -1.3562501668930054, 'epoch': 7.49}

 83%|████████▎ | 20112/24156 [19:08:59<20:34:20, 18.31s/it]

 83%|████████▎ | 20113/24156 [19:09:11<18:31:56, 16.50s/it]

 83%|████████▎ | 20114/24156 [19:09:26<17:51:31, 15.91s/it]


 83%|████████▎ | 20116/24156 [19:10:00<19:01:39, 16.96s/it]

 83%|████████▎ | 20117/24156 [19:10:19<19:30:13, 17.38s/it]
{'loss': 0.4469, 'learning_rate': 1.8983514621088088e-06, 'rewards/chosen': -1.3833236694335938, 'rewards/rejected': -2.8985471725463867, 'rewards/accuracies': 0.75, 'rewards/margins': 1.515223503112793, 'policy_logps/rejected': -277.48785400390625, 'policy_logps/chosen': -373.3792419433594, 'referece_logps/rejected': -248.5023956298828, 'referece_logps/chosen': -359.5459899902344, 'logits/rejected': -0.8039712905883789, 'logits/chosen': -0.9309796094894409, 'epoch': 7.5}


 83%|████████▎ | 20119/24156 [19:10:49<17:58:03, 16.02s/it]
{'loss': 0.4953, 'learning_rate': 1.8982336345215702e-06, 'rewards/chosen': -1.6657218933105469, 'rewards/rejected': -2.4865100383758545, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8207882642745972, 'policy_logps/rejected': -358.6632080078125, 'policy_logps/chosen': -274.24395751953125, 'referece_logps/rejected': -333.7981262207031, 'referece_logps/chosen': -257.58673095703125, 'logits/rejected': -0.6970126032829285, 'logits/chosen': -0.7336037755012512, 'epoch': 7.5}

 83%|████████▎ | 20120/24156 [19:11:03<17:24:06, 15.52s/it]

 83%|████████▎ | 20121/24156 [19:11:22<18:26:10, 16.45s/it]


 83%|████████▎ | 20123/24156 [19:12:03<20:56:31, 18.69s/it]

 83%|████████▎ | 20124/24156 [19:12:21<20:40:59, 18.47s/it]

 83%|████████▎ | 20125/24156 [19:12:41<21:11:56, 18.93s/it]

 83%|████████▎ | 20126/24156 [19:13:00<21:20:35, 19.07s/it]

 83%|████████▎ | 20127/24156 [19:13:21<21:52:52, 19.55s/it]
[2024-04-06 10:18:18,339] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20128/24156 [19:13:40<21:42:07, 19.40s/it]
{'loss': 0.2412, 'learning_rate': 1.8977026111962883e-06, 'rewards/chosen': -1.1762938499450684, 'rewards/rejected': -4.244180202484131, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0678861141204834, 'policy_logps/rejected': -435.0383605957031, 'policy_logps/chosen': -348.841552734375, 'referece_logps/rejected': -392.5965881347656, 'referece_logps/chosen': -337.0786437988281, 'logits/rejected': -0.614379346370697, 'logits/chosen': -0.4990665316581726, 'epoch': 7.5}


 83%|████████▎ | 20130/24156 [19:14:17<21:15:58, 19.02s/it]

 83%|████████▎ | 20131/24156 [19:14:34<20:41:43, 18.51s/it]

 83%|████████▎ | 20132/24156 [19:14:54<21:18:14, 19.06s/it]
{'loss': 0.2909, 'learning_rate': 1.8974661811946703e-06, 'rewards/chosen': -2.75156569480896, 'rewards/rejected': -5.0605692863464355, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3090038299560547, 'policy_logps/rejected': -279.44903564453125, 'policy_logps/chosen': -248.57911682128906, 'referece_logps/rejected': -228.84335327148438, 'referece_logps/chosen': -221.06344604492188, 'logits/rejected': -1.1052695512771606, 'logits/chosen': -1.1885662078857422, 'epoch': 7.5}

 83%|████████▎ | 20133/24156 [19:15:05<18:29:51, 16.55s/it]

 83%|████████▎ | 20134/24156 [19:15:22<18:33:45, 16.61s/it]

 83%|████████▎ | 20135/24156 [19:15:42<19:37:44, 17.57s/it]


 83%|████████▎ | 20137/24156 [19:16:09<16:59:03, 15.21s/it]

 83%|████████▎ | 20138/24156 [19:16:21<15:57:23, 14.30s/it]

 83%|████████▎ | 20139/24156 [19:16:33<15:14:43, 13.66s/it]
{'loss': 0.4894, 'learning_rate': 1.8970518075821897e-06, 'rewards/chosen': -1.6509073972702026, 'rewards/rejected': -2.9089195728302, 'rewards/accuracies': 0.75, 'rewards/margins': 1.258012294769287, 'policy_logps/rejected': -342.6992492675781, 'policy_logps/chosen': -391.7488708496094, 'referece_logps/rejected': -313.61004638671875, 'referece_logps/chosen': -375.23980712890625, 'logits/rejected': -0.24999916553497314, 'logits/chosen': -0.1547977179288864, 'epoch': 7.5}

 83%|████████▎ | 20140/24156 [19:16:44<14:14:57, 12.77s/it]


 83%|████████▎ | 20142/24156 [19:17:13<14:54:40, 13.37s/it]

 83%|████████▎ | 20143/24156 [19:17:26<14:59:35, 13.45s/it]

 83%|████████▎ | 20144/24156 [19:17:49<18:02:22, 16.19s/it]

 83%|████████▎ | 20145/24156 [19:18:09<19:19:09, 17.34s/it]

 83%|████████▎ | 20146/24156 [19:18:23<18:06:03, 16.25s/it]

 83%|████████▎ | 20147/24156 [19:18:38<17:51:10, 16.03s/it]

 83%|████████▎ | 20148/24156 [19:18:55<17:59:40, 16.16s/it]
{'loss': 0.2975, 'learning_rate': 1.8965178804642717e-06, 'rewards/chosen': -2.6449999809265137, 'rewards/rejected': -4.890385627746582, 'rewards/accuracies': 1.0, 'rewards/margins': 2.24538516998291, 'policy_logps/rejected': -640.340576171875, 'policy_logps/chosen': -427.7806091308594, 'referece_logps/rejected': -591.4366455078125, 'referece_logps/chosen': -401.3305969238281, 'logits/rejected': -0.8560702204704285, 'logits/chosen': -0.6661771535873413, 'epoch': 7.51}

 83%|████████▎ | 20149/24156 [19:19:13<18:50:36, 16.93s/it]

 83%|████████▎ | 20150/24156 [19:19:29<18:31:30, 16.65s/it]
[2024-04-06 10:24:51,262] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20151/24156 [19:19:54<21:05:05, 18.95s/it]
[2024-04-06 10:25:13,165] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20152/24156 [19:20:16<22:03:51, 19.84s/it]

 83%|████████▎ | 20153/24156 [19:20:28<19:34:56, 17.61s/it]

 83%|████████▎ | 20154/24156 [19:20:46<19:32:42, 17.58s/it]


 83%|████████▎ | 20156/24156 [19:21:07<15:42:06, 14.13s/it]

 83%|████████▎ | 20157/24156 [19:21:21<15:36:35, 14.05s/it]

 83%|████████▎ | 20158/24156 [19:21:43<18:20:32, 16.52s/it]

 83%|████████▎ | 20159/24156 [19:21:59<17:53:46, 16.12s/it]
[2024-04-06 10:26:55,991] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20160/24156 [19:22:19<19:19:41, 17.41s/it]

 83%|████████▎ | 20161/24156 [19:22:33<18:16:29, 16.47s/it]

 83%|████████▎ | 20162/24156 [19:22:55<19:58:46, 18.01s/it]

 83%|████████▎ | 20163/24156 [19:23:15<20:40:00, 18.63s/it]

 83%|████████▎ | 20164/24156 [19:23:35<21:03:56, 19.00s/it]

 83%|████████▎ | 20165/24156 [19:23:55<21:25:47, 19.33s/it]

 83%|████████▎ | 20166/24156 [19:24:13<20:57:44, 18.91s/it]
[2024-04-06 10:29:10,275] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4011, 'learning_rate': 1.8954461106417876e-06, 'rewards/chosen': -2.6267242431640625, 'rewards/rejected': -4.057744026184082, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4310195446014404, 'policy_logps/rejected': -481.01373291015625, 'policy_logps/chosen': -455.2056884765625, 'referece_logps/rejected': -440.436279296875, 'referece_logps/chosen': -428.9384460449219, 'logits/rejected': 0.1928224414587021, 'logits/chosen': 0.31633055210113525, 'epoch': 7.51}


 83%|████████▎ | 20168/24156 [19:24:43<19:02:27, 17.19s/it]

 83%|████████▎ | 20169/24156 [19:24:56<17:24:45, 15.72s/it]
{'loss': 0.379, 'learning_rate': 1.8952669750990332e-06, 'rewards/chosen': -2.1232833862304688, 'rewards/rejected': -3.366767406463623, 'rewards/accuracies': 0.75, 'rewards/margins': 1.243484377861023, 'policy_logps/rejected': -384.7586364746094, 'policy_logps/chosen': -392.7485656738281, 'referece_logps/rejected': -351.0909729003906, 'referece_logps/chosen': -371.51568603515625, 'logits/rejected': 0.05667078495025635, 'logits/chosen': 0.05211600661277771, 'epoch': 7.51}
[2024-04-06 10:30:16,142] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 20170/24156 [19:25:19<19:52:00, 17.94s/it]

 84%|████████▎ | 20171/24156 [19:25:32<18:22:24, 16.60s/it]

 84%|████████▎ | 20172/24156 [19:25:48<18:14:52, 16.49s/it]


 84%|████████▎ | 20174/24156 [19:26:19<17:48:46, 16.10s/it]
{'loss': 0.3338, 'learning_rate': 1.8949680939904973e-06, 'rewards/chosen': -1.471288800239563, 'rewards/rejected': -4.981185436248779, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5098965167999268, 'policy_logps/rejected': -296.62420654296875, 'policy_logps/chosen': -448.5376892089844, 'referece_logps/rejected': -246.81236267089844, 'referece_logps/chosen': -433.8248291015625, 'logits/rejected': -0.6166413426399231, 'logits/chosen': -0.5029484033584595, 'epoch': 7.52}

 84%|████████▎ | 20175/24156 [19:26:32<16:47:03, 15.18s/it]

 84%|████████▎ | 20176/24156 [19:26:49<17:06:58, 15.48s/it]


 84%|████████▎ | 20178/24156 [19:27:29<19:53:44, 18.01s/it]
[2024-04-06 10:32:26,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3485, 'learning_rate': 1.8947286994974939e-06, 'rewards/chosen': -2.231952667236328, 'rewards/rejected': -3.36276912689209, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1308165788650513, 'policy_logps/rejected': -489.05596923828125, 'policy_logps/chosen': -439.2607727050781, 'referece_logps/rejected': -455.4283142089844, 'referece_logps/chosen': -416.9412841796875, 'logits/rejected': -0.03541748225688934, 'logits/chosen': 0.06397949904203415, 'epoch': 7.52}


 84%|████████▎ | 20180/24156 [19:27:59<17:40:59, 16.01s/it]

 84%|████████▎ | 20181/24156 [19:28:14<17:18:19, 15.67s/it]
[2024-04-06 10:33:11,256] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 20182/24156 [19:28:31<17:52:43, 16.20s/it]
{'loss': 0.3374, 'learning_rate': 1.8944890476513705e-06, 'rewards/chosen': -2.010855197906494, 'rewards/rejected': -3.337468385696411, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3266128301620483, 'policy_logps/rejected': -358.8656005859375, 'policy_logps/chosen': -542.3760986328125, 'referece_logps/rejected': -325.49090576171875, 'referece_logps/chosen': -522.2675170898438, 'logits/rejected': -0.8495588302612305, 'logits/chosen': -1.024627685546875, 'epoch': 7.52}

 84%|████████▎ | 20183/24156 [19:28:50<18:45:56, 17.00s/it]


 84%|████████▎ | 20185/24156 [19:29:19<17:25:26, 15.80s/it]
{'loss': 0.2634, 'learning_rate': 1.894309139920257e-06, 'rewards/chosen': -2.1266589164733887, 'rewards/rejected': -4.848421573638916, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7217631340026855, 'policy_logps/rejected': -386.9632873535156, 'policy_logps/chosen': -368.6475830078125, 'referece_logps/rejected': -338.47906494140625, 'referece_logps/chosen': -347.3809509277344, 'logits/rejected': 0.06293457746505737, 'logits/chosen': 0.04945005476474762, 'epoch': 7.52}

 84%|████████▎ | 20186/24156 [19:29:31<15:58:59, 14.49s/it]


 84%|████████▎ | 20188/24156 [19:30:12<19:32:56, 17.74s/it]

 84%|████████▎ | 20189/24156 [19:30:27<18:44:35, 17.01s/it]

 84%|████████▎ | 20190/24156 [19:30:41<17:43:20, 16.09s/it]

 84%|████████▎ | 20191/24156 [19:30:58<17:48:20, 16.17s/it]

 84%|████████▎ | 20192/24156 [19:31:10<16:35:30, 15.07s/it]
{'loss': 0.2854, 'learning_rate': 1.893888792568709e-06, 'rewards/chosen': -2.2117550373077393, 'rewards/rejected': -5.081125736236572, 'rewards/accuracies': 1.0, 'rewards/margins': 2.869370698928833, 'policy_logps/rejected': -427.2348937988281, 'policy_logps/chosen': -397.045654296875, 'referece_logps/rejected': -376.42364501953125, 'referece_logps/chosen': -374.9281311035156, 'logits/rejected': -0.8719986081123352, 'logits/chosen': -0.8046352863311768, 'epoch': 7.52}

 84%|████████▎ | 20193/24156 [19:31:27<17:11:17, 15.61s/it]

 84%|████████▎ | 20194/24156 [19:31:46<18:29:35, 16.80s/it]


 84%|████████▎ | 20196/24156 [19:32:19<17:49:09, 16.20s/it]

 84%|████████▎ | 20197/24156 [19:32:31<16:26:06, 14.94s/it]

 84%|████████▎ | 20198/24156 [19:32:52<18:14:27, 16.59s/it]

 84%|████████▎ | 20199/24156 [19:33:08<18:06:58, 16.48s/it]
{'loss': 0.2812, 'learning_rate': 1.8934676578131222e-06, 'rewards/chosen': -2.18650221824646, 'rewards/rejected': -5.004029273986816, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8175270557403564, 'policy_logps/rejected': -501.8714294433594, 'policy_logps/chosen': -547.3968505859375, 'referece_logps/rejected': -451.8311462402344, 'referece_logps/chosen': -525.5317993164062, 'logits/rejected': -0.1606147438287735, 'logits/chosen': -0.1972147673368454, 'epoch': 7.53}


 84%|████████▎ | 20201/24156 [19:33:46<19:51:00, 18.07s/it]
{'loss': 0.3878, 'learning_rate': 1.8933471890239055e-06, 'rewards/chosen': -3.492827892303467, 'rewards/rejected': -5.501408100128174, 'rewards/accuracies': 1.0, 'rewards/margins': 2.008580207824707, 'policy_logps/rejected': -382.29541015625, 'policy_logps/chosen': -381.5822448730469, 'referece_logps/rejected': -327.2813415527344, 'referece_logps/chosen': -346.65399169921875, 'logits/rejected': 0.14468124508857727, 'logits/chosen': 0.38001370429992676, 'epoch': 7.53}

 84%|████████▎ | 20202/24156 [19:33:59<18:10:51, 16.55s/it]
[2024-04-06 10:39:18,445] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▎ | 20204/24156 [19:34:40<20:17:03, 18.48s/it]

 84%|████████▎ | 20205/24156 [19:34:58<19:51:54, 18.10s/it]
{'loss': 0.3733, 'learning_rate': 1.893106058737321e-06, 'rewards/chosen': -3.0850884914398193, 'rewards/rejected': -4.862768650054932, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7776799201965332, 'policy_logps/rejected': -394.76446533203125, 'policy_logps/chosen': -423.67767333984375, 'referece_logps/rejected': -346.1368103027344, 'referece_logps/chosen': -392.8268127441406, 'logits/rejected': -1.441495656967163, 'logits/chosen': -1.4075452089309692, 'epoch': 7.53}

 84%|████████▎ | 20206/24156 [19:35:11<18:14:46, 16.63s/it]


 84%|████████▎ | 20208/24156 [19:35:38<16:22:08, 14.93s/it]
{'loss': 0.4632, 'learning_rate': 1.8929250424368902e-06, 'rewards/chosen': -1.6167107820510864, 'rewards/rejected': -3.1318864822387695, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5151759386062622, 'policy_logps/rejected': -618.1444091796875, 'policy_logps/chosen': -393.7474670410156, 'referece_logps/rejected': -586.8256225585938, 'referece_logps/chosen': -377.5803527832031, 'logits/rejected': -0.7918330430984497, 'logits/chosen': -0.6035578846931458, 'epoch': 7.53}

 84%|████████▎ | 20209/24156 [19:35:57<17:30:59, 15.98s/it]

 84%|████████▎ | 20210/24156 [19:36:18<19:10:07, 17.49s/it]

 84%|████████▎ | 20211/24156 [19:36:37<19:45:18, 18.03s/it]

 84%|████████▎ | 20212/24156 [19:36:57<20:21:14, 18.58s/it]
[2024-04-06 10:42:16,470] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 20213/24156 [19:37:19<21:30:28, 19.64s/it]
[2024-04-06 10:42:34,524] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▎ | 20215/24156 [19:37:52<19:43:30, 18.02s/it]
{'loss': 0.4047, 'learning_rate': 1.8925021092947798e-06, 'rewards/chosen': -1.599298119544983, 'rewards/rejected': -2.754122734069824, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1548246145248413, 'policy_logps/rejected': -323.6589050292969, 'policy_logps/chosen': -238.34024047851562, 'referece_logps/rejected': -296.1177062988281, 'referece_logps/chosen': -222.34725952148438, 'logits/rejected': -0.7585707902908325, 'logits/chosen': -0.4645005166530609, 'epoch': 7.53}

 84%|████████▎ | 20216/24156 [19:38:06<18:11:48, 16.63s/it]

 84%|████████▎ | 20217/24156 [19:38:27<19:38:22, 17.95s/it]

 84%|████████▎ | 20218/24156 [19:38:46<19:56:43, 18.23s/it]

 84%|████████▎ | 20219/24156 [19:39:07<20:49:07, 19.04s/it]

 84%|████████▎ | 20220/24156 [19:39:23<19:54:55, 18.22s/it]
[2024-04-06 10:44:42,223] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 20221/24156 [19:39:45<21:05:41, 19.30s/it]

 84%|████████▎ | 20222/24156 [19:40:03<20:52:08, 19.10s/it]
[2024-04-06 10:45:22,443] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 20223/24156 [19:40:25<21:40:54, 19.85s/it]
[2024-04-06 10:45:37,491] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 20224/24156 [19:40:40<20:06:14, 18.41s/it]

 84%|████████▎ | 20225/24156 [19:40:51<17:48:19, 16.31s/it]

 84%|████████▎ | 20226/24156 [19:41:07<17:37:55, 16.15s/it]


 84%|████████▎ | 20228/24156 [19:41:36<16:35:04, 15.20s/it]

 84%|████████▎ | 20229/24156 [19:41:54<17:26:18, 15.99s/it]
{'loss': 0.2837, 'learning_rate': 1.8916538848361713e-06, 'rewards/chosen': -2.2523820400238037, 'rewards/rejected': -3.3854150772094727, 'rewards/accuracies': 0.75, 'rewards/margins': 1.133033275604248, 'policy_logps/rejected': -481.3973693847656, 'policy_logps/chosen': -363.60186767578125, 'referece_logps/rejected': -447.543212890625, 'referece_logps/chosen': -341.07806396484375, 'logits/rejected': 0.48560479283332825, 'logits/chosen': 0.49289631843566895, 'epoch': 7.54}

 84%|████████▎ | 20230/24156 [19:42:11<17:41:24, 16.22s/it]

 84%|████████▍ | 20231/24156 [19:42:22<16:02:08, 14.71s/it]
[2024-04-06 10:47:41,177] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20232/24156 [19:42:44<18:13:53, 16.73s/it]
[2024-04-06 10:47:59,253] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20233/24156 [19:43:02<18:40:05, 17.13s/it]

 84%|████████▍ | 20234/24156 [19:43:14<16:58:07, 15.58s/it]


 84%|████████▍ | 20236/24156 [19:43:49<17:57:43, 16.50s/it]
{'loss': 0.3224, 'learning_rate': 1.8912285942668524e-06, 'rewards/chosen': -2.042764902114868, 'rewards/rejected': -3.118602752685547, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0758377313613892, 'policy_logps/rejected': -396.693359375, 'policy_logps/chosen': -327.679443359375, 'referece_logps/rejected': -365.50732421875, 'referece_logps/chosen': -307.2518310546875, 'logits/rejected': 0.21303299069404602, 'logits/chosen': 0.1499512493610382, 'epoch': 7.54}

 84%|████████▍ | 20237/24156 [19:44:05<17:54:59, 16.46s/it]
[2024-04-06 10:49:20,978] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20238/24156 [19:44:23<18:33:37, 17.05s/it]

 84%|████████▍ | 20239/24156 [19:44:46<20:21:07, 18.70s/it]


 84%|████████▍ | 20241/24156 [19:45:19<18:38:25, 17.14s/it]
{'loss': 0.4057, 'learning_rate': 1.8909243346174654e-06, 'rewards/chosen': -2.964691400527954, 'rewards/rejected': -6.0995259284973145, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1348345279693604, 'policy_logps/rejected': -438.2637634277344, 'policy_logps/chosen': -530.4522094726562, 'referece_logps/rejected': -377.2685241699219, 'referece_logps/chosen': -500.8052673339844, 'logits/rejected': -0.353892058134079, 'logits/chosen': -0.45561715960502625, 'epoch': 7.54}

 84%|████████▍ | 20242/24156 [19:45:40<20:02:32, 18.43s/it]

 84%|████████▍ | 20243/24156 [19:45:57<19:42:14, 18.13s/it]

 84%|████████▍ | 20244/24156 [19:46:14<19:10:34, 17.65s/it]

 84%|████████▍ | 20245/24156 [19:46:30<18:49:11, 17.32s/it]

 84%|████████▍ | 20246/24156 [19:46:49<19:19:19, 17.79s/it]

 84%|████████▍ | 20247/24156 [19:47:12<20:44:10, 19.10s/it]

 84%|████████▍ | 20248/24156 [19:47:25<18:58:44, 17.48s/it]

 84%|████████▍ | 20249/24156 [19:47:44<19:26:17, 17.91s/it]


 84%|████████▍ | 20251/24156 [19:48:21<19:50:14, 18.29s/it]
{'loss': 0.3968, 'learning_rate': 1.89031461424222e-06, 'rewards/chosen': -1.6650406122207642, 'rewards/rejected': -2.958829164505005, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2937886714935303, 'policy_logps/rejected': -443.6156005859375, 'policy_logps/chosen': -461.344970703125, 'referece_logps/rejected': -414.02728271484375, 'referece_logps/chosen': -444.694580078125, 'logits/rejected': 0.8360072374343872, 'logits/chosen': 0.6928013563156128, 'epoch': 7.55}

 84%|████████▍ | 20252/24156 [19:48:32<17:34:25, 16.21s/it]

 84%|████████▍ | 20253/24156 [19:48:51<18:30:01, 17.06s/it]

 84%|████████▍ | 20254/24156 [19:49:06<17:34:21, 16.21s/it]

 84%|████████▍ | 20255/24156 [19:49:16<15:46:34, 14.56s/it]

 84%|████████▍ | 20256/24156 [19:49:28<14:39:34, 13.53s/it]

 84%|████████▍ | 20257/24156 [19:49:38<13:43:48, 12.68s/it]

 84%|████████▍ | 20258/24156 [19:49:58<16:11:17, 14.95s/it]

 84%|████████▍ | 20259/24156 [19:50:16<16:52:15, 15.59s/it]

 84%|████████▍ | 20260/24156 [19:50:26<15:14:32, 14.08s/it]

 84%|████████▍ | 20261/24156 [19:50:38<14:27:03, 13.36s/it]


 84%|████████▍ | 20263/24156 [19:50:59<13:00:48, 12.03s/it]
{'loss': 0.3269, 'learning_rate': 1.8895808372000977e-06, 'rewards/chosen': -1.6249408721923828, 'rewards/rejected': -2.9462924003601074, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3213516473770142, 'policy_logps/rejected': -405.5892333984375, 'policy_logps/chosen': -484.2288818359375, 'referece_logps/rejected': -376.1263122558594, 'referece_logps/chosen': -467.9795227050781, 'logits/rejected': -0.3700503706932068, 'logits/chosen': -0.40984928607940674, 'epoch': 7.55}

 84%|████████▍ | 20264/24156 [19:51:12<13:11:16, 12.20s/it]

 84%|████████▍ | 20265/24156 [19:51:23<12:42:56, 11.76s/it]


 84%|████████▍ | 20267/24156 [19:51:47<12:57:42, 12.00s/it]
{'loss': 0.3911, 'learning_rate': 1.8893357330139923e-06, 'rewards/chosen': -1.5113475322723389, 'rewards/rejected': -4.4440813064575195, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9327335357666016, 'policy_logps/rejected': -489.7377014160156, 'policy_logps/chosen': -502.2320861816406, 'referece_logps/rejected': -445.2968444824219, 'referece_logps/chosen': -487.1186218261719, 'logits/rejected': -0.09942089021205902, 'logits/chosen': 0.03114398941397667, 'epoch': 7.55}
[2024-04-06 10:57:06,147] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20268/24156 [19:52:09<16:04:22, 14.88s/it]
[2024-04-06 10:57:19,199] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20269/24156 [19:52:22<15:28:33, 14.33s/it]

 84%|████████▍ | 20270/24156 [19:52:34<14:48:05, 13.71s/it]


 84%|████████▍ | 20272/24156 [19:53:05<16:14:20, 15.05s/it]
{'loss': 0.3671, 'learning_rate': 1.889028993068169e-06, 'rewards/chosen': -2.933812141418457, 'rewards/rejected': -4.194320201873779, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2605079412460327, 'policy_logps/rejected': -395.165283203125, 'policy_logps/chosen': -373.3804016113281, 'referece_logps/rejected': -353.22210693359375, 'referece_logps/chosen': -344.0422668457031, 'logits/rejected': -0.2566652297973633, 'logits/chosen': -0.12832000851631165, 'epoch': 7.55}


 84%|████████▍ | 20274/24156 [19:53:37<16:30:04, 15.30s/it]
{'loss': 0.3005, 'learning_rate': 1.8889061852073722e-06, 'rewards/chosen': -2.5759546756744385, 'rewards/rejected': -3.975193738937378, 'rewards/accuracies': 0.5, 'rewards/margins': 1.399239182472229, 'policy_logps/rejected': -474.9631652832031, 'policy_logps/chosen': -319.0188903808594, 'referece_logps/rejected': -435.21124267578125, 'referece_logps/chosen': -293.25933837890625, 'logits/rejected': -1.2366278171539307, 'logits/chosen': -0.9431938529014587, 'epoch': 7.55}

 84%|████████▍ | 20275/24156 [19:53:57<17:51:53, 16.57s/it]


 84%|████████▍ | 20277/24156 [19:54:23<16:15:13, 15.08s/it]
{'loss': 0.4813, 'learning_rate': 1.8887218535696972e-06, 'rewards/chosen': -1.8857861757278442, 'rewards/rejected': -3.936389207839966, 'rewards/accuracies': 0.875, 'rewards/margins': 2.050603151321411, 'policy_logps/rejected': -297.19085693359375, 'policy_logps/chosen': -376.9615173339844, 'referece_logps/rejected': -257.8269348144531, 'referece_logps/chosen': -358.1036682128906, 'logits/rejected': -0.18300114572048187, 'logits/chosen': -0.41972243785858154, 'epoch': 7.55}

 84%|████████▍ | 20278/24156 [19:54:43<17:33:18, 16.30s/it]

 84%|████████▍ | 20279/24156 [19:55:01<18:06:54, 16.82s/it]

 84%|████████▍ | 20280/24156 [19:55:12<16:24:19, 15.24s/it]

 84%|████████▍ | 20281/24156 [19:55:30<17:15:16, 16.03s/it]

 84%|████████▍ | 20282/24156 [19:55:43<16:09:57, 15.02s/it]

 84%|████████▍ | 20283/24156 [19:55:57<15:46:45, 14.67s/it]
[2024-04-06 11:01:14,502] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20284/24156 [19:56:17<17:39:14, 16.41s/it]

 84%|████████▍ | 20285/24156 [19:56:32<17:18:11, 16.09s/it]


 84%|████████▍ | 20287/24156 [19:56:56<14:57:01, 13.91s/it]
{'loss': 0.3761, 'learning_rate': 1.888106376467132e-06, 'rewards/chosen': -1.7567030191421509, 'rewards/rejected': -4.592538833618164, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8358356952667236, 'policy_logps/rejected': -410.705322265625, 'policy_logps/chosen': -345.3827819824219, 'referece_logps/rejected': -364.7799377441406, 'referece_logps/chosen': -327.81573486328125, 'logits/rejected': -1.0164486169815063, 'logits/chosen': -0.9049296379089355, 'epoch': 7.56}


 84%|████████▍ | 20289/24156 [19:57:34<17:45:21, 16.53s/it]
{'loss': 0.2907, 'learning_rate': 1.887983089424943e-06, 'rewards/chosen': -2.724569320678711, 'rewards/rejected': -4.776743412017822, 'rewards/accuracies': 0.875, 'rewards/margins': 2.052173614501953, 'policy_logps/rejected': -478.67724609375, 'policy_logps/chosen': -369.2518005371094, 'referece_logps/rejected': -430.9098205566406, 'referece_logps/chosen': -342.006103515625, 'logits/rejected': -1.27837336063385, 'logits/chosen': -1.1754416227340698, 'epoch': 7.56}


 84%|████████▍ | 20291/24156 [19:58:08<17:53:38, 16.67s/it]
{'loss': 0.3167, 'learning_rate': 1.8878597385295377e-06, 'rewards/chosen': -2.105607509613037, 'rewards/rejected': -3.484253406524658, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3786460161209106, 'policy_logps/rejected': -267.6279296875, 'policy_logps/chosen': -508.2567138671875, 'referece_logps/rejected': -232.78538513183594, 'referece_logps/chosen': -487.20062255859375, 'logits/rejected': -0.4079052209854126, 'logits/chosen': -0.22739467024803162, 'epoch': 7.56}

 84%|████████▍ | 20292/24156 [19:58:28<18:58:56, 17.69s/it]

 84%|████████▍ | 20293/24156 [19:58:45<18:48:27, 17.53s/it]
[2024-04-06 11:04:04,551] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20294/24156 [19:59:07<20:16:19, 18.90s/it]

 84%|████████▍ | 20295/24156 [19:59:26<20:17:00, 18.91s/it]


 84%|████████▍ | 20297/24156 [19:59:59<19:29:54, 18.19s/it]

 84%|████████▍ | 20298/24156 [20:00:12<17:30:47, 16.34s/it]
[2024-04-06 11:05:09,000] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20299/24156 [20:00:26<16:45:05, 15.64s/it]
{'loss': 0.3, 'learning_rate': 1.887365696593214e-06, 'rewards/chosen': -1.31023108959198, 'rewards/rejected': -3.2000153064727783, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8897839784622192, 'policy_logps/rejected': -410.74072265625, 'policy_logps/chosen': -355.96600341796875, 'referece_logps/rejected': -378.7406005859375, 'referece_logps/chosen': -342.8637390136719, 'logits/rejected': -0.42881208658218384, 'logits/chosen': -0.43966585397720337, 'epoch': 7.56}

 84%|████████▍ | 20300/24156 [20:00:39<16:03:26, 14.99s/it]

 84%|████████▍ | 20301/24156 [20:00:59<17:42:02, 16.53s/it]


 84%|████████▍ | 20303/24156 [20:01:22<14:54:48, 13.93s/it]
{'loss': 0.3932, 'learning_rate': 1.8871182927365879e-06, 'rewards/chosen': -2.040449857711792, 'rewards/rejected': -3.3000049591064453, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2595549821853638, 'policy_logps/rejected': -329.7613525390625, 'policy_logps/chosen': -461.298095703125, 'referece_logps/rejected': -296.7613220214844, 'referece_logps/chosen': -440.8935852050781, 'logits/rejected': -0.6022824645042419, 'logits/chosen': -0.4104457497596741, 'epoch': 7.56}

 84%|████████▍ | 20304/24156 [20:01:37<15:15:12, 14.26s/it]

 84%|████████▍ | 20305/24156 [20:01:57<17:00:55, 15.91s/it]

 84%|████████▍ | 20306/24156 [20:02:11<16:27:45, 15.39s/it]

 84%|████████▍ | 20307/24156 [20:02:28<16:54:40, 15.82s/it]


 84%|████████▍ | 20309/24156 [20:03:02<17:33:45, 16.44s/it]

 84%|████████▍ | 20310/24156 [20:03:13<15:48:35, 14.80s/it]

 84%|████████▍ | 20311/24156 [20:03:32<17:19:09, 16.22s/it]

 84%|████████▍ | 20312/24156 [20:03:45<16:10:53, 15.15s/it]

 84%|████████▍ | 20313/24156 [20:03:59<15:45:47, 14.77s/it]

 84%|████████▍ | 20314/24156 [20:04:12<15:04:15, 14.12s/it]

 84%|████████▍ | 20315/24156 [20:04:32<17:12:04, 16.12s/it]

 84%|████████▍ | 20316/24156 [20:04:46<16:23:49, 15.37s/it]

 84%|████████▍ | 20317/24156 [20:04:58<15:25:44, 14.47s/it]

 84%|████████▍ | 20318/24156 [20:05:15<16:05:31, 15.09s/it]

 84%|████████▍ | 20319/24156 [20:05:35<17:44:05, 16.64s/it]

 84%|████████▍ | 20320/24156 [20:05:52<17:54:46, 16.81s/it]

 84%|████████▍ | 20321/24156 [20:06:15<19:51:54, 18.65s/it]

 84%|████████▍ | 20322/24156 [20:06:34<19:53:25, 18.68s/it]

 84%|████████▍ | 20323/24156 [20:06:53<20:06:15, 18.88s/it]
[2024-04-06 11:11:50,912] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20324/24156 [20:07:10<19:13:54, 18.07s/it]

 84%|████████▍ | 20325/24156 [20:07:26<18:43:40, 17.60s/it]

 84%|████████▍ | 20326/24156 [20:07:43<18:32:18, 17.43s/it]

 84%|████████▍ | 20327/24156 [20:08:05<19:48:47, 18.63s/it]

 84%|████████▍ | 20328/24156 [20:08:17<17:52:48, 16.82s/it]

 84%|████████▍ | 20329/24156 [20:08:39<19:33:26, 18.40s/it]

 84%|████████▍ | 20330/24156 [20:08:56<19:01:45, 17.91s/it]

 84%|████████▍ | 20331/24156 [20:09:16<19:33:44, 18.41s/it]

 84%|████████▍ | 20332/24156 [20:09:37<20:25:05, 19.22s/it]

 84%|████████▍ | 20333/24156 [20:09:49<18:10:00, 17.11s/it]

 84%|████████▍ | 20334/24156 [20:10:01<16:28:20, 15.52s/it]

 84%|████████▍ | 20335/24156 [20:10:19<17:31:05, 16.50s/it]

 84%|████████▍ | 20336/24156 [20:10:40<18:54:16, 17.82s/it]
[2024-04-06 11:15:37,838] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20337/24156 [20:10:54<17:42:14, 16.69s/it]

 84%|████████▍ | 20338/24156 [20:11:14<18:34:57, 17.52s/it]

 84%|████████▍ | 20339/24156 [20:11:30<18:07:18, 17.09s/it]

 84%|████████▍ | 20340/24156 [20:11:43<16:46:53, 15.83s/it]

 84%|████████▍ | 20341/24156 [20:11:56<15:56:26, 15.04s/it]

 84%|████████▍ | 20342/24156 [20:12:07<14:34:30, 13.76s/it]

 84%|████████▍ | 20343/24156 [20:12:19<13:57:56, 13.19s/it]

 84%|████████▍ | 20344/24156 [20:12:30<13:29:17, 12.74s/it]

 84%|████████▍ | 20345/24156 [20:12:44<13:50:36, 13.08s/it]

 84%|████████▍ | 20346/24156 [20:13:04<16:03:52, 15.18s/it]

 84%|████████▍ | 20347/24156 [20:13:18<15:40:09, 14.81s/it]

 84%|████████▍ | 20348/24156 [20:13:31<14:57:49, 14.15s/it]

 84%|████████▍ | 20349/24156 [20:13:48<15:46:55, 14.92s/it]

 84%|████████▍ | 20350/24156 [20:14:09<17:58:52, 17.01s/it]
[2024-04-06 11:19:06,955] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20351/24156 [20:14:27<18:02:10, 17.06s/it]
[2024-04-06 11:19:24,152] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20352/24156 [20:14:38<16:19:59, 15.46s/it]

 84%|████████▍ | 20353/24156 [20:14:50<14:58:24, 14.17s/it]

 84%|████████▍ | 20354/24156 [20:15:09<16:39:06, 15.77s/it]

 84%|████████▍ | 20355/24156 [20:15:23<16:00:19, 15.16s/it]

 84%|████████▍ | 20356/24156 [20:15:33<14:32:02, 13.77s/it]

 84%|████████▍ | 20357/24156 [20:15:47<14:32:25, 13.78s/it]

 84%|████████▍ | 20358/24156 [20:16:04<15:37:15, 14.81s/it]

 84%|████████▍ | 20359/24156 [20:16:24<17:02:32, 16.16s/it]

 84%|████████▍ | 20360/24156 [20:16:37<16:11:45, 15.36s/it]

 84%|████████▍ | 20361/24156 [20:16:49<15:01:10, 14.25s/it]

 84%|████████▍ | 20362/24156 [20:17:03<15:00:12, 14.24s/it]

 84%|████████▍ | 20363/24156 [20:17:14<13:52:47, 13.17s/it]

 84%|████████▍ | 20364/24156 [20:17:28<14:11:05, 13.47s/it]

 84%|████████▍ | 20365/24156 [20:17:44<15:01:07, 14.26s/it]

 84%|████████▍ | 20366/24156 [20:18:03<16:31:22, 15.69s/it]
[2024-04-06 11:23:00,466] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 20367/24156 [20:18:21<17:19:05, 16.45s/it]

 84%|████████▍ | 20368/24156 [20:18:39<17:44:20, 16.86s/it]

 84%|████████▍ | 20369/24156 [20:18:52<16:31:38, 15.71s/it]

 84%|████████▍ | 20370/24156 [20:19:09<17:04:24, 16.23s/it]

 84%|████████▍ | 20371/24156 [20:19:24<16:30:51, 15.71s/it]

 84%|████████▍ | 20372/24156 [20:19:44<17:45:06, 16.89s/it]

 84%|████████▍ | 20373/24156 [20:20:01<17:55:29, 17.06s/it]

 84%|████████▍ | 20374/24156 [20:20:17<17:25:04, 16.58s/it]

 84%|████████▍ | 20375/24156 [20:20:31<16:48:10, 16.00s/it]

 84%|████████▍ | 20376/24156 [20:20:47<16:52:08, 16.07s/it]

 84%|████████▍ | 20377/24156 [20:21:03<16:42:52, 15.92s/it]

 84%|████████▍ | 20378/24156 [20:21:21<17:30:34, 16.68s/it]

 84%|████████▍ | 20379/24156 [20:21:36<16:42:39, 15.93s/it]

 84%|████████▍ | 20380/24156 [20:21:54<17:27:02, 16.64s/it]

 84%|████████▍ | 20381/24156 [20:22:06<15:54:59, 15.18s/it]

 84%|████████▍ | 20382/24156 [20:22:22<16:10:50, 15.43s/it]

 84%|████████▍ | 20383/24156 [20:22:36<15:43:17, 15.00s/it]

 84%|████████▍ | 20384/24156 [20:22:57<17:42:21, 16.90s/it]

 84%|████████▍ | 20385/24156 [20:23:13<17:20:37, 16.56s/it]

 84%|████████▍ | 20386/24156 [20:23:30<17:41:05, 16.89s/it]

 84%|████████▍ | 20387/24156 [20:23:44<16:37:59, 15.89s/it]

 84%|████████▍ | 20388/24156 [20:23:58<16:07:38, 15.41s/it]

 84%|████████▍ | 20389/24156 [20:24:15<16:27:06, 15.72s/it]

 84%|████████▍ | 20390/24156 [20:24:36<18:04:33, 17.28s/it]

 84%|████████▍ | 20391/24156 [20:24:52<17:39:08, 16.88s/it]

 84%|████████▍ | 20392/24156 [20:25:12<18:46:44, 17.96s/it]

 84%|████████▍ | 20393/24156 [20:25:25<17:06:45, 16.37s/it]

 84%|████████▍ | 20394/24156 [20:25:41<17:07:17, 16.38s/it]

 84%|████████▍ | 20395/24156 [20:25:55<16:22:05, 15.67s/it]

 84%|████████▍ | 20396/24156 [20:26:15<17:36:35, 16.86s/it]

 84%|████████▍ | 20397/24156 [20:26:28<16:30:53, 15.82s/it]

 84%|████████▍ | 20398/24156 [20:26:49<18:13:15, 17.45s/it]

 84%|████████▍ | 20399/24156 [20:27:03<17:01:54, 16.32s/it]

 84%|████████▍ | 20400/24156 [20:27:16<15:59:04, 15.32s/it]

 84%|████████▍ | 20401/24156 [20:27:36<17:17:35, 16.58s/it]

 84%|████████▍ | 20402/24156 [20:27:53<17:37:22, 16.90s/it]

 84%|████████▍ | 20403/24156 [20:28:06<16:23:53, 15.73s/it]

 84%|████████▍ | 20404/24156 [20:28:23<16:36:21, 15.93s/it]

 84%|████████▍ | 20405/24156 [20:28:40<17:04:01, 16.38s/it]

 84%|████████▍ | 20406/24156 [20:29:01<18:26:40, 17.71s/it]

 84%|████████▍ | 20407/24156 [20:29:17<17:55:49, 17.22s/it]

 84%|████████▍ | 20408/24156 [20:29:37<18:38:52, 17.91s/it]
{'loss': 0.3218, 'learning_rate': 1.8805328962317196e-06, 'rewards/chosen': -2.0569605827331543, 'rewards/rejected': -4.250785827636719, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1938252449035645, 'policy_logps/rejected': -495.7106018066406, 'policy_logps/chosen': -305.8963623046875, 'referece_logps/rejected': -453.2027587890625, 'referece_logps/chosen': -285.3267517089844, 'logits/rejected': -1.7345223426818848, 'logits/chosen': -1.4752720594406128, 'epoch': 7.6}


 84%|████████▍ | 20410/24156 [20:30:08<17:14:28, 16.57s/it]

 84%|████████▍ | 20411/24156 [20:30:25<17:11:12, 16.52s/it]
{'loss': 0.331, 'learning_rate': 1.8803421714361762e-06, 'rewards/chosen': -2.2886569499969482, 'rewards/rejected': -4.013241767883301, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7245845794677734, 'policy_logps/rejected': -381.3661193847656, 'policy_logps/chosen': -252.59976196289062, 'referece_logps/rejected': -341.2336730957031, 'referece_logps/chosen': -229.71319580078125, 'logits/rejected': -0.06571739912033081, 'logits/chosen': 0.029961079359054565, 'epoch': 7.6}


 85%|████████▍ | 20413/24156 [20:30:51<15:03:43, 14.49s/it]

 85%|████████▍ | 20414/24156 [20:31:04<14:41:04, 14.13s/it]

 85%|████████▍ | 20415/24156 [20:31:23<16:17:43, 15.68s/it]
{'loss': 0.4152, 'learning_rate': 1.8800876501509198e-06, 'rewards/chosen': -1.3437695503234863, 'rewards/rejected': -2.920551061630249, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5767812728881836, 'policy_logps/rejected': -400.54669189453125, 'policy_logps/chosen': -412.39617919921875, 'referece_logps/rejected': -371.3411560058594, 'referece_logps/chosen': -398.95843505859375, 'logits/rejected': 0.1909923255443573, 'logits/chosen': 0.29019054770469666, 'epoch': 7.61}


 85%|████████▍ | 20417/24156 [20:32:02<18:15:28, 17.58s/it]

 85%|████████▍ | 20418/24156 [20:32:19<18:00:38, 17.35s/it]

 85%|████████▍ | 20419/24156 [20:32:37<18:23:09, 17.71s/it]

 85%|████████▍ | 20420/24156 [20:32:55<18:21:17, 17.69s/it]

 85%|████████▍ | 20421/24156 [20:33:16<19:20:48, 18.65s/it]

 85%|████████▍ | 20422/24156 [20:33:32<18:34:40, 17.91s/it]

 85%|████████▍ | 20423/24156 [20:33:50<18:39:16, 17.99s/it]

 85%|████████▍ | 20424/24156 [20:34:02<16:46:06, 16.18s/it]

 85%|████████▍ | 20425/24156 [20:34:23<18:18:47, 17.67s/it]

 85%|████████▍ | 20426/24156 [20:34:43<18:59:30, 18.33s/it]

 85%|████████▍ | 20427/24156 [20:34:55<17:05:36, 16.50s/it]

 85%|████████▍ | 20428/24156 [20:35:08<15:48:51, 15.27s/it]

 85%|████████▍ | 20429/24156 [20:35:25<16:18:37, 15.75s/it]

 85%|████████▍ | 20430/24156 [20:35:41<16:28:17, 15.91s/it]

 85%|████████▍ | 20431/24156 [20:35:57<16:41:24, 16.13s/it]

 85%|████████▍ | 20432/24156 [20:36:18<17:57:41, 17.36s/it]

 85%|████████▍ | 20433/24156 [20:36:38<18:47:14, 18.17s/it]

 85%|████████▍ | 20434/24156 [20:36:57<19:13:38, 18.60s/it]

 85%|████████▍ | 20435/24156 [20:37:10<17:21:39, 16.80s/it]

 85%|████████▍ | 20436/24156 [20:37:31<18:40:48, 18.08s/it]
{'loss': 0.2784, 'learning_rate': 1.8787472619989946e-06, 'rewards/chosen': -2.3172988891601562, 'rewards/rejected': -3.8616042137145996, 'rewards/accuracies': 0.875, 'rewards/margins': 1.544305443763733, 'policy_logps/rejected': -295.0050964355469, 'policy_logps/chosen': -368.5721435546875, 'referece_logps/rejected': -256.3890686035156, 'referece_logps/chosen': -345.399169921875, 'logits/rejected': -0.6427384614944458, 'logits/chosen': -0.5775527358055115, 'epoch': 7.61}
[2024-04-06 11:42:52,113] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 85%|████████▍ | 20438/24156 [20:38:16<20:46:20, 20.11s/it]
[2024-04-06 11:43:13,092] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2895, 'learning_rate': 1.8786192424799223e-06, 'rewards/chosen': -2.6402010917663574, 'rewards/rejected': -5.713007926940918, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0728063583374023, 'policy_logps/rejected': -530.8253173828125, 'policy_logps/chosen': -515.1929931640625, 'referece_logps/rejected': -473.6952819824219, 'referece_logps/chosen': -488.79095458984375, 'logits/rejected': -0.19529449939727783, 'logits/chosen': -0.17348787188529968, 'epoch': 7.61}
[2024-04-06 11:43:29,884] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 85%|████████▍ | 20440/24156 [20:38:50<19:14:29, 18.64s/it]

 85%|████████▍ | 20441/24156 [20:39:12<20:10:46, 19.55s/it]

 85%|████████▍ | 20442/24156 [20:39:32<20:18:58, 19.69s/it]
{'loss': 0.3085, 'learning_rate': 1.8783630139113482e-06, 'rewards/chosen': -1.6598546504974365, 'rewards/rejected': -3.578533172607422, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9186785221099854, 'policy_logps/rejected': -338.3703918457031, 'policy_logps/chosen': -366.89532470703125, 'referece_logps/rejected': -302.5850830078125, 'referece_logps/chosen': -350.2967224121094, 'logits/rejected': -0.6531416773796082, 'logits/chosen': -0.7373979091644287, 'epoch': 7.62}


 85%|████████▍ | 20444/24156 [20:40:12<20:47:05, 20.16s/it]

 85%|████████▍ | 20445/24156 [20:40:26<18:42:33, 18.15s/it]

 85%|████████▍ | 20446/24156 [20:40:38<16:54:21, 16.40s/it]

 85%|████████▍ | 20447/24156 [20:40:52<16:03:03, 15.58s/it]

 85%|████████▍ | 20448/24156 [20:41:08<16:08:36, 15.67s/it]
{'loss': 0.3301, 'learning_rate': 1.8779781973706337e-06, 'rewards/chosen': -2.075963020324707, 'rewards/rejected': -3.8424510955810547, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7664880752563477, 'policy_logps/rejected': -487.4417419433594, 'policy_logps/chosen': -415.4839782714844, 'referece_logps/rejected': -449.0172119140625, 'referece_logps/chosen': -394.7243347167969, 'logits/rejected': -1.1164298057556152, 'logits/chosen': -0.9703488945960999, 'epoch': 7.62}


 85%|████████▍ | 20450/24156 [20:41:45<17:56:06, 17.42s/it]
[2024-04-06 11:46:42,895] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 20451/24156 [20:42:07<19:21:14, 18.81s/it]
[2024-04-06 11:47:04,928] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 20452/24156 [20:42:20<17:16:48, 16.80s/it]

 85%|████████▍ | 20453/24156 [20:42:32<15:52:15, 15.43s/it]

 85%|████████▍ | 20454/24156 [20:42:50<16:37:26, 16.17s/it]

 85%|████████▍ | 20455/24156 [20:43:04<16:02:17, 15.60s/it]

 85%|████████▍ | 20456/24156 [20:43:21<16:20:48, 15.90s/it]
{'loss': 0.3902, 'learning_rate': 1.8774642248198504e-06, 'rewards/chosen': -1.8514363765716553, 'rewards/rejected': -2.663767099380493, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8123306035995483, 'policy_logps/rejected': -372.35986328125, 'policy_logps/chosen': -318.67218017578125, 'referece_logps/rejected': -345.7221984863281, 'referece_logps/chosen': -300.1578369140625, 'logits/rejected': -0.5208024978637695, 'logits/chosen': -0.4397478699684143, 'epoch': 7.62}
[2024-04-06 11:48:38,477] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 85%|████████▍ | 20458/24156 [20:43:56<16:56:08, 16.49s/it]

 85%|████████▍ | 20459/24156 [20:44:18<18:41:40, 18.20s/it]

 85%|████████▍ | 20460/24156 [20:44:36<18:36:54, 18.13s/it]

 85%|████████▍ | 20461/24156 [20:44:47<16:19:06, 15.90s/it]
{'loss': 0.4415, 'learning_rate': 1.8771424792763706e-06, 'rewards/chosen': -1.6607825756072998, 'rewards/rejected': -2.71650767326355, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0557252168655396, 'policy_logps/rejected': -279.0194091796875, 'policy_logps/chosen': -318.11590576171875, 'referece_logps/rejected': -251.85430908203125, 'referece_logps/chosen': -301.5080871582031, 'logits/rejected': -0.8282883763313293, 'logits/chosen': -0.8534489870071411, 'epoch': 7.62}


 85%|████████▍ | 20463/24156 [20:45:12<14:43:26, 14.35s/it]

 85%|████████▍ | 20464/24156 [20:45:24<14:06:41, 13.76s/it]
{'loss': 0.3905, 'learning_rate': 1.8769492427199612e-06, 'rewards/chosen': -1.1714931726455688, 'rewards/rejected': -2.424468994140625, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2529758214950562, 'policy_logps/rejected': -238.17140197753906, 'policy_logps/chosen': -480.94384765625, 'referece_logps/rejected': -213.9267120361328, 'referece_logps/chosen': -469.2289733886719, 'logits/rejected': -0.41374480724334717, 'logits/chosen': -0.34302398562431335, 'epoch': 7.62}

 85%|████████▍ | 20465/24156 [20:45:41<15:06:47, 14.74s/it]


 85%|████████▍ | 20467/24156 [20:46:04<13:17:39, 12.97s/it]

 85%|████████▍ | 20468/24156 [20:46:18<13:43:08, 13.39s/it]

 85%|████████▍ | 20469/24156 [20:46:32<13:51:16, 13.53s/it]

 85%|████████▍ | 20470/24156 [20:46:44<13:15:23, 12.95s/it]

 85%|████████▍ | 20471/24156 [20:46:54<12:31:32, 12.24s/it]
{'loss': 0.2805, 'learning_rate': 1.8764978057025642e-06, 'rewards/chosen': -1.3614879846572876, 'rewards/rejected': -4.423771858215332, 'rewards/accuracies': 1.0, 'rewards/margins': 3.062283992767334, 'policy_logps/rejected': -331.7516784667969, 'policy_logps/chosen': -410.4940185546875, 'referece_logps/rejected': -287.51397705078125, 'referece_logps/chosen': -396.879150390625, 'logits/rejected': -0.784076452255249, 'logits/chosen': -0.8828112483024597, 'epoch': 7.63}


 85%|████████▍ | 20473/24156 [20:47:20<12:37:15, 12.34s/it]

 85%|████████▍ | 20474/24156 [20:47:32<12:26:46, 12.17s/it]

 85%|████████▍ | 20475/24156 [20:47:44<12:35:59, 12.32s/it]

 85%|████████▍ | 20476/24156 [20:47:58<12:54:25, 12.63s/it]

 85%|████████▍ | 20477/24156 [20:48:16<14:39:13, 14.34s/it]

 85%|████████▍ | 20478/24156 [20:48:27<13:30:31, 13.22s/it]

 85%|████████▍ | 20479/24156 [20:48:42<14:17:49, 14.00s/it]

 85%|████████▍ | 20480/24156 [20:48:55<13:49:45, 13.54s/it]
{'loss': 0.2943, 'learning_rate': 1.8759162522440355e-06, 'rewards/chosen': -1.8848695755004883, 'rewards/rejected': -2.9269626140594482, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0420931577682495, 'policy_logps/rejected': -425.4801025390625, 'policy_logps/chosen': -360.92529296875, 'referece_logps/rejected': -396.21051025390625, 'referece_logps/chosen': -342.07659912109375, 'logits/rejected': -0.7862460017204285, 'logits/chosen': -0.7807812094688416, 'epoch': 7.63}

 85%|████████▍ | 20481/24156 [20:49:13<15:22:10, 15.06s/it]


 85%|████████▍ | 20483/24156 [20:49:53<17:37:02, 17.27s/it]

 85%|████████▍ | 20484/24156 [20:50:04<15:48:15, 15.49s/it]

 85%|████████▍ | 20485/24156 [20:50:25<17:15:25, 16.92s/it]

 85%|████████▍ | 20486/24156 [20:50:44<18:09:33, 17.81s/it]
[2024-04-06 11:55:41,961] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3466, 'learning_rate': 1.875527841299061e-06, 'rewards/chosen': -1.5466175079345703, 'rewards/rejected': -4.3840651512146, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8374478816986084, 'policy_logps/rejected': -574.60986328125, 'policy_logps/chosen': -566.98388671875, 'referece_logps/rejected': -530.7691650390625, 'referece_logps/chosen': -551.5177001953125, 'logits/rejected': -0.26951661705970764, 'logits/chosen': -0.2576563060283661, 'epoch': 7.63}


 85%|████████▍ | 20488/24156 [20:51:25<19:14:42, 18.89s/it]
{'loss': 0.3716, 'learning_rate': 1.8753982450564858e-06, 'rewards/chosen': -1.2518819570541382, 'rewards/rejected': -3.1878435611724854, 'rewards/accuracies': 1.0, 'rewards/margins': 1.935961365699768, 'policy_logps/rejected': -446.5007629394531, 'policy_logps/chosen': -377.1929626464844, 'referece_logps/rejected': -414.6223449707031, 'referece_logps/chosen': -364.6741027832031, 'logits/rejected': -0.14712132513523102, 'logits/chosen': -0.2726907432079315, 'epoch': 7.63}
[2024-04-06 11:56:43,149] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 20489/24156 [20:51:46<19:54:23, 19.54s/it]
[2024-04-06 11:56:55,079] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 85%|████████▍ | 20491/24156 [20:52:10<16:06:59, 15.83s/it]

 85%|████████▍ | 20492/24156 [20:52:28<16:52:36, 16.58s/it]

 85%|████████▍ | 20493/24156 [20:52:39<15:06:37, 14.85s/it]
{'loss': 0.3562, 'learning_rate': 1.8750739790717888e-06, 'rewards/chosen': -1.3300107717514038, 'rewards/rejected': -3.2158284187316895, 'rewards/accuracies': 0.875, 'rewards/margins': 1.885817527770996, 'policy_logps/rejected': -328.4653015136719, 'policy_logps/chosen': -287.98638916015625, 'referece_logps/rejected': -296.3070068359375, 'referece_logps/chosen': -274.686279296875, 'logits/rejected': -1.499579906463623, 'logits/chosen': -1.4114900827407837, 'epoch': 7.64}


 85%|████████▍ | 20495/24156 [20:53:04<14:02:52, 13.81s/it]

 85%|████████▍ | 20496/24156 [20:53:23<15:33:18, 15.30s/it]

 85%|████████▍ | 20497/24156 [20:53:43<16:51:30, 16.59s/it]

 85%|████████▍ | 20498/24156 [20:53:55<15:34:40, 15.33s/it]
{'loss': 0.3628, 'learning_rate': 1.8747493198061876e-06, 'rewards/chosen': -0.9337059855461121, 'rewards/rejected': -3.0145459175109863, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0808401107788086, 'policy_logps/rejected': -489.2069091796875, 'policy_logps/chosen': -392.1005554199219, 'referece_logps/rejected': -459.0614318847656, 'referece_logps/chosen': -382.76348876953125, 'logits/rejected': -0.803341269493103, 'logits/chosen': -0.6896408796310425, 'epoch': 7.64}


 85%|████████▍ | 20500/24156 [20:54:35<17:59:20, 17.71s/it]

 85%|████████▍ | 20501/24156 [20:55:09<22:44:55, 22.41s/it]

 85%|████████▍ | 20502/24156 [20:55:25<20:54:25, 20.60s/it]

 85%|████████▍ | 20503/24156 [20:55:43<20:05:55, 19.81s/it]

 85%|████████▍ | 20504/24156 [20:56:01<19:36:40, 19.33s/it]

 85%|████████▍ | 20505/24156 [20:56:19<19:09:48, 18.90s/it]

 85%|████████▍ | 20506/24156 [20:56:35<18:24:16, 18.15s/it]
{'loss': 0.2703, 'learning_rate': 1.8742290473212032e-06, 'rewards/chosen': -2.644587993621826, 'rewards/rejected': -4.547637939453125, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9030500650405884, 'policy_logps/rejected': -344.4561767578125, 'policy_logps/chosen': -378.9935302734375, 'referece_logps/rejected': -298.97979736328125, 'referece_logps/chosen': -352.5476379394531, 'logits/rejected': -0.7958711385726929, 'logits/chosen': -0.7307629585266113, 'epoch': 7.64}


 85%|████████▍ | 20508/24156 [20:57:04<16:37:58, 16.41s/it]

 85%|████████▍ | 20509/24156 [20:57:26<18:21:16, 18.12s/it]
[2024-04-06 12:02:23,876] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 20510/24156 [20:57:47<19:09:50, 18.92s/it]
[2024-04-06 12:02:44,675] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3726, 'learning_rate': 1.8739685338561528e-06, 'rewards/chosen': -2.022836923599243, 'rewards/rejected': -3.467393398284912, 'rewards/accuracies': 0.75, 'rewards/margins': 1.444556474685669, 'policy_logps/rejected': -332.8394470214844, 'policy_logps/chosen': -317.81317138671875, 'referece_logps/rejected': -298.16552734375, 'referece_logps/chosen': -297.58477783203125, 'logits/rejected': -0.25704824924468994, 'logits/chosen': -0.34140005707740784, 'epoch': 7.64}


 85%|████████▍ | 20512/24156 [20:58:25<19:20:57, 19.12s/it]

 85%|████████▍ | 20513/24156 [20:58:41<18:31:23, 18.30s/it]

 85%|████████▍ | 20514/24156 [20:58:57<17:38:09, 17.43s/it]

 85%|████████▍ | 20515/24156 [20:59:13<17:18:39, 17.12s/it]

 85%|████████▍ | 20516/24156 [20:59:32<17:56:11, 17.74s/it]

 85%|████████▍ | 20517/24156 [20:59:45<16:17:07, 16.11s/it]
{'loss': 0.3572, 'learning_rate': 1.8735120304499271e-06, 'rewards/chosen': -1.436500906944275, 'rewards/rejected': -4.444732666015625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0082318782806396, 'policy_logps/rejected': -424.335693359375, 'policy_logps/chosen': -428.9750061035156, 'referece_logps/rejected': -379.8883361816406, 'referece_logps/chosen': -414.6100158691406, 'logits/rejected': -0.81504887342453, 'logits/chosen': -0.8035387992858887, 'epoch': 7.64}


 85%|████████▍ | 20519/24156 [21:00:13<15:37:33, 15.47s/it]
{'loss': 0.3567, 'learning_rate': 1.8733814595592814e-06, 'rewards/chosen': -2.415271759033203, 'rewards/rejected': -4.272883892059326, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8576117753982544, 'policy_logps/rejected': -256.87200927734375, 'policy_logps/chosen': -377.19091796875, 'referece_logps/rejected': -214.14321899414062, 'referece_logps/chosen': -353.0382080078125, 'logits/rejected': 0.12970444560050964, 'logits/chosen': -0.0173739492893219, 'epoch': 7.64}

 85%|████████▍ | 20520/24156 [21:00:24<14:14:10, 14.10s/it]


 85%|████████▍ | 20522/24156 [21:00:59<15:47:09, 15.64s/it]

 85%|████████▍ | 20523/24156 [21:01:15<15:55:01, 15.77s/it]

 85%|████████▍ | 20524/24156 [21:01:25<14:22:02, 14.24s/it]

 85%|████████▍ | 20525/24156 [21:01:41<14:44:48, 14.62s/it]
{'loss': 0.3282, 'learning_rate': 1.8729893701054781e-06, 'rewards/chosen': -1.68408203125, 'rewards/rejected': -3.387315034866333, 'rewards/accuracies': 1.0, 'rewards/margins': 1.703233242034912, 'policy_logps/rejected': -412.7095947265625, 'policy_logps/chosen': -380.24517822265625, 'referece_logps/rejected': -378.8364562988281, 'referece_logps/chosen': -363.4043884277344, 'logits/rejected': -1.0906715393066406, 'logits/chosen': -1.028527021408081, 'epoch': 7.65}

 85%|████████▍ | 20526/24156 [21:01:52<13:49:02, 13.70s/it]

 85%|████████▍ | 20527/24156 [21:02:12<15:43:00, 15.59s/it]


 85%|████████▍ | 20529/24156 [21:02:37<13:59:59, 13.90s/it]

 85%|████████▍ | 20530/24156 [21:02:55<15:12:06, 15.09s/it]

 85%|████████▍ | 20531/24156 [21:03:12<15:33:55, 15.46s/it]

 85%|████████▍ | 20532/24156 [21:03:23<14:26:39, 14.35s/it]

 85%|████████▌ | 20533/24156 [21:03:45<16:33:45, 16.46s/it]

 85%|████████▌ | 20534/24156 [21:04:02<16:44:50, 16.65s/it]

 85%|████████▌ | 20535/24156 [21:04:13<15:14:24, 15.15s/it]

 85%|████████▌ | 20536/24156 [21:04:29<15:27:16, 15.37s/it]

 85%|████████▌ | 20537/24156 [21:04:49<16:44:30, 16.65s/it]
{'loss': 0.3959, 'learning_rate': 1.8722034965258192e-06, 'rewards/chosen': -1.654296636581421, 'rewards/rejected': -3.500974655151367, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8466781377792358, 'policy_logps/rejected': -407.7314147949219, 'policy_logps/chosen': -345.2062683105469, 'referece_logps/rejected': -372.7216796875, 'referece_logps/chosen': -328.6632995605469, 'logits/rejected': -0.7348616123199463, 'logits/chosen': -0.7993631958961487, 'epoch': 7.65}


 85%|████████▌ | 20539/24156 [21:05:28<18:11:54, 18.11s/it]

 85%|████████▌ | 20540/24156 [21:05:43<17:21:29, 17.28s/it]
{'loss': 0.4062, 'learning_rate': 1.872006675259619e-06, 'rewards/chosen': -2.019498586654663, 'rewards/rejected': -3.722212553024292, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7027140855789185, 'policy_logps/rejected': -407.56207275390625, 'policy_logps/chosen': -370.7400817871094, 'referece_logps/rejected': -370.3399353027344, 'referece_logps/chosen': -350.54510498046875, 'logits/rejected': -0.32560956478118896, 'logits/chosen': -0.3244609534740448, 'epoch': 7.65}


 85%|████████▌ | 20542/24156 [21:06:14<16:07:51, 16.07s/it]
{'loss': 0.3636, 'learning_rate': 1.8718753826997082e-06, 'rewards/chosen': -1.5099090337753296, 'rewards/rejected': -3.1071808338165283, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5972716808319092, 'policy_logps/rejected': -351.8826904296875, 'policy_logps/chosen': -349.4373474121094, 'referece_logps/rejected': -320.8109130859375, 'referece_logps/chosen': -334.3382263183594, 'logits/rejected': -1.0413881540298462, 'logits/chosen': -1.0076520442962646, 'epoch': 7.65}
[2024-04-06 12:11:34,212] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20543/24156 [21:06:37<18:09:44, 18.10s/it]

 85%|████████▌ | 20544/24156 [21:06:57<18:45:53, 18.70s/it]

 85%|████████▌ | 20545/24156 [21:07:12<17:47:37, 17.74s/it]

 85%|████████▌ | 20546/24156 [21:07:29<17:20:38, 17.30s/it]


 85%|████████▌ | 20548/24156 [21:08:06<18:07:26, 18.08s/it]
{'loss': 0.3379, 'learning_rate': 1.871481128888113e-06, 'rewards/chosen': -1.488555669784546, 'rewards/rejected': -3.334916591644287, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8463610410690308, 'policy_logps/rejected': -332.8165283203125, 'policy_logps/chosen': -377.3118896484375, 'referece_logps/rejected': -299.46734619140625, 'referece_logps/chosen': -362.42633056640625, 'logits/rejected': -0.2148687243461609, 'logits/chosen': -0.19841158390045166, 'epoch': 7.66}


 85%|████████▌ | 20550/24156 [21:08:38<16:47:53, 16.77s/it]
{'loss': 0.3066, 'learning_rate': 1.8713495856051269e-06, 'rewards/chosen': -1.1492180824279785, 'rewards/rejected': -4.611933708190918, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4627158641815186, 'policy_logps/rejected': -323.9122314453125, 'policy_logps/chosen': -386.92572021484375, 'referece_logps/rejected': -277.7928771972656, 'referece_logps/chosen': -375.43353271484375, 'logits/rejected': -0.974044680595398, 'logits/chosen': -0.9431431293487549, 'epoch': 7.66}

 85%|████████▌ | 20551/24156 [21:08:55<16:49:00, 16.79s/it]

 85%|████████▌ | 20552/24156 [21:09:15<17:47:44, 17.78s/it]

 85%|████████▌ | 20553/24156 [21:09:32<17:43:00, 17.70s/it]


 85%|████████▌ | 20555/24156 [21:10:08<18:01:39, 18.02s/it]
{'loss': 0.3299, 'learning_rate': 1.8710204532934104e-06, 'rewards/chosen': -2.6851186752319336, 'rewards/rejected': -4.571877479553223, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8867590427398682, 'policy_logps/rejected': -587.5106811523438, 'policy_logps/chosen': -511.08160400390625, 'referece_logps/rejected': -541.7919921875, 'referece_logps/chosen': -484.23040771484375, 'logits/rejected': -0.4576740860939026, 'logits/chosen': -0.3103196322917938, 'epoch': 7.66}

 85%|████████▌ | 20556/24156 [21:10:24<17:20:54, 17.35s/it]

 85%|████████▌ | 20557/24156 [21:10:47<19:16:39, 19.28s/it]


 85%|████████▌ | 20559/24156 [21:11:20<17:34:50, 17.60s/it]
{'loss': 0.3526, 'learning_rate': 1.870756865586347e-06, 'rewards/chosen': -1.9527771472930908, 'rewards/rejected': -4.075045585632324, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1222681999206543, 'policy_logps/rejected': -342.1169128417969, 'policy_logps/chosen': -323.00762939453125, 'referece_logps/rejected': -301.366455078125, 'referece_logps/chosen': -303.4798889160156, 'logits/rejected': -0.639201819896698, 'logits/chosen': -0.5243464112281799, 'epoch': 7.66}

 85%|████████▌ | 20560/24156 [21:11:41<18:39:13, 18.67s/it]

 85%|████████▌ | 20561/24156 [21:11:55<17:02:52, 17.07s/it]

 85%|████████▌ | 20562/24156 [21:12:17<18:30:54, 18.55s/it]

 85%|████████▌ | 20563/24156 [21:12:38<19:18:12, 19.34s/it]


 85%|████████▌ | 20565/24156 [21:13:17<19:17:35, 19.34s/it]
{'loss': 0.3671, 'learning_rate': 1.8703610144406398e-06, 'rewards/chosen': -2.4548420906066895, 'rewards/rejected': -4.133991718292236, 'rewards/accuracies': 0.5, 'rewards/margins': 1.6791491508483887, 'policy_logps/rejected': -513.7749633789062, 'policy_logps/chosen': -605.09619140625, 'referece_logps/rejected': -472.43505859375, 'referece_logps/chosen': -580.5478515625, 'logits/rejected': -0.31561416387557983, 'logits/chosen': -0.37734317779541016, 'epoch': 7.66}
[2024-04-06 12:18:34,609] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20566/24156 [21:13:37<19:39:33, 19.71s/it]


 85%|████████▌ | 20568/24156 [21:14:17<19:38:01, 19.70s/it]
{'loss': 0.3278, 'learning_rate': 1.8701628776238652e-06, 'rewards/chosen': -1.8677568435668945, 'rewards/rejected': -3.4045162200927734, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5367597341537476, 'policy_logps/rejected': -446.75408935546875, 'policy_logps/chosen': -336.0240478515625, 'referece_logps/rejected': -412.7089538574219, 'referece_logps/chosen': -317.346435546875, 'logits/rejected': -0.6421970129013062, 'logits/chosen': -0.673291027545929, 'epoch': 7.66}
[2024-04-06 12:19:35,383] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20569/24156 [21:14:38<20:08:05, 20.21s/it]
[2024-04-06 12:19:52,372] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20570/24156 [21:14:55<19:10:02, 19.24s/it]

 85%|████████▌ | 20571/24156 [21:15:12<18:27:21, 18.53s/it]


 85%|████████▌ | 20573/24156 [21:15:51<18:39:22, 18.74s/it]

 85%|████████▌ | 20574/24156 [21:16:04<17:04:17, 17.16s/it]
{'loss': 0.3702, 'learning_rate': 1.8697661816627587e-06, 'rewards/chosen': -1.6626030206680298, 'rewards/rejected': -2.2270421981811523, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5644395351409912, 'policy_logps/rejected': -409.1788330078125, 'policy_logps/chosen': -310.693603515625, 'referece_logps/rejected': -386.90838623046875, 'referece_logps/chosen': -294.06756591796875, 'logits/rejected': -0.6955109238624573, 'logits/chosen': -0.7396867275238037, 'epoch': 7.67}

 85%|████████▌ | 20575/24156 [21:16:21<16:55:35, 17.02s/it]

 85%|████████▌ | 20576/24156 [21:16:32<15:12:04, 15.29s/it]

 85%|████████▌ | 20577/24156 [21:16:44<14:07:06, 14.20s/it]

 85%|████████▌ | 20578/24156 [21:17:05<16:12:22, 16.31s/it]

 85%|████████▌ | 20579/24156 [21:17:19<15:30:18, 15.60s/it]


 85%|████████▌ | 20581/24156 [21:17:49<15:20:53, 15.46s/it]
{'loss': 0.335, 'learning_rate': 1.8693026582964882e-06, 'rewards/chosen': -2.061335325241089, 'rewards/rejected': -3.193981885910034, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1326465606689453, 'policy_logps/rejected': -363.3507080078125, 'policy_logps/chosen': -478.65130615234375, 'referece_logps/rejected': -331.410888671875, 'referece_logps/chosen': -458.0379333496094, 'logits/rejected': -1.4993250370025635, 'logits/chosen': -1.6395450830459595, 'epoch': 7.67}

 85%|████████▌ | 20582/24156 [21:18:10<16:57:37, 17.08s/it]


 85%|████████▌ | 20584/24156 [21:18:41<15:56:48, 16.07s/it]

 85%|████████▌ | 20585/24156 [21:18:58<16:21:13, 16.49s/it]

 85%|████████▌ | 20586/24156 [21:19:14<16:14:10, 16.37s/it]
{'loss': 0.3927, 'learning_rate': 1.8689711013293577e-06, 'rewards/chosen': -2.0520520210266113, 'rewards/rejected': -2.6202304363250732, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5681787729263306, 'policy_logps/rejected': -317.8629150390625, 'policy_logps/chosen': -475.5448303222656, 'referece_logps/rejected': -291.6606140136719, 'referece_logps/chosen': -455.0242919921875, 'logits/rejected': -0.3294674754142761, 'logits/chosen': -0.31788718700408936, 'epoch': 7.67}

 85%|████████▌ | 20587/24156 [21:19:26<14:51:42, 14.99s/it]


 85%|████████▌ | 20589/24156 [21:20:01<15:50:51, 15.99s/it]
{'loss': 0.302, 'learning_rate': 1.8687719796812412e-06, 'rewards/chosen': -2.073638677597046, 'rewards/rejected': -3.346424102783203, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2727856636047363, 'policy_logps/rejected': -455.2451477050781, 'policy_logps/chosen': -505.7708740234375, 'referece_logps/rejected': -421.7808837890625, 'referece_logps/chosen': -485.03448486328125, 'logits/rejected': 0.4791722893714905, 'logits/chosen': 0.5755913257598877, 'epoch': 7.67}

 85%|████████▌ | 20590/24156 [21:20:18<16:02:26, 16.19s/it]
[2024-04-06 12:25:38,641] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 85%|████████▌ | 20592/24156 [21:21:01<18:34:01, 18.75s/it]
{'loss': 0.4732, 'learning_rate': 1.8685727174716172e-06, 'rewards/chosen': -2.069780111312866, 'rewards/rejected': -3.5432288646698, 'rewards/accuracies': 0.75, 'rewards/margins': 1.473448634147644, 'policy_logps/rejected': -370.94879150390625, 'policy_logps/chosen': -370.5099182128906, 'referece_logps/rejected': -335.5164794921875, 'referece_logps/chosen': -349.8121032714844, 'logits/rejected': -0.022795449942350388, 'logits/chosen': -0.12139350920915604, 'epoch': 7.67}

 85%|████████▌ | 20593/24156 [21:21:12<16:13:51, 16.40s/it]


 85%|████████▌ | 20595/24156 [21:21:33<13:23:11, 13.53s/it]
{'loss': 0.3754, 'learning_rate': 1.8683733147327252e-06, 'rewards/chosen': -1.3417221307754517, 'rewards/rejected': -4.146862030029297, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8051400184631348, 'policy_logps/rejected': -230.87319946289062, 'policy_logps/chosen': -400.5144958496094, 'referece_logps/rejected': -189.4045867919922, 'referece_logps/chosen': -387.0972595214844, 'logits/rejected': -0.03614597022533417, 'logits/chosen': -0.07638305425643921, 'epoch': 7.67}

 85%|████████▌ | 20596/24156 [21:21:44<12:37:52, 12.77s/it]

 85%|████████▌ | 20597/24156 [21:21:58<12:51:33, 13.01s/it]

 85%|████████▌ | 20598/24156 [21:22:14<13:54:50, 14.08s/it]

 85%|████████▌ | 20599/24156 [21:22:29<14:13:34, 14.40s/it]


 85%|████████▌ | 20601/24156 [21:23:03<15:01:44, 15.22s/it]

 85%|████████▌ | 20602/24156 [21:23:21<15:57:46, 16.17s/it]

 85%|████████▌ | 20603/24156 [21:23:43<17:39:52, 17.90s/it]
{'loss': 0.3017, 'learning_rate': 1.8678408873091246e-06, 'rewards/chosen': -2.6114115715026855, 'rewards/rejected': -4.443193435668945, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8317816257476807, 'policy_logps/rejected': -452.9986572265625, 'policy_logps/chosen': -415.6982727050781, 'referece_logps/rejected': -408.56671142578125, 'referece_logps/chosen': -389.58416748046875, 'logits/rejected': -0.2810211777687073, 'logits/chosen': -0.36233991384506226, 'epoch': 7.68}

 85%|████████▌ | 20604/24156 [21:23:59<16:57:06, 17.18s/it]

 85%|████████▌ | 20605/24156 [21:24:20<18:06:59, 18.37s/it]
[2024-04-06 12:29:38,878] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20606/24156 [21:24:41<19:06:44, 19.38s/it]


 85%|████████▌ | 20608/24156 [21:25:13<17:44:56, 18.01s/it]
[2024-04-06 12:30:10,478] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20609/24156 [21:25:33<18:23:48, 18.67s/it]

 85%|████████▌ | 20610/24156 [21:25:55<19:12:52, 19.51s/it]

 85%|████████▌ | 20611/24156 [21:26:13<18:51:50, 19.16s/it]
{'loss': 0.3584, 'learning_rate': 1.8673074614083586e-06, 'rewards/chosen': -2.529306173324585, 'rewards/rejected': -4.570511341094971, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0412046909332275, 'policy_logps/rejected': -395.5537109375, 'policy_logps/chosen': -379.1752014160156, 'referece_logps/rejected': -349.8486328125, 'referece_logps/chosen': -353.88214111328125, 'logits/rejected': -0.3599940538406372, 'logits/chosen': -0.34493499994277954, 'epoch': 7.68}
[2024-04-06 12:31:27,382] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20612/24156 [21:26:30<18:11:23, 18.48s/it]


 85%|████████▌ | 20614/24156 [21:27:11<19:14:18, 19.55s/it]

 85%|████████▌ | 20615/24156 [21:27:31<19:27:12, 19.78s/it]
[2024-04-06 12:32:28,620] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.336, 'learning_rate': 1.867040374220782e-06, 'rewards/chosen': -2.3008360862731934, 'rewards/rejected': -3.7849254608154297, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4840893745422363, 'policy_logps/rejected': -489.63714599609375, 'policy_logps/chosen': -415.908203125, 'referece_logps/rejected': -451.7878723144531, 'referece_logps/chosen': -392.89984130859375, 'logits/rejected': -0.5534665584564209, 'logits/chosen': -0.7633504867553711, 'epoch': 7.68}


 85%|████████▌ | 20617/24156 [21:28:09<18:56:07, 19.26s/it]
{'loss': 0.4457, 'learning_rate': 1.866906737101294e-06, 'rewards/chosen': -1.9384922981262207, 'rewards/rejected': -3.9524941444396973, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0140018463134766, 'policy_logps/rejected': -281.704833984375, 'policy_logps/chosen': -287.66455078125, 'referece_logps/rejected': -242.17991638183594, 'referece_logps/chosen': -268.27960205078125, 'logits/rejected': -0.5139829516410828, 'logits/chosen': -0.33651289343833923, 'epoch': 7.68}

 85%|████████▌ | 20618/24156 [21:28:29<19:06:04, 19.44s/it]

 85%|████████▌ | 20619/24156 [21:28:49<19:14:16, 19.58s/it]

 85%|████████▌ | 20620/24156 [21:29:08<19:09:30, 19.51s/it]


 85%|████████▌ | 20622/24156 [21:29:43<18:01:56, 18.37s/it]
{'loss': 0.3789, 'learning_rate': 1.8665723715963621e-06, 'rewards/chosen': -1.3007524013519287, 'rewards/rejected': -2.5780768394470215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2773244380950928, 'policy_logps/rejected': -331.3904113769531, 'policy_logps/chosen': -511.4595031738281, 'referece_logps/rejected': -305.6096496582031, 'referece_logps/chosen': -498.4519958496094, 'logits/rejected': -0.04983510449528694, 'logits/chosen': -0.2727595865726471, 'epoch': 7.68}

 85%|████████▌ | 20623/24156 [21:30:03<18:18:32, 18.66s/it]

 85%|████████▌ | 20624/24156 [21:30:21<18:10:54, 18.53s/it]

 85%|████████▌ | 20625/24156 [21:30:32<16:08:57, 16.47s/it]

 85%|████████▌ | 20626/24156 [21:30:49<16:02:07, 16.35s/it]

 85%|████████▌ | 20627/24156 [21:31:04<15:44:13, 16.05s/it]

 85%|████████▌ | 20628/24156 [21:31:20<15:48:00, 16.12s/it]

 85%|████████▌ | 20629/24156 [21:31:36<15:42:06, 16.03s/it]

 85%|████████▌ | 20630/24156 [21:31:47<14:07:54, 14.43s/it]

 85%|████████▌ | 20631/24156 [21:31:58<13:08:08, 13.42s/it]


 85%|████████▌ | 20633/24156 [21:32:38<16:28:50, 16.84s/it]
[2024-04-06 12:37:35,167] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3015, 'learning_rate': 1.8658353967979386e-06, 'rewards/chosen': -2.6605641841888428, 'rewards/rejected': -4.902403831481934, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2418394088745117, 'policy_logps/rejected': -356.37078857421875, 'policy_logps/chosen': -574.6648559570312, 'referece_logps/rejected': -307.3467102050781, 'referece_logps/chosen': -548.0592041015625, 'logits/rejected': 0.3985157310962677, 'logits/chosen': 0.02657056227326393, 'epoch': 7.69}
[2024-04-06 12:37:54,598] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20634/24156 [21:32:57<17:14:10, 17.62s/it]

 85%|████████▌ | 20635/24156 [21:33:09<15:25:22, 15.77s/it]

 85%|████████▌ | 20636/24156 [21:33:21<14:24:39, 14.74s/it]

 85%|████████▌ | 20637/24156 [21:33:37<14:40:45, 15.02s/it]


 85%|████████▌ | 20639/24156 [21:34:00<12:53:26, 13.19s/it]

 85%|████████▌ | 20640/24156 [21:34:17<14:09:46, 14.50s/it]
{'loss': 0.3423, 'learning_rate': 1.8653654321294124e-06, 'rewards/chosen': -2.2258031368255615, 'rewards/rejected': -4.462584495544434, 'rewards/accuracies': 0.75, 'rewards/margins': 2.236781120300293, 'policy_logps/rejected': -448.14306640625, 'policy_logps/chosen': -495.0335388183594, 'referece_logps/rejected': -403.5171813964844, 'referece_logps/chosen': -472.7755126953125, 'logits/rejected': -0.9265705943107605, 'logits/chosen': -0.8054537177085876, 'epoch': 7.69}

 85%|████████▌ | 20641/24156 [21:34:30<13:47:30, 14.13s/it]

 85%|████████▌ | 20642/24156 [21:34:51<15:35:17, 15.97s/it]

 85%|████████▌ | 20643/24156 [21:35:11<16:41:15, 17.10s/it]


 85%|████████▌ | 20645/24156 [21:35:41<16:18:20, 16.72s/it]
[2024-04-06 12:40:38,982] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20646/24156 [21:35:53<14:53:12, 15.27s/it]
[2024-04-06 12:40:50,867] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5491, 'learning_rate': 1.8649619985434834e-06, 'rewards/chosen': -2.2566757202148438, 'rewards/rejected': -3.5354154109954834, 'rewards/accuracies': 0.625, 'rewards/margins': 1.2787398099899292, 'policy_logps/rejected': -610.5443115234375, 'policy_logps/chosen': -542.2447509765625, 'referece_logps/rejected': -575.190185546875, 'referece_logps/chosen': -519.6780395507812, 'logits/rejected': -0.34575241804122925, 'logits/chosen': -0.27295684814453125, 'epoch': 7.69}

 85%|████████▌ | 20647/24156 [21:36:05<13:50:38, 14.20s/it]

 85%|████████▌ | 20648/24156 [21:36:26<15:47:54, 16.21s/it]


 85%|████████▌ | 20650/24156 [21:36:50<13:30:23, 13.87s/it]

 85%|████████▌ | 20651/24156 [21:37:09<15:16:45, 15.69s/it]
{'loss': 0.3443, 'learning_rate': 1.8646253762675083e-06, 'rewards/chosen': -1.5323512554168701, 'rewards/rejected': -3.5598104000091553, 'rewards/accuracies': 1.0, 'rewards/margins': 2.027459144592285, 'policy_logps/rejected': -241.5966796875, 'policy_logps/chosen': -327.3046875, 'referece_logps/rejected': -205.99856567382812, 'referece_logps/chosen': -311.9811706542969, 'logits/rejected': -0.5227916836738586, 'logits/chosen': -0.6213683485984802, 'epoch': 7.69}

 85%|████████▌ | 20652/24156 [21:37:23<14:43:16, 15.12s/it]
[2024-04-06 12:42:38,850] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▌ | 20653/24156 [21:37:41<15:34:48, 16.01s/it]

 86%|████████▌ | 20654/24156 [21:38:00<16:26:19, 16.90s/it]

 86%|████████▌ | 20655/24156 [21:38:16<16:07:31, 16.58s/it]

 86%|████████▌ | 20656/24156 [21:38:33<16:09:17, 16.62s/it]

 86%|████████▌ | 20657/24156 [21:38:44<14:37:01, 15.04s/it]

 86%|████████▌ | 20658/24156 [21:39:05<16:15:59, 16.74s/it]

 86%|████████▌ | 20659/24156 [21:39:21<16:05:21, 16.56s/it]

 86%|████████▌ | 20660/24156 [21:39:42<17:13:17, 17.73s/it]

 86%|████████▌ | 20661/24156 [21:39:56<16:13:11, 16.71s/it]

 86%|████████▌ | 20662/24156 [21:40:07<14:37:37, 15.07s/it]

 86%|████████▌ | 20663/24156 [21:40:26<15:51:26, 16.34s/it]

 86%|████████▌ | 20664/24156 [21:40:41<15:19:13, 15.79s/it]

 86%|████████▌ | 20665/24156 [21:40:55<14:51:04, 15.31s/it]
[2024-04-06 12:46:10,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20666/24156 [21:41:13<15:27:14, 15.94s/it]

 86%|████████▌ | 20667/24156 [21:41:33<16:53:06, 17.42s/it]

 86%|████████▌ | 20668/24156 [21:41:48<16:03:50, 16.58s/it]

 86%|████████▌ | 20669/24156 [21:42:01<14:54:34, 15.39s/it]

 86%|████████▌ | 20670/24156 [21:42:21<16:12:43, 16.74s/it]

 86%|████████▌ | 20671/24156 [21:42:40<16:53:43, 17.45s/it]

 86%|████████▌ | 20672/24156 [21:43:00<17:46:16, 18.36s/it]

 86%|████████▌ | 20673/24156 [21:43:14<16:22:11, 16.92s/it]

 86%|████████▌ | 20674/24156 [21:43:26<15:03:35, 15.57s/it]
[2024-04-06 12:48:44,364] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20675/24156 [21:43:47<16:33:35, 17.13s/it]
[2024-04-06 12:49:01,501] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20676/24156 [21:44:04<16:33:32, 17.13s/it]

 86%|████████▌ | 20677/24156 [21:44:26<17:56:13, 18.56s/it]

 86%|████████▌ | 20678/24156 [21:44:38<16:05:03, 16.65s/it]

 86%|████████▌ | 20679/24156 [21:44:57<16:43:59, 17.33s/it]

 86%|████████▌ | 20680/24156 [21:45:14<16:35:32, 17.18s/it]

 86%|████████▌ | 20681/24156 [21:45:26<15:04:58, 15.63s/it]

 86%|████████▌ | 20682/24156 [21:45:47<16:44:33, 17.35s/it]

 86%|████████▌ | 20683/24156 [21:46:08<17:46:51, 18.43s/it]

 86%|████████▌ | 20684/24156 [21:46:29<18:19:39, 19.00s/it]

 86%|████████▌ | 20685/24156 [21:46:39<15:57:35, 16.55s/it]

 86%|████████▌ | 20686/24156 [21:46:59<16:50:54, 17.48s/it]

 86%|████████▌ | 20687/24156 [21:47:11<15:17:19, 15.87s/it]

 86%|████████▌ | 20688/24156 [21:47:22<13:46:36, 14.30s/it]

 86%|████████▌ | 20689/24156 [21:47:38<14:21:37, 14.91s/it]
[2024-04-06 12:52:51,431] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20690/24156 [21:47:54<14:37:53, 15.20s/it]

 86%|████████▌ | 20691/24156 [21:48:10<14:50:29, 15.42s/it]


 86%|████████▌ | 20693/24156 [21:48:38<14:34:49, 15.16s/it]

 86%|████████▌ | 20694/24156 [21:48:58<15:49:42, 16.46s/it]

 86%|████████▌ | 20695/24156 [21:49:14<15:46:15, 16.40s/it]

 86%|████████▌ | 20696/24156 [21:49:34<16:52:54, 17.56s/it]
[2024-04-06 12:54:31,947] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20697/24156 [21:49:50<16:13:56, 16.89s/it]

 86%|████████▌ | 20698/24156 [21:50:08<16:42:29, 17.39s/it]

 86%|████████▌ | 20699/24156 [21:50:28<17:22:11, 18.09s/it]

 86%|████████▌ | 20700/24156 [21:50:48<17:49:06, 18.56s/it]

 86%|████████▌ | 20701/24156 [21:51:06<17:35:44, 18.33s/it]

 86%|████████▌ | 20702/24156 [21:51:25<17:59:00, 18.74s/it]

 86%|████████▌ | 20703/24156 [21:51:41<17:10:32, 17.91s/it]

 86%|████████▌ | 20704/24156 [21:51:58<16:56:18, 17.66s/it]

 86%|████████▌ | 20705/24156 [21:52:14<16:14:04, 16.94s/it]

 86%|████████▌ | 20706/24156 [21:52:30<16:00:58, 16.71s/it]

 86%|████████▌ | 20707/24156 [21:52:48<16:31:24, 17.25s/it]

 86%|████████▌ | 20708/24156 [21:53:08<17:15:56, 18.03s/it]

 86%|████████▌ | 20709/24156 [21:53:29<18:10:01, 18.97s/it]

 86%|████████▌ | 20710/24156 [21:53:49<18:22:27, 19.20s/it]

 86%|████████▌ | 20711/24156 [21:54:06<17:52:42, 18.68s/it]

 86%|████████▌ | 20712/24156 [21:54:29<18:59:33, 19.85s/it]
[2024-04-06 12:59:26,499] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20713/24156 [21:54:40<16:29:54, 17.25s/it]

 86%|████████▌ | 20714/24156 [21:54:55<15:46:15, 16.49s/it]

 86%|████████▌ | 20715/24156 [21:55:12<15:49:11, 16.55s/it]

 86%|████████▌ | 20716/24156 [21:55:28<15:41:29, 16.42s/it]

 86%|████████▌ | 20717/24156 [21:55:42<15:11:03, 15.90s/it]

 86%|████████▌ | 20718/24156 [21:55:54<13:54:48, 14.57s/it]

 86%|████████▌ | 20719/24156 [21:56:11<14:31:01, 15.21s/it]

 86%|████████▌ | 20720/24156 [21:56:30<15:40:59, 16.43s/it]

 86%|████████▌ | 20721/24156 [21:56:42<14:31:16, 15.22s/it]

 86%|████████▌ | 20722/24156 [21:57:02<15:44:45, 16.51s/it]

 86%|████████▌ | 20723/24156 [21:57:22<16:54:40, 17.73s/it]

 86%|████████▌ | 20724/24156 [21:57:43<17:52:42, 18.75s/it]

 86%|████████▌ | 20725/24156 [21:58:06<18:48:59, 19.74s/it]

 86%|████████▌ | 20726/24156 [21:58:18<16:47:02, 17.62s/it]

 86%|████████▌ | 20727/24156 [21:58:35<16:33:52, 17.39s/it]

 86%|████████▌ | 20728/24156 [21:58:49<15:40:50, 16.47s/it]

 86%|████████▌ | 20729/24156 [21:59:05<15:24:38, 16.19s/it]

 86%|████████▌ | 20730/24156 [21:59:24<16:16:02, 17.09s/it]

 86%|████████▌ | 20731/24156 [21:59:44<17:08:34, 18.02s/it]
[2024-04-06 13:04:41,772] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20732/24156 [22:00:03<17:13:35, 18.11s/it]

 86%|████████▌ | 20733/24156 [22:00:22<17:38:08, 18.55s/it]

 86%|████████▌ | 20734/24156 [22:00:39<17:11:32, 18.09s/it]

 86%|████████▌ | 20735/24156 [22:01:01<18:15:18, 19.21s/it]

 86%|████████▌ | 20736/24156 [22:01:17<17:23:02, 18.30s/it]

 86%|████████▌ | 20737/24156 [22:01:29<15:36:19, 16.43s/it]

 86%|████████▌ | 20738/24156 [22:01:48<16:08:08, 16.99s/it]

 86%|████████▌ | 20739/24156 [22:02:01<15:12:37, 16.03s/it]

 86%|████████▌ | 20740/24156 [22:02:18<15:17:26, 16.11s/it]

 86%|████████▌ | 20741/24156 [22:02:39<16:40:33, 17.58s/it]

 86%|████████▌ | 20742/24156 [22:02:52<15:27:49, 16.31s/it]

 86%|████████▌ | 20743/24156 [22:03:10<15:55:27, 16.80s/it]

 86%|████████▌ | 20744/24156 [22:03:29<16:42:13, 17.62s/it]

 86%|████████▌ | 20745/24156 [22:03:48<16:52:58, 17.82s/it]

 86%|████████▌ | 20746/24156 [22:04:08<17:25:26, 18.39s/it]
[2024-04-06 13:09:04,990] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20747/24156 [22:04:24<16:55:47, 17.88s/it]

 86%|████████▌ | 20748/24156 [22:04:44<17:25:36, 18.41s/it]

 86%|████████▌ | 20749/24156 [22:04:56<15:31:38, 16.41s/it]

 86%|████████▌ | 20750/24156 [22:05:07<14:05:21, 14.89s/it]

 86%|████████▌ | 20751/24156 [22:05:27<15:26:35, 16.33s/it]

 86%|████████▌ | 20752/24156 [22:05:47<16:34:21, 17.53s/it]

 86%|████████▌ | 20753/24156 [22:06:00<15:16:58, 16.17s/it]

 86%|████████▌ | 20754/24156 [22:06:17<15:37:20, 16.53s/it]

 86%|████████▌ | 20755/24156 [22:06:29<14:11:51, 15.03s/it]

 86%|████████▌ | 20756/24156 [22:06:50<15:50:38, 16.78s/it]

 86%|████████▌ | 20757/24156 [22:07:00<14:05:40, 14.93s/it]

 86%|████████▌ | 20758/24156 [22:07:13<13:23:11, 14.18s/it]

 86%|████████▌ | 20759/24156 [22:07:32<14:50:57, 15.74s/it]

 86%|████████▌ | 20760/24156 [22:07:44<13:41:47, 14.52s/it]

 86%|████████▌ | 20761/24156 [22:07:56<12:58:23, 13.76s/it]

 86%|████████▌ | 20762/24156 [22:08:14<14:15:46, 15.13s/it]

 86%|████████▌ | 20763/24156 [22:08:30<14:25:15, 15.30s/it]

 86%|████████▌ | 20764/24156 [22:08:41<13:21:10, 14.17s/it]

 86%|████████▌ | 20765/24156 [22:08:52<12:30:13, 13.27s/it]

 86%|████████▌ | 20766/24156 [22:09:04<11:53:31, 12.63s/it]

 86%|████████▌ | 20767/24156 [22:09:14<11:21:29, 12.07s/it]

 86%|████████▌ | 20768/24156 [22:09:27<11:33:42, 12.29s/it]

 86%|████████▌ | 20769/24156 [22:09:45<13:07:07, 13.94s/it]

 86%|████████▌ | 20770/24156 [22:10:05<14:54:57, 15.86s/it]

 86%|████████▌ | 20771/24156 [22:10:20<14:35:14, 15.51s/it]

 86%|████████▌ | 20772/24156 [22:10:34<14:03:26, 14.95s/it]

 86%|████████▌ | 20773/24156 [22:10:50<14:31:00, 15.45s/it]

 86%|████████▌ | 20774/24156 [22:11:11<15:52:31, 16.90s/it]

 86%|████████▌ | 20775/24156 [22:11:27<15:47:09, 16.81s/it]

 86%|████████▌ | 20776/24156 [22:11:51<17:40:53, 18.83s/it]
[2024-04-06 13:16:48,191] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20777/24156 [22:12:03<15:45:23, 16.79s/it]
[2024-04-06 13:17:00,206] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20778/24156 [22:12:24<16:58:52, 18.10s/it]

 86%|████████▌ | 20779/24156 [22:12:35<15:00:35, 16.00s/it]

 86%|████████▌ | 20780/24156 [22:12:52<15:13:27, 16.23s/it]

 86%|████████▌ | 20781/24156 [22:13:11<16:04:18, 17.14s/it]
{'loss': 0.3029, 'learning_rate': 1.8557372498289037e-06, 'rewards/chosen': -3.1327052116394043, 'rewards/rejected': -4.850392818450928, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7176873683929443, 'policy_logps/rejected': -458.73626708984375, 'policy_logps/chosen': -274.49163818359375, 'referece_logps/rejected': -410.2322998046875, 'referece_logps/chosen': -243.16458129882812, 'logits/rejected': -0.08496531844139099, 'logits/chosen': 0.17867907881736755, 'epoch': 7.74}


 86%|████████▌ | 20783/24156 [22:13:51<17:33:06, 18.73s/it]
{'loss': 0.3187, 'learning_rate': 1.8555984718112487e-06, 'rewards/chosen': -2.312844753265381, 'rewards/rejected': -4.537724494934082, 'rewards/accuracies': 0.75, 'rewards/margins': 2.224879741668701, 'policy_logps/rejected': -417.4337158203125, 'policy_logps/chosen': -423.9357604980469, 'referece_logps/rejected': -372.05645751953125, 'referece_logps/chosen': -400.807373046875, 'logits/rejected': -1.0043058395385742, 'logits/chosen': -1.0128449201583862, 'epoch': 7.74}


 86%|████████▌ | 20785/24156 [22:14:26<17:10:18, 18.34s/it]

 86%|████████▌ | 20786/24156 [22:14:42<16:36:49, 17.75s/it]

 86%|████████▌ | 20787/24156 [22:14:56<15:32:10, 16.60s/it]

 86%|████████▌ | 20788/24156 [22:15:16<16:30:28, 17.64s/it]
{'loss': 0.3457, 'learning_rate': 1.8552512576192693e-06, 'rewards/chosen': -1.9004926681518555, 'rewards/rejected': -3.8746838569641113, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9741911888122559, 'policy_logps/rejected': -350.43475341796875, 'policy_logps/chosen': -415.2091979980469, 'referece_logps/rejected': -311.68792724609375, 'referece_logps/chosen': -396.20428466796875, 'logits/rejected': -1.2888151407241821, 'logits/chosen': -1.4897791147232056, 'epoch': 7.75}


 86%|████████▌ | 20790/24156 [22:15:48<15:28:24, 16.55s/it]
{'loss': 0.4097, 'learning_rate': 1.8551122643095565e-06, 'rewards/chosen': -2.1699576377868652, 'rewards/rejected': -4.514115333557129, 'rewards/accuracies': 1.0, 'rewards/margins': 2.344158172607422, 'policy_logps/rejected': -307.9611511230469, 'policy_logps/chosen': -387.6640625, 'referece_logps/rejected': -262.8199768066406, 'referece_logps/chosen': -365.9644775390625, 'logits/rejected': -0.2700420022010803, 'logits/chosen': -0.3235996961593628, 'epoch': 7.75}


 86%|████████▌ | 20792/24156 [22:16:15<14:11:40, 15.19s/it]

 86%|████████▌ | 20793/24156 [22:16:35<15:27:06, 16.54s/it]

 86%|████████▌ | 20794/24156 [22:16:54<16:07:10, 17.26s/it]

 86%|████████▌ | 20795/24156 [22:17:14<16:56:40, 18.15s/it]
[2024-04-06 13:22:11,781] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20796/24156 [22:17:25<14:58:55, 16.05s/it]

 86%|████████▌ | 20797/24156 [22:17:44<15:34:46, 16.70s/it]
{'loss': 0.4951, 'learning_rate': 1.854625303561089e-06, 'rewards/chosen': -1.8557146787643433, 'rewards/rejected': -3.0221660137176514, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1664514541625977, 'policy_logps/rejected': -477.10107421875, 'policy_logps/chosen': -306.1581115722656, 'referece_logps/rejected': -446.8794860839844, 'referece_logps/chosen': -287.6009521484375, 'logits/rejected': -0.5533438324928284, 'logits/chosen': -0.5479140877723694, 'epoch': 7.75}


 86%|████████▌ | 20799/24156 [22:18:13<14:16:38, 15.31s/it]

 86%|████████▌ | 20800/24156 [22:18:25<13:20:14, 14.31s/it]
{'loss': 0.4778, 'learning_rate': 1.8544163756179893e-06, 'rewards/chosen': -1.4742388725280762, 'rewards/rejected': -2.2698581218719482, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7956191897392273, 'policy_logps/rejected': -400.139404296875, 'policy_logps/chosen': -486.2507019042969, 'referece_logps/rejected': -377.4408264160156, 'referece_logps/chosen': -471.50830078125, 'logits/rejected': -0.8762980699539185, 'logits/chosen': -0.8820995092391968, 'epoch': 7.75}


 86%|████████▌ | 20802/24156 [22:18:47<11:52:34, 12.75s/it]

 86%|████████▌ | 20803/24156 [22:19:03<12:46:33, 13.72s/it]

 86%|████████▌ | 20804/24156 [22:19:14<11:55:51, 12.81s/it]
{'loss': 0.3744, 'learning_rate': 1.8541375899947987e-06, 'rewards/chosen': -1.7273311614990234, 'rewards/rejected': -4.48770809173584, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7603766918182373, 'policy_logps/rejected': -534.321044921875, 'policy_logps/chosen': -386.46563720703125, 'referece_logps/rejected': -489.4439392089844, 'referece_logps/chosen': -369.1923522949219, 'logits/rejected': -0.691652774810791, 'logits/chosen': -0.49897682666778564, 'epoch': 7.75}


 86%|████████▌ | 20806/24156 [22:19:41<12:24:53, 13.34s/it]

 86%|████████▌ | 20807/24156 [22:20:01<14:17:23, 15.36s/it]
{'loss': 0.4163, 'learning_rate': 1.853928339546965e-06, 'rewards/chosen': -1.9406299591064453, 'rewards/rejected': -3.1196062564849854, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1789765357971191, 'policy_logps/rejected': -378.6820373535156, 'policy_logps/chosen': -457.2627868652344, 'referece_logps/rejected': -347.4859924316406, 'referece_logps/chosen': -437.85650634765625, 'logits/rejected': 0.4081600308418274, 'logits/chosen': 0.43511316180229187, 'epoch': 7.75}


 86%|████████▌ | 20809/24156 [22:20:36<15:32:24, 16.71s/it]
[2024-04-06 13:25:33,103] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20810/24156 [22:20:55<16:10:36, 17.40s/it]
[2024-04-06 13:25:52,118] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20811/24156 [22:21:16<17:09:54, 18.47s/it]
{'loss': 0.4513, 'learning_rate': 1.8536491240402993e-06, 'rewards/chosen': -3.0265605449676514, 'rewards/rejected': -3.3505067825317383, 'rewards/accuracies': 0.5, 'rewards/margins': 0.32394611835479736, 'policy_logps/rejected': -432.3525390625, 'policy_logps/chosen': -499.4844055175781, 'referece_logps/rejected': -398.84747314453125, 'referece_logps/chosen': -469.21881103515625, 'logits/rejected': 0.06305383145809174, 'logits/chosen': -0.052457407116889954, 'epoch': 7.75}


 86%|████████▌ | 20813/24156 [22:21:55<17:40:45, 19.04s/it]

 86%|████████▌ | 20814/24156 [22:22:14<17:48:10, 19.18s/it]

 86%|████████▌ | 20815/24156 [22:22:26<15:37:48, 16.84s/it]

 86%|████████▌ | 20816/24156 [22:22:40<14:49:38, 15.98s/it]
{'loss': 0.2763, 'learning_rate': 1.8532997593795635e-06, 'rewards/chosen': -2.0782630443573, 'rewards/rejected': -3.720140218734741, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6418770551681519, 'policy_logps/rejected': -363.51336669921875, 'policy_logps/chosen': -417.6051025390625, 'referece_logps/rejected': -326.3119812011719, 'referece_logps/chosen': -396.8224182128906, 'logits/rejected': -1.0360445976257324, 'logits/chosen': -1.1133662462234497, 'epoch': 7.76}


 86%|████████▌ | 20818/24156 [22:23:19<16:32:07, 17.83s/it]
{'loss': 0.3142, 'learning_rate': 1.8531599061278689e-06, 'rewards/chosen': -1.5481657981872559, 'rewards/rejected': -3.3819947242736816, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8338290452957153, 'policy_logps/rejected': -323.35980224609375, 'policy_logps/chosen': -357.04290771484375, 'referece_logps/rejected': -289.53985595703125, 'referece_logps/chosen': -341.5612487792969, 'logits/rejected': 0.465899258852005, 'logits/chosen': 0.40372052788734436, 'epoch': 7.76}


 86%|████████▌ | 20820/24156 [22:23:58<17:29:39, 18.88s/it]
[2024-04-06 13:28:55,614] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 20821/24156 [22:24:14<16:43:18, 18.05s/it]
[2024-04-06 13:29:11,732] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3428, 'learning_rate': 1.8529500112238212e-06, 'rewards/chosen': -2.6232118606567383, 'rewards/rejected': -3.688624858856201, 'rewards/accuracies': 0.75, 'rewards/margins': 1.065412998199463, 'policy_logps/rejected': -454.9202880859375, 'policy_logps/chosen': -377.989501953125, 'referece_logps/rejected': -418.0340576171875, 'referece_logps/chosen': -351.7573547363281, 'logits/rejected': 0.0036038868129253387, 'logits/chosen': -0.042501501739025116, 'epoch': 7.76}


 86%|████████▌ | 20823/24156 [22:24:42<14:31:51, 15.69s/it]

 86%|████████▌ | 20824/24156 [22:25:01<15:29:23, 16.74s/it]

 86%|████████▌ | 20825/24156 [22:25:23<16:53:20, 18.25s/it]

 86%|████████▌ | 20826/24156 [22:25:42<17:16:12, 18.67s/it]

 86%|████████▌ | 20827/24156 [22:25:55<15:35:41, 16.86s/it]

 86%|████████▌ | 20828/24156 [22:26:15<16:23:18, 17.73s/it]

 86%|████████▌ | 20829/24156 [22:26:27<14:51:35, 16.08s/it]

 86%|████████▌ | 20830/24156 [22:26:46<15:47:50, 17.10s/it]

 86%|████████▌ | 20831/24156 [22:26:58<14:19:27, 15.51s/it]

 86%|████████▌ | 20832/24156 [22:27:15<14:35:14, 15.80s/it]

 86%|████████▌ | 20833/24156 [22:27:25<13:07:17, 14.22s/it]

 86%|████████▌ | 20834/24156 [22:27:43<14:11:20, 15.38s/it]

 86%|████████▋ | 20835/24156 [22:27:59<14:12:23, 15.40s/it]

 86%|████████▋ | 20836/24156 [22:28:16<14:50:16, 16.09s/it]

 86%|████████▋ | 20837/24156 [22:28:33<15:01:44, 16.30s/it]

 86%|████████▋ | 20838/24156 [22:28:54<16:19:42, 17.72s/it]
[2024-04-06 13:33:51,603] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 20839/24156 [22:29:14<16:55:22, 18.37s/it]
[2024-04-06 13:34:11,487] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 20840/24156 [22:29:30<16:23:08, 17.79s/it]

 86%|████████▋ | 20841/24156 [22:29:52<17:28:57, 18.99s/it]

 86%|████████▋ | 20842/24156 [22:30:08<16:41:48, 18.14s/it]

 86%|████████▋ | 20843/24156 [22:30:20<14:57:12, 16.25s/it]

 86%|████████▋ | 20844/24156 [22:30:32<13:47:03, 14.98s/it]

 86%|████████▋ | 20845/24156 [22:30:49<14:13:07, 15.46s/it]
{'loss': 0.2604, 'learning_rate': 1.851265886790466e-06, 'rewards/chosen': -3.0310235023498535, 'rewards/rejected': -4.309793949127197, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2787704467773438, 'policy_logps/rejected': -271.6700744628906, 'policy_logps/chosen': -331.38006591796875, 'referece_logps/rejected': -228.5721435546875, 'referece_logps/chosen': -301.06982421875, 'logits/rejected': 0.2591901421546936, 'logits/chosen': 0.32665178179740906, 'epoch': 7.77}


 86%|████████▋ | 20847/24156 [22:31:25<15:46:38, 17.16s/it]
{'loss': 0.2784, 'learning_rate': 1.851125144962668e-06, 'rewards/chosen': -1.9399796724319458, 'rewards/rejected': -5.16829776763916, 'rewards/accuracies': 1.0, 'rewards/margins': 3.228318452835083, 'policy_logps/rejected': -301.4423522949219, 'policy_logps/chosen': -271.4897155761719, 'referece_logps/rejected': -249.75936889648438, 'referece_logps/chosen': -252.08990478515625, 'logits/rejected': -0.5244932174682617, 'logits/chosen': -0.41823625564575195, 'epoch': 7.77}


 86%|████████▋ | 20849/24156 [22:31:51<13:52:36, 15.11s/it]

 86%|████████▋ | 20850/24156 [22:32:07<14:04:22, 15.32s/it]

 86%|████████▋ | 20851/24156 [22:32:28<15:42:24, 17.11s/it]

 86%|████████▋ | 20852/24156 [22:32:47<16:00:19, 17.44s/it]

 86%|████████▋ | 20853/24156 [22:32:59<14:29:23, 15.79s/it]
{'loss': 0.3235, 'learning_rate': 1.8507025523027955e-06, 'rewards/chosen': -1.8926351070404053, 'rewards/rejected': -4.1422929763793945, 'rewards/accuracies': 1.0, 'rewards/margins': 2.24965763092041, 'policy_logps/rejected': -334.8731994628906, 'policy_logps/chosen': -308.67388916015625, 'referece_logps/rejected': -293.45025634765625, 'referece_logps/chosen': -289.7475280761719, 'logits/rejected': -0.42553579807281494, 'logits/chosen': -0.26550954580307007, 'epoch': 7.77}


 86%|████████▋ | 20855/24156 [22:33:27<13:25:16, 14.64s/it]

 86%|████████▋ | 20856/24156 [22:33:43<13:59:08, 15.26s/it]

 86%|████████▋ | 20857/24156 [22:34:03<15:11:13, 16.57s/it]
{'loss': 0.4717, 'learning_rate': 1.8504205179837855e-06, 'rewards/chosen': -3.0085437297821045, 'rewards/rejected': -4.543115615844727, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5345715284347534, 'policy_logps/rejected': -438.8598327636719, 'policy_logps/chosen': -374.86749267578125, 'referece_logps/rejected': -393.4286804199219, 'referece_logps/chosen': -344.78204345703125, 'logits/rejected': -0.3006289601325989, 'logits/chosen': -0.22636425495147705, 'epoch': 7.77}


 86%|████████▋ | 20859/24156 [22:34:31<13:55:03, 15.20s/it]
{'loss': 0.4196, 'learning_rate': 1.8502794090909663e-06, 'rewards/chosen': -2.445988893508911, 'rewards/rejected': -4.281196117401123, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8352069854736328, 'policy_logps/rejected': -451.8359680175781, 'policy_logps/chosen': -465.66217041015625, 'referece_logps/rejected': -409.0239562988281, 'referece_logps/chosen': -441.2022705078125, 'logits/rejected': -0.5669974088668823, 'logits/chosen': -0.6104795932769775, 'epoch': 7.77}


 86%|████████▋ | 20861/24156 [22:35:11<16:16:18, 17.78s/it]

 86%|████████▋ | 20862/24156 [22:35:31<16:48:32, 18.37s/it]
[2024-04-06 13:40:28,437] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 20863/24156 [22:35:51<17:20:03, 18.95s/it]
{'loss': 0.2479, 'learning_rate': 1.8499970078894336e-06, 'rewards/chosen': -2.277818202972412, 'rewards/rejected': -5.055217742919922, 'rewards/accuracies': 0.75, 'rewards/margins': 2.777400016784668, 'policy_logps/rejected': -257.3367614746094, 'policy_logps/chosen': -354.97113037109375, 'referece_logps/rejected': -206.78456115722656, 'referece_logps/chosen': -332.19293212890625, 'logits/rejected': -0.4659590721130371, 'logits/chosen': -0.5613657832145691, 'epoch': 7.77}


 86%|████████▋ | 20865/24156 [22:36:22<15:45:59, 17.25s/it]
{'loss': 0.2775, 'learning_rate': 1.8498557156010275e-06, 'rewards/chosen': -1.2989857196807861, 'rewards/rejected': -3.5772805213928223, 'rewards/accuracies': 0.875, 'rewards/margins': 2.278294801712036, 'policy_logps/rejected': -333.6578369140625, 'policy_logps/chosen': -229.06784057617188, 'referece_logps/rejected': -297.88507080078125, 'referece_logps/chosen': -216.07797241210938, 'logits/rejected': -0.7918421626091003, 'logits/chosen': -0.838201642036438, 'epoch': 7.77}

 86%|████████▋ | 20866/24156 [22:36:36<15:04:13, 16.49s/it]

 86%|████████▋ | 20867/24156 [22:36:50<14:19:22, 15.68s/it]

 86%|████████▋ | 20868/24156 [22:37:02<13:16:18, 14.53s/it]


 86%|████████▋ | 20870/24156 [22:37:42<15:43:43, 17.23s/it]

 86%|████████▋ | 20871/24156 [22:38:01<16:22:22, 17.94s/it]
{'loss': 0.3901, 'learning_rate': 1.8494314721071817e-06, 'rewards/chosen': -2.1801397800445557, 'rewards/rejected': -3.8567583560943604, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6766186952590942, 'policy_logps/rejected': -432.5995178222656, 'policy_logps/chosen': -458.6075744628906, 'referece_logps/rejected': -394.0319519042969, 'referece_logps/chosen': -436.80615234375, 'logits/rejected': 0.36691465973854065, 'logits/chosen': 0.3632466197013855, 'epoch': 7.78}

 86%|████████▋ | 20872/24156 [22:38:18<16:07:57, 17.69s/it]


 86%|████████▋ | 20874/24156 [22:38:48<14:36:19, 16.02s/it]

 86%|████████▋ | 20875/24156 [22:38:59<13:22:14, 14.67s/it]

 86%|████████▋ | 20876/24156 [22:39:15<13:41:32, 15.03s/it]

 86%|████████▋ | 20877/24156 [22:39:38<15:46:40, 17.32s/it]

 86%|████████▋ | 20878/24156 [22:39:57<16:18:41, 17.91s/it]

 86%|████████▋ | 20879/24156 [22:40:10<14:57:04, 16.42s/it]

 86%|████████▋ | 20880/24156 [22:40:23<14:08:05, 15.53s/it]

 86%|████████▋ | 20881/24156 [22:40:40<14:21:36, 15.79s/it]

 86%|████████▋ | 20882/24156 [22:40:53<13:37:00, 14.97s/it]
{'loss': 0.3998, 'learning_rate': 1.848652264797514e-06, 'rewards/chosen': -2.818789482116699, 'rewards/rejected': -3.8192458152770996, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0004563331604004, 'policy_logps/rejected': -364.17803955078125, 'policy_logps/chosen': -431.8778991699219, 'referece_logps/rejected': -325.9855651855469, 'referece_logps/chosen': -403.6900329589844, 'logits/rejected': -0.24044112861156464, 'logits/chosen': 0.0036360174417495728, 'epoch': 7.78}


 86%|████████▋ | 20884/24156 [22:41:15<11:50:37, 13.03s/it]

 86%|████████▋ | 20885/24156 [22:41:31<12:40:43, 13.95s/it]

 86%|████████▋ | 20886/24156 [22:41:43<12:01:21, 13.24s/it]

 86%|████████▋ | 20887/24156 [22:42:00<13:04:40, 14.40s/it]

 86%|████████▋ | 20888/24156 [22:42:15<13:13:43, 14.57s/it]

 86%|████████▋ | 20889/24156 [22:42:32<13:47:44, 15.20s/it]

 86%|████████▋ | 20890/24156 [22:42:51<14:59:22, 16.52s/it]

 86%|████████▋ | 20891/24156 [22:43:05<14:13:20, 15.68s/it]

 86%|████████▋ | 20892/24156 [22:43:25<15:25:24, 17.01s/it]
{'loss': 0.3124, 'learning_rate': 1.847942292565114e-06, 'rewards/chosen': -1.8876994848251343, 'rewards/rejected': -3.5527374744415283, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6650378704071045, 'policy_logps/rejected': -391.01715087890625, 'policy_logps/chosen': -250.5165252685547, 'referece_logps/rejected': -355.48974609375, 'referece_logps/chosen': -231.63951110839844, 'logits/rejected': -0.7244188189506531, 'logits/chosen': -0.6237607598304749, 'epoch': 7.78}


 86%|████████▋ | 20894/24156 [22:44:08<17:23:50, 19.20s/it]

 87%|████████▋ | 20895/24156 [22:44:27<17:31:30, 19.35s/it]

 87%|████████▋ | 20896/24156 [22:44:49<18:05:44, 19.98s/it]
{'loss': 0.3094, 'learning_rate': 1.847657876782948e-06, 'rewards/chosen': -2.158621072769165, 'rewards/rejected': -5.939440727233887, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7808196544647217, 'policy_logps/rejected': -415.64495849609375, 'policy_logps/chosen': -412.8974304199219, 'referece_logps/rejected': -356.25054931640625, 'referece_logps/chosen': -391.3112487792969, 'logits/rejected': -0.37169551849365234, 'logits/chosen': -0.3297821879386902, 'epoch': 7.79}


 87%|████████▋ | 20898/24156 [22:45:18<15:14:04, 16.83s/it]
{'loss': 0.3921, 'learning_rate': 1.847515577456492e-06, 'rewards/chosen': -1.935168981552124, 'rewards/rejected': -3.894841432571411, 'rewards/accuracies': 0.875, 'rewards/margins': 1.959672451019287, 'policy_logps/rejected': -522.6613159179688, 'policy_logps/chosen': -442.7958679199219, 'referece_logps/rejected': -483.712890625, 'referece_logps/chosen': -423.4442138671875, 'logits/rejected': 0.47575947642326355, 'logits/chosen': 0.5428977012634277, 'epoch': 7.79}

 87%|████████▋ | 20899/24156 [22:45:35<15:15:09, 16.86s/it]


 87%|████████▋ | 20901/24156 [22:46:16<17:02:39, 18.85s/it]
{'loss': 0.4604, 'learning_rate': 1.8473020142013704e-06, 'rewards/chosen': -1.553755521774292, 'rewards/rejected': -2.576984167098999, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0232288837432861, 'policy_logps/rejected': -255.35568237304688, 'policy_logps/chosen': -289.18890380859375, 'referece_logps/rejected': -229.58584594726562, 'referece_logps/chosen': -273.6513671875, 'logits/rejected': -0.8815284967422485, 'logits/chosen': -0.9802051186561584, 'epoch': 7.79}


 87%|████████▋ | 20903/24156 [22:46:50<16:12:40, 17.94s/it]

 87%|████████▋ | 20904/24156 [22:47:08<16:14:36, 17.98s/it]
{'loss': 0.3309, 'learning_rate': 1.8470883138584384e-06, 'rewards/chosen': -1.7575173377990723, 'rewards/rejected': -4.2660298347473145, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5085127353668213, 'policy_logps/rejected': -349.126220703125, 'policy_logps/chosen': -319.845458984375, 'referece_logps/rejected': -306.4659118652344, 'referece_logps/chosen': -302.270263671875, 'logits/rejected': -0.9721744060516357, 'logits/chosen': -0.9249676465988159, 'epoch': 7.79}

 87%|████████▋ | 20905/24156 [22:47:25<15:55:30, 17.63s/it]

 87%|████████▋ | 20906/24156 [22:47:45<16:32:44, 18.33s/it]

 87%|████████▋ | 20907/24156 [22:48:05<17:06:55, 18.96s/it]

 87%|████████▋ | 20908/24156 [22:48:23<16:41:46, 18.51s/it]


 87%|████████▋ | 20910/24156 [22:49:02<17:14:05, 19.11s/it]
{'loss': 0.319, 'learning_rate': 1.8466605020474664e-06, 'rewards/chosen': -1.7808938026428223, 'rewards/rejected': -3.7842788696289062, 'rewards/accuracies': 0.75, 'rewards/margins': 2.003385066986084, 'policy_logps/rejected': -328.4043273925781, 'policy_logps/chosen': -327.093017578125, 'referece_logps/rejected': -290.5615234375, 'referece_logps/chosen': -309.28411865234375, 'logits/rejected': -0.3109363317489624, 'logits/chosen': -0.3028741776943207, 'epoch': 7.79}


 87%|████████▋ | 20912/24156 [22:49:36<15:55:32, 17.67s/it]

 87%|████████▋ | 20913/24156 [22:49:50<15:06:50, 16.78s/it]
{'loss': 0.4177, 'learning_rate': 1.8464463906486436e-06, 'rewards/chosen': -1.435004472732544, 'rewards/rejected': -3.3468353748321533, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9118306636810303, 'policy_logps/rejected': -506.1860656738281, 'policy_logps/chosen': -442.7466735839844, 'referece_logps/rejected': -472.7178039550781, 'referece_logps/chosen': -428.3966064453125, 'logits/rejected': -1.304956078529358, 'logits/chosen': -1.4591569900512695, 'epoch': 7.79}

 87%|████████▋ | 20914/24156 [22:50:13<16:35:16, 18.42s/it]


 87%|████████▋ | 20916/24156 [22:50:42<14:40:14, 16.30s/it]
{'loss': 0.4925, 'learning_rate': 1.8462321423004448e-06, 'rewards/chosen': -2.324169158935547, 'rewards/rejected': -3.1531195640563965, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8289503455162048, 'policy_logps/rejected': -430.53240966796875, 'policy_logps/chosen': -383.80377197265625, 'referece_logps/rejected': -399.00128173828125, 'referece_logps/chosen': -360.56207275390625, 'logits/rejected': -0.4563237130641937, 'logits/chosen': -0.4815395474433899, 'epoch': 7.79}


 87%|████████▋ | 20918/24156 [22:51:16<15:16:05, 16.98s/it]

 87%|████████▋ | 20919/24156 [22:51:33<15:06:39, 16.81s/it]
{'loss': 0.3761, 'learning_rate': 1.8460177570375334e-06, 'rewards/chosen': -2.2698705196380615, 'rewards/rejected': -2.899789810180664, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6299192309379578, 'policy_logps/rejected': -452.0576477050781, 'policy_logps/chosen': -301.31024169921875, 'referece_logps/rejected': -423.0596923828125, 'referece_logps/chosen': -278.6115417480469, 'logits/rejected': -0.660706639289856, 'logits/chosen': -0.6464093923568726, 'epoch': 7.79}

 87%|████████▋ | 20920/24156 [22:51:43<13:28:38, 14.99s/it]

 87%|████████▋ | 20921/24156 [22:52:03<14:45:16, 16.42s/it]

 87%|████████▋ | 20922/24156 [22:52:23<15:36:47, 17.38s/it]

 87%|████████▋ | 20923/24156 [22:52:34<13:51:30, 15.43s/it]
[2024-04-06 13:57:54,728] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 20924/24156 [22:52:57<16:03:03, 17.88s/it]

 87%|████████▋ | 20925/24156 [22:53:13<15:31:19, 17.29s/it]

 87%|████████▋ | 20926/24156 [22:53:35<16:42:56, 18.63s/it]


 87%|████████▋ | 20928/24156 [22:54:07<15:38:41, 17.45s/it]
[2024-04-06 13:59:03,988] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 20929/24156 [22:54:18<14:08:59, 15.79s/it]
{'loss': 0.4901, 'learning_rate': 1.845302151111537e-06, 'rewards/chosen': -1.9774335622787476, 'rewards/rejected': -2.5652873516082764, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5878539085388184, 'policy_logps/rejected': -263.7575988769531, 'policy_logps/chosen': -320.16326904296875, 'referece_logps/rejected': -238.10472106933594, 'referece_logps/chosen': -300.388916015625, 'logits/rejected': 0.0786975771188736, 'logits/chosen': 0.0919170081615448, 'epoch': 7.8}


 87%|████████▋ | 20931/24156 [22:54:42<12:22:31, 13.81s/it]
{'loss': 0.4208, 'learning_rate': 1.8451588475328208e-06, 'rewards/chosen': -1.795242428779602, 'rewards/rejected': -3.4638876914978027, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6686452627182007, 'policy_logps/rejected': -342.59210205078125, 'policy_logps/chosen': -298.67779541015625, 'referece_logps/rejected': -307.9532470703125, 'referece_logps/chosen': -280.7253723144531, 'logits/rejected': -0.44452965259552, 'logits/chosen': -0.4528292119503021, 'epoch': 7.8}


 87%|████████▋ | 20933/24156 [22:55:18<14:17:09, 15.96s/it]

 87%|████████▋ | 20934/24156 [22:55:34<14:18:31, 15.99s/it]
{'loss': 0.3037, 'learning_rate': 1.8449437782170828e-06, 'rewards/chosen': -0.8623372912406921, 'rewards/rejected': -3.63314151763916, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7708044052124023, 'policy_logps/rejected': -346.5069580078125, 'policy_logps/chosen': -469.9606628417969, 'referece_logps/rejected': -310.175537109375, 'referece_logps/chosen': -461.3373107910156, 'logits/rejected': 0.19970932602882385, 'logits/chosen': 0.14272843301296234, 'epoch': 7.8}

 87%|████████▋ | 20935/24156 [22:55:54<15:14:15, 17.03s/it]


 87%|████████▋ | 20937/24156 [22:56:23<14:08:22, 15.81s/it]
{'loss': 0.3087, 'learning_rate': 1.8447285721950814e-06, 'rewards/chosen': -1.8755476474761963, 'rewards/rejected': -2.6272854804992676, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7517374753952026, 'policy_logps/rejected': -398.6366882324219, 'policy_logps/chosen': -479.9952392578125, 'referece_logps/rejected': -372.36383056640625, 'referece_logps/chosen': -461.2397155761719, 'logits/rejected': -0.45582711696624756, 'logits/chosen': -0.4045752286911011, 'epoch': 7.8}


 87%|████████▋ | 20939/24156 [22:56:49<12:58:46, 14.52s/it]

 87%|████████▋ | 20940/24156 [22:57:01<12:22:42, 13.86s/it]

 87%|████████▋ | 20941/24156 [22:57:14<12:15:52, 13.73s/it]
{'loss': 0.2791, 'learning_rate': 1.8444414182384088e-06, 'rewards/chosen': -1.5115596055984497, 'rewards/rejected': -3.0608303546905518, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5492709875106812, 'policy_logps/rejected': -383.7716979980469, 'policy_logps/chosen': -337.1598205566406, 'referece_logps/rejected': -353.16339111328125, 'referece_logps/chosen': -322.0442199707031, 'logits/rejected': 0.1938394010066986, 'logits/chosen': 0.26792100071907043, 'epoch': 7.8}


 87%|████████▋ | 20943/24156 [22:57:47<13:26:52, 15.07s/it]

 87%|████████▋ | 20944/24156 [22:58:07<14:49:48, 16.62s/it]

 87%|████████▋ | 20945/24156 [22:58:21<14:02:27, 15.74s/it]
{'loss': 0.3026, 'learning_rate': 1.8441540213928768e-06, 'rewards/chosen': -1.6407454013824463, 'rewards/rejected': -3.8945634365081787, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2538177967071533, 'policy_logps/rejected': -446.17279052734375, 'policy_logps/chosen': -381.4884948730469, 'referece_logps/rejected': -407.2271728515625, 'referece_logps/chosen': -365.0810546875, 'logits/rejected': -0.22046460211277008, 'logits/chosen': -0.335889607667923, 'epoch': 7.8}


 87%|████████▋ | 20947/24156 [22:58:57<14:53:16, 16.70s/it]
{'loss': 0.3347, 'learning_rate': 1.8440102319126185e-06, 'rewards/chosen': -1.42888605594635, 'rewards/rejected': -5.669778347015381, 'rewards/accuracies': 1.0, 'rewards/margins': 4.240891933441162, 'policy_logps/rejected': -447.8426513671875, 'policy_logps/chosen': -307.3304443359375, 'referece_logps/rejected': -391.1448974609375, 'referece_logps/chosen': -293.0415954589844, 'logits/rejected': -1.221645474433899, 'logits/chosen': -1.0499082803726196, 'epoch': 7.8}

 87%|████████▋ | 20948/24156 [22:59:16<15:22:56, 17.26s/it]


 87%|████████▋ | 20950/24156 [22:59:51<15:47:12, 17.73s/it]
[2024-04-06 14:04:48,440] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3599, 'learning_rate': 1.843794433899444e-06, 'rewards/chosen': -2.1826231479644775, 'rewards/rejected': -3.6453890800476074, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4627656936645508, 'policy_logps/rejected': -452.912841796875, 'policy_logps/chosen': -360.2432556152344, 'referece_logps/rejected': -416.4589538574219, 'referece_logps/chosen': -338.4170227050781, 'logits/rejected': -0.2594892978668213, 'logits/chosen': -0.1537187099456787, 'epoch': 7.81}

 87%|████████▋ | 20951/24156 [23:00:06<14:57:44, 16.81s/it]


 87%|████████▋ | 20953/24156 [23:00:34<13:33:53, 15.25s/it]

 87%|████████▋ | 20954/24156 [23:00:53<14:28:43, 16.28s/it]
{'loss': 0.4263, 'learning_rate': 1.8435064908563266e-06, 'rewards/chosen': -2.184211015701294, 'rewards/rejected': -2.84000301361084, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6557920575141907, 'policy_logps/rejected': -359.6664123535156, 'policy_logps/chosen': -453.9612731933594, 'referece_logps/rejected': -331.26641845703125, 'referece_logps/chosen': -432.1191711425781, 'logits/rejected': -0.3933013677597046, 'logits/chosen': -0.4858132600784302, 'epoch': 7.81}

 87%|████████▋ | 20955/24156 [23:01:10<14:36:15, 16.42s/it]

 87%|████████▋ | 20956/24156 [23:01:24<14:05:52, 15.86s/it]


 87%|████████▋ | 20958/24156 [23:01:53<13:27:34, 15.15s/it]

 87%|████████▋ | 20959/24156 [23:02:07<13:19:25, 15.00s/it]

 87%|████████▋ | 20960/24156 [23:02:19<12:25:14, 13.99s/it]

 87%|████████▋ | 20961/24156 [23:02:37<13:28:20, 15.18s/it]

 87%|████████▋ | 20962/24156 [23:02:55<14:11:20, 15.99s/it]

 87%|████████▋ | 20963/24156 [23:03:15<15:17:45, 17.25s/it]

 87%|████████▋ | 20964/24156 [23:03:33<15:28:25, 17.45s/it]

 87%|████████▋ | 20965/24156 [23:03:49<15:11:06, 17.13s/it]
{'loss': 0.4364, 'learning_rate': 1.8427133967280289e-06, 'rewards/chosen': -2.4054553508758545, 'rewards/rejected': -3.910033702850342, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5045785903930664, 'policy_logps/rejected': -454.10546875, 'policy_logps/chosen': -321.083251953125, 'referece_logps/rejected': -415.005126953125, 'referece_logps/chosen': -297.0286560058594, 'logits/rejected': -0.9075865149497986, 'logits/chosen': -0.6356783509254456, 'epoch': 7.81}

 87%|████████▋ | 20966/24156 [23:04:08<15:30:38, 17.50s/it]


 87%|████████▋ | 20968/24156 [23:04:48<16:45:18, 18.92s/it]

 87%|████████▋ | 20969/24156 [23:05:07<16:51:48, 19.05s/it]
{'loss': 0.3326, 'learning_rate': 1.8424245442877405e-06, 'rewards/chosen': -2.6224217414855957, 'rewards/rejected': -3.307837724685669, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6854161024093628, 'policy_logps/rejected': -397.2997131347656, 'policy_logps/chosen': -461.3427734375, 'referece_logps/rejected': -364.2213134765625, 'referece_logps/chosen': -435.1185302734375, 'logits/rejected': 0.05434670299291611, 'logits/chosen': 0.03818110376596451, 'epoch': 7.81}

 87%|████████▋ | 20970/24156 [23:05:27<17:02:35, 19.26s/it]

 87%|████████▋ | 20971/24156 [23:05:44<16:33:08, 18.71s/it]

 87%|████████▋ | 20972/24156 [23:06:04<16:50:23, 19.04s/it]

 87%|████████▋ | 20973/24156 [23:06:25<17:17:10, 19.55s/it]

 87%|████████▋ | 20974/24156 [23:06:39<15:49:56, 17.91s/it]


 87%|████████▋ | 20976/24156 [23:07:15<15:53:50, 18.00s/it]

 87%|████████▋ | 20977/24156 [23:07:34<15:57:59, 18.08s/it]

 87%|████████▋ | 20978/24156 [23:07:53<16:22:27, 18.55s/it]
{'loss': 0.3333, 'learning_rate': 1.8417737404824424e-06, 'rewards/chosen': -2.5431857109069824, 'rewards/rejected': -4.346067428588867, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8028817176818848, 'policy_logps/rejected': -302.3057861328125, 'policy_logps/chosen': -284.4358215332031, 'referece_logps/rejected': -258.8450927734375, 'referece_logps/chosen': -259.00396728515625, 'logits/rejected': -0.6041673421859741, 'logits/chosen': -0.5451099276542664, 'epoch': 7.82}

 87%|████████▋ | 20979/24156 [23:08:13<16:37:34, 18.84s/it]

 87%|████████▋ | 20980/24156 [23:08:34<17:14:43, 19.55s/it]

 87%|████████▋ | 20981/24156 [23:08:46<15:20:08, 17.39s/it]

 87%|████████▋ | 20982/24156 [23:09:08<16:27:45, 18.67s/it]

 87%|████████▋ | 20983/24156 [23:09:20<14:47:44, 16.79s/it]

 87%|████████▋ | 20984/24156 [23:09:40<15:31:48, 17.63s/it]

 87%|████████▋ | 20985/24156 [23:10:01<16:18:07, 18.51s/it]

 87%|████████▋ | 20986/24156 [23:10:14<14:59:50, 17.03s/it]

 87%|████████▋ | 20987/24156 [23:10:30<14:46:53, 16.79s/it]

 87%|████████▋ | 20988/24156 [23:10:50<15:34:29, 17.70s/it]

 87%|████████▋ | 20989/24156 [23:11:07<15:15:35, 17.35s/it]


 87%|████████▋ | 20991/24156 [23:11:44<15:39:08, 17.80s/it]

 87%|████████▋ | 20992/24156 [23:12:02<15:38:57, 17.81s/it]
{'loss': 0.3842, 'learning_rate': 1.8407589430090904e-06, 'rewards/chosen': -2.093141794204712, 'rewards/rejected': -3.524289608001709, 'rewards/accuracies': 0.75, 'rewards/margins': 1.431147575378418, 'policy_logps/rejected': -465.71759033203125, 'policy_logps/chosen': -570.8012084960938, 'referece_logps/rejected': -430.4747009277344, 'referece_logps/chosen': -549.8698120117188, 'logits/rejected': -0.6430273652076721, 'logits/chosen': -0.7103280425071716, 'epoch': 7.82}

 87%|████████▋ | 20993/24156 [23:12:12<13:45:18, 15.66s/it]

 87%|████████▋ | 20994/24156 [23:12:24<12:46:00, 14.54s/it]


 87%|████████▋ | 20996/24156 [23:13:02<14:52:31, 16.95s/it]

 87%|████████▋ | 20997/24156 [23:13:22<15:35:36, 17.77s/it]
{'loss': 0.4605, 'learning_rate': 1.840395797222419e-06, 'rewards/chosen': -1.6011462211608887, 'rewards/rejected': -3.9188826084136963, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3177361488342285, 'policy_logps/rejected': -493.33880615234375, 'policy_logps/chosen': -561.4969482421875, 'referece_logps/rejected': -454.14996337890625, 'referece_logps/chosen': -545.4854736328125, 'logits/rejected': -0.8383291363716125, 'logits/chosen': -0.7965508699417114, 'epoch': 7.82}


 87%|████████▋ | 20999/24156 [23:14:00<16:05:10, 18.34s/it]

 87%|████████▋ | 21000/24156 [23:14:15<15:26:01, 17.60s/it]

 87%|████████▋ | 21001/24156 [23:14:53<20:48:06, 23.74s/it]
{'loss': 0.3522, 'learning_rate': 1.8401050086443879e-06, 'rewards/chosen': -1.9186794757843018, 'rewards/rejected': -3.524132013320923, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6054527759552002, 'policy_logps/rejected': -320.5437927246094, 'policy_logps/chosen': -390.43792724609375, 'referece_logps/rejected': -285.30242919921875, 'referece_logps/chosen': -371.25115966796875, 'logits/rejected': -0.722166895866394, 'logits/chosen': -0.6019484996795654, 'epoch': 7.82}

 87%|████████▋ | 21002/24156 [23:15:16<20:22:27, 23.26s/it]

 87%|████████▋ | 21003/24156 [23:15:35<19:25:49, 22.18s/it]

 87%|████████▋ | 21004/24156 [23:15:53<18:19:53, 20.94s/it]

 87%|████████▋ | 21005/24156 [23:16:13<18:02:27, 20.61s/it]


 87%|████████▋ | 21007/24156 [23:16:52<17:31:37, 20.04s/it]
{'loss': 0.361, 'learning_rate': 1.8396683727255593e-06, 'rewards/chosen': -2.3929619789123535, 'rewards/rejected': -4.01248025894165, 'rewards/accuracies': 0.75, 'rewards/margins': 1.619518518447876, 'policy_logps/rejected': -362.7090759277344, 'policy_logps/chosen': -386.0098571777344, 'referece_logps/rejected': -322.58428955078125, 'referece_logps/chosen': -362.0802307128906, 'logits/rejected': -0.5193281173706055, 'logits/chosen': -0.6546054482460022, 'epoch': 7.83}

 87%|████████▋ | 21008/24156 [23:17:10<16:53:23, 19.31s/it]

 87%|████████▋ | 21009/24156 [23:17:27<16:21:14, 18.71s/it]

 87%|████████▋ | 21010/24156 [23:17:44<15:45:52, 18.04s/it]

 87%|████████▋ | 21011/24156 [23:17:57<14:32:09, 16.64s/it]

 87%|████████▋ | 21012/24156 [23:18:17<15:21:43, 17.59s/it]

 87%|████████▋ | 21013/24156 [23:18:32<14:51:18, 17.02s/it]

 87%|████████▋ | 21014/24156 [23:18:44<13:17:42, 15.23s/it]


 87%|████████▋ | 21016/24156 [23:19:20<14:20:27, 16.44s/it]
{'loss': 0.407, 'learning_rate': 1.839012400040224e-06, 'rewards/chosen': -2.0885379314422607, 'rewards/rejected': -3.767969846725464, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6794317960739136, 'policy_logps/rejected': -404.909912109375, 'policy_logps/chosen': -323.6460266113281, 'referece_logps/rejected': -367.230224609375, 'referece_logps/chosen': -302.7606201171875, 'logits/rejected': -0.4063154458999634, 'logits/chosen': -0.22231827676296234, 'epoch': 7.83}

 87%|████████▋ | 21017/24156 [23:19:40<15:03:08, 17.26s/it]

 87%|████████▋ | 21018/24156 [23:20:00<15:47:22, 18.11s/it]

 87%|████████▋ | 21019/24156 [23:20:19<16:02:33, 18.41s/it]

 87%|████████▋ | 21020/24156 [23:20:33<15:04:17, 17.30s/it]

 87%|████████▋ | 21021/24156 [23:20:46<13:44:06, 15.77s/it]

 87%|████████▋ | 21022/24156 [23:20:58<12:43:44, 14.62s/it]

 87%|████████▋ | 21023/24156 [23:21:11<12:25:48, 14.28s/it]


 87%|████████▋ | 21025/24156 [23:21:39<12:19:00, 14.16s/it]

 87%|████████▋ | 21026/24156 [23:21:52<12:13:10, 14.05s/it]
{'loss': 0.3531, 'learning_rate': 1.8382821086618266e-06, 'rewards/chosen': -2.157565116882324, 'rewards/rejected': -4.093801498413086, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9362366199493408, 'policy_logps/rejected': -384.9410705566406, 'policy_logps/chosen': -369.06378173828125, 'referece_logps/rejected': -344.0030517578125, 'referece_logps/chosen': -347.4881591796875, 'logits/rejected': -0.6509337425231934, 'logits/chosen': -0.6173824667930603, 'epoch': 7.83}

 87%|████████▋ | 21027/24156 [23:22:12<13:44:16, 15.81s/it]


 87%|████████▋ | 21029/24156 [23:22:48<14:48:04, 17.04s/it]
{'loss': 0.4693, 'learning_rate': 1.8380627273269007e-06, 'rewards/chosen': -2.3538756370544434, 'rewards/rejected': -3.1003193855285645, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7464438676834106, 'policy_logps/rejected': -362.9334716796875, 'policy_logps/chosen': -401.2417907714844, 'referece_logps/rejected': -331.9302978515625, 'referece_logps/chosen': -377.7030334472656, 'logits/rejected': 0.11991355568170547, 'logits/chosen': 0.12504631280899048, 'epoch': 7.83}

 87%|████████▋ | 21030/24156 [23:23:05<14:47:03, 17.03s/it]

 87%|████████▋ | 21031/24156 [23:23:20<14:10:34, 16.33s/it]


 87%|████████▋ | 21033/24156 [23:23:51<13:50:24, 15.95s/it]
{'loss': 0.3376, 'learning_rate': 1.8377700079640956e-06, 'rewards/chosen': -2.371351480484009, 'rewards/rejected': -3.5980641841888428, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2267128229141235, 'policy_logps/rejected': -421.98736572265625, 'policy_logps/chosen': -367.38226318359375, 'referece_logps/rejected': -386.0067138671875, 'referece_logps/chosen': -343.6687316894531, 'logits/rejected': -0.48636746406555176, 'logits/chosen': -0.5218247771263123, 'epoch': 7.84}

 87%|████████▋ | 21034/24156 [23:24:05<13:33:09, 15.63s/it]

 87%|████████▋ | 21035/24156 [23:24:24<14:24:08, 16.61s/it]

 87%|████████▋ | 21036/24156 [23:24:41<14:22:02, 16.58s/it]

 87%|████████▋ | 21037/24156 [23:24:54<13:22:02, 15.43s/it]

 87%|████████▋ | 21038/24156 [23:25:06<12:37:59, 14.59s/it]

 87%|████████▋ | 21039/24156 [23:25:20<12:21:34, 14.27s/it]

 87%|████████▋ | 21040/24156 [23:25:39<13:42:56, 15.85s/it]

 87%|████████▋ | 21041/24156 [23:25:50<12:23:38, 14.32s/it]

 87%|████████▋ | 21042/24156 [23:26:02<11:39:43, 13.48s/it]

 87%|████████▋ | 21043/24156 [23:26:20<12:50:36, 14.85s/it]
[2024-04-06 14:31:39,044] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21044/24156 [23:26:42<14:40:17, 16.97s/it]

 87%|████████▋ | 21045/24156 [23:26:57<14:09:38, 16.39s/it]


 87%|████████▋ | 21047/24156 [23:27:27<13:54:25, 16.10s/it]

 87%|████████▋ | 21048/24156 [23:27:45<14:25:00, 16.70s/it]
{'loss': 0.3088, 'learning_rate': 1.8366701649031975e-06, 'rewards/chosen': -1.9947988986968994, 'rewards/rejected': -3.4946279525756836, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4998290538787842, 'policy_logps/rejected': -225.0550994873047, 'policy_logps/chosen': -291.0235900878906, 'referece_logps/rejected': -190.10882568359375, 'referece_logps/chosen': -271.0755920410156, 'logits/rejected': -0.5169505476951599, 'logits/chosen': -0.5781376957893372, 'epoch': 7.84}

 87%|████████▋ | 21049/24156 [23:28:02<14:32:00, 16.84s/it]

 87%|████████▋ | 21050/24156 [23:28:18<14:14:47, 16.51s/it]

 87%|████████▋ | 21051/24156 [23:28:35<14:17:42, 16.57s/it]

 87%|████████▋ | 21052/24156 [23:28:52<14:26:12, 16.74s/it]

 87%|████████▋ | 21053/24156 [23:29:11<15:03:48, 17.48s/it]

 87%|████████▋ | 21054/24156 [23:29:28<15:02:09, 17.45s/it]

 87%|████████▋ | 21055/24156 [23:29:48<15:33:33, 18.06s/it]

 87%|████████▋ | 21056/24156 [23:30:09<16:21:21, 18.99s/it]

 87%|████████▋ | 21057/24156 [23:30:25<15:34:02, 18.08s/it]

 87%|████████▋ | 21058/24156 [23:30:44<15:48:20, 18.37s/it]

 87%|████████▋ | 21059/24156 [23:31:06<16:41:23, 19.40s/it]

 87%|████████▋ | 21060/24156 [23:31:20<15:19:28, 17.82s/it]


 87%|████████▋ | 21062/24156 [23:32:01<16:41:21, 19.42s/it]
{'loss': 0.286, 'learning_rate': 1.835640591332112e-06, 'rewards/chosen': -3.593716621398926, 'rewards/rejected': -6.517569065093994, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9238522052764893, 'policy_logps/rejected': -446.5032653808594, 'policy_logps/chosen': -413.4538879394531, 'referece_logps/rejected': -381.32757568359375, 'referece_logps/chosen': -377.5167541503906, 'logits/rejected': 0.0248488187789917, 'logits/chosen': -0.07202188670635223, 'epoch': 7.85}

 87%|████████▋ | 21063/24156 [23:32:13<14:35:11, 16.98s/it]

 87%|████████▋ | 21064/24156 [23:32:32<15:15:33, 17.77s/it]

 87%|████████▋ | 21065/24156 [23:32:53<15:58:37, 18.61s/it]


 87%|████████▋ | 21067/24156 [23:33:33<16:42:14, 19.47s/it]
{'loss': 0.493, 'learning_rate': 1.8352721727355832e-06, 'rewards/chosen': -2.55269718170166, 'rewards/rejected': -3.3396060466766357, 'rewards/accuracies': 0.625, 'rewards/margins': 0.786908745765686, 'policy_logps/rejected': -330.81219482421875, 'policy_logps/chosen': -334.59515380859375, 'referece_logps/rejected': -297.4161682128906, 'referece_logps/chosen': -309.06817626953125, 'logits/rejected': -0.22902828454971313, 'logits/chosen': -0.1982295662164688, 'epoch': 7.85}


 87%|████████▋ | 21069/24156 [23:34:09<15:57:47, 18.62s/it]
{'loss': 0.2709, 'learning_rate': 1.8351247001776699e-06, 'rewards/chosen': -3.5247294902801514, 'rewards/rejected': -6.623869895935059, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0991408824920654, 'policy_logps/rejected': -405.99053955078125, 'policy_logps/chosen': -528.1953125, 'referece_logps/rejected': -339.7518615722656, 'referece_logps/chosen': -492.9480895996094, 'logits/rejected': -0.08409188687801361, 'logits/chosen': -0.16960757970809937, 'epoch': 7.85}

 87%|████████▋ | 21070/24156 [23:34:25<15:17:09, 17.83s/it]

 87%|████████▋ | 21071/24156 [23:34:46<16:11:01, 18.89s/it]

 87%|████████▋ | 21072/24156 [23:35:08<16:54:27, 19.74s/it]

 87%|████████▋ | 21073/24156 [23:35:28<16:52:14, 19.70s/it]

 87%|████████▋ | 21074/24156 [23:35:44<15:59:16, 18.67s/it]


 87%|████████▋ | 21076/24156 [23:36:17<15:24:28, 18.01s/it]
{'loss': 0.3132, 'learning_rate': 1.8346080733829933e-06, 'rewards/chosen': -2.7177364826202393, 'rewards/rejected': -3.9480183124542236, 'rewards/accuracies': 0.875, 'rewards/margins': 1.230281949043274, 'policy_logps/rejected': -282.0841064453125, 'policy_logps/chosen': -306.0603332519531, 'referece_logps/rejected': -242.60394287109375, 'referece_logps/chosen': -278.88299560546875, 'logits/rejected': -1.2086670398712158, 'logits/chosen': -1.2107329368591309, 'epoch': 7.85}

 87%|████████▋ | 21077/24156 [23:36:38<16:02:34, 18.76s/it]
[2024-04-06 14:41:56,418] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21078/24156 [23:36:59<16:36:07, 19.42s/it]
[2024-04-06 14:42:16,432] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21079/24156 [23:37:19<16:44:58, 19.60s/it]

 87%|████████▋ | 21080/24156 [23:37:30<14:28:06, 16.93s/it]

 87%|████████▋ | 21081/24156 [23:37:45<13:57:37, 16.34s/it]

 87%|████████▋ | 21082/24156 [23:38:03<14:28:29, 16.95s/it]


 87%|████████▋ | 21084/24156 [23:38:45<16:15:42, 19.06s/it]

 87%|████████▋ | 21085/24156 [23:39:05<16:26:12, 19.27s/it]

 87%|████████▋ | 21086/24156 [23:39:25<16:39:48, 19.54s/it]

 87%|████████▋ | 21087/24156 [23:39:45<16:52:22, 19.79s/it]

 87%|████████▋ | 21088/24156 [23:40:04<16:33:23, 19.43s/it]

 87%|████████▋ | 21089/24156 [23:40:21<15:57:40, 18.74s/it]

 87%|████████▋ | 21090/24156 [23:40:31<13:54:21, 16.33s/it]

 87%|████████▋ | 21091/24156 [23:40:48<13:52:57, 16.31s/it]

 87%|████████▋ | 21092/24156 [23:41:00<12:57:52, 15.23s/it]

 87%|████████▋ | 21093/24156 [23:41:11<11:50:08, 13.91s/it]

 87%|████████▋ | 21094/24156 [23:41:32<13:38:33, 16.04s/it]

 87%|████████▋ | 21095/24156 [23:41:52<14:35:29, 17.16s/it]

 87%|████████▋ | 21096/24156 [23:42:14<15:48:09, 18.59s/it]

 87%|████████▋ | 21097/24156 [23:42:30<15:12:54, 17.91s/it]

 87%|████████▋ | 21098/24156 [23:42:50<15:32:50, 18.30s/it]

 87%|████████▋ | 21099/24156 [23:43:01<13:50:45, 16.31s/it]

 87%|████████▋ | 21100/24156 [23:43:24<15:33:16, 18.32s/it]
[2024-04-06 14:48:21,702] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21101/24156 [23:43:44<15:52:59, 18.72s/it]

 87%|████████▋ | 21102/24156 [23:44:06<16:40:03, 19.65s/it]
[2024-04-06 14:49:03,156] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21103/24156 [23:44:18<14:52:32, 17.54s/it]
[2024-04-06 14:49:15,782] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21104/24156 [23:44:39<15:39:49, 18.48s/it]

 87%|████████▋ | 21105/24156 [23:44:58<15:48:15, 18.65s/it]

 87%|████████▋ | 21106/24156 [23:45:16<15:42:09, 18.53s/it]

 87%|████████▋ | 21107/24156 [23:45:35<15:49:28, 18.68s/it]
[2024-04-06 14:50:32,792] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21108/24156 [23:45:47<14:02:51, 16.59s/it]

 87%|████████▋ | 21109/24156 [23:45:59<12:49:54, 15.16s/it]

 87%|████████▋ | 21110/24156 [23:46:18<13:51:24, 16.38s/it]
[2024-04-06 14:51:15,538] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21111/24156 [23:46:37<14:37:45, 17.30s/it]

 87%|████████▋ | 21112/24156 [23:46:48<13:01:10, 15.40s/it]

 87%|████████▋ | 21113/24156 [23:47:00<12:05:12, 14.30s/it]
{'loss': 0.3541, 'learning_rate': 1.8318651295061325e-06, 'rewards/chosen': -2.1380369663238525, 'rewards/rejected': -3.926424503326416, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7883877754211426, 'policy_logps/rejected': -374.3713684082031, 'policy_logps/chosen': -373.43084716796875, 'referece_logps/rejected': -335.10711669921875, 'referece_logps/chosen': -352.0505065917969, 'logits/rejected': 0.46842333674430847, 'logits/chosen': 0.35781165957450867, 'epoch': 7.87}


 87%|████████▋ | 21115/24156 [23:47:33<12:41:26, 15.02s/it]

 87%|████████▋ | 21116/24156 [23:47:52<13:36:13, 16.11s/it]

 87%|████████▋ | 21117/24156 [23:48:06<13:12:55, 15.65s/it]

 87%|████████▋ | 21118/24156 [23:48:27<14:35:04, 17.28s/it]
[2024-04-06 14:53:24,785] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21119/24156 [23:48:41<13:47:26, 16.35s/it]
[2024-04-06 14:53:38,950] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21120/24156 [23:49:00<14:20:54, 17.01s/it]

 87%|████████▋ | 21121/24156 [23:49:19<14:52:52, 17.65s/it]

 87%|████████▋ | 21122/24156 [23:49:34<14:12:18, 16.86s/it]

 87%|████████▋ | 21123/24156 [23:49:57<15:37:55, 18.55s/it]
[2024-04-06 14:54:54,175] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 21124/24156 [23:50:16<15:54:46, 18.89s/it]

 87%|████████▋ | 21125/24156 [23:50:37<16:27:05, 19.54s/it]
{'loss': 0.3556, 'learning_rate': 1.8309711262022441e-06, 'rewards/chosen': -2.1971678733825684, 'rewards/rejected': -3.205552339553833, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0083845853805542, 'policy_logps/rejected': -422.51947021484375, 'policy_logps/chosen': -414.82745361328125, 'referece_logps/rejected': -390.46392822265625, 'referece_logps/chosen': -392.85577392578125, 'logits/rejected': -0.548035740852356, 'logits/chosen': -0.6731246709823608, 'epoch': 7.87}


 87%|████████▋ | 21127/24156 [23:51:03<13:28:36, 16.02s/it]
{'loss': 0.4121, 'learning_rate': 1.830821916451485e-06, 'rewards/chosen': -2.000253915786743, 'rewards/rejected': -3.7900235652923584, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7897697687149048, 'policy_logps/rejected': -330.7918395996094, 'policy_logps/chosen': -277.2100830078125, 'referece_logps/rejected': -292.8915710449219, 'referece_logps/chosen': -257.20751953125, 'logits/rejected': -0.6495693922042847, 'logits/chosen': -0.5931296944618225, 'epoch': 7.87}


 87%|████████▋ | 21129/24156 [23:51:41<14:52:10, 17.68s/it]

 87%|████████▋ | 21130/24156 [23:51:56<14:10:09, 16.86s/it]

 87%|████████▋ | 21131/24156 [23:52:09<13:17:16, 15.81s/it]

 87%|████████▋ | 21132/24156 [23:52:30<14:26:13, 17.19s/it]

 87%|████████▋ | 21133/24156 [23:52:41<13:00:14, 15.49s/it]

 87%|████████▋ | 21134/24156 [23:52:52<11:55:19, 14.20s/it]

 87%|████████▋ | 21135/24156 [23:53:09<12:33:57, 14.97s/it]

 87%|████████▋ | 21136/24156 [23:53:22<12:00:04, 14.31s/it]

 88%|████████▊ | 21137/24156 [23:53:37<12:18:20, 14.67s/it]

 88%|████████▊ | 21138/24156 [23:53:50<11:46:04, 14.04s/it]

 88%|████████▊ | 21139/24156 [23:54:11<13:37:12, 16.25s/it]

 88%|████████▊ | 21140/24156 [23:54:27<13:32:55, 16.17s/it]

 88%|████████▊ | 21141/24156 [23:54:46<14:08:50, 16.89s/it]

 88%|████████▊ | 21142/24156 [23:55:00<13:23:55, 16.00s/it]

 88%|████████▊ | 21143/24156 [23:55:11<12:04:13, 14.42s/it]
{'loss': 0.2958, 'learning_rate': 1.8296260886045185e-06, 'rewards/chosen': -1.8201158046722412, 'rewards/rejected': -5.24753475189209, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4274187088012695, 'policy_logps/rejected': -401.90814208984375, 'policy_logps/chosen': -434.0048522949219, 'referece_logps/rejected': -349.43280029296875, 'referece_logps/chosen': -415.8036804199219, 'logits/rejected': 0.37873509526252747, 'logits/chosen': 0.27063632011413574, 'epoch': 7.88}


 88%|████████▊ | 21145/24156 [23:55:44<13:17:23, 15.89s/it]

 88%|████████▊ | 21146/24156 [23:56:04<14:10:01, 16.94s/it]

 88%|████████▊ | 21147/24156 [23:56:22<14:27:03, 17.29s/it]

 88%|████████▊ | 21148/24156 [23:56:39<14:17:20, 17.10s/it]

 88%|████████▊ | 21149/24156 [23:56:50<12:48:54, 15.34s/it]

 88%|████████▊ | 21150/24156 [23:57:04<12:33:20, 15.04s/it]
{'loss': 0.2749, 'learning_rate': 1.8291017130022431e-06, 'rewards/chosen': -1.9753341674804688, 'rewards/rejected': -3.454256296157837, 'rewards/accuracies': 1.0, 'rewards/margins': 1.47892165184021, 'policy_logps/rejected': -358.085693359375, 'policy_logps/chosen': -331.1783447265625, 'referece_logps/rejected': -323.54315185546875, 'referece_logps/chosen': -311.42498779296875, 'logits/rejected': -0.09872296452522278, 'logits/chosen': -0.2062189131975174, 'epoch': 7.88}

 88%|████████▊ | 21151/24156 [23:57:19<12:30:50, 14.99s/it]


 88%|████████▊ | 21153/24156 [23:57:53<13:21:55, 16.02s/it]

 88%|████████▊ | 21154/24156 [23:58:13<14:22:23, 17.24s/it]

 88%|████████▊ | 21155/24156 [23:58:32<14:47:38, 17.75s/it]

 88%|████████▊ | 21156/24156 [23:58:52<15:32:57, 18.66s/it]
[2024-04-06 15:03:49,977] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21157/24156 [23:59:12<15:39:31, 18.80s/it]
[2024-04-06 15:04:09,096] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21158/24156 [23:59:35<16:53:54, 20.29s/it]

 88%|████████▊ | 21159/24156 [23:59:57<17:07:12, 20.56s/it]
{'loss': 0.4231, 'learning_rate': 1.8284264427191114e-06, 'rewards/chosen': -2.5733349323272705, 'rewards/rejected': -4.441159248352051, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8678240776062012, 'policy_logps/rejected': -457.58148193359375, 'policy_logps/chosen': -479.8113708496094, 'referece_logps/rejected': -413.1698913574219, 'referece_logps/chosen': -454.0780029296875, 'logits/rejected': -0.284287691116333, 'logits/chosen': -0.1547565460205078, 'epoch': 7.88}


 88%|████████▊ | 21161/24156 [24:00:24<14:27:09, 17.37s/it]

 88%|████████▊ | 21162/24156 [24:00:46<15:31:49, 18.67s/it]

 88%|████████▊ | 21163/24156 [24:00:56<13:22:51, 16.09s/it]

 88%|████████▊ | 21164/24156 [24:01:08<12:29:21, 15.03s/it]

 88%|████████▊ | 21165/24156 [24:01:27<13:19:12, 16.03s/it]
{'loss': 0.3655, 'learning_rate': 1.8279755923003154e-06, 'rewards/chosen': -2.5089948177337646, 'rewards/rejected': -4.355252265930176, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8462573289871216, 'policy_logps/rejected': -474.22088623046875, 'policy_logps/chosen': -459.38720703125, 'referece_logps/rejected': -430.66839599609375, 'referece_logps/chosen': -434.2972412109375, 'logits/rejected': -1.0357545614242554, 'logits/chosen': -1.0746610164642334, 'epoch': 7.89}


 88%|████████▊ | 21167/24156 [24:02:06<14:51:42, 17.90s/it]
[2024-04-06 15:07:03,926] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21168/24156 [24:02:20<13:47:01, 16.61s/it]

 88%|████████▊ | 21169/24156 [24:02:42<15:05:01, 18.18s/it]

 88%|████████▊ | 21170/24156 [24:03:01<15:26:02, 18.61s/it]

 88%|████████▊ | 21171/24156 [24:03:16<14:20:51, 17.30s/it]
{'loss': 0.4091, 'learning_rate': 1.8275242060378444e-06, 'rewards/chosen': -1.7149351835250854, 'rewards/rejected': -3.6234800815582275, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9085447788238525, 'policy_logps/rejected': -273.0383605957031, 'policy_logps/chosen': -391.6799621582031, 'referece_logps/rejected': -236.8035888671875, 'referece_logps/chosen': -374.5306091308594, 'logits/rejected': -0.06166946887969971, 'logits/chosen': 0.019883781671524048, 'epoch': 7.89}


 88%|████████▊ | 21173/24156 [24:03:53<14:45:08, 17.80s/it]

 88%|████████▊ | 21174/24156 [24:04:05<13:17:18, 16.04s/it]

 88%|████████▊ | 21175/24156 [24:04:23<13:55:13, 16.81s/it]

 88%|████████▊ | 21176/24156 [24:04:40<13:53:29, 16.78s/it]

 88%|████████▊ | 21177/24156 [24:04:57<13:56:13, 16.84s/it]
{'loss': 0.3311, 'learning_rate': 1.8270722842238224e-06, 'rewards/chosen': -2.1158721446990967, 'rewards/rejected': -4.164039611816406, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0481674671173096, 'policy_logps/rejected': -368.9703063964844, 'policy_logps/chosen': -403.6857604980469, 'referece_logps/rejected': -327.32989501953125, 'referece_logps/chosen': -382.52703857421875, 'logits/rejected': -0.382700115442276, 'logits/chosen': -0.31976422667503357, 'epoch': 7.89}


 88%|████████▊ | 21179/24156 [24:05:34<14:36:08, 17.66s/it]

 88%|████████▊ | 21180/24156 [24:05:45<13:02:47, 15.78s/it]

 88%|████████▊ | 21181/24156 [24:06:05<14:02:27, 16.99s/it]

 88%|████████▊ | 21182/24156 [24:06:19<13:13:52, 16.02s/it]
{'loss': 0.315, 'learning_rate': 1.8266952738212524e-06, 'rewards/chosen': -1.9804826974868774, 'rewards/rejected': -3.127741813659668, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1472593545913696, 'policy_logps/rejected': -471.2291259765625, 'policy_logps/chosen': -759.07470703125, 'referece_logps/rejected': -439.95166015625, 'referece_logps/chosen': -739.2698364257812, 'logits/rejected': -1.0973536968231201, 'logits/chosen': -1.2156682014465332, 'epoch': 7.89}

 88%|████████▊ | 21183/24156 [24:06:29<11:53:56, 14.41s/it]


 88%|████████▊ | 21185/24156 [24:07:06<13:30:39, 16.37s/it]

 88%|████████▊ | 21186/24156 [24:07:25<14:09:44, 17.17s/it]

 88%|████████▊ | 21187/24156 [24:07:41<13:48:20, 16.74s/it]

 88%|████████▊ | 21188/24156 [24:08:01<14:40:21, 17.80s/it]
[2024-04-06 15:12:58,410] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21189/24156 [24:08:20<14:52:13, 18.04s/it]

 88%|████████▊ | 21190/24156 [24:08:34<14:04:21, 17.08s/it]

 88%|████████▊ | 21191/24156 [24:08:57<15:28:46, 18.79s/it]

 88%|████████▊ | 21192/24156 [24:09:11<14:13:56, 17.29s/it]

 88%|████████▊ | 21193/24156 [24:09:30<14:43:43, 17.90s/it]
{'loss': 0.4275, 'learning_rate': 1.8258645433597342e-06, 'rewards/chosen': -2.40006947517395, 'rewards/rejected': -4.154990196228027, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7549206018447876, 'policy_logps/rejected': -361.3387451171875, 'policy_logps/chosen': -578.560302734375, 'referece_logps/rejected': -319.788818359375, 'referece_logps/chosen': -554.5596313476562, 'logits/rejected': -0.5473783612251282, 'logits/chosen': -0.7914243936538696, 'epoch': 7.9}


 88%|████████▊ | 21195/24156 [24:10:09<15:22:40, 18.70s/it]

 88%|████████▊ | 21196/24156 [24:10:23<14:08:55, 17.21s/it]

 88%|████████▊ | 21197/24156 [24:10:35<12:47:37, 15.57s/it]

 88%|████████▊ | 21198/24156 [24:10:50<12:38:09, 15.38s/it]

 88%|████████▊ | 21199/24156 [24:11:09<13:43:32, 16.71s/it]

 88%|████████▊ | 21200/24156 [24:11:29<14:31:45, 17.69s/it]

 88%|████████▊ | 21201/24156 [24:11:45<14:03:42, 17.13s/it]

 88%|████████▊ | 21202/24156 [24:12:02<13:53:05, 16.92s/it]

 88%|████████▊ | 21203/24156 [24:12:15<13:03:10, 15.91s/it]

 88%|████████▊ | 21204/24156 [24:12:29<12:34:16, 15.33s/it]

 88%|████████▊ | 21205/24156 [24:12:43<12:05:00, 14.74s/it]

 88%|████████▊ | 21206/24156 [24:12:58<12:07:41, 14.80s/it]
{'loss': 0.3936, 'learning_rate': 1.8248804550688562e-06, 'rewards/chosen': -2.5465753078460693, 'rewards/rejected': -2.8026328086853027, 'rewards/accuracies': 0.5, 'rewards/margins': 0.25605717301368713, 'policy_logps/rejected': -496.50347900390625, 'policy_logps/chosen': -460.3572998046875, 'referece_logps/rejected': -468.4771423339844, 'referece_logps/chosen': -434.89154052734375, 'logits/rejected': 0.4963557720184326, 'logits/chosen': 0.39731287956237793, 'epoch': 7.9}


 88%|████████▊ | 21208/24156 [24:13:28<12:35:46, 15.38s/it]

 88%|████████▊ | 21209/24156 [24:13:39<11:27:07, 13.99s/it]

 88%|████████▊ | 21210/24156 [24:13:57<12:28:18, 15.24s/it]

 88%|████████▊ | 21211/24156 [24:14:11<12:09:39, 14.87s/it]

 88%|████████▊ | 21212/24156 [24:14:30<13:10:32, 16.11s/it]
{'loss': 0.4338, 'learning_rate': 1.8244254150443649e-06, 'rewards/chosen': -1.914605736732483, 'rewards/rejected': -3.3032338619232178, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3886281251907349, 'policy_logps/rejected': -475.4915771484375, 'policy_logps/chosen': -336.2337646484375, 'referece_logps/rejected': -442.4592590332031, 'referece_logps/chosen': -317.08770751953125, 'logits/rejected': -0.4592885375022888, 'logits/chosen': -0.2090306580066681, 'epoch': 7.9}


 88%|████████▊ | 21214/24156 [24:15:01<12:38:30, 15.47s/it]

 88%|████████▊ | 21215/24156 [24:15:15<12:09:57, 14.89s/it]
{'loss': 0.3987, 'learning_rate': 1.8241976949339141e-06, 'rewards/chosen': -2.137378215789795, 'rewards/rejected': -3.369427442550659, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2320489883422852, 'policy_logps/rejected': -377.4450378417969, 'policy_logps/chosen': -325.08123779296875, 'referece_logps/rejected': -343.75079345703125, 'referece_logps/chosen': -303.70745849609375, 'logits/rejected': -0.4698100984096527, 'logits/chosen': -0.4236856997013092, 'epoch': 7.9}


 88%|████████▊ | 21217/24156 [24:15:39<11:00:39, 13.49s/it]
{'loss': 0.2394, 'learning_rate': 1.8240458074415132e-06, 'rewards/chosen': -2.1034445762634277, 'rewards/rejected': -3.886382579803467, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7829381227493286, 'policy_logps/rejected': -362.7449035644531, 'policy_logps/chosen': -378.88885498046875, 'referece_logps/rejected': -323.8810729980469, 'referece_logps/chosen': -357.8544006347656, 'logits/rejected': -0.8547824025154114, 'logits/chosen': -0.865424633026123, 'epoch': 7.9}


 88%|████████▊ | 21219/24156 [24:16:07<10:55:37, 13.39s/it]

 88%|████████▊ | 21220/24156 [24:16:19<10:41:03, 13.10s/it]

 88%|████████▊ | 21221/24156 [24:16:31<10:25:03, 12.78s/it]
{'loss': 0.2797, 'learning_rate': 1.8237418547008213e-06, 'rewards/chosen': -2.2742557525634766, 'rewards/rejected': -4.642861366271973, 'rewards/accuracies': 1.0, 'rewards/margins': 2.368605613708496, 'policy_logps/rejected': -417.9601135253906, 'policy_logps/chosen': -466.6402893066406, 'referece_logps/rejected': -371.5315246582031, 'referece_logps/chosen': -443.8977355957031, 'logits/rejected': 0.0877062976360321, 'logits/chosen': 0.019956499338150024, 'epoch': 7.91}


 88%|████████▊ | 21223/24156 [24:16:57<10:14:11, 12.56s/it]

 88%|████████▊ | 21224/24156 [24:17:12<10:50:37, 13.31s/it]

 88%|████████▊ | 21225/24156 [24:17:30<11:52:51, 14.59s/it]

 88%|████████▊ | 21226/24156 [24:17:41<11:11:36, 13.75s/it]

 88%|████████▊ | 21227/24156 [24:18:03<13:06:06, 16.10s/it]

 88%|████████▊ | 21228/24156 [24:18:22<13:50:48, 17.02s/it]

 88%|████████▊ | 21229/24156 [24:18:41<14:17:33, 17.58s/it]

 88%|████████▊ | 21230/24156 [24:19:00<14:33:28, 17.91s/it]

 88%|████████▊ | 21231/24156 [24:19:17<14:27:34, 17.80s/it]

 88%|████████▊ | 21232/24156 [24:19:40<15:35:26, 19.20s/it]

 88%|████████▊ | 21233/24156 [24:19:59<15:41:56, 19.33s/it]

 88%|████████▊ | 21234/24156 [24:20:11<13:56:11, 17.17s/it]

 88%|████████▊ | 21235/24156 [24:20:24<12:45:27, 15.72s/it]

 88%|████████▊ | 21236/24156 [24:20:36<11:55:49, 14.71s/it]

 88%|████████▊ | 21237/24156 [24:20:50<11:36:36, 14.32s/it]

 88%|████████▊ | 21238/24156 [24:21:07<12:24:46, 15.31s/it]
{'loss': 0.3743, 'learning_rate': 1.8224474133048068e-06, 'rewards/chosen': -1.4324054718017578, 'rewards/rejected': -2.5545098781585693, 'rewards/accuracies': 1.0, 'rewards/margins': 1.122104525566101, 'policy_logps/rejected': -270.5976867675781, 'policy_logps/chosen': -299.0207824707031, 'referece_logps/rejected': -245.05258178710938, 'referece_logps/chosen': -284.69677734375, 'logits/rejected': -0.3795766234397888, 'logits/chosen': -0.3371354043483734, 'epoch': 7.91}


 88%|████████▊ | 21240/24156 [24:21:42<13:23:50, 16.54s/it]

 88%|████████▊ | 21241/24156 [24:21:54<12:23:59, 15.31s/it]

 88%|████████▊ | 21242/24156 [24:22:12<13:00:48, 16.08s/it]

 88%|████████▊ | 21243/24156 [24:22:24<11:56:17, 14.75s/it]
{'loss': 0.3121, 'learning_rate': 1.8220658817616043e-06, 'rewards/chosen': -2.2956979274749756, 'rewards/rejected': -4.397994041442871, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1022965908050537, 'policy_logps/rejected': -388.20098876953125, 'policy_logps/chosen': -329.8441467285156, 'referece_logps/rejected': -344.22100830078125, 'referece_logps/chosen': -306.88714599609375, 'logits/rejected': -0.9271247386932373, 'logits/chosen': -0.7971810698509216, 'epoch': 7.91}


 88%|████████▊ | 21245/24156 [24:22:59<13:14:42, 16.38s/it]
{'loss': 0.2968, 'learning_rate': 1.8219131656865616e-06, 'rewards/chosen': -2.0997989177703857, 'rewards/rejected': -3.7526559829711914, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6528570652008057, 'policy_logps/rejected': -427.2471923828125, 'policy_logps/chosen': -343.111328125, 'referece_logps/rejected': -389.72064208984375, 'referece_logps/chosen': -322.11334228515625, 'logits/rejected': -0.017122581601142883, 'logits/chosen': 0.08508835732936859, 'epoch': 7.92}

 88%|████████▊ | 21246/24156 [24:23:13<12:28:30, 15.43s/it]

 88%|████████▊ | 21247/24156 [24:23:23<11:17:57, 13.98s/it]


 88%|████████▊ | 21249/24156 [24:24:02<13:44:27, 17.02s/it]

 88%|████████▊ | 21250/24156 [24:24:25<14:58:03, 18.54s/it]

 88%|████████▊ | 21251/24156 [24:24:44<15:11:02, 18.82s/it]

 88%|████████▊ | 21252/24156 [24:25:04<15:26:47, 19.15s/it]

 88%|████████▊ | 21253/24156 [24:25:22<15:08:22, 18.77s/it]

 88%|████████▊ | 21254/24156 [24:25:38<14:29:36, 17.98s/it]
{'loss': 0.3408, 'learning_rate': 1.821225212117156e-06, 'rewards/chosen': -2.3219525814056396, 'rewards/rejected': -3.541334629058838, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2193820476531982, 'policy_logps/rejected': -462.50421142578125, 'policy_logps/chosen': -502.47894287109375, 'referece_logps/rejected': -427.0908508300781, 'referece_logps/chosen': -479.2593994140625, 'logits/rejected': -1.0612577199935913, 'logits/chosen': -1.0360265970230103, 'epoch': 7.92}
[2024-04-06 15:30:59,007] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21255/24156 [24:26:02<15:50:45, 19.66s/it]
[2024-04-06 15:31:16,857] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21256/24156 [24:26:19<15:24:07, 19.12s/it]


 88%|████████▊ | 21258/24156 [24:26:52<14:12:20, 17.65s/it]
{'loss': 0.3333, 'learning_rate': 1.8209190710725512e-06, 'rewards/chosen': -1.958683967590332, 'rewards/rejected': -4.277926445007324, 'rewards/accuracies': 0.75, 'rewards/margins': 2.319242000579834, 'policy_logps/rejected': -380.1771240234375, 'policy_logps/chosen': -525.3574829101562, 'referece_logps/rejected': -337.3978271484375, 'referece_logps/chosen': -505.7706298828125, 'logits/rejected': -0.33628255128860474, 'logits/chosen': -0.3275351822376251, 'epoch': 7.92}

 88%|████████▊ | 21259/24156 [24:27:05<13:15:22, 16.47s/it]


 88%|████████▊ | 21261/24156 [24:27:40<13:34:56, 16.89s/it]
{'loss': 0.3069, 'learning_rate': 1.8206893103285175e-06, 'rewards/chosen': -2.0159223079681396, 'rewards/rejected': -3.494795560836792, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4788731336593628, 'policy_logps/rejected': -351.2921142578125, 'policy_logps/chosen': -353.3993225097656, 'referece_logps/rejected': -316.3441467285156, 'referece_logps/chosen': -333.2401123046875, 'logits/rejected': -0.7214904427528381, 'logits/chosen': -0.6733834743499756, 'epoch': 7.92}
[2024-04-06 15:32:58,432] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21262/24156 [24:28:01<14:37:41, 18.20s/it]
[2024-04-06 15:33:10,439] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 21264/24156 [24:28:32<13:44:59, 17.12s/it]

 88%|████████▊ | 21265/24156 [24:28:46<13:06:54, 16.33s/it]

 88%|████████▊ | 21266/24156 [24:29:08<14:29:50, 18.06s/it]
{'loss': 0.3623, 'learning_rate': 1.8203060807022668e-06, 'rewards/chosen': -2.6715784072875977, 'rewards/rejected': -4.17366361618042, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5020850896835327, 'policy_logps/rejected': -418.5404968261719, 'policy_logps/chosen': -433.9431457519531, 'referece_logps/rejected': -376.8038024902344, 'referece_logps/chosen': -407.2273864746094, 'logits/rejected': 0.2936410903930664, 'logits/chosen': 0.24525856971740723, 'epoch': 7.92}

 88%|████████▊ | 21267/24156 [24:29:28<14:44:58, 18.38s/it]

 88%|████████▊ | 21268/24156 [24:29:43<14:06:49, 17.59s/it]
[2024-04-06 15:35:00,752] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 21270/24156 [24:30:21<14:29:07, 18.07s/it]

 88%|████████▊ | 21271/24156 [24:30:34<13:24:53, 16.74s/it]

 88%|████████▊ | 21272/24156 [24:30:46<12:11:05, 15.21s/it]
{'loss': 0.4113, 'learning_rate': 1.8198457185257681e-06, 'rewards/chosen': -2.8650832176208496, 'rewards/rejected': -3.6046056747436523, 'rewards/accuracies': 0.5, 'rewards/margins': 0.739522397518158, 'policy_logps/rejected': -495.0794677734375, 'policy_logps/chosen': -465.18426513671875, 'referece_logps/rejected': -459.0334167480469, 'referece_logps/chosen': -436.5334167480469, 'logits/rejected': 0.38456815481185913, 'logits/chosen': 0.26324084401130676, 'epoch': 7.93}

 88%|████████▊ | 21273/24156 [24:30:59<11:44:21, 14.66s/it]


 88%|████████▊ | 21275/24156 [24:31:31<12:11:32, 15.24s/it]
{'loss': 0.3794, 'learning_rate': 1.8196153384505447e-06, 'rewards/chosen': -2.3412556648254395, 'rewards/rejected': -4.637181758880615, 'rewards/accuracies': 1.0, 'rewards/margins': 2.295926094055176, 'policy_logps/rejected': -387.7496337890625, 'policy_logps/chosen': -339.0826721191406, 'referece_logps/rejected': -341.3778381347656, 'referece_logps/chosen': -315.67010498046875, 'logits/rejected': 0.0645638108253479, 'logits/chosen': 0.003072679042816162, 'epoch': 7.93}


 88%|████████▊ | 21277/24156 [24:32:02<12:30:36, 15.64s/it]

 88%|████████▊ | 21278/24156 [24:32:19<12:43:45, 15.92s/it]

 88%|████████▊ | 21279/24156 [24:32:32<12:06:41, 15.16s/it]
{'loss': 0.2857, 'learning_rate': 1.8193079587437957e-06, 'rewards/chosen': -1.2348109483718872, 'rewards/rejected': -5.669459819793701, 'rewards/accuracies': 1.0, 'rewards/margins': 4.4346489906311035, 'policy_logps/rejected': -439.3112487792969, 'policy_logps/chosen': -419.13720703125, 'referece_logps/rejected': -382.61663818359375, 'referece_logps/chosen': -406.7890930175781, 'logits/rejected': 0.0972820520401001, 'logits/chosen': 0.1995660662651062, 'epoch': 7.93}

 88%|████████▊ | 21280/24156 [24:32:43<11:10:54, 14.00s/it]


 88%|████████▊ | 21282/24156 [24:33:15<11:45:52, 14.74s/it]
{'loss': 0.4073, 'learning_rate': 1.8190772693072456e-06, 'rewards/chosen': -1.3556584119796753, 'rewards/rejected': -3.768691062927246, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4130325317382812, 'policy_logps/rejected': -506.70599365234375, 'policy_logps/chosen': -467.2337646484375, 'referece_logps/rejected': -469.01910400390625, 'referece_logps/chosen': -453.67718505859375, 'logits/rejected': 0.03777756541967392, 'logits/chosen': 0.003950461745262146, 'epoch': 7.93}


 88%|████████▊ | 21284/24156 [24:33:40<10:49:00, 13.56s/it]

 88%|████████▊ | 21285/24156 [24:33:52<10:28:34, 13.14s/it]

 88%|████████▊ | 21286/24156 [24:34:09<11:14:34, 14.10s/it]

 88%|████████▊ | 21287/24156 [24:34:20<10:39:50, 13.38s/it]

 88%|████████▊ | 21288/24156 [24:34:36<11:13:31, 14.09s/it]

 88%|████████▊ | 21289/24156 [24:34:49<10:57:01, 13.75s/it]

 88%|████████▊ | 21290/24156 [24:35:04<11:18:13, 14.20s/it]

 88%|████████▊ | 21291/24156 [24:35:23<12:18:54, 15.47s/it]
{'loss': 0.4081, 'learning_rate': 1.8183844060195883e-06, 'rewards/chosen': -1.9841073751449585, 'rewards/rejected': -4.003544807434082, 'rewards/accuracies': 0.875, 'rewards/margins': 2.019437551498413, 'policy_logps/rejected': -470.0506286621094, 'policy_logps/chosen': -409.746826171875, 'referece_logps/rejected': -430.0152282714844, 'referece_logps/chosen': -389.90576171875, 'logits/rejected': -0.8234174847602844, 'logits/chosen': -0.7507364749908447, 'epoch': 7.93}

 88%|████████▊ | 21292/24156 [24:35:40<12:37:28, 15.87s/it]


 88%|████████▊ | 21294/24156 [24:36:11<12:31:28, 15.75s/it]

 88%|████████▊ | 21295/24156 [24:36:31<13:25:59, 16.90s/it]
{'loss': 0.3271, 'learning_rate': 1.818076084205359e-06, 'rewards/chosen': -2.308624267578125, 'rewards/rejected': -5.697979927062988, 'rewards/accuracies': 0.875, 'rewards/margins': 3.389355182647705, 'policy_logps/rejected': -359.7051086425781, 'policy_logps/chosen': -355.1324462890625, 'referece_logps/rejected': -302.725341796875, 'referece_logps/chosen': -332.0462341308594, 'logits/rejected': -1.0764045715332031, 'logits/chosen': -1.0697345733642578, 'epoch': 7.93}

 88%|████████▊ | 21296/24156 [24:36:44<12:29:50, 15.73s/it]

 88%|████████▊ | 21297/24156 [24:36:55<11:34:47, 14.58s/it]


 88%|████████▊ | 21299/24156 [24:37:33<13:25:20, 16.91s/it]
{'loss': 0.3249, 'learning_rate': 1.8177675270857992e-06, 'rewards/chosen': -2.3845486640930176, 'rewards/rejected': -3.830408811569214, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4458600282669067, 'policy_logps/rejected': -302.45367431640625, 'policy_logps/chosen': -390.35430908203125, 'referece_logps/rejected': -264.14959716796875, 'referece_logps/chosen': -366.50885009765625, 'logits/rejected': -1.2172565460205078, 'logits/chosen': -1.2608652114868164, 'epoch': 7.94}


 88%|████████▊ | 21301/24156 [24:37:57<11:26:51, 14.44s/it]

 88%|████████▊ | 21302/24156 [24:38:07<10:33:45, 13.32s/it]

 88%|████████▊ | 21303/24156 [24:38:25<11:33:53, 14.59s/it]

 88%|████████▊ | 21304/24156 [24:38:39<11:30:33, 14.53s/it]

 88%|████████▊ | 21305/24156 [24:38:53<11:18:45, 14.28s/it]

 88%|████████▊ | 21306/24156 [24:39:05<10:44:16, 13.56s/it]
{'loss': 0.351, 'learning_rate': 1.8172269861901017e-06, 'rewards/chosen': -2.1598117351531982, 'rewards/rejected': -3.6114494800567627, 'rewards/accuracies': 0.75, 'rewards/margins': 1.451637864112854, 'policy_logps/rejected': -377.5299377441406, 'policy_logps/chosen': -394.49554443359375, 'referece_logps/rejected': -341.41546630859375, 'referece_logps/chosen': -372.89739990234375, 'logits/rejected': -0.12612363696098328, 'logits/chosen': 0.007507771253585815, 'epoch': 7.94}


 88%|████████▊ | 21308/24156 [24:39:39<12:03:49, 15.25s/it]

 88%|████████▊ | 21309/24156 [24:39:53<11:40:51, 14.77s/it]
{'loss': 0.4552, 'learning_rate': 1.8169951054086723e-06, 'rewards/chosen': -1.5740207433700562, 'rewards/rejected': -4.164291858673096, 'rewards/accuracies': 0.75, 'rewards/margins': 2.590270757675171, 'policy_logps/rejected': -305.9667053222656, 'policy_logps/chosen': -496.63092041015625, 'referece_logps/rejected': -264.3238220214844, 'referece_logps/chosen': -480.8907165527344, 'logits/rejected': 0.1520276516675949, 'logits/chosen': 0.0969725251197815, 'epoch': 7.94}


 88%|████████▊ | 21311/24156 [24:40:17<10:28:27, 13.25s/it]

 88%|████████▊ | 21312/24156 [24:40:35<11:30:57, 14.58s/it]
{'loss': 0.3293, 'learning_rate': 1.8167630924428882e-06, 'rewards/chosen': -1.5012900829315186, 'rewards/rejected': -3.045140266418457, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5438498258590698, 'policy_logps/rejected': -339.78265380859375, 'policy_logps/chosen': -431.73370361328125, 'referece_logps/rejected': -309.33123779296875, 'referece_logps/chosen': -416.7208251953125, 'logits/rejected': -0.5687999725341797, 'logits/chosen': -0.7480002045631409, 'epoch': 7.94}

 88%|████████▊ | 21313/24156 [24:40:52<12:05:56, 15.32s/it]

 88%|████████▊ | 21314/24156 [24:41:06<11:50:35, 15.00s/it]


 88%|████████▊ | 21316/24156 [24:41:30<10:32:25, 13.36s/it]

 88%|████████▊ | 21317/24156 [24:41:41<9:59:00, 12.66s/it]

 88%|████████▊ | 21318/24156 [24:41:59<11:22:52, 14.44s/it]
{'loss': 0.3409, 'learning_rate': 1.8162986701084292e-06, 'rewards/chosen': -2.7202205657958984, 'rewards/rejected': -4.606233596801758, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8860132694244385, 'policy_logps/rejected': -488.6988830566406, 'policy_logps/chosen': -426.8568420410156, 'referece_logps/rejected': -442.63653564453125, 'referece_logps/chosen': -399.65460205078125, 'logits/rejected': -0.15075528621673584, 'logits/chosen': -0.21226894855499268, 'epoch': 7.94}

 88%|████████▊ | 21319/24156 [24:42:10<10:29:57, 13.32s/it]


 88%|████████▊ | 21321/24156 [24:42:39<11:04:57, 14.07s/it]

 88%|████████▊ | 21322/24156 [24:42:53<11:02:34, 14.03s/it]
{'loss': 0.2572, 'learning_rate': 1.8159887617076211e-06, 'rewards/chosen': -3.111636161804199, 'rewards/rejected': -5.787347793579102, 'rewards/accuracies': 0.75, 'rewards/margins': 2.6757118701934814, 'policy_logps/rejected': -466.9725341796875, 'policy_logps/chosen': -507.2313232421875, 'referece_logps/rejected': -409.09906005859375, 'referece_logps/chosen': -476.11492919921875, 'logits/rejected': -0.8824681639671326, 'logits/chosen': -0.8729279041290283, 'epoch': 7.94}


 88%|████████▊ | 21324/24156 [24:43:24<11:32:44, 14.68s/it]

 88%|████████▊ | 21325/24156 [24:43:43<12:45:15, 16.22s/it]
{'loss': 0.3518, 'learning_rate': 1.8157561763770147e-06, 'rewards/chosen': -2.811580181121826, 'rewards/rejected': -3.96651029586792, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1549298763275146, 'policy_logps/rejected': -400.4542541503906, 'policy_logps/chosen': -413.2887268066406, 'referece_logps/rejected': -360.7891540527344, 'referece_logps/chosen': -385.17291259765625, 'logits/rejected': -0.6279173493385315, 'logits/chosen': -0.6462249159812927, 'epoch': 7.95}


 88%|████████▊ | 21327/24156 [24:44:13<12:15:49, 15.61s/it]
{'loss': 0.3197, 'learning_rate': 1.8156010461632288e-06, 'rewards/chosen': -2.066845655441284, 'rewards/rejected': -4.3500823974609375, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2832369804382324, 'policy_logps/rejected': -265.4420166015625, 'policy_logps/chosen': -356.6949462890625, 'referece_logps/rejected': -221.94122314453125, 'referece_logps/chosen': -336.0265197753906, 'logits/rejected': -0.7186040282249451, 'logits/chosen': -0.5984582901000977, 'epoch': 7.95}


 88%|████████▊ | 21329/24156 [24:44:41<11:25:24, 14.55s/it]

 88%|████████▊ | 21330/24156 [24:45:01<12:48:01, 16.31s/it]
{'loss': 0.2422, 'learning_rate': 1.8153682408803648e-06, 'rewards/chosen': -2.4697813987731934, 'rewards/rejected': -4.497045040130615, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0272634029388428, 'policy_logps/rejected': -517.05810546875, 'policy_logps/chosen': -348.28912353515625, 'referece_logps/rejected': -472.0876159667969, 'referece_logps/chosen': -323.59130859375, 'logits/rejected': 0.7005968689918518, 'logits/chosen': 0.7894691824913025, 'epoch': 7.95}


 88%|████████▊ | 21332/24156 [24:45:39<13:47:53, 17.59s/it]

 88%|████████▊ | 21333/24156 [24:45:53<13:00:04, 16.58s/it]
{'loss': 0.2541, 'learning_rate': 1.815135303676362e-06, 'rewards/chosen': -1.3824126720428467, 'rewards/rejected': -2.8668365478515625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4844239950180054, 'policy_logps/rejected': -381.1573181152344, 'policy_logps/chosen': -301.8179626464844, 'referece_logps/rejected': -352.4889221191406, 'referece_logps/chosen': -287.99383544921875, 'logits/rejected': -1.0537360906600952, 'logits/chosen': -0.987129271030426, 'epoch': 7.95}

 88%|████████▊ | 21334/24156 [24:46:07<12:11:22, 15.55s/it]
[2024-04-06 15:51:26,299] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 21336/24156 [24:46:43<12:58:30, 16.56s/it]

 88%|████████▊ | 21337/24156 [24:47:03<13:47:24, 17.61s/it]
{'loss': 0.3549, 'learning_rate': 1.814824515592173e-06, 'rewards/chosen': -2.859314441680908, 'rewards/rejected': -6.203521728515625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.344207525253296, 'policy_logps/rejected': -336.5816650390625, 'policy_logps/chosen': -374.407958984375, 'referece_logps/rejected': -274.5464172363281, 'referece_logps/chosen': -345.8148498535156, 'logits/rejected': -0.8501941561698914, 'logits/chosen': -0.8801078200340271, 'epoch': 7.95}

 88%|████████▊ | 21338/24156 [24:47:23<14:17:42, 18.26s/it]

 88%|████████▊ | 21339/24156 [24:47:34<12:38:49, 16.16s/it]


 88%|████████▊ | 21341/24156 [24:48:04<11:56:50, 15.28s/it]

 88%|████████▊ | 21342/24156 [24:48:23<12:58:04, 16.59s/it]

 88%|████████▊ | 21343/24156 [24:48:43<13:47:08, 17.64s/it]
{'loss': 0.2394, 'learning_rate': 1.8143578940499553e-06, 'rewards/chosen': -2.3260059356689453, 'rewards/rejected': -6.2169718742370605, 'rewards/accuracies': 0.875, 'rewards/margins': 3.890965461730957, 'policy_logps/rejected': -437.46826171875, 'policy_logps/chosen': -476.93511962890625, 'referece_logps/rejected': -375.2985534667969, 'referece_logps/chosen': -453.6750793457031, 'logits/rejected': 0.031368009746074677, 'logits/chosen': -0.09071952849626541, 'epoch': 7.95}

 88%|████████▊ | 21344/24156 [24:49:03<14:15:35, 18.26s/it]

 88%|████████▊ | 21345/24156 [24:49:21<14:08:28, 18.11s/it]

 88%|████████▊ | 21346/24156 [24:49:40<14:28:56, 18.55s/it]

 88%|████████▊ | 21347/24156 [24:49:59<14:27:32, 18.53s/it]

 88%|████████▊ | 21348/24156 [24:50:15<13:52:04, 17.78s/it]


 88%|████████▊ | 21350/24156 [24:50:52<13:56:48, 17.89s/it]

 88%|████████▊ | 21351/24156 [24:51:14<14:57:08, 19.19s/it]

 88%|████████▊ | 21352/24156 [24:51:32<14:40:26, 18.84s/it]

 88%|████████▊ | 21353/24156 [24:51:44<13:07:54, 16.87s/it]

 88%|████████▊ | 21354/24156 [24:51:56<11:54:48, 15.31s/it]
{'loss': 0.3045, 'learning_rate': 1.813501052624904e-06, 'rewards/chosen': -1.560115933418274, 'rewards/rejected': -2.996842622756958, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4367268085479736, 'policy_logps/rejected': -401.85443115234375, 'policy_logps/chosen': -667.932373046875, 'referece_logps/rejected': -371.885986328125, 'referece_logps/chosen': -652.3312377929688, 'logits/rejected': -0.43466097116470337, 'logits/chosen': -0.06778615713119507, 'epoch': 7.96}


 88%|████████▊ | 21356/24156 [24:52:40<14:37:05, 18.79s/it]
[2024-04-06 15:57:37,053] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 21357/24156 [24:52:52<13:06:40, 16.86s/it]
{'loss': 0.4269, 'learning_rate': 1.8132670614103374e-06, 'rewards/chosen': -2.2632994651794434, 'rewards/rejected': -3.07232666015625, 'rewards/accuracies': 0.625, 'rewards/margins': 0.8090271949768066, 'policy_logps/rejected': -457.8109436035156, 'policy_logps/chosen': -398.07611083984375, 'referece_logps/rejected': -427.0876770019531, 'referece_logps/chosen': -375.4430847167969, 'logits/rejected': 0.2993347644805908, 'logits/chosen': 0.36856305599212646, 'epoch': 7.96}

 88%|████████▊ | 21358/24156 [24:53:05<12:15:49, 15.78s/it]

 88%|████████▊ | 21359/24156 [24:53:17<11:25:24, 14.70s/it]

 88%|████████▊ | 21360/24156 [24:53:34<11:47:38, 15.19s/it]


 88%|████████▊ | 21362/24156 [24:54:12<13:25:32, 17.30s/it]
{'loss': 0.3191, 'learning_rate': 1.8128767836688045e-06, 'rewards/chosen': -2.1296820640563965, 'rewards/rejected': -4.554727077484131, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4250450134277344, 'policy_logps/rejected': -378.7254333496094, 'policy_logps/chosen': -511.2394714355469, 'referece_logps/rejected': -333.1781921386719, 'referece_logps/chosen': -489.9426574707031, 'logits/rejected': 0.2122451663017273, 'logits/chosen': 0.2532002031803131, 'epoch': 7.96}

 88%|████████▊ | 21363/24156 [24:54:31<13:45:22, 17.73s/it]

 88%|████████▊ | 21364/24156 [24:54:50<13:58:33, 18.02s/it]

 88%|████████▊ | 21365/24156 [24:55:03<12:53:47, 16.63s/it]

 88%|████████▊ | 21366/24156 [24:55:20<12:53:59, 16.64s/it]

 88%|████████▊ | 21367/24156 [24:55:32<11:45:49, 15.18s/it]

 88%|████████▊ | 21368/24156 [24:55:48<11:58:53, 15.47s/it]

 88%|████████▊ | 21369/24156 [24:56:08<13:00:24, 16.80s/it]


 88%|████████▊ | 21371/24156 [24:56:35<11:49:04, 15.28s/it]
{'loss': 0.4892, 'learning_rate': 1.8121733632257769e-06, 'rewards/chosen': -2.0418763160705566, 'rewards/rejected': -4.051633358001709, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0097572803497314, 'policy_logps/rejected': -366.48980712890625, 'policy_logps/chosen': -397.85528564453125, 'referece_logps/rejected': -325.97344970703125, 'referece_logps/chosen': -377.4365234375, 'logits/rejected': -0.6521505117416382, 'logits/chosen': -0.7699375152587891, 'epoch': 7.96}

 88%|████████▊ | 21372/24156 [24:56:54<12:44:23, 16.47s/it]

 88%|████████▊ | 21373/24156 [24:57:11<12:50:08, 16.60s/it]

 88%|████████▊ | 21374/24156 [24:57:21<11:24:57, 14.77s/it]

 88%|████████▊ | 21375/24156 [24:57:35<11:10:22, 14.46s/it]

 88%|████████▊ | 21376/24156 [24:57:55<12:30:15, 16.19s/it]


 88%|████████▊ | 21378/24156 [24:58:32<13:28:39, 17.47s/it]
{'loss': 0.456, 'learning_rate': 1.8116254407580124e-06, 'rewards/chosen': -2.487863540649414, 'rewards/rejected': -3.8435282707214355, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3556649684906006, 'policy_logps/rejected': -408.93988037109375, 'policy_logps/chosen': -342.0657043457031, 'referece_logps/rejected': -370.5046081542969, 'referece_logps/chosen': -317.1870422363281, 'logits/rejected': -0.21592611074447632, 'logits/chosen': -0.25022032856941223, 'epoch': 7.96}

 89%|████████▊ | 21379/24156 [24:58:54<14:23:38, 18.66s/it]


 89%|████████▊ | 21381/24156 [24:59:25<12:59:52, 16.86s/it]
{'loss': 0.3439, 'learning_rate': 1.8113903979558063e-06, 'rewards/chosen': -1.536603331565857, 'rewards/rejected': -3.1551976203918457, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6185946464538574, 'policy_logps/rejected': -309.4217834472656, 'policy_logps/chosen': -383.725341796875, 'referece_logps/rejected': -277.86981201171875, 'referece_logps/chosen': -368.35931396484375, 'logits/rejected': -0.5232253670692444, 'logits/chosen': -0.5717105269432068, 'epoch': 7.97}

 89%|████████▊ | 21382/24156 [24:59:36<11:37:09, 15.08s/it]


 89%|████████▊ | 21384/24156 [25:00:08<12:11:34, 15.83s/it]
{'loss': 0.4182, 'learning_rate': 1.8111552238760496e-06, 'rewards/chosen': -2.182298183441162, 'rewards/rejected': -3.5508573055267334, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3685593605041504, 'policy_logps/rejected': -230.98394775390625, 'policy_logps/chosen': -399.5534973144531, 'referece_logps/rejected': -195.47537231445312, 'referece_logps/chosen': -377.73052978515625, 'logits/rejected': -0.9784534573554993, 'logits/chosen': -1.1114474534988403, 'epoch': 7.97}

 89%|████████▊ | 21385/24156 [25:00:19<11:02:08, 14.34s/it]


 89%|████████▊ | 21387/24156 [25:00:56<12:47:13, 16.62s/it]
{'loss': 0.2656, 'learning_rate': 1.8109199185567915e-06, 'rewards/chosen': -1.5569660663604736, 'rewards/rejected': -5.125100135803223, 'rewards/accuracies': 1.0, 'rewards/margins': 3.56813383102417, 'policy_logps/rejected': -313.09271240234375, 'policy_logps/chosen': -336.95855712890625, 'referece_logps/rejected': -261.8417053222656, 'referece_logps/chosen': -321.38885498046875, 'logits/rejected': -0.22499896585941315, 'logits/chosen': -0.1678113341331482, 'epoch': 7.97}


 89%|████████▊ | 21389/24156 [25:01:31<13:18:22, 17.31s/it]

 89%|████████▊ | 21390/24156 [25:01:45<12:32:53, 16.33s/it]
{'loss': 0.3633, 'learning_rate': 1.8106844820361036e-06, 'rewards/chosen': -1.8627370595932007, 'rewards/rejected': -3.247389316558838, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3846521377563477, 'policy_logps/rejected': -353.3081970214844, 'policy_logps/chosen': -385.26312255859375, 'referece_logps/rejected': -320.8343505859375, 'referece_logps/chosen': -366.6357727050781, 'logits/rejected': 0.20967042446136475, 'logits/chosen': 0.0769994854927063, 'epoch': 7.97}

 89%|████████▊ | 21391/24156 [25:02:02<12:38:04, 16.45s/it]


 89%|████████▊ | 21393/24156 [25:02:45<14:45:27, 19.23s/it]
[2024-04-06 16:07:42,218] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▊ | 21394/24156 [25:02:59<13:32:00, 17.64s/it]

 89%|████████▊ | 21395/24156 [25:03:17<13:35:21, 17.72s/it]
[2024-04-06 16:08:14,055] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.4146, 'learning_rate': 1.8102917963796926e-06, 'rewards/chosen': -1.6190199851989746, 'rewards/rejected': -2.174698829650879, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5556789040565491, 'policy_logps/rejected': -320.5929870605469, 'policy_logps/chosen': -351.4548034667969, 'referece_logps/rejected': -298.84600830078125, 'referece_logps/chosen': -335.26458740234375, 'logits/rejected': -1.1746599674224854, 'logits/chosen': -1.2152893543243408, 'epoch': 7.97}

 89%|████████▊ | 21396/24156 [25:03:37<14:16:11, 18.61s/it]

 89%|████████▊ | 21397/24156 [25:03:58<14:48:25, 19.32s/it]


 89%|████████▊ | 21399/24156 [25:04:31<13:49:59, 18.06s/it]
{'loss': 0.4228, 'learning_rate': 1.8099773856464852e-06, 'rewards/chosen': -3.1442463397979736, 'rewards/rejected': -3.8576788902282715, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7134326696395874, 'policy_logps/rejected': -316.90899658203125, 'policy_logps/chosen': -363.42425537109375, 'referece_logps/rejected': -278.3321838378906, 'referece_logps/chosen': -331.98175048828125, 'logits/rejected': -0.31696152687072754, 'logits/chosen': -0.33055004477500916, 'epoch': 7.97}


 89%|████████▊ | 21401/24156 [25:05:07<13:41:33, 17.89s/it]
{'loss': 0.2874, 'learning_rate': 1.8098200929082714e-06, 'rewards/chosen': -1.6141018867492676, 'rewards/rejected': -3.5244877338409424, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9103859663009644, 'policy_logps/rejected': -513.6845092773438, 'policy_logps/chosen': -467.10638427734375, 'referece_logps/rejected': -478.43963623046875, 'referece_logps/chosen': -450.9653625488281, 'logits/rejected': -0.11286148428916931, 'logits/chosen': 0.13303102552890778, 'epoch': 7.97}

 89%|████████▊ | 21402/24156 [25:05:20<12:36:00, 16.47s/it]

 89%|████████▊ | 21403/24156 [25:05:39<13:19:25, 17.42s/it]

 89%|████████▊ | 21404/24156 [25:05:58<13:35:39, 17.78s/it]

 89%|████████▊ | 21405/24156 [25:06:18<14:06:25, 18.46s/it]

 89%|████████▊ | 21406/24156 [25:06:32<12:57:50, 16.97s/it]


 89%|████████▊ | 21408/24156 [25:07:05<12:49:38, 16.80s/it]
{'loss': 0.3204, 'learning_rate': 1.8092691098165828e-06, 'rewards/chosen': -1.8565648794174194, 'rewards/rejected': -4.011650562286377, 'rewards/accuracies': 0.875, 'rewards/margins': 2.155085563659668, 'policy_logps/rejected': -309.1385803222656, 'policy_logps/chosen': -432.48529052734375, 'referece_logps/rejected': -269.0220947265625, 'referece_logps/chosen': -413.91961669921875, 'logits/rejected': -0.42568081617355347, 'logits/chosen': -0.4665745198726654, 'epoch': 7.98}

 89%|████████▊ | 21409/24156 [25:07:16<11:30:26, 15.08s/it]


 89%|████████▊ | 21411/24156 [25:07:51<12:17:28, 16.12s/it]
{'loss': 0.3905, 'learning_rate': 1.8090327559536495e-06, 'rewards/chosen': -1.9935799837112427, 'rewards/rejected': -3.4681992530822754, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4746193885803223, 'policy_logps/rejected': -353.3583068847656, 'policy_logps/chosen': -531.1940307617188, 'referece_logps/rejected': -318.6763000488281, 'referece_logps/chosen': -511.25823974609375, 'logits/rejected': -0.4753054678440094, 'logits/chosen': -0.5770084261894226, 'epoch': 7.98}

 89%|████████▊ | 21412/24156 [25:08:06<12:05:42, 15.87s/it]

 89%|████████▊ | 21413/24156 [25:08:25<12:39:29, 16.61s/it]

 89%|████████▊ | 21414/24156 [25:08:39<12:15:41, 16.10s/it]

 89%|████████▊ | 21415/24156 [25:08:58<12:52:26, 16.91s/it]

 89%|████████▊ | 21416/24156 [25:09:14<12:38:24, 16.61s/it]

 89%|████████▊ | 21417/24156 [25:09:32<13:01:23, 17.12s/it]

 89%|████████▊ | 21418/24156 [25:09:51<13:18:01, 17.49s/it]

 89%|████████▊ | 21419/24156 [25:10:02<11:57:57, 15.74s/it]

 89%|████████▊ | 21420/24156 [25:10:14<11:05:28, 14.59s/it]


 89%|████████▊ | 21422/24156 [25:10:47<11:54:27, 15.68s/it]
{'loss': 0.3272, 'learning_rate': 1.8081650055250845e-06, 'rewards/chosen': -1.529462456703186, 'rewards/rejected': -4.825111389160156, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2956490516662598, 'policy_logps/rejected': -279.3529052734375, 'policy_logps/chosen': -471.02679443359375, 'referece_logps/rejected': -231.101806640625, 'referece_logps/chosen': -455.732177734375, 'logits/rejected': -1.0361624956130981, 'logits/chosen': -1.177536129951477, 'epoch': 7.98}


 89%|████████▊ | 21424/24156 [25:11:24<12:46:00, 16.82s/it]
{'loss': 0.3897, 'learning_rate': 1.8080070437952042e-06, 'rewards/chosen': -3.083423376083374, 'rewards/rejected': -5.4467926025390625, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3633689880371094, 'policy_logps/rejected': -415.5254211425781, 'policy_logps/chosen': -537.3399658203125, 'referece_logps/rejected': -361.0574645996094, 'referece_logps/chosen': -506.5057373046875, 'logits/rejected': -0.12537668645381927, 'logits/chosen': -0.08901239186525345, 'epoch': 7.98}

 89%|████████▊ | 21425/24156 [25:11:42<13:08:47, 17.33s/it]

 89%|████████▊ | 21426/24156 [25:12:02<13:45:52, 18.15s/it]

 89%|████████▊ | 21427/24156 [25:12:19<13:24:47, 17.69s/it]

 89%|████████▊ | 21428/24156 [25:12:38<13:49:30, 18.24s/it]

 89%|████████▊ | 21429/24156 [25:12:57<13:51:06, 18.29s/it]

 89%|████████▊ | 21430/24156 [25:13:08<12:18:06, 16.25s/it]

 89%|████████▊ | 21431/24156 [25:13:25<12:26:00, 16.43s/it]

 89%|████████▊ | 21432/24156 [25:13:45<13:10:03, 17.40s/it]

 89%|████████▊ | 21433/24156 [25:14:02<13:12:53, 17.47s/it]

 89%|████████▊ | 21434/24156 [25:14:16<12:25:43, 16.44s/it]

 89%|████████▊ | 21435/24156 [25:14:29<11:33:08, 15.28s/it]

 89%|████████▊ | 21436/24156 [25:14:42<11:07:43, 14.73s/it]

 89%|████████▊ | 21437/24156 [25:15:00<11:49:34, 15.66s/it]

 89%|████████▊ | 21438/24156 [25:15:18<12:22:09, 16.38s/it]


 89%|████████▉ | 21440/24156 [25:15:45<11:05:46, 14.71s/it]
{'loss': 0.3646, 'learning_rate': 1.8067412592288137e-06, 'rewards/chosen': -2.1203410625457764, 'rewards/rejected': -3.9338200092315674, 'rewards/accuracies': 0.875, 'rewards/margins': 1.813478946685791, 'policy_logps/rejected': -343.5929870605469, 'policy_logps/chosen': -334.7181396484375, 'referece_logps/rejected': -304.2547302246094, 'referece_logps/chosen': -313.51470947265625, 'logits/rejected': -0.6997643709182739, 'logits/chosen': -0.6628432273864746, 'epoch': 7.99}

 89%|████████▉ | 21441/24156 [25:16:05<12:06:50, 16.06s/it]

 89%|████████▉ | 21442/24156 [25:16:23<12:31:22, 16.61s/it]

 89%|████████▉ | 21443/24156 [25:16:41<12:53:24, 17.10s/it]

 89%|████████▉ | 21444/24156 [25:17:01<13:30:37, 17.93s/it]

 89%|████████▉ | 21445/24156 [25:17:15<12:38:49, 16.79s/it]

 89%|████████▉ | 21446/24156 [25:17:33<12:59:09, 17.25s/it]
[2024-04-06 16:22:54,484] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21447/24156 [25:17:57<14:28:29, 19.24s/it]
[2024-04-06 16:23:08,422] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21448/24156 [25:18:11<13:16:27, 17.65s/it]


 89%|████████▉ | 21450/24156 [25:18:45<12:57:37, 17.24s/it]
{'loss': 0.3648, 'learning_rate': 1.8059482581388555e-06, 'rewards/chosen': -1.9561655521392822, 'rewards/rejected': -5.213209629058838, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2570440769195557, 'policy_logps/rejected': -362.2087097167969, 'policy_logps/chosen': -259.77911376953125, 'referece_logps/rejected': -310.07659912109375, 'referece_logps/chosen': -240.21746826171875, 'logits/rejected': -0.5778893828392029, 'logits/chosen': -0.47315868735313416, 'epoch': 7.99}

 89%|████████▉ | 21451/24156 [25:19:07<13:52:36, 18.47s/it]

 89%|████████▉ | 21452/24156 [25:19:23<13:25:45, 17.88s/it]
[2024-04-06 16:24:39,832] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21453/24156 [25:19:42<13:41:22, 18.23s/it]

 89%|████████▉ | 21454/24156 [25:19:53<11:59:37, 15.98s/it]

 89%|████████▉ | 21455/24156 [25:20:05<11:10:56, 14.90s/it]

 89%|████████▉ | 21456/24156 [25:20:23<11:40:01, 15.56s/it]


 89%|████████▉ | 21458/24156 [25:21:00<13:03:49, 17.43s/it]
{'loss': 0.4425, 'learning_rate': 1.8053128140221264e-06, 'rewards/chosen': -2.6841278076171875, 'rewards/rejected': -5.236388206481934, 'rewards/accuracies': 0.875, 'rewards/margins': 2.552260398864746, 'policy_logps/rejected': -585.021240234375, 'policy_logps/chosen': -619.954345703125, 'referece_logps/rejected': -532.6573486328125, 'referece_logps/chosen': -593.1130981445312, 'logits/rejected': -0.389687180519104, 'logits/chosen': -0.5346508622169495, 'epoch': 7.99}


 89%|████████▉ | 21460/24156 [25:21:30<11:58:38, 15.99s/it]
{'loss': 0.3441, 'learning_rate': 1.8051538081929984e-06, 'rewards/chosen': -1.8946207761764526, 'rewards/rejected': -4.025811195373535, 'rewards/accuracies': 0.875, 'rewards/margins': 2.131190538406372, 'policy_logps/rejected': -382.03228759765625, 'policy_logps/chosen': -357.7278137207031, 'referece_logps/rejected': -341.7741394042969, 'referece_logps/chosen': -338.7815856933594, 'logits/rejected': -0.30535924434661865, 'logits/chosen': -0.3376632332801819, 'epoch': 8.0}
[2024-04-06 16:26:46,355] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21461/24156 [25:21:49<12:35:03, 16.81s/it]

 89%|████████▉ | 21462/24156 [25:22:00<11:11:51, 14.96s/it]

 89%|████████▉ | 21463/24156 [25:22:18<11:52:48, 15.88s/it]


 89%|████████▉ | 21465/24156 [25:22:50<12:18:42, 16.47s/it]

 89%|████████▉ | 21466/24156 [25:23:04<11:48:47, 15.81s/it]
{'loss': 0.3388, 'learning_rate': 1.8046764433686699e-06, 'rewards/chosen': -2.188154458999634, 'rewards/rejected': -5.185632705688477, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9974780082702637, 'policy_logps/rejected': -358.15753173828125, 'policy_logps/chosen': -356.8013610839844, 'referece_logps/rejected': -306.30120849609375, 'referece_logps/chosen': -334.91986083984375, 'logits/rejected': -0.5335970520973206, 'logits/chosen': -0.45304155349731445, 'epoch': 8.0}

 89%|████████▉ | 21467/24156 [25:23:24<12:40:30, 16.97s/it]

 89%|████████▉ | 21468/24156 [25:23:39<12:13:44, 16.38s/it]

 89%|████████▉ | 21469/24156 [25:23:52<11:29:52, 15.40s/it]

 89%|████████▉ | 21470/24156 [25:24:09<11:51:16, 15.89s/it]

 89%|████████▉ | 21471/24156 [25:24:29<12:41:07, 17.01s/it]

 89%|████████▉ | 21472/24156 [25:24:42<11:56:48, 16.02s/it]

 89%|████████▉ | 21473/24156 [25:24:56<11:24:08, 15.30s/it]

 89%|████████▉ | 21474/24156 [25:25:16<12:24:52, 16.66s/it]

 89%|████████▉ | 21475/24156 [25:25:28<11:23:19, 15.29s/it]

 89%|████████▉ | 21476/24156 [25:25:46<11:59:38, 16.11s/it]

 89%|████████▉ | 21477/24156 [25:26:02<12:02:00, 16.17s/it]

 89%|████████▉ | 21478/24156 [25:26:22<12:46:27, 17.17s/it]

 89%|████████▉ | 21479/24156 [25:26:32<11:19:20, 15.23s/it]

 89%|████████▉ | 21480/24156 [25:26:44<10:37:28, 14.29s/it]

 89%|████████▉ | 21481/24156 [25:26:55<9:49:38, 13.23s/it]

 89%|████████▉ | 21482/24156 [25:27:06<9:16:07, 12.48s/it]

 89%|████████▉ | 21483/24156 [25:27:17<8:53:04, 11.97s/it]

 89%|████████▉ | 21484/24156 [25:27:27<8:35:06, 11.57s/it]

 89%|████████▉ | 21485/24156 [25:27:43<9:30:29, 12.82s/it]


 89%|████████▉ | 21487/24156 [25:28:15<10:56:24, 14.76s/it]

 89%|████████▉ | 21488/24156 [25:28:35<12:04:23, 16.29s/it]

 89%|████████▉ | 21489/24156 [25:28:52<12:07:52, 16.37s/it]

 89%|████████▉ | 21490/24156 [25:29:04<11:11:27, 15.11s/it]

 89%|████████▉ | 21491/24156 [25:29:16<10:37:15, 14.35s/it]

 89%|████████▉ | 21492/24156 [25:29:32<10:55:15, 14.76s/it]

 89%|████████▉ | 21493/24156 [25:29:45<10:32:23, 14.25s/it]

 89%|████████▉ | 21494/24156 [25:29:56<9:44:24, 13.17s/it]

 89%|████████▉ | 21495/24156 [25:30:16<11:10:39, 15.12s/it]
[2024-04-06 16:35:13,013] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21496/24156 [25:30:28<10:41:20, 14.47s/it]

 89%|████████▉ | 21497/24156 [25:30:39<9:48:28, 13.28s/it]

 89%|████████▉ | 21498/24156 [25:30:52<9:46:11, 13.23s/it]

 89%|████████▉ | 21499/24156 [25:31:10<10:45:58, 14.59s/it]

 89%|████████▉ | 21500/24156 [25:31:26<11:10:17, 15.14s/it]

 89%|████████▉ | 21501/24156 [25:32:06<16:38:13, 22.56s/it]

 89%|████████▉ | 21502/24156 [25:32:22<15:13:54, 20.66s/it]

 89%|████████▉ | 21503/24156 [25:32:42<15:02:58, 20.42s/it]

 89%|████████▉ | 21504/24156 [25:33:00<14:25:37, 19.58s/it]

 89%|████████▉ | 21505/24156 [25:33:18<14:02:32, 19.07s/it]

 89%|████████▉ | 21506/24156 [25:33:35<13:43:55, 18.65s/it]

 89%|████████▉ | 21507/24156 [25:33:48<12:24:05, 16.85s/it]

 89%|████████▉ | 21508/24156 [25:34:05<12:30:46, 17.01s/it]

 89%|████████▉ | 21509/24156 [25:34:28<13:41:12, 18.61s/it]

 89%|████████▉ | 21510/24156 [25:34:45<13:16:35, 18.06s/it]

 89%|████████▉ | 21511/24156 [25:35:00<12:46:19, 17.38s/it]

 89%|████████▉ | 21512/24156 [25:35:20<13:09:11, 17.91s/it]

 89%|████████▉ | 21513/24156 [25:35:30<11:33:39, 15.75s/it]

 89%|████████▉ | 21514/24156 [25:35:47<11:52:52, 16.19s/it]

 89%|████████▉ | 21515/24156 [25:36:07<12:31:03, 17.06s/it]

 89%|████████▉ | 21516/24156 [25:36:24<12:31:28, 17.08s/it]

 89%|████████▉ | 21517/24156 [25:36:40<12:26:26, 16.97s/it]

 89%|████████▉ | 21518/24156 [25:36:58<12:39:26, 17.27s/it]

 89%|████████▉ | 21519/24156 [25:37:14<12:24:11, 16.93s/it]

 89%|████████▉ | 21520/24156 [25:37:30<12:00:16, 16.39s/it]

 89%|████████▉ | 21521/24156 [25:37:40<10:45:22, 14.70s/it]

 89%|████████▉ | 21522/24156 [25:37:51<9:52:47, 13.50s/it]

 89%|████████▉ | 21523/24156 [25:38:10<11:01:32, 15.08s/it]

 89%|████████▉ | 21524/24156 [25:38:29<12:01:05, 16.44s/it]

 89%|████████▉ | 21525/24156 [25:38:44<11:30:22, 15.74s/it]

 89%|████████▉ | 21526/24156 [25:39:02<12:04:42, 16.53s/it]

 89%|████████▉ | 21527/24156 [25:39:23<13:05:15, 17.92s/it]

 89%|████████▉ | 21528/24156 [25:39:41<13:10:49, 18.06s/it]
{'loss': 0.3391, 'learning_rate': 1.7997132362973588e-06, 'rewards/chosen': -1.4369785785675049, 'rewards/rejected': -3.434892416000366, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9979138374328613, 'policy_logps/rejected': -365.5509033203125, 'policy_logps/chosen': -318.9853210449219, 'referece_logps/rejected': -331.2019958496094, 'referece_logps/chosen': -304.6155700683594, 'logits/rejected': -0.4914577603340149, 'logits/chosen': -0.41870608925819397, 'epoch': 8.02}


 89%|████████▉ | 21530/24156 [25:40:09<11:28:38, 15.73s/it]

 89%|████████▉ | 21531/24156 [25:40:23<11:02:05, 15.13s/it]
{'loss': 0.4637, 'learning_rate': 1.7994716766431344e-06, 'rewards/chosen': -2.6649398803710938, 'rewards/rejected': -4.219663619995117, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5547237396240234, 'policy_logps/rejected': -503.55084228515625, 'policy_logps/chosen': -626.1126708984375, 'referece_logps/rejected': -461.3542175292969, 'referece_logps/chosen': -599.4632568359375, 'logits/rejected': -1.0207860469818115, 'logits/chosen': -1.016372561454773, 'epoch': 8.02}

 89%|████████▉ | 21532/24156 [25:40:35<10:26:34, 14.33s/it]


 89%|████████▉ | 21534/24156 [25:41:08<11:22:15, 15.61s/it]

 89%|████████▉ | 21535/24156 [25:41:28<12:15:02, 16.83s/it]

 89%|████████▉ | 21536/24156 [25:41:48<12:54:14, 17.73s/it]

 89%|████████▉ | 21537/24156 [25:41:59<11:23:33, 15.66s/it]

 89%|████████▉ | 21538/24156 [25:42:09<10:18:54, 14.18s/it]

 89%|████████▉ | 21539/24156 [25:42:26<10:55:19, 15.02s/it]

 89%|████████▉ | 21540/24156 [25:42:37<9:57:45, 13.71s/it]

 89%|████████▉ | 21541/24156 [25:42:50<9:46:14, 13.45s/it]

 89%|████████▉ | 21542/24156 [25:43:05<10:13:01, 14.07s/it]

 89%|████████▉ | 21543/24156 [25:43:27<11:55:31, 16.43s/it]
[2024-04-06 16:48:24,708] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2848, 'learning_rate': 1.7985041449255673e-06, 'rewards/chosen': -2.480395793914795, 'rewards/rejected': -4.314454078674316, 'rewards/accuracies': 0.875, 'rewards/margins': 1.834058403968811, 'policy_logps/rejected': -452.76611328125, 'policy_logps/chosen': -375.25225830078125, 'referece_logps/rejected': -409.62158203125, 'referece_logps/chosen': -350.4482421875, 'logits/rejected': 0.3749690055847168, 'logits/chosen': 0.4289891719818115, 'epoch': 8.03}


 89%|████████▉ | 21545/24156 [25:44:05<12:57:51, 17.87s/it]
[2024-04-06 16:49:02,373] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21546/24156 [25:44:26<13:39:53, 18.85s/it]
[2024-04-06 16:49:23,492] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21547/24156 [25:44:46<13:59:53, 19.32s/it]

 89%|████████▉ | 21548/24156 [25:45:03<13:20:08, 18.41s/it]

 89%|████████▉ | 21549/24156 [25:45:20<13:09:29, 18.17s/it]
[2024-04-06 16:50:17,803] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21550/24156 [25:45:37<12:50:18, 17.74s/it]

 89%|████████▉ | 21551/24156 [25:45:50<11:49:02, 16.33s/it]
{'loss': 0.2044, 'learning_rate': 1.7978579752468975e-06, 'rewards/chosen': -2.7855119705200195, 'rewards/rejected': -4.907629013061523, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1221163272857666, 'policy_logps/rejected': -388.51904296875, 'policy_logps/chosen': -428.234375, 'referece_logps/rejected': -339.4427490234375, 'referece_logps/chosen': -400.37921142578125, 'logits/rejected': 0.22082774341106415, 'logits/chosen': 0.11099229007959366, 'epoch': 8.03}


 89%|████████▉ | 21553/24156 [25:46:29<13:00:44, 18.00s/it]

 89%|████████▉ | 21554/24156 [25:46:44<12:23:19, 17.14s/it]

 89%|████████▉ | 21555/24156 [25:47:04<12:52:49, 17.83s/it]

 89%|████████▉ | 21556/24156 [25:47:15<11:29:04, 15.90s/it]

 89%|████████▉ | 21557/24156 [25:47:26<10:21:06, 14.34s/it]

 89%|████████▉ | 21558/24156 [25:47:38<9:55:37, 13.76s/it]

 89%|████████▉ | 21559/24156 [25:47:50<9:32:29, 13.23s/it]

 89%|████████▉ | 21560/24156 [25:48:04<9:31:13, 13.20s/it]

 89%|████████▉ | 21561/24156 [25:48:22<10:40:29, 14.81s/it]

 89%|████████▉ | 21562/24156 [25:48:40<11:17:35, 15.67s/it]

 89%|████████▉ | 21563/24156 [25:48:57<11:43:06, 16.27s/it]

 89%|████████▉ | 21564/24156 [25:49:11<11:02:08, 15.33s/it]

 89%|████████▉ | 21565/24156 [25:49:30<12:01:02, 16.70s/it]
[2024-04-06 16:54:27,916] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21566/24156 [25:49:51<12:44:47, 17.72s/it]

 89%|████████▉ | 21567/24156 [25:50:08<12:45:02, 17.73s/it]

 89%|████████▉ | 21568/24156 [25:50:23<12:11:13, 16.95s/it]

 89%|████████▉ | 21569/24156 [25:50:36<11:16:40, 15.69s/it]

 89%|████████▉ | 21570/24156 [25:50:50<10:48:02, 15.04s/it]

 89%|████████▉ | 21571/24156 [25:51:00<9:51:18, 13.72s/it]

 89%|████████▉ | 21572/24156 [25:51:11<9:10:46, 12.79s/it]

 89%|████████▉ | 21573/24156 [25:51:31<10:39:27, 14.85s/it]

 89%|████████▉ | 21574/24156 [25:51:46<10:46:02, 15.01s/it]

 89%|████████▉ | 21575/24156 [25:52:00<10:32:35, 14.71s/it]

 89%|████████▉ | 21576/24156 [25:52:18<11:20:21, 15.82s/it]

 89%|████████▉ | 21577/24156 [25:52:38<12:09:18, 16.97s/it]

 89%|████████▉ | 21578/24156 [25:52:55<12:12:43, 17.05s/it]

 89%|████████▉ | 21579/24156 [25:53:08<11:09:57, 15.60s/it]

 89%|████████▉ | 21580/24156 [25:53:27<12:01:57, 16.82s/it]

 89%|████████▉ | 21581/24156 [25:53:46<12:22:44, 17.31s/it]

 89%|████████▉ | 21582/24156 [25:54:06<13:02:18, 18.24s/it]

 89%|████████▉ | 21583/24156 [25:54:27<13:40:26, 19.13s/it]
[2024-04-06 16:59:24,744] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21584/24156 [25:54:47<13:48:27, 19.33s/it]
[2024-04-06 16:59:44,525] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 21585/24156 [25:54:58<11:57:44, 16.75s/it]
{'loss': 0.376, 'learning_rate': 1.7951015221296125e-06, 'rewards/chosen': -2.0602598190307617, 'rewards/rejected': -4.498286724090576, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4380266666412354, 'policy_logps/rejected': -389.8061828613281, 'policy_logps/chosen': -471.2475280761719, 'referece_logps/rejected': -344.82330322265625, 'referece_logps/chosen': -450.64495849609375, 'logits/rejected': -0.6546451449394226, 'logits/chosen': -0.6755864024162292, 'epoch': 8.04}


 89%|████████▉ | 21587/24156 [25:55:30<11:56:58, 16.75s/it]
[2024-04-06 17:00:26,995] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2748, 'learning_rate': 1.7949388626996193e-06, 'rewards/chosen': -3.153388500213623, 'rewards/rejected': -5.757109642028809, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6037206649780273, 'policy_logps/rejected': -567.3404541015625, 'policy_logps/chosen': -504.1553649902344, 'referece_logps/rejected': -509.76934814453125, 'referece_logps/chosen': -472.6214904785156, 'logits/rejected': 0.21740847826004028, 'logits/chosen': 0.22618170082569122, 'epoch': 8.04}
[2024-04-06 17:00:38,253] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 89%|████████▉ | 21589/24156 [25:55:56<10:43:02, 15.03s/it]

 89%|████████▉ | 21590/24156 [25:56:14<11:28:24, 16.10s/it]

 89%|████████▉ | 21591/24156 [25:56:34<12:13:49, 17.17s/it]

 89%|████████▉ | 21592/24156 [25:56:54<12:49:50, 18.01s/it]
{'loss': 0.2329, 'learning_rate': 1.7945319640639491e-06, 'rewards/chosen': -1.87360417842865, 'rewards/rejected': -4.094840049743652, 'rewards/accuracies': 0.875, 'rewards/margins': 2.221235752105713, 'policy_logps/rejected': -351.4023132324219, 'policy_logps/chosen': -334.7445068359375, 'referece_logps/rejected': -310.45391845703125, 'referece_logps/chosen': -316.0084533691406, 'logits/rejected': 0.40630197525024414, 'logits/chosen': 0.41763120889663696, 'epoch': 8.04}

 89%|████████▉ | 21593/24156 [25:57:13<13:00:16, 18.27s/it]


 89%|████████▉ | 21595/24156 [25:57:39<11:14:40, 15.81s/it]

 89%|████████▉ | 21596/24156 [25:57:59<12:07:04, 17.04s/it]
{'loss': 0.2397, 'learning_rate': 1.794206188046699e-06, 'rewards/chosen': -2.019963026046753, 'rewards/rejected': -3.9468140602111816, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9268512725830078, 'policy_logps/rejected': -532.4100952148438, 'policy_logps/chosen': -473.9464111328125, 'referece_logps/rejected': -492.94195556640625, 'referece_logps/chosen': -453.7467956542969, 'logits/rejected': 0.6792323589324951, 'logits/chosen': 0.7735079526901245, 'epoch': 8.05}


 89%|████████▉ | 21598/24156 [25:58:37<12:48:07, 18.02s/it]

 89%|████████▉ | 21599/24156 [25:58:57<13:14:19, 18.64s/it]

 89%|████████▉ | 21600/24156 [25:59:17<13:28:45, 18.99s/it]
{'loss': 0.1782, 'learning_rate': 1.7938801835898779e-06, 'rewards/chosen': -3.239856719970703, 'rewards/rejected': -6.6477155685424805, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4078593254089355, 'policy_logps/rejected': -324.27508544921875, 'policy_logps/chosen': -346.11376953125, 'referece_logps/rejected': -257.79791259765625, 'referece_logps/chosen': -313.7151794433594, 'logits/rejected': -0.6278340816497803, 'logits/chosen': -0.8425437211990356, 'epoch': 8.05}


 89%|████████▉ | 21602/24156 [25:59:50<12:14:20, 17.25s/it]
{'loss': 0.3497, 'learning_rate': 1.7937170957259284e-06, 'rewards/chosen': -2.6944358348846436, 'rewards/rejected': -6.268057823181152, 'rewards/accuracies': 0.875, 'rewards/margins': 3.573622226715088, 'policy_logps/rejected': -438.12506103515625, 'policy_logps/chosen': -347.65496826171875, 'referece_logps/rejected': -375.44451904296875, 'referece_logps/chosen': -320.7106018066406, 'logits/rejected': -0.15893158316612244, 'logits/chosen': -0.15850257873535156, 'epoch': 8.05}


 89%|████████▉ | 21604/24156 [26:00:22<11:57:52, 16.88s/it]

 89%|████████▉ | 21605/24156 [26:00:40<12:15:52, 17.31s/it]
{'loss': 0.2513, 'learning_rate': 1.7934723569185625e-06, 'rewards/chosen': -2.453550338745117, 'rewards/rejected': -5.3707733154296875, 'rewards/accuracies': 0.75, 'rewards/margins': 2.9172234535217285, 'policy_logps/rejected': -436.7364501953125, 'policy_logps/chosen': -537.8400268554688, 'referece_logps/rejected': -383.0287170410156, 'referece_logps/chosen': -513.3045043945312, 'logits/rejected': 0.6198611259460449, 'logits/chosen': 0.3766881227493286, 'epoch': 8.05}

 89%|████████▉ | 21606/24156 [26:00:53<11:10:49, 15.78s/it]

 89%|████████▉ | 21607/24156 [26:01:11<11:41:54, 16.52s/it]

 89%|████████▉ | 21608/24156 [26:01:31<12:24:19, 17.53s/it]


 89%|████████▉ | 21610/24156 [26:02:00<11:23:13, 16.10s/it]

 89%|████████▉ | 21611/24156 [26:02:12<10:35:48, 14.99s/it]

 89%|████████▉ | 21612/24156 [26:02:33<11:47:24, 16.68s/it]
{'loss': 0.4018, 'learning_rate': 1.792900800520011e-06, 'rewards/chosen': -2.3659870624542236, 'rewards/rejected': -4.417381763458252, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0513947010040283, 'policy_logps/rejected': -279.6656188964844, 'policy_logps/chosen': -319.3397216796875, 'referece_logps/rejected': -235.49179077148438, 'referece_logps/chosen': -295.67987060546875, 'logits/rejected': -0.7830491662025452, 'logits/chosen': -0.9076392650604248, 'epoch': 8.05}

 89%|████████▉ | 21613/24156 [26:02:43<10:30:27, 14.88s/it]


 89%|████████▉ | 21615/24156 [26:03:15<10:33:06, 14.95s/it]

 89%|████████▉ | 21616/24156 [26:03:34<11:32:14, 16.35s/it]

 89%|████████▉ | 21617/24156 [26:03:54<12:13:40, 17.34s/it]

 89%|████████▉ | 21618/24156 [26:04:06<11:06:27, 15.76s/it]

 89%|████████▉ | 21619/24156 [26:04:25<11:40:24, 16.56s/it]

 90%|████████▉ | 21620/24156 [26:04:38<11:07:09, 15.78s/it]
{'loss': 0.2994, 'learning_rate': 1.7922467379964358e-06, 'rewards/chosen': -2.2717907428741455, 'rewards/rejected': -4.071318626403809, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7995274066925049, 'policy_logps/rejected': -469.4437255859375, 'policy_logps/chosen': -446.4268493652344, 'referece_logps/rejected': -428.73052978515625, 'referece_logps/chosen': -423.70892333984375, 'logits/rejected': -0.9207430481910706, 'logits/chosen': -0.9129031896591187, 'epoch': 8.06}


 90%|████████▉ | 21622/24156 [26:05:14<11:48:51, 16.78s/it]
{'loss': 0.3501, 'learning_rate': 1.7920830799136532e-06, 'rewards/chosen': -1.9862761497497559, 'rewards/rejected': -4.489787578582764, 'rewards/accuracies': 0.875, 'rewards/margins': 2.503511428833008, 'policy_logps/rejected': -448.9248352050781, 'policy_logps/chosen': -380.01251220703125, 'referece_logps/rejected': -404.0269775390625, 'referece_logps/chosen': -360.1497802734375, 'logits/rejected': -0.3652764558792114, 'logits/chosen': -0.3927765190601349, 'epoch': 8.06}

 90%|████████▉ | 21623/24156 [26:05:27<11:03:50, 15.72s/it]


 90%|████████▉ | 21625/24156 [26:06:00<11:06:44, 15.81s/it]

 90%|████████▉ | 21626/24156 [26:06:18<11:34:56, 16.48s/it]

 90%|████████▉ | 21627/24156 [26:06:36<11:50:16, 16.85s/it]

 90%|████████▉ | 21628/24156 [26:06:50<11:14:35, 16.01s/it]
{'loss': 0.3563, 'learning_rate': 1.7915917639690487e-06, 'rewards/chosen': -2.5999372005462646, 'rewards/rejected': -5.281350612640381, 'rewards/accuracies': 1.0, 'rewards/margins': 2.681413412094116, 'policy_logps/rejected': -356.16107177734375, 'policy_logps/chosen': -374.9775390625, 'referece_logps/rejected': -303.3475646972656, 'referece_logps/chosen': -348.9781494140625, 'logits/rejected': -0.585689127445221, 'logits/chosen': -0.4374678432941437, 'epoch': 8.06}

 90%|████████▉ | 21629/24156 [26:07:07<11:28:26, 16.35s/it]


 90%|████████▉ | 21631/24156 [26:07:35<10:25:06, 14.85s/it]
{'loss': 0.3113, 'learning_rate': 1.7913459138654728e-06, 'rewards/chosen': -2.4872381687164307, 'rewards/rejected': -5.679318428039551, 'rewards/accuracies': 0.875, 'rewards/margins': 3.192080497741699, 'policy_logps/rejected': -351.0132141113281, 'policy_logps/chosen': -404.2889709472656, 'referece_logps/rejected': -294.2200622558594, 'referece_logps/chosen': -379.4165954589844, 'logits/rejected': -0.4245884418487549, 'logits/chosen': -0.4574912488460541, 'epoch': 8.06}

 90%|████████▉ | 21632/24156 [26:07:52<10:51:26, 15.49s/it]


 90%|████████▉ | 21634/24156 [26:08:25<11:17:27, 16.12s/it]
{'loss': 0.4157, 'learning_rate': 1.7910999357274098e-06, 'rewards/chosen': -2.3524630069732666, 'rewards/rejected': -3.5007152557373047, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1482526063919067, 'policy_logps/rejected': -582.6915893554688, 'policy_logps/chosen': -490.371826171875, 'referece_logps/rejected': -547.6844482421875, 'referece_logps/chosen': -466.84716796875, 'logits/rejected': -0.027568034827709198, 'logits/chosen': -0.12801367044448853, 'epoch': 8.06}


 90%|████████▉ | 21636/24156 [26:08:58<11:30:21, 16.44s/it]

 90%|████████▉ | 21637/24156 [26:09:13<11:07:30, 15.90s/it]
{'loss': 0.417, 'learning_rate': 1.790853829594658e-06, 'rewards/chosen': -1.8244647979736328, 'rewards/rejected': -3.2039361000061035, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3794713020324707, 'policy_logps/rejected': -317.4813232421875, 'policy_logps/chosen': -335.4520263671875, 'referece_logps/rejected': -285.44195556640625, 'referece_logps/chosen': -317.2073974609375, 'logits/rejected': 0.1630302369594574, 'logits/chosen': 0.1425861120223999, 'epoch': 8.06}

 90%|████████▉ | 21638/24156 [26:09:26<10:28:11, 14.97s/it]

 90%|████████▉ | 21639/24156 [26:09:45<11:27:26, 16.39s/it]

 90%|████████▉ | 21640/24156 [26:10:00<11:03:26, 15.82s/it]


 90%|████████▉ | 21642/24156 [26:10:26<10:03:48, 14.41s/it]

 90%|████████▉ | 21643/24156 [26:10:40<10:00:42, 14.34s/it]
{'loss': 0.3996, 'learning_rate': 1.7903612335043805e-06, 'rewards/chosen': -1.8546990156173706, 'rewards/rejected': -2.4773170948028564, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6226180791854858, 'policy_logps/rejected': -504.41949462890625, 'policy_logps/chosen': -554.0701293945312, 'referece_logps/rejected': -479.6463623046875, 'referece_logps/chosen': -535.5231323242188, 'logits/rejected': 0.16127413511276245, 'logits/chosen': 0.2951120436191559, 'epoch': 8.06}

 90%|████████▉ | 21644/24156 [26:10:57<10:32:40, 15.11s/it]

 90%|████████▉ | 21645/24156 [26:11:17<11:34:09, 16.59s/it]


 90%|████████▉ | 21647/24156 [26:11:45<10:31:24, 15.10s/it]

 90%|████████▉ | 21648/24156 [26:11:57<9:53:08, 14.19s/it]

 90%|████████▉ | 21649/24156 [26:12:17<11:02:28, 15.85s/it]

 90%|████████▉ | 21650/24156 [26:12:28<10:08:38, 14.57s/it]
{'loss': 0.4301, 'learning_rate': 1.7897858916081154e-06, 'rewards/chosen': -1.6838334798812866, 'rewards/rejected': -3.222078800201416, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5382452011108398, 'policy_logps/rejected': -378.62518310546875, 'policy_logps/chosen': -520.9523315429688, 'referece_logps/rejected': -346.4044189453125, 'referece_logps/chosen': -504.11395263671875, 'logits/rejected': -0.7339280247688293, 'logits/chosen': -0.6450668573379517, 'epoch': 8.07}


 90%|████████▉ | 21652/24156 [26:12:53<9:23:30, 13.50s/it]

 90%|████████▉ | 21653/24156 [26:13:10<10:04:14, 14.48s/it]

 90%|████████▉ | 21654/24156 [26:13:29<10:55:21, 15.72s/it]
{'loss': 0.4159, 'learning_rate': 1.7894568124215277e-06, 'rewards/chosen': -0.9729598164558411, 'rewards/rejected': -3.9970245361328125, 'rewards/accuracies': 0.75, 'rewards/margins': 3.024064540863037, 'policy_logps/rejected': -317.8810729980469, 'policy_logps/chosen': -380.3924560546875, 'referece_logps/rejected': -277.9108581542969, 'referece_logps/chosen': -370.662841796875, 'logits/rejected': -0.30776041746139526, 'logits/chosen': -0.21656082570552826, 'epoch': 8.07}


 90%|████████▉ | 21656/24156 [26:14:04<11:48:38, 17.01s/it]

 90%|████████▉ | 21657/24156 [26:14:23<12:10:29, 17.54s/it]

 90%|████████▉ | 21658/24156 [26:14:43<12:42:09, 18.31s/it]

 90%|████████▉ | 21659/24156 [26:14:55<11:26:20, 16.49s/it]

 90%|████████▉ | 21660/24156 [26:15:14<11:56:43, 17.23s/it]

 90%|████████▉ | 21661/24156 [26:15:25<10:35:32, 15.28s/it]
{'loss': 0.4117, 'learning_rate': 1.7888803775063762e-06, 'rewards/chosen': -2.0203728675842285, 'rewards/rejected': -3.8347795009613037, 'rewards/accuracies': 0.5, 'rewards/margins': 1.8144065141677856, 'policy_logps/rejected': -261.667236328125, 'policy_logps/chosen': -485.3910827636719, 'referece_logps/rejected': -223.3194580078125, 'referece_logps/chosen': -465.1873779296875, 'logits/rejected': -0.19617971777915955, 'logits/chosen': -0.3810124397277832, 'epoch': 8.07}


 90%|████████▉ | 21663/24156 [26:15:47<9:03:40, 13.09s/it]

 90%|████████▉ | 21664/24156 [26:15:58<8:43:13, 12.60s/it]
{'loss': 0.3477, 'learning_rate': 1.788633121215729e-06, 'rewards/chosen': -2.1564130783081055, 'rewards/rejected': -3.238308906555176, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0818960666656494, 'policy_logps/rejected': -455.22052001953125, 'policy_logps/chosen': -447.0498046875, 'referece_logps/rejected': -422.8374328613281, 'referece_logps/chosen': -425.4856262207031, 'logits/rejected': 0.20753878355026245, 'logits/chosen': 0.1390131711959839, 'epoch': 8.07}

 90%|████████▉ | 21665/24156 [26:16:20<10:31:15, 15.21s/it]


 90%|████████▉ | 21667/24156 [26:16:47<10:03:56, 14.56s/it]

 90%|████████▉ | 21668/24156 [26:17:05<10:46:05, 15.58s/it]

 90%|████████▉ | 21669/24156 [26:17:23<11:16:05, 16.31s/it]

 90%|████████▉ | 21670/24156 [26:17:42<11:41:29, 16.93s/it]

 90%|████████▉ | 21671/24156 [26:17:57<11:28:56, 16.63s/it]

 90%|████████▉ | 21672/24156 [26:18:16<11:48:45, 17.12s/it]

 90%|████████▉ | 21673/24156 [26:18:27<10:41:26, 15.50s/it]

 90%|████████▉ | 21674/24156 [26:18:39<9:49:42, 14.26s/it]

 90%|████████▉ | 21675/24156 [26:18:51<9:26:58, 13.71s/it]
{'loss': 0.3692, 'learning_rate': 1.787725423470112e-06, 'rewards/chosen': -1.4634137153625488, 'rewards/rejected': -3.5835390090942383, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1201252937316895, 'policy_logps/rejected': -467.9884033203125, 'policy_logps/chosen': -360.76666259765625, 'referece_logps/rejected': -432.1529846191406, 'referece_logps/chosen': -346.1325378417969, 'logits/rejected': -0.6894558668136597, 'logits/chosen': -0.4643208980560303, 'epoch': 8.08}


 90%|████████▉ | 21677/24156 [26:19:21<10:02:39, 14.59s/it]
{'loss': 0.2211, 'learning_rate': 1.7875602033658918e-06, 'rewards/chosen': -1.9720025062561035, 'rewards/rejected': -3.7348473072052, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7628448009490967, 'policy_logps/rejected': -457.93731689453125, 'policy_logps/chosen': -443.406005859375, 'referece_logps/rejected': -420.5888366699219, 'referece_logps/chosen': -423.6860046386719, 'logits/rejected': -0.7648778557777405, 'logits/chosen': -0.5650495886802673, 'epoch': 8.08}


 90%|████████▉ | 21679/24156 [26:19:49<9:50:11, 14.30s/it]

 90%|████████▉ | 21680/24156 [26:20:00<9:05:36, 13.22s/it]

 90%|████████▉ | 21681/24156 [26:20:20<10:27:07, 15.20s/it]

 90%|████████▉ | 21682/24156 [26:20:33<10:04:38, 14.66s/it]

 90%|████████▉ | 21683/24156 [26:20:46<9:38:28, 14.03s/it]
{'loss': 0.3438, 'learning_rate': 1.7870642033088139e-06, 'rewards/chosen': -2.32553768157959, 'rewards/rejected': -3.7106871604919434, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3851492404937744, 'policy_logps/rejected': -461.64898681640625, 'policy_logps/chosen': -396.57586669921875, 'referece_logps/rejected': -424.5421142578125, 'referece_logps/chosen': -373.32049560546875, 'logits/rejected': 0.11416955292224884, 'logits/chosen': 0.1504766345024109, 'epoch': 8.08}


 90%|████████▉ | 21685/24156 [26:21:14<9:53:22, 14.41s/it]

 90%|████████▉ | 21686/24156 [26:21:25<9:10:40, 13.38s/it]
{'loss': 0.3377, 'learning_rate': 1.7868160122476056e-06, 'rewards/chosen': -1.7910614013671875, 'rewards/rejected': -4.873044490814209, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0819830894470215, 'policy_logps/rejected': -354.53997802734375, 'policy_logps/chosen': -423.95501708984375, 'referece_logps/rejected': -305.80950927734375, 'referece_logps/chosen': -406.0444030761719, 'logits/rejected': -0.3197072744369507, 'logits/chosen': -0.4582909941673279, 'epoch': 8.08}


 90%|████████▉ | 21688/24156 [26:22:04<11:17:21, 16.47s/it]

 90%|████████▉ | 21689/24156 [26:22:23<11:53:24, 17.35s/it]

 90%|████████▉ | 21690/24156 [26:22:41<11:59:21, 17.50s/it]
{'loss': 0.3319, 'learning_rate': 1.7864848928149262e-06, 'rewards/chosen': -2.1144509315490723, 'rewards/rejected': -4.669926166534424, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5554752349853516, 'policy_logps/rejected': -415.91986083984375, 'policy_logps/chosen': -504.4273681640625, 'referece_logps/rejected': -369.2206115722656, 'referece_logps/chosen': -483.2828369140625, 'logits/rejected': 0.16089686751365662, 'logits/chosen': 0.16828061640262604, 'epoch': 8.08}


 90%|████████▉ | 21692/24156 [26:23:03<9:38:47, 14.09s/it]
{'loss': 0.3864, 'learning_rate': 1.7863192482606281e-06, 'rewards/chosen': -2.8957817554473877, 'rewards/rejected': -3.8730225563049316, 'rewards/accuracies': 0.625, 'rewards/margins': 0.9772410988807678, 'policy_logps/rejected': -293.6110534667969, 'policy_logps/chosen': -438.2322998046875, 'referece_logps/rejected': -254.8808135986328, 'referece_logps/chosen': -409.2745361328125, 'logits/rejected': -1.011541724205017, 'logits/chosen': -1.1137850284576416, 'epoch': 8.08}


 90%|████████▉ | 21694/24156 [26:23:27<8:59:56, 13.16s/it]

 90%|████████▉ | 21695/24156 [26:23:47<10:21:30, 15.15s/it]
{'loss': 0.2889, 'learning_rate': 1.7860706754152322e-06, 'rewards/chosen': -2.1510729789733887, 'rewards/rejected': -3.8409082889556885, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6898350715637207, 'policy_logps/rejected': -553.2830810546875, 'policy_logps/chosen': -442.5534973144531, 'referece_logps/rejected': -514.8740234375, 'referece_logps/chosen': -421.04278564453125, 'logits/rejected': -0.6246322393417358, 'logits/chosen': -0.5434300303459167, 'epoch': 8.08}

 90%|████████▉ | 21696/24156 [26:24:07<11:18:06, 16.54s/it]


 90%|████████▉ | 21698/24156 [26:24:41<11:21:42, 16.64s/it]
{'loss': 0.3448, 'learning_rate': 1.7858219753888475e-06, 'rewards/chosen': -1.610120177268982, 'rewards/rejected': -3.071418046951294, 'rewards/accuracies': 0.875, 'rewards/margins': 1.461297869682312, 'policy_logps/rejected': -429.6455078125, 'policy_logps/chosen': -414.0114440917969, 'referece_logps/rejected': -398.93133544921875, 'referece_logps/chosen': -397.9101867675781, 'logits/rejected': 0.3243306279182434, 'logits/chosen': 0.2921893000602722, 'epoch': 8.08}

 90%|████████▉ | 21699/24156 [26:25:01<11:58:51, 17.55s/it]

 90%|████████▉ | 21700/24156 [26:25:21<12:24:37, 18.19s/it]

 90%|████████▉ | 21701/24156 [26:25:41<12:49:23, 18.80s/it]


 90%|████████▉ | 21703/24156 [26:26:16<12:29:46, 18.34s/it]
{'loss': 0.324, 'learning_rate': 1.7854071928297523e-06, 'rewards/chosen': -1.371482253074646, 'rewards/rejected': -4.229433536529541, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8579511642456055, 'policy_logps/rejected': -369.6950378417969, 'policy_logps/chosen': -349.4502868652344, 'referece_logps/rejected': -327.4007263183594, 'referece_logps/chosen': -335.7354431152344, 'logits/rejected': -0.6800890564918518, 'logits/chosen': -0.7006844282150269, 'epoch': 8.09}

 90%|████████▉ | 21704/24156 [26:26:29<11:22:08, 16.69s/it]

 90%|████████▉ | 21705/24156 [26:26:49<12:02:56, 17.70s/it]

 90%|████████▉ | 21706/24156 [26:27:01<11:00:24, 16.17s/it]

 90%|████████▉ | 21707/24156 [26:27:15<10:24:36, 15.30s/it]


 90%|████████▉ | 21709/24156 [26:27:52<11:32:32, 16.98s/it]

 90%|████████▉ | 21710/24156 [26:28:12<12:08:03, 17.86s/it]
{'loss': 0.2788, 'learning_rate': 1.7848259042784936e-06, 'rewards/chosen': -3.273979425430298, 'rewards/rejected': -5.43866491317749, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1646857261657715, 'policy_logps/rejected': -400.47979736328125, 'policy_logps/chosen': -410.36370849609375, 'referece_logps/rejected': -346.0931396484375, 'referece_logps/chosen': -377.6239013671875, 'logits/rejected': -0.4259963631629944, 'logits/chosen': -0.4013230800628662, 'epoch': 8.09}

 90%|████████▉ | 21711/24156 [26:28:25<11:03:44, 16.29s/it]

 90%|████████▉ | 21712/24156 [26:28:41<11:06:05, 16.35s/it]


 90%|████████▉ | 21714/24156 [26:29:20<12:00:00, 17.69s/it]

 90%|████████▉ | 21715/24156 [26:29:36<11:37:42, 17.15s/it]
{'loss': 0.2702, 'learning_rate': 1.7844102748752872e-06, 'rewards/chosen': -3.1887316703796387, 'rewards/rejected': -6.7249555587768555, 'rewards/accuracies': 0.875, 'rewards/margins': 3.536223888397217, 'policy_logps/rejected': -334.3563537597656, 'policy_logps/chosen': -395.3994140625, 'referece_logps/rejected': -267.1068115234375, 'referece_logps/chosen': -363.51214599609375, 'logits/rejected': -0.3384551405906677, 'logits/chosen': -0.5094616413116455, 'epoch': 8.09}


 90%|████████▉ | 21717/24156 [26:30:06<10:51:07, 16.02s/it]

 90%|████████▉ | 21718/24156 [26:30:24<11:14:28, 16.60s/it]

 90%|████████▉ | 21719/24156 [26:30:42<11:36:01, 17.14s/it]
{'loss': 0.4161, 'learning_rate': 1.784077517519063e-06, 'rewards/chosen': -1.4940612316131592, 'rewards/rejected': -3.2660679817199707, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7720067501068115, 'policy_logps/rejected': -478.0215759277344, 'policy_logps/chosen': -414.6011657714844, 'referece_logps/rejected': -445.3609619140625, 'referece_logps/chosen': -399.6605224609375, 'logits/rejected': -0.10567224770784378, 'logits/chosen': -0.04771734029054642, 'epoch': 8.09}

 90%|████████▉ | 21720/24156 [26:30:54<10:24:46, 15.39s/it]

 90%|████████▉ | 21721/24156 [26:31:08<10:06:11, 14.94s/it]

 90%|████████▉ | 21722/24156 [26:31:23<10:08:41, 15.00s/it]

 90%|████████▉ | 21723/24156 [26:31:33<9:15:45, 13.71s/it]

 90%|████████▉ | 21724/24156 [26:31:52<10:08:40, 15.02s/it]


 90%|████████▉ | 21726/24156 [26:32:18<9:27:57, 14.02s/it]
{'loss': 0.253, 'learning_rate': 1.78349464953078e-06, 'rewards/chosen': -2.528618097305298, 'rewards/rejected': -4.510568141937256, 'rewards/accuracies': 0.875, 'rewards/margins': 1.981950283050537, 'policy_logps/rejected': -467.4524841308594, 'policy_logps/chosen': -530.6609497070312, 'referece_logps/rejected': -422.3468017578125, 'referece_logps/chosen': -505.374755859375, 'logits/rejected': 0.10987146198749542, 'logits/chosen': 0.15186113119125366, 'epoch': 8.09}


 90%|████████▉ | 21728/24156 [26:32:54<10:48:50, 16.03s/it]
{'loss': 0.4489, 'learning_rate': 1.783327989033176e-06, 'rewards/chosen': -3.0577502250671387, 'rewards/rejected': -5.483979225158691, 'rewards/accuracies': 0.875, 'rewards/margins': 2.426229476928711, 'policy_logps/rejected': -356.96417236328125, 'policy_logps/chosen': -397.888671875, 'referece_logps/rejected': -302.1243896484375, 'referece_logps/chosen': -367.3111267089844, 'logits/rejected': -0.6144434213638306, 'logits/chosen': -0.707206130027771, 'epoch': 8.1}

 90%|████████▉ | 21729/24156 [26:33:14<11:32:46, 17.13s/it]

 90%|████████▉ | 21730/24156 [26:33:34<12:07:54, 18.00s/it]

 90%|████████▉ | 21731/24156 [26:33:53<12:28:40, 18.52s/it]


 90%|████████▉ | 21733/24156 [26:34:26<11:44:24, 17.44s/it]
{'loss': 0.3374, 'learning_rate': 1.782911091381868e-06, 'rewards/chosen': -2.618992805480957, 'rewards/rejected': -4.957893371582031, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3389008045196533, 'policy_logps/rejected': -409.2287292480469, 'policy_logps/chosen': -502.7403564453125, 'referece_logps/rejected': -359.6497497558594, 'referece_logps/chosen': -476.55047607421875, 'logits/rejected': 0.46218717098236084, 'logits/chosen': 0.4919429123401642, 'epoch': 8.1}

 90%|████████▉ | 21734/24156 [26:34:47<12:24:34, 18.45s/it]

 90%|████████▉ | 21735/24156 [26:35:07<12:46:55, 19.01s/it]


 90%|████████▉ | 21737/24156 [26:35:41<11:54:10, 17.71s/it]

 90%|████████▉ | 21738/24156 [26:35:58<11:50:16, 17.62s/it]

 90%|████████▉ | 21739/24156 [26:36:18<12:20:42, 18.39s/it]
{'loss': 0.348, 'learning_rate': 1.7824103497609164e-06, 'rewards/chosen': -2.391786813735962, 'rewards/rejected': -4.189113616943359, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7973272800445557, 'policy_logps/rejected': -423.162353515625, 'policy_logps/chosen': -447.80279541015625, 'referece_logps/rejected': -381.27117919921875, 'referece_logps/chosen': -423.8849182128906, 'logits/rejected': 0.32328248023986816, 'logits/chosen': 0.32778334617614746, 'epoch': 8.1}

 90%|████████▉ | 21740/24156 [26:36:36<12:06:48, 18.05s/it]


 90%|█████████ | 21742/24156 [26:37:09<11:36:54, 17.32s/it]

 90%|█████████ | 21743/24156 [26:37:29<12:09:32, 18.14s/it]
{'loss': 0.2349, 'learning_rate': 1.7820762406852264e-06, 'rewards/chosen': -2.2783138751983643, 'rewards/rejected': -4.6549072265625, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3765931129455566, 'policy_logps/rejected': -466.68865966796875, 'policy_logps/chosen': -401.83001708984375, 'referece_logps/rejected': -420.13958740234375, 'referece_logps/chosen': -379.046875, 'logits/rejected': -0.4898204505443573, 'logits/chosen': -0.43006521463394165, 'epoch': 8.1}

 90%|█████████ | 21744/24156 [26:37:46<11:55:09, 17.79s/it]


 90%|█████████ | 21746/24156 [26:38:25<12:26:30, 18.59s/it]

 90%|█████████ | 21747/24156 [26:38:41<11:59:38, 17.92s/it]
{'loss': 0.4109, 'learning_rate': 1.7817419066589332e-06, 'rewards/chosen': -2.4958624839782715, 'rewards/rejected': -3.867309808731079, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3714473247528076, 'policy_logps/rejected': -418.2459411621094, 'policy_logps/chosen': -421.8462829589844, 'referece_logps/rejected': -379.5728454589844, 'referece_logps/chosen': -396.8876953125, 'logits/rejected': 0.4030464291572571, 'logits/chosen': 0.5569444894790649, 'epoch': 8.1}


 90%|█████████ | 21749/24156 [26:39:16<12:00:25, 17.96s/it]
{'loss': 0.3623, 'learning_rate': 1.7815746553193597e-06, 'rewards/chosen': -1.3706108331680298, 'rewards/rejected': -3.5791499614715576, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2085390090942383, 'policy_logps/rejected': -358.71624755859375, 'policy_logps/chosen': -373.9787292480469, 'referece_logps/rejected': -322.9247741699219, 'referece_logps/chosen': -360.2726135253906, 'logits/rejected': 0.29142051935195923, 'logits/chosen': 0.3108099699020386, 'epoch': 8.1}


 90%|█████████ | 21751/24156 [26:39:39<9:43:43, 14.56s/it]
{'loss': 0.3954, 'learning_rate': 1.7814073477782023e-06, 'rewards/chosen': -2.8498897552490234, 'rewards/rejected': -3.6944196224212646, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8445294499397278, 'policy_logps/rejected': -329.2559814453125, 'policy_logps/chosen': -339.4947509765625, 'referece_logps/rejected': -292.311767578125, 'referece_logps/chosen': -310.995849609375, 'logits/rejected': -0.4241829514503479, 'logits/chosen': -0.37769296765327454, 'epoch': 8.1}

 90%|█████████ | 21752/24156 [26:39:55<10:03:43, 15.07s/it]


 90%|█████████ | 21754/24156 [26:40:29<10:39:47, 15.98s/it]
{'loss': 0.3266, 'learning_rate': 1.7811562811148154e-06, 'rewards/chosen': -1.8352792263031006, 'rewards/rejected': -4.095423698425293, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2601444721221924, 'policy_logps/rejected': -340.70513916015625, 'policy_logps/chosen': -595.2139892578125, 'referece_logps/rejected': -299.7508850097656, 'referece_logps/chosen': -576.8612060546875, 'logits/rejected': -0.1718001663684845, 'logits/chosen': -0.4242818355560303, 'epoch': 8.11}


 90%|█████████ | 21756/24156 [26:40:57<9:58:37, 14.97s/it]
{'loss': 0.3424, 'learning_rate': 1.7809888331223423e-06, 'rewards/chosen': -1.7343456745147705, 'rewards/rejected': -4.0622758865356445, 'rewards/accuracies': 1.0, 'rewards/margins': 2.327929973602295, 'policy_logps/rejected': -343.6253662109375, 'policy_logps/chosen': -430.3125, 'referece_logps/rejected': -303.0025939941406, 'referece_logps/chosen': -412.96905517578125, 'logits/rejected': -0.5944307446479797, 'logits/chosen': -0.5322291851043701, 'epoch': 8.11}

 90%|█████████ | 21757/24156 [26:41:16<10:56:42, 16.42s/it]

 90%|█████████ | 21758/24156 [26:41:38<11:56:13, 17.92s/it]

 90%|█████████ | 21759/24156 [26:41:58<12:18:11, 18.48s/it]


 90%|█████████ | 21761/24156 [26:42:35<12:29:11, 18.77s/it]
{'loss': 0.2154, 'learning_rate': 1.7805699674698786e-06, 'rewards/chosen': -5.126194000244141, 'rewards/rejected': -6.761747360229492, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6355526447296143, 'policy_logps/rejected': -389.54510498046875, 'policy_logps/chosen': -429.9237365722656, 'referece_logps/rejected': -321.9276123046875, 'referece_logps/chosen': -378.66180419921875, 'logits/rejected': -1.026357889175415, 'logits/chosen': -1.0326400995254517, 'epoch': 8.11}

 90%|█████████ | 21762/24156 [26:42:56<13:00:59, 19.57s/it]

 90%|█████████ | 21763/24156 [26:43:12<12:17:30, 18.49s/it]

 90%|█████████ | 21764/24156 [26:43:32<12:31:38, 18.85s/it]

 90%|█████████ | 21765/24156 [26:43:49<12:14:55, 18.44s/it]


 90%|█████████ | 21767/24156 [26:44:15<10:10:26, 15.33s/it]
{'loss': 0.4561, 'learning_rate': 1.7800668656364714e-06, 'rewards/chosen': -1.605780839920044, 'rewards/rejected': -2.9623992443084717, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3566182851791382, 'policy_logps/rejected': -480.45086669921875, 'policy_logps/chosen': -337.33221435546875, 'referece_logps/rejected': -450.8268737792969, 'referece_logps/chosen': -321.27435302734375, 'logits/rejected': -0.37368452548980713, 'logits/chosen': -0.24595065414905548, 'epoch': 8.11}


 90%|█████████ | 21769/24156 [26:44:47<10:19:39, 15.58s/it]
{'loss': 0.3296, 'learning_rate': 1.7798990528229303e-06, 'rewards/chosen': -1.3494995832443237, 'rewards/rejected': -2.981793165206909, 'rewards/accuracies': 0.75, 'rewards/margins': 1.632293462753296, 'policy_logps/rejected': -521.1138916015625, 'policy_logps/chosen': -561.0695190429688, 'referece_logps/rejected': -491.29595947265625, 'referece_logps/chosen': -547.5745849609375, 'logits/rejected': -0.198443204164505, 'logits/chosen': -0.22490166127681732, 'epoch': 8.11}

 90%|█████████ | 21770/24156 [26:45:09<11:31:22, 17.39s/it]


 90%|█████████ | 21772/24156 [26:45:39<10:30:50, 15.88s/it]
{'loss': 0.3639, 'learning_rate': 1.7796472284543388e-06, 'rewards/chosen': -1.0449215173721313, 'rewards/rejected': -3.9078478813171387, 'rewards/accuracies': 0.75, 'rewards/margins': 2.862926483154297, 'policy_logps/rejected': -451.2010498046875, 'policy_logps/chosen': -340.347412109375, 'referece_logps/rejected': -412.1225891113281, 'referece_logps/chosen': -329.898193359375, 'logits/rejected': -0.5311121940612793, 'logits/chosen': -0.3392810523509979, 'epoch': 8.11}

 90%|█████████ | 21773/24156 [26:45:56<10:41:21, 16.15s/it]


 90%|█████████ | 21775/24156 [26:46:33<11:36:21, 17.55s/it]
{'loss': 0.3309, 'learning_rate': 1.77939527794403e-06, 'rewards/chosen': -1.875951886177063, 'rewards/rejected': -4.262351036071777, 'rewards/accuracies': 0.875, 'rewards/margins': 2.386399269104004, 'policy_logps/rejected': -458.052001953125, 'policy_logps/chosen': -315.0912780761719, 'referece_logps/rejected': -415.428466796875, 'referece_logps/chosen': -296.3317565917969, 'logits/rejected': -0.5856810808181763, 'logits/chosen': -0.47902488708496094, 'epoch': 8.11}

 90%|█████████ | 21776/24156 [26:46:54<12:08:23, 18.36s/it]

 90%|█████████ | 21777/24156 [26:47:04<10:36:20, 16.05s/it]


 90%|█████████ | 21779/24156 [26:47:41<11:29:25, 17.40s/it]
{'loss': 0.3419, 'learning_rate': 1.779059147780294e-06, 'rewards/chosen': -1.5746556520462036, 'rewards/rejected': -3.1617486476898193, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5870929956436157, 'policy_logps/rejected': -394.7445373535156, 'policy_logps/chosen': -351.55450439453125, 'referece_logps/rejected': -363.1270446777344, 'referece_logps/chosen': -335.80792236328125, 'logits/rejected': 0.3625880777835846, 'logits/chosen': 0.3788640797138214, 'epoch': 8.11}

 90%|█████████ | 21780/24156 [26:47:52<10:08:42, 15.37s/it]


 90%|█████████ | 21782/24156 [26:48:25<10:37:06, 16.10s/it]
{'loss': 0.3001, 'learning_rate': 1.778806903097872e-06, 'rewards/chosen': -2.395061731338501, 'rewards/rejected': -4.6031494140625, 'rewards/accuracies': 0.875, 'rewards/margins': 2.208087682723999, 'policy_logps/rejected': -638.8079833984375, 'policy_logps/chosen': -420.4505920410156, 'referece_logps/rejected': -592.7765502929688, 'referece_logps/chosen': -396.5000305175781, 'logits/rejected': -0.7245084643363953, 'logits/chosen': -0.8278659582138062, 'epoch': 8.12}

 90%|█████████ | 21783/24156 [26:48:45<11:18:18, 17.15s/it]


 90%|█████████ | 21785/24156 [26:49:17<10:46:38, 16.36s/it]
{'loss': 0.2103, 'learning_rate': 1.7785545324096917e-06, 'rewards/chosen': -1.8398441076278687, 'rewards/rejected': -4.814563274383545, 'rewards/accuracies': 0.875, 'rewards/margins': 2.974719285964966, 'policy_logps/rejected': -328.8403015136719, 'policy_logps/chosen': -288.4075927734375, 'referece_logps/rejected': -280.6946716308594, 'referece_logps/chosen': -270.0091552734375, 'logits/rejected': -1.0418970584869385, 'logits/chosen': -0.785514235496521, 'epoch': 8.12}

 90%|█████████ | 21786/24156 [26:49:30<10:05:11, 15.32s/it]

 90%|█████████ | 21787/24156 [26:49:42<9:30:27, 14.45s/it]

 90%|█████████ | 21788/24156 [26:50:05<11:02:00, 16.77s/it]


 90%|█████████ | 21790/24156 [26:50:30<9:29:22, 14.44s/it]
{'loss': 0.3185, 'learning_rate': 1.7781336346941712e-06, 'rewards/chosen': -1.8967722654342651, 'rewards/rejected': -3.412431240081787, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5156593322753906, 'policy_logps/rejected': -289.2434997558594, 'policy_logps/chosen': -382.8383483886719, 'referece_logps/rejected': -255.11917114257812, 'referece_logps/chosen': -363.87066650390625, 'logits/rejected': -0.38657018542289734, 'logits/chosen': -0.4929520785808563, 'epoch': 8.12}

 90%|█████████ | 21791/24156 [26:50:40<8:45:19, 13.33s/it]

 90%|█████████ | 21792/24156 [26:50:56<9:14:35, 14.08s/it]


 90%|█████████ | 21794/24156 [26:51:32<10:31:17, 16.04s/it]
{'loss': 0.3176, 'learning_rate': 1.7777966647190197e-06, 'rewards/chosen': -3.058820962905884, 'rewards/rejected': -5.639596462249756, 'rewards/accuracies': 1.0, 'rewards/margins': 2.580775499343872, 'policy_logps/rejected': -361.6266174316406, 'policy_logps/chosen': -420.26318359375, 'referece_logps/rejected': -305.23065185546875, 'referece_logps/chosen': -389.6749267578125, 'logits/rejected': 0.32462993264198303, 'logits/chosen': 0.3775562047958374, 'epoch': 8.12}


 90%|█████████ | 21796/24156 [26:52:09<11:19:53, 17.29s/it]
{'loss': 0.2485, 'learning_rate': 1.7776280958305119e-06, 'rewards/chosen': -2.2212207317352295, 'rewards/rejected': -4.942712306976318, 'rewards/accuracies': 0.875, 'rewards/margins': 2.721491575241089, 'policy_logps/rejected': -393.85296630859375, 'policy_logps/chosen': -305.9503479003906, 'referece_logps/rejected': -344.42584228515625, 'referece_logps/chosen': -283.7381591796875, 'logits/rejected': -0.5994861721992493, 'logits/chosen': -0.577802836894989, 'epoch': 8.12}

 90%|█████████ | 21797/24156 [26:52:20<10:01:57, 15.31s/it]

 90%|█████████ | 21798/24156 [26:52:38<10:36:13, 16.19s/it]

 90%|█████████ | 21799/24156 [26:52:55<10:40:14, 16.30s/it]

 90%|█████████ | 21800/24156 [26:53:15<11:25:57, 17.47s/it]

 90%|█████████ | 21801/24156 [26:53:35<11:53:30, 18.18s/it]


 90%|█████████ | 21803/24156 [26:54:13<12:16:59, 18.79s/it]
{'loss': 0.2633, 'learning_rate': 1.777037664447691e-06, 'rewards/chosen': -1.5171252489089966, 'rewards/rejected': -5.421359539031982, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9042341709136963, 'policy_logps/rejected': -311.0126953125, 'policy_logps/chosen': -360.5963134765625, 'referece_logps/rejected': -256.7991027832031, 'referece_logps/chosen': -345.4250793457031, 'logits/rejected': -0.11720028519630432, 'logits/chosen': -0.13807570934295654, 'epoch': 8.12}

 90%|█████████ | 21804/24156 [26:54:33<12:28:18, 19.09s/it]

 90%|█████████ | 21805/24156 [26:54:53<12:34:35, 19.26s/it]

 90%|█████████ | 21806/24156 [26:55:04<11:01:26, 16.89s/it]

 90%|█████████ | 21807/24156 [26:55:21<10:56:28, 16.77s/it]

 90%|█████████ | 21808/24156 [26:55:40<11:27:09, 17.56s/it]

 90%|█████████ | 21809/24156 [26:55:55<10:59:26, 16.86s/it]

 90%|█████████ | 21810/24156 [26:56:11<10:48:25, 16.58s/it]

 90%|█████████ | 21811/24156 [26:56:23<9:45:45, 14.99s/it]

 90%|█████████ | 21812/24156 [26:56:42<10:40:47, 16.40s/it]

 90%|█████████ | 21813/24156 [26:56:54<9:48:17, 15.07s/it]

 90%|█████████ | 21814/24156 [26:57:15<10:59:38, 16.90s/it]

 90%|█████████ | 21815/24156 [26:57:35<11:30:43, 17.70s/it]

 90%|█████████ | 21816/24156 [26:57:52<11:21:59, 17.49s/it]

 90%|█████████ | 21817/24156 [26:58:11<11:36:40, 17.87s/it]

 90%|█████████ | 21818/24156 [26:58:21<10:12:08, 15.71s/it]

 90%|█████████ | 21819/24156 [26:58:40<10:41:28, 16.47s/it]

 90%|█████████ | 21820/24156 [26:58:51<9:46:08, 15.06s/it]

 90%|█████████ | 21821/24156 [26:59:07<9:57:22, 15.35s/it]

 90%|█████████ | 21822/24156 [26:59:25<10:25:25, 16.08s/it]

 90%|█████████ | 21823/24156 [26:59:44<10:53:01, 16.79s/it]

 90%|█████████ | 21824/24156 [27:00:00<10:42:52, 16.54s/it]

 90%|█████████ | 21825/24156 [27:00:11<9:46:13, 15.09s/it]

 90%|█████████ | 21826/24156 [27:00:31<10:38:35, 16.44s/it]

 90%|█████████ | 21827/24156 [27:00:51<11:24:12, 17.63s/it]

 90%|█████████ | 21828/24156 [27:01:06<10:55:48, 16.90s/it]

 90%|█████████ | 21829/24156 [27:01:25<11:10:42, 17.29s/it]

 90%|█████████ | 21830/24156 [27:01:45<11:44:34, 18.17s/it]

 90%|█████████ | 21831/24156 [27:02:05<12:03:26, 18.67s/it]

 90%|█████████ | 21832/24156 [27:02:22<11:47:51, 18.28s/it]

 90%|█████████ | 21833/24156 [27:02:42<12:05:03, 18.73s/it]

 90%|█████████ | 21834/24156 [27:02:57<11:25:02, 17.70s/it]

 90%|█████████ | 21835/24156 [27:03:17<11:50:14, 18.36s/it]

 90%|█████████ | 21836/24156 [27:03:37<12:04:58, 18.75s/it]


 90%|█████████ | 21838/24156 [27:04:13<11:53:12, 18.46s/it]
{'loss': 0.4069, 'learning_rate': 1.7740752508640982e-06, 'rewards/chosen': -1.5150175094604492, 'rewards/rejected': -3.065310478210449, 'rewards/accuracies': 1.0, 'rewards/margins': 1.55029296875, 'policy_logps/rejected': -429.61279296875, 'policy_logps/chosen': -384.77783203125, 'referece_logps/rejected': -398.9596862792969, 'referece_logps/chosen': -369.62762451171875, 'logits/rejected': -1.0327963829040527, 'logits/chosen': -0.9429827928543091, 'epoch': 8.14}

 90%|█████████ | 21839/24156 [27:04:29<11:31:52, 17.92s/it]

 90%|█████████ | 21840/24156 [27:04:40<10:07:19, 15.73s/it]

 90%|█████████ | 21841/24156 [27:04:58<10:30:45, 16.35s/it]

 90%|█████████ | 21842/24156 [27:05:14<10:35:18, 16.47s/it]


 90%|█████████ | 21844/24156 [27:05:53<11:24:13, 17.76s/it]
{'loss': 0.324, 'learning_rate': 1.7735656951075942e-06, 'rewards/chosen': -1.390694260597229, 'rewards/rejected': -3.6370697021484375, 'rewards/accuracies': 0.875, 'rewards/margins': 2.246375322341919, 'policy_logps/rejected': -316.3003234863281, 'policy_logps/chosen': -320.9410400390625, 'referece_logps/rejected': -279.9295959472656, 'referece_logps/chosen': -307.0340576171875, 'logits/rejected': -0.5502643585205078, 'logits/chosen': -0.6884081959724426, 'epoch': 8.14}


 90%|█████████ | 21846/24156 [27:06:23<10:21:38, 16.15s/it]

 90%|█████████ | 21847/24156 [27:06:43<11:04:52, 17.28s/it]
{'loss': 0.4093, 'learning_rate': 1.7733107294720772e-06, 'rewards/chosen': -2.1996986865997314, 'rewards/rejected': -3.229161262512207, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0294625759124756, 'policy_logps/rejected': -435.51922607421875, 'policy_logps/chosen': -515.8948364257812, 'referece_logps/rejected': -403.2276611328125, 'referece_logps/chosen': -493.8978576660156, 'logits/rejected': -0.7421673536300659, 'logits/chosen': -0.8629205822944641, 'epoch': 8.14}


 90%|█████████ | 21849/24156 [27:07:23<11:49:30, 18.45s/it]
{'loss': 0.3696, 'learning_rate': 1.7731406828700108e-06, 'rewards/chosen': -1.4667717218399048, 'rewards/rejected': -3.5339837074279785, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0672123432159424, 'policy_logps/rejected': -379.7479248046875, 'policy_logps/chosen': -392.1832580566406, 'referece_logps/rejected': -344.4080810546875, 'referece_logps/chosen': -377.5155334472656, 'logits/rejected': -0.6165544986724854, 'logits/chosen': -0.8574862480163574, 'epoch': 8.14}

 90%|█████████ | 21850/24156 [27:07:44<12:18:39, 19.22s/it]

 90%|█████████ | 21851/24156 [27:08:02<12:04:22, 18.86s/it]

 90%|█████████ | 21852/24156 [27:08:24<12:41:10, 19.82s/it]

 90%|█████████ | 21853/24156 [27:08:44<12:46:54, 19.98s/it]


 90%|█████████ | 21855/24156 [27:09:17<11:24:17, 17.84s/it]
{'loss': 0.2458, 'learning_rate': 1.772630209542068e-06, 'rewards/chosen': -2.658614158630371, 'rewards/rejected': -4.9749860763549805, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3163716793060303, 'policy_logps/rejected': -434.1556091308594, 'policy_logps/chosen': -371.55645751953125, 'referece_logps/rejected': -384.40570068359375, 'referece_logps/chosen': -344.9703063964844, 'logits/rejected': 0.024910449981689453, 'logits/chosen': 0.05374762415885925, 'epoch': 8.14}

 90%|█████████ | 21856/24156 [27:09:28<10:11:48, 15.96s/it]

 90%|█████████ | 21857/24156 [27:09:48<10:54:36, 17.08s/it]

 90%|█████████ | 21858/24156 [27:10:00<9:56:11, 15.57s/it]

 90%|█████████ | 21859/24156 [27:10:17<10:06:05, 15.83s/it]

 90%|█████████ | 21860/24156 [27:10:34<10:19:10, 16.18s/it]

 90%|█████████ | 21861/24156 [27:10:54<11:01:52, 17.30s/it]

 91%|█████████ | 21862/24156 [27:11:13<11:28:52, 18.02s/it]


 91%|█████████ | 21864/24156 [27:11:49<11:36:27, 18.23s/it]
{'loss': 0.3042, 'learning_rate': 1.7718635621054383e-06, 'rewards/chosen': -1.97366201877594, 'rewards/rejected': -2.7447896003723145, 'rewards/accuracies': 0.5, 'rewards/margins': 0.7711275219917297, 'policy_logps/rejected': -431.9294738769531, 'policy_logps/chosen': -585.22802734375, 'referece_logps/rejected': -404.48150634765625, 'referece_logps/chosen': -565.491455078125, 'logits/rejected': 0.760174572467804, 'logits/chosen': 0.803462564945221, 'epoch': 8.15}

 91%|█████████ | 21865/24156 [27:12:06<11:16:25, 17.72s/it]

 91%|█████████ | 21866/24156 [27:12:25<11:35:27, 18.22s/it]

 91%|█████████ | 21867/24156 [27:12:41<11:14:41, 17.69s/it]

 91%|█████████ | 21868/24156 [27:13:01<11:36:04, 18.25s/it]

 91%|█████████ | 21869/24156 [27:13:15<10:43:46, 16.89s/it]

 91%|█████████ | 21870/24156 [27:13:33<10:57:27, 17.26s/it]

 91%|█████████ | 21871/24156 [27:13:50<10:55:01, 17.20s/it]

 91%|█████████ | 21872/24156 [27:14:07<10:48:46, 17.04s/it]

 91%|█████████ | 21873/24156 [27:14:26<11:18:38, 17.84s/it]

 91%|█████████ | 21874/24156 [27:14:43<11:06:14, 17.52s/it]

 91%|█████████ | 21875/24156 [27:14:59<10:49:45, 17.09s/it]

 91%|█████████ | 21876/24156 [27:15:19<11:22:25, 17.96s/it]

 91%|█████████ | 21877/24156 [27:15:32<10:21:33, 16.36s/it]

 91%|█████████ | 21878/24156 [27:15:47<10:10:27, 16.08s/it]

 91%|█████████ | 21879/24156 [27:16:07<10:51:32, 17.17s/it]

 91%|█████████ | 21880/24156 [27:16:26<11:15:00, 17.79s/it]


 91%|█████████ | 21882/24156 [27:17:02<11:23:39, 18.04s/it]

 91%|█████████ | 21883/24156 [27:17:23<11:57:23, 18.94s/it]

 91%|█████████ | 21884/24156 [27:17:37<11:04:08, 17.54s/it]

 91%|█████████ | 21885/24156 [27:17:53<10:43:51, 17.01s/it]

 91%|█████████ | 21886/24156 [27:18:10<10:46:49, 17.10s/it]

 91%|█████████ | 21887/24156 [27:18:33<11:45:43, 18.66s/it]

 91%|█████████ | 21888/24156 [27:18:50<11:31:29, 18.29s/it]

 91%|█████████ | 21889/24156 [27:19:01<10:06:23, 16.05s/it]

 91%|█████████ | 21890/24156 [27:19:20<10:43:00, 17.03s/it]

 91%|█████████ | 21891/24156 [27:19:31<9:30:52, 15.12s/it]

 91%|█████████ | 21892/24156 [27:19:43<8:50:47, 14.07s/it]

 91%|█████████ | 21893/24156 [27:20:02<9:51:01, 15.67s/it]

 91%|█████████ | 21894/24156 [27:20:14<9:11:45, 14.64s/it]

 91%|█████████ | 21895/24156 [27:20:26<8:37:36, 13.74s/it]

 91%|█████████ | 21896/24156 [27:20:43<9:11:52, 14.65s/it]

 91%|█████████ | 21897/24156 [27:21:03<10:16:35, 16.38s/it]

 91%|█████████ | 21898/24156 [27:21:24<11:04:29, 17.66s/it]

 91%|█████████ | 21899/24156 [27:21:41<11:05:34, 17.69s/it]

 91%|█████████ | 21900/24156 [27:21:53<9:52:38, 15.76s/it]

 91%|█████████ | 21901/24156 [27:22:15<11:05:31, 17.71s/it]

 91%|█████████ | 21902/24156 [27:22:34<11:25:38, 18.25s/it]

 91%|█████████ | 21903/24156 [27:22:47<10:21:47, 16.56s/it]

 91%|█████████ | 21904/24156 [27:23:10<11:27:44, 18.32s/it]

 91%|█████████ | 21905/24156 [27:23:29<11:43:01, 18.74s/it]

 91%|█████████ | 21906/24156 [27:23:49<11:54:49, 19.06s/it]

 91%|█████████ | 21907/24156 [27:24:09<12:00:28, 19.22s/it]

 91%|█████████ | 21908/24156 [27:24:30<12:22:04, 19.81s/it]

 91%|█████████ | 21909/24156 [27:24:50<12:23:46, 19.86s/it]

 91%|█████████ | 21910/24156 [27:25:08<12:07:34, 19.44s/it]

 91%|█████████ | 21911/24156 [27:25:22<10:58:10, 17.59s/it]

 91%|█████████ | 21912/24156 [27:25:41<11:20:02, 18.18s/it]

 91%|█████████ | 21913/24156 [27:25:52<9:56:01, 15.94s/it]

 91%|█████████ | 21914/24156 [27:26:09<10:12:52, 16.40s/it]

 91%|█████████ | 21915/24156 [27:26:20<9:08:16, 14.68s/it]

 91%|█████████ | 21916/24156 [27:26:32<8:34:35, 13.78s/it]

 91%|█████████ | 21917/24156 [27:26:52<9:48:54, 15.78s/it]

 91%|█████████ | 21918/24156 [27:27:08<9:49:03, 15.79s/it]

 91%|█████████ | 21919/24156 [27:27:21<9:18:23, 14.98s/it]

 91%|█████████ | 21920/24156 [27:27:32<8:30:44, 13.71s/it]

 91%|█████████ | 21921/24156 [27:27:43<8:01:39, 12.93s/it]

 91%|█████████ | 21922/24156 [27:27:54<7:36:45, 12.27s/it]

 91%|█████████ | 21923/24156 [27:28:14<9:03:56, 14.62s/it]

 91%|█████████ | 21924/24156 [27:28:26<8:36:50, 13.89s/it]

 91%|█████████ | 21925/24156 [27:28:46<9:40:57, 15.62s/it]

 91%|█████████ | 21926/24156 [27:28:58<9:01:04, 14.56s/it]

 91%|█████████ | 21927/24156 [27:29:12<8:59:17, 14.52s/it]

 91%|█████████ | 21928/24156 [27:29:35<10:33:28, 17.06s/it]

 91%|█████████ | 21929/24156 [27:29:55<11:05:46, 17.94s/it]

 91%|█████████ | 21930/24156 [27:30:08<10:09:31, 16.43s/it]

 91%|█████████ | 21931/24156 [27:30:24<10:08:17, 16.40s/it]

 91%|█████████ | 21932/24156 [27:30:39<9:48:03, 15.86s/it]
{'loss': 0.4191, 'learning_rate': 1.7660348670362071e-06, 'rewards/chosen': -2.148819923400879, 'rewards/rejected': -4.377098560333252, 'rewards/accuracies': 1.0, 'rewards/margins': 2.228278875350952, 'policy_logps/rejected': -507.35040283203125, 'policy_logps/chosen': -440.1318664550781, 'referece_logps/rejected': -463.57940673828125, 'referece_logps/chosen': -418.6437072753906, 'logits/rejected': -0.25686880946159363, 'logits/chosen': -0.39984917640686035, 'epoch': 8.17}


 91%|█████████ | 21934/24156 [27:31:15<10:42:31, 17.35s/it]

 91%|█████████ | 21935/24156 [27:31:29<10:05:51, 16.37s/it]

 91%|█████████ | 21936/24156 [27:31:45<10:01:22, 16.25s/it]

 91%|█████████ | 21937/24156 [27:31:56<9:05:07, 14.74s/it]

 91%|█████████ | 21938/24156 [27:32:16<9:59:01, 16.20s/it]

 91%|█████████ | 21939/24156 [27:32:36<10:46:13, 17.49s/it]

 91%|█████████ | 21940/24156 [27:32:47<9:30:28, 15.45s/it]

 91%|█████████ | 21941/24156 [27:33:05<10:02:29, 16.32s/it]

 91%|█████████ | 21942/24156 [27:33:28<11:10:16, 18.16s/it]

 91%|█████████ | 21943/24156 [27:33:41<10:19:35, 16.80s/it]

 91%|█████████ | 21944/24156 [27:33:55<9:45:07, 15.87s/it]

 91%|█████████ | 21945/24156 [27:34:09<9:25:14, 15.34s/it]
{'loss': 0.2571, 'learning_rate': 1.7649132923212529e-06, 'rewards/chosen': -1.931252121925354, 'rewards/rejected': -5.950296878814697, 'rewards/accuracies': 0.875, 'rewards/margins': 4.019044399261475, 'policy_logps/rejected': -397.2222900390625, 'policy_logps/chosen': -380.8623046875, 'referece_logps/rejected': -337.71929931640625, 'referece_logps/chosen': -361.5498046875, 'logits/rejected': -0.6989096403121948, 'logits/chosen': -0.6196393966674805, 'epoch': 8.18}


 91%|█████████ | 21947/24156 [27:34:42<9:44:56, 15.89s/it]
{'loss': 0.3014, 'learning_rate': 1.7647405360166485e-06, 'rewards/chosen': -2.231834888458252, 'rewards/rejected': -5.099037170410156, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8672022819519043, 'policy_logps/rejected': -466.4286804199219, 'policy_logps/chosen': -464.77099609375, 'referece_logps/rejected': -415.4383239746094, 'referece_logps/chosen': -442.4526672363281, 'logits/rejected': 0.2932512164115906, 'logits/chosen': 0.1805378943681717, 'epoch': 8.18}

 91%|█████████ | 21948/24156 [27:35:03<10:35:28, 17.27s/it]


 91%|█████████ | 21950/24156 [27:35:42<11:19:29, 18.48s/it]

 91%|█████████ | 21951/24156 [27:36:02<11:30:08, 18.78s/it]

 91%|█████████ | 21952/24156 [27:36:16<10:34:40, 17.28s/it]

 91%|█████████ | 21953/24156 [27:36:33<10:33:05, 17.24s/it]

 91%|█████████ | 21954/24156 [27:36:51<10:40:51, 17.46s/it]

 91%|█████████ | 21955/24156 [27:37:02<9:34:51, 15.67s/it]

 91%|█████████ | 21956/24156 [27:37:20<10:00:15, 16.37s/it]
{'loss': 0.2317, 'learning_rate': 1.7639624523108396e-06, 'rewards/chosen': -1.866801381111145, 'rewards/rejected': -5.447868824005127, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5810675621032715, 'policy_logps/rejected': -490.5986328125, 'policy_logps/chosen': -344.0626220703125, 'referece_logps/rejected': -436.11993408203125, 'referece_logps/chosen': -325.3946838378906, 'logits/rejected': -0.02955354005098343, 'logits/chosen': 0.1828797161579132, 'epoch': 8.18}


 91%|█████████ | 21958/24156 [27:37:45<8:47:28, 14.40s/it]

 91%|█████████ | 21959/24156 [27:38:03<9:28:13, 15.52s/it]

 91%|█████████ | 21960/24156 [27:38:15<8:49:37, 14.47s/it]

 91%|█████████ | 21961/24156 [27:38:33<9:27:32, 15.51s/it]
{'loss': 0.3306, 'learning_rate': 1.7635297028305254e-06, 'rewards/chosen': -3.070578098297119, 'rewards/rejected': -6.1243181228637695, 'rewards/accuracies': 0.875, 'rewards/margins': 3.053739309310913, 'policy_logps/rejected': -315.4371032714844, 'policy_logps/chosen': -319.8531494140625, 'referece_logps/rejected': -254.19393920898438, 'referece_logps/chosen': -289.1473693847656, 'logits/rejected': -0.13964182138442993, 'logits/chosen': -0.12164326012134552, 'epoch': 8.18}


 91%|█████████ | 21963/24156 [27:39:11<10:43:50, 17.62s/it]
{'loss': 0.369, 'learning_rate': 1.7633565069454983e-06, 'rewards/chosen': -3.620359420776367, 'rewards/rejected': -5.068673610687256, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4483134746551514, 'policy_logps/rejected': -334.8192138671875, 'policy_logps/chosen': -332.3725280761719, 'referece_logps/rejected': -284.1324768066406, 'referece_logps/chosen': -296.1689147949219, 'logits/rejected': -0.5191212892532349, 'logits/chosen': -0.5684183835983276, 'epoch': 8.18}


 91%|█████████ | 21965/24156 [27:39:53<11:43:34, 19.27s/it]

 91%|█████████ | 21966/24156 [27:40:12<11:36:07, 19.07s/it]

 91%|█████████ | 21967/24156 [27:40:32<11:42:52, 19.27s/it]
{'loss': 0.3766, 'learning_rate': 1.7630099505132505e-06, 'rewards/chosen': -1.6406559944152832, 'rewards/rejected': -4.547861099243164, 'rewards/accuracies': 0.875, 'rewards/margins': 2.907205104827881, 'policy_logps/rejected': -389.099609375, 'policy_logps/chosen': -338.0626220703125, 'referece_logps/rejected': -343.6209716796875, 'referece_logps/chosen': -321.6560363769531, 'logits/rejected': -0.35873305797576904, 'logits/chosen': -0.17952387034893036, 'epoch': 8.18}


 91%|█████████ | 21969/24156 [27:41:10<11:36:27, 19.11s/it]

 91%|█████████ | 21970/24156 [27:41:29<11:35:04, 19.08s/it]

 91%|█████████ | 21971/24156 [27:41:49<11:42:16, 19.28s/it]

 91%|█████████ | 21972/24156 [27:42:08<11:44:36, 19.36s/it]
{'loss': 0.3245, 'learning_rate': 1.7625764463598386e-06, 'rewards/chosen': -2.8481597900390625, 'rewards/rejected': -4.975220680236816, 'rewards/accuracies': 1.0, 'rewards/margins': 2.127060651779175, 'policy_logps/rejected': -347.8213806152344, 'policy_logps/chosen': -402.77587890625, 'referece_logps/rejected': -298.0691833496094, 'referece_logps/chosen': -374.29425048828125, 'logits/rejected': -0.7305847406387329, 'logits/chosen': -0.6836710572242737, 'epoch': 8.19}


 91%|█████████ | 21974/24156 [27:42:42<11:06:59, 18.34s/it]
{'loss': 0.4741, 'learning_rate': 1.7624029487255104e-06, 'rewards/chosen': -2.2635879516601562, 'rewards/rejected': -4.402170658111572, 'rewards/accuracies': 0.875, 'rewards/margins': 2.138582706451416, 'policy_logps/rejected': -448.38922119140625, 'policy_logps/chosen': -350.0856628417969, 'referece_logps/rejected': -404.3675537109375, 'referece_logps/chosen': -327.44976806640625, 'logits/rejected': -0.6625896096229553, 'logits/chosen': -0.616005003452301, 'epoch': 8.19}


 91%|█████████ | 21976/24156 [27:43:20<11:27:20, 18.92s/it]

 91%|█████████ | 21977/24156 [27:43:39<11:19:58, 18.72s/it]
{'loss': 0.3596, 'learning_rate': 1.762142599484827e-06, 'rewards/chosen': -3.0890488624572754, 'rewards/rejected': -5.942687511444092, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8536386489868164, 'policy_logps/rejected': -511.7975769042969, 'policy_logps/chosen': -500.16693115234375, 'referece_logps/rejected': -452.3707275390625, 'referece_logps/chosen': -469.27642822265625, 'logits/rejected': -0.6456968784332275, 'logits/chosen': -0.6531164646148682, 'epoch': 8.19}


 91%|█████████ | 21979/24156 [27:44:09<10:24:29, 17.21s/it]

 91%|█████████ | 21980/24156 [27:44:28<10:48:00, 17.87s/it]

 91%|█████████ | 21981/24156 [27:44:40<9:46:11, 16.17s/it]

 91%|█████████ | 21982/24156 [27:44:53<9:06:56, 15.09s/it]
{'loss': 0.2284, 'learning_rate': 1.7617084100831967e-06, 'rewards/chosen': -2.49837064743042, 'rewards/rejected': -5.474390983581543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.976020574569702, 'policy_logps/rejected': -276.1041259765625, 'policy_logps/chosen': -312.30743408203125, 'referece_logps/rejected': -221.3601837158203, 'referece_logps/chosen': -287.32373046875, 'logits/rejected': -0.7341321110725403, 'logits/chosen': -0.7028550505638123, 'epoch': 8.19}


 91%|█████████ | 21984/24156 [27:45:31<10:28:43, 17.37s/it]

 91%|█████████ | 21985/24156 [27:45:49<10:30:00, 17.41s/it]

 91%|█████████ | 21986/24156 [27:46:08<10:48:51, 17.94s/it]
{'loss': 0.3433, 'learning_rate': 1.761360812073854e-06, 'rewards/chosen': -2.253061532974243, 'rewards/rejected': -3.942344903945923, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6892831325531006, 'policy_logps/rejected': -379.65936279296875, 'policy_logps/chosen': -364.6455078125, 'referece_logps/rejected': -340.23590087890625, 'referece_logps/chosen': -342.1148376464844, 'logits/rejected': -1.0787785053253174, 'logits/chosen': -0.9881121516227722, 'epoch': 8.19}


 91%|█████████ | 21988/24156 [27:46:40<10:03:37, 16.71s/it]

 91%|█████████ | 21989/24156 [27:46:57<10:04:28, 16.74s/it]

 91%|█████████ | 21990/24156 [27:47:16<10:34:22, 17.57s/it]
{'loss': 0.3402, 'learning_rate': 1.76101299507234e-06, 'rewards/chosen': -1.843316912651062, 'rewards/rejected': -4.8762030601501465, 'rewards/accuracies': 0.875, 'rewards/margins': 3.032885789871216, 'policy_logps/rejected': -510.44647216796875, 'policy_logps/chosen': -505.81219482421875, 'referece_logps/rejected': -461.6844482421875, 'referece_logps/chosen': -487.3789978027344, 'logits/rejected': -0.5804972052574158, 'logits/chosen': -0.5225281715393066, 'epoch': 8.19}


 91%|█████████ | 21992/24156 [27:47:51<10:16:25, 17.09s/it]
{'loss': 0.2918, 'learning_rate': 1.76083900448078e-06, 'rewards/chosen': -1.8857537508010864, 'rewards/rejected': -5.273967266082764, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3882129192352295, 'policy_logps/rejected': -238.57382202148438, 'policy_logps/chosen': -295.6759948730469, 'referece_logps/rejected': -185.8341522216797, 'referece_logps/chosen': -276.8184814453125, 'logits/rejected': -0.5484753847122192, 'logits/chosen': -0.7641882300376892, 'epoch': 8.19}


 91%|█████████ | 21994/24156 [27:48:20<9:28:08, 15.77s/it]

 91%|█████████ | 21995/24156 [27:48:38<9:51:38, 16.43s/it]

 91%|█████████ | 21996/24156 [27:48:49<8:50:25, 14.73s/it]

 91%|█████████ | 21997/24156 [27:49:01<8:24:48, 14.03s/it]

 91%|█████████ | 21998/24156 [27:49:21<9:27:06, 15.77s/it]

 91%|█████████ | 21999/24156 [27:49:41<10:13:21, 17.06s/it]

 91%|█████████ | 22000/24156 [27:50:01<10:41:24, 17.85s/it]

 91%|█████████ | 22001/24156 [27:50:28<12:25:01, 20.74s/it]
{'loss': 0.4732, 'learning_rate': 1.7600553699567808e-06, 'rewards/chosen': -1.6387884616851807, 'rewards/rejected': -3.094672679901123, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4558842182159424, 'policy_logps/rejected': -419.6968688964844, 'policy_logps/chosen': -579.0905151367188, 'referece_logps/rejected': -388.7501525878906, 'referece_logps/chosen': -562.70263671875, 'logits/rejected': -0.6441913843154907, 'logits/chosen': -0.7099133729934692, 'epoch': 8.2}


 91%|█████████ | 22003/24156 [27:50:50<9:25:58, 15.77s/it]

 91%|█████████ | 22004/24156 [27:51:06<9:26:38, 15.80s/it]

 91%|█████████ | 22005/24156 [27:51:28<10:25:32, 17.45s/it]
{'loss': 0.2815, 'learning_rate': 1.7597067326261519e-06, 'rewards/chosen': -1.727781057357788, 'rewards/rejected': -3.938267230987549, 'rewards/accuracies': 0.875, 'rewards/margins': 2.21048641204834, 'policy_logps/rejected': -310.12664794921875, 'policy_logps/chosen': -282.341552734375, 'referece_logps/rejected': -270.7439880371094, 'referece_logps/chosen': -265.06378173828125, 'logits/rejected': 0.06607849895954132, 'logits/chosen': 0.1740749627351761, 'epoch': 8.2}


 91%|█████████ | 22007/24156 [27:52:04<10:41:05, 17.90s/it]

 91%|█████████ | 22008/24156 [27:52:24<11:04:28, 18.56s/it]

 91%|█████████ | 22009/24156 [27:52:44<11:21:24, 19.04s/it]

 91%|█████████ | 22010/24156 [27:53:04<11:28:48, 19.26s/it]

 91%|█████████ | 22011/24156 [27:53:27<12:02:52, 20.22s/it]
{'loss': 0.29, 'learning_rate': 1.7591833669433057e-06, 'rewards/chosen': -2.4309780597686768, 'rewards/rejected': -4.452697277069092, 'rewards/accuracies': 0.875, 'rewards/margins': 2.021719455718994, 'policy_logps/rejected': -382.69940185546875, 'policy_logps/chosen': -315.1779479980469, 'referece_logps/rejected': -338.1724548339844, 'referece_logps/chosen': -290.8681945800781, 'logits/rejected': -0.9128608107566833, 'logits/chosen': -0.8831151127815247, 'epoch': 8.2}

 91%|█████████ | 22012/24156 [27:53:44<11:31:53, 19.36s/it]


 91%|█████████ | 22014/24156 [27:54:17<10:16:43, 17.28s/it]

 91%|█████████ | 22015/24156 [27:54:39<11:10:41, 18.80s/it]

 91%|█████████ | 22016/24156 [27:54:59<11:28:36, 19.31s/it]

 91%|█████████ | 22017/24156 [27:55:17<11:14:44, 18.93s/it]

 91%|█████████ | 22018/24156 [27:55:33<10:37:44, 17.90s/it]

 91%|█████████ | 22019/24156 [27:55:53<10:59:05, 18.51s/it]
{'loss': 0.2448, 'learning_rate': 1.7584847818109205e-06, 'rewards/chosen': -1.560364842414856, 'rewards/rejected': -5.384507179260254, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8241422176361084, 'policy_logps/rejected': -351.12030029296875, 'policy_logps/chosen': -407.2299499511719, 'referece_logps/rejected': -297.2752380371094, 'referece_logps/chosen': -391.6263427734375, 'logits/rejected': -0.5362900495529175, 'logits/chosen': -0.7156124114990234, 'epoch': 8.2}

 91%|█████████ | 22020/24156 [27:56:10<10:46:28, 18.16s/it]


 91%|█████████ | 22022/24156 [27:56:48<11:05:19, 18.71s/it]
{'loss': 0.3095, 'learning_rate': 1.7582225873605055e-06, 'rewards/chosen': -2.642315626144409, 'rewards/rejected': -5.289428234100342, 'rewards/accuracies': 0.75, 'rewards/margins': 2.6471128463745117, 'policy_logps/rejected': -342.9433898925781, 'policy_logps/chosen': -317.30841064453125, 'referece_logps/rejected': -290.0491027832031, 'referece_logps/chosen': -290.88525390625, 'logits/rejected': -1.0249857902526855, 'logits/chosen': -0.9711581468582153, 'epoch': 8.2}

 91%|█████████ | 22023/24156 [27:57:08<11:24:01, 19.24s/it]

 91%|█████████ | 22024/24156 [27:57:29<11:37:46, 19.64s/it]


 91%|█████████ | 22026/24156 [27:58:08<11:32:36, 19.51s/it]

 91%|█████████ | 22027/24156 [27:58:21<10:26:23, 17.65s/it]

 91%|█████████ | 22028/24156 [27:58:44<11:19:41, 19.16s/it]

 91%|█████████ | 22029/24156 [27:59:04<11:27:47, 19.40s/it]

 91%|█████████ | 22030/24156 [27:59:22<11:15:35, 19.07s/it]

 91%|█████████ | 22031/24156 [27:59:42<11:21:29, 19.24s/it]

 91%|█████████ | 22032/24156 [28:00:01<11:24:55, 19.35s/it]
{'loss': 0.3261, 'learning_rate': 1.757347720108897e-06, 'rewards/chosen': -1.7442175149917603, 'rewards/rejected': -3.353224277496338, 'rewards/accuracies': 0.625, 'rewards/margins': 1.609006643295288, 'policy_logps/rejected': -345.78662109375, 'policy_logps/chosen': -422.1669921875, 'referece_logps/rejected': -312.25433349609375, 'referece_logps/chosen': -404.724853515625, 'logits/rejected': -0.41776713728904724, 'logits/chosen': -0.511120080947876, 'epoch': 8.21}


 91%|█████████ | 22034/24156 [28:00:26<9:14:23, 15.68s/it]
{'loss': 0.394, 'learning_rate': 1.7571725832298187e-06, 'rewards/chosen': -1.9948652982711792, 'rewards/rejected': -2.9137394428253174, 'rewards/accuracies': 0.875, 'rewards/margins': 0.918874204158783, 'policy_logps/rejected': -503.25848388671875, 'policy_logps/chosen': -512.035400390625, 'referece_logps/rejected': -474.12109375, 'referece_logps/chosen': -492.086669921875, 'logits/rejected': 0.3599494695663452, 'logits/chosen': 0.6522148251533508, 'epoch': 8.21}


 91%|█████████ | 22036/24156 [28:01:02<9:51:50, 16.75s/it]

 91%|█████████ | 22037/24156 [28:01:16<9:24:06, 15.97s/it]
{'loss': 0.3661, 'learning_rate': 1.756909775827245e-06, 'rewards/chosen': -2.3558385372161865, 'rewards/rejected': -4.265411853790283, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9095735549926758, 'policy_logps/rejected': -434.00726318359375, 'policy_logps/chosen': -351.57696533203125, 'referece_logps/rejected': -391.3531494140625, 'referece_logps/chosen': -328.0185546875, 'logits/rejected': -0.2521161139011383, 'logits/chosen': -0.20023754239082336, 'epoch': 8.21}

 91%|█████████ | 22038/24156 [28:01:37<10:14:36, 17.41s/it]

 91%|█████████ | 22039/24156 [28:01:48<9:14:21, 15.71s/it]

 91%|█████████ | 22040/24156 [28:01:59<8:22:37, 14.25s/it]


 91%|█████████ | 22042/24156 [28:02:36<9:21:34, 15.94s/it]

 91%|█████████▏| 22043/24156 [28:02:51<9:14:11, 15.74s/it]

 91%|█████████▏| 22044/24156 [28:03:02<8:18:36, 14.17s/it]
{'loss': 0.2644, 'learning_rate': 1.756296082383299e-06, 'rewards/chosen': -2.0526182651519775, 'rewards/rejected': -4.665135860443115, 'rewards/accuracies': 0.875, 'rewards/margins': 2.612517833709717, 'policy_logps/rejected': -254.026123046875, 'policy_logps/chosen': -409.74664306640625, 'referece_logps/rejected': -207.37474060058594, 'referece_logps/chosen': -389.220458984375, 'logits/rejected': -0.9555531740188599, 'logits/chosen': -0.8240457773208618, 'epoch': 8.21}


 91%|█████████▏| 22046/24156 [28:03:38<9:31:57, 16.26s/it]

 91%|█████████▏| 22047/24156 [28:03:55<9:45:22, 16.65s/it]
{'loss': 0.3478, 'learning_rate': 1.7560328669363623e-06, 'rewards/chosen': -3.3828697204589844, 'rewards/rejected': -6.254641532897949, 'rewards/accuracies': 1.0, 'rewards/margins': 2.871772289276123, 'policy_logps/rejected': -414.3591003417969, 'policy_logps/chosen': -420.2664794921875, 'referece_logps/rejected': -351.8126525878906, 'referece_logps/chosen': -386.43780517578125, 'logits/rejected': -0.3275182247161865, 'logits/chosen': -0.4281497597694397, 'epoch': 8.21}

 91%|█████████▏| 22048/24156 [28:04:11<9:32:21, 16.29s/it]

 91%|█████████▏| 22049/24156 [28:04:29<9:52:42, 16.88s/it]

 91%|█████████▏| 22050/24156 [28:04:42<9:14:49, 15.81s/it]


 91%|█████████▏| 22052/24156 [28:05:20<10:06:35, 17.30s/it]

 91%|█████████▏| 22053/24156 [28:05:40<10:31:51, 18.03s/it]

 91%|█████████▏| 22054/24156 [28:05:54<9:47:50, 16.78s/it]
{'loss': 0.3922, 'learning_rate': 1.755418221940764e-06, 'rewards/chosen': -3.299916982650757, 'rewards/rejected': -3.974479913711548, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6745628714561462, 'policy_logps/rejected': -319.03289794921875, 'policy_logps/chosen': -335.7386169433594, 'referece_logps/rejected': -279.2881164550781, 'referece_logps/chosen': -302.7394714355469, 'logits/rejected': -0.2256706953048706, 'logits/chosen': -0.2640637457370758, 'epoch': 8.22}

 91%|█████████▏| 22055/24156 [28:06:15<10:31:08, 18.02s/it]


 91%|█████████▏| 22057/24156 [28:06:53<10:34:31, 18.14s/it]

 91%|█████████▏| 22058/24156 [28:07:12<10:51:07, 18.62s/it]

 91%|█████████▏| 22059/24156 [28:07:34<11:23:59, 19.57s/it]
{'loss': 0.2249, 'learning_rate': 1.7549787823640618e-06, 'rewards/chosen': -1.9338433742523193, 'rewards/rejected': -4.73714017868042, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8032970428466797, 'policy_logps/rejected': -326.0746154785156, 'policy_logps/chosen': -298.64410400390625, 'referece_logps/rejected': -278.7032470703125, 'referece_logps/chosen': -279.3056640625, 'logits/rejected': -0.9939902424812317, 'logits/chosen': -0.9046074748039246, 'epoch': 8.22}

 91%|█████████▏| 22060/24156 [28:07:47<10:13:30, 17.56s/it]


 91%|█████████▏| 22062/24156 [28:08:29<11:07:48, 19.13s/it]

 91%|█████████▏| 22063/24156 [28:08:44<10:28:28, 18.02s/it]

 91%|█████████▏| 22064/24156 [28:09:00<10:05:02, 17.35s/it]
{'loss': 0.2772, 'learning_rate': 1.7545390034803465e-06, 'rewards/chosen': -2.4826719760894775, 'rewards/rejected': -3.425548553466797, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9428765773773193, 'policy_logps/rejected': -306.1989440917969, 'policy_logps/chosen': -379.2252502441406, 'referece_logps/rejected': -271.9434814453125, 'referece_logps/chosen': -354.39849853515625, 'logits/rejected': -0.2204505205154419, 'logits/chosen': -0.22144047915935516, 'epoch': 8.22}


 91%|█████████▏| 22066/24156 [28:09:40<10:51:49, 18.71s/it]
{'loss': 0.1524, 'learning_rate': 1.7543629969651645e-06, 'rewards/chosen': -1.7593510150909424, 'rewards/rejected': -5.330272674560547, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5709216594696045, 'policy_logps/rejected': -318.37689208984375, 'policy_logps/chosen': -355.81256103515625, 'referece_logps/rejected': -265.0741882324219, 'referece_logps/chosen': -338.21905517578125, 'logits/rejected': 0.06794630736112595, 'logits/chosen': 0.04922465234994888, 'epoch': 8.22}


 91%|█████████▏| 22068/24156 [28:10:19<11:09:17, 19.23s/it]
{'loss': 0.2205, 'learning_rate': 1.754186936205139e-06, 'rewards/chosen': -2.5279507637023926, 'rewards/rejected': -3.7905426025390625, 'rewards/accuracies': 0.75, 'rewards/margins': 1.262592077255249, 'policy_logps/rejected': -392.04046630859375, 'policy_logps/chosen': -344.723876953125, 'referece_logps/rejected': -354.135009765625, 'referece_logps/chosen': -319.4443359375, 'logits/rejected': -0.9323616623878479, 'logits/chosen': -0.9951832890510559, 'epoch': 8.22}


 91%|█████████▏| 22070/24156 [28:10:52<10:12:52, 17.63s/it]
{'loss': 0.2892, 'learning_rate': 1.7540108212129297e-06, 'rewards/chosen': -2.1601247787475586, 'rewards/rejected': -3.9379234313964844, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7777986526489258, 'policy_logps/rejected': -358.6790466308594, 'policy_logps/chosen': -315.2615051269531, 'referece_logps/rejected': -319.2998046875, 'referece_logps/chosen': -293.6602478027344, 'logits/rejected': -0.8682675957679749, 'logits/chosen': -0.8530796766281128, 'epoch': 8.22}

 91%|█████████▏| 22071/24156 [28:11:05<9:26:13, 16.29s/it]

 91%|█████████▏| 22072/24156 [28:11:27<10:24:52, 17.99s/it]


 91%|█████████▏| 22074/24156 [28:12:03<10:26:57, 18.07s/it]
{'loss': 0.3197, 'learning_rate': 1.7536584285826206e-06, 'rewards/chosen': -1.521817684173584, 'rewards/rejected': -4.487508296966553, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9656906127929688, 'policy_logps/rejected': -411.3471374511719, 'policy_logps/chosen': -480.8197326660156, 'referece_logps/rejected': -366.47210693359375, 'referece_logps/chosen': -465.6015625, 'logits/rejected': 0.47399336099624634, 'logits/chosen': 0.47757792472839355, 'epoch': 8.22}


 91%|█████████▏| 22076/24156 [28:12:38<10:27:13, 18.09s/it]
{'loss': 0.3105, 'learning_rate': 1.7534821509698604e-06, 'rewards/chosen': -2.0531222820281982, 'rewards/rejected': -4.067549705505371, 'rewards/accuracies': 0.875, 'rewards/margins': 2.014427661895752, 'policy_logps/rejected': -450.2090148925781, 'policy_logps/chosen': -514.4443359375, 'referece_logps/rejected': -409.53350830078125, 'referece_logps/chosen': -493.9131164550781, 'logits/rejected': 0.09649409353733063, 'logits/chosen': 0.1271517127752304, 'epoch': 8.23}

 91%|█████████▏| 22077/24156 [28:12:58<10:44:08, 18.59s/it]


 91%|█████████▏| 22079/24156 [28:13:27<9:24:22, 16.30s/it]

 91%|█████████▏| 22080/24156 [28:13:39<8:42:19, 15.10s/it]

 91%|█████████▏| 22081/24156 [28:13:57<9:11:22, 15.94s/it]

 91%|█████████▏| 22082/24156 [28:14:17<9:53:38, 17.17s/it]

 91%|█████████▏| 22083/24156 [28:14:36<10:16:39, 17.85s/it]

 91%|█████████▏| 22084/24156 [28:14:54<10:17:47, 17.89s/it]
{'loss': 0.2125, 'learning_rate': 1.7527764988305989e-06, 'rewards/chosen': -3.347993850708008, 'rewards/rejected': -5.915802478790283, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5678083896636963, 'policy_logps/rejected': -351.4158935546875, 'policy_logps/chosen': -382.76898193359375, 'referece_logps/rejected': -292.2579040527344, 'referece_logps/chosen': -349.2890625, 'logits/rejected': -0.27656620740890503, 'logits/chosen': -0.39634501934051514, 'epoch': 8.23}

 91%|█████████▏| 22085/24156 [28:15:08<9:34:07, 16.63s/it]

 91%|█████████▏| 22086/24156 [28:15:28<10:08:19, 17.63s/it]

 91%|█████████▏| 22087/24156 [28:15:48<10:30:07, 18.27s/it]

 91%|█████████▏| 22088/24156 [28:16:08<10:51:47, 18.91s/it]

 91%|█████████▏| 22089/24156 [28:16:24<10:20:16, 18.01s/it]

 91%|█████████▏| 22090/24156 [28:16:36<9:14:22, 16.10s/it]

 91%|█████████▏| 22091/24156 [28:16:55<9:52:44, 17.22s/it]


 91%|█████████▏| 22093/24156 [28:17:25<9:04:16, 15.83s/it]

 91%|█████████▏| 22094/24156 [28:17:47<10:06:54, 17.66s/it]

 91%|█████████▏| 22095/24156 [28:18:04<10:05:38, 17.63s/it]

 91%|█████████▏| 22096/24156 [28:18:23<10:13:43, 17.88s/it]
{'loss': 0.2721, 'learning_rate': 1.7517163969529841e-06, 'rewards/chosen': -2.0074310302734375, 'rewards/rejected': -6.127415180206299, 'rewards/accuracies': 1.0, 'rewards/margins': 4.119983673095703, 'policy_logps/rejected': -471.4306945800781, 'policy_logps/chosen': -381.6238708496094, 'referece_logps/rejected': -410.1565246582031, 'referece_logps/chosen': -361.549560546875, 'logits/rejected': -0.3133792281150818, 'logits/chosen': -0.29494789242744446, 'epoch': 8.23}


 91%|█████████▏| 22098/24156 [28:18:49<8:45:15, 15.31s/it]

 91%|█████████▏| 22099/24156 [28:19:01<8:04:11, 14.12s/it]

 91%|█████████▏| 22100/24156 [28:19:11<7:28:26, 13.09s/it]
{'loss': 0.3384, 'learning_rate': 1.7513625970886177e-06, 'rewards/chosen': -1.852250576019287, 'rewards/rejected': -4.448430061340332, 'rewards/accuracies': 1.0, 'rewards/margins': 2.596179723739624, 'policy_logps/rejected': -341.8934631347656, 'policy_logps/chosen': -418.1219482421875, 'referece_logps/rejected': -297.4091796875, 'referece_logps/chosen': -399.5994873046875, 'logits/rejected': -0.4398205578327179, 'logits/chosen': -0.6432861089706421, 'epoch': 8.23}

 91%|█████████▏| 22101/24156 [28:19:27<7:59:18, 13.99s/it]

 91%|█████████▏| 22102/24156 [28:19:38<7:26:04, 13.03s/it]

 92%|█████████▏| 22103/24156 [28:20:00<8:51:02, 15.52s/it]


 92%|█████████▏| 22105/24156 [28:20:31<8:50:47, 15.53s/it]
{'loss': 0.3087, 'learning_rate': 1.7509200433564615e-06, 'rewards/chosen': -2.021667957305908, 'rewards/rejected': -3.8759233951568604, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8542553186416626, 'policy_logps/rejected': -328.9873046875, 'policy_logps/chosen': -356.9616394042969, 'referece_logps/rejected': -290.22808837890625, 'referece_logps/chosen': -336.7449645996094, 'logits/rejected': 0.3246387243270874, 'logits/chosen': 0.2643571197986603, 'epoch': 8.24}

 92%|█████████▏| 22106/24156 [28:20:48<9:03:54, 15.92s/it]

 92%|█████████▏| 22107/24156 [28:21:04<9:01:46, 15.86s/it]


 92%|█████████▏| 22109/24156 [28:21:32<8:28:32, 14.91s/it]

 92%|█████████▏| 22110/24156 [28:21:47<8:20:01, 14.66s/it]

 92%|█████████▏| 22111/24156 [28:22:03<8:36:41, 15.16s/it]

 92%|█████████▏| 22112/24156 [28:22:21<9:02:07, 15.91s/it]
{'loss': 0.2276, 'learning_rate': 1.750299901204734e-06, 'rewards/chosen': -1.48909330368042, 'rewards/rejected': -4.330864906311035, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8417716026306152, 'policy_logps/rejected': -349.0133056640625, 'policy_logps/chosen': -362.887451171875, 'referece_logps/rejected': -305.70465087890625, 'referece_logps/chosen': -347.9964599609375, 'logits/rejected': -0.29946839809417725, 'logits/chosen': -0.2587437033653259, 'epoch': 8.24}


 92%|█████████▏| 22114/24156 [28:22:51<8:46:32, 15.47s/it]
{'loss': 0.2249, 'learning_rate': 1.7501225963153993e-06, 'rewards/chosen': -0.7124466896057129, 'rewards/rejected': -4.24624490737915, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5337984561920166, 'policy_logps/rejected': -363.6131896972656, 'policy_logps/chosen': -394.2891845703125, 'referece_logps/rejected': -321.1507263183594, 'referece_logps/chosen': -387.164794921875, 'logits/rejected': -0.3425542712211609, 'logits/chosen': -0.4682692885398865, 'epoch': 8.24}

 92%|█████████▏| 22115/24156 [28:23:08<8:58:36, 15.83s/it]


 92%|█████████▏| 22117/24156 [28:23:49<10:14:32, 18.08s/it]

 92%|█████████▏| 22118/24156 [28:24:09<10:38:53, 18.81s/it]

 92%|█████████▏| 22119/24156 [28:24:27<10:32:00, 18.62s/it]

 92%|█████████▏| 22120/24156 [28:24:45<10:26:36, 18.47s/it]

 92%|█████████▏| 22121/24156 [28:24:57<9:17:24, 16.43s/it]
{'loss': 0.257, 'learning_rate': 1.7495016045095276e-06, 'rewards/chosen': -2.685607671737671, 'rewards/rejected': -6.741119384765625, 'rewards/accuracies': 0.875, 'rewards/margins': 4.055511474609375, 'policy_logps/rejected': -230.046142578125, 'policy_logps/chosen': -398.70343017578125, 'referece_logps/rejected': -162.6349639892578, 'referece_logps/chosen': -371.84735107421875, 'logits/rejected': -0.3898073434829712, 'logits/chosen': -0.553687572479248, 'epoch': 8.24}

 92%|█████████▏| 22122/24156 [28:25:15<9:26:19, 16.71s/it]

 92%|█████████▏| 22123/24156 [28:25:26<8:37:49, 15.28s/it]

 92%|█████████▏| 22124/24156 [28:25:49<9:47:36, 17.35s/it]

 92%|█████████▏| 22125/24156 [28:26:11<10:34:37, 18.75s/it]


 92%|█████████▏| 22127/24156 [28:26:47<10:23:19, 18.43s/it]
{'loss': 0.2312, 'learning_rate': 1.7489688003191838e-06, 'rewards/chosen': -2.2188727855682373, 'rewards/rejected': -5.21845817565918, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9995856285095215, 'policy_logps/rejected': -327.6200866699219, 'policy_logps/chosen': -517.4083251953125, 'referece_logps/rejected': -275.4355163574219, 'referece_logps/chosen': -495.2196044921875, 'logits/rejected': -0.5351604223251343, 'logits/chosen': -0.6946448087692261, 'epoch': 8.24}

 92%|█████████▏| 22128/24156 [28:27:02<9:44:12, 17.28s/it]

 92%|█████████▏| 22129/24156 [28:27:17<9:17:35, 16.50s/it]

 92%|█████████▏| 22130/24156 [28:27:33<9:17:42, 16.52s/it]

 92%|█████████▏| 22131/24156 [28:27:53<9:49:34, 17.47s/it]

 92%|█████████▏| 22132/24156 [28:28:11<9:54:54, 17.64s/it]

 92%|█████████▏| 22133/24156 [28:28:31<10:15:52, 18.27s/it]

 92%|█████████▏| 22134/24156 [28:28:47<9:59:24, 17.79s/it]

 92%|█████████▏| 22135/24156 [28:29:07<10:13:40, 18.22s/it]

 92%|█████████▏| 22136/24156 [28:29:27<10:30:44, 18.74s/it]


 92%|█████████▏| 22138/24156 [28:29:58<9:37:58, 17.18s/it]
{'loss': 0.3929, 'learning_rate': 1.747990733979559e-06, 'rewards/chosen': -2.034360647201538, 'rewards/rejected': -3.6500790119171143, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6157184839248657, 'policy_logps/rejected': -491.9803771972656, 'policy_logps/chosen': -325.4971618652344, 'referece_logps/rejected': -455.4796142578125, 'referece_logps/chosen': -305.153564453125, 'logits/rejected': -0.21766452491283417, 'logits/chosen': -0.2810961902141571, 'epoch': 8.25}

 92%|█████████▏| 22139/24156 [28:30:12<9:10:16, 16.37s/it]

 92%|█████████▏| 22140/24156 [28:30:30<9:26:37, 16.86s/it]


 92%|█████████▏| 22142/24156 [28:31:04<9:25:59, 16.86s/it]
{'loss': 0.3679, 'learning_rate': 1.7476346699807963e-06, 'rewards/chosen': -2.311122179031372, 'rewards/rejected': -4.2745137214660645, 'rewards/accuracies': 0.75, 'rewards/margins': 1.963391661643982, 'policy_logps/rejected': -525.9805297851562, 'policy_logps/chosen': -437.5277099609375, 'referece_logps/rejected': -483.2353820800781, 'referece_logps/chosen': -414.4164733886719, 'logits/rejected': -1.5047047138214111, 'logits/chosen': -1.378070592880249, 'epoch': 8.25}


 92%|█████████▏| 22144/24156 [28:31:42<9:53:32, 17.70s/it]

 92%|█████████▏| 22145/24156 [28:31:58<9:39:45, 17.30s/it]
{'loss': 0.2773, 'learning_rate': 1.7473674808534403e-06, 'rewards/chosen': -2.341395378112793, 'rewards/rejected': -5.803084850311279, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4616899490356445, 'policy_logps/rejected': -294.51531982421875, 'policy_logps/chosen': -383.47625732421875, 'referece_logps/rejected': -236.48446655273438, 'referece_logps/chosen': -360.06231689453125, 'logits/rejected': -0.7125065922737122, 'logits/chosen': -0.8770390152931213, 'epoch': 8.25}


 92%|█████████▏| 22147/24156 [28:32:34<9:50:11, 17.63s/it]
{'loss': 0.2217, 'learning_rate': 1.747189287588606e-06, 'rewards/chosen': -3.501786470413208, 'rewards/rejected': -5.972846508026123, 'rewards/accuracies': 1.0, 'rewards/margins': 2.471060037612915, 'policy_logps/rejected': -504.04376220703125, 'policy_logps/chosen': -514.2471313476562, 'referece_logps/rejected': -444.3153076171875, 'referece_logps/chosen': -479.22930908203125, 'logits/rejected': -0.03290996700525284, 'logits/chosen': 0.0828963965177536, 'epoch': 8.25}

 92%|█████████▏| 22148/24156 [28:32:51<9:37:12, 17.25s/it]


 92%|█████████▏| 22150/24156 [28:33:22<9:22:25, 16.82s/it]
{'loss': 0.2854, 'learning_rate': 1.7469218969534925e-06, 'rewards/chosen': -2.5170559883117676, 'rewards/rejected': -4.2457427978515625, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7286869287490845, 'policy_logps/rejected': -411.0282897949219, 'policy_logps/chosen': -380.0828857421875, 'referece_logps/rejected': -368.57086181640625, 'referece_logps/chosen': -354.91229248046875, 'logits/rejected': -0.913481593132019, 'logits/chosen': -0.955612063407898, 'epoch': 8.25}

 92%|█████████▏| 22151/24156 [28:33:40<9:37:57, 17.30s/it]

 92%|█████████▏| 22152/24156 [28:33:51<8:31:38, 15.32s/it]

 92%|█████████▏| 22153/24156 [28:34:11<9:14:50, 16.62s/it]

 92%|█████████▏| 22154/24156 [28:34:30<9:38:01, 17.32s/it]


 92%|█████████▏| 22156/24156 [28:35:10<10:26:55, 18.81s/it]
{'loss': 0.3057, 'learning_rate': 1.746386753185615e-06, 'rewards/chosen': -2.4173882007598877, 'rewards/rejected': -5.345091819763184, 'rewards/accuracies': 0.875, 'rewards/margins': 2.927703380584717, 'policy_logps/rejected': -447.98553466796875, 'policy_logps/chosen': -408.9844055175781, 'referece_logps/rejected': -394.5346374511719, 'referece_logps/chosen': -384.8105163574219, 'logits/rejected': -0.689007580280304, 'logits/chosen': -0.6671784520149231, 'epoch': 8.25}


 92%|█████████▏| 22158/24156 [28:35:44<9:50:36, 17.74s/it]
{'loss': 0.2018, 'learning_rate': 1.7462082645699757e-06, 'rewards/chosen': -1.7920987606048584, 'rewards/rejected': -5.562002658843994, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7699036598205566, 'policy_logps/rejected': -410.079833984375, 'policy_logps/chosen': -466.47088623046875, 'referece_logps/rejected': -354.4598693847656, 'referece_logps/chosen': -448.5498962402344, 'logits/rejected': -0.21077947318553925, 'logits/chosen': -0.3443939685821533, 'epoch': 8.26}

 92%|█████████▏| 22159/24156 [28:36:04<10:08:17, 18.28s/it]

 92%|█████████▏| 22160/24156 [28:36:22<10:05:35, 18.20s/it]

 92%|█████████▏| 22161/24156 [28:36:40<10:04:15, 18.17s/it]

 92%|█████████▏| 22162/24156 [28:36:57<9:59:51, 18.05s/it]


 92%|█████████▏| 22164/24156 [28:37:32<9:55:56, 17.95s/it]
{'loss': 0.349, 'learning_rate': 1.745672476823702e-06, 'rewards/chosen': -2.0900611877441406, 'rewards/rejected': -2.968686103820801, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8786249756813049, 'policy_logps/rejected': -375.47467041015625, 'policy_logps/chosen': -313.5856018066406, 'referece_logps/rejected': -345.787841796875, 'referece_logps/chosen': -292.6849670410156, 'logits/rejected': 0.5502196550369263, 'logits/chosen': 0.3946306109428406, 'epoch': 8.26}

 92%|█████████▏| 22165/24156 [28:37:52<10:12:02, 18.44s/it]

 92%|█████████▏| 22166/24156 [28:38:09<9:52:23, 17.86s/it]

 92%|█████████▏| 22167/24156 [28:38:27<9:55:56, 17.98s/it]

 92%|█████████▏| 22168/24156 [28:38:39<8:58:32, 16.25s/it]


 92%|█████████▏| 22170/24156 [28:39:18<9:55:57, 18.00s/it]

 92%|█████████▏| 22171/24156 [28:39:30<8:55:35, 16.19s/it]
{'loss': 0.2816, 'learning_rate': 1.7450467812174674e-06, 'rewards/chosen': -3.5045058727264404, 'rewards/rejected': -5.516202449798584, 'rewards/accuracies': 0.625, 'rewards/margins': 2.0116968154907227, 'policy_logps/rejected': -415.9940185546875, 'policy_logps/chosen': -457.1364440917969, 'referece_logps/rejected': -360.83197021484375, 'referece_logps/chosen': -422.09136962890625, 'logits/rejected': -0.23875227570533752, 'logits/chosen': -0.26914912462234497, 'epoch': 8.26}

 92%|█████████▏| 22172/24156 [28:39:49<9:16:21, 16.83s/it]

 92%|█████████▏| 22173/24156 [28:40:10<10:03:04, 18.25s/it]

 92%|█████████▏| 22174/24156 [28:40:24<9:13:36, 16.76s/it]


 92%|█████████▏| 22176/24156 [28:41:01<9:48:51, 17.84s/it]

 92%|█████████▏| 22177/24156 [28:41:20<10:06:59, 18.40s/it]
{'loss': 0.3697, 'learning_rate': 1.7445099483213805e-06, 'rewards/chosen': -2.8722872734069824, 'rewards/rejected': -5.097332000732422, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2250449657440186, 'policy_logps/rejected': -288.5311279296875, 'policy_logps/chosen': -347.4470520019531, 'referece_logps/rejected': -237.55783081054688, 'referece_logps/chosen': -318.72412109375, 'logits/rejected': -0.48454007506370544, 'logits/chosen': -0.5560699105262756, 'epoch': 8.26}

 92%|█████████▏| 22178/24156 [28:41:39<10:06:52, 18.41s/it]


 92%|█████████▏| 22180/24156 [28:42:07<8:50:12, 16.10s/it]
{'loss': 0.3282, 'learning_rate': 1.744241351166508e-06, 'rewards/chosen': -2.574259042739868, 'rewards/rejected': -4.832685947418213, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2584269046783447, 'policy_logps/rejected': -391.7872619628906, 'policy_logps/chosen': -250.173828125, 'referece_logps/rejected': -343.4604187011719, 'referece_logps/chosen': -224.43124389648438, 'logits/rejected': -0.7732997536659241, 'logits/chosen': -0.7404984831809998, 'epoch': 8.26}


 92%|█████████▏| 22182/24156 [28:42:33<7:52:18, 14.36s/it]
{'loss': 0.3548, 'learning_rate': 1.7440622194976412e-06, 'rewards/chosen': -2.633617877960205, 'rewards/rejected': -4.0973052978515625, 'rewards/accuracies': 0.625, 'rewards/margins': 1.463687777519226, 'policy_logps/rejected': -394.6517639160156, 'policy_logps/chosen': -356.20867919921875, 'referece_logps/rejected': -353.6787109375, 'referece_logps/chosen': -329.8725280761719, 'logits/rejected': -0.17329543828964233, 'logits/chosen': -0.023678719997406006, 'epoch': 8.26}


 92%|█████████▏| 22184/24156 [28:42:59<7:32:59, 13.78s/it]
{'loss': 0.3855, 'learning_rate': 1.7438830343246402e-06, 'rewards/chosen': -2.5570061206817627, 'rewards/rejected': -4.647350788116455, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0903446674346924, 'policy_logps/rejected': -449.2862854003906, 'policy_logps/chosen': -431.0869140625, 'referece_logps/rejected': -402.81280517578125, 'referece_logps/chosen': -405.5168762207031, 'logits/rejected': -0.3057425916194916, 'logits/chosen': -0.3482300341129303, 'epoch': 8.27}

 92%|█████████▏| 22185/24156 [28:43:18<8:29:38, 15.51s/it]

 92%|█████████▏| 22186/24156 [28:43:29<7:42:21, 14.08s/it]


 92%|█████████▏| 22188/24156 [28:43:57<7:43:25, 14.13s/it]
{'loss': 0.2118, 'learning_rate': 1.7435245035177798e-06, 'rewards/chosen': -3.0290982723236084, 'rewards/rejected': -5.425595283508301, 'rewards/accuracies': 1.0, 'rewards/margins': 2.396496534347534, 'policy_logps/rejected': -304.430908203125, 'policy_logps/chosen': -254.9078369140625, 'referece_logps/rejected': -250.17495727539062, 'referece_logps/chosen': -224.6168670654297, 'logits/rejected': -0.3983384072780609, 'logits/chosen': -0.3094141185283661, 'epoch': 8.27}

 92%|█████████▏| 22189/24156 [28:44:16<8:36:59, 15.77s/it]

 92%|█████████▏| 22190/24156 [28:44:28<7:56:01, 14.53s/it]


 92%|█████████▏| 22192/24156 [28:44:59<8:02:21, 14.74s/it]
{'loss': 0.2164, 'learning_rate': 1.7431657588490516e-06, 'rewards/chosen': -2.6292171478271484, 'rewards/rejected': -5.890436172485352, 'rewards/accuracies': 1.0, 'rewards/margins': 3.261219024658203, 'policy_logps/rejected': -386.16571044921875, 'policy_logps/chosen': -417.4108581542969, 'referece_logps/rejected': -327.2613525390625, 'referece_logps/chosen': -391.1186828613281, 'logits/rejected': -0.003871321678161621, 'logits/chosen': -0.0946962833404541, 'epoch': 8.27}

 92%|█████████▏| 22193/24156 [28:45:20<9:03:18, 16.61s/it]

 92%|█████████▏| 22194/24156 [28:45:35<8:52:24, 16.28s/it]

 92%|█████████▏| 22195/24156 [28:45:50<8:40:03, 15.91s/it]


 92%|█████████▏| 22197/24156 [28:46:19<8:10:47, 15.03s/it]
{'loss': 0.4618, 'learning_rate': 1.7427170274270942e-06, 'rewards/chosen': -1.3396852016448975, 'rewards/rejected': -4.5868144035339355, 'rewards/accuracies': 0.625, 'rewards/margins': 3.247128963470459, 'policy_logps/rejected': -524.1790771484375, 'policy_logps/chosen': -554.1561279296875, 'referece_logps/rejected': -478.31097412109375, 'referece_logps/chosen': -540.7592163085938, 'logits/rejected': -0.16481029987335205, 'logits/chosen': -0.12974286079406738, 'epoch': 8.27}


 92%|█████████▏| 22199/24156 [28:46:49<8:18:04, 15.27s/it]
{'loss': 0.346, 'learning_rate': 1.742537441384059e-06, 'rewards/chosen': -2.0839734077453613, 'rewards/rejected': -3.631256341934204, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5472828149795532, 'policy_logps/rejected': -234.14573669433594, 'policy_logps/chosen': -534.8177490234375, 'referece_logps/rejected': -197.8331756591797, 'referece_logps/chosen': -513.9779663085938, 'logits/rejected': -0.11082765460014343, 'logits/chosen': -0.36985301971435547, 'epoch': 8.27}


 92%|█████████▏| 22201/24156 [28:47:25<9:08:05, 16.82s/it]

 92%|█████████▏| 22202/24156 [28:47:45<9:36:10, 17.69s/it]
{'loss': 0.3723, 'learning_rate': 1.742267962208874e-06, 'rewards/chosen': -2.340287446975708, 'rewards/rejected': -3.409791946411133, 'rewards/accuracies': 0.75, 'rewards/margins': 1.069504976272583, 'policy_logps/rejected': -351.44586181640625, 'policy_logps/chosen': -397.7725830078125, 'referece_logps/rejected': -317.347900390625, 'referece_logps/chosen': -374.3697204589844, 'logits/rejected': -0.24377630650997162, 'logits/chosen': -0.3657812178134918, 'epoch': 8.27}

 92%|█████████▏| 22203/24156 [28:48:05<10:02:17, 18.50s/it]

 92%|█████████▏| 22204/24156 [28:48:25<10:16:24, 18.95s/it]

 92%|█████████▏| 22205/24156 [28:48:40<9:36:53, 17.74s/it]


 92%|█████████▏| 22207/24156 [28:49:15<9:26:30, 17.44s/it]

 92%|█████████▏| 22208/24156 [28:49:27<8:30:17, 15.72s/it]
{'loss': 0.3424, 'learning_rate': 1.7417286436201175e-06, 'rewards/chosen': -2.5426244735717773, 'rewards/rejected': -3.9104373455047607, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3678127527236938, 'policy_logps/rejected': -348.2652893066406, 'policy_logps/chosen': -294.2922668457031, 'referece_logps/rejected': -309.16094970703125, 'referece_logps/chosen': -268.86602783203125, 'logits/rejected': -0.7757376432418823, 'logits/chosen': -0.692081093788147, 'epoch': 8.27}


 92%|█████████▏| 22210/24156 [28:49:59<8:37:16, 15.95s/it]

 92%|█████████▏| 22211/24156 [28:50:15<8:39:30, 16.03s/it]
{'loss': 0.3496, 'learning_rate': 1.7414588042938044e-06, 'rewards/chosen': -1.9461815357208252, 'rewards/rejected': -4.890063285827637, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9438817501068115, 'policy_logps/rejected': -311.58709716796875, 'policy_logps/chosen': -411.8467102050781, 'referece_logps/rejected': -262.68646240234375, 'referece_logps/chosen': -392.38482666015625, 'logits/rejected': -0.22623185813426971, 'logits/chosen': -0.1996856927871704, 'epoch': 8.28}


 92%|█████████▏| 22213/24156 [28:50:43<8:14:45, 15.28s/it]
{'loss': 0.235, 'learning_rate': 1.7412788447607411e-06, 'rewards/chosen': -2.34780216217041, 'rewards/rejected': -4.72354793548584, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3757457733154297, 'policy_logps/rejected': -404.1676330566406, 'policy_logps/chosen': -534.0799560546875, 'referece_logps/rejected': -356.93212890625, 'referece_logps/chosen': -510.60198974609375, 'logits/rejected': -1.1429977416992188, 'logits/chosen': -1.3215303421020508, 'epoch': 8.28}


 92%|█████████▏| 22215/24156 [28:51:13<8:14:57, 15.30s/it]
{'loss': 0.2321, 'learning_rate': 1.7410988319236914e-06, 'rewards/chosen': -2.3444528579711914, 'rewards/rejected': -3.9905242919921875, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6460716724395752, 'policy_logps/rejected': -356.1748046875, 'policy_logps/chosen': -500.5648193359375, 'referece_logps/rejected': -316.2695617675781, 'referece_logps/chosen': -477.12030029296875, 'logits/rejected': -0.534766674041748, 'logits/chosen': -0.6201267242431641, 'epoch': 8.28}

 92%|█████████▏| 22216/24156 [28:51:26<7:51:16, 14.58s/it]

 92%|█████████▏| 22217/24156 [28:51:37<7:13:22, 13.41s/it]

 92%|█████████▏| 22218/24156 [28:51:48<6:52:31, 12.77s/it]


 92%|█████████▏| 22220/24156 [28:52:11<6:33:04, 12.18s/it]
{'loss': 0.3581, 'learning_rate': 1.7406485667110817e-06, 'rewards/chosen': -1.12937331199646, 'rewards/rejected': -4.363182067871094, 'rewards/accuracies': 0.875, 'rewards/margins': 3.233808755874634, 'policy_logps/rejected': -387.5083923339844, 'policy_logps/chosen': -395.9945068359375, 'referece_logps/rejected': -343.8765869140625, 'referece_logps/chosen': -384.7007751464844, 'logits/rejected': -0.1596224308013916, 'logits/chosen': -0.117987722158432, 'epoch': 8.28}

 92%|█████████▏| 22221/24156 [28:52:33<8:09:56, 15.19s/it]

 92%|█████████▏| 22222/24156 [28:52:50<8:27:02, 15.73s/it]

 92%|█████████▏| 22223/24156 [28:53:07<8:31:02, 15.86s/it]


 92%|█████████▏| 22225/24156 [28:53:35<8:11:30, 15.27s/it]
{'loss': 0.4354, 'learning_rate': 1.740197968631829e-06, 'rewards/chosen': -3.1473076343536377, 'rewards/rejected': -4.114877700805664, 'rewards/accuracies': 0.5, 'rewards/margins': 0.9675697684288025, 'policy_logps/rejected': -462.7735595703125, 'policy_logps/chosen': -462.04437255859375, 'referece_logps/rejected': -421.6247863769531, 'referece_logps/chosen': -430.5713195800781, 'logits/rejected': -1.0214771032333374, 'logits/chosen': -1.0536916255950928, 'epoch': 8.28}

 92%|█████████▏| 22226/24156 [28:53:46<7:26:24, 13.88s/it]

 92%|█████████▏| 22227/24156 [28:54:00<7:25:58, 13.87s/it]

 92%|█████████▏| 22228/24156 [28:54:17<7:56:15, 14.82s/it]

 92%|█████████▏| 22229/24156 [28:54:32<8:00:03, 14.95s/it]

 92%|█████████▏| 22230/24156 [28:54:46<7:48:56, 14.61s/it]

 92%|█████████▏| 22231/24156 [28:55:06<8:43:09, 16.31s/it]

 92%|█████████▏| 22232/24156 [28:55:26<9:19:41, 17.45s/it]

 92%|█████████▏| 22233/24156 [28:55:46<9:39:48, 18.09s/it]

 92%|█████████▏| 22234/24156 [28:56:02<9:24:04, 17.61s/it]


 92%|█████████▏| 22236/24156 [28:56:38<9:20:56, 17.53s/it]
{'loss': 0.3094, 'learning_rate': 1.7392054821650812e-06, 'rewards/chosen': -1.0252132415771484, 'rewards/rejected': -3.5603106021881104, 'rewards/accuracies': 0.625, 'rewards/margins': 2.535097599029541, 'policy_logps/rejected': -459.4792175292969, 'policy_logps/chosen': -397.68359375, 'referece_logps/rejected': -423.8760986328125, 'referece_logps/chosen': -387.43145751953125, 'logits/rejected': -0.5917529463768005, 'logits/chosen': -0.6547161936759949, 'epoch': 8.28}

 92%|█████████▏| 22237/24156 [28:56:59<9:55:34, 18.62s/it]

 92%|█████████▏| 22238/24156 [28:57:18<10:05:44, 18.95s/it]

 92%|█████████▏| 22239/24156 [28:57:30<8:50:52, 16.62s/it]

 92%|█████████▏| 22240/24156 [28:57:46<8:51:58, 16.66s/it]

 92%|█████████▏| 22241/24156 [28:58:03<8:54:14, 16.74s/it]

 92%|█████████▏| 22242/24156 [28:58:23<9:21:14, 17.59s/it]

 92%|█████████▏| 22243/24156 [28:58:35<8:31:46, 16.05s/it]

 92%|█████████▏| 22244/24156 [28:58:53<8:47:18, 16.55s/it]

 92%|█████████▏| 22245/24156 [28:59:04<7:50:42, 14.78s/it]

 92%|█████████▏| 22246/24156 [28:59:18<7:41:56, 14.51s/it]

 92%|█████████▏| 22247/24156 [28:59:36<8:22:18, 15.79s/it]

 92%|█████████▏| 22248/24156 [28:59:53<8:27:47, 15.97s/it]

 92%|█████████▏| 22249/24156 [29:00:04<7:42:38, 14.56s/it]

 92%|█████████▏| 22250/24156 [29:00:24<8:31:55, 16.11s/it]

 92%|█████████▏| 22251/24156 [29:00:43<8:57:53, 16.94s/it]

 92%|█████████▏| 22252/24156 [29:00:58<8:39:39, 16.38s/it]

 92%|█████████▏| 22253/24156 [29:01:16<9:00:29, 17.04s/it]

 92%|█████████▏| 22254/24156 [29:01:31<8:34:41, 16.24s/it]

 92%|█████████▏| 22255/24156 [29:01:47<8:35:08, 16.26s/it]

 92%|█████████▏| 22256/24156 [29:02:06<8:57:50, 16.98s/it]

 92%|█████████▏| 22257/24156 [29:02:16<7:57:51, 15.10s/it]

 92%|█████████▏| 22258/24156 [29:02:30<7:40:43, 14.56s/it]

 92%|█████████▏| 22259/24156 [29:02:41<7:05:59, 13.47s/it]

 92%|█████████▏| 22260/24156 [29:02:51<6:40:27, 12.67s/it]

 92%|█████████▏| 22261/24156 [29:03:06<7:00:55, 13.33s/it]

 92%|█████████▏| 22262/24156 [29:03:22<7:20:25, 13.95s/it]

 92%|█████████▏| 22263/24156 [29:03:35<7:10:18, 13.64s/it]

 92%|█████████▏| 22264/24156 [29:03:57<8:29:13, 16.15s/it]

 92%|█████████▏| 22265/24156 [29:04:10<8:03:40, 15.35s/it]

 92%|█████████▏| 22266/24156 [29:04:22<7:28:14, 14.23s/it]

 92%|█████████▏| 22267/24156 [29:04:41<8:19:07, 15.85s/it]

 92%|█████████▏| 22268/24156 [29:05:01<8:59:09, 17.13s/it]

 92%|█████████▏| 22269/24156 [29:05:16<8:36:05, 16.41s/it]

 92%|█████████▏| 22270/24156 [29:05:32<8:30:13, 16.23s/it]

 92%|█████████▏| 22271/24156 [29:05:50<8:43:37, 16.67s/it]

 92%|█████████▏| 22272/24156 [29:06:11<9:24:54, 17.99s/it]


 92%|█████████▏| 22274/24156 [29:06:47<9:31:50, 18.23s/it]

 92%|█████████▏| 22275/24156 [29:07:08<9:52:29, 18.90s/it]

 92%|█████████▏| 22276/24156 [29:07:20<8:50:02, 16.92s/it]

 92%|█████████▏| 22277/24156 [29:07:31<7:50:12, 15.01s/it]

 92%|█████████▏| 22278/24156 [29:07:41<7:09:00, 13.71s/it]
{'loss': 0.3366, 'learning_rate': 1.7354012167345143e-06, 'rewards/chosen': -2.7303993701934814, 'rewards/rejected': -4.376003265380859, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6456042528152466, 'policy_logps/rejected': -313.481201171875, 'policy_logps/chosen': -509.48028564453125, 'referece_logps/rejected': -269.7211608886719, 'referece_logps/chosen': -482.17626953125, 'logits/rejected': -0.11116670072078705, 'logits/chosen': -0.3217686414718628, 'epoch': 8.3}


 92%|█████████▏| 22280/24156 [29:08:17<8:19:49, 15.99s/it]

 92%|█████████▏| 22281/24156 [29:08:33<8:17:52, 15.93s/it]

 92%|█████████▏| 22282/24156 [29:08:51<8:37:34, 16.57s/it]

 92%|█████████▏| 22283/24156 [29:09:07<8:30:52, 16.37s/it]

 92%|█████████▏| 22284/24156 [29:09:22<8:18:47, 15.99s/it]

 92%|█████████▏| 22285/24156 [29:09:33<7:27:37, 14.35s/it]

 92%|█████████▏| 22286/24156 [29:09:51<8:05:13, 15.57s/it]
{'loss': 0.332, 'learning_rate': 1.7346739469845801e-06, 'rewards/chosen': -2.36850643157959, 'rewards/rejected': -5.03679895401001, 'rewards/accuracies': 0.75, 'rewards/margins': 2.66829252243042, 'policy_logps/rejected': -349.9655456542969, 'policy_logps/chosen': -311.306396484375, 'referece_logps/rejected': -299.5975341796875, 'referece_logps/chosen': -287.621337890625, 'logits/rejected': -0.30903834104537964, 'logits/chosen': -0.3463677167892456, 'epoch': 8.3}


 92%|█████████▏| 22288/24156 [29:10:31<9:10:55, 17.70s/it]

 92%|█████████▏| 22289/24156 [29:10:43<8:19:31, 16.05s/it]

 92%|█████████▏| 22290/24156 [29:10:57<8:06:01, 15.63s/it]

 92%|█████████▏| 22291/24156 [29:11:13<8:04:03, 15.57s/it]

 92%|█████████▏| 22292/24156 [29:11:30<8:16:05, 15.97s/it]

 92%|█████████▏| 22293/24156 [29:11:48<8:36:54, 16.65s/it]

 92%|█████████▏| 22294/24156 [29:12:08<9:06:04, 17.60s/it]

 92%|█████████▏| 22295/24156 [29:12:20<8:19:40, 16.11s/it]

 92%|█████████▏| 22296/24156 [29:12:39<8:42:22, 16.85s/it]

 92%|█████████▏| 22297/24156 [29:12:55<8:34:28, 16.60s/it]

 92%|█████████▏| 22298/24156 [29:13:14<8:59:00, 17.41s/it]

 92%|█████████▏| 22299/24156 [29:13:26<8:05:00, 15.67s/it]

 92%|█████████▏| 22300/24156 [29:13:43<8:14:26, 15.98s/it]

 92%|█████████▏| 22301/24156 [29:13:55<7:40:34, 14.90s/it]

 92%|█████████▏| 22302/24156 [29:14:09<7:28:09, 14.50s/it]

 92%|█████████▏| 22303/24156 [29:14:19<6:52:08, 13.35s/it]

 92%|█████████▏| 22304/24156 [29:14:39<7:49:09, 15.20s/it]

 92%|█████████▏| 22305/24156 [29:14:59<8:36:34, 16.74s/it]

 92%|█████████▏| 22306/24156 [29:15:14<8:14:42, 16.04s/it]

 92%|█████████▏| 22307/24156 [29:15:31<8:30:00, 16.55s/it]

 92%|█████████▏| 22308/24156 [29:15:51<8:55:09, 17.38s/it]

 92%|█████████▏| 22309/24156 [29:16:07<8:47:17, 17.13s/it]

 92%|█████████▏| 22310/24156 [29:16:27<9:10:55, 17.91s/it]

 92%|█████████▏| 22311/24156 [29:16:47<9:27:40, 18.46s/it]

 92%|█████████▏| 22312/24156 [29:17:06<9:33:20, 18.66s/it]

 92%|█████████▏| 22313/24156 [29:17:17<8:23:44, 16.40s/it]

 92%|█████████▏| 22314/24156 [29:17:34<8:32:36, 16.70s/it]

 92%|█████████▏| 22315/24156 [29:17:51<8:29:31, 16.61s/it]

 92%|█████████▏| 22316/24156 [29:18:10<8:53:43, 17.40s/it]

 92%|█████████▏| 22317/24156 [29:18:32<9:35:08, 18.76s/it]

 92%|█████████▏| 22318/24156 [29:18:50<9:31:42, 18.66s/it]
{'loss': 0.2319, 'learning_rate': 1.7317564237211197e-06, 'rewards/chosen': -1.883239507675171, 'rewards/rejected': -4.594749927520752, 'rewards/accuracies': 1.0, 'rewards/margins': 2.71151065826416, 'policy_logps/rejected': -315.9498291015625, 'policy_logps/chosen': -395.0841064453125, 'referece_logps/rejected': -270.0023193359375, 'referece_logps/chosen': -376.2516784667969, 'logits/rejected': -0.13017305731773376, 'logits/chosen': -0.20005761086940765, 'epoch': 8.32}

 92%|█████████▏| 22319/24156 [29:19:11<9:52:41, 19.36s/it]


 92%|█████████▏| 22321/24156 [29:19:51<9:58:34, 19.57s/it]

 92%|█████████▏| 22322/24156 [29:20:03<8:46:03, 17.21s/it]

 92%|█████████▏| 22323/24156 [29:20:16<8:07:56, 15.97s/it]

 92%|█████████▏| 22324/24156 [29:20:26<7:19:16, 14.39s/it]

 92%|█████████▏| 22325/24156 [29:20:38<6:52:47, 13.53s/it]
{'loss': 0.2815, 'learning_rate': 1.7311164180090182e-06, 'rewards/chosen': -2.0874650478363037, 'rewards/rejected': -3.6932973861694336, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6058322191238403, 'policy_logps/rejected': -395.89093017578125, 'policy_logps/chosen': -353.7373352050781, 'referece_logps/rejected': -358.95794677734375, 'referece_logps/chosen': -332.86273193359375, 'logits/rejected': -0.16883505880832672, 'logits/chosen': -0.14577877521514893, 'epoch': 8.32}


 92%|█████████▏| 22327/24156 [29:21:10<7:21:42, 14.49s/it]
{'loss': 0.347, 'learning_rate': 1.7309334409197143e-06, 'rewards/chosen': -2.309523582458496, 'rewards/rejected': -4.748035430908203, 'rewards/accuracies': 0.75, 'rewards/margins': 2.438511371612549, 'policy_logps/rejected': -390.8980407714844, 'policy_logps/chosen': -494.96673583984375, 'referece_logps/rejected': -343.417724609375, 'referece_logps/chosen': -471.8714599609375, 'logits/rejected': -0.009308815002441406, 'logits/chosen': -0.1688014417886734, 'epoch': 8.32}


 92%|█████████▏| 22329/24156 [29:21:34<6:37:04, 13.04s/it]

 92%|█████████▏| 22330/24156 [29:21:48<6:45:09, 13.31s/it]

 92%|█████████▏| 22331/24156 [29:21:58<6:20:59, 12.53s/it]

 92%|█████████▏| 22332/24156 [29:22:17<7:15:34, 14.33s/it]

 92%|█████████▏| 22333/24156 [29:22:34<7:39:41, 15.13s/it]
{'loss': 0.2962, 'learning_rate': 1.7303841943440439e-06, 'rewards/chosen': -1.6641185283660889, 'rewards/rejected': -3.655167579650879, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9910489320755005, 'policy_logps/rejected': -413.05780029296875, 'policy_logps/chosen': -467.5682067871094, 'referece_logps/rejected': -376.506103515625, 'referece_logps/chosen': -450.9270324707031, 'logits/rejected': 0.3665877878665924, 'logits/chosen': 0.2856018543243408, 'epoch': 8.32}

 92%|█████████▏| 22334/24156 [29:22:49<7:44:36, 15.30s/it]


 92%|█████████▏| 22336/24156 [29:23:27<8:39:38, 17.13s/it]

 92%|█████████▏| 22337/24156 [29:23:47<9:04:02, 17.95s/it]

 92%|█████████▏| 22338/24156 [29:23:57<7:55:48, 15.70s/it]

 92%|█████████▏| 22339/24156 [29:24:09<7:21:32, 14.58s/it]

 92%|█████████▏| 22340/24156 [29:24:28<7:56:15, 15.74s/it]

 92%|█████████▏| 22341/24156 [29:24:45<8:11:20, 16.24s/it]

 92%|█████████▏| 22342/24156 [29:24:59<7:50:06, 15.55s/it]

 92%|█████████▏| 22343/24156 [29:25:10<7:05:51, 14.09s/it]

 92%|█████████▏| 22344/24156 [29:25:23<6:56:20, 13.79s/it]

 93%|█████████▎| 22345/24156 [29:25:40<7:28:37, 14.86s/it]

 93%|█████████▎| 22346/24156 [29:25:59<8:04:16, 16.05s/it]

 93%|█████████▎| 22347/24156 [29:26:18<8:33:52, 17.04s/it]

 93%|█████████▎| 22348/24156 [29:26:37<8:51:50, 17.65s/it]
{'loss': 0.2916, 'learning_rate': 1.729009010685736e-06, 'rewards/chosen': -2.805473804473877, 'rewards/rejected': -5.257621765136719, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4521477222442627, 'policy_logps/rejected': -375.6785888671875, 'policy_logps/chosen': -318.8955078125, 'referece_logps/rejected': -323.10235595703125, 'referece_logps/chosen': -290.8408203125, 'logits/rejected': -0.45551797747612, 'logits/chosen': -0.40362387895584106, 'epoch': 8.33}


 93%|█████████▎| 22350/24156 [29:27:21<9:52:56, 19.70s/it]

 93%|█████████▎| 22351/24156 [29:27:41<9:58:06, 19.88s/it]

 93%|█████████▎| 22352/24156 [29:28:01<9:54:45, 19.78s/it]

 93%|█████████▎| 22353/24156 [29:28:21<9:54:00, 19.77s/it]

 93%|█████████▎| 22354/24156 [29:28:31<8:31:27, 17.03s/it]

 93%|█████████▎| 22355/24156 [29:28:45<8:04:16, 16.13s/it]

 93%|█████████▎| 22356/24156 [29:28:59<7:39:30, 15.32s/it]

 93%|█████████▎| 22357/24156 [29:29:09<6:58:39, 13.96s/it]

 93%|█████████▎| 22358/24156 [29:29:23<6:57:08, 13.92s/it]
{'loss': 0.3096, 'learning_rate': 1.7280905830592132e-06, 'rewards/chosen': -2.1293110847473145, 'rewards/rejected': -4.248034477233887, 'rewards/accuracies': 1.0, 'rewards/margins': 2.118723154067993, 'policy_logps/rejected': -467.6789855957031, 'policy_logps/chosen': -423.29205322265625, 'referece_logps/rejected': -425.1986389160156, 'referece_logps/chosen': -401.9989318847656, 'logits/rejected': -1.199777603149414, 'logits/chosen': -1.1082011461257935, 'epoch': 8.33}


 93%|█████████▎| 22360/24156 [29:29:45<6:07:32, 12.28s/it]

 93%|█████████▎| 22361/24156 [29:30:02<6:50:51, 13.73s/it]

 93%|█████████▎| 22362/24156 [29:30:13<6:30:10, 13.05s/it]

 93%|█████████▎| 22363/24156 [29:30:23<6:04:48, 12.21s/it]
{'loss': 0.3608, 'learning_rate': 1.7276308783085832e-06, 'rewards/chosen': -2.5432803630828857, 'rewards/rejected': -5.012279987335205, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4689998626708984, 'policy_logps/rejected': -293.8808288574219, 'policy_logps/chosen': -322.3743896484375, 'referece_logps/rejected': -243.75802612304688, 'referece_logps/chosen': -296.94158935546875, 'logits/rejected': -0.35554587841033936, 'logits/chosen': -0.45801007747650146, 'epoch': 8.33}


 93%|█████████▎| 22365/24156 [29:30:53<6:44:15, 13.54s/it]

 93%|█████████▎| 22366/24156 [29:31:07<6:49:28, 13.73s/it]

 93%|█████████▎| 22367/24156 [29:31:27<7:40:43, 15.45s/it]
{'loss': 0.292, 'learning_rate': 1.7272628790465242e-06, 'rewards/chosen': -2.044074773788452, 'rewards/rejected': -5.0067853927612305, 'rewards/accuracies': 1.0, 'rewards/margins': 2.962710380554199, 'policy_logps/rejected': -313.2955627441406, 'policy_logps/chosen': -519.1138305664062, 'referece_logps/rejected': -263.22772216796875, 'referece_logps/chosen': -498.67303466796875, 'logits/rejected': 0.2739296853542328, 'logits/chosen': 0.22292368113994598, 'epoch': 8.33}


 93%|█████████▎| 22369/24156 [29:32:02<8:16:38, 16.68s/it]

 93%|█████████▎| 22370/24156 [29:32:14<7:35:14, 15.29s/it]
{'loss': 0.3693, 'learning_rate': 1.7269867423168646e-06, 'rewards/chosen': -2.361851930618286, 'rewards/rejected': -3.803776264190674, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4419242143630981, 'policy_logps/rejected': -407.39300537109375, 'policy_logps/chosen': -424.26751708984375, 'referece_logps/rejected': -369.355224609375, 'referece_logps/chosen': -400.64898681640625, 'logits/rejected': -0.21236535906791687, 'logits/chosen': -0.24476826190948486, 'epoch': 8.33}


 93%|█████████▎| 22372/24156 [29:32:42<7:19:52, 14.79s/it]

 93%|█████████▎| 22373/24156 [29:32:55<7:03:40, 14.26s/it]
{'loss': 0.25, 'learning_rate': 1.7267104879656023e-06, 'rewards/chosen': -2.9638848304748535, 'rewards/rejected': -4.268478870391846, 'rewards/accuracies': 0.875, 'rewards/margins': 1.3045941591262817, 'policy_logps/rejected': -430.03277587890625, 'policy_logps/chosen': -432.5883483886719, 'referece_logps/rejected': -387.34796142578125, 'referece_logps/chosen': -402.94952392578125, 'logits/rejected': -0.33577877283096313, 'logits/chosen': -0.29550114274024963, 'epoch': 8.34}

 93%|█████████▎| 22374/24156 [29:33:06<6:39:45, 13.46s/it]


 93%|█████████▎| 22376/24156 [29:33:44<7:55:31, 16.03s/it]

 93%|█████████▎| 22377/24156 [29:34:06<8:47:28, 17.79s/it]
{'loss': 0.334, 'learning_rate': 1.7263419659409043e-06, 'rewards/chosen': -2.2313661575317383, 'rewards/rejected': -5.42678689956665, 'rewards/accuracies': 1.0, 'rewards/margins': 3.195420026779175, 'policy_logps/rejected': -386.17620849609375, 'policy_logps/chosen': -364.55517578125, 'referece_logps/rejected': -331.90838623046875, 'referece_logps/chosen': -342.2414855957031, 'logits/rejected': -0.7755868434906006, 'logits/chosen': -0.7318736910820007, 'epoch': 8.34}

 93%|█████████▎| 22378/24156 [29:34:23<8:40:54, 17.58s/it]


 93%|█████████▎| 22380/24156 [29:34:50<7:32:26, 15.28s/it]
{'loss': 0.3672, 'learning_rate': 1.7260654373130875e-06, 'rewards/chosen': -2.479320764541626, 'rewards/rejected': -4.021181583404541, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5418609380722046, 'policy_logps/rejected': -516.3480224609375, 'policy_logps/chosen': -418.1109313964844, 'referece_logps/rejected': -476.13616943359375, 'referece_logps/chosen': -393.3177490234375, 'logits/rejected': -0.5573011636734009, 'logits/chosen': -0.546280026435852, 'epoch': 8.34}


 93%|█████████▎| 22382/24156 [29:35:19<7:13:02, 14.65s/it]

 93%|█████████▎| 22383/24156 [29:35:30<6:37:38, 13.46s/it]

 93%|█████████▎| 22384/24156 [29:35:45<6:52:31, 13.97s/it]

 93%|█████████▎| 22385/24156 [29:35:56<6:22:48, 12.97s/it]
{'loss': 0.4599, 'learning_rate': 1.7256042952387367e-06, 'rewards/chosen': -2.5140862464904785, 'rewards/rejected': -3.6941640377044678, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1800779104232788, 'policy_logps/rejected': -384.5498046875, 'policy_logps/chosen': -400.8187255859375, 'referece_logps/rejected': -347.6081237792969, 'referece_logps/chosen': -375.6778869628906, 'logits/rejected': -0.5094672441482544, 'logits/chosen': -0.5134841203689575, 'epoch': 8.34}

 93%|█████████▎| 22386/24156 [29:36:06<6:02:32, 12.29s/it]


 93%|█████████▎| 22388/24156 [29:36:33<6:24:50, 13.06s/it]
{'loss': 0.5334, 'learning_rate': 1.7253274534502842e-06, 'rewards/chosen': -2.2809743881225586, 'rewards/rejected': -3.0645751953125, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7836011052131653, 'policy_logps/rejected': -302.9058837890625, 'policy_logps/chosen': -352.52008056640625, 'referece_logps/rejected': -272.2601013183594, 'referece_logps/chosen': -329.7103271484375, 'logits/rejected': 0.4531535506248474, 'logits/chosen': 0.4582206904888153, 'epoch': 8.34}


 93%|█████████▎| 22390/24156 [29:37:01<6:23:53, 13.04s/it]

 93%|█████████▎| 22391/24156 [29:37:18<6:58:20, 14.22s/it]

 93%|█████████▎| 22392/24156 [29:37:34<7:18:19, 14.91s/it]

 93%|█████████▎| 22393/24156 [29:37:54<8:00:21, 16.35s/it]

 93%|█████████▎| 22394/24156 [29:38:13<8:26:03, 17.23s/it]
{'loss': 0.3069, 'learning_rate': 1.7247734178587678e-06, 'rewards/chosen': -1.3667688369750977, 'rewards/rejected': -3.9712817668914795, 'rewards/accuracies': 0.875, 'rewards/margins': 2.604513168334961, 'policy_logps/rejected': -500.40081787109375, 'policy_logps/chosen': -467.8765563964844, 'referece_logps/rejected': -460.6880798339844, 'referece_logps/chosen': -454.2088317871094, 'logits/rejected': -0.47304147481918335, 'logits/chosen': -0.44551360607147217, 'epoch': 8.34}


 93%|█████████▎| 22396/24156 [29:38:48<8:31:26, 17.44s/it]

 93%|█████████▎| 22397/24156 [29:39:04<8:16:55, 16.95s/it]
{'loss': 0.5192, 'learning_rate': 1.7244962241453428e-06, 'rewards/chosen': -1.8883938789367676, 'rewards/rejected': -3.08940052986145, 'rewards/accuracies': 1.0, 'rewards/margins': 1.201006293296814, 'policy_logps/rejected': -374.38494873046875, 'policy_logps/chosen': -301.25616455078125, 'referece_logps/rejected': -343.490966796875, 'referece_logps/chosen': -282.37225341796875, 'logits/rejected': -0.34157562255859375, 'logits/chosen': -0.26051265001296997, 'epoch': 8.34}


 93%|█████████▎| 22399/24156 [29:39:36<7:55:35, 16.24s/it]

 93%|█████████▎| 22400/24156 [29:39:50<7:37:59, 15.65s/it]
{'loss': 0.3803, 'learning_rate': 1.724218913213265e-06, 'rewards/chosen': -1.944715976715088, 'rewards/rejected': -3.121981143951416, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1772651672363281, 'policy_logps/rejected': -391.52630615234375, 'policy_logps/chosen': -296.44500732421875, 'referece_logps/rejected': -360.3065490722656, 'referece_logps/chosen': -276.9978332519531, 'logits/rejected': -0.283014178276062, 'logits/chosen': -0.08420250564813614, 'epoch': 8.35}


 93%|█████████▎| 22402/24156 [29:40:30<8:38:22, 17.73s/it]

 93%|█████████▎| 22403/24156 [29:40:50<8:56:32, 18.36s/it]
{'loss': 0.3796, 'learning_rate': 1.7239414851074008e-06, 'rewards/chosen': -2.2210772037506104, 'rewards/rejected': -3.241643190383911, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0205657482147217, 'policy_logps/rejected': -433.9079895019531, 'policy_logps/chosen': -421.36346435546875, 'referece_logps/rejected': -401.4915771484375, 'referece_logps/chosen': -399.1527099609375, 'logits/rejected': -0.23673926293849945, 'logits/chosen': -0.1412852555513382, 'epoch': 8.35}

 93%|█████████▎| 22404/24156 [29:41:07<8:46:11, 18.02s/it]


 93%|█████████▎| 22406/24156 [29:41:33<7:27:08, 15.33s/it]

 93%|█████████▎| 22407/24156 [29:41:48<7:17:26, 15.01s/it]

 93%|█████████▎| 22408/24156 [29:42:06<7:47:21, 16.04s/it]

 93%|█████████▎| 22409/24156 [29:42:22<7:45:58, 16.00s/it]

 93%|█████████▎| 22410/24156 [29:42:42<8:18:46, 17.14s/it]
{'loss': 0.2506, 'learning_rate': 1.7232936974367221e-06, 'rewards/chosen': -2.12863826751709, 'rewards/rejected': -4.224076747894287, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0954389572143555, 'policy_logps/rejected': -351.1446533203125, 'policy_logps/chosen': -462.8354187011719, 'referece_logps/rejected': -308.90386962890625, 'referece_logps/chosen': -441.549072265625, 'logits/rejected': -0.12284518033266068, 'logits/chosen': 0.021447524428367615, 'epoch': 8.35}

 93%|█████████▎| 22411/24156 [29:42:57<7:59:54, 16.50s/it]


 93%|█████████▎| 22413/24156 [29:43:38<8:56:53, 18.48s/it]

 93%|█████████▎| 22414/24156 [29:43:58<9:08:05, 18.88s/it]
{'loss': 0.3131, 'learning_rate': 1.7229232469579791e-06, 'rewards/chosen': -2.28535532951355, 'rewards/rejected': -4.045951843261719, 'rewards/accuracies': 0.75, 'rewards/margins': 1.760596752166748, 'policy_logps/rejected': -411.57666015625, 'policy_logps/chosen': -466.3841857910156, 'referece_logps/rejected': -371.11712646484375, 'referece_logps/chosen': -443.5306091308594, 'logits/rejected': 0.03221359848976135, 'logits/chosen': 0.0558830201625824, 'epoch': 8.35}

 93%|█████████▎| 22415/24156 [29:44:13<8:33:37, 17.70s/it]

 93%|█████████▎| 22416/24156 [29:44:35<9:13:52, 19.10s/it]


 93%|█████████▎| 22418/24156 [29:45:02<7:45:08, 16.06s/it]

 93%|█████████▎| 22419/24156 [29:45:22<8:16:13, 17.14s/it]

 93%|█████████▎| 22420/24156 [29:45:43<8:46:27, 18.20s/it]

 93%|█████████▎| 22421/24156 [29:46:02<8:57:59, 18.61s/it]
{'loss': 0.2665, 'learning_rate': 1.7222744583376493e-06, 'rewards/chosen': -1.8682279586791992, 'rewards/rejected': -4.76678991317749, 'rewards/accuracies': 1.0, 'rewards/margins': 2.898561954498291, 'policy_logps/rejected': -329.1240234375, 'policy_logps/chosen': -306.5830078125, 'referece_logps/rejected': -281.4560852050781, 'referece_logps/chosen': -287.9007263183594, 'logits/rejected': -0.038056593388319016, 'logits/chosen': 0.2015753537416458, 'epoch': 8.35}


 93%|█████████▎| 22423/24156 [29:46:25<7:10:00, 14.89s/it]
{'loss': 0.2654, 'learning_rate': 1.7220889732762256e-06, 'rewards/chosen': -2.0903913974761963, 'rewards/rejected': -4.884770393371582, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7943787574768066, 'policy_logps/rejected': -508.86016845703125, 'policy_logps/chosen': -415.6684875488281, 'referece_logps/rejected': -460.0124816894531, 'referece_logps/chosen': -394.76458740234375, 'logits/rejected': -0.6264534592628479, 'logits/chosen': -0.7159368395805359, 'epoch': 8.35}


 93%|█████████▎| 22425/24156 [29:47:02<8:09:22, 16.96s/it]

 93%|█████████▎| 22426/24156 [29:47:14<7:29:24, 15.59s/it]

 93%|█████████▎| 22427/24156 [29:47:30<7:27:36, 15.53s/it]

 93%|█████████▎| 22428/24156 [29:47:44<7:17:33, 15.19s/it]

 93%|█████████▎| 22429/24156 [29:48:04<7:56:38, 16.56s/it]
{'loss': 0.3201, 'learning_rate': 1.721532206600854e-06, 'rewards/chosen': -2.426455020904541, 'rewards/rejected': -5.105747222900391, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6792919635772705, 'policy_logps/rejected': -462.1873779296875, 'policy_logps/chosen': -445.50067138671875, 'referece_logps/rejected': -411.1298828125, 'referece_logps/chosen': -421.2361145019531, 'logits/rejected': 0.008914075791835785, 'logits/chosen': -0.028544582426548004, 'epoch': 8.36}

 93%|█████████▎| 22430/24156 [29:48:23<8:18:52, 17.34s/it]


 93%|█████████▎| 22432/24156 [29:49:00<8:28:33, 17.70s/it]
{'loss': 0.4232, 'learning_rate': 1.7212536481320005e-06, 'rewards/chosen': -3.9720396995544434, 'rewards/rejected': -6.056685924530029, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0846457481384277, 'policy_logps/rejected': -414.95159912109375, 'policy_logps/chosen': -416.7149963378906, 'referece_logps/rejected': -354.384765625, 'referece_logps/chosen': -376.9945983886719, 'logits/rejected': 0.3002817928791046, 'logits/chosen': 0.380704402923584, 'epoch': 8.36}

 93%|█████████▎| 22433/24156 [29:49:16<8:11:57, 17.13s/it]


 93%|█████████▎| 22435/24156 [29:49:51<8:13:26, 17.20s/it]

 93%|█████████▎| 22436/24156 [29:50:08<8:14:37, 17.25s/it]

 93%|█████████▎| 22437/24156 [29:50:24<8:05:08, 16.93s/it]

 93%|█████████▎| 22438/24156 [29:50:41<7:57:54, 16.69s/it]

 93%|█████████▎| 22439/24156 [29:50:59<8:08:57, 17.09s/it]

 93%|█████████▎| 22440/24156 [29:51:18<8:30:26, 17.85s/it]
{'loss': 0.4278, 'learning_rate': 1.7205102551667335e-06, 'rewards/chosen': -1.5341973304748535, 'rewards/rejected': -3.273437738418579, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7392404079437256, 'policy_logps/rejected': -423.618896484375, 'policy_logps/chosen': -492.2406921386719, 'referece_logps/rejected': -390.884521484375, 'referece_logps/chosen': -476.8987121582031, 'logits/rejected': -0.4485462009906769, 'logits/chosen': -0.4007488489151001, 'epoch': 8.36}


 93%|█████████▎| 22442/24156 [29:51:54<8:28:54, 17.81s/it]
{'loss': 0.3666, 'learning_rate': 1.7203242773656075e-06, 'rewards/chosen': -2.6084206104278564, 'rewards/rejected': -5.119449615478516, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5110292434692383, 'policy_logps/rejected': -436.0096435546875, 'policy_logps/chosen': -440.8299255371094, 'referece_logps/rejected': -384.81512451171875, 'referece_logps/chosen': -414.74566650390625, 'logits/rejected': -0.1681857854127884, 'logits/chosen': -0.40267226099967957, 'epoch': 8.36}


 93%|█████████▎| 22444/24156 [29:52:33<8:50:13, 18.58s/it]

 93%|█████████▎| 22445/24156 [29:52:53<8:57:14, 18.84s/it]

 93%|█████████▎| 22446/24156 [29:53:11<8:53:17, 18.71s/it]

 93%|█████████▎| 22447/24156 [29:53:29<8:48:40, 18.56s/it]
{'loss': 0.3833, 'learning_rate': 1.7198591062793815e-06, 'rewards/chosen': -1.7349786758422852, 'rewards/rejected': -4.1292314529418945, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3942527770996094, 'policy_logps/rejected': -610.9496459960938, 'policy_logps/chosen': -479.41522216796875, 'referece_logps/rejected': -569.6573486328125, 'referece_logps/chosen': -462.0654296875, 'logits/rejected': -0.305264949798584, 'logits/chosen': -0.41353321075439453, 'epoch': 8.36}

 93%|█████████▎| 22448/24156 [29:53:46<8:32:19, 18.00s/it]

 93%|█████████▎| 22449/24156 [29:53:58<7:38:10, 16.10s/it]

 93%|█████████▎| 22450/24156 [29:54:16<7:53:57, 16.67s/it]


 93%|█████████▎| 22452/24156 [29:54:43<7:17:30, 15.41s/it]

 93%|█████████▎| 22453/24156 [29:54:57<7:01:33, 14.85s/it]

 93%|█████████▎| 22454/24156 [29:55:16<7:41:10, 16.26s/it]

 93%|█████████▎| 22455/24156 [29:55:29<7:07:48, 15.09s/it]
{'loss': 0.2709, 'learning_rate': 1.7191141596999349e-06, 'rewards/chosen': -1.079152226448059, 'rewards/rejected': -3.104567050933838, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0254149436950684, 'policy_logps/rejected': -268.0774230957031, 'policy_logps/chosen': -397.9354248046875, 'referece_logps/rejected': -237.03176879882812, 'referece_logps/chosen': -387.1439514160156, 'logits/rejected': -0.3279223144054413, 'logits/chosen': -0.37269407510757446, 'epoch': 8.37}


 93%|█████████▎| 22457/24156 [29:56:02<7:36:16, 16.11s/it]
{'loss': 0.3029, 'learning_rate': 1.7189277937461707e-06, 'rewards/chosen': -1.8683481216430664, 'rewards/rejected': -4.948669910430908, 'rewards/accuracies': 1.0, 'rewards/margins': 3.080321788787842, 'policy_logps/rejected': -338.1785583496094, 'policy_logps/chosen': -396.2317199707031, 'referece_logps/rejected': -288.69189453125, 'referece_logps/chosen': -377.5482482910156, 'logits/rejected': 0.0284518301486969, 'logits/chosen': 0.06908795237541199, 'epoch': 8.37}


 93%|█████████▎| 22459/24156 [29:56:33<7:29:37, 15.90s/it]
{'loss': 0.3163, 'learning_rate': 1.7187413760956422e-06, 'rewards/chosen': -1.79701828956604, 'rewards/rejected': -2.6527371406555176, 'rewards/accuracies': 0.875, 'rewards/margins': 0.855718731880188, 'policy_logps/rejected': -364.9874267578125, 'policy_logps/chosen': -358.1187744140625, 'referece_logps/rejected': -338.4600830078125, 'referece_logps/chosen': -340.14862060546875, 'logits/rejected': 0.13283421099185944, 'logits/chosen': 0.11430323123931885, 'epoch': 8.37}

 93%|█████████▎| 22460/24156 [29:56:44<6:44:10, 14.30s/it]

 93%|█████████▎| 22461/24156 [29:56:54<6:13:08, 13.21s/it]

 93%|█████████▎| 22462/24156 [29:57:08<6:18:35, 13.41s/it]


 93%|█████████▎| 22464/24156 [29:57:33<5:59:13, 12.74s/it]

 93%|█████████▎| 22465/24156 [29:57:49<6:29:19, 13.81s/it]

 93%|█████████▎| 22466/24156 [29:58:09<7:16:42, 15.50s/it]

 93%|█████████▎| 22467/24156 [29:58:29<7:58:51, 17.01s/it]
{'loss': 0.3853, 'learning_rate': 1.7179951887940432e-06, 'rewards/chosen': -2.300487995147705, 'rewards/rejected': -6.7237372398376465, 'rewards/accuracies': 0.875, 'rewards/margins': 4.423248767852783, 'policy_logps/rejected': -447.53741455078125, 'policy_logps/chosen': -486.53802490234375, 'referece_logps/rejected': -380.300048828125, 'referece_logps/chosen': -463.5331726074219, 'logits/rejected': 0.11773882061243057, 'logits/chosen': -0.11741063743829727, 'epoch': 8.37}


 93%|█████████▎| 22469/24156 [29:58:55<7:05:30, 15.13s/it]

 93%|█████████▎| 22470/24156 [29:59:15<7:43:33, 16.50s/it]
{'loss': 0.2843, 'learning_rate': 1.7177151555372992e-06, 'rewards/chosen': -1.8203580379486084, 'rewards/rejected': -4.986712455749512, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1663544178009033, 'policy_logps/rejected': -516.8721313476562, 'policy_logps/chosen': -499.318359375, 'referece_logps/rejected': -467.0050048828125, 'referece_logps/chosen': -481.11474609375, 'logits/rejected': -0.4719693064689636, 'logits/chosen': -0.46205785870552063, 'epoch': 8.37}

 93%|█████████▎| 22471/24156 [29:59:35<8:13:02, 17.56s/it]

 93%|█████████▎| 22472/24156 [29:59:49<7:40:50, 16.42s/it]


 93%|█████████▎| 22474/24156 [30:00:22<7:50:17, 16.78s/it]
{'loss': 0.4305, 'learning_rate': 1.717341597235996e-06, 'rewards/chosen': -2.3069326877593994, 'rewards/rejected': -5.445613384246826, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1386806964874268, 'policy_logps/rejected': -448.1475524902344, 'policy_logps/chosen': -591.0900268554688, 'referece_logps/rejected': -393.69140625, 'referece_logps/chosen': -568.020751953125, 'logits/rejected': -0.24434293806552887, 'logits/chosen': -0.5036752223968506, 'epoch': 8.37}


 93%|█████████▎| 22476/24156 [30:00:44<6:28:12, 13.86s/it]

 93%|█████████▎| 22477/24156 [30:00:59<6:43:58, 14.44s/it]
{'loss': 0.4857, 'learning_rate': 1.7170612930995456e-06, 'rewards/chosen': -1.835479497909546, 'rewards/rejected': -3.676213264465332, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8407337665557861, 'policy_logps/rejected': -466.7420959472656, 'policy_logps/chosen': -470.2601318359375, 'referece_logps/rejected': -429.97998046875, 'referece_logps/chosen': -451.9053649902344, 'logits/rejected': 0.04535666108131409, 'logits/chosen': -0.019897736608982086, 'epoch': 8.37}


 93%|█████████▎| 22479/24156 [30:01:21<5:54:18, 12.68s/it]
{'loss': 0.4216, 'learning_rate': 1.7168743592192613e-06, 'rewards/chosen': -1.975365161895752, 'rewards/rejected': -4.323466777801514, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3481016159057617, 'policy_logps/rejected': -299.2670593261719, 'policy_logps/chosen': -304.1101989746094, 'referece_logps/rejected': -256.0323791503906, 'referece_logps/chosen': -284.3565368652344, 'logits/rejected': -0.059356316924095154, 'logits/chosen': -0.04489697515964508, 'epoch': 8.38}

 93%|█████████▎| 22480/24156 [30:01:39<6:31:45, 14.02s/it]

 93%|█████████▎| 22481/24156 [30:01:57<7:05:26, 15.24s/it]

 93%|█████████▎| 22482/24156 [30:02:18<7:55:23, 17.04s/it]

 93%|█████████▎| 22483/24156 [30:02:33<7:38:52, 16.46s/it]

 93%|█████████▎| 22484/24156 [30:02:55<8:25:38, 18.14s/it]


 93%|█████████▎| 22486/24156 [30:03:24<7:28:29, 16.11s/it]
{'loss': 0.2544, 'learning_rate': 1.716219684777308e-06, 'rewards/chosen': -1.8721749782562256, 'rewards/rejected': -4.360592842102051, 'rewards/accuracies': 0.875, 'rewards/margins': 2.488417625427246, 'policy_logps/rejected': -356.8720397949219, 'policy_logps/chosen': -378.1169738769531, 'referece_logps/rejected': -313.26611328125, 'referece_logps/chosen': -359.3952331542969, 'logits/rejected': -0.46954405307769775, 'logits/chosen': -0.5694555640220642, 'epoch': 8.38}

 93%|█████████▎| 22487/24156 [30:03:38<7:14:02, 15.60s/it]

 93%|█████████▎| 22488/24156 [30:03:51<6:52:19, 14.83s/it]


 93%|█████████▎| 22490/24156 [30:04:22<7:00:55, 15.16s/it]
{'loss': 0.3318, 'learning_rate': 1.71584530179804e-06, 'rewards/chosen': -1.6574032306671143, 'rewards/rejected': -2.3710737228393555, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7136704325675964, 'policy_logps/rejected': -328.3265075683594, 'policy_logps/chosen': -576.57275390625, 'referece_logps/rejected': -304.6157531738281, 'referece_logps/chosen': -559.9986572265625, 'logits/rejected': -0.6125203371047974, 'logits/chosen': -0.6844503283500671, 'epoch': 8.38}


 93%|█████████▎| 22492/24156 [30:04:45<6:16:31, 13.58s/it]

 93%|█████████▎| 22493/24156 [30:05:02<6:39:01, 14.40s/it]

 93%|█████████▎| 22494/24156 [30:05:13<6:17:04, 13.61s/it]

 93%|█████████▎| 22495/24156 [30:05:36<7:29:17, 16.23s/it]
{'loss': 0.3577, 'learning_rate': 1.7153770335391034e-06, 'rewards/chosen': -1.9805097579956055, 'rewards/rejected': -4.317996025085449, 'rewards/accuracies': 0.75, 'rewards/margins': 2.337486505508423, 'policy_logps/rejected': -379.1738586425781, 'policy_logps/chosen': -475.80413818359375, 'referece_logps/rejected': -335.9938659667969, 'referece_logps/chosen': -455.99896240234375, 'logits/rejected': -0.29502785205841064, 'logits/chosen': -0.27213960886001587, 'epoch': 8.38}


 93%|█████████▎| 22497/24156 [30:06:02<6:40:34, 14.49s/it]
{'loss': 0.3885, 'learning_rate': 1.7151896362012292e-06, 'rewards/chosen': -2.8370964527130127, 'rewards/rejected': -3.993060350418091, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1559638977050781, 'policy_logps/rejected': -449.97454833984375, 'policy_logps/chosen': -459.114013671875, 'referece_logps/rejected': -410.0439453125, 'referece_logps/chosen': -430.7430725097656, 'logits/rejected': -0.025401290506124496, 'logits/chosen': -0.03629768267273903, 'epoch': 8.38}

 93%|█████████▎| 22498/24156 [30:06:17<6:50:28, 14.85s/it]

 93%|█████████▎| 22499/24156 [30:06:30<6:36:49, 14.37s/it]

 93%|█████████▎| 22500/24156 [30:06:46<6:47:59, 14.78s/it]

 93%|█████████▎| 22501/24156 [30:07:23<9:51:22, 21.44s/it]


 93%|█████████▎| 22503/24156 [30:08:00<8:55:21, 19.43s/it]
{'loss': 0.3016, 'learning_rate': 1.7146271356737664e-06, 'rewards/chosen': -1.7263222932815552, 'rewards/rejected': -3.3919966220855713, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6656742095947266, 'policy_logps/rejected': -416.0487365722656, 'policy_logps/chosen': -366.05047607421875, 'referece_logps/rejected': -382.1287536621094, 'referece_logps/chosen': -348.7872619628906, 'logits/rejected': -1.2264729738235474, 'logits/chosen': -1.2230018377304077, 'epoch': 8.38}


 93%|█████████▎| 22505/24156 [30:08:36<8:32:45, 18.63s/it]
{'loss': 0.2405, 'learning_rate': 1.714439532704941e-06, 'rewards/chosen': -2.5029895305633545, 'rewards/rejected': -3.950380802154541, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4473910331726074, 'policy_logps/rejected': -270.9939880371094, 'policy_logps/chosen': -324.7530517578125, 'referece_logps/rejected': -231.4901885986328, 'referece_logps/chosen': -299.72314453125, 'logits/rejected': -0.08774635195732117, 'logits/chosen': -0.06364162266254425, 'epoch': 8.38}


 93%|█████████▎| 22507/24156 [30:09:14<8:34:32, 18.72s/it]

 93%|█████████▎| 22508/24156 [30:09:32<8:28:32, 18.51s/it]
{'loss': 0.3291, 'learning_rate': 1.7141580319296292e-06, 'rewards/chosen': -2.8748199939727783, 'rewards/rejected': -5.113278388977051, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2384583950042725, 'policy_logps/rejected': -376.3197021484375, 'policy_logps/chosen': -429.734375, 'referece_logps/rejected': -325.1869201660156, 'referece_logps/chosen': -400.9862060546875, 'logits/rejected': -0.448517382144928, 'logits/chosen': -0.5344308614730835, 'epoch': 8.39}

 93%|█████████▎| 22509/24156 [30:09:45<7:41:35, 16.82s/it]

 93%|█████████▎| 22510/24156 [30:10:01<7:34:33, 16.57s/it]


 93%|█████████▎| 22512/24156 [30:10:42<8:26:48, 18.50s/it]
{'loss': 0.301, 'learning_rate': 1.713782517832195e-06, 'rewards/chosen': -2.2380566596984863, 'rewards/rejected': -5.521553993225098, 'rewards/accuracies': 1.0, 'rewards/margins': 3.283496618270874, 'policy_logps/rejected': -330.96710205078125, 'policy_logps/chosen': -324.48785400390625, 'referece_logps/rejected': -275.7515869140625, 'referece_logps/chosen': -302.1072692871094, 'logits/rejected': -0.3931541442871094, 'logits/chosen': -0.3167305588722229, 'epoch': 8.39}

 93%|█████████▎| 22513/24156 [30:11:03<8:46:39, 19.23s/it]

 93%|█████████▎| 22514/24156 [30:11:15<7:47:46, 17.09s/it]


 93%|█████████▎| 22516/24156 [30:11:52<8:12:48, 18.03s/it]

 93%|█████████▎| 22517/24156 [30:12:10<8:17:54, 18.23s/it]
{'loss': 0.2872, 'learning_rate': 1.713312836509951e-06, 'rewards/chosen': -2.372174024581909, 'rewards/rejected': -4.164589881896973, 'rewards/accuracies': 0.875, 'rewards/margins': 1.792415738105774, 'policy_logps/rejected': -359.1551818847656, 'policy_logps/chosen': -305.0476379394531, 'referece_logps/rejected': -317.5093078613281, 'referece_logps/chosen': -281.3258972167969, 'logits/rejected': -0.2922596037387848, 'logits/chosen': -0.2431655377149582, 'epoch': 8.39}


 93%|█████████▎| 22519/24156 [30:12:50<8:44:09, 19.21s/it]

 93%|█████████▎| 22520/24156 [30:13:10<8:47:51, 19.36s/it]
{'loss': 0.3668, 'learning_rate': 1.713030873824085e-06, 'rewards/chosen': -2.675757646560669, 'rewards/rejected': -4.652105808258057, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9763481616973877, 'policy_logps/rejected': -493.04595947265625, 'policy_logps/chosen': -416.9454345703125, 'referece_logps/rejected': -446.52490234375, 'referece_logps/chosen': -390.18780517578125, 'logits/rejected': -0.00030726566910743713, 'logits/chosen': 0.017635032534599304, 'epoch': 8.39}

 93%|█████████▎| 22521/24156 [30:13:25<8:12:28, 18.07s/it]

 93%|█████████▎| 22522/24156 [30:13:43<8:08:06, 17.92s/it]

 93%|█████████▎| 22523/24156 [30:14:05<8:42:17, 19.19s/it]


 93%|█████████▎| 22525/24156 [30:14:38<8:05:52, 17.87s/it]

 93%|█████████▎| 22526/24156 [30:14:51<7:18:38, 16.15s/it]
{'loss': 0.2866, 'learning_rate': 1.7124666024070829e-06, 'rewards/chosen': -2.351437568664551, 'rewards/rejected': -4.142329216003418, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7908921241760254, 'policy_logps/rejected': -395.4478759765625, 'policy_logps/chosen': -382.3626403808594, 'referece_logps/rejected': -354.02459716796875, 'referece_logps/chosen': -358.8482360839844, 'logits/rejected': -0.4457438588142395, 'logits/chosen': -0.39629632234573364, 'epoch': 8.39}


 93%|█████████▎| 22528/24156 [30:15:20<6:57:50, 15.40s/it]
{'loss': 0.2996, 'learning_rate': 1.712278409452408e-06, 'rewards/chosen': -1.2544547319412231, 'rewards/rejected': -3.846440315246582, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5919854640960693, 'policy_logps/rejected': -345.7143249511719, 'policy_logps/chosen': -342.0734558105469, 'referece_logps/rejected': -307.2498779296875, 'referece_logps/chosen': -329.5289001464844, 'logits/rejected': -0.46979379653930664, 'logits/chosen': -0.5083879828453064, 'epoch': 8.39}

 93%|█████████▎| 22529/24156 [30:15:33<6:38:27, 14.69s/it]

 93%|█████████▎| 22530/24156 [30:15:53<7:15:33, 16.07s/it]

 93%|█████████▎| 22531/24156 [30:16:14<7:53:16, 17.47s/it]

 93%|█████████▎| 22532/24156 [30:16:35<8:23:50, 18.61s/it]


 93%|█████████▎| 22534/24156 [30:17:06<7:42:09, 17.10s/it]
{'loss': 0.3831, 'learning_rate': 1.7117135233308164e-06, 'rewards/chosen': -2.234867572784424, 'rewards/rejected': -3.284298896789551, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0494309663772583, 'policy_logps/rejected': -370.1338195800781, 'policy_logps/chosen': -285.3604736328125, 'referece_logps/rejected': -337.29083251953125, 'referece_logps/chosen': -263.0118408203125, 'logits/rejected': 0.18746837973594666, 'logits/chosen': 0.03115861304104328, 'epoch': 8.4}

 93%|█████████▎| 22535/24156 [30:17:25<7:55:45, 17.61s/it]

 93%|█████████▎| 22536/24156 [30:17:46<8:20:27, 18.54s/it]


 93%|█████████▎| 22538/24156 [30:18:23<8:19:45, 18.53s/it]

 93%|█████████▎| 22539/24156 [30:18:37<7:43:21, 17.19s/it]

 93%|█████████▎| 22540/24156 [30:18:56<8:01:49, 17.89s/it]
{'loss': 0.3544, 'learning_rate': 1.7111481766072532e-06, 'rewards/chosen': -3.2902371883392334, 'rewards/rejected': -4.243341445922852, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9531044960021973, 'policy_logps/rejected': -419.8333435058594, 'policy_logps/chosen': -479.60235595703125, 'referece_logps/rejected': -377.3999328613281, 'referece_logps/chosen': -446.6999816894531, 'logits/rejected': -0.32931792736053467, 'logits/chosen': -0.35299962759017944, 'epoch': 8.4}


 93%|█████████▎| 22542/24156 [30:19:27<7:33:10, 16.85s/it]
{'loss': 0.2835, 'learning_rate': 1.7109596254066347e-06, 'rewards/chosen': -2.136040687561035, 'rewards/rejected': -3.7421176433563232, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6060771942138672, 'policy_logps/rejected': -423.4826354980469, 'policy_logps/chosen': -441.21844482421875, 'referece_logps/rejected': -386.06146240234375, 'referece_logps/chosen': -419.8580322265625, 'logits/rejected': -0.17239892482757568, 'logits/chosen': -0.12181279063224792, 'epoch': 8.4}

 93%|█████████▎| 22543/24156 [30:19:45<7:46:25, 17.35s/it]


 93%|█████████▎| 22545/24156 [30:20:21<7:55:51, 17.72s/it]

 93%|█████████▎| 22546/24156 [30:20:41<8:12:16, 18.35s/it]
{'loss': 0.4451, 'learning_rate': 1.7105823696475962e-06, 'rewards/chosen': -1.8966388702392578, 'rewards/rejected': -3.8130242824554443, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9163851737976074, 'policy_logps/rejected': -353.48297119140625, 'policy_logps/chosen': -232.23504638671875, 'referece_logps/rejected': -315.35272216796875, 'referece_logps/chosen': -213.26866149902344, 'logits/rejected': -0.8913397192955017, 'logits/chosen': -0.7670975923538208, 'epoch': 8.4}

 93%|█████████▎| 22547/24156 [30:21:00<8:22:46, 18.75s/it]


 93%|█████████▎| 22549/24156 [30:21:31<7:24:34, 16.60s/it]

 93%|█████████▎| 22550/24156 [30:21:53<8:09:11, 18.28s/it]

 93%|█████████▎| 22551/24156 [30:22:13<8:20:06, 18.70s/it]

 93%|█████████▎| 22552/24156 [30:22:25<7:25:02, 16.65s/it]

 93%|█████████▎| 22553/24156 [30:22:41<7:21:47, 16.54s/it]
{'loss': 0.3742, 'learning_rate': 1.7099216803292423e-06, 'rewards/chosen': -2.402346611022949, 'rewards/rejected': -3.9011783599853516, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4988319873809814, 'policy_logps/rejected': -281.5105285644531, 'policy_logps/chosen': -253.6269989013672, 'referece_logps/rejected': -242.49874877929688, 'referece_logps/chosen': -229.60353088378906, 'logits/rejected': -1.0370454788208008, 'logits/chosen': -1.1535518169403076, 'epoch': 8.4}

 93%|█████████▎| 22554/24156 [30:22:59<7:37:01, 17.12s/it]

 93%|█████████▎| 22555/24156 [30:23:10<6:45:06, 15.18s/it]

 93%|█████████▎| 22556/24156 [30:23:30<7:21:05, 16.54s/it]

 93%|█████████▎| 22557/24156 [30:23:50<7:52:54, 17.75s/it]

 93%|█████████▎| 22558/24156 [30:24:02<7:03:23, 15.90s/it]

 93%|█████████▎| 22559/24156 [30:24:16<6:45:30, 15.24s/it]

 93%|█████████▎| 22560/24156 [30:24:36<7:24:13, 16.70s/it]

 93%|█████████▎| 22561/24156 [30:24:56<7:49:24, 17.66s/it]


 93%|█████████▎| 22563/24156 [30:25:25<6:58:11, 15.75s/it]
{'loss': 0.2087, 'learning_rate': 1.7089767537958308e-06, 'rewards/chosen': -1.8941712379455566, 'rewards/rejected': -4.4659600257873535, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5717885494232178, 'policy_logps/rejected': -400.2410888671875, 'policy_logps/chosen': -278.9552001953125, 'referece_logps/rejected': -355.5815124511719, 'referece_logps/chosen': -260.0134582519531, 'logits/rejected': -0.1422642022371292, 'logits/chosen': -0.10119397193193436, 'epoch': 8.41}


 93%|█████████▎| 22565/24156 [30:25:51<6:20:38, 14.35s/it]
{'loss': 0.3272, 'learning_rate': 1.7087876154911621e-06, 'rewards/chosen': -2.2105860710144043, 'rewards/rejected': -3.647787094116211, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4372010231018066, 'policy_logps/rejected': -567.2825317382812, 'policy_logps/chosen': -393.97552490234375, 'referece_logps/rejected': -530.8046264648438, 'referece_logps/chosen': -371.8696594238281, 'logits/rejected': -0.6038091778755188, 'logits/chosen': -0.49540379643440247, 'epoch': 8.41}


 93%|█████████▎| 22567/24156 [30:26:25<7:04:42, 16.04s/it]

 93%|█████████▎| 22568/24156 [30:26:43<7:18:54, 16.58s/it]

 93%|█████████▎| 22569/24156 [30:27:01<7:31:44, 17.08s/it]
{'loss': 0.2954, 'learning_rate': 1.7084091859926207e-06, 'rewards/chosen': -1.9660608768463135, 'rewards/rejected': -4.338172912597656, 'rewards/accuracies': 0.875, 'rewards/margins': 2.372112274169922, 'policy_logps/rejected': -311.9195861816406, 'policy_logps/chosen': -387.1076965332031, 'referece_logps/rejected': -268.537841796875, 'referece_logps/chosen': -367.44708251953125, 'logits/rejected': 0.18624690175056458, 'logits/chosen': 0.02654983103275299, 'epoch': 8.41}


 93%|█████████▎| 22571/24156 [30:27:35<7:35:18, 17.24s/it]
{'loss': 0.3575, 'learning_rate': 1.7082198948259604e-06, 'rewards/chosen': -2.2972805500030518, 'rewards/rejected': -3.380695104598999, 'rewards/accuracies': 0.75, 'rewards/margins': 1.0834145545959473, 'policy_logps/rejected': -463.343505859375, 'policy_logps/chosen': -490.5133056640625, 'referece_logps/rejected': -429.5365905761719, 'referece_logps/chosen': -467.54052734375, 'logits/rejected': -0.18450875580310822, 'logits/chosen': -0.32009539008140564, 'epoch': 8.41}


 93%|█████████▎| 22573/24156 [30:28:05<7:01:07, 15.96s/it]
{'loss': 0.2834, 'learning_rate': 1.708030552732521e-06, 'rewards/chosen': -3.2350687980651855, 'rewards/rejected': -5.0550713539123535, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8200026750564575, 'policy_logps/rejected': -402.50677490234375, 'policy_logps/chosen': -339.3962097167969, 'referece_logps/rejected': -351.95611572265625, 'referece_logps/chosen': -307.0455322265625, 'logits/rejected': 0.22636467218399048, 'logits/chosen': 0.2111416459083557, 'epoch': 8.41}


 93%|█████████▎| 22575/24156 [30:28:41<7:26:42, 16.95s/it]
{'loss': 0.4423, 'learning_rate': 1.707841159725918e-06, 'rewards/chosen': -2.2671735286712646, 'rewards/rejected': -3.639070749282837, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3718971014022827, 'policy_logps/rejected': -455.6440124511719, 'policy_logps/chosen': -522.14599609375, 'referece_logps/rejected': -419.2532958984375, 'referece_logps/chosen': -499.4742431640625, 'logits/rejected': 0.28587546944618225, 'logits/chosen': 0.28696131706237793, 'epoch': 8.41}


 93%|█████████▎| 22577/24156 [30:29:15<7:22:14, 16.80s/it]
{'loss': 0.3717, 'learning_rate': 1.7076517158197698e-06, 'rewards/chosen': -3.3795440196990967, 'rewards/rejected': -5.77044677734375, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3909027576446533, 'policy_logps/rejected': -350.6568908691406, 'policy_logps/chosen': -404.40875244140625, 'referece_logps/rejected': -292.95245361328125, 'referece_logps/chosen': -370.61328125, 'logits/rejected': -0.26953747868537903, 'logits/chosen': -0.29862597584724426, 'epoch': 8.41}


 93%|█████████▎| 22579/24156 [30:29:43<6:36:13, 15.08s/it]
{'loss': 0.2512, 'learning_rate': 1.7074622210276997e-06, 'rewards/chosen': -2.1298670768737793, 'rewards/rejected': -5.8390350341796875, 'rewards/accuracies': 1.0, 'rewards/margins': 3.709167957305908, 'policy_logps/rejected': -329.9063720703125, 'policy_logps/chosen': -437.0296630859375, 'referece_logps/rejected': -271.51605224609375, 'referece_logps/chosen': -415.73101806640625, 'logits/rejected': -0.060117922723293304, 'logits/chosen': -0.21177011728286743, 'epoch': 8.41}

 93%|█████████▎| 22580/24156 [30:30:04<7:23:10, 16.87s/it]

 93%|█████████▎| 22581/24156 [30:30:18<7:00:20, 16.01s/it]

 93%|█████████▎| 22582/24156 [30:30:32<6:41:11, 15.29s/it]

 93%|█████████▎| 22583/24156 [30:30:48<6:47:07, 15.53s/it]

 93%|█████████▎| 22584/24156 [30:31:08<7:21:15, 16.84s/it]

 93%|█████████▎| 22585/24156 [30:31:27<7:38:17, 17.50s/it]

 94%|█████████▎| 22586/24156 [30:31:42<7:19:28, 16.80s/it]

 94%|█████████▎| 22587/24156 [30:31:55<6:51:21, 15.73s/it]

 94%|█████████▎| 22588/24156 [30:32:09<6:34:30, 15.10s/it]


 94%|█████████▎| 22590/24156 [30:32:44<7:14:46, 16.66s/it]
{'loss': 0.3276, 'learning_rate': 1.7064190906945588e-06, 'rewards/chosen': -1.9345319271087646, 'rewards/rejected': -3.2647149562835693, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3301829099655151, 'policy_logps/rejected': -307.22564697265625, 'policy_logps/chosen': -492.7693176269531, 'referece_logps/rejected': -274.5785217285156, 'referece_logps/chosen': -473.4239501953125, 'logits/rejected': -0.18889489769935608, 'logits/chosen': -0.270269513130188, 'epoch': 8.42}

 94%|█████████▎| 22591/24156 [30:32:58<6:52:16, 15.81s/it]

 94%|█████████▎| 22592/24156 [30:33:16<7:12:47, 16.60s/it]

 94%|█████████▎| 22593/24156 [30:33:30<6:49:24, 15.72s/it]

 94%|█████████▎| 22594/24156 [30:33:44<6:35:49, 15.20s/it]

 94%|█████████▎| 22595/24156 [30:33:59<6:40:31, 15.39s/it]

 94%|█████████▎| 22596/24156 [30:34:16<6:46:37, 15.64s/it]

 94%|█████████▎| 22597/24156 [30:34:33<7:00:22, 16.18s/it]

 94%|█████████▎| 22598/24156 [30:34:50<7:06:04, 16.41s/it]

 94%|█████████▎| 22599/24156 [30:35:08<7:17:21, 16.85s/it]

 94%|█████████▎| 22600/24156 [30:35:28<7:40:07, 17.74s/it]

 94%|█████████▎| 22601/24156 [30:35:38<6:44:56, 15.62s/it]

 94%|█████████▎| 22602/24156 [30:35:49<6:06:06, 14.14s/it]

 94%|█████████▎| 22603/24156 [30:36:02<5:59:13, 13.88s/it]

 94%|█████████▎| 22604/24156 [30:36:13<5:34:15, 12.92s/it]

 94%|█████████▎| 22605/24156 [30:36:30<6:08:12, 14.24s/it]

 94%|█████████▎| 22606/24156 [30:36:41<5:40:32, 13.18s/it]

 94%|█████████▎| 22607/24156 [30:37:01<6:32:41, 15.21s/it]

 94%|█████████▎| 22608/24156 [30:37:12<5:57:30, 13.86s/it]

 94%|█████████▎| 22609/24156 [30:37:26<6:02:25, 14.06s/it]

 94%|█████████▎| 22610/24156 [30:37:37<5:38:18, 13.13s/it]

 94%|█████████▎| 22611/24156 [30:37:50<5:37:23, 13.10s/it]

 94%|█████████▎| 22612/24156 [30:38:06<5:54:14, 13.77s/it]

 94%|█████████▎| 22613/24156 [30:38:21<6:08:53, 14.34s/it]

 94%|█████████▎| 22614/24156 [30:38:37<6:21:44, 14.85s/it]

 94%|█████████▎| 22615/24156 [30:38:58<7:10:00, 16.74s/it]

 94%|█████████▎| 22616/24156 [30:39:13<6:54:43, 16.16s/it]

 94%|█████████▎| 22617/24156 [30:39:33<7:20:43, 17.18s/it]

 94%|█████████▎| 22618/24156 [30:39:55<7:59:48, 18.72s/it]

 94%|█████████▎| 22619/24156 [30:40:11<7:36:29, 17.82s/it]

 94%|█████████▎| 22620/24156 [30:40:27<7:22:14, 17.27s/it]

 94%|█████████▎| 22621/24156 [30:40:38<6:35:03, 15.44s/it]

 94%|█████████▎| 22622/24156 [30:40:53<6:27:47, 15.17s/it]

 94%|█████████▎| 22623/24156 [30:41:06<6:17:50, 14.79s/it]

 94%|█████████▎| 22624/24156 [30:41:22<6:22:59, 15.00s/it]

 94%|█████████▎| 22625/24156 [30:41:43<7:08:01, 16.77s/it]

 94%|█████████▎| 22626/24156 [30:42:03<7:31:53, 17.72s/it]

 94%|█████████▎| 22627/24156 [30:42:22<7:46:15, 18.30s/it]

 94%|█████████▎| 22628/24156 [30:42:35<7:00:28, 16.51s/it]

 94%|█████████▎| 22629/24156 [30:42:49<6:44:21, 15.89s/it]

 94%|█████████▎| 22630/24156 [30:43:06<6:48:32, 16.06s/it]

 94%|█████████▎| 22631/24156 [30:43:27<7:30:32, 17.73s/it]


 94%|█████████▎| 22633/24156 [30:43:56<6:46:18, 16.01s/it]
{'loss': 0.2926, 'learning_rate': 1.7023266766401633e-06, 'rewards/chosen': -3.059943437576294, 'rewards/rejected': -5.770554065704346, 'rewards/accuracies': 0.875, 'rewards/margins': 2.710610866546631, 'policy_logps/rejected': -359.5813293457031, 'policy_logps/chosen': -370.8039855957031, 'referece_logps/rejected': -301.87579345703125, 'referece_logps/chosen': -340.20452880859375, 'logits/rejected': -0.2571808993816376, 'logits/chosen': -0.21626953780651093, 'epoch': 8.43}

 94%|█████████▎| 22634/24156 [30:44:11<6:38:58, 15.73s/it]

 94%|█████████▎| 22635/24156 [30:44:28<6:45:53, 16.01s/it]

 94%|█████████▎| 22636/24156 [30:44:41<6:20:06, 15.00s/it]

 94%|█████████▎| 22637/24156 [30:45:00<6:52:52, 16.31s/it]

 94%|█████████▎| 22638/24156 [30:45:20<7:18:02, 17.31s/it]

 94%|█████████▎| 22639/24156 [30:45:38<7:27:42, 17.71s/it]


 94%|█████████▎| 22641/24156 [30:46:02<6:17:27, 14.95s/it]
{'loss': 0.4515, 'learning_rate': 1.701562717550193e-06, 'rewards/chosen': -2.2876904010772705, 'rewards/rejected': -3.666141986846924, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3784517049789429, 'policy_logps/rejected': -406.8985290527344, 'policy_logps/chosen': -464.36279296875, 'referece_logps/rejected': -370.23712158203125, 'referece_logps/chosen': -441.4859313964844, 'logits/rejected': -0.8097736835479736, 'logits/chosen': -0.8581552505493164, 'epoch': 8.44}

 94%|█████████▎| 22642/24156 [30:46:21<6:45:06, 16.05s/it]


 94%|█████████▎| 22644/24156 [30:46:45<5:48:56, 13.85s/it]
{'loss': 0.3373, 'learning_rate': 1.7012760247459296e-06, 'rewards/chosen': -2.734381675720215, 'rewards/rejected': -4.834938049316406, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1005566120147705, 'policy_logps/rejected': -561.7926635742188, 'policy_logps/chosen': -546.9026489257812, 'referece_logps/rejected': -513.4432373046875, 'referece_logps/chosen': -519.558837890625, 'logits/rejected': -0.6936646103858948, 'logits/chosen': -0.6981582641601562, 'epoch': 8.44}

 94%|█████████▎| 22645/24156 [30:46:55<5:25:03, 12.91s/it]

 94%|█████████▎| 22646/24156 [30:47:18<6:41:11, 15.94s/it]

 94%|█████████▍| 22647/24156 [30:47:32<6:21:41, 15.18s/it]

 94%|█████████▍| 22648/24156 [30:47:51<6:56:05, 16.56s/it]

 94%|█████████▍| 22649/24156 [30:48:09<7:04:54, 16.92s/it]

 94%|█████████▍| 22650/24156 [30:48:22<6:35:56, 15.77s/it]

 94%|█████████▍| 22651/24156 [30:48:45<7:28:13, 17.87s/it]

 94%|█████████▍| 22652/24156 [30:48:58<6:48:33, 16.30s/it]

 94%|█████████▍| 22653/24156 [30:49:18<7:19:58, 17.56s/it]

 94%|█████████▍| 22654/24156 [30:49:31<6:45:52, 16.21s/it]

 94%|█████████▍| 22655/24156 [30:49:51<7:12:51, 17.30s/it]

 94%|█████████▍| 22656/24156 [30:50:04<6:38:24, 15.94s/it]

 94%|█████████▍| 22657/24156 [30:50:15<5:58:34, 14.35s/it]

 94%|█████████▍| 22658/24156 [30:50:26<5:35:00, 13.42s/it]

 94%|█████████▍| 22659/24156 [30:50:47<6:29:16, 15.60s/it]

 94%|█████████▍| 22660/24156 [30:51:04<6:41:04, 16.09s/it]

 94%|█████████▍| 22661/24156 [30:51:24<7:08:33, 17.20s/it]

 94%|█████████▍| 22662/24156 [30:51:43<7:21:34, 17.73s/it]

 94%|█████████▍| 22663/24156 [30:51:56<6:49:28, 16.46s/it]

 94%|█████████▍| 22664/24156 [30:52:17<7:20:33, 17.72s/it]

 94%|█████████▍| 22665/24156 [30:52:37<7:41:04, 18.55s/it]

 94%|█████████▍| 22666/24156 [30:52:58<7:54:48, 19.12s/it]

 94%|█████████▍| 22667/24156 [30:53:16<7:50:37, 18.96s/it]

 94%|█████████▍| 22668/24156 [30:53:32<7:29:39, 18.13s/it]

 94%|█████████▍| 22669/24156 [30:53:49<7:21:18, 17.81s/it]


 94%|█████████▍| 22671/24156 [30:54:25<7:15:57, 17.61s/it]
{'loss': 0.3486, 'learning_rate': 1.698690689299713e-06, 'rewards/chosen': -2.2083778381347656, 'rewards/rejected': -3.1585004329681396, 'rewards/accuracies': 0.75, 'rewards/margins': 0.950122594833374, 'policy_logps/rejected': -326.80511474609375, 'policy_logps/chosen': -440.86614990234375, 'referece_logps/rejected': -295.2200927734375, 'referece_logps/chosen': -418.7823486328125, 'logits/rejected': 0.16603322327136993, 'logits/chosen': 0.17513135075569153, 'epoch': 8.45}

 94%|█████████▍| 22672/24156 [30:54:42<7:12:32, 17.49s/it]

 94%|█████████▍| 22673/24156 [30:54:58<7:00:20, 17.01s/it]

 94%|█████████▍| 22674/24156 [30:55:14<6:53:41, 16.75s/it]

 94%|█████████▍| 22675/24156 [30:55:26<6:16:43, 15.26s/it]


 94%|█████████▍| 22677/24156 [30:56:01<6:54:48, 16.83s/it]

 94%|█████████▍| 22678/24156 [30:56:19<7:02:26, 17.15s/it]

 94%|█████████▍| 22679/24156 [30:56:39<7:21:53, 17.95s/it]

 94%|█████████▍| 22680/24156 [30:56:51<6:38:14, 16.19s/it]

 94%|█████████▍| 22681/24156 [30:57:10<6:56:55, 16.96s/it]

 94%|█████████▍| 22682/24156 [30:57:30<7:16:53, 17.78s/it]

 94%|█████████▍| 22683/24156 [30:57:47<7:14:20, 17.69s/it]

 94%|█████████▍| 22684/24156 [30:58:04<7:07:32, 17.43s/it]

 94%|█████████▍| 22685/24156 [30:58:24<7:25:29, 18.17s/it]

 94%|█████████▍| 22686/24156 [30:58:40<7:10:52, 17.59s/it]

 94%|█████████▍| 22687/24156 [30:58:57<7:03:36, 17.30s/it]

 94%|█████████▍| 22688/24156 [30:59:14<7:06:49, 17.45s/it]

 94%|█████████▍| 22689/24156 [30:59:25<6:16:55, 15.42s/it]

 94%|█████████▍| 22690/24156 [30:59:38<5:57:13, 14.62s/it]

 94%|█████████▍| 22691/24156 [30:59:49<5:29:16, 13.49s/it]

 94%|█████████▍| 22692/24156 [31:00:02<5:30:23, 13.54s/it]

 94%|█████████▍| 22693/24156 [31:00:16<5:31:07, 13.58s/it]

 94%|█████████▍| 22694/24156 [31:00:29<5:25:50, 13.37s/it]

 94%|█████████▍| 22695/24156 [31:00:48<6:07:17, 15.08s/it]

 94%|█████████▍| 22696/24156 [31:01:04<6:16:54, 15.49s/it]

 94%|█████████▍| 22697/24156 [31:01:24<6:48:49, 16.81s/it]

 94%|█████████▍| 22698/24156 [31:01:42<6:55:14, 17.09s/it]

 94%|█████████▍| 22699/24156 [31:01:55<6:22:21, 15.75s/it]

 94%|█████████▍| 22700/24156 [31:02:12<6:30:04, 16.07s/it]

 94%|█████████▍| 22701/24156 [31:02:29<6:39:03, 16.46s/it]

 94%|█████████▍| 22702/24156 [31:02:46<6:46:16, 16.77s/it]

 94%|█████████▍| 22703/24156 [31:03:08<7:18:13, 18.10s/it]

 94%|█████████▍| 22704/24156 [31:03:23<6:55:47, 17.18s/it]
{'loss': 0.315, 'learning_rate': 1.695518403512982e-06, 'rewards/chosen': -1.4564683437347412, 'rewards/rejected': -3.3526318073272705, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8961635828018188, 'policy_logps/rejected': -369.75567626953125, 'policy_logps/chosen': -370.2378845214844, 'referece_logps/rejected': -336.2293701171875, 'referece_logps/chosen': -355.6731872558594, 'logits/rejected': 0.1912028193473816, 'logits/chosen': 0.12535123527050018, 'epoch': 8.46}


 94%|█████████▍| 22706/24156 [31:03:55<6:47:45, 16.87s/it]

 94%|█████████▍| 22707/24156 [31:04:15<7:12:14, 17.90s/it]

 94%|█████████▍| 22708/24156 [31:04:33<7:16:02, 18.07s/it]

 94%|█████████▍| 22709/24156 [31:04:53<7:27:54, 18.57s/it]

 94%|█████████▍| 22710/24156 [31:05:11<7:24:44, 18.45s/it]

 94%|█████████▍| 22711/24156 [31:05:31<7:33:57, 18.85s/it]

 94%|█████████▍| 22712/24156 [31:05:51<7:38:07, 19.04s/it]

 94%|█████████▍| 22713/24156 [31:06:06<7:09:12, 17.85s/it]

 94%|█████████▍| 22714/24156 [31:06:23<7:06:15, 17.74s/it]

 94%|█████████▍| 22715/24156 [31:06:43<7:20:01, 18.32s/it]

 94%|█████████▍| 22716/24156 [31:07:02<7:28:33, 18.69s/it]

 94%|█████████▍| 22717/24156 [31:07:18<7:02:14, 17.61s/it]

 94%|█████████▍| 22718/24156 [31:07:39<7:28:19, 18.71s/it]
{'loss': 0.4183, 'learning_rate': 1.694168468091596e-06, 'rewards/chosen': -2.1695761680603027, 'rewards/rejected': -2.436763286590576, 'rewards/accuracies': 0.625, 'rewards/margins': 0.26718732714653015, 'policy_logps/rejected': -516.92529296875, 'policy_logps/chosen': -577.6847534179688, 'referece_logps/rejected': -492.5576477050781, 'referece_logps/chosen': -555.989013671875, 'logits/rejected': -0.06636153161525726, 'logits/chosen': -0.05698278546333313, 'epoch': 8.46}


 94%|█████████▍| 22720/24156 [31:08:13<7:13:54, 18.13s/it]

 94%|█████████▍| 22721/24156 [31:08:33<7:27:39, 18.72s/it]

 94%|█████████▍| 22722/24156 [31:08:52<7:27:02, 18.70s/it]

 94%|█████████▍| 22723/24156 [31:09:11<7:26:01, 18.67s/it]

 94%|█████████▍| 22724/24156 [31:09:28<7:14:36, 18.21s/it]

 94%|█████████▍| 22725/24156 [31:09:47<7:23:26, 18.59s/it]

 94%|█████████▍| 22726/24156 [31:10:09<7:43:21, 19.44s/it]

 94%|█████████▍| 22727/24156 [31:10:24<7:10:50, 18.09s/it]

 94%|█████████▍| 22728/24156 [31:10:44<7:24:36, 18.68s/it]

 94%|█████████▍| 22729/24156 [31:10:55<6:31:49, 16.47s/it]

 94%|█████████▍| 22730/24156 [31:11:17<7:13:22, 18.23s/it]

 94%|█████████▍| 22731/24156 [31:11:37<7:22:08, 18.62s/it]

 94%|█████████▍| 22732/24156 [31:11:52<6:56:03, 17.53s/it]
{'loss': 0.335, 'learning_rate': 1.6928160867689342e-06, 'rewards/chosen': -1.7592973709106445, 'rewards/rejected': -3.6633713245391846, 'rewards/accuracies': 1.0, 'rewards/margins': 1.90407395362854, 'policy_logps/rejected': -373.6724853515625, 'policy_logps/chosen': -320.8610534667969, 'referece_logps/rejected': -337.0387878417969, 'referece_logps/chosen': -303.26806640625, 'logits/rejected': -0.42115387320518494, 'logits/chosen': -0.31285592913627625, 'epoch': 8.47}

 94%|█████████▍| 22733/24156 [31:12:03<6:07:56, 15.51s/it]


 94%|█████████▍| 22735/24156 [31:12:35<6:19:20, 16.02s/it]

 94%|█████████▍| 22736/24156 [31:12:57<7:01:07, 17.79s/it]

 94%|█████████▍| 22737/24156 [31:13:17<7:14:51, 18.39s/it]

 94%|█████████▍| 22738/24156 [31:13:28<6:20:44, 16.11s/it]

 94%|█████████▍| 22739/24156 [31:13:44<6:20:34, 16.11s/it]

 94%|█████████▍| 22740/24156 [31:14:02<6:33:47, 16.69s/it]

 94%|█████████▍| 22741/24156 [31:14:15<6:06:18, 15.53s/it]

 94%|█████████▍| 22742/24156 [31:14:33<6:24:45, 16.33s/it]

 94%|█████████▍| 22743/24156 [31:14:51<6:38:46, 16.93s/it]

 94%|█████████▍| 22744/24156 [31:15:08<6:34:18, 16.76s/it]

 94%|█████████▍| 22745/24156 [31:15:27<6:54:21, 17.62s/it]

 94%|█████████▍| 22746/24156 [31:15:47<7:07:34, 18.19s/it]

 94%|█████████▍| 22747/24156 [31:16:08<7:28:58, 19.12s/it]

 94%|█████████▍| 22748/24156 [31:16:29<7:40:56, 19.64s/it]

 94%|█████████▍| 22749/24156 [31:16:47<7:30:57, 19.23s/it]

 94%|█████████▍| 22750/24156 [31:17:05<7:15:48, 18.60s/it]

 94%|█████████▍| 22751/24156 [31:17:20<6:54:46, 17.71s/it]
{'loss': 0.3053, 'learning_rate': 1.6909768084530937e-06, 'rewards/chosen': -1.5752190351486206, 'rewards/rejected': -3.8437271118164062, 'rewards/accuracies': 0.75, 'rewards/margins': 2.268507957458496, 'policy_logps/rejected': -472.07861328125, 'policy_logps/chosen': -440.8006286621094, 'referece_logps/rejected': -433.641357421875, 'referece_logps/chosen': -425.0484619140625, 'logits/rejected': -0.5040249824523926, 'logits/chosen': -0.4391491413116455, 'epoch': 8.48}


 94%|█████████▍| 22753/24156 [31:17:44<5:43:22, 14.68s/it]
{'loss': 0.3187, 'learning_rate': 1.6907829391460865e-06, 'rewards/chosen': -2.080195426940918, 'rewards/rejected': -4.841175556182861, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7609798908233643, 'policy_logps/rejected': -243.95138549804688, 'policy_logps/chosen': -314.4263916015625, 'referece_logps/rejected': -195.53961181640625, 'referece_logps/chosen': -293.6244201660156, 'logits/rejected': -1.0164159536361694, 'logits/chosen': -0.8451451659202576, 'epoch': 8.48}


 94%|█████████▍| 22755/24156 [31:18:13<5:43:09, 14.70s/it]
{'loss': 0.3672, 'learning_rate': 1.6905890201661594e-06, 'rewards/chosen': -1.843334436416626, 'rewards/rejected': -4.1526288986206055, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3092942237854004, 'policy_logps/rejected': -666.9725341796875, 'policy_logps/chosen': -616.99755859375, 'referece_logps/rejected': -625.4462280273438, 'referece_logps/chosen': -598.5642700195312, 'logits/rejected': -0.8057864904403687, 'logits/chosen': -0.849878191947937, 'epoch': 8.48}

 94%|█████████▍| 22756/24156 [31:18:27<5:33:28, 14.29s/it]


 94%|█████████▍| 22758/24156 [31:18:56<5:31:14, 14.22s/it]

 94%|█████████▍| 22759/24156 [31:19:17<6:18:23, 16.25s/it]
{'loss': 0.3539, 'learning_rate': 1.6902010332433273e-06, 'rewards/chosen': -2.2742414474487305, 'rewards/rejected': -4.494329929351807, 'rewards/accuracies': 0.875, 'rewards/margins': 2.220088481903076, 'policy_logps/rejected': -497.9408874511719, 'policy_logps/chosen': -495.41278076171875, 'referece_logps/rejected': -452.9976501464844, 'referece_logps/chosen': -472.67041015625, 'logits/rejected': 0.24755710363388062, 'logits/chosen': 0.2543638050556183, 'epoch': 8.48}


 94%|█████████▍| 22761/24156 [31:19:46<5:59:21, 15.46s/it]

 94%|█████████▍| 22762/24156 [31:20:04<6:18:10, 16.28s/it]

 94%|█████████▍| 22763/24156 [31:20:16<5:48:09, 15.00s/it]

 94%|█████████▍| 22764/24156 [31:20:38<6:38:36, 17.18s/it]

 94%|█████████▍| 22765/24156 [31:20:54<6:24:48, 16.60s/it]

 94%|█████████▍| 22766/24156 [31:21:15<6:56:35, 17.98s/it]

 94%|█████████▍| 22767/24156 [31:21:30<6:39:16, 17.25s/it]

 94%|█████████▍| 22768/24156 [31:21:42<6:03:51, 15.73s/it]

 94%|█████████▍| 22769/24156 [31:22:01<6:20:10, 16.45s/it]

 94%|█████████▍| 22770/24156 [31:22:12<5:42:27, 14.83s/it]

 94%|█████████▍| 22771/24156 [31:22:24<5:23:17, 14.01s/it]

 94%|█████████▍| 22772/24156 [31:22:44<6:03:06, 15.74s/it]

 94%|█████████▍| 22773/24156 [31:22:58<5:52:08, 15.28s/it]

 94%|█████████▍| 22774/24156 [31:23:10<5:31:02, 14.37s/it]

 94%|█████████▍| 22775/24156 [31:23:23<5:19:27, 13.88s/it]

 94%|█████████▍| 22776/24156 [31:23:41<5:48:04, 15.13s/it]

 94%|█████████▍| 22777/24156 [31:23:56<5:45:30, 15.03s/it]

 94%|█████████▍| 22778/24156 [31:24:08<5:28:51, 14.32s/it]

 94%|█████████▍| 22779/24156 [31:24:28<6:03:22, 15.83s/it]
{'loss': 0.3168, 'learning_rate': 1.6882581229986083e-06, 'rewards/chosen': -2.1781764030456543, 'rewards/rejected': -3.8948628902435303, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7166863679885864, 'policy_logps/rejected': -268.0571594238281, 'policy_logps/chosen': -474.5360412597656, 'referece_logps/rejected': -229.1085205078125, 'referece_logps/chosen': -452.7543029785156, 'logits/rejected': -0.5178343057632446, 'logits/chosen': -0.7282108068466187, 'epoch': 8.49}


 94%|█████████▍| 22781/24156 [31:24:53<5:28:45, 14.35s/it]

 94%|█████████▍| 22782/24156 [31:25:06<5:20:18, 13.99s/it]

 94%|█████████▍| 22783/24156 [31:25:18<5:03:42, 13.27s/it]

 94%|█████████▍| 22784/24156 [31:25:30<4:58:42, 13.06s/it]

 94%|█████████▍| 22785/24156 [31:25:46<5:16:09, 13.84s/it]

 94%|█████████▍| 22786/24156 [31:26:00<5:15:34, 13.82s/it]
{'loss': 0.4604, 'learning_rate': 1.6875769344639362e-06, 'rewards/chosen': -2.230281114578247, 'rewards/rejected': -3.441030502319336, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2107490301132202, 'policy_logps/rejected': -305.84600830078125, 'policy_logps/chosen': -268.44024658203125, 'referece_logps/rejected': -271.4356994628906, 'referece_logps/chosen': -246.13743591308594, 'logits/rejected': -0.7810549736022949, 'logits/chosen': -0.7075275778770447, 'epoch': 8.49}


 94%|█████████▍| 22788/24156 [31:26:34<5:54:32, 15.55s/it]

 94%|█████████▍| 22789/24156 [31:26:48<5:41:25, 14.99s/it]

 94%|█████████▍| 22790/24156 [31:27:06<6:01:46, 15.89s/it]

 94%|█████████▍| 22791/24156 [31:27:26<6:28:13, 17.06s/it]

 94%|█████████▍| 22792/24156 [31:27:44<6:36:21, 17.44s/it]

 94%|█████████▍| 22793/24156 [31:27:55<5:51:47, 15.49s/it]

 94%|█████████▍| 22794/24156 [31:28:14<6:15:11, 16.53s/it]

 94%|█████████▍| 22795/24156 [31:28:33<6:28:37, 17.13s/it]

 94%|█████████▍| 22796/24156 [31:28:49<6:24:41, 16.97s/it]

 94%|█████████▍| 22797/24156 [31:29:07<6:31:57, 17.31s/it]

 94%|█████████▍| 22798/24156 [31:29:27<6:48:49, 18.06s/it]
{'loss': 0.3326, 'learning_rate': 1.6864077739150802e-06, 'rewards/chosen': -1.8737713098526, 'rewards/rejected': -5.050642490386963, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1768717765808105, 'policy_logps/rejected': -429.04815673828125, 'policy_logps/chosen': -420.3420104980469, 'referece_logps/rejected': -378.541748046875, 'referece_logps/chosen': -401.60430908203125, 'logits/rejected': -0.662366509437561, 'logits/chosen': -0.6262904405593872, 'epoch': 8.49}


 94%|█████████▍| 22800/24156 [31:30:01<6:38:22, 17.63s/it]

 94%|█████████▍| 22801/24156 [31:30:16<6:20:50, 16.86s/it]

 94%|█████████▍| 22802/24156 [31:30:27<5:39:56, 15.06s/it]

 94%|█████████▍| 22803/24156 [31:30:49<6:27:32, 17.19s/it]

 94%|█████████▍| 22804/24156 [31:31:02<6:02:42, 16.10s/it]
{'loss': 0.3578, 'learning_rate': 1.6858225271142676e-06, 'rewards/chosen': -2.364346504211426, 'rewards/rejected': -3.793092727661133, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4287463426589966, 'policy_logps/rejected': -305.76611328125, 'policy_logps/chosen': -458.9817199707031, 'referece_logps/rejected': -267.835205078125, 'referece_logps/chosen': -435.3382568359375, 'logits/rejected': -0.20100940763950348, 'logits/chosen': -0.33035996556282043, 'epoch': 8.5}


 94%|█████████▍| 22806/24156 [31:31:33<5:51:19, 15.61s/it]

 94%|█████████▍| 22807/24156 [31:31:53<6:18:47, 16.85s/it]

 94%|█████████▍| 22808/24156 [31:32:05<5:48:31, 15.51s/it]

 94%|█████████▍| 22809/24156 [31:32:26<6:24:41, 17.14s/it]
{'loss': 0.2335, 'learning_rate': 1.6853344823818055e-06, 'rewards/chosen': -3.095600128173828, 'rewards/rejected': -6.581817150115967, 'rewards/accuracies': 0.75, 'rewards/margins': 3.486217498779297, 'policy_logps/rejected': -333.3956298828125, 'policy_logps/chosen': -466.937744140625, 'referece_logps/rejected': -267.57745361328125, 'referece_logps/chosen': -435.981689453125, 'logits/rejected': -0.14018964767456055, 'logits/chosen': -0.35332873463630676, 'epoch': 8.5}


 94%|█████████▍| 22811/24156 [31:33:07<7:00:12, 18.75s/it]

 94%|█████████▍| 22812/24156 [31:33:26<7:04:36, 18.96s/it]

 94%|█████████▍| 22813/24156 [31:33:41<6:35:55, 17.69s/it]

 94%|█████████▍| 22814/24156 [31:34:03<7:03:03, 18.92s/it]

 94%|█████████▍| 22815/24156 [31:34:22<7:07:12, 19.11s/it]

 94%|█████████▍| 22816/24156 [31:34:44<7:21:28, 19.77s/it]

 94%|█████████▍| 22817/24156 [31:34:57<6:39:32, 17.90s/it]
{'loss': 0.3722, 'learning_rate': 1.6845529702464752e-06, 'rewards/chosen': -2.563410997390747, 'rewards/rejected': -3.3963873386383057, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8329764008522034, 'policy_logps/rejected': -465.72625732421875, 'policy_logps/chosen': -473.0843505859375, 'referece_logps/rejected': -431.76239013671875, 'referece_logps/chosen': -447.45025634765625, 'logits/rejected': -0.5195164680480957, 'logits/chosen': -0.40500617027282715, 'epoch': 8.5}


 94%|█████████▍| 22819/24156 [31:35:25<5:52:11, 15.81s/it]
{'loss': 0.4338, 'learning_rate': 1.6843574691151746e-06, 'rewards/chosen': -2.4019882678985596, 'rewards/rejected': -3.533681869506836, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1316933631896973, 'policy_logps/rejected': -366.5794372558594, 'policy_logps/chosen': -343.9759826660156, 'referece_logps/rejected': -331.24261474609375, 'referece_logps/chosen': -319.95611572265625, 'logits/rejected': -0.33751216530799866, 'logits/chosen': -0.2871187627315521, 'epoch': 8.5}


 94%|█████████▍| 22821/24156 [31:35:47<4:58:26, 13.41s/it]

 94%|█████████▍| 22822/24156 [31:35:59<4:45:35, 12.85s/it]

 94%|█████████▍| 22823/24156 [31:36:10<4:31:07, 12.20s/it]
{'loss': 0.4217, 'learning_rate': 1.683966319234007e-06, 'rewards/chosen': -2.1205127239227295, 'rewards/rejected': -3.2876486778259277, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1671359539031982, 'policy_logps/rejected': -467.78302001953125, 'policy_logps/chosen': -372.3675231933594, 'referece_logps/rejected': -434.90655517578125, 'referece_logps/chosen': -351.16241455078125, 'logits/rejected': -0.4762701392173767, 'logits/chosen': -0.12925758957862854, 'epoch': 8.5}

 94%|█████████▍| 22824/24156 [31:36:26<4:59:09, 13.48s/it]


 94%|█████████▍| 22826/24156 [31:37:00<5:41:17, 15.40s/it]

 94%|█████████▍| 22827/24156 [31:37:21<6:21:48, 17.24s/it]

 95%|█████████▍| 22828/24156 [31:37:42<6:43:27, 18.23s/it]
{'loss': 0.4546, 'learning_rate': 1.6834771052427771e-06, 'rewards/chosen': -2.365267038345337, 'rewards/rejected': -3.7410449981689453, 'rewards/accuracies': 0.625, 'rewards/margins': 1.3757779598236084, 'policy_logps/rejected': -393.5194396972656, 'policy_logps/chosen': -371.93060302734375, 'referece_logps/rejected': -356.1089782714844, 'referece_logps/chosen': -348.2779235839844, 'logits/rejected': -1.0470045804977417, 'logits/chosen': -1.0964486598968506, 'epoch': 8.51}


 95%|█████████▍| 22830/24156 [31:38:09<5:53:54, 16.01s/it]
{'loss': 0.3906, 'learning_rate': 1.683281333625722e-06, 'rewards/chosen': -2.481937885284424, 'rewards/rejected': -4.346972465515137, 'rewards/accuracies': 0.75, 'rewards/margins': 1.865034580230713, 'policy_logps/rejected': -344.81805419921875, 'policy_logps/chosen': -399.6300354003906, 'referece_logps/rejected': -301.3483581542969, 'referece_logps/chosen': -374.81060791015625, 'logits/rejected': -0.8756093382835388, 'logits/chosen': -0.9799314737319946, 'epoch': 8.51}

 95%|█████████▍| 22831/24156 [31:38:28<6:16:36, 17.05s/it]

 95%|█████████▍| 22832/24156 [31:38:49<6:38:03, 18.04s/it]


 95%|█████████▍| 22834/24156 [31:39:30<7:04:17, 19.26s/it]

 95%|█████████▍| 22835/24156 [31:39:50<7:06:37, 19.38s/it]
{'loss': 0.2319, 'learning_rate': 1.6827916896548541e-06, 'rewards/chosen': -1.8087348937988281, 'rewards/rejected': -3.938628911972046, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1298940181732178, 'policy_logps/rejected': -467.5913391113281, 'policy_logps/chosen': -432.73516845703125, 'referece_logps/rejected': -428.205078125, 'referece_logps/chosen': -414.64788818359375, 'logits/rejected': -0.49132898449897766, 'logits/chosen': -0.5725844502449036, 'epoch': 8.51}

 95%|█████████▍| 22836/24156 [31:40:07<6:53:18, 18.79s/it]


 95%|█████████▍| 22838/24156 [31:40:48<7:12:26, 19.69s/it]

 95%|█████████▍| 22839/24156 [31:41:00<6:21:34, 17.38s/it]

 95%|█████████▍| 22840/24156 [31:41:15<6:08:18, 16.79s/it]

 95%|█████████▍| 22841/24156 [31:41:37<6:41:50, 18.33s/it]
{'loss': 0.3344, 'learning_rate': 1.6822037118483583e-06, 'rewards/chosen': -1.9145606756210327, 'rewards/rejected': -4.244460105895996, 'rewards/accuracies': 1.0, 'rewards/margins': 2.329899311065674, 'policy_logps/rejected': -403.3924560546875, 'policy_logps/chosen': -401.11865234375, 'referece_logps/rejected': -360.94781494140625, 'referece_logps/chosen': -381.9729919433594, 'logits/rejected': -0.909697413444519, 'logits/chosen': -0.8442357778549194, 'epoch': 8.51}


 95%|█████████▍| 22843/24156 [31:42:06<5:51:05, 16.04s/it]

 95%|█████████▍| 22844/24156 [31:42:25<6:14:07, 17.11s/it]

 95%|█████████▍| 22845/24156 [31:42:38<5:44:56, 15.79s/it]

 95%|█████████▍| 22846/24156 [31:42:51<5:27:21, 14.99s/it]
{'loss': 0.304, 'learning_rate': 1.6817133930667798e-06, 'rewards/chosen': -2.9374818801879883, 'rewards/rejected': -4.52100944519043, 'rewards/accuracies': 0.875, 'rewards/margins': 1.583527684211731, 'policy_logps/rejected': -477.7073974609375, 'policy_logps/chosen': -396.3520812988281, 'referece_logps/rejected': -432.49725341796875, 'referece_logps/chosen': -366.97723388671875, 'logits/rejected': 0.22952575981616974, 'logits/chosen': 0.2863485515117645, 'epoch': 8.51}

 95%|█████████▍| 22847/24156 [31:43:09<5:43:24, 15.74s/it]

 95%|█████████▍| 22848/24156 [31:43:30<6:21:10, 17.48s/it]


 95%|█████████▍| 22850/24156 [31:44:08<6:33:32, 18.08s/it]

 95%|█████████▍| 22851/24156 [31:44:27<6:40:50, 18.43s/it]
{'loss': 0.3142, 'learning_rate': 1.681222767905551e-06, 'rewards/chosen': -2.3902480602264404, 'rewards/rejected': -3.589385986328125, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1991379261016846, 'policy_logps/rejected': -360.0272216796875, 'policy_logps/chosen': -378.9099426269531, 'referece_logps/rejected': -324.13336181640625, 'referece_logps/chosen': -355.0074462890625, 'logits/rejected': -0.27387160062789917, 'logits/chosen': -0.21827031672000885, 'epoch': 8.51}

 95%|█████████▍| 22852/24156 [31:44:47<6:48:13, 18.78s/it]


 95%|█████████▍| 22854/24156 [31:45:26<6:55:41, 19.16s/it]
{'loss': 0.2601, 'learning_rate': 1.6809282458383038e-06, 'rewards/chosen': -2.612179756164551, 'rewards/rejected': -4.537915229797363, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9257358312606812, 'policy_logps/rejected': -422.34613037109375, 'policy_logps/chosen': -354.3436279296875, 'referece_logps/rejected': -376.9670104980469, 'referece_logps/chosen': -328.22186279296875, 'logits/rejected': -0.11589605361223221, 'logits/chosen': -0.04702256619930267, 'epoch': 8.51}


 95%|█████████▍| 22856/24156 [31:46:02<6:49:04, 18.88s/it]

 95%|█████████▍| 22857/24156 [31:46:19<6:39:01, 18.43s/it]
{'loss': 0.2022, 'learning_rate': 1.6806336136014117e-06, 'rewards/chosen': -2.088495969772339, 'rewards/rejected': -4.215353488922119, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1268575191497803, 'policy_logps/rejected': -393.49981689453125, 'policy_logps/chosen': -405.66339111328125, 'referece_logps/rejected': -351.34625244140625, 'referece_logps/chosen': -384.7784423828125, 'logits/rejected': 0.6714288592338562, 'logits/chosen': 0.6951223611831665, 'epoch': 8.52}


 95%|█████████▍| 22859/24156 [31:46:56<6:35:16, 18.29s/it]
{'loss': 0.3137, 'learning_rate': 1.6804371309283312e-06, 'rewards/chosen': -2.1673054695129395, 'rewards/rejected': -3.3096418380737305, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1423366069793701, 'policy_logps/rejected': -236.2047576904297, 'policy_logps/chosen': -335.1241455078125, 'referece_logps/rejected': -203.1083526611328, 'referece_logps/chosen': -313.45111083984375, 'logits/rejected': -0.5977241396903992, 'logits/chosen': -0.5951765179634094, 'epoch': 8.52}


 95%|█████████▍| 22861/24156 [31:47:20<5:23:42, 15.00s/it]
{'loss': 0.2261, 'learning_rate': 1.6802405993262792e-06, 'rewards/chosen': -2.6527090072631836, 'rewards/rejected': -6.357865810394287, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7051568031311035, 'policy_logps/rejected': -391.767578125, 'policy_logps/chosen': -359.7046203613281, 'referece_logps/rejected': -328.1889343261719, 'referece_logps/chosen': -333.17755126953125, 'logits/rejected': -0.1655842363834381, 'logits/chosen': -0.022343508899211884, 'epoch': 8.52}

 95%|█████████▍| 22862/24156 [31:47:33<5:16:24, 14.67s/it]


 95%|█████████▍| 22864/24156 [31:48:12<6:04:45, 16.94s/it]

 95%|█████████▍| 22865/24156 [31:48:24<5:33:09, 15.48s/it]
{'loss': 0.3796, 'learning_rate': 1.6798473893917913e-06, 'rewards/chosen': -2.185502052307129, 'rewards/rejected': -4.330068111419678, 'rewards/accuracies': 0.75, 'rewards/margins': 2.144566297531128, 'policy_logps/rejected': -467.9508056640625, 'policy_logps/chosen': -406.60418701171875, 'referece_logps/rejected': -424.650146484375, 'referece_logps/chosen': -384.7491760253906, 'logits/rejected': -0.29711899161338806, 'logits/chosen': -0.25802546739578247, 'epoch': 8.52}


 95%|█████████▍| 22867/24156 [31:49:04<6:27:08, 18.02s/it]
{'loss': 0.32, 'learning_rate': 1.6796507110876309e-06, 'rewards/chosen': -1.644540786743164, 'rewards/rejected': -3.8253068923950195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1807661056518555, 'policy_logps/rejected': -395.49493408203125, 'policy_logps/chosen': -407.1248779296875, 'referece_logps/rejected': -357.24188232421875, 'referece_logps/chosen': -390.67950439453125, 'logits/rejected': -0.7202668190002441, 'logits/chosen': -0.6774076223373413, 'epoch': 8.52}


 95%|█████████▍| 22869/24156 [31:49:46<6:56:01, 19.40s/it]
{'loss': 0.3263, 'learning_rate': 1.6794539839110481e-06, 'rewards/chosen': -1.8515312671661377, 'rewards/rejected': -5.216881275177002, 'rewards/accuracies': 0.875, 'rewards/margins': 3.365349769592285, 'policy_logps/rejected': -334.0379638671875, 'policy_logps/chosen': -381.5608825683594, 'referece_logps/rejected': -281.8691711425781, 'referece_logps/chosen': -363.04559326171875, 'logits/rejected': -0.7535248398780823, 'logits/chosen': -0.5519238710403442, 'epoch': 8.52}


 95%|█████████▍| 22871/24156 [31:50:18<6:16:36, 17.58s/it]

 95%|█████████▍| 22872/24156 [31:50:37<6:20:08, 17.76s/it]

 95%|█████████▍| 22873/24156 [31:50:52<6:07:58, 17.21s/it]
{'loss': 0.297, 'learning_rate': 1.6790603829972062e-06, 'rewards/chosen': -3.346175193786621, 'rewards/rejected': -5.819207191467285, 'rewards/accuracies': 1.0, 'rewards/margins': 2.473031520843506, 'policy_logps/rejected': -559.930908203125, 'policy_logps/chosen': -686.6704711914062, 'referece_logps/rejected': -501.7388916015625, 'referece_logps/chosen': -653.2088012695312, 'logits/rejected': 0.45679569244384766, 'logits/chosen': 0.3504493534564972, 'epoch': 8.52}


 95%|█████████▍| 22875/24156 [31:51:24<5:56:16, 16.69s/it]
{'loss': 0.2076, 'learning_rate': 1.6788635092882496e-06, 'rewards/chosen': -2.576854944229126, 'rewards/rejected': -6.75142765045166, 'rewards/accuracies': 0.875, 'rewards/margins': 4.174572467803955, 'policy_logps/rejected': -427.4931640625, 'policy_logps/chosen': -426.642333984375, 'referece_logps/rejected': -359.9788818359375, 'referece_logps/chosen': -400.8738098144531, 'logits/rejected': 0.23309215903282166, 'logits/chosen': 0.19146181643009186, 'epoch': 8.52}

 95%|█████████▍| 22876/24156 [31:51:39<5:43:12, 16.09s/it]


 95%|█████████▍| 22878/24156 [31:52:13<5:51:30, 16.50s/it]

 95%|█████████▍| 22879/24156 [31:52:30<5:57:04, 16.78s/it]

 95%|█████████▍| 22880/24156 [31:52:47<5:55:12, 16.70s/it]

 95%|█████████▍| 22881/24156 [31:53:06<6:13:36, 17.58s/it]

 95%|█████████▍| 22882/24156 [31:53:24<6:15:28, 17.68s/it]
{'loss': 0.2261, 'learning_rate': 1.6781740669752886e-06, 'rewards/chosen': -1.7217962741851807, 'rewards/rejected': -5.5965704917907715, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8747739791870117, 'policy_logps/rejected': -305.5507507324219, 'policy_logps/chosen': -340.8812255859375, 'referece_logps/rejected': -249.58505249023438, 'referece_logps/chosen': -323.66326904296875, 'logits/rejected': -0.48475080728530884, 'logits/chosen': -0.5477115511894226, 'epoch': 8.53}

 95%|█████████▍| 22883/24156 [31:53:40<6:02:27, 17.08s/it]

 95%|█████████▍| 22884/24156 [31:53:56<5:56:08, 16.80s/it]

 95%|█████████▍| 22885/24156 [31:54:17<6:25:54, 18.22s/it]


 95%|█████████▍| 22887/24156 [31:54:46<5:46:29, 16.38s/it]
{'loss': 0.2865, 'learning_rate': 1.6776812423981146e-06, 'rewards/chosen': -1.8502166271209717, 'rewards/rejected': -3.4659528732299805, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6157360076904297, 'policy_logps/rejected': -341.2730407714844, 'policy_logps/chosen': -444.760009765625, 'referece_logps/rejected': -306.6135559082031, 'referece_logps/chosen': -426.25787353515625, 'logits/rejected': -1.163771629333496, 'logits/chosen': -1.107072353363037, 'epoch': 8.53}

 95%|█████████▍| 22888/24156 [31:55:04<5:51:43, 16.64s/it]


 95%|█████████▍| 22890/24156 [31:55:38<5:57:13, 16.93s/it]

 95%|█████████▍| 22891/24156 [31:55:52<5:38:49, 16.07s/it]
{'loss': 0.2772, 'learning_rate': 1.677286763437141e-06, 'rewards/chosen': -2.5012001991271973, 'rewards/rejected': -4.080920696258545, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5797207355499268, 'policy_logps/rejected': -333.23419189453125, 'policy_logps/chosen': -414.41259765625, 'referece_logps/rejected': -292.42498779296875, 'referece_logps/chosen': -389.4006042480469, 'logits/rejected': -0.45161429047584534, 'logits/chosen': -0.6188894510269165, 'epoch': 8.53}


 95%|█████████▍| 22893/24156 [31:56:35<6:33:41, 18.70s/it]
{'loss': 0.3575, 'learning_rate': 1.6770894508959096e-06, 'rewards/chosen': -2.477553129196167, 'rewards/rejected': -3.5742738246917725, 'rewards/accuracies': 0.75, 'rewards/margins': 1.096720576286316, 'policy_logps/rejected': -358.6753845214844, 'policy_logps/chosen': -384.67315673828125, 'referece_logps/rejected': -322.9326171875, 'referece_logps/chosen': -359.89764404296875, 'logits/rejected': -0.026792440563440323, 'logits/chosen': -0.1753513067960739, 'epoch': 8.53}


 95%|█████████▍| 22895/24156 [31:57:00<5:31:45, 15.79s/it]
{'loss': 0.3869, 'learning_rate': 1.6768920896664315e-06, 'rewards/chosen': -2.3264920711517334, 'rewards/rejected': -5.181992530822754, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8555006980895996, 'policy_logps/rejected': -384.8285217285156, 'policy_logps/chosen': -376.0768127441406, 'referece_logps/rejected': -333.0085754394531, 'referece_logps/chosen': -352.8118896484375, 'logits/rejected': 0.09689967334270477, 'logits/chosen': 0.1809336543083191, 'epoch': 8.53}


 95%|█████████▍| 22897/24156 [31:57:23<4:43:35, 13.51s/it]
{'loss': 0.3234, 'learning_rate': 1.6766946797628991e-06, 'rewards/chosen': -1.7610069513320923, 'rewards/rejected': -3.459292411804199, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6982853412628174, 'policy_logps/rejected': -373.464599609375, 'policy_logps/chosen': -358.75823974609375, 'referece_logps/rejected': -338.87164306640625, 'referece_logps/chosen': -341.148193359375, 'logits/rejected': -0.8473063707351685, 'logits/chosen': -0.7188805341720581, 'epoch': 8.53}

 95%|█████████▍| 22898/24156 [31:57:44<5:30:27, 15.76s/it]


 95%|█████████▍| 22900/24156 [31:58:11<5:09:38, 14.79s/it]

 95%|█████████▍| 22901/24156 [31:58:31<5:41:06, 16.31s/it]

 95%|█████████▍| 22902/24156 [31:58:49<5:53:20, 16.91s/it]

 95%|█████████▍| 22903/24156 [31:59:09<6:11:55, 17.81s/it]

 95%|█████████▍| 22904/24156 [31:59:19<5:23:12, 15.49s/it]

 95%|█████████▍| 22905/24156 [31:59:31<5:01:57, 14.48s/it]

 95%|█████████▍| 22906/24156 [31:59:47<5:11:19, 14.94s/it]
{'loss': 0.2686, 'learning_rate': 1.675805733236273e-06, 'rewards/chosen': -2.6372969150543213, 'rewards/rejected': -4.422313690185547, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7850167751312256, 'policy_logps/rejected': -346.3820495605469, 'policy_logps/chosen': -296.1797180175781, 'referece_logps/rejected': -302.15887451171875, 'referece_logps/chosen': -269.8067626953125, 'logits/rejected': 0.5721958875656128, 'logits/chosen': 0.6445202231407166, 'epoch': 8.53}

 95%|█████████▍| 22907/24156 [32:00:06<5:36:43, 16.18s/it]


 95%|█████████▍| 22909/24156 [32:00:49<6:30:43, 18.80s/it]
{'loss': 0.3423, 'learning_rate': 1.6755091989817438e-06, 'rewards/chosen': -2.263890266418457, 'rewards/rejected': -4.067869186401367, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8039788007736206, 'policy_logps/rejected': -428.4270935058594, 'policy_logps/chosen': -326.49249267578125, 'referece_logps/rejected': -387.74847412109375, 'referece_logps/chosen': -303.8536071777344, 'logits/rejected': -0.5006817579269409, 'logits/chosen': -0.4260786473751068, 'epoch': 8.54}


 95%|█████████▍| 22911/24156 [32:01:23<6:04:50, 17.58s/it]

 95%|█████████▍| 22912/24156 [32:01:35<5:32:38, 16.04s/it]

 95%|█████████▍| 22913/24156 [32:01:57<6:09:58, 17.86s/it]

 95%|█████████▍| 22914/24156 [32:02:13<5:57:49, 17.29s/it]
{'loss': 0.2469, 'learning_rate': 1.67501473237483e-06, 'rewards/chosen': -1.9514963626861572, 'rewards/rejected': -4.357599258422852, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4061028957366943, 'policy_logps/rejected': -453.35528564453125, 'policy_logps/chosen': -490.361083984375, 'referece_logps/rejected': -409.779296875, 'referece_logps/chosen': -470.84613037109375, 'logits/rejected': 0.1124575212597847, 'logits/chosen': 0.17557601630687714, 'epoch': 8.54}

 95%|█████████▍| 22915/24156 [32:02:32<6:05:32, 17.67s/it]


 95%|█████████▍| 22917/24156 [32:03:09<6:18:43, 18.34s/it]
{'loss': 0.2941, 'learning_rate': 1.674717906779286e-06, 'rewards/chosen': -2.2312238216400146, 'rewards/rejected': -3.968191623687744, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7369673252105713, 'policy_logps/rejected': -505.7085266113281, 'policy_logps/chosen': -383.5716552734375, 'referece_logps/rejected': -466.0265808105469, 'referece_logps/chosen': -361.2594299316406, 'logits/rejected': -0.2628156244754791, 'logits/chosen': -0.24771016836166382, 'epoch': 8.54}

 95%|█████████▍| 22918/24156 [32:03:30<6:36:57, 19.24s/it]

 95%|█████████▍| 22919/24156 [32:03:48<6:27:56, 18.82s/it]


 95%|█████████▍| 22921/24156 [32:04:25<6:23:02, 18.61s/it]
{'loss': 0.2132, 'learning_rate': 1.6743219695148692e-06, 'rewards/chosen': -1.6550323963165283, 'rewards/rejected': -5.248437404632568, 'rewards/accuracies': 0.875, 'rewards/margins': 3.593404769897461, 'policy_logps/rejected': -403.53668212890625, 'policy_logps/chosen': -329.23321533203125, 'referece_logps/rejected': -351.05230712890625, 'referece_logps/chosen': -312.68292236328125, 'logits/rejected': -0.5581396818161011, 'logits/chosen': -0.5757803320884705, 'epoch': 8.54}

 95%|█████████▍| 22922/24156 [32:04:43<6:15:04, 18.24s/it]

 95%|█████████▍| 22923/24156 [32:05:02<6:23:08, 18.64s/it]

 95%|█████████▍| 22924/24156 [32:05:22<6:32:41, 19.12s/it]


 95%|█████████▍| 22926/24156 [32:06:00<6:27:15, 18.89s/it]
{'loss': 0.287, 'learning_rate': 1.673826775195718e-06, 'rewards/chosen': -3.14176082611084, 'rewards/rejected': -5.144260406494141, 'rewards/accuracies': 0.875, 'rewards/margins': 2.002499580383301, 'policy_logps/rejected': -487.0655212402344, 'policy_logps/chosen': -619.1148071289062, 'referece_logps/rejected': -435.6229248046875, 'referece_logps/chosen': -587.6972045898438, 'logits/rejected': -0.1871718466281891, 'logits/chosen': -0.2266700118780136, 'epoch': 8.54}

 95%|█████████▍| 22927/24156 [32:06:18<6:22:52, 18.69s/it]

 95%|█████████▍| 22928/24156 [32:06:35<6:13:15, 18.24s/it]

 95%|█████████▍| 22929/24156 [32:06:56<6:31:28, 19.14s/it]


 95%|█████████▍| 22931/24156 [32:07:34<6:29:39, 19.08s/it]
{'loss': 0.2598, 'learning_rate': 1.6733312780413669e-06, 'rewards/chosen': -2.113842725753784, 'rewards/rejected': -3.8503475189208984, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7365046739578247, 'policy_logps/rejected': -622.6566162109375, 'policy_logps/chosen': -414.75848388671875, 'referece_logps/rejected': -584.1531982421875, 'referece_logps/chosen': -393.62005615234375, 'logits/rejected': -0.5758861899375916, 'logits/chosen': -0.3485361933708191, 'epoch': 8.54}

 95%|█████████▍| 22932/24156 [32:07:55<6:41:03, 19.66s/it]

 95%|█████████▍| 22933/24156 [32:08:06<5:52:57, 17.32s/it]

 95%|█████████▍| 22934/24156 [32:08:21<5:32:57, 16.35s/it]

 95%|█████████▍| 22935/24156 [32:08:36<5:29:25, 16.19s/it]

 95%|█████████▍| 22936/24156 [32:08:54<5:38:37, 16.65s/it]

 95%|█████████▍| 22937/24156 [32:09:09<5:26:03, 16.05s/it]


 95%|█████████▍| 22939/24156 [32:09:38<5:02:42, 14.92s/it]
{'loss': 0.3946, 'learning_rate': 1.6725378532530723e-06, 'rewards/chosen': -2.6764566898345947, 'rewards/rejected': -4.9441375732421875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.267681360244751, 'policy_logps/rejected': -370.0608825683594, 'policy_logps/chosen': -431.5691833496094, 'referece_logps/rejected': -320.61944580078125, 'referece_logps/chosen': -404.8045959472656, 'logits/rejected': 0.26972395181655884, 'logits/chosen': 0.1603788137435913, 'epoch': 8.55}

 95%|█████████▍| 22940/24156 [32:09:51<4:53:49, 14.50s/it]

 95%|█████████▍| 22941/24156 [32:10:02<4:34:31, 13.56s/it]

 95%|█████████▍| 22942/24156 [32:10:14<4:22:43, 12.98s/it]

 95%|█████████▍| 22943/24156 [32:10:27<4:25:00, 13.11s/it]

 95%|█████████▍| 22944/24156 [32:10:47<5:04:58, 15.10s/it]

 95%|█████████▍| 22945/24156 [32:11:00<4:51:02, 14.42s/it]

 95%|█████████▍| 22946/24156 [32:11:12<4:37:31, 13.76s/it]

 95%|█████████▍| 22947/24156 [32:11:23<4:21:54, 13.00s/it]

 95%|█████████▍| 22948/24156 [32:11:34<4:07:08, 12.28s/it]

 95%|█████████▌| 22949/24156 [32:11:46<4:05:21, 12.20s/it]

 95%|█████████▌| 22950/24156 [32:11:57<3:57:58, 11.84s/it]

 95%|█████████▌| 22951/24156 [32:12:11<4:09:58, 12.45s/it]

 95%|█████████▌| 22952/24156 [32:12:25<4:17:08, 12.81s/it]

 95%|█████████▌| 22953/24156 [32:12:36<4:05:25, 12.24s/it]

 95%|█████████▌| 22954/24156 [32:12:53<4:34:55, 13.72s/it]

 95%|█████████▌| 22955/24156 [32:13:13<5:14:14, 15.70s/it]

 95%|█████████▌| 22956/24156 [32:13:27<5:06:37, 15.33s/it]

 95%|█████████▌| 22957/24156 [32:13:43<5:08:35, 15.44s/it]

 95%|█████████▌| 22958/24156 [32:13:54<4:41:43, 14.11s/it]


 95%|█████████▌| 22960/24156 [32:14:16<4:08:57, 12.49s/it]
{'loss': 0.3963, 'learning_rate': 1.6704514340620728e-06, 'rewards/chosen': -2.5103189945220947, 'rewards/rejected': -3.572223663330078, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0619046688079834, 'policy_logps/rejected': -430.395263671875, 'policy_logps/chosen': -370.7566833496094, 'referece_logps/rejected': -394.67303466796875, 'referece_logps/chosen': -345.65350341796875, 'logits/rejected': -0.14085587859153748, 'logits/chosen': -0.2566032409667969, 'epoch': 8.55}

 95%|█████████▌| 22961/24156 [32:14:35<4:45:27, 14.33s/it]

 95%|█████████▌| 22962/24156 [32:14:49<4:43:20, 14.24s/it]


 95%|█████████▌| 22964/24156 [32:15:14<4:23:57, 13.29s/it]
{'loss': 0.3156, 'learning_rate': 1.67005341773998e-06, 'rewards/chosen': -1.7103813886642456, 'rewards/rejected': -4.180372714996338, 'rewards/accuracies': 0.875, 'rewards/margins': 2.469991683959961, 'policy_logps/rejected': -325.318359375, 'policy_logps/chosen': -335.64544677734375, 'referece_logps/rejected': -283.5146179199219, 'referece_logps/chosen': -318.5416564941406, 'logits/rejected': -0.6303428411483765, 'logits/chosen': -0.6473865509033203, 'epoch': 8.56}

 95%|█████████▌| 22965/24156 [32:15:35<5:07:14, 15.48s/it]

 95%|█████████▌| 22966/24156 [32:15:52<5:16:50, 15.97s/it]

 95%|█████████▌| 22967/24156 [32:16:05<5:00:45, 15.18s/it]

 95%|█████████▌| 22968/24156 [32:16:20<4:57:51, 15.04s/it]

 95%|█████████▌| 22969/24156 [32:16:38<5:12:59, 15.82s/it]

 95%|█████████▌| 22970/24156 [32:16:56<5:26:07, 16.50s/it]

 95%|█████████▌| 22971/24156 [32:17:08<4:59:43, 15.18s/it]


 95%|█████████▌| 22973/24156 [32:17:42<5:16:03, 16.03s/it]
{'loss': 0.2989, 'learning_rate': 1.669157176523667e-06, 'rewards/chosen': -2.5998647212982178, 'rewards/rejected': -5.558103084564209, 'rewards/accuracies': 0.875, 'rewards/margins': 2.958237886428833, 'policy_logps/rejected': -308.1567687988281, 'policy_logps/chosen': -268.25836181640625, 'referece_logps/rejected': -252.5757598876953, 'referece_logps/chosen': -242.25970458984375, 'logits/rejected': -0.8607036471366882, 'logits/chosen': -0.8547909259796143, 'epoch': 8.56}

 95%|█████████▌| 22974/24156 [32:17:55<4:54:53, 14.97s/it]

 95%|█████████▌| 22975/24156 [32:18:17<5:35:40, 17.05s/it]


 95%|█████████▌| 22977/24156 [32:18:56<6:04:45, 18.56s/it]
{'loss': 0.3032, 'learning_rate': 1.6687585342505699e-06, 'rewards/chosen': -2.171649932861328, 'rewards/rejected': -4.75598669052124, 'rewards/accuracies': 0.875, 'rewards/margins': 2.584336757659912, 'policy_logps/rejected': -411.44647216796875, 'policy_logps/chosen': -365.6327819824219, 'referece_logps/rejected': -363.8865966796875, 'referece_logps/chosen': -343.916259765625, 'logits/rejected': 0.08874140679836273, 'logits/chosen': 0.13812509179115295, 'epoch': 8.56}

 95%|█████████▌| 22978/24156 [32:19:17<6:15:44, 19.14s/it]

 95%|█████████▌| 22979/24156 [32:19:32<5:49:18, 17.81s/it]

 95%|█████████▌| 22980/24156 [32:19:48<5:40:16, 17.36s/it]

 95%|█████████▌| 22981/24156 [32:20:00<5:07:52, 15.72s/it]


 95%|█████████▌| 22983/24156 [32:20:31<5:09:48, 15.85s/it]
{'loss': 0.2368, 'learning_rate': 1.6681602102078857e-06, 'rewards/chosen': -2.489000082015991, 'rewards/rejected': -4.446820259094238, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9578197002410889, 'policy_logps/rejected': -409.6407470703125, 'policy_logps/chosen': -506.1204528808594, 'referece_logps/rejected': -365.1725158691406, 'referece_logps/chosen': -481.23040771484375, 'logits/rejected': -0.6454694867134094, 'logits/chosen': -0.6813002228736877, 'epoch': 8.56}

 95%|█████████▌| 22984/24156 [32:20:47<5:15:55, 16.17s/it]

 95%|█████████▌| 22985/24156 [32:21:10<5:50:41, 17.97s/it]

 95%|█████████▌| 22986/24156 [32:21:29<6:00:52, 18.51s/it]


 95%|█████████▌| 22988/24156 [32:22:05<5:47:28, 17.85s/it]
{'loss': 0.3925, 'learning_rate': 1.667661276505197e-06, 'rewards/chosen': -1.784578800201416, 'rewards/rejected': -3.9083709716796875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1237921714782715, 'policy_logps/rejected': -375.9307556152344, 'policy_logps/chosen': -317.6014709472656, 'referece_logps/rejected': -336.8470764160156, 'referece_logps/chosen': -299.7557067871094, 'logits/rejected': -0.7276508808135986, 'logits/chosen': -0.5670351386070251, 'epoch': 8.56}

 95%|█████████▌| 22989/24156 [32:22:23<5:50:59, 18.05s/it]

 95%|█████████▌| 22990/24156 [32:22:40<5:42:30, 17.62s/it]

 95%|█████████▌| 22991/24156 [32:23:00<5:55:54, 18.33s/it]


 95%|█████████▌| 22993/24156 [32:23:25<4:54:18, 15.18s/it]
{'loss': 0.3921, 'learning_rate': 1.667162042738243e-06, 'rewards/chosen': -2.753568172454834, 'rewards/rejected': -5.281793117523193, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5282251834869385, 'policy_logps/rejected': -415.13885498046875, 'policy_logps/chosen': -599.16015625, 'referece_logps/rejected': -362.3209533691406, 'referece_logps/chosen': -571.62451171875, 'logits/rejected': -0.7221110463142395, 'logits/chosen': -0.9868994951248169, 'epoch': 8.57}


 95%|█████████▌| 22995/24156 [32:24:04<5:41:21, 17.64s/it]
{'loss': 0.3689, 'learning_rate': 1.6669622652637215e-06, 'rewards/chosen': -2.349801778793335, 'rewards/rejected': -4.000968933105469, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6511669158935547, 'policy_logps/rejected': -357.69390869140625, 'policy_logps/chosen': -372.1440734863281, 'referece_logps/rejected': -317.6842041015625, 'referece_logps/chosen': -348.64605712890625, 'logits/rejected': -0.31922194361686707, 'logits/chosen': -0.2936278283596039, 'epoch': 8.57}

 95%|█████████▌| 22996/24156 [32:24:21<5:35:32, 17.36s/it]

 95%|█████████▌| 22997/24156 [32:24:38<5:31:43, 17.17s/it]


 95%|█████████▌| 22999/24156 [32:25:07<5:05:37, 15.85s/it]
{'loss': 0.4294, 'learning_rate': 1.666562566448989e-06, 'rewards/chosen': -1.7846962213516235, 'rewards/rejected': -4.757265090942383, 'rewards/accuracies': 1.0, 'rewards/margins': 2.972568988800049, 'policy_logps/rejected': -318.6604919433594, 'policy_logps/chosen': -338.460205078125, 'referece_logps/rejected': -271.0878601074219, 'referece_logps/chosen': -320.6132507324219, 'logits/rejected': 0.08351290225982666, 'logits/chosen': 0.18661879003047943, 'epoch': 8.57}

 95%|█████████▌| 23000/24156 [32:25:19<4:45:09, 14.80s/it]


 95%|█████████▌| 23002/24156 [32:26:09<6:13:24, 19.41s/it]
{'loss': 0.3497, 'learning_rate': 1.6662626665120487e-06, 'rewards/chosen': -1.3688546419143677, 'rewards/rejected': -3.1256673336029053, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7568128108978271, 'policy_logps/rejected': -258.3194885253906, 'policy_logps/chosen': -353.0307922363281, 'referece_logps/rejected': -227.06280517578125, 'referece_logps/chosen': -339.34228515625, 'logits/rejected': -0.4180373549461365, 'logits/chosen': -0.42424944043159485, 'epoch': 8.57}

 95%|█████████▌| 23003/24156 [32:26:22<5:37:27, 17.56s/it]

 95%|█████████▌| 23004/24156 [32:26:40<5:39:15, 17.67s/it]

 95%|█████████▌| 23005/24156 [32:26:57<5:37:06, 17.57s/it]


 95%|█████████▌| 23007/24156 [32:27:37<5:56:17, 18.60s/it]
{'loss': 0.2884, 'learning_rate': 1.6657625937592233e-06, 'rewards/chosen': -2.935070514678955, 'rewards/rejected': -4.821385860443115, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8863155841827393, 'policy_logps/rejected': -446.8418884277344, 'policy_logps/chosen': -544.739990234375, 'referece_logps/rejected': -398.6280517578125, 'referece_logps/chosen': -515.3892822265625, 'logits/rejected': -0.4193474054336548, 'logits/chosen': -0.4069201350212097, 'epoch': 8.57}

 95%|█████████▌| 23008/24156 [32:27:56<5:56:25, 18.63s/it]


 95%|█████████▌| 23010/24156 [32:28:31<5:47:54, 18.22s/it]
{'loss': 0.2635, 'learning_rate': 1.6654624064718837e-06, 'rewards/chosen': -2.31420636177063, 'rewards/rejected': -5.858749866485596, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5445430278778076, 'policy_logps/rejected': -382.4385070800781, 'policy_logps/chosen': -397.3546447753906, 'referece_logps/rejected': -323.8509826660156, 'referece_logps/chosen': -374.21258544921875, 'logits/rejected': 0.008323810994625092, 'logits/chosen': 0.00895649567246437, 'epoch': 8.57}

 95%|█████████▌| 23011/24156 [32:28:51<5:58:55, 18.81s/it]


 95%|█████████▌| 23013/24156 [32:29:23<5:26:36, 17.15s/it]
{'loss': 0.3374, 'learning_rate': 1.6651621115171683e-06, 'rewards/chosen': -1.7022924423217773, 'rewards/rejected': -4.036679744720459, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3343873023986816, 'policy_logps/rejected': -409.99078369140625, 'policy_logps/chosen': -409.55584716796875, 'referece_logps/rejected': -369.6240234375, 'referece_logps/chosen': -392.5329895019531, 'logits/rejected': -0.6613408327102661, 'logits/chosen': -0.8253749012947083, 'epoch': 8.57}

 95%|█████████▌| 23014/24156 [32:29:43<5:41:20, 17.93s/it]

 95%|█████████▌| 23015/24156 [32:30:00<5:40:45, 17.92s/it]

 95%|█████████▌| 23016/24156 [32:30:14<5:13:34, 16.50s/it]

 95%|█████████▌| 23017/24156 [32:30:34<5:32:15, 17.50s/it]

 95%|█████████▌| 23018/24156 [32:30:52<5:37:01, 17.77s/it]

 95%|█████████▌| 23019/24156 [32:31:06<5:14:38, 16.60s/it]

 95%|█████████▌| 23020/24156 [32:31:25<5:30:57, 17.48s/it]

 95%|█████████▌| 23021/24156 [32:31:41<5:22:23, 17.04s/it]

 95%|█████████▌| 23022/24156 [32:31:52<4:46:16, 15.15s/it]

 95%|█████████▌| 23023/24156 [32:32:10<5:00:44, 15.93s/it]

 95%|█████████▌| 23024/24156 [32:32:32<5:35:25, 17.78s/it]

 95%|█████████▌| 23025/24156 [32:32:45<5:10:48, 16.49s/it]


 95%|█████████▌| 23027/24156 [32:33:15<4:55:19, 15.70s/it]
{'loss': 0.3587, 'learning_rate': 1.6637593128879144e-06, 'rewards/chosen': -2.034168243408203, 'rewards/rejected': -4.178820610046387, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1446518898010254, 'policy_logps/rejected': -434.5887145996094, 'policy_logps/chosen': -417.2795715332031, 'referece_logps/rejected': -392.8005065917969, 'referece_logps/chosen': -396.9378662109375, 'logits/rejected': 0.42459243535995483, 'logits/chosen': 0.2431529313325882, 'epoch': 8.58}

 95%|█████████▌| 23028/24156 [32:33:30<4:47:02, 15.27s/it]

 95%|█████████▌| 23029/24156 [32:33:44<4:39:03, 14.86s/it]


 95%|█████████▌| 23031/24156 [32:34:15<4:44:23, 15.17s/it]
{'loss': 0.3029, 'learning_rate': 1.6633580834960093e-06, 'rewards/chosen': -2.0322694778442383, 'rewards/rejected': -4.627334117889404, 'rewards/accuracies': 0.875, 'rewards/margins': 2.595064878463745, 'policy_logps/rejected': -309.27294921875, 'policy_logps/chosen': -360.2593688964844, 'referece_logps/rejected': -262.9996032714844, 'referece_logps/chosen': -339.9366455078125, 'logits/rejected': 0.29077020287513733, 'logits/chosen': 0.32696500420570374, 'epoch': 8.58}

 95%|█████████▌| 23032/24156 [32:34:32<4:54:28, 15.72s/it]


 95%|█████████▌| 23034/24156 [32:35:07<5:03:28, 16.23s/it]
{'loss': 0.2923, 'learning_rate': 1.6630570362310397e-06, 'rewards/chosen': -3.223569869995117, 'rewards/rejected': -5.724275588989258, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5007059574127197, 'policy_logps/rejected': -455.13714599609375, 'policy_logps/chosen': -494.4617919921875, 'referece_logps/rejected': -397.8944091796875, 'referece_logps/chosen': -462.2261047363281, 'logits/rejected': -0.30126136541366577, 'logits/chosen': -0.22605088353157043, 'epoch': 8.58}

 95%|█████████▌| 23035/24156 [32:35:27<5:23:14, 17.30s/it]

 95%|█████████▌| 23036/24156 [32:35:39<4:55:09, 15.81s/it]

 95%|█████████▌| 23037/24156 [32:35:50<4:26:24, 14.28s/it]

 95%|█████████▌| 23038/24156 [32:36:07<4:41:47, 15.12s/it]

 95%|█████████▌| 23039/24156 [32:36:24<4:48:48, 15.51s/it]

 95%|█████████▌| 23040/24156 [32:36:42<5:06:43, 16.49s/it]

 95%|█████████▌| 23041/24156 [32:37:02<5:24:40, 17.47s/it]

 95%|█████████▌| 23042/24156 [32:37:16<5:05:34, 16.46s/it]

 95%|█████████▌| 23043/24156 [32:37:34<5:12:55, 16.87s/it]

 95%|█████████▌| 23044/24156 [32:37:56<5:40:10, 18.36s/it]

 95%|█████████▌| 23045/24156 [32:38:10<5:15:06, 17.02s/it]

 95%|█████████▌| 23046/24156 [32:38:22<4:46:42, 15.50s/it]

 95%|█████████▌| 23047/24156 [32:38:32<4:19:25, 14.04s/it]

 95%|█████████▌| 23048/24156 [32:38:50<4:38:03, 15.06s/it]

 95%|█████████▌| 23049/24156 [32:39:10<5:04:02, 16.48s/it]

 95%|█████████▌| 23050/24156 [32:39:29<5:20:29, 17.39s/it]

 95%|█████████▌| 23051/24156 [32:39:49<5:36:16, 18.26s/it]

 95%|█████████▌| 23052/24156 [32:40:10<5:49:44, 19.01s/it]

 95%|█████████▌| 23053/24156 [32:40:32<6:04:35, 19.83s/it]

 95%|█████████▌| 23054/24156 [32:40:52<6:05:30, 19.90s/it]

 95%|█████████▌| 23055/24156 [32:41:07<5:39:47, 18.52s/it]

 95%|█████████▌| 23056/24156 [32:41:24<5:27:59, 17.89s/it]

 95%|█████████▌| 23057/24156 [32:41:41<5:27:38, 17.89s/it]

 95%|█████████▌| 23058/24156 [32:41:58<5:22:17, 17.61s/it]

 95%|█████████▌| 23059/24156 [32:42:11<4:51:55, 15.97s/it]

 95%|█████████▌| 23060/24156 [32:42:30<5:12:41, 17.12s/it]

 95%|█████████▌| 23061/24156 [32:42:46<5:05:41, 16.75s/it]

 95%|█████████▌| 23062/24156 [32:43:06<5:22:57, 17.71s/it]

 95%|█████████▌| 23063/24156 [32:43:24<5:24:57, 17.84s/it]

 95%|█████████▌| 23064/24156 [32:43:44<5:35:20, 18.43s/it]

 95%|█████████▌| 23065/24156 [32:44:01<5:23:39, 17.80s/it]

 95%|█████████▌| 23066/24156 [32:44:17<5:17:19, 17.47s/it]

 95%|█████████▌| 23067/24156 [32:44:35<5:19:16, 17.59s/it]

 95%|█████████▌| 23068/24156 [32:44:53<5:21:27, 17.73s/it]

 96%|█████████▌| 23069/24156 [32:45:15<5:42:25, 18.90s/it]

 96%|█████████▌| 23070/24156 [32:45:34<5:45:49, 19.11s/it]

 96%|█████████▌| 23071/24156 [32:45:56<5:57:46, 19.79s/it]

 96%|█████████▌| 23072/24156 [32:46:14<5:48:14, 19.28s/it]


 96%|█████████▌| 23074/24156 [32:46:44<5:09:05, 17.14s/it]

 96%|█████████▌| 23075/24156 [32:47:01<5:09:32, 17.18s/it]

 96%|█████████▌| 23076/24156 [32:47:23<5:36:04, 18.67s/it]

 96%|█████████▌| 23077/24156 [32:47:44<5:43:55, 19.12s/it]

 96%|█████████▌| 23078/24156 [32:47:57<5:10:53, 17.30s/it]

 96%|█████████▌| 23079/24156 [32:48:17<5:24:16, 18.07s/it]

 96%|█████████▌| 23080/24156 [32:48:28<4:46:03, 15.95s/it]

 96%|█████████▌| 23081/24156 [32:48:45<4:51:51, 16.29s/it]

 96%|█████████▌| 23082/24156 [32:49:06<5:18:38, 17.80s/it]
{'loss': 0.345, 'learning_rate': 1.65822572333006e-06, 'rewards/chosen': -2.393648862838745, 'rewards/rejected': -4.084664344787598, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6910154819488525, 'policy_logps/rejected': -404.2583923339844, 'policy_logps/chosen': -319.79681396484375, 'referece_logps/rejected': -363.4117431640625, 'referece_logps/chosen': -295.8603210449219, 'logits/rejected': -0.9729499220848083, 'logits/chosen': -0.8661328554153442, 'epoch': 8.6}


 96%|█████████▌| 23084/24156 [32:49:40<5:07:17, 17.20s/it]

 96%|█████████▌| 23085/24156 [32:49:59<5:19:35, 17.90s/it]

 96%|█████████▌| 23086/24156 [32:50:18<5:22:20, 18.08s/it]

 96%|█████████▌| 23087/24156 [32:50:30<4:48:15, 16.18s/it]

 96%|█████████▌| 23088/24156 [32:50:51<5:14:56, 17.69s/it]

 96%|█████████▌| 23089/24156 [32:51:08<5:11:37, 17.52s/it]

 96%|█████████▌| 23090/24156 [32:51:24<5:06:03, 17.23s/it]

 96%|█████████▌| 23091/24156 [32:51:38<4:48:33, 16.26s/it]

 96%|█████████▌| 23092/24156 [32:51:58<5:04:14, 17.16s/it]

 96%|█████████▌| 23093/24156 [32:52:17<5:17:24, 17.92s/it]

 96%|█████████▌| 23094/24156 [32:52:33<5:06:59, 17.34s/it]

 96%|█████████▌| 23095/24156 [32:52:51<5:07:28, 17.39s/it]

 96%|█████████▌| 23096/24156 [32:53:06<4:57:04, 16.82s/it]

 96%|█████████▌| 23097/24156 [32:53:25<5:06:30, 17.37s/it]

 96%|█████████▌| 23098/24156 [32:53:44<5:12:51, 17.74s/it]

 96%|█████████▌| 23099/24156 [32:53:59<5:00:12, 17.04s/it]

 96%|█████████▌| 23100/24156 [32:54:19<5:13:05, 17.79s/it]

 96%|█████████▌| 23101/24156 [32:54:32<4:47:38, 16.36s/it]

 96%|█████████▌| 23102/24156 [32:54:44<4:25:35, 15.12s/it]

 96%|█████████▌| 23103/24156 [32:54:59<4:25:33, 15.13s/it]

 96%|█████████▌| 23104/24156 [32:55:17<4:38:15, 15.87s/it]

 96%|█████████▌| 23105/24156 [32:55:31<4:30:21, 15.43s/it]

 96%|█████████▌| 23106/24156 [32:55:51<4:52:34, 16.72s/it]

 96%|█████████▌| 23107/24156 [32:56:07<4:50:34, 16.62s/it]

 96%|█████████▌| 23108/24156 [32:56:26<5:00:19, 17.19s/it]

 96%|█████████▌| 23109/24156 [32:56:44<5:04:30, 17.45s/it]

 96%|█████████▌| 23110/24156 [32:57:01<5:06:08, 17.56s/it]

 96%|█████████▌| 23111/24156 [32:57:24<5:30:30, 18.98s/it]

 96%|█████████▌| 23112/24156 [32:57:44<5:36:10, 19.32s/it]

 96%|█████████▌| 23113/24156 [32:57:54<4:49:28, 16.65s/it]

 96%|█████████▌| 23114/24156 [32:58:12<4:56:21, 17.06s/it]

 96%|█████████▌| 23115/24156 [32:58:31<5:02:47, 17.45s/it]
{'loss': 0.4098, 'learning_rate': 1.6548883688931081e-06, 'rewards/chosen': -3.204612970352173, 'rewards/rejected': -5.457817554473877, 'rewards/accuracies': 1.0, 'rewards/margins': 2.253204822540283, 'policy_logps/rejected': -468.2636413574219, 'policy_logps/chosen': -411.3333740234375, 'referece_logps/rejected': -413.6854248046875, 'referece_logps/chosen': -379.2872619628906, 'logits/rejected': 0.013850819319486618, 'logits/chosen': 0.012557546608150005, 'epoch': 8.61}


 96%|█████████▌| 23117/24156 [32:59:10<5:18:13, 18.38s/it]

 96%|█████████▌| 23118/24156 [32:59:29<5:24:44, 18.77s/it]

 96%|█████████▌| 23119/24156 [32:59:49<5:31:59, 19.21s/it]

 96%|█████████▌| 23120/24156 [33:00:08<5:30:33, 19.14s/it]

 96%|█████████▌| 23121/24156 [33:00:30<5:40:59, 19.77s/it]
{'loss': 0.3512, 'learning_rate': 1.6542801978257324e-06, 'rewards/chosen': -3.8597848415374756, 'rewards/rejected': -5.844690322875977, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9849061965942383, 'policy_logps/rejected': -586.777099609375, 'policy_logps/chosen': -671.24951171875, 'referece_logps/rejected': -528.3302001953125, 'referece_logps/chosen': -632.6516723632812, 'logits/rejected': -0.8497796058654785, 'logits/chosen': -0.9246146082878113, 'epoch': 8.61}


 96%|█████████▌| 23123/24156 [33:01:10<5:42:57, 19.92s/it]

 96%|█████████▌| 23124/24156 [33:01:30<5:42:22, 19.91s/it]

 96%|█████████▌| 23125/24156 [33:01:47<5:26:33, 19.00s/it]

 96%|█████████▌| 23126/24156 [33:02:03<5:08:46, 17.99s/it]

 96%|█████████▌| 23127/24156 [33:02:14<4:36:14, 16.11s/it]

 96%|█████████▌| 23128/24156 [33:02:29<4:26:37, 15.56s/it]

 96%|█████████▌| 23129/24156 [33:02:46<4:34:03, 16.01s/it]

 96%|█████████▌| 23130/24156 [33:02:59<4:17:11, 15.04s/it]

 96%|█████████▌| 23131/24156 [33:03:18<4:40:16, 16.41s/it]

 96%|█████████▌| 23132/24156 [33:03:29<4:10:35, 14.68s/it]

 96%|█████████▌| 23133/24156 [33:03:44<4:14:49, 14.95s/it]

 96%|█████████▌| 23134/24156 [33:03:56<3:58:31, 14.00s/it]

 96%|█████████▌| 23135/24156 [33:04:18<4:38:55, 16.39s/it]

 96%|█████████▌| 23136/24156 [33:04:31<4:21:23, 15.38s/it]

 96%|█████████▌| 23137/24156 [33:04:47<4:21:52, 15.42s/it]

 96%|█████████▌| 23138/24156 [33:05:06<4:42:12, 16.63s/it]

 96%|█████████▌| 23139/24156 [33:05:28<5:09:34, 18.26s/it]

 96%|█████████▌| 23140/24156 [33:05:48<5:18:45, 18.82s/it]

 96%|█████████▌| 23141/24156 [33:06:09<5:29:56, 19.50s/it]

 96%|█████████▌| 23142/24156 [33:06:27<5:21:13, 19.01s/it]
{'loss': 0.374, 'learning_rate': 1.6521482671431542e-06, 'rewards/chosen': -1.4968088865280151, 'rewards/rejected': -4.592579364776611, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0957705974578857, 'policy_logps/rejected': -371.9970397949219, 'policy_logps/chosen': -386.4710998535156, 'referece_logps/rejected': -326.07122802734375, 'referece_logps/chosen': -371.5030517578125, 'logits/rejected': -0.5496378540992737, 'logits/chosen': -0.519853413105011, 'epoch': 8.62}


 96%|█████████▌| 23144/24156 [33:07:00<4:59:54, 17.78s/it]

 96%|█████████▌| 23145/24156 [33:07:13<4:35:20, 16.34s/it]

 96%|█████████▌| 23146/24156 [33:07:30<4:38:41, 16.56s/it]

 96%|█████████▌| 23147/24156 [33:07:41<4:08:44, 14.79s/it]

 96%|█████████▌| 23148/24156 [33:07:54<3:56:59, 14.11s/it]

 96%|█████████▌| 23149/24156 [33:08:04<3:39:56, 13.10s/it]

 96%|█████████▌| 23150/24156 [33:08:15<3:27:25, 12.37s/it]

 96%|█████████▌| 23151/24156 [33:08:27<3:27:29, 12.39s/it]
{'loss': 0.4299, 'learning_rate': 1.651232998880408e-06, 'rewards/chosen': -2.03653621673584, 'rewards/rejected': -5.933313369750977, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8967771530151367, 'policy_logps/rejected': -413.5000305175781, 'policy_logps/chosen': -502.0702209472656, 'referece_logps/rejected': -354.16693115234375, 'referece_logps/chosen': -481.70489501953125, 'logits/rejected': -0.08752240240573883, 'logits/chosen': -0.0857328474521637, 'epoch': 8.63}


 96%|█████████▌| 23153/24156 [33:09:01<4:11:51, 15.07s/it]
{'loss': 0.3538, 'learning_rate': 1.6510294771065631e-06, 'rewards/chosen': -2.442429304122925, 'rewards/rejected': -5.648444652557373, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2060155868530273, 'policy_logps/rejected': -586.4486083984375, 'policy_logps/chosen': -503.9869384765625, 'referece_logps/rejected': -529.964111328125, 'referece_logps/chosen': -479.5626525878906, 'logits/rejected': -0.9515066146850586, 'logits/chosen': -0.8497637510299683, 'epoch': 8.63}


 96%|█████████▌| 23155/24156 [33:09:30<4:02:05, 14.51s/it]
{'loss': 0.3197, 'learning_rate': 1.6508259085183961e-06, 'rewards/chosen': -1.77141535282135, 'rewards/rejected': -3.9739463329315186, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2025306224823, 'policy_logps/rejected': -325.2944030761719, 'policy_logps/chosen': -323.1828918457031, 'referece_logps/rejected': -285.5549621582031, 'referece_logps/chosen': -305.46875, 'logits/rejected': 0.1738899201154709, 'logits/chosen': 0.2852513790130615, 'epoch': 8.63}


 96%|█████████▌| 23157/24156 [33:09:59<4:08:42, 14.94s/it]

 96%|█████████▌| 23158/24156 [33:10:16<4:17:24, 15.48s/it]
{'loss': 0.3589, 'learning_rate': 1.6505204678913143e-06, 'rewards/chosen': -1.7684861421585083, 'rewards/rejected': -2.8157756328582764, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0472896099090576, 'policy_logps/rejected': -477.7712707519531, 'policy_logps/chosen': -628.472900390625, 'referece_logps/rejected': -449.613525390625, 'referece_logps/chosen': -610.7880249023438, 'logits/rejected': 0.1272648572921753, 'logits/chosen': 0.14678147435188293, 'epoch': 8.63}

 96%|█████████▌| 23159/24156 [33:10:26<3:53:29, 14.05s/it]

 96%|█████████▌| 23160/24156 [33:10:46<4:20:38, 15.70s/it]

 96%|█████████▌| 23161/24156 [33:11:02<4:23:39, 15.90s/it]


 96%|█████████▌| 23163/24156 [33:11:43<5:02:30, 18.28s/it]
{'loss': 0.228, 'learning_rate': 1.6500111663153238e-06, 'rewards/chosen': -2.290851593017578, 'rewards/rejected': -4.310859680175781, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0200083255767822, 'policy_logps/rejected': -321.43798828125, 'policy_logps/chosen': -464.7990417480469, 'referece_logps/rejected': -278.3293762207031, 'referece_logps/chosen': -441.8905029296875, 'logits/rejected': -0.5939791202545166, 'logits/chosen': -0.5629485249519348, 'epoch': 8.63}


 96%|█████████▌| 23165/24156 [33:12:13<4:36:15, 16.73s/it]
{'loss': 0.2379, 'learning_rate': 1.6498073638751873e-06, 'rewards/chosen': -2.5428719520568848, 'rewards/rejected': -6.27656364440918, 'rewards/accuracies': 1.0, 'rewards/margins': 3.733691692352295, 'policy_logps/rejected': -339.5078125, 'policy_logps/chosen': -435.7515869140625, 'referece_logps/rejected': -276.7422180175781, 'referece_logps/chosen': -410.3228759765625, 'logits/rejected': -0.459433913230896, 'logits/chosen': -0.5268898606300354, 'epoch': 8.63}


 96%|█████████▌| 23167/24156 [33:12:56<5:18:58, 19.35s/it]

 96%|█████████▌| 23168/24156 [33:13:18<5:31:23, 20.13s/it]
{'loss': 0.2399, 'learning_rate': 1.6495015726074844e-06, 'rewards/chosen': -2.609511137008667, 'rewards/rejected': -4.905570030212402, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2960586547851562, 'policy_logps/rejected': -486.1922912597656, 'policy_logps/chosen': -357.64727783203125, 'referece_logps/rejected': -437.1366271972656, 'referece_logps/chosen': -331.5521240234375, 'logits/rejected': -0.33057063817977905, 'logits/chosen': -0.36031007766723633, 'epoch': 8.63}


 96%|█████████▌| 23170/24156 [33:13:51<5:09:50, 18.85s/it]

 96%|█████████▌| 23171/24156 [33:14:08<4:57:31, 18.12s/it]

 96%|█████████▌| 23172/24156 [33:14:19<4:24:40, 16.14s/it]

 96%|█████████▌| 23173/24156 [33:14:35<4:22:09, 16.00s/it]
{'loss': 0.3362, 'learning_rate': 1.6489916869968207e-06, 'rewards/chosen': -2.2824461460113525, 'rewards/rejected': -4.582693576812744, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3002476692199707, 'policy_logps/rejected': -396.7900695800781, 'policy_logps/chosen': -441.96771240234375, 'referece_logps/rejected': -350.9631042480469, 'referece_logps/chosen': -419.143310546875, 'logits/rejected': -0.2251535952091217, 'logits/chosen': -0.21158787608146667, 'epoch': 8.63}


 96%|█████████▌| 23175/24156 [33:15:05<4:07:22, 15.13s/it]

 96%|█████████▌| 23176/24156 [33:15:25<4:31:42, 16.64s/it]

 96%|█████████▌| 23177/24156 [33:15:37<4:07:34, 15.17s/it]

 96%|█████████▌| 23178/24156 [33:15:49<3:53:23, 14.32s/it]
{'loss': 0.3024, 'learning_rate': 1.648481509712489e-06, 'rewards/chosen': -2.954331159591675, 'rewards/rejected': -5.00855827331543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.054227113723755, 'policy_logps/rejected': -392.86370849609375, 'policy_logps/chosen': -421.687255859375, 'referece_logps/rejected': -342.7781677246094, 'referece_logps/chosen': -392.1439514160156, 'logits/rejected': -0.25060951709747314, 'logits/chosen': -0.07001394778490067, 'epoch': 8.64}


 96%|█████████▌| 23180/24156 [33:16:19<3:55:28, 14.48s/it]

 96%|█████████▌| 23181/24156 [33:16:33<3:53:23, 14.36s/it]

 96%|█████████▌| 23182/24156 [33:16:49<3:59:06, 14.73s/it]

 96%|█████████▌| 23183/24156 [33:17:07<4:16:12, 15.80s/it]

 96%|█████████▌| 23184/24156 [33:17:27<4:35:32, 17.01s/it]
{'loss': 0.3019, 'learning_rate': 1.647868912284894e-06, 'rewards/chosen': -2.5944998264312744, 'rewards/rejected': -5.335390567779541, 'rewards/accuracies': 0.875, 'rewards/margins': 2.740891218185425, 'policy_logps/rejected': -383.57037353515625, 'policy_logps/chosen': -453.7006530761719, 'referece_logps/rejected': -330.21649169921875, 'referece_logps/chosen': -427.75567626953125, 'logits/rejected': -0.736786425113678, 'logits/chosen': -0.8747502565383911, 'epoch': 8.64}

 96%|█████████▌| 23185/24156 [33:17:44<4:36:01, 17.06s/it]

 96%|█████████▌| 23186/24156 [33:18:03<4:41:21, 17.40s/it]


 96%|█████████▌| 23188/24156 [33:18:35<4:27:55, 16.61s/it]
{'loss': 0.3003, 'learning_rate': 1.6474602810400993e-06, 'rewards/chosen': -2.227370262145996, 'rewards/rejected': -4.5472259521484375, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3198556900024414, 'policy_logps/rejected': -482.3460388183594, 'policy_logps/chosen': -434.15191650390625, 'referece_logps/rejected': -436.8737487792969, 'referece_logps/chosen': -411.878173828125, 'logits/rejected': -0.7444489002227783, 'logits/chosen': -0.7861182689666748, 'epoch': 8.64}


 96%|█████████▌| 23190/24156 [33:19:06<4:11:46, 15.64s/it]

 96%|█████████▌| 23191/24156 [33:19:18<3:55:44, 14.66s/it]

 96%|█████████▌| 23192/24156 [33:19:34<4:01:49, 15.05s/it]
{'loss': 0.334, 'learning_rate': 1.6470514635646365e-06, 'rewards/chosen': -2.192965030670166, 'rewards/rejected': -4.889863967895508, 'rewards/accuracies': 0.625, 'rewards/margins': 2.6968986988067627, 'policy_logps/rejected': -552.0012817382812, 'policy_logps/chosen': -478.2110595703125, 'referece_logps/rejected': -503.10260009765625, 'referece_logps/chosen': -456.2814025878906, 'logits/rejected': 0.5456016063690186, 'logits/chosen': 0.617484986782074, 'epoch': 8.64}

 96%|█████████▌| 23193/24156 [33:19:49<3:59:34, 14.93s/it]


 96%|█████████▌| 23195/24156 [33:20:28<4:32:17, 17.00s/it]

 96%|█████████▌| 23196/24156 [33:20:48<4:46:43, 17.92s/it]
{'loss': 0.3343, 'learning_rate': 1.646642459976095e-06, 'rewards/chosen': -2.6417007446289062, 'rewards/rejected': -4.4539875984191895, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8122868537902832, 'policy_logps/rejected': -392.57769775390625, 'policy_logps/chosen': -402.6950378417969, 'referece_logps/rejected': -348.0378112792969, 'referece_logps/chosen': -376.27801513671875, 'logits/rejected': -1.1030826568603516, 'logits/chosen': -1.1373964548110962, 'epoch': 8.64}

 96%|█████████▌| 23197/24156 [33:20:59<4:16:01, 16.02s/it]

 96%|█████████▌| 23198/24156 [33:21:15<4:15:26, 16.00s/it]


 96%|█████████▌| 23200/24156 [33:21:38<3:37:07, 13.63s/it]

 96%|█████████▌| 23201/24156 [33:21:50<3:30:23, 13.22s/it]

 96%|█████████▌| 23202/24156 [33:22:06<3:43:35, 14.06s/it]
{'loss': 0.3933, 'learning_rate': 1.6460286058886181e-06, 'rewards/chosen': -1.9377892017364502, 'rewards/rejected': -3.037980079650879, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1001908779144287, 'policy_logps/rejected': -439.90557861328125, 'policy_logps/chosen': -517.3341674804688, 'referece_logps/rejected': -409.5257568359375, 'referece_logps/chosen': -497.956298828125, 'logits/rejected': -0.3135327100753784, 'logits/chosen': -0.5646540522575378, 'epoch': 8.64}


 96%|█████████▌| 23204/24156 [33:22:41<4:15:16, 16.09s/it]

 96%|█████████▌| 23205/24156 [33:22:52<3:49:29, 14.48s/it]
{'loss': 0.3757, 'learning_rate': 1.6457215220353718e-06, 'rewards/chosen': -2.523331880569458, 'rewards/rejected': -4.929083824157715, 'rewards/accuracies': 0.875, 'rewards/margins': 2.405751943588257, 'policy_logps/rejected': -411.5495300292969, 'policy_logps/chosen': -530.817626953125, 'referece_logps/rejected': -362.25872802734375, 'referece_logps/chosen': -505.5843200683594, 'logits/rejected': 0.16416087746620178, 'logits/chosen': 0.10346654057502747, 'epoch': 8.65}

 96%|█████████▌| 23206/24156 [33:23:03<3:31:30, 13.36s/it]


 96%|█████████▌| 23208/24156 [33:23:32<3:44:00, 14.18s/it]

 96%|█████████▌| 23209/24156 [33:23:47<3:50:47, 14.62s/it]

 96%|█████████▌| 23210/24156 [33:24:06<4:09:11, 15.80s/it]
{'loss': 0.3564, 'learning_rate': 1.6452094834746543e-06, 'rewards/chosen': -2.4212875366210938, 'rewards/rejected': -3.2665653228759766, 'rewards/accuracies': 0.5, 'rewards/margins': 0.8452779054641724, 'policy_logps/rejected': -335.6683654785156, 'policy_logps/chosen': -348.21868896484375, 'referece_logps/rejected': -303.0027160644531, 'referece_logps/chosen': -324.0058288574219, 'logits/rejected': -1.0458818674087524, 'logits/chosen': -1.0815370082855225, 'epoch': 8.65}


 96%|█████████▌| 23212/24156 [33:24:39<4:05:47, 15.62s/it]
{'loss': 0.3042, 'learning_rate': 1.645004586844799e-06, 'rewards/chosen': -2.177259922027588, 'rewards/rejected': -3.8010523319244385, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6237925291061401, 'policy_logps/rejected': -402.0107727050781, 'policy_logps/chosen': -447.5030212402344, 'referece_logps/rejected': -364.000244140625, 'referece_logps/chosen': -425.73046875, 'logits/rejected': -0.5497168898582458, 'logits/chosen': -0.6757838129997253, 'epoch': 8.65}


 96%|█████████▌| 23214/24156 [33:25:06<3:45:06, 14.34s/it]

 96%|█████████▌| 23215/24156 [33:25:19<3:37:33, 13.87s/it]

 96%|█████████▌| 23216/24156 [33:25:38<4:04:17, 15.59s/it]

 96%|█████████▌| 23217/24156 [33:25:52<3:55:27, 15.05s/it]

 96%|█████████▌| 23218/24156 [33:26:10<4:10:45, 16.04s/it]

 96%|█████████▌| 23219/24156 [33:26:28<4:20:07, 16.66s/it]

 96%|█████████▌| 23220/24156 [33:26:51<4:46:06, 18.34s/it]

 96%|█████████▌| 23221/24156 [33:27:12<4:58:51, 19.18s/it]

 96%|█████████▌| 23222/24156 [33:27:32<5:01:05, 19.34s/it]

 96%|█████████▌| 23223/24156 [33:27:52<5:04:08, 19.56s/it]
{'loss': 0.2924, 'learning_rate': 1.6438768267139427e-06, 'rewards/chosen': -3.8213155269622803, 'rewards/rejected': -4.93952751159668, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1182118654251099, 'policy_logps/rejected': -339.2823486328125, 'policy_logps/chosen': -583.8905639648438, 'referece_logps/rejected': -289.8870544433594, 'referece_logps/chosen': -545.6774291992188, 'logits/rejected': -0.372574120759964, 'logits/chosen': -0.670240044593811, 'epoch': 8.65}

 96%|█████████▌| 23224/24156 [33:28:03<4:25:08, 17.07s/it]


 96%|█████████▌| 23226/24156 [33:28:26<3:38:58, 14.13s/it]

 96%|█████████▌| 23227/24156 [33:28:42<3:49:27, 14.82s/it]
{'loss': 0.3218, 'learning_rate': 1.6434663847419433e-06, 'rewards/chosen': -2.5903103351593018, 'rewards/rejected': -4.805566787719727, 'rewards/accuracies': 0.875, 'rewards/margins': 2.215256690979004, 'policy_logps/rejected': -342.9643859863281, 'policy_logps/chosen': -328.6878967285156, 'referece_logps/rejected': -294.90875244140625, 'referece_logps/chosen': -302.7847900390625, 'logits/rejected': -0.4781323969364166, 'logits/chosen': -0.3680167496204376, 'epoch': 8.65}


 96%|█████████▌| 23229/24156 [33:29:21<4:24:29, 17.12s/it]
{'loss': 0.2852, 'learning_rate': 1.6432610943428528e-06, 'rewards/chosen': -2.38179874420166, 'rewards/rejected': -5.130204677581787, 'rewards/accuracies': 0.875, 'rewards/margins': 2.748406171798706, 'policy_logps/rejected': -592.738525390625, 'policy_logps/chosen': -680.9656982421875, 'referece_logps/rejected': -541.4365234375, 'referece_logps/chosen': -657.1476440429688, 'logits/rejected': -0.5882632732391357, 'logits/chosen': -0.5018296241760254, 'epoch': 8.65}

 96%|█████████▌| 23230/24156 [33:29:33<4:02:15, 15.70s/it]

 96%|█████████▌| 23231/24156 [33:29:54<4:24:22, 17.15s/it]

 96%|█████████▌| 23232/24156 [33:30:14<4:37:43, 18.03s/it]


 96%|█████████▌| 23234/24156 [33:30:49<4:31:09, 17.65s/it]
{'loss': 0.3055, 'learning_rate': 1.6427476660086868e-06, 'rewards/chosen': -2.124631881713867, 'rewards/rejected': -4.242557048797607, 'rewards/accuracies': 1.0, 'rewards/margins': 2.117924928665161, 'policy_logps/rejected': -421.8789978027344, 'policy_logps/chosen': -259.58892822265625, 'referece_logps/rejected': -379.45343017578125, 'referece_logps/chosen': -238.34262084960938, 'logits/rejected': -0.46654459834098816, 'logits/chosen': -0.3685914874076843, 'epoch': 8.66}


 96%|█████████▌| 23236/24156 [33:31:26<4:41:52, 18.38s/it]

 96%|█████████▌| 23237/24156 [33:31:46<4:48:48, 18.86s/it]

 96%|█████████▌| 23238/24156 [33:32:06<4:52:41, 19.13s/it]

 96%|█████████▌| 23239/24156 [33:32:23<4:41:45, 18.44s/it]

 96%|█████████▌| 23240/24156 [33:32:39<4:30:26, 17.71s/it]
{'loss': 0.3979, 'learning_rate': 1.6421311707229768e-06, 'rewards/chosen': -2.2377021312713623, 'rewards/rejected': -3.987130880355835, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7494287490844727, 'policy_logps/rejected': -294.1739196777344, 'policy_logps/chosen': -354.8501892089844, 'referece_logps/rejected': -254.30262756347656, 'referece_logps/chosen': -332.4731750488281, 'logits/rejected': -0.8757389783859253, 'logits/chosen': -0.9650189280509949, 'epoch': 8.66}


 96%|█████████▌| 23242/24156 [33:33:01<3:36:27, 14.21s/it]

 96%|█████████▌| 23243/24156 [33:33:14<3:33:24, 14.02s/it]
{'loss': 0.309, 'learning_rate': 1.6418227672163754e-06, 'rewards/chosen': -3.643259286880493, 'rewards/rejected': -7.8633270263671875, 'rewards/accuracies': 0.875, 'rewards/margins': 4.220068454742432, 'policy_logps/rejected': -626.1717529296875, 'policy_logps/chosen': -635.4031372070312, 'referece_logps/rejected': -547.5384521484375, 'referece_logps/chosen': -598.9705200195312, 'logits/rejected': -0.2795809507369995, 'logits/chosen': -0.3727666139602661, 'epoch': 8.66}

 96%|█████████▌| 23244/24156 [33:33:26<3:20:54, 13.22s/it]


 96%|█████████▌| 23246/24156 [33:33:56<3:36:18, 14.26s/it]

 96%|█████████▌| 23247/24156 [33:34:11<3:37:57, 14.39s/it]

 96%|█████████▌| 23248/24156 [33:34:31<4:01:52, 15.98s/it]
{'loss': 0.2454, 'learning_rate': 1.6413085306352663e-06, 'rewards/chosen': -2.1862285137176514, 'rewards/rejected': -4.1667280197143555, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9804991483688354, 'policy_logps/rejected': -326.51031494140625, 'policy_logps/chosen': -379.08758544921875, 'referece_logps/rejected': -284.8430480957031, 'referece_logps/chosen': -357.2253112792969, 'logits/rejected': -0.4704251289367676, 'logits/chosen': -0.48006725311279297, 'epoch': 8.66}


 96%|█████████▌| 23250/24156 [33:35:03<3:57:14, 15.71s/it]
{'loss': 0.3213, 'learning_rate': 1.6411027552880918e-06, 'rewards/chosen': -2.4726786613464355, 'rewards/rejected': -5.369102478027344, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8964247703552246, 'policy_logps/rejected': -438.48651123046875, 'policy_logps/chosen': -400.9798583984375, 'referece_logps/rejected': -384.79547119140625, 'referece_logps/chosen': -376.25311279296875, 'logits/rejected': -1.0714476108551025, 'logits/chosen': -0.9975839257240295, 'epoch': 8.66}


 96%|█████████▋| 23252/24156 [33:35:37<4:12:05, 16.73s/it]
{'loss': 0.2787, 'learning_rate': 1.6408969338404073e-06, 'rewards/chosen': -3.2146084308624268, 'rewards/rejected': -4.599782466888428, 'rewards/accuracies': 1.0, 'rewards/margins': 1.385174036026001, 'policy_logps/rejected': -333.9377746582031, 'policy_logps/chosen': -356.95880126953125, 'referece_logps/rejected': -287.9399719238281, 'referece_logps/chosen': -324.812744140625, 'logits/rejected': -0.32177790999412537, 'logits/chosen': -0.3263774812221527, 'epoch': 8.66}


 96%|█████████▋| 23254/24156 [33:36:17<4:38:04, 18.50s/it]

 96%|█████████▋| 23255/24156 [33:36:29<4:08:18, 16.54s/it]
{'loss': 0.2901, 'learning_rate': 1.6405881152628006e-06, 'rewards/chosen': -2.216508150100708, 'rewards/rejected': -4.681040287017822, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4645321369171143, 'policy_logps/rejected': -500.2806091308594, 'policy_logps/chosen': -590.0272216796875, 'referece_logps/rejected': -453.4702453613281, 'referece_logps/chosen': -567.862060546875, 'logits/rejected': -1.2838337421417236, 'logits/chosen': -1.2887071371078491, 'epoch': 8.66}


 96%|█████████▋| 23257/24156 [33:37:06<4:20:52, 17.41s/it]
{'loss': 0.3533, 'learning_rate': 1.6403821786285996e-06, 'rewards/chosen': -2.458033323287964, 'rewards/rejected': -4.489203453063965, 'rewards/accuracies': 1.0, 'rewards/margins': 2.031170129776001, 'policy_logps/rejected': -474.11358642578125, 'policy_logps/chosen': -378.5576171875, 'referece_logps/rejected': -429.2215270996094, 'referece_logps/chosen': -353.977294921875, 'logits/rejected': -0.5509170293807983, 'logits/chosen': -0.33306705951690674, 'epoch': 8.67}


 96%|█████████▋| 23259/24156 [33:37:41<4:18:59, 17.32s/it]
{'loss': 0.4695, 'learning_rate': 1.6401761959457037e-06, 'rewards/chosen': -3.4616243839263916, 'rewards/rejected': -5.486072063446045, 'rewards/accuracies': 1.0, 'rewards/margins': 2.024447441101074, 'policy_logps/rejected': -696.727294921875, 'policy_logps/chosen': -500.38287353515625, 'referece_logps/rejected': -641.8665771484375, 'referece_logps/chosen': -465.7666320800781, 'logits/rejected': -0.5397204160690308, 'logits/chosen': -0.5613436698913574, 'epoch': 8.67}

 96%|█████████▋| 23260/24156 [33:37:56<4:09:21, 16.70s/it]

 96%|█████████▋| 23261/24156 [33:38:12<4:03:38, 16.33s/it]

 96%|█████████▋| 23262/24156 [33:38:22<3:38:10, 14.64s/it]

 96%|█████████▋| 23263/24156 [33:38:34<3:26:58, 13.91s/it]

 96%|█████████▋| 23264/24156 [33:38:46<3:15:41, 13.16s/it]

 96%|█████████▋| 23265/24156 [33:38:58<3:10:08, 12.80s/it]


 96%|█████████▋| 23267/24156 [33:39:30<3:26:35, 13.94s/it]
{'loss': 0.3507, 'learning_rate': 1.6393518050234608e-06, 'rewards/chosen': -1.9660797119140625, 'rewards/rejected': -2.6757829189300537, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7097030878067017, 'policy_logps/rejected': -415.0384521484375, 'policy_logps/chosen': -304.7997741699219, 'referece_logps/rejected': -388.28057861328125, 'referece_logps/chosen': -285.1390075683594, 'logits/rejected': -0.5794808268547058, 'logits/chosen': -0.3628090023994446, 'epoch': 8.67}

 96%|█████████▋| 23268/24156 [33:39:42<3:18:36, 13.42s/it]

 96%|█████████▋| 23269/24156 [33:40:00<3:40:52, 14.94s/it]

 96%|█████████▋| 23270/24156 [33:40:20<4:01:19, 16.34s/it]

 96%|█████████▋| 23271/24156 [33:40:40<4:16:26, 17.39s/it]


 96%|█████████▋| 23273/24156 [33:41:19<4:32:57, 18.55s/it]
{'loss': 0.3344, 'learning_rate': 1.638733029046595e-06, 'rewards/chosen': -2.914248466491699, 'rewards/rejected': -5.778770446777344, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8645222187042236, 'policy_logps/rejected': -451.0928649902344, 'policy_logps/chosen': -481.598876953125, 'referece_logps/rejected': -393.30517578125, 'referece_logps/chosen': -452.45635986328125, 'logits/rejected': 0.6911109685897827, 'logits/chosen': 0.5577317476272583, 'epoch': 8.67}

 96%|█████████▋| 23274/24156 [33:41:34<4:16:28, 17.45s/it]

 96%|█████████▋| 23275/24156 [33:41:50<4:09:39, 17.00s/it]


 96%|█████████▋| 23277/24156 [33:42:22<4:04:45, 16.71s/it]
{'loss': 0.4058, 'learning_rate': 1.6383202820534282e-06, 'rewards/chosen': -3.027409315109253, 'rewards/rejected': -4.212289810180664, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1848807334899902, 'policy_logps/rejected': -564.879638671875, 'policy_logps/chosen': -552.3980102539062, 'referece_logps/rejected': -522.7567138671875, 'referece_logps/chosen': -522.1238403320312, 'logits/rejected': -0.004222705960273743, 'logits/chosen': 0.08479726314544678, 'epoch': 8.67}

 96%|█████████▋| 23278/24156 [33:42:41<4:15:02, 17.43s/it]

 96%|█████████▋| 23279/24156 [33:42:52<3:47:24, 15.56s/it]


 96%|█████████▋| 23281/24156 [33:43:23<3:48:22, 15.66s/it]

 96%|█████████▋| 23282/24156 [33:43:35<3:32:35, 14.59s/it]

 96%|█████████▋| 23283/24156 [33:43:54<3:48:19, 15.69s/it]

 96%|█████████▋| 23284/24156 [33:44:13<4:05:39, 16.90s/it]

 96%|█████████▋| 23285/24156 [33:44:35<4:28:26, 18.49s/it]

 96%|█████████▋| 23286/24156 [33:44:57<4:43:01, 19.52s/it]
{'loss': 0.3108, 'learning_rate': 1.6373909302060112e-06, 'rewards/chosen': -3.389371395111084, 'rewards/rejected': -6.394805908203125, 'rewards/accuracies': 1.0, 'rewards/margins': 3.005434989929199, 'policy_logps/rejected': -450.247802734375, 'policy_logps/chosen': -485.6380310058594, 'referece_logps/rejected': -386.29974365234375, 'referece_logps/chosen': -451.7442932128906, 'logits/rejected': -0.3927508592605591, 'logits/chosen': -0.42616719007492065, 'epoch': 8.68}

 96%|█████████▋| 23287/24156 [33:45:17<4:43:04, 19.54s/it]

 96%|█████████▋| 23288/24156 [33:45:33<4:25:51, 18.38s/it]

 96%|█████████▋| 23289/24156 [33:45:43<3:52:13, 16.07s/it]


 96%|█████████▋| 23291/24156 [33:46:19<4:06:39, 17.11s/it]

 96%|█████████▋| 23292/24156 [33:46:34<3:55:05, 16.33s/it]
{'loss': 0.3559, 'learning_rate': 1.6367708465962075e-06, 'rewards/chosen': -2.037564516067505, 'rewards/rejected': -3.869788885116577, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8322243690490723, 'policy_logps/rejected': -228.01071166992188, 'policy_logps/chosen': -255.73565673828125, 'referece_logps/rejected': -189.31283569335938, 'referece_logps/chosen': -235.36004638671875, 'logits/rejected': -0.6629287004470825, 'logits/chosen': -0.6385355591773987, 'epoch': 8.68}

 96%|█████████▋| 23293/24156 [33:46:53<4:06:22, 17.13s/it]

 96%|█████████▋| 23294/24156 [33:47:04<3:40:02, 15.32s/it]

 96%|█████████▋| 23295/24156 [33:47:21<3:47:52, 15.88s/it]


 96%|█████████▋| 23297/24156 [33:47:58<4:09:05, 17.40s/it]

 96%|█████████▋| 23298/24156 [33:48:18<4:18:06, 18.05s/it]
{'loss': 0.2384, 'learning_rate': 1.6361503508853298e-06, 'rewards/chosen': -3.1187620162963867, 'rewards/rejected': -5.508599758148193, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3898379802703857, 'policy_logps/rejected': -349.0860290527344, 'policy_logps/chosen': -408.48724365234375, 'referece_logps/rejected': -294.00006103515625, 'referece_logps/chosen': -377.2996520996094, 'logits/rejected': -0.8772737979888916, 'logits/chosen': -0.9153592586517334, 'epoch': 8.68}

 96%|█████████▋| 23299/24156 [33:48:39<4:31:23, 19.00s/it]

 96%|█████████▋| 23300/24156 [33:48:58<4:33:15, 19.15s/it]

 96%|█████████▋| 23301/24156 [33:49:15<4:21:32, 18.35s/it]


 96%|█████████▋| 23303/24156 [33:49:50<4:11:57, 17.72s/it]
{'loss': 0.2289, 'learning_rate': 1.6356329566165372e-06, 'rewards/chosen': -2.1533305644989014, 'rewards/rejected': -4.0603437423706055, 'rewards/accuracies': 0.875, 'rewards/margins': 1.907013177871704, 'policy_logps/rejected': -280.7159423828125, 'policy_logps/chosen': -330.4441223144531, 'referece_logps/rejected': -240.11248779296875, 'referece_logps/chosen': -308.9107971191406, 'logits/rejected': 0.0013004615902900696, 'logits/chosen': 0.1110427975654602, 'epoch': 8.68}

 96%|█████████▋| 23304/24156 [33:50:09<4:19:11, 18.25s/it]


 96%|█████████▋| 23306/24156 [33:50:42<4:05:39, 17.34s/it]

 96%|█████████▋| 23307/24156 [33:50:58<4:00:17, 16.98s/it]

 96%|█████████▋| 23308/24156 [33:51:18<4:11:39, 17.81s/it]
{'loss': 0.2858, 'learning_rate': 1.6351152766778355e-06, 'rewards/chosen': -2.2803661823272705, 'rewards/rejected': -4.621767997741699, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3414015769958496, 'policy_logps/rejected': -462.7677307128906, 'policy_logps/chosen': -376.760986328125, 'referece_logps/rejected': -416.550048828125, 'referece_logps/chosen': -353.95733642578125, 'logits/rejected': 0.0299638994038105, 'logits/chosen': 0.03032674267888069, 'epoch': 8.68}


 96%|█████████▋| 23310/24156 [33:51:46<3:45:12, 15.97s/it]
{'loss': 0.4071, 'learning_rate': 1.6349081247668916e-06, 'rewards/chosen': -2.7888259887695312, 'rewards/rejected': -4.699963569641113, 'rewards/accuracies': 0.875, 'rewards/margins': 1.911137342453003, 'policy_logps/rejected': -248.85165405273438, 'policy_logps/chosen': -327.6011657714844, 'referece_logps/rejected': -201.85202026367188, 'referece_logps/chosen': -299.7129211425781, 'logits/rejected': -0.0884665697813034, 'logits/chosen': -0.36391928791999817, 'epoch': 8.68}

 97%|█████████▋| 23311/24156 [33:52:01<3:39:47, 15.61s/it]

 97%|█████████▋| 23312/24156 [33:52:17<3:41:12, 15.73s/it]

 97%|█████████▋| 23313/24156 [33:52:29<3:24:12, 14.53s/it]

 97%|█████████▋| 23314/24156 [33:52:47<3:40:37, 15.72s/it]

 97%|█████████▋| 23315/24156 [33:53:02<3:38:04, 15.56s/it]


 97%|█████████▋| 23317/24156 [33:53:38<3:46:51, 16.22s/it]

 97%|█████████▋| 23318/24156 [33:53:50<3:28:55, 14.96s/it]

 97%|█████████▋| 23319/24156 [33:54:08<3:41:20, 15.87s/it]

 97%|█████████▋| 23320/24156 [33:54:24<3:42:01, 15.93s/it]
{'loss': 0.256, 'learning_rate': 1.633871680684222e-06, 'rewards/chosen': -2.3906049728393555, 'rewards/rejected': -5.188434600830078, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7978296279907227, 'policy_logps/rejected': -384.90130615234375, 'policy_logps/chosen': -439.4434814453125, 'referece_logps/rejected': -333.0169677734375, 'referece_logps/chosen': -415.5374450683594, 'logits/rejected': -1.0545839071273804, 'logits/chosen': -1.0292918682098389, 'epoch': 8.69}

 97%|█████████▋| 23321/24156 [33:54:43<3:56:03, 16.96s/it]


 97%|█████████▋| 23323/24156 [33:55:16<3:50:42, 16.62s/it]
{'loss': 0.3087, 'learning_rate': 1.633560525169506e-06, 'rewards/chosen': -1.2347567081451416, 'rewards/rejected': -3.0890324115753174, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8542754650115967, 'policy_logps/rejected': -461.450439453125, 'policy_logps/chosen': -483.3953552246094, 'referece_logps/rejected': -430.5600891113281, 'referece_logps/chosen': -471.04779052734375, 'logits/rejected': 0.41533297300338745, 'logits/chosen': 0.42749208211898804, 'epoch': 8.69}

 97%|█████████▋| 23324/24156 [33:55:37<4:08:16, 17.90s/it]

 97%|█████████▋| 23325/24156 [33:56:00<4:26:28, 19.24s/it]


 97%|█████████▋| 23327/24156 [33:56:32<3:58:22, 17.25s/it]

 97%|█████████▋| 23328/24156 [33:56:52<4:10:00, 18.12s/it]
{'loss': 0.4274, 'learning_rate': 1.6330417048790387e-06, 'rewards/chosen': -2.6934752464294434, 'rewards/rejected': -3.4048521518707275, 'rewards/accuracies': 0.625, 'rewards/margins': 0.7113770246505737, 'policy_logps/rejected': -558.202880859375, 'policy_logps/chosen': -383.5263671875, 'referece_logps/rejected': -524.1543579101562, 'referece_logps/chosen': -356.59161376953125, 'logits/rejected': -0.4979165196418762, 'logits/chosen': -0.4010515809059143, 'epoch': 8.69}

 97%|█████████▋| 23329/24156 [33:57:04<3:42:21, 16.13s/it]

 97%|█████████▋| 23330/24156 [33:57:21<3:44:37, 16.32s/it]

 97%|█████████▋| 23331/24156 [33:57:43<4:08:06, 18.04s/it]


 97%|█████████▋| 23333/24156 [33:58:08<3:33:28, 15.56s/it]
{'loss': 0.2696, 'learning_rate': 1.632522600083238e-06, 'rewards/chosen': -2.2697176933288574, 'rewards/rejected': -3.9421401023864746, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6724225282669067, 'policy_logps/rejected': -357.0835266113281, 'policy_logps/chosen': -402.1675720214844, 'referece_logps/rejected': -317.662109375, 'referece_logps/chosen': -379.47039794921875, 'logits/rejected': 0.037855640053749084, 'logits/chosen': 0.041883498430252075, 'epoch': 8.69}

 97%|█████████▋| 23334/24156 [33:58:19<3:13:39, 14.14s/it]

 97%|█████████▋| 23335/24156 [33:58:37<3:28:53, 15.27s/it]


 97%|█████████▋| 23337/24156 [33:59:13<3:50:10, 16.86s/it]

 97%|█████████▋| 23338/24156 [33:59:33<4:02:12, 17.77s/it]

 97%|█████████▋| 23339/24156 [33:59:53<4:11:06, 18.44s/it]
{'loss': 0.3024, 'learning_rate': 1.6318992991097313e-06, 'rewards/chosen': -2.0965399742126465, 'rewards/rejected': -4.5789289474487305, 'rewards/accuracies': 0.875, 'rewards/margins': 2.482388973236084, 'policy_logps/rejected': -348.68170166015625, 'policy_logps/chosen': -297.3661804199219, 'referece_logps/rejected': -302.89239501953125, 'referece_logps/chosen': -276.4007873535156, 'logits/rejected': -0.64113450050354, 'logits/chosen': -0.5244855880737305, 'epoch': 8.7}


 97%|█████████▋| 23341/24156 [34:00:35<4:26:10, 19.60s/it]
{'loss': 0.2188, 'learning_rate': 1.6316914412212257e-06, 'rewards/chosen': -2.4237399101257324, 'rewards/rejected': -4.466335296630859, 'rewards/accuracies': 0.75, 'rewards/margins': 2.042595624923706, 'policy_logps/rejected': -544.8896484375, 'policy_logps/chosen': -415.77569580078125, 'referece_logps/rejected': -500.2262878417969, 'referece_logps/chosen': -391.538330078125, 'logits/rejected': 0.39089998602867126, 'logits/chosen': 0.5172195434570312, 'epoch': 8.7}


 97%|█████████▋| 23343/24156 [34:01:12<4:18:19, 19.06s/it]
{'loss': 0.2488, 'learning_rate': 1.6314835379089607e-06, 'rewards/chosen': -2.1538474559783936, 'rewards/rejected': -5.4831390380859375, 'rewards/accuracies': 1.0, 'rewards/margins': 3.329291820526123, 'policy_logps/rejected': -426.66998291015625, 'policy_logps/chosen': -428.7180480957031, 'referece_logps/rejected': -371.83856201171875, 'referece_logps/chosen': -407.1795654296875, 'logits/rejected': 0.13143187761306763, 'logits/chosen': 0.18929660320281982, 'epoch': 8.7}

 97%|█████████▋| 23344/24156 [34:01:28<4:04:36, 18.07s/it]

 97%|█████████▋| 23345/24156 [34:01:43<3:52:44, 17.22s/it]

 97%|█████████▋| 23346/24156 [34:02:02<3:58:38, 17.68s/it]

 97%|█████████▋| 23347/24156 [34:02:22<4:08:01, 18.40s/it]

 97%|█████████▋| 23348/24156 [34:02:40<4:06:10, 18.28s/it]

 97%|█████████▋| 23349/24156 [34:03:00<4:11:46, 18.72s/it]


 97%|█████████▋| 23351/24156 [34:03:41<4:22:31, 19.57s/it]
{'loss': 0.3523, 'learning_rate': 1.63065147072135e-06, 'rewards/chosen': -3.6263983249664307, 'rewards/rejected': -4.370113372802734, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7437152862548828, 'policy_logps/rejected': -449.90203857421875, 'policy_logps/chosen': -355.10614013671875, 'referece_logps/rejected': -406.2008972167969, 'referece_logps/chosen': -318.8421936035156, 'logits/rejected': -0.44579511880874634, 'logits/chosen': -0.2698515057563782, 'epoch': 8.7}

 97%|█████████▋| 23352/24156 [34:03:59<4:18:32, 19.29s/it]

 97%|█████████▋| 23353/24156 [34:04:15<4:05:55, 18.38s/it]


 97%|█████████▋| 23355/24156 [34:04:53<4:08:52, 18.64s/it]
{'loss': 0.3778, 'learning_rate': 1.6302351649738392e-06, 'rewards/chosen': -1.666355848312378, 'rewards/rejected': -3.362139940261841, 'rewards/accuracies': 0.875, 'rewards/margins': 1.695784091949463, 'policy_logps/rejected': -479.13763427734375, 'policy_logps/chosen': -371.57275390625, 'referece_logps/rejected': -445.5162658691406, 'referece_logps/chosen': -354.9091796875, 'logits/rejected': -0.8223087191581726, 'logits/chosen': -0.7677879333496094, 'epoch': 8.7}

 97%|█████████▋| 23356/24156 [34:05:10<4:00:42, 18.05s/it]


 97%|█████████▋| 23358/24156 [34:05:35<3:24:52, 15.40s/it]

 97%|█████████▋| 23359/24156 [34:05:47<3:11:08, 14.39s/it]

 97%|█████████▋| 23360/24156 [34:06:03<3:16:09, 14.79s/it]
{'loss': 0.3262, 'learning_rate': 1.629714527883883e-06, 'rewards/chosen': -2.3979616165161133, 'rewards/rejected': -4.792049407958984, 'rewards/accuracies': 0.75, 'rewards/margins': 2.394087553024292, 'policy_logps/rejected': -336.001953125, 'policy_logps/chosen': -369.07781982421875, 'referece_logps/rejected': -288.08148193359375, 'referece_logps/chosen': -345.0981750488281, 'logits/rejected': -0.24046772718429565, 'logits/chosen': -0.2395763099193573, 'epoch': 8.7}


 97%|█████████▋| 23362/24156 [34:06:33<3:23:31, 15.38s/it]
{'loss': 0.3302, 'learning_rate': 1.6295061937919893e-06, 'rewards/chosen': -1.7309601306915283, 'rewards/rejected': -3.1749582290649414, 'rewards/accuracies': 0.75, 'rewards/margins': 1.443998098373413, 'policy_logps/rejected': -377.0367126464844, 'policy_logps/chosen': -433.21600341796875, 'referece_logps/rejected': -345.287109375, 'referece_logps/chosen': -415.90643310546875, 'logits/rejected': -0.6898494958877563, 'logits/chosen': -0.5428972840309143, 'epoch': 8.7}

 97%|█████████▋| 23363/24156 [34:06:53<3:39:48, 16.63s/it]

 97%|█████████▋| 23364/24156 [34:07:07<3:29:00, 15.83s/it]


 97%|█████████▋| 23366/24156 [34:07:41<3:31:39, 16.08s/it]
{'loss': 0.2832, 'learning_rate': 1.6290893898233177e-06, 'rewards/chosen': -3.140907049179077, 'rewards/rejected': -5.092923164367676, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9520158767700195, 'policy_logps/rejected': -507.5405578613281, 'policy_logps/chosen': -422.02203369140625, 'referece_logps/rejected': -456.611328125, 'referece_logps/chosen': -390.6129455566406, 'logits/rejected': -0.44811809062957764, 'logits/chosen': -0.46914249658584595, 'epoch': 8.71}


 97%|█████████▋| 23368/24156 [34:08:09<3:20:47, 15.29s/it]
{'loss': 0.3139, 'learning_rate': 1.6288809199765114e-06, 'rewards/chosen': -2.269704580307007, 'rewards/rejected': -5.479734897613525, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2100303173065186, 'policy_logps/rejected': -607.3922729492188, 'policy_logps/chosen': -300.73846435546875, 'referece_logps/rejected': -552.5949096679688, 'referece_logps/chosen': -278.0414123535156, 'logits/rejected': -0.26425227522850037, 'logits/chosen': 0.11068680882453918, 'epoch': 8.71}

 97%|█████████▋| 23369/24156 [34:08:21<3:06:43, 14.24s/it]


 97%|█████████▋| 23371/24156 [34:08:53<3:24:15, 15.61s/it]
{'loss': 0.2558, 'learning_rate': 1.628568130420374e-06, 'rewards/chosen': -2.4404144287109375, 'rewards/rejected': -5.1259331703186035, 'rewards/accuracies': 1.0, 'rewards/margins': 2.685518980026245, 'policy_logps/rejected': -396.9267578125, 'policy_logps/chosen': -507.9508361816406, 'referece_logps/rejected': -345.6674499511719, 'referece_logps/chosen': -483.5466613769531, 'logits/rejected': -0.12442419677972794, 'logits/chosen': -0.46124276518821716, 'epoch': 8.71}

 97%|█████████▋| 23372/24156 [34:09:08<3:18:37, 15.20s/it]

 97%|█████████▋| 23373/24156 [34:09:20<3:09:08, 14.49s/it]

 97%|█████████▋| 23374/24156 [34:09:40<3:29:18, 16.06s/it]

 97%|█████████▋| 23375/24156 [34:09:52<3:12:05, 14.76s/it]

 97%|█████████▋| 23376/24156 [34:10:14<3:42:33, 17.12s/it]

 97%|█████████▋| 23377/24156 [34:10:36<3:57:48, 18.32s/it]

 97%|█████████▋| 23378/24156 [34:10:54<3:57:34, 18.32s/it]

 97%|█████████▋| 23379/24156 [34:11:11<3:50:55, 17.83s/it]

 97%|█████████▋| 23380/24156 [34:11:30<3:56:11, 18.26s/it]

 97%|█████████▋| 23381/24156 [34:11:51<4:08:17, 19.22s/it]

 97%|█████████▋| 23382/24156 [34:12:11<4:09:23, 19.33s/it]

 97%|█████████▋| 23383/24156 [34:12:26<3:53:04, 18.09s/it]

 97%|█████████▋| 23384/24156 [34:12:47<4:05:28, 19.08s/it]

 97%|█████████▋| 23385/24156 [34:13:04<3:56:44, 18.42s/it]

 97%|█████████▋| 23386/24156 [34:13:22<3:53:05, 18.16s/it]

 97%|█████████▋| 23387/24156 [34:13:41<3:55:42, 18.39s/it]

 97%|█████████▋| 23388/24156 [34:14:02<4:07:39, 19.35s/it]

 97%|█████████▋| 23389/24156 [34:14:24<4:15:06, 19.96s/it]

 97%|█████████▋| 23390/24156 [34:14:43<4:12:51, 19.81s/it]

 97%|█████████▋| 23391/24156 [34:15:03<4:11:30, 19.73s/it]

 97%|█████████▋| 23392/24156 [34:15:21<4:04:53, 19.23s/it]

 97%|█████████▋| 23393/24156 [34:15:34<3:43:01, 17.54s/it]

 97%|█████████▋| 23394/24156 [34:15:53<3:46:32, 17.84s/it]


 97%|█████████▋| 23396/24156 [34:16:18<3:11:44, 15.14s/it]
{'loss': 0.3828, 'learning_rate': 1.6259576006735508e-06, 'rewards/chosen': -2.0787205696105957, 'rewards/rejected': -4.513002872467041, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4342823028564453, 'policy_logps/rejected': -387.1003112792969, 'policy_logps/chosen': -237.838623046875, 'referece_logps/rejected': -341.9703063964844, 'referece_logps/chosen': -217.05142211914062, 'logits/rejected': -0.06110388785600662, 'logits/chosen': 0.009441450238227844, 'epoch': 8.72}

 97%|█████████▋| 23397/24156 [34:16:34<3:16:25, 15.53s/it]

 97%|█████████▋| 23398/24156 [34:16:54<3:34:03, 16.94s/it]

 97%|█████████▋| 23399/24156 [34:17:07<3:15:37, 15.51s/it]

 97%|█████████▋| 23400/24156 [34:17:19<3:01:55, 14.44s/it]


 97%|█████████▋| 23402/24156 [34:17:50<3:10:38, 15.17s/it]
{'loss': 0.3044, 'learning_rate': 1.6253300259117445e-06, 'rewards/chosen': -2.744919538497925, 'rewards/rejected': -4.427902698516846, 'rewards/accuracies': 0.625, 'rewards/margins': 1.6829832792282104, 'policy_logps/rejected': -475.4357604980469, 'policy_logps/chosen': -418.92584228515625, 'referece_logps/rejected': -431.15673828125, 'referece_logps/chosen': -391.4766845703125, 'logits/rejected': -0.18665869534015656, 'logits/chosen': 0.05038858950138092, 'epoch': 8.72}

 97%|█████████▋| 23403/24156 [34:18:06<3:11:54, 15.29s/it]

 97%|█████████▋| 23404/24156 [34:18:25<3:28:43, 16.65s/it]


 97%|█████████▋| 23406/24156 [34:18:58<3:30:30, 16.84s/it]
{'loss': 0.3837, 'learning_rate': 1.6249114178805245e-06, 'rewards/chosen': -1.8704166412353516, 'rewards/rejected': -4.587310314178467, 'rewards/accuracies': 0.75, 'rewards/margins': 2.716893434524536, 'policy_logps/rejected': -362.48516845703125, 'policy_logps/chosen': -389.5674133300781, 'referece_logps/rejected': -316.612060546875, 'referece_logps/chosen': -370.8632507324219, 'logits/rejected': -0.25610077381134033, 'logits/chosen': -0.23385776579380035, 'epoch': 8.72}

 97%|█████████▋| 23407/24156 [34:19:13<3:23:46, 16.32s/it]

 97%|█████████▋| 23408/24156 [34:19:33<3:35:56, 17.32s/it]

 97%|█████████▋| 23409/24156 [34:19:45<3:15:03, 15.67s/it]

 97%|█████████▋| 23410/24156 [34:20:03<3:23:52, 16.40s/it]

 97%|█████████▋| 23411/24156 [34:20:14<3:04:08, 14.83s/it]

 97%|█████████▋| 23412/24156 [34:20:34<3:21:49, 16.28s/it]

 97%|█████████▋| 23413/24156 [34:20:53<3:34:12, 17.30s/it]

 97%|█████████▋| 23414/24156 [34:21:04<3:09:24, 15.32s/it]

 97%|█████████▋| 23415/24156 [34:21:24<3:25:02, 16.60s/it]

 97%|█████████▋| 23416/24156 [34:21:43<3:34:23, 17.38s/it]

 97%|█████████▋| 23417/24156 [34:22:04<3:48:57, 18.59s/it]

 97%|█████████▋| 23418/24156 [34:22:24<3:54:43, 19.08s/it]

 97%|█████████▋| 23419/24156 [34:22:42<3:48:42, 18.62s/it]

 97%|█████████▋| 23420/24156 [34:22:57<3:35:12, 17.54s/it]

 97%|█████████▋| 23421/24156 [34:23:16<3:41:32, 18.08s/it]

 97%|█████████▋| 23422/24156 [34:23:35<3:44:45, 18.37s/it]

 97%|█████████▋| 23423/24156 [34:23:53<3:42:21, 18.20s/it]

 97%|█████████▋| 23424/24156 [34:24:10<3:36:18, 17.73s/it]

 97%|█████████▋| 23425/24156 [34:24:30<3:44:16, 18.41s/it]

 97%|█████████▋| 23426/24156 [34:24:45<3:33:33, 17.55s/it]

 97%|█████████▋| 23427/24156 [34:25:01<3:26:54, 17.03s/it]

 97%|█████████▋| 23428/24156 [34:25:21<3:38:38, 18.02s/it]

 97%|█████████▋| 23429/24156 [34:25:42<3:46:38, 18.71s/it]

 97%|█████████▋| 23430/24156 [34:25:57<3:35:00, 17.77s/it]

 97%|█████████▋| 23431/24156 [34:26:13<3:27:13, 17.15s/it]

 97%|█████████▋| 23432/24156 [34:26:28<3:20:36, 16.63s/it]

 97%|█████████▋| 23433/24156 [34:26:48<3:32:11, 17.61s/it]

 97%|█████████▋| 23434/24156 [34:27:01<3:14:59, 16.20s/it]

 97%|█████████▋| 23435/24156 [34:27:14<3:03:54, 15.30s/it]

 97%|█████████▋| 23436/24156 [34:27:32<3:11:53, 15.99s/it]

 97%|█████████▋| 23437/24156 [34:27:44<2:55:59, 14.69s/it]

 97%|█████████▋| 23438/24156 [34:28:03<3:12:48, 16.11s/it]

 97%|█████████▋| 23439/24156 [34:28:21<3:18:17, 16.59s/it]

 97%|█████████▋| 23440/24156 [34:28:31<2:56:34, 14.80s/it]


 97%|█████████▋| 23442/24156 [34:28:53<2:30:35, 12.65s/it]
{'loss': 0.2775, 'learning_rate': 1.6211358715456104e-06, 'rewards/chosen': -1.9459073543548584, 'rewards/rejected': -4.044175624847412, 'rewards/accuracies': 0.875, 'rewards/margins': 2.098268508911133, 'policy_logps/rejected': -356.87060546875, 'policy_logps/chosen': -297.4728088378906, 'referece_logps/rejected': -316.42877197265625, 'referece_logps/chosen': -278.01373291015625, 'logits/rejected': -0.30182403326034546, 'logits/chosen': -0.17358914017677307, 'epoch': 8.73}

 97%|█████████▋| 23443/24156 [34:29:06<2:34:23, 12.99s/it]

 97%|█████████▋| 23444/24156 [34:29:17<2:26:01, 12.30s/it]

 97%|█████████▋| 23445/24156 [34:29:31<2:32:38, 12.88s/it]

 97%|█████████▋| 23446/24156 [34:29:46<2:36:56, 13.26s/it]

 97%|█████████▋| 23447/24156 [34:30:02<2:47:02, 14.14s/it]

 97%|█████████▋| 23448/24156 [34:30:12<2:33:53, 13.04s/it]

 97%|█████████▋| 23449/24156 [34:30:32<2:58:18, 15.13s/it]

 97%|█████████▋| 23450/24156 [34:30:45<2:51:29, 14.57s/it]

 97%|█████████▋| 23451/24156 [34:31:02<2:57:46, 15.13s/it]

 97%|█████████▋| 23452/24156 [34:31:20<3:08:26, 16.06s/it]

 97%|█████████▋| 23453/24156 [34:31:40<3:21:38, 17.21s/it]

 97%|█████████▋| 23454/24156 [34:31:55<3:12:47, 16.48s/it]

 97%|█████████▋| 23455/24156 [34:32:15<3:24:17, 17.49s/it]

 97%|█████████▋| 23456/24156 [34:32:31<3:19:37, 17.11s/it]

 97%|█████████▋| 23457/24156 [34:32:48<3:20:16, 17.19s/it]

 97%|█████████▋| 23458/24156 [34:33:02<3:06:59, 16.07s/it]

 97%|█████████▋| 23459/24156 [34:33:21<3:19:39, 17.19s/it]

 97%|█████████▋| 23460/24156 [34:33:32<2:56:37, 15.23s/it]

 97%|█████████▋| 23461/24156 [34:33:54<3:20:03, 17.27s/it]

 97%|█████████▋| 23462/24156 [34:34:11<3:17:03, 17.04s/it]

 97%|█████████▋| 23463/24156 [34:34:23<2:59:20, 15.53s/it]

 97%|█████████▋| 23464/24156 [34:34:33<2:42:21, 14.08s/it]

 97%|█████████▋| 23465/24156 [34:34:50<2:51:16, 14.87s/it]


 97%|█████████▋| 23467/24156 [34:35:20<2:54:38, 15.21s/it]

 97%|█████████▋| 23468/24156 [34:35:40<3:09:28, 16.52s/it]

 97%|█████████▋| 23469/24156 [34:35:55<3:04:42, 16.13s/it]

 97%|█████████▋| 23470/24156 [34:36:12<3:06:29, 16.31s/it]

 97%|█████████▋| 23471/24156 [34:36:27<3:03:42, 16.09s/it]

 97%|█████████▋| 23472/24156 [34:36:42<2:58:12, 15.63s/it]

 97%|█████████▋| 23473/24156 [34:37:00<3:06:22, 16.37s/it]

 97%|█████████▋| 23474/24156 [34:37:18<3:11:50, 16.88s/it]

 97%|█████████▋| 23475/24156 [34:37:38<3:21:37, 17.76s/it]

 97%|█████████▋| 23476/24156 [34:37:57<3:24:13, 18.02s/it]

 97%|█████████▋| 23477/24156 [34:38:10<3:07:45, 16.59s/it]

 97%|█████████▋| 23478/24156 [34:38:29<3:15:48, 17.33s/it]

 97%|█████████▋| 23479/24156 [34:38:41<2:58:53, 15.85s/it]

 97%|█████████▋| 23480/24156 [34:38:59<3:04:55, 16.41s/it]

 97%|█████████▋| 23481/24156 [34:39:11<2:50:43, 15.18s/it]

 97%|█████████▋| 23482/24156 [34:39:22<2:35:20, 13.83s/it]

 97%|█████████▋| 23483/24156 [34:39:44<3:02:50, 16.30s/it]
{'loss': 0.3383, 'learning_rate': 1.6168183229659774e-06, 'rewards/chosen': -2.399282693862915, 'rewards/rejected': -4.168808937072754, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7695261240005493, 'policy_logps/rejected': -526.65576171875, 'policy_logps/chosen': -610.992919921875, 'referece_logps/rejected': -484.9676513671875, 'referece_logps/chosen': -587.0001220703125, 'logits/rejected': -0.20245380699634552, 'logits/chosen': -0.08601547032594681, 'epoch': 8.75}


 97%|█████████▋| 23485/24156 [34:40:13<2:49:47, 15.18s/it]

 97%|█████████▋| 23486/24156 [34:40:31<2:59:06, 16.04s/it]

 97%|█████████▋| 23487/24156 [34:40:51<3:10:29, 17.08s/it]

 97%|█████████▋| 23488/24156 [34:41:03<2:53:24, 15.58s/it]

 97%|█████████▋| 23489/24156 [34:41:17<2:48:35, 15.17s/it]

 97%|█████████▋| 23490/24156 [34:41:36<3:02:21, 16.43s/it]

 97%|█████████▋| 23491/24156 [34:41:58<3:19:59, 18.04s/it]
{'loss': 0.3775, 'learning_rate': 1.6159736970180946e-06, 'rewards/chosen': -2.519256830215454, 'rewards/rejected': -4.980530738830566, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4612741470336914, 'policy_logps/rejected': -441.5614013671875, 'policy_logps/chosen': -473.5867004394531, 'referece_logps/rejected': -391.7560729980469, 'referece_logps/chosen': -448.3941345214844, 'logits/rejected': -0.04755999147891998, 'logits/chosen': -0.0795370414853096, 'epoch': 8.75}


 97%|█████████▋| 23493/24156 [34:42:37<3:28:14, 18.85s/it]

 97%|█████████▋| 23494/24156 [34:42:56<3:25:20, 18.61s/it]

 97%|█████████▋| 23495/24156 [34:43:07<3:00:03, 16.34s/it]

 97%|█████████▋| 23496/24156 [34:43:27<3:11:36, 17.42s/it]

 97%|█████████▋| 23497/24156 [34:43:46<3:18:51, 18.10s/it]

 97%|█████████▋| 23498/24156 [34:44:08<3:29:16, 19.08s/it]

 97%|█████████▋| 23499/24156 [34:44:21<3:11:12, 17.46s/it]

 97%|█████████▋| 23500/24156 [34:44:33<2:51:36, 15.70s/it]

 97%|█████████▋| 23501/24156 [34:45:02<3:33:55, 19.60s/it]

 97%|█████████▋| 23502/24156 [34:45:19<3:27:05, 19.00s/it]

 97%|█████████▋| 23503/24156 [34:45:39<3:29:01, 19.21s/it]

 97%|█████████▋| 23504/24156 [34:45:59<3:30:44, 19.39s/it]
{'loss': 0.237, 'learning_rate': 1.6145996687684669e-06, 'rewards/chosen': -2.4101569652557373, 'rewards/rejected': -5.001702785491943, 'rewards/accuracies': 1.0, 'rewards/margins': 2.591546058654785, 'policy_logps/rejected': -300.9000244140625, 'policy_logps/chosen': -387.41351318359375, 'referece_logps/rejected': -250.88299560546875, 'referece_logps/chosen': -363.3119201660156, 'logits/rejected': -0.0015787184238433838, 'logits/chosen': -0.065361887216568, 'epoch': 8.76}


 97%|█████████▋| 23506/24156 [34:46:31<3:16:29, 18.14s/it]
{'loss': 0.3434, 'learning_rate': 1.6143881139722379e-06, 'rewards/chosen': -2.4757182598114014, 'rewards/rejected': -4.596707820892334, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1209893226623535, 'policy_logps/rejected': -350.56341552734375, 'policy_logps/chosen': -482.75927734375, 'referece_logps/rejected': -304.59637451171875, 'referece_logps/chosen': -458.00213623046875, 'logits/rejected': 0.004902444779872894, 'logits/chosen': -0.09931624680757523, 'epoch': 8.76}


 97%|█████████▋| 23508/24156 [34:47:08<3:16:42, 18.21s/it]
{'loss': 0.3148, 'learning_rate': 1.6141765149964994e-06, 'rewards/chosen': -2.3500640392303467, 'rewards/rejected': -5.022031784057617, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6719672679901123, 'policy_logps/rejected': -394.0552978515625, 'policy_logps/chosen': -436.418212890625, 'referece_logps/rejected': -343.8349304199219, 'referece_logps/chosen': -412.9175720214844, 'logits/rejected': -0.013243719935417175, 'logits/chosen': -0.08477834612131119, 'epoch': 8.76}


 97%|█████████▋| 23510/24156 [34:47:41<3:07:29, 17.41s/it]

 97%|█████████▋| 23511/24156 [34:47:55<2:58:27, 16.60s/it]

 97%|█████████▋| 23512/24156 [34:48:15<3:08:20, 17.55s/it]

 97%|█████████▋| 23513/24156 [34:48:27<2:48:06, 15.69s/it]

 97%|█████████▋| 23514/24156 [34:48:47<3:04:04, 17.20s/it]
{'loss': 0.248, 'learning_rate': 1.6135414531443988e-06, 'rewards/chosen': -2.1914560794830322, 'rewards/rejected': -4.290731906890869, 'rewards/accuracies': 0.875, 'rewards/margins': 2.099275827407837, 'policy_logps/rejected': -426.26593017578125, 'policy_logps/chosen': -450.8668212890625, 'referece_logps/rejected': -383.3586120605469, 'referece_logps/chosen': -428.95220947265625, 'logits/rejected': 0.009060516953468323, 'logits/chosen': -0.030010655522346497, 'epoch': 8.76}


 97%|█████████▋| 23516/24156 [34:49:18<2:52:51, 16.21s/it]
{'loss': 0.2568, 'learning_rate': 1.6133296776028103e-06, 'rewards/chosen': -2.1455116271972656, 'rewards/rejected': -3.5644137859344482, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4189021587371826, 'policy_logps/rejected': -421.21527099609375, 'policy_logps/chosen': -466.8460693359375, 'referece_logps/rejected': -385.5711364746094, 'referece_logps/chosen': -445.39093017578125, 'logits/rejected': -0.5319339632987976, 'logits/chosen': -0.3492985963821411, 'epoch': 8.76}


 97%|█████████▋| 23518/24156 [34:49:58<3:13:46, 18.22s/it]

 97%|█████████▋| 23519/24156 [34:50:14<3:06:40, 17.58s/it]
{'loss': 0.2277, 'learning_rate': 1.6130119316013134e-06, 'rewards/chosen': -2.612908363342285, 'rewards/rejected': -5.998061656951904, 'rewards/accuracies': 1.0, 'rewards/margins': 3.385153293609619, 'policy_logps/rejected': -391.72808837890625, 'policy_logps/chosen': -359.4383239746094, 'referece_logps/rejected': -331.74749755859375, 'referece_logps/chosen': -333.3092041015625, 'logits/rejected': -0.533572793006897, 'logits/chosen': -0.39848387241363525, 'epoch': 8.76}


 97%|█████████▋| 23521/24156 [34:50:48<3:03:35, 17.35s/it]

 97%|█████████▋| 23522/24156 [34:51:05<3:01:43, 17.20s/it]

 97%|█████████▋| 23523/24156 [34:51:24<3:07:17, 17.75s/it]

 97%|█████████▋| 23524/24156 [34:51:38<2:55:27, 16.66s/it]

 97%|█████████▋| 23525/24156 [34:51:55<2:56:48, 16.81s/it]
{'loss': 0.3061, 'learning_rate': 1.6123761421060283e-06, 'rewards/chosen': -3.002134084701538, 'rewards/rejected': -4.699568748474121, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6974347829818726, 'policy_logps/rejected': -490.29962158203125, 'policy_logps/chosen': -358.2252197265625, 'referece_logps/rejected': -443.3039245605469, 'referece_logps/chosen': -328.203857421875, 'logits/rejected': -0.32865580916404724, 'logits/chosen': -0.3043513596057892, 'epoch': 8.76}


 97%|█████████▋| 23527/24156 [34:52:24<2:42:02, 15.46s/it]
{'loss': 0.2613, 'learning_rate': 1.612164124184279e-06, 'rewards/chosen': -1.9557368755340576, 'rewards/rejected': -5.007572174072266, 'rewards/accuracies': 0.75, 'rewards/margins': 3.051835060119629, 'policy_logps/rejected': -279.1860046386719, 'policy_logps/chosen': -415.665771484375, 'referece_logps/rejected': -229.1102752685547, 'referece_logps/chosen': -396.1083984375, 'logits/rejected': -0.09092380106449127, 'logits/chosen': -0.27648240327835083, 'epoch': 8.77}


 97%|█████████▋| 23529/24156 [34:52:58<2:51:13, 16.39s/it]

 97%|█████████▋| 23530/24156 [34:53:17<2:59:26, 17.20s/it]

 97%|█████████▋| 23531/24156 [34:53:37<3:06:33, 17.91s/it]
{'loss': 0.2277, 'learning_rate': 1.6117399562972699e-06, 'rewards/chosen': -2.452104091644287, 'rewards/rejected': -6.436173439025879, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9840691089630127, 'policy_logps/rejected': -296.33935546875, 'policy_logps/chosen': -320.0361633300781, 'referece_logps/rejected': -231.97763061523438, 'referece_logps/chosen': -295.5151062011719, 'logits/rejected': -0.7837246060371399, 'logits/chosen': -0.8351406455039978, 'epoch': 8.77}

 97%|█████████▋| 23532/24156 [34:53:53<2:59:46, 17.29s/it]


 97%|█████████▋| 23534/24156 [34:54:30<3:07:57, 18.13s/it]
{'loss': 0.2447, 'learning_rate': 1.6114217149039914e-06, 'rewards/chosen': -3.8445606231689453, 'rewards/rejected': -6.179748058319092, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3351874351501465, 'policy_logps/rejected': -517.9993286132812, 'policy_logps/chosen': -570.8159790039062, 'referece_logps/rejected': -456.2018737792969, 'referece_logps/chosen': -532.3704223632812, 'logits/rejected': -0.04606589674949646, 'logits/chosen': -0.10614488273859024, 'epoch': 8.77}


 97%|█████████▋| 23536/24156 [34:55:00<2:47:33, 16.22s/it]

 97%|█████████▋| 23537/24156 [34:55:16<2:46:41, 16.16s/it]

 97%|█████████▋| 23538/24156 [34:55:37<3:03:57, 17.86s/it]
{'loss': 0.3167, 'learning_rate': 1.6109972391734848e-06, 'rewards/chosen': -2.533780574798584, 'rewards/rejected': -7.71604061126709, 'rewards/accuracies': 1.0, 'rewards/margins': 5.182259559631348, 'policy_logps/rejected': -428.5957946777344, 'policy_logps/chosen': -399.659912109375, 'referece_logps/rejected': -351.4354553222656, 'referece_logps/chosen': -374.3221740722656, 'logits/rejected': -0.4501132369041443, 'logits/chosen': -0.544359564781189, 'epoch': 8.77}

 97%|█████████▋| 23539/24156 [34:55:52<2:55:11, 17.04s/it]

 97%|█████████▋| 23540/24156 [34:56:09<2:52:13, 16.77s/it]


 97%|█████████▋| 23542/24156 [34:56:42<2:55:58, 17.20s/it]

 97%|█████████▋| 23543/24156 [34:56:58<2:52:42, 16.90s/it]
{'loss': 0.322, 'learning_rate': 1.6104663973864792e-06, 'rewards/chosen': -2.7360877990722656, 'rewards/rejected': -5.504515647888184, 'rewards/accuracies': 0.75, 'rewards/margins': 2.768428087234497, 'policy_logps/rejected': -228.51475524902344, 'policy_logps/chosen': -439.03717041015625, 'referece_logps/rejected': -173.4696044921875, 'referece_logps/chosen': -411.67626953125, 'logits/rejected': -0.5670858025550842, 'logits/chosen': -0.6477722525596619, 'epoch': 8.77}


 97%|█████████▋| 23545/24156 [34:57:28<2:43:45, 16.08s/it]
{'loss': 0.2394, 'learning_rate': 1.6102539838376793e-06, 'rewards/chosen': -3.2381668090820312, 'rewards/rejected': -5.8810553550720215, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6428890228271484, 'policy_logps/rejected': -342.4925537109375, 'policy_logps/chosen': -623.5284423828125, 'referece_logps/rejected': -283.6819763183594, 'referece_logps/chosen': -591.146728515625, 'logits/rejected': -0.30800795555114746, 'logits/chosen': -0.26063671708106995, 'epoch': 8.77}

 97%|█████████▋| 23546/24156 [34:57:49<2:56:59, 17.41s/it]

 97%|█████████▋| 23547/24156 [34:58:07<2:58:24, 17.58s/it]


 97%|█████████▋| 23549/24156 [34:58:46<3:07:56, 18.58s/it]

 97%|█████████▋| 23550/24156 [34:59:05<3:10:37, 18.87s/it]

 97%|█████████▋| 23551/24156 [34:59:21<3:00:52, 17.94s/it]
{'loss': 0.2784, 'learning_rate': 1.6096164799589994e-06, 'rewards/chosen': -1.7505725622177124, 'rewards/rejected': -4.591021537780762, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8404488563537598, 'policy_logps/rejected': -444.7406005859375, 'policy_logps/chosen': -509.75286865234375, 'referece_logps/rejected': -398.830322265625, 'referece_logps/chosen': -492.24713134765625, 'logits/rejected': -0.03131139278411865, 'logits/chosen': -0.15999451279640198, 'epoch': 8.77}


 98%|█████████▊| 23553/24156 [34:59:54<2:50:43, 16.99s/it]

 98%|█████████▊| 23554/24156 [35:00:15<3:00:03, 17.95s/it]

 98%|█████████▊| 23555/24156 [35:00:29<2:50:07, 16.98s/it]

 98%|█████████▊| 23556/24156 [35:00:43<2:38:46, 15.88s/it]

 98%|█████████▊| 23557/24156 [35:01:02<2:50:24, 17.07s/it]

 98%|█████████▊| 23558/24156 [35:01:17<2:41:20, 16.19s/it]

 98%|█████████▊| 23559/24156 [35:01:28<2:25:57, 14.67s/it]

 98%|█████████▊| 23560/24156 [35:01:48<2:42:33, 16.36s/it]

 98%|█████████▊| 23561/24156 [35:02:04<2:42:01, 16.34s/it]

 98%|█████████▊| 23562/24156 [35:02:26<2:56:26, 17.82s/it]

 98%|█████████▊| 23563/24156 [35:02:48<3:09:12, 19.14s/it]
{'loss': 0.3383, 'learning_rate': 1.60834028903199e-06, 'rewards/chosen': -1.8028111457824707, 'rewards/rejected': -3.950443744659424, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1476328372955322, 'policy_logps/rejected': -536.8683471679688, 'policy_logps/chosen': -415.1691589355469, 'referece_logps/rejected': -497.3639221191406, 'referece_logps/chosen': -397.1410827636719, 'logits/rejected': -0.5296552181243896, 'logits/chosen': -0.3629299998283386, 'epoch': 8.78}

 98%|█████████▊| 23564/24156 [35:03:05<3:02:55, 18.54s/it]


 98%|█████████▊| 23566/24156 [35:03:32<2:35:39, 15.83s/it]

 98%|█████████▊| 23567/24156 [35:03:48<2:36:05, 15.90s/it]
{'loss': 0.3429, 'learning_rate': 1.6079145419361578e-06, 'rewards/chosen': -2.529100179672241, 'rewards/rejected': -3.8016810417175293, 'rewards/accuracies': 0.875, 'rewards/margins': 1.272580862045288, 'policy_logps/rejected': -299.6632080078125, 'policy_logps/chosen': -230.7500762939453, 'referece_logps/rejected': -261.6463623046875, 'referece_logps/chosen': -205.45907592773438, 'logits/rejected': -0.8424341082572937, 'logits/chosen': -0.8239779472351074, 'epoch': 8.78}


 98%|█████████▊| 23569/24156 [35:04:28<2:56:33, 18.05s/it]

 98%|█████████▊| 23570/24156 [35:04:39<2:38:08, 16.19s/it]

 98%|█████████▊| 23571/24156 [35:04:50<2:21:18, 14.49s/it]

 98%|█████████▊| 23572/24156 [35:05:02<2:14:56, 13.86s/it]

 98%|█████████▊| 23573/24156 [35:05:22<2:30:03, 15.44s/it]

 98%|█████████▊| 23574/24156 [35:05:38<2:33:17, 15.80s/it]

 98%|█████████▊| 23575/24156 [35:05:55<2:35:22, 16.05s/it]

 98%|█████████▊| 23576/24156 [35:06:15<2:45:55, 17.16s/it]

 98%|█████████▊| 23577/24156 [35:06:36<2:57:05, 18.35s/it]

 98%|█████████▊| 23578/24156 [35:06:52<2:50:57, 17.75s/it]

 98%|█████████▊| 23579/24156 [35:07:09<2:47:42, 17.44s/it]

 98%|█████████▊| 23580/24156 [35:07:29<2:54:27, 18.17s/it]

 98%|█████████▊| 23581/24156 [35:07:47<2:54:30, 18.21s/it]

 98%|█████████▊| 23582/24156 [35:08:06<2:56:33, 18.46s/it]
{'loss': 0.3458, 'learning_rate': 1.6063164340156636e-06, 'rewards/chosen': -2.7832982540130615, 'rewards/rejected': -4.7869439125061035, 'rewards/accuracies': 0.75, 'rewards/margins': 2.003645420074463, 'policy_logps/rejected': -439.56866455078125, 'policy_logps/chosen': -341.40594482421875, 'referece_logps/rejected': -391.69921875, 'referece_logps/chosen': -313.5729675292969, 'logits/rejected': -0.9013450145721436, 'logits/chosen': -0.8657297492027283, 'epoch': 8.79}


 98%|█████████▊| 23584/24156 [35:08:44<2:59:31, 18.83s/it]

 98%|█████████▊| 23585/24156 [35:09:04<3:01:11, 19.04s/it]

 98%|█████████▊| 23586/24156 [35:09:24<3:03:08, 19.28s/it]
{'loss': 0.303, 'learning_rate': 1.605889857445218e-06, 'rewards/chosen': -1.6119316816329956, 'rewards/rejected': -3.924121856689453, 'rewards/accuracies': 1.0, 'rewards/margins': 2.312190055847168, 'policy_logps/rejected': -511.3179931640625, 'policy_logps/chosen': -488.6292724609375, 'referece_logps/rejected': -472.0768127441406, 'referece_logps/chosen': -472.510009765625, 'logits/rejected': -0.1684195101261139, 'logits/chosen': -0.13865827023983002, 'epoch': 8.79}


 98%|█████████▊| 23588/24156 [35:10:03<3:04:12, 19.46s/it]
{'loss': 0.3792, 'learning_rate': 1.6056765037997022e-06, 'rewards/chosen': -2.0667216777801514, 'rewards/rejected': -2.9336135387420654, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8668918609619141, 'policy_logps/rejected': -517.4449462890625, 'policy_logps/chosen': -461.1882019042969, 'referece_logps/rejected': -488.1087646484375, 'referece_logps/chosen': -440.5210266113281, 'logits/rejected': 0.023175448179244995, 'logits/chosen': -0.03639398515224457, 'epoch': 8.79}

 98%|█████████▊| 23589/24156 [35:10:24<3:07:17, 19.82s/it]


 98%|█████████▊| 23591/24156 [35:11:02<3:05:19, 19.68s/it]

 98%|█████████▊| 23592/24156 [35:11:17<2:50:34, 18.15s/it]
{'loss': 0.267, 'learning_rate': 1.6052496658647937e-06, 'rewards/chosen': -1.8265777826309204, 'rewards/rejected': -5.265676975250244, 'rewards/accuracies': 1.0, 'rewards/margins': 3.439099073410034, 'policy_logps/rejected': -334.93255615234375, 'policy_logps/chosen': -270.02069091796875, 'referece_logps/rejected': -282.2757873535156, 'referece_logps/chosen': -251.7549285888672, 'logits/rejected': -0.4275605380535126, 'logits/chosen': -0.42715486884117126, 'epoch': 8.79}

 98%|█████████▊| 23593/24156 [35:11:29<2:34:30, 16.47s/it]


 98%|█████████▊| 23595/24156 [35:12:07<2:42:58, 17.43s/it]

 98%|█████████▊| 23596/24156 [35:12:18<2:26:56, 15.74s/it]

 98%|█████████▊| 23597/24156 [35:12:37<2:34:23, 16.57s/it]

 98%|█████████▊| 23598/24156 [35:12:57<2:42:55, 17.52s/it]
{'loss': 0.371, 'learning_rate': 1.6046090825829613e-06, 'rewards/chosen': -2.0154521465301514, 'rewards/rejected': -3.595930576324463, 'rewards/accuracies': 0.875, 'rewards/margins': 1.580478310585022, 'policy_logps/rejected': -523.998779296875, 'policy_logps/chosen': -515.5333251953125, 'referece_logps/rejected': -488.03948974609375, 'referece_logps/chosen': -495.3787841796875, 'logits/rejected': -0.3617323040962219, 'logits/chosen': -0.4665360152721405, 'epoch': 8.79}


 98%|█████████▊| 23600/24156 [35:13:29<2:41:31, 17.43s/it]

 98%|█████████▊| 23601/24156 [35:13:45<2:35:19, 16.79s/it]
{'loss': 0.3484, 'learning_rate': 1.6042886441835594e-06, 'rewards/chosen': -1.221702218055725, 'rewards/rejected': -2.8062973022460938, 'rewards/accuracies': 1.0, 'rewards/margins': 1.584594964981079, 'policy_logps/rejected': -332.6678771972656, 'policy_logps/chosen': -340.1676940917969, 'referece_logps/rejected': -304.6048889160156, 'referece_logps/chosen': -327.95068359375, 'logits/rejected': 0.08512268960475922, 'logits/chosen': -0.0032833367586135864, 'epoch': 8.79}


 98%|█████████▊| 23603/24156 [35:14:07<2:07:05, 13.79s/it]

 98%|█████████▊| 23604/24156 [35:14:25<2:20:31, 15.28s/it]

 98%|█████████▊| 23605/24156 [35:14:45<2:32:15, 16.58s/it]
{'loss': 0.3834, 'learning_rate': 1.6038612409068572e-06, 'rewards/chosen': -2.0749588012695312, 'rewards/rejected': -3.3417434692382812, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2667847871780396, 'policy_logps/rejected': -466.243408203125, 'policy_logps/chosen': -366.9565124511719, 'referece_logps/rejected': -432.8260192871094, 'referece_logps/chosen': -346.2069091796875, 'logits/rejected': -0.7986340522766113, 'logits/chosen': -0.4399692416191101, 'epoch': 8.79}

 98%|█████████▊| 23606/24156 [35:14:58<2:22:28, 15.54s/it]


 98%|█████████▊| 23608/24156 [35:15:27<2:13:48, 14.65s/it]

 98%|█████████▊| 23609/24156 [35:15:47<2:27:55, 16.23s/it]

 98%|█████████▊| 23610/24156 [35:16:03<2:28:07, 16.28s/it]
{'loss': 0.2945, 'learning_rate': 1.6033267425735978e-06, 'rewards/chosen': -1.5131195783615112, 'rewards/rejected': -3.3092827796936035, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7961630821228027, 'policy_logps/rejected': -510.07525634765625, 'policy_logps/chosen': -482.7335205078125, 'referece_logps/rejected': -476.982421875, 'referece_logps/chosen': -467.6023254394531, 'logits/rejected': -0.7426103949546814, 'logits/chosen': -0.7648806571960449, 'epoch': 8.8}

 98%|█████████▊| 23611/24156 [35:16:22<2:33:47, 16.93s/it]

 98%|█████████▊| 23612/24156 [35:16:34<2:21:33, 15.61s/it]

 98%|█████████▊| 23613/24156 [35:16:46<2:10:37, 14.43s/it]

 98%|█████████▊| 23614/24156 [35:16:57<2:00:16, 13.32s/it]


 98%|█████████▊| 23616/24156 [35:17:35<2:25:22, 16.15s/it]
{'loss': 0.3653, 'learning_rate': 1.602684986675963e-06, 'rewards/chosen': -1.955077886581421, 'rewards/rejected': -4.057644367218018, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1025662422180176, 'policy_logps/rejected': -361.634521484375, 'policy_logps/chosen': -371.5691223144531, 'referece_logps/rejected': -321.0580749511719, 'referece_logps/chosen': -352.01837158203125, 'logits/rejected': -0.9351122379302979, 'logits/chosen': -1.0553494691848755, 'epoch': 8.8}

 98%|█████████▊| 23617/24156 [35:17:53<2:28:43, 16.56s/it]

 98%|█████████▊| 23618/24156 [35:18:08<2:25:14, 16.20s/it]


 98%|█████████▊| 23620/24156 [35:18:46<2:37:38, 17.65s/it]

 98%|█████████▊| 23621/24156 [35:19:02<2:32:47, 17.14s/it]
{'loss': 0.2702, 'learning_rate': 1.6021498921285104e-06, 'rewards/chosen': -2.643566370010376, 'rewards/rejected': -4.135834693908691, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4922683238983154, 'policy_logps/rejected': -253.9944305419922, 'policy_logps/chosen': -391.51611328125, 'referece_logps/rejected': -212.6361083984375, 'referece_logps/chosen': -365.0804443359375, 'logits/rejected': -0.5399539470672607, 'logits/chosen': -0.6892307996749878, 'epoch': 8.8}

 98%|█████████▊| 23622/24156 [35:19:12<2:15:20, 15.21s/it]


 98%|█████████▊| 23624/24156 [35:19:42<2:09:43, 14.63s/it]

 98%|█████████▊| 23625/24156 [35:20:04<2:29:22, 16.88s/it]

 98%|█████████▊| 23626/24156 [35:20:22<2:31:26, 17.14s/it]

 98%|█████████▊| 23627/24156 [35:20:37<2:27:27, 16.73s/it]
{'loss': 0.2835, 'learning_rate': 1.6015074214720245e-06, 'rewards/chosen': -2.080765724182129, 'rewards/rejected': -5.430582046508789, 'rewards/accuracies': 0.75, 'rewards/margins': 3.349815607070923, 'policy_logps/rejected': -409.55914306640625, 'policy_logps/chosen': -336.10382080078125, 'referece_logps/rejected': -355.2532958984375, 'referece_logps/chosen': -315.296142578125, 'logits/rejected': 0.09552913159132004, 'logits/chosen': 0.10283268988132477, 'epoch': 8.8}

 98%|█████████▊| 23628/24156 [35:20:57<2:33:46, 17.47s/it]

 98%|█████████▊| 23629/24156 [35:21:15<2:34:44, 17.62s/it]

 98%|█████████▊| 23630/24156 [35:21:34<2:40:06, 18.26s/it]

 98%|█████████▊| 23631/24156 [35:21:54<2:43:44, 18.71s/it]


 98%|█████████▊| 23633/24156 [35:22:25<2:30:55, 17.31s/it]

 98%|█████████▊| 23634/24156 [35:22:40<2:23:02, 16.44s/it]
{'loss': 0.4601, 'learning_rate': 1.600757380395931e-06, 'rewards/chosen': -2.584909439086914, 'rewards/rejected': -5.635679721832275, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0507709980010986, 'policy_logps/rejected': -462.5246276855469, 'policy_logps/chosen': -299.12176513671875, 'referece_logps/rejected': -406.1678466796875, 'referece_logps/chosen': -273.2726745605469, 'logits/rejected': -0.2535950839519501, 'logits/chosen': -0.09538229554891586, 'epoch': 8.81}

 98%|█████████▊| 23635/24156 [35:22:56<2:22:26, 16.40s/it]


 98%|█████████▊| 23637/24156 [35:23:22<2:05:29, 14.51s/it]

 98%|█████████▊| 23638/24156 [35:23:39<2:12:32, 15.35s/it]

 98%|█████████▊| 23639/24156 [35:23:50<1:59:41, 13.89s/it]

 98%|█████████▊| 23640/24156 [35:24:00<1:50:25, 12.84s/it]
{'loss': 0.3981, 'learning_rate': 1.6001140668263164e-06, 'rewards/chosen': -2.283327341079712, 'rewards/rejected': -4.864914894104004, 'rewards/accuracies': 0.625, 'rewards/margins': 2.581587791442871, 'policy_logps/rejected': -488.530029296875, 'policy_logps/chosen': -368.0235595703125, 'referece_logps/rejected': -439.8808898925781, 'referece_logps/chosen': -345.1902770996094, 'logits/rejected': -0.021510999649763107, 'logits/chosen': 0.3213266432285309, 'epoch': 8.81}


 98%|█████████▊| 23642/24156 [35:24:33<2:03:31, 14.42s/it]

 98%|█████████▊| 23643/24156 [35:24:53<2:17:18, 16.06s/it]
{'loss': 0.2664, 'learning_rate': 1.5997922643738088e-06, 'rewards/chosen': -2.9129230976104736, 'rewards/rejected': -7.185842514038086, 'rewards/accuracies': 0.875, 'rewards/margins': 4.272919654846191, 'policy_logps/rejected': -593.2944946289062, 'policy_logps/chosen': -507.1675720214844, 'referece_logps/rejected': -521.4360961914062, 'referece_logps/chosen': -478.0383605957031, 'logits/rejected': 0.6436622142791748, 'logits/chosen': 0.5700948238372803, 'epoch': 8.81}


 98%|█████████▊| 23645/24156 [35:25:30<2:27:24, 17.31s/it]

 98%|█████████▊| 23646/24156 [35:25:50<2:33:03, 18.01s/it]
{'loss': 0.2779, 'learning_rate': 1.5994703648789165e-06, 'rewards/chosen': -2.7047059535980225, 'rewards/rejected': -6.6062469482421875, 'rewards/accuracies': 1.0, 'rewards/margins': 3.901541233062744, 'policy_logps/rejected': -503.5163879394531, 'policy_logps/chosen': -377.0976257324219, 'referece_logps/rejected': -437.4539794921875, 'referece_logps/chosen': -350.05059814453125, 'logits/rejected': -1.073769450187683, 'logits/chosen': -0.8568375706672668, 'epoch': 8.81}

 98%|█████████▊| 23647/24156 [35:26:06<2:29:44, 17.65s/it]

 98%|█████████▊| 23648/24156 [35:26:23<2:26:14, 17.27s/it]

 98%|█████████▊| 23649/24156 [35:26:45<2:38:49, 18.80s/it]


 98%|█████████▊| 23651/24156 [35:27:24<2:41:22, 19.17s/it]
{'loss': 0.3189, 'learning_rate': 1.5989336502124805e-06, 'rewards/chosen': -3.5041818618774414, 'rewards/rejected': -4.561174392700195, 'rewards/accuracies': 0.75, 'rewards/margins': 1.056993007659912, 'policy_logps/rejected': -394.0714111328125, 'policy_logps/chosen': -354.4095458984375, 'referece_logps/rejected': -348.4596862792969, 'referece_logps/chosen': -319.36773681640625, 'logits/rejected': 0.21084657311439514, 'logits/chosen': 0.28169530630111694, 'epoch': 8.81}

 98%|█████████▊| 23652/24156 [35:27:39<2:30:09, 17.88s/it]

 98%|█████████▊| 23653/24156 [35:27:55<2:24:43, 17.26s/it]

 98%|█████████▊| 23654/24156 [35:28:15<2:30:44, 18.02s/it]

 98%|█████████▊| 23655/24156 [35:28:37<2:40:32, 19.23s/it]


 98%|█████████▊| 23657/24156 [35:29:02<2:10:37, 15.71s/it]
{'loss': 0.2833, 'learning_rate': 1.5982892373212885e-06, 'rewards/chosen': -1.9825551509857178, 'rewards/rejected': -4.08524751663208, 'rewards/accuracies': 0.875, 'rewards/margins': 2.102692127227783, 'policy_logps/rejected': -300.06951904296875, 'policy_logps/chosen': -360.23870849609375, 'referece_logps/rejected': -259.21697998046875, 'referece_logps/chosen': -340.41314697265625, 'logits/rejected': -0.053809911012649536, 'logits/chosen': -0.05801946669816971, 'epoch': 8.81}


 98%|█████████▊| 23659/24156 [35:29:36<2:18:16, 16.69s/it]

 98%|█████████▊| 23660/24156 [35:29:52<2:15:58, 16.45s/it]

 98%|█████████▊| 23661/24156 [35:30:10<2:19:36, 16.92s/it]

 98%|█████████▊| 23662/24156 [35:30:30<2:26:02, 17.74s/it]

 98%|█████████▊| 23663/24156 [35:30:50<2:31:01, 18.38s/it]

 98%|█████████▊| 23664/24156 [35:31:12<2:40:04, 19.52s/it]

 98%|█████████▊| 23665/24156 [35:31:32<2:41:36, 19.75s/it]
{'loss': 0.21, 'learning_rate': 1.5974294178990105e-06, 'rewards/chosen': -2.702498197555542, 'rewards/rejected': -5.1070451736450195, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4045467376708984, 'policy_logps/rejected': -497.1383972167969, 'policy_logps/chosen': -440.27154541015625, 'referece_logps/rejected': -446.0679931640625, 'referece_logps/chosen': -413.2464904785156, 'logits/rejected': -0.23201343417167664, 'logits/chosen': -0.23168250918388367, 'epoch': 8.82}


 98%|█████████▊| 23667/24156 [35:32:12<2:41:32, 19.82s/it]
{'loss': 0.2647, 'learning_rate': 1.5972143556046875e-06, 'rewards/chosen': -2.7280683517456055, 'rewards/rejected': -4.607618808746338, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8795506954193115, 'policy_logps/rejected': -399.9846496582031, 'policy_logps/chosen': -338.0379333496094, 'referece_logps/rejected': -353.908447265625, 'referece_logps/chosen': -310.75726318359375, 'logits/rejected': -0.5899348258972168, 'logits/chosen': -0.5275195837020874, 'epoch': 8.82}


 98%|█████████▊| 23669/24156 [35:32:46<2:31:17, 18.64s/it]
{'loss': 0.3123, 'learning_rate': 1.596999250365788e-06, 'rewards/chosen': -2.605087995529175, 'rewards/rejected': -3.3049769401550293, 'rewards/accuracies': 0.75, 'rewards/margins': 0.6998889446258545, 'policy_logps/rejected': -361.49688720703125, 'policy_logps/chosen': -379.5961608886719, 'referece_logps/rejected': -328.4471130371094, 'referece_logps/chosen': -353.5452880859375, 'logits/rejected': 0.10269103944301605, 'logits/chosen': -0.014899447560310364, 'epoch': 8.82}


 98%|█████████▊| 23671/24156 [35:33:27<2:37:03, 19.43s/it]
{'loss': 0.2879, 'learning_rate': 1.59678410219778e-06, 'rewards/chosen': -2.30206298828125, 'rewards/rejected': -4.144347190856934, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8422843217849731, 'policy_logps/rejected': -391.4917907714844, 'policy_logps/chosen': -327.8027648925781, 'referece_logps/rejected': -350.04833984375, 'referece_logps/chosen': -304.7821350097656, 'logits/rejected': -0.25935637950897217, 'logits/chosen': -0.2624596953392029, 'epoch': 8.82}


 98%|█████████▊| 23673/24156 [35:34:00<2:23:48, 17.87s/it]

 98%|█████████▊| 23674/24156 [35:34:13<2:09:37, 16.14s/it]
{'loss': 0.3513, 'learning_rate': 1.5964612994875322e-06, 'rewards/chosen': -2.576298236846924, 'rewards/rejected': -4.715758800506592, 'rewards/accuracies': 0.875, 'rewards/margins': 2.139460563659668, 'policy_logps/rejected': -455.29058837890625, 'policy_logps/chosen': -418.32568359375, 'referece_logps/rejected': -408.13299560546875, 'referece_logps/chosen': -392.5627136230469, 'logits/rejected': -0.3439697325229645, 'logits/chosen': -0.46200597286224365, 'epoch': 8.82}

 98%|█████████▊| 23675/24156 [35:34:33<2:19:41, 17.43s/it]


 98%|█████████▊| 23677/24156 [35:35:04<2:13:26, 16.72s/it]

 98%|█████████▊| 23678/24156 [35:35:27<2:26:02, 18.33s/it]
{'loss': 0.3014, 'learning_rate': 1.5960307457664088e-06, 'rewards/chosen': -2.356837272644043, 'rewards/rejected': -4.680046081542969, 'rewards/accuracies': 0.875, 'rewards/margins': 2.323209285736084, 'policy_logps/rejected': -434.7898254394531, 'policy_logps/chosen': -445.15191650390625, 'referece_logps/rejected': -387.9893493652344, 'referece_logps/chosen': -421.58355712890625, 'logits/rejected': 0.21931874752044678, 'logits/chosen': 0.3166458308696747, 'epoch': 8.82}

 98%|█████████▊| 23679/24156 [35:35:47<2:30:46, 18.97s/it]


 98%|█████████▊| 23681/24156 [35:36:16<2:13:36, 16.88s/it]
{'loss': 0.3535, 'learning_rate': 1.5957077179626976e-06, 'rewards/chosen': -2.2153875827789307, 'rewards/rejected': -4.031069278717041, 'rewards/accuracies': 0.875, 'rewards/margins': 1.815682053565979, 'policy_logps/rejected': -451.5040283203125, 'policy_logps/chosen': -395.67608642578125, 'referece_logps/rejected': -411.1932678222656, 'referece_logps/chosen': -373.522216796875, 'logits/rejected': 0.40142595767974854, 'logits/chosen': 0.3158438801765442, 'epoch': 8.82}


 98%|█████████▊| 23683/24156 [35:36:48<2:07:25, 16.16s/it]
{'loss': 0.3833, 'learning_rate': 1.5954923125450342e-06, 'rewards/chosen': -2.159975290298462, 'rewards/rejected': -6.751692771911621, 'rewards/accuracies': 0.875, 'rewards/margins': 4.591716766357422, 'policy_logps/rejected': -384.76788330078125, 'policy_logps/chosen': -427.6015930175781, 'referece_logps/rejected': -317.2509460449219, 'referece_logps/chosen': -406.0018310546875, 'logits/rejected': 0.030197948217391968, 'logits/chosen': -0.07957419008016586, 'epoch': 8.82}

 98%|█████████▊| 23684/24156 [35:37:02<2:00:48, 15.36s/it]

 98%|█████████▊| 23685/24156 [35:37:18<2:03:06, 15.68s/it]

 98%|█████████▊| 23686/24156 [35:37:38<2:11:53, 16.84s/it]

 98%|█████████▊| 23687/24156 [35:37:50<2:00:33, 15.42s/it]

 98%|█████████▊| 23688/24156 [35:38:10<2:10:30, 16.73s/it]

 98%|█████████▊| 23689/24156 [35:38:29<2:17:00, 17.60s/it]

 98%|█████████▊| 23690/24156 [35:38:47<2:18:19, 17.81s/it]


 98%|█████████▊| 23692/24156 [35:39:18<2:06:38, 16.38s/it]
{'loss': 0.3758, 'learning_rate': 1.594522458482497e-06, 'rewards/chosen': -2.405244827270508, 'rewards/rejected': -4.978981018066406, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5737364292144775, 'policy_logps/rejected': -303.3694152832031, 'policy_logps/chosen': -528.2130737304688, 'referece_logps/rejected': -253.57960510253906, 'referece_logps/chosen': -504.16064453125, 'logits/rejected': -0.9203409552574158, 'logits/chosen': -1.1560125350952148, 'epoch': 8.83}

 98%|█████████▊| 23693/24156 [35:39:38<2:14:12, 17.39s/it]

 98%|█████████▊| 23694/24156 [35:39:54<2:09:53, 16.87s/it]

 98%|█████████▊| 23695/24156 [35:40:14<2:16:55, 17.82s/it]

 98%|█████████▊| 23696/24156 [35:40:27<2:06:22, 16.48s/it]

 98%|█████████▊| 23697/24156 [35:40:47<2:14:25, 17.57s/it]

 98%|█████████▊| 23698/24156 [35:41:01<2:04:31, 16.31s/it]

 98%|█████████▊| 23699/24156 [35:41:20<2:11:58, 17.33s/it]

 98%|█████████▊| 23700/24156 [35:41:32<1:57:36, 15.47s/it]

 98%|█████████▊| 23701/24156 [35:41:50<2:03:52, 16.34s/it]

 98%|█████████▊| 23702/24156 [35:42:09<2:10:52, 17.30s/it]

 98%|█████████▊| 23703/24156 [35:42:27<2:10:45, 17.32s/it]

 98%|█████████▊| 23704/24156 [35:42:38<1:56:30, 15.47s/it]

 98%|█████████▊| 23705/24156 [35:42:50<1:48:06, 14.38s/it]

 98%|█████████▊| 23706/24156 [35:43:10<2:00:05, 16.01s/it]

 98%|█████████▊| 23707/24156 [35:43:21<1:48:34, 14.51s/it]

 98%|█████████▊| 23708/24156 [35:43:40<2:00:06, 16.09s/it]

 98%|█████████▊| 23709/24156 [35:44:01<2:08:57, 17.31s/it]

 98%|█████████▊| 23710/24156 [35:44:18<2:08:17, 17.26s/it]

 98%|█████████▊| 23711/24156 [35:44:32<2:01:40, 16.41s/it]


 98%|█████████▊| 23713/24156 [35:45:11<2:13:26, 18.07s/it]

 98%|█████████▊| 23714/24156 [35:45:31<2:17:19, 18.64s/it]
{'loss': 0.2589, 'learning_rate': 1.5921480624498863e-06, 'rewards/chosen': -2.1003100872039795, 'rewards/rejected': -4.9082441329956055, 'rewards/accuracies': 0.875, 'rewards/margins': 2.807933807373047, 'policy_logps/rejected': -377.7891845703125, 'policy_logps/chosen': -334.5968322753906, 'referece_logps/rejected': -328.70672607421875, 'referece_logps/chosen': -313.59368896484375, 'logits/rejected': 0.1781744509935379, 'logits/chosen': 0.3216789960861206, 'epoch': 8.84}

 98%|█████████▊| 23715/24156 [35:45:51<2:18:42, 18.87s/it]


 98%|█████████▊| 23717/24156 [35:46:19<2:02:40, 16.77s/it]
{'loss': 0.307, 'learning_rate': 1.591823881521645e-06, 'rewards/chosen': -2.657663345336914, 'rewards/rejected': -4.267001628875732, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6093381643295288, 'policy_logps/rejected': -454.45989990234375, 'policy_logps/chosen': -430.77850341796875, 'referece_logps/rejected': -411.7898254394531, 'referece_logps/chosen': -404.20184326171875, 'logits/rejected': -0.9720019102096558, 'logits/chosen': -1.0234622955322266, 'epoch': 8.84}

 98%|█████████▊| 23718/24156 [35:46:40<2:10:33, 17.89s/it]


 98%|█████████▊| 23720/24156 [35:47:05<1:50:33, 15.21s/it]

 98%|█████████▊| 23721/24156 [35:47:21<1:52:05, 15.46s/it]

 98%|█████████▊| 23722/24156 [35:47:37<1:53:00, 15.62s/it]
{'loss': 0.3615, 'learning_rate': 1.5912833672156987e-06, 'rewards/chosen': -1.7937304973602295, 'rewards/rejected': -4.18762731552124, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3938968181610107, 'policy_logps/rejected': -454.1291198730469, 'policy_logps/chosen': -624.8690795898438, 'referece_logps/rejected': -412.25286865234375, 'referece_logps/chosen': -606.9317626953125, 'logits/rejected': 0.2780037522315979, 'logits/chosen': 0.3880188763141632, 'epoch': 8.84}

 98%|█████████▊| 23723/24156 [35:47:50<1:46:24, 14.74s/it]

 98%|█████████▊| 23724/24156 [35:48:01<1:37:30, 13.54s/it]

 98%|█████████▊| 23725/24156 [35:48:18<1:45:14, 14.65s/it]

 98%|█████████▊| 23726/24156 [35:48:37<1:54:56, 16.04s/it]

 98%|█████████▊| 23727/24156 [35:48:58<2:05:02, 17.49s/it]

 98%|█████████▊| 23728/24156 [35:49:18<2:09:17, 18.13s/it]

 98%|█████████▊| 23729/24156 [35:49:38<2:13:12, 18.72s/it]

 98%|█████████▊| 23730/24156 [35:49:57<2:12:50, 18.71s/it]


 98%|█████████▊| 23732/24156 [35:50:29<2:02:03, 17.27s/it]
{'loss': 0.2954, 'learning_rate': 1.590201541632681e-06, 'rewards/chosen': -2.3052783012390137, 'rewards/rejected': -6.285001277923584, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9797234535217285, 'policy_logps/rejected': -407.43902587890625, 'policy_logps/chosen': -487.6540222167969, 'referece_logps/rejected': -344.58905029296875, 'referece_logps/chosen': -464.6012268066406, 'logits/rejected': 0.06346175074577332, 'logits/chosen': 0.12508705258369446, 'epoch': 8.84}


 98%|█████████▊| 23734/24156 [35:51:02<1:55:05, 16.36s/it]
{'loss': 0.2502, 'learning_rate': 1.589985049132934e-06, 'rewards/chosen': -2.153716802597046, 'rewards/rejected': -4.253921985626221, 'rewards/accuracies': 0.875, 'rewards/margins': 2.100205421447754, 'policy_logps/rejected': -314.8973693847656, 'policy_logps/chosen': -312.0606689453125, 'referece_logps/rejected': -272.3581237792969, 'referece_logps/chosen': -290.5235290527344, 'logits/rejected': -0.6199782490730286, 'logits/chosen': -0.622866153717041, 'epoch': 8.84}

 98%|█████████▊| 23735/24156 [35:51:20<1:58:39, 16.91s/it]

 98%|█████████▊| 23736/24156 [35:51:38<2:01:53, 17.41s/it]

 98%|█████████▊| 23737/24156 [35:51:54<1:57:50, 16.88s/it]

 98%|█████████▊| 23738/24156 [35:52:07<1:50:13, 15.82s/it]

 98%|█████████▊| 23739/24156 [35:52:27<1:57:26, 16.90s/it]

 98%|█████████▊| 23740/24156 [35:52:40<1:49:24, 15.78s/it]


 98%|█████████▊| 23742/24156 [35:53:18<2:00:07, 17.41s/it]

 98%|█████████▊| 23743/24156 [35:53:38<2:05:23, 18.22s/it]
{'loss': 0.362, 'learning_rate': 1.5890103081028618e-06, 'rewards/chosen': -2.846158504486084, 'rewards/rejected': -5.253311634063721, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4071531295776367, 'policy_logps/rejected': -492.073486328125, 'policy_logps/chosen': -494.19439697265625, 'referece_logps/rejected': -439.54034423828125, 'referece_logps/chosen': -465.7327575683594, 'logits/rejected': -0.050853900611400604, 'logits/chosen': -0.14046315848827362, 'epoch': 8.85}

 98%|█████████▊| 23744/24156 [35:53:55<2:02:03, 17.77s/it]

 98%|█████████▊| 23745/24156 [35:54:13<2:02:18, 17.86s/it]

 98%|█████████▊| 23746/24156 [35:54:29<1:59:29, 17.49s/it]


 98%|█████████▊| 23748/24156 [35:55:04<1:58:31, 17.43s/it]
{'loss': 0.4061, 'learning_rate': 1.5884684146144207e-06, 'rewards/chosen': -2.1395184993743896, 'rewards/rejected': -5.523497581481934, 'rewards/accuracies': 0.875, 'rewards/margins': 3.383979320526123, 'policy_logps/rejected': -320.44256591796875, 'policy_logps/chosen': -432.7031555175781, 'referece_logps/rejected': -265.2076110839844, 'referece_logps/chosen': -411.3079833984375, 'logits/rejected': -0.3710046708583832, 'logits/chosen': -0.4972571134567261, 'epoch': 8.85}

 98%|█████████▊| 23749/24156 [35:55:23<2:02:03, 17.99s/it]


 98%|█████████▊| 23751/24156 [35:56:03<2:06:54, 18.80s/it]
{'loss': 0.404, 'learning_rate': 1.588143151558752e-06, 'rewards/chosen': -2.243623971939087, 'rewards/rejected': -4.760523796081543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.516899585723877, 'policy_logps/rejected': -437.6667175292969, 'policy_logps/chosen': -381.83984375, 'referece_logps/rejected': -390.06146240234375, 'referece_logps/chosen': -359.403564453125, 'logits/rejected': -0.3679415285587311, 'logits/chosen': -0.47679346799850464, 'epoch': 8.85}

 98%|█████████▊| 23752/24156 [35:56:19<2:02:41, 18.22s/it]

 98%|█████████▊| 23753/24156 [35:56:39<2:05:11, 18.64s/it]

 98%|█████████▊| 23754/24156 [35:56:59<2:08:01, 19.11s/it]

 98%|█████████▊| 23755/24156 [35:57:19<2:08:37, 19.25s/it]

 98%|█████████▊| 23756/24156 [35:57:36<2:03:19, 18.50s/it]

 98%|█████████▊| 23757/24156 [35:57:57<2:09:49, 19.52s/it]

 98%|█████████▊| 23758/24156 [35:58:18<2:11:04, 19.76s/it]


 98%|█████████▊| 23760/24156 [35:58:56<2:07:14, 19.28s/it]
{'loss': 0.273, 'learning_rate': 1.587166791656507e-06, 'rewards/chosen': -2.1352529525756836, 'rewards/rejected': -4.241122722625732, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1058695316314697, 'policy_logps/rejected': -340.6887512207031, 'policy_logps/chosen': -395.63958740234375, 'referece_logps/rejected': -298.2775573730469, 'referece_logps/chosen': -374.28704833984375, 'logits/rejected': -0.5557631254196167, 'logits/chosen': -0.6664773225784302, 'epoch': 8.85}

 98%|█████████▊| 23761/24156 [35:59:10<1:55:14, 17.51s/it]

 98%|█████████▊| 23762/24156 [35:59:22<1:43:45, 15.80s/it]


 98%|█████████▊| 23764/24156 [36:00:02<1:57:31, 17.99s/it]
{'loss': 0.2281, 'learning_rate': 1.5867325793941658e-06, 'rewards/chosen': -2.852081060409546, 'rewards/rejected': -5.6981611251831055, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8460803031921387, 'policy_logps/rejected': -406.50360107421875, 'policy_logps/chosen': -464.38067626953125, 'referece_logps/rejected': -349.52197265625, 'referece_logps/chosen': -435.8598327636719, 'logits/rejected': -0.8001325726509094, 'logits/chosen': -0.7495465278625488, 'epoch': 8.85}

 98%|█████████▊| 23765/24156 [36:00:21<1:59:32, 18.34s/it]


 98%|█████████▊| 23767/24156 [36:00:57<1:55:16, 17.78s/it]

 98%|█████████▊| 23768/24156 [36:01:16<1:58:41, 18.35s/it]
{'loss': 0.3471, 'learning_rate': 1.5862981983684223e-06, 'rewards/chosen': -2.0492637157440186, 'rewards/rejected': -5.717205047607422, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6679415702819824, 'policy_logps/rejected': -352.654296875, 'policy_logps/chosen': -416.16046142578125, 'referece_logps/rejected': -295.48223876953125, 'referece_logps/chosen': -395.6678466796875, 'logits/rejected': -0.23069676756858826, 'logits/chosen': -0.29574495553970337, 'epoch': 8.86}

 98%|█████████▊| 23769/24156 [36:01:36<2:01:04, 18.77s/it]

 98%|█████████▊| 23770/24156 [36:01:54<1:59:06, 18.51s/it]

 98%|█████████▊| 23771/24156 [36:02:09<1:51:27, 17.37s/it]


 98%|█████████▊| 23773/24156 [36:02:43<1:48:37, 17.02s/it]
{'loss': 0.3341, 'learning_rate': 1.5857549849530534e-06, 'rewards/chosen': -2.5948424339294434, 'rewards/rejected': -4.618605613708496, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0237631797790527, 'policy_logps/rejected': -370.5960388183594, 'policy_logps/chosen': -327.57464599609375, 'referece_logps/rejected': -324.4100036621094, 'referece_logps/chosen': -301.626220703125, 'logits/rejected': 0.42062118649482727, 'logits/chosen': 0.19919613003730774, 'epoch': 8.86}

 98%|█████████▊| 23774/24156 [36:02:57<1:43:28, 16.25s/it]

 98%|█████████▊| 23775/24156 [36:03:10<1:36:08, 15.14s/it]

 98%|█████████▊| 23776/24156 [36:03:29<1:44:38, 16.52s/it]

 98%|█████████▊| 23777/24156 [36:03:49<1:49:54, 17.40s/it]

 98%|█████████▊| 23778/24156 [36:04:01<1:39:26, 15.78s/it]

 98%|█████████▊| 23779/24156 [36:04:18<1:42:26, 16.30s/it]

 98%|█████████▊| 23780/24156 [36:04:29<1:31:36, 14.62s/it]

 98%|█████████▊| 23781/24156 [36:04:42<1:27:32, 14.01s/it]

 98%|█████████▊| 23782/24156 [36:04:52<1:20:55, 12.98s/it]

 98%|█████████▊| 23783/24156 [36:05:03<1:16:16, 12.27s/it]

 98%|█████████▊| 23784/24156 [36:05:17<1:19:40, 12.85s/it]

 98%|█████████▊| 23785/24156 [36:05:30<1:20:30, 13.02s/it]

 98%|█████████▊| 23786/24156 [36:05:41<1:16:45, 12.45s/it]

 98%|█████████▊| 23787/24156 [36:05:58<1:24:52, 13.80s/it]

 98%|█████████▊| 23788/24156 [36:06:18<1:35:22, 15.55s/it]

 98%|█████████▊| 23789/24156 [36:06:34<1:36:16, 15.74s/it]

 98%|█████████▊| 23790/24156 [36:06:52<1:39:36, 16.33s/it]

 98%|█████████▊| 23791/24156 [36:07:08<1:37:58, 16.11s/it]

 98%|█████████▊| 23792/24156 [36:07:27<1:44:22, 17.20s/it]

 98%|█████████▊| 23793/24156 [36:07:44<1:43:06, 17.04s/it]

 99%|█████████▊| 23794/24156 [36:08:04<1:48:18, 17.95s/it]

 99%|█████████▊| 23795/24156 [36:08:19<1:43:21, 17.18s/it]

 99%|█████████▊| 23796/24156 [36:08:30<1:31:07, 15.19s/it]

 99%|█████████▊| 23797/24156 [36:08:50<1:38:44, 16.50s/it]

 99%|█████████▊| 23798/24156 [36:09:02<1:31:26, 15.33s/it]

 99%|█████████▊| 23799/24156 [36:09:14<1:25:24, 14.35s/it]

 99%|█████████▊| 23800/24156 [36:09:33<1:32:25, 15.58s/it]

 99%|█████████▊| 23801/24156 [36:09:52<1:38:16, 16.61s/it]

 99%|█████████▊| 23802/24156 [36:10:04<1:30:54, 15.41s/it]

 99%|█████████▊| 23803/24156 [36:10:24<1:37:35, 16.59s/it]

 99%|█████████▊| 23804/24156 [36:10:40<1:36:36, 16.47s/it]

 99%|█████████▊| 23805/24156 [36:10:52<1:28:11, 15.08s/it]

 99%|█████████▊| 23806/24156 [36:11:06<1:26:11, 14.77s/it]

 99%|█████████▊| 23807/24156 [36:11:24<1:32:40, 15.93s/it]

 99%|█████████▊| 23808/24156 [36:11:41<1:33:31, 16.12s/it]

 99%|█████████▊| 23809/24156 [36:11:56<1:32:11, 15.94s/it]

 99%|█████████▊| 23810/24156 [36:12:10<1:28:16, 15.31s/it]

 99%|█████████▊| 23811/24156 [36:12:29<1:33:09, 16.20s/it]

 99%|█████████▊| 23812/24156 [36:12:48<1:38:47, 17.23s/it]

 99%|█████████▊| 23813/24156 [36:13:04<1:36:01, 16.80s/it]

 99%|█████████▊| 23814/24156 [36:13:15<1:25:18, 14.97s/it]

 99%|█████████▊| 23815/24156 [36:13:27<1:20:59, 14.25s/it]

 99%|█████████▊| 23816/24156 [36:13:47<1:29:46, 15.84s/it]

 99%|█████████▊| 23817/24156 [36:14:01<1:26:01, 15.22s/it]

 99%|█████████▊| 23818/24156 [36:14:19<1:31:24, 16.23s/it]

 99%|█████████▊| 23819/24156 [36:14:32<1:25:35, 15.24s/it]

 99%|█████████▊| 23820/24156 [36:14:49<1:27:33, 15.64s/it]

 99%|█████████▊| 23821/24156 [36:14:59<1:19:07, 14.17s/it]

 99%|█████████▊| 23822/24156 [36:15:18<1:25:28, 15.35s/it]

 99%|█████████▊| 23823/24156 [36:15:33<1:25:09, 15.34s/it]

 99%|█████████▊| 23824/24156 [36:15:44<1:18:11, 14.13s/it]

 99%|█████████▊| 23825/24156 [36:16:00<1:20:17, 14.55s/it]

 99%|█████████▊| 23826/24156 [36:16:16<1:22:09, 14.94s/it]

 99%|█████████▊| 23827/24156 [36:16:35<1:29:37, 16.35s/it]

 99%|█████████▊| 23828/24156 [36:16:51<1:28:52, 16.26s/it]

 99%|█████████▊| 23829/24156 [36:17:09<1:31:52, 16.86s/it]

 99%|█████████▊| 23830/24156 [36:17:29<1:36:21, 17.73s/it]

 99%|█████████▊| 23831/24156 [36:17:43<1:29:46, 16.58s/it]

 99%|█████████▊| 23832/24156 [36:17:56<1:22:46, 15.33s/it]

 99%|█████████▊| 23833/24156 [36:18:14<1:27:54, 16.33s/it]

 99%|█████████▊| 23834/24156 [36:18:30<1:26:20, 16.09s/it]


 99%|█████████▊| 23836/24156 [36:19:02<1:26:23, 16.20s/it]
{'loss': 0.2721, 'learning_rate': 1.5788880213866709e-06, 'rewards/chosen': -2.1215779781341553, 'rewards/rejected': -5.415380954742432, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2938032150268555, 'policy_logps/rejected': -365.5874938964844, 'policy_logps/chosen': -299.3666076660156, 'referece_logps/rejected': -311.4336853027344, 'referece_logps/chosen': -278.15081787109375, 'logits/rejected': -0.44955089688301086, 'logits/chosen': -0.34489378333091736, 'epoch': 8.88}

 99%|█████████▊| 23837/24156 [36:19:18<1:25:19, 16.05s/it]


 99%|█████████▊| 23839/24156 [36:19:56<1:31:40, 17.35s/it]
{'loss': 0.363, 'learning_rate': 1.578559988973558e-06, 'rewards/chosen': -2.068202495574951, 'rewards/rejected': -3.0557055473327637, 'rewards/accuracies': 0.75, 'rewards/margins': 0.987502932548523, 'policy_logps/rejected': -549.697509765625, 'policy_logps/chosen': -435.896484375, 'referece_logps/rejected': -519.1405029296875, 'referece_logps/chosen': -415.2144470214844, 'logits/rejected': -0.01922467350959778, 'logits/chosen': 0.11200916767120361, 'epoch': 8.88}

 99%|█████████▊| 23840/24156 [36:20:08<1:23:18, 15.82s/it]


 99%|█████████▊| 23842/24156 [36:20:38<1:18:48, 15.06s/it]
{'loss': 0.349, 'learning_rate': 1.578231862953301e-06, 'rewards/chosen': -2.3342747688293457, 'rewards/rejected': -4.346241474151611, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0119669437408447, 'policy_logps/rejected': -386.8127746582031, 'policy_logps/chosen': -454.04559326171875, 'referece_logps/rejected': -343.3503723144531, 'referece_logps/chosen': -430.702880859375, 'logits/rejected': -0.5898848176002502, 'logits/chosen': -0.5788868069648743, 'epoch': 8.88}

 99%|█████████▊| 23843/24156 [36:20:57<1:24:04, 16.12s/it]

 99%|█████████▊| 23844/24156 [36:21:11<1:21:40, 15.71s/it]

 99%|█████████▊| 23845/24156 [36:21:31<1:26:48, 16.75s/it]

 99%|█████████▊| 23846/24156 [36:21:50<1:30:09, 17.45s/it]

 99%|█████████▊| 23847/24156 [36:22:07<1:29:51, 17.45s/it]

 99%|█████████▊| 23848/24156 [36:22:19<1:20:31, 15.69s/it]

 99%|█████████▊| 23849/24156 [36:22:29<1:12:44, 14.22s/it]

 99%|█████████▊| 23850/24156 [36:22:41<1:09:12, 13.57s/it]

 99%|█████████▊| 23851/24156 [36:22:58<1:13:26, 14.45s/it]

 99%|█████████▊| 23852/24156 [36:23:17<1:20:05, 15.81s/it]

 99%|█████████▊| 23853/24156 [36:23:30<1:15:00, 14.85s/it]

 99%|█████████▊| 23854/24156 [36:23:42<1:10:37, 14.03s/it]

 99%|█████████▉| 23855/24156 [36:23:53<1:06:06, 13.18s/it]

 99%|█████████▉| 23856/24156 [36:24:14<1:17:16, 15.45s/it]

 99%|█████████▉| 23857/24156 [36:24:31<1:20:19, 16.12s/it]


 99%|█████████▉| 23859/24156 [36:25:10<1:27:34, 17.69s/it]

 99%|█████████▉| 23860/24156 [36:25:30<1:30:42, 18.39s/it]

 99%|█████████▉| 23861/24156 [36:25:50<1:33:25, 19.00s/it]

 99%|█████████▉| 23862/24156 [36:26:09<1:32:46, 18.93s/it]

 99%|█████████▉| 23863/24156 [36:26:24<1:26:26, 17.70s/it]

 99%|█████████▉| 23864/24156 [36:26:39<1:23:01, 17.06s/it]

 99%|█████████▉| 23865/24156 [36:26:57<1:23:16, 17.17s/it]

 99%|█████████▉| 23866/24156 [36:27:14<1:22:25, 17.05s/it]

 99%|█████████▉| 23867/24156 [36:27:32<1:24:45, 17.60s/it]

 99%|█████████▉| 23868/24156 [36:27:44<1:15:45, 15.78s/it]

 99%|█████████▉| 23869/24156 [36:28:01<1:17:40, 16.24s/it]

 99%|█████████▉| 23870/24156 [36:28:16<1:15:19, 15.80s/it]

 99%|█████████▉| 23871/24156 [36:28:30<1:12:38, 15.29s/it]

 99%|█████████▉| 23872/24156 [36:28:47<1:14:17, 15.69s/it]

 99%|█████████▉| 23873/24156 [36:28:59<1:08:35, 14.54s/it]

 99%|█████████▉| 23874/24156 [36:29:15<1:11:02, 15.12s/it]

 99%|█████████▉| 23875/24156 [36:29:30<1:10:10, 14.98s/it]

 99%|█████████▉| 23876/24156 [36:29:46<1:11:18, 15.28s/it]

 99%|█████████▉| 23877/24156 [36:30:05<1:17:10, 16.60s/it]

 99%|█████████▉| 23878/24156 [36:30:25<1:21:11, 17.52s/it]

 99%|█████████▉| 23879/24156 [36:30:47<1:26:55, 18.83s/it]

 99%|█████████▉| 23880/24156 [36:30:58<1:16:05, 16.54s/it]

 99%|█████████▉| 23881/24156 [36:31:12<1:11:43, 15.65s/it]

 99%|█████████▉| 23882/24156 [36:31:28<1:11:43, 15.70s/it]

 99%|█████████▉| 23883/24156 [36:31:45<1:13:30, 16.16s/it]

 99%|█████████▉| 23884/24156 [36:32:03<1:15:31, 16.66s/it]

 99%|█████████▉| 23885/24156 [36:32:19<1:15:29, 16.71s/it]

 99%|█████████▉| 23886/24156 [36:32:34<1:12:14, 16.05s/it]

 99%|█████████▉| 23887/24156 [36:32:52<1:15:07, 16.76s/it]

 99%|█████████▉| 23888/24156 [36:33:11<1:17:53, 17.44s/it]

 99%|█████████▉| 23889/24156 [36:33:22<1:08:29, 15.39s/it]

 99%|█████████▉| 23890/24156 [36:33:36<1:06:31, 15.01s/it]

 99%|█████████▉| 23891/24156 [36:33:56<1:12:10, 16.34s/it]

 99%|█████████▉| 23892/24156 [36:34:07<1:05:46, 14.95s/it]

 99%|█████████▉| 23893/24156 [36:34:18<59:58, 13.68s/it]

 99%|█████████▉| 23894/24156 [36:34:30<58:01, 13.29s/it]

 99%|█████████▉| 23895/24156 [36:34:41<54:22, 12.50s/it]

 99%|█████████▉| 23896/24156 [36:34:52<51:50, 11.96s/it]

 99%|█████████▉| 23897/24156 [36:35:02<49:52, 11.56s/it]

 99%|█████████▉| 23898/24156 [36:35:13<48:38, 11.31s/it]

 99%|█████████▉| 23899/24156 [36:35:24<47:39, 11.13s/it]

 99%|█████████▉| 23900/24156 [36:35:34<46:48, 10.97s/it]

 99%|█████████▉| 23901/24156 [36:35:47<49:21, 11.61s/it]

 99%|█████████▉| 23902/24156 [36:35:59<49:08, 11.61s/it]

 99%|█████████▉| 23903/24156 [36:36:11<49:52, 11.83s/it]

 99%|█████████▉| 23904/24156 [36:36:24<50:13, 11.96s/it]

 99%|█████████▉| 23905/24156 [36:36:43<59:38, 14.26s/it]

 99%|█████████▉| 23906/24156 [36:37:01<1:03:17, 15.19s/it]

 99%|█████████▉| 23907/24156 [36:37:13<1:00:00, 14.46s/it]

 99%|█████████▉| 23908/24156 [36:37:33<1:06:04, 15.99s/it]

 99%|█████████▉| 23909/24156 [36:37:46<1:02:11, 15.11s/it]

 99%|█████████▉| 23910/24156 [36:38:06<1:07:38, 16.50s/it]

 99%|█████████▉| 23911/24156 [36:38:23<1:08:35, 16.80s/it]

 99%|█████████▉| 23912/24156 [36:38:45<1:13:57, 18.19s/it]

 99%|█████████▉| 23913/24156 [36:39:00<1:10:26, 17.39s/it]

 99%|█████████▉| 23914/24156 [36:39:19<1:11:15, 17.67s/it]

 99%|█████████▉| 23915/24156 [36:39:32<1:05:30, 16.31s/it]

 99%|█████████▉| 23916/24156 [36:39:47<1:03:47, 15.95s/it]

 99%|█████████▉| 23917/24156 [36:39:58<57:25, 14.42s/it]

 99%|█████████▉| 23918/24156 [36:40:17<1:03:22, 15.98s/it]

 99%|█████████▉| 23919/24156 [36:40:33<1:03:02, 15.96s/it]

 99%|█████████▉| 23920/24156 [36:40:49<1:03:08, 16.05s/it]

 99%|█████████▉| 23921/24156 [36:41:02<58:10, 14.85s/it]

 99%|█████████▉| 23922/24156 [36:41:18<1:00:08, 15.42s/it]

 99%|█████████▉| 23923/24156 [36:41:33<59:35, 15.35s/it]

 99%|█████████▉| 23924/24156 [36:41:45<54:26, 14.08s/it]

 99%|█████████▉| 23925/24156 [36:41:55<50:13, 13.05s/it]

 99%|█████████▉| 23926/24156 [36:42:13<55:17, 14.43s/it]

 99%|█████████▉| 23927/24156 [36:42:24<50:47, 13.31s/it]

 99%|█████████▉| 23928/24156 [36:42:43<57:54, 15.24s/it]

 99%|█████████▉| 23929/24156 [36:42:54<52:27, 13.86s/it]

 99%|█████████▉| 23930/24156 [36:43:07<51:36, 13.70s/it]

 99%|█████████▉| 23931/24156 [36:43:25<55:22, 14.76s/it]

 99%|█████████▉| 23932/24156 [36:43:36<51:52, 13.90s/it]

 99%|█████████▉| 23933/24156 [36:43:48<49:00, 13.19s/it]

 99%|█████████▉| 23934/24156 [36:44:08<55:56, 15.12s/it]

 99%|█████████▉| 23935/24156 [36:44:18<50:43, 13.77s/it]

 99%|█████████▉| 23936/24156 [36:44:36<55:04, 15.02s/it]

 99%|█████████▉| 23937/24156 [36:44:54<58:15, 15.96s/it]
{'loss': 0.2573, 'learning_rate': 1.5677930987677274e-06, 'rewards/chosen': -2.093400478363037, 'rewards/rejected': -4.512048244476318, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4186477661132812, 'policy_logps/rejected': -490.7428894042969, 'policy_logps/chosen': -468.56207275390625, 'referece_logps/rejected': -445.62237548828125, 'referece_logps/chosen': -447.6280822753906, 'logits/rejected': -0.7209852337837219, 'logits/chosen': -0.7059704661369324, 'epoch': 8.92}

 99%|█████████▉| 23938/24156 [36:45:10<57:39, 15.87s/it]


 99%|█████████▉| 23940/24156 [36:45:52<1:05:46, 18.27s/it]

 99%|█████████▉| 23941/24156 [36:46:11<1:07:11, 18.75s/it]

 99%|█████████▉| 23942/24156 [36:46:31<1:07:43, 18.99s/it]

 99%|█████████▉| 23943/24156 [36:46:46<1:03:44, 17.95s/it]
{'loss': 0.3221, 'learning_rate': 1.5671306978094974e-06, 'rewards/chosen': -2.535308361053467, 'rewards/rejected': -5.017951965332031, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4826436042785645, 'policy_logps/rejected': -402.51568603515625, 'policy_logps/chosen': -360.884765625, 'referece_logps/rejected': -352.336181640625, 'referece_logps/chosen': -335.53167724609375, 'logits/rejected': 0.5758565664291382, 'logits/chosen': 0.6230374574661255, 'epoch': 8.92}


 99%|█████████▉| 23945/24156 [36:47:27<1:07:54, 19.31s/it]

 99%|█████████▉| 23946/24156 [36:47:49<1:10:23, 20.11s/it]
{'loss': 0.2565, 'learning_rate': 1.5667993596666436e-06, 'rewards/chosen': -2.1537137031555176, 'rewards/rejected': -3.766331911087036, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6126186847686768, 'policy_logps/rejected': -315.175537109375, 'policy_logps/chosen': -518.7317504882812, 'referece_logps/rejected': -277.51220703125, 'referece_logps/chosen': -497.19464111328125, 'logits/rejected': -0.7964714169502258, 'logits/chosen': -0.8174639940261841, 'epoch': 8.92}


 99%|█████████▉| 23948/24156 [36:48:11<52:59, 15.29s/it]

 99%|█████████▉| 23949/24156 [36:48:25<51:31, 14.94s/it]

 99%|█████████▉| 23950/24156 [36:48:44<56:05, 16.34s/it]

 99%|█████████▉| 23951/24156 [36:49:01<56:22, 16.50s/it]

 99%|█████████▉| 23952/24156 [36:49:15<53:22, 15.70s/it]

 99%|█████████▉| 23953/24156 [36:49:31<53:02, 15.68s/it]

 99%|█████████▉| 23954/24156 [36:49:43<49:18, 14.64s/it]

 99%|█████████▉| 23955/24156 [36:50:01<52:56, 15.80s/it]

 99%|█████████▉| 23956/24156 [36:50:20<55:26, 16.63s/it]

 99%|█████████▉| 23957/24156 [36:50:31<49:18, 14.87s/it]

 99%|█████████▉| 23958/24156 [36:50:47<50:08, 15.20s/it]

 99%|█████████▉| 23959/24156 [36:50:58<46:29, 14.16s/it]

 99%|█████████▉| 23960/24156 [36:51:17<50:05, 15.33s/it]

 99%|█████████▉| 23961/24156 [36:51:27<45:17, 13.93s/it]

 99%|█████████▉| 23962/24156 [36:51:45<49:12, 15.22s/it]

 99%|█████████▉| 23963/24156 [36:51:56<44:31, 13.84s/it]

 99%|█████████▉| 23964/24156 [36:52:07<41:31, 12.98s/it]

 99%|█████████▉| 23965/24156 [36:52:29<49:56, 15.69s/it]
{'loss': 0.403, 'learning_rate': 1.5646987573974724e-06, 'rewards/chosen': -2.933471918106079, 'rewards/rejected': -4.749856472015381, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8163845539093018, 'policy_logps/rejected': -529.0535278320312, 'policy_logps/chosen': -558.79833984375, 'referece_logps/rejected': -481.55499267578125, 'referece_logps/chosen': -529.463623046875, 'logits/rejected': 0.030931666493415833, 'logits/chosen': 0.08114302903413773, 'epoch': 8.93}


 99%|█████████▉| 23967/24156 [36:53:02<50:09, 15.92s/it]
{'loss': 0.3596, 'learning_rate': 1.5644774279487112e-06, 'rewards/chosen': -2.0128872394561768, 'rewards/rejected': -3.7545628547668457, 'rewards/accuracies': 0.875, 'rewards/margins': 1.741675853729248, 'policy_logps/rejected': -471.71014404296875, 'policy_logps/chosen': -371.986083984375, 'referece_logps/rejected': -434.16448974609375, 'referece_logps/chosen': -351.857177734375, 'logits/rejected': 0.08447357267141342, 'logits/chosen': 0.17218057811260223, 'epoch': 8.93}


 99%|█████████▉| 23969/24156 [36:53:25<43:00, 13.80s/it]

 99%|█████████▉| 23970/24156 [36:53:35<39:47, 12.83s/it]

 99%|█████████▉| 23971/24156 [36:53:55<46:01, 14.92s/it]

 99%|█████████▉| 23972/24156 [36:54:14<49:00, 15.98s/it]

 99%|█████████▉| 23973/24156 [36:54:27<46:42, 15.31s/it]

 99%|█████████▉| 23974/24156 [36:54:40<43:32, 14.36s/it]

 99%|█████████▉| 23975/24156 [36:54:51<40:58, 13.58s/it]

 99%|█████████▉| 23976/24156 [36:55:10<45:01, 15.01s/it]

 99%|█████████▉| 23977/24156 [36:55:27<47:08, 15.80s/it]
{'loss': 0.3898, 'learning_rate': 1.563370172165443e-06, 'rewards/chosen': -2.2645504474639893, 'rewards/rejected': -3.7113142013549805, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4467639923095703, 'policy_logps/rejected': -301.6523742675781, 'policy_logps/chosen': -418.4547119140625, 'referece_logps/rejected': -264.53924560546875, 'referece_logps/chosen': -395.80914306640625, 'logits/rejected': -0.5508076548576355, 'logits/chosen': -0.6939525604248047, 'epoch': 8.93}


 99%|█████████▉| 23979/24156 [36:56:00<48:15, 16.36s/it]

 99%|█████████▉| 23980/24156 [36:56:14<46:13, 15.76s/it]

 99%|█████████▉| 23981/24156 [36:56:32<47:32, 16.30s/it]
{'loss': 0.3035, 'learning_rate': 1.5629269861643178e-06, 'rewards/chosen': -2.9941632747650146, 'rewards/rejected': -4.086214065551758, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0920507907867432, 'policy_logps/rejected': -340.66522216796875, 'policy_logps/chosen': -434.328369140625, 'referece_logps/rejected': -299.8030700683594, 'referece_logps/chosen': -404.38677978515625, 'logits/rejected': 0.5946599841117859, 'logits/chosen': 0.4662474989891052, 'epoch': 8.93}


 99%|█████████▉| 23983/24156 [36:57:02<46:05, 15.99s/it]

 99%|█████████▉| 23984/24156 [36:57:22<49:08, 17.14s/it]

 99%|█████████▉| 23985/24156 [36:57:42<50:59, 17.89s/it]

 99%|█████████▉| 23986/24156 [36:58:01<51:51, 18.30s/it]

 99%|█████████▉| 23987/24156 [36:58:21<53:06, 18.86s/it]

 99%|█████████▉| 23988/24156 [36:58:33<46:55, 16.76s/it]
{'loss': 0.2876, 'learning_rate': 1.5621510211283502e-06, 'rewards/chosen': -1.3278234004974365, 'rewards/rejected': -3.9453303813934326, 'rewards/accuracies': 0.875, 'rewards/margins': 2.617506980895996, 'policy_logps/rejected': -451.3114318847656, 'policy_logps/chosen': -471.7041015625, 'referece_logps/rejected': -411.8581237792969, 'referece_logps/chosen': -458.4259033203125, 'logits/rejected': -0.10284285247325897, 'logits/chosen': 0.15244042873382568, 'epoch': 8.94}


 99%|█████████▉| 23990/24156 [36:58:59<41:03, 14.84s/it]

 99%|█████████▉| 23991/24156 [36:59:20<45:45, 16.64s/it]

 99%|█████████▉| 23992/24156 [36:59:33<42:28, 15.54s/it]

 99%|█████████▉| 23993/24156 [36:59:53<46:00, 16.94s/it]

 99%|█████████▉| 23994/24156 [37:00:09<44:21, 16.43s/it]

 99%|█████████▉| 23995/24156 [37:00:24<43:22, 16.16s/it]

 99%|█████████▉| 23996/24156 [37:00:45<47:00, 17.63s/it]

 99%|█████████▉| 23997/24156 [37:01:02<45:58, 17.35s/it]

 99%|█████████▉| 23998/24156 [37:01:24<49:16, 18.71s/it]
{'loss': 0.2685, 'learning_rate': 1.5610416408241442e-06, 'rewards/chosen': -2.227816104888916, 'rewards/rejected': -4.120527744293213, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8927115201950073, 'policy_logps/rejected': -345.3022155761719, 'policy_logps/chosen': -398.4826965332031, 'referece_logps/rejected': -304.0969543457031, 'referece_logps/chosen': -376.20452880859375, 'logits/rejected': -0.076717309653759, 'logits/chosen': -0.1899769902229309, 'epoch': 8.94}


 99%|█████████▉| 24000/24156 [37:01:50<41:23, 15.92s/it]

 99%|█████████▉| 24001/24156 [37:02:28<58:25, 22.62s/it]

 99%|█████████▉| 24002/24156 [37:02:48<56:06, 21.86s/it]

 99%|█████████▉| 24003/24156 [37:03:08<54:11, 21.25s/it]

 99%|█████████▉| 24004/24156 [37:03:22<48:19, 19.08s/it]

 99%|█████████▉| 24005/24156 [37:03:40<47:02, 18.69s/it]

 99%|█████████▉| 24006/24156 [37:03:51<41:05, 16.44s/it]

 99%|█████████▉| 24007/24156 [37:04:11<43:53, 17.67s/it]

 99%|█████████▉| 24008/24156 [37:04:25<40:27, 16.40s/it]

 99%|█████████▉| 24009/24156 [37:04:43<41:25, 16.91s/it]

 99%|█████████▉| 24010/24156 [37:04:54<37:15, 15.31s/it]

 99%|█████████▉| 24011/24156 [37:05:12<38:49, 16.07s/it]
{'loss': 0.1989, 'learning_rate': 1.559597938890314e-06, 'rewards/chosen': -3.403503656387329, 'rewards/rejected': -5.4564690589904785, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0529654026031494, 'policy_logps/rejected': -465.495849609375, 'policy_logps/chosen': -379.067138671875, 'referece_logps/rejected': -410.93115234375, 'referece_logps/chosen': -345.0321350097656, 'logits/rejected': 0.2693086862564087, 'logits/chosen': 0.46711504459381104, 'epoch': 8.95}


 99%|█████████▉| 24013/24156 [37:05:47<39:43, 16.67s/it]

 99%|█████████▉| 24014/24156 [37:06:04<40:06, 16.95s/it]

 99%|█████████▉| 24015/24156 [37:06:24<41:53, 17.83s/it]

 99%|█████████▉| 24016/24156 [37:06:44<43:02, 18.45s/it]

 99%|█████████▉| 24017/24156 [37:07:00<41:17, 17.82s/it]

 99%|█████████▉| 24018/24156 [37:07:22<43:17, 18.82s/it]

 99%|█████████▉| 24019/24156 [37:07:40<42:29, 18.61s/it]

 99%|█████████▉| 24020/24156 [37:08:01<43:54, 19.37s/it]

 99%|█████████▉| 24021/24156 [37:08:16<40:47, 18.13s/it]

 99%|█████████▉| 24022/24156 [37:08:35<40:48, 18.27s/it]

 99%|█████████▉| 24023/24156 [37:08:54<41:21, 18.66s/it]

 99%|█████████▉| 24024/24156 [37:09:12<40:29, 18.40s/it]
{'loss': 0.3215, 'learning_rate': 1.5581525368317008e-06, 'rewards/chosen': -3.629746198654175, 'rewards/rejected': -6.229735851287842, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5999886989593506, 'policy_logps/rejected': -381.94427490234375, 'policy_logps/chosen': -334.5198974609375, 'referece_logps/rejected': -319.6469421386719, 'referece_logps/chosen': -298.222412109375, 'logits/rejected': -0.1984422653913498, 'logits/chosen': -0.08347222208976746, 'epoch': 8.95}


 99%|█████████▉| 24026/24156 [37:09:51<41:09, 19.00s/it]
{'loss': 0.3231, 'learning_rate': 1.557930016665331e-06, 'rewards/chosen': -2.0986788272857666, 'rewards/rejected': -4.419094562530518, 'rewards/accuracies': 0.75, 'rewards/margins': 2.320415496826172, 'policy_logps/rejected': -441.323486328125, 'policy_logps/chosen': -314.085693359375, 'referece_logps/rejected': -397.132568359375, 'referece_logps/chosen': -293.09893798828125, 'logits/rejected': -0.7427132725715637, 'logits/chosen': -0.5183398127555847, 'epoch': 8.95}

 99%|█████████▉| 24027/24156 [37:10:08<39:07, 18.20s/it]


 99%|█████████▉| 24029/24156 [37:10:49<40:50, 19.29s/it]

 99%|█████████▉| 24030/24156 [37:11:09<41:03, 19.55s/it]

 99%|█████████▉| 24031/24156 [37:11:25<38:17, 18.38s/it]
{'loss': 0.2641, 'learning_rate': 1.5573735407606731e-06, 'rewards/chosen': -2.310918092727661, 'rewards/rejected': -3.696932792663574, 'rewards/accuracies': 0.75, 'rewards/margins': 1.386014461517334, 'policy_logps/rejected': -526.306396484375, 'policy_logps/chosen': -449.95361328125, 'referece_logps/rejected': -489.3370056152344, 'referece_logps/chosen': -426.84442138671875, 'logits/rejected': 0.38115888833999634, 'logits/chosen': 0.442126989364624, 'epoch': 8.95}


 99%|█████████▉| 24033/24156 [37:12:06<40:18, 19.66s/it]
{'loss': 0.2969, 'learning_rate': 1.5571508802453327e-06, 'rewards/chosen': -2.435288190841675, 'rewards/rejected': -4.836406707763672, 'rewards/accuracies': 0.75, 'rewards/margins': 2.401118278503418, 'policy_logps/rejected': -435.8234558105469, 'policy_logps/chosen': -462.0050964355469, 'referece_logps/rejected': -387.4593811035156, 'referece_logps/chosen': -437.6522216796875, 'logits/rejected': -0.24211464822292328, 'logits/chosen': -0.34022852778434753, 'epoch': 8.95}


 99%|█████████▉| 24035/24156 [37:12:36<34:24, 17.06s/it]

100%|█████████▉| 24036/24156 [37:12:56<35:49, 17.91s/it]

100%|█████████▉| 24037/24156 [37:13:15<35:43, 18.01s/it]

100%|█████████▉| 24038/24156 [37:13:34<36:21, 18.48s/it]

100%|█████████▉| 24039/24156 [37:13:49<34:02, 17.46s/it]
{'loss': 0.3412, 'learning_rate': 1.5564826583812525e-06, 'rewards/chosen': -2.6485748291015625, 'rewards/rejected': -5.342647075653076, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6940722465515137, 'policy_logps/rejected': -531.466064453125, 'policy_logps/chosen': -469.4031982421875, 'referece_logps/rejected': -478.03961181640625, 'referece_logps/chosen': -442.9174499511719, 'logits/rejected': -0.254673570394516, 'logits/chosen': -0.26304230093955994, 'epoch': 8.96}


100%|█████████▉| 24041/24156 [37:14:25<34:18, 17.90s/it]

100%|█████████▉| 24042/24156 [37:14:36<29:56, 15.76s/it]

100%|█████████▉| 24043/24156 [37:14:51<29:10, 15.49s/it]

100%|█████████▉| 24044/24156 [37:15:13<32:34, 17.45s/it]

100%|█████████▉| 24045/24156 [37:15:35<34:49, 18.82s/it]
{'loss': 0.3675, 'learning_rate': 1.5558140763764674e-06, 'rewards/chosen': -2.5874531269073486, 'rewards/rejected': -3.8942625522613525, 'rewards/accuracies': 0.625, 'rewards/margins': 1.306809425354004, 'policy_logps/rejected': -491.5890808105469, 'policy_logps/chosen': -659.9337158203125, 'referece_logps/rejected': -452.6464538574219, 'referece_logps/chosen': -634.05908203125, 'logits/rejected': -0.12378695607185364, 'logits/chosen': -0.34391307830810547, 'epoch': 8.96}


100%|█████████▉| 24047/24156 [37:16:05<30:01, 16.52s/it]

100%|█████████▉| 24048/24156 [37:16:21<29:19, 16.30s/it]
{'loss': 0.3015, 'learning_rate': 1.5554796504565167e-06, 'rewards/chosen': -2.1176702976226807, 'rewards/rejected': -4.336207389831543, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2185370922088623, 'policy_logps/rejected': -384.42559814453125, 'policy_logps/chosen': -593.6370239257812, 'referece_logps/rejected': -341.06353759765625, 'referece_logps/chosen': -572.4603271484375, 'logits/rejected': 0.8972827196121216, 'logits/chosen': 1.0257505178451538, 'epoch': 8.96}


100%|█████████▉| 24050/24156 [37:16:51<28:31, 16.15s/it]

100%|█████████▉| 24051/24156 [37:17:08<28:18, 16.17s/it]

100%|█████████▉| 24052/24156 [37:17:26<28:59, 16.73s/it]
{'loss': 0.3125, 'learning_rate': 1.5550336094369815e-06, 'rewards/chosen': -1.9009219408035278, 'rewards/rejected': -3.843073606491089, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9421520233154297, 'policy_logps/rejected': -496.78839111328125, 'policy_logps/chosen': -441.7687072753906, 'referece_logps/rejected': -458.357666015625, 'referece_logps/chosen': -422.759521484375, 'logits/rejected': 0.005691707134246826, 'logits/chosen': -0.11388031393289566, 'epoch': 8.96}


100%|█████████▉| 24054/24156 [37:17:53<25:26, 14.97s/it]
{'loss': 0.434, 'learning_rate': 1.5548105290520386e-06, 'rewards/chosen': -3.197903871536255, 'rewards/rejected': -4.4920806884765625, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2941770553588867, 'policy_logps/rejected': -355.2659912109375, 'policy_logps/chosen': -370.4140319824219, 'referece_logps/rejected': -310.34515380859375, 'referece_logps/chosen': -338.43499755859375, 'logits/rejected': -0.8092272877693176, 'logits/chosen': -0.7364317178726196, 'epoch': 8.96}

100%|█████████▉| 24055/24156 [37:18:04<23:22, 13.88s/it]

100%|█████████▉| 24056/24156 [37:18:23<25:24, 15.24s/it]

100%|█████████▉| 24057/24156 [37:18:42<27:19, 16.56s/it]

100%|█████████▉| 24058/24156 [37:19:02<28:47, 17.63s/it]


100%|█████████▉| 24060/24156 [37:19:32<25:16, 15.79s/it]

100%|█████████▉| 24061/24156 [37:19:46<24:14, 15.31s/it]
{'loss': 0.2614, 'learning_rate': 1.5540294336337918e-06, 'rewards/chosen': -1.797924518585205, 'rewards/rejected': -3.573223114013672, 'rewards/accuracies': 0.875, 'rewards/margins': 1.775298833847046, 'policy_logps/rejected': -455.5013427734375, 'policy_logps/chosen': -420.6018371582031, 'referece_logps/rejected': -419.76910400390625, 'referece_logps/chosen': -402.62255859375, 'logits/rejected': -0.5196912288665771, 'logits/chosen': -0.6293982267379761, 'epoch': 8.96}

100%|█████████▉| 24062/24156 [37:19:59<22:55, 14.63s/it]


100%|█████████▉| 24064/24156 [37:20:35<24:47, 16.17s/it]

100%|█████████▉| 24065/24156 [37:20:54<25:25, 16.77s/it]

100%|█████████▉| 24066/24156 [37:21:10<24:56, 16.63s/it]
{'loss': 0.2601, 'learning_rate': 1.5534712095007113e-06, 'rewards/chosen': -2.574446678161621, 'rewards/rejected': -4.089433670043945, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5149863958358765, 'policy_logps/rejected': -573.5700073242188, 'policy_logps/chosen': -532.5670166015625, 'referece_logps/rejected': -532.6756591796875, 'referece_logps/chosen': -506.82257080078125, 'logits/rejected': -0.4898427724838257, 'logits/chosen': -0.4868828058242798, 'epoch': 8.97}

100%|█████████▉| 24067/24156 [37:21:25<23:58, 16.16s/it]

100%|█████████▉| 24068/24156 [37:21:46<25:56, 17.68s/it]

100%|█████████▉| 24069/24156 [37:21:57<22:40, 15.64s/it]


100%|█████████▉| 24071/24156 [37:22:28<21:33, 15.22s/it]

100%|█████████▉| 24072/24156 [37:22:43<21:21, 15.25s/it]

100%|█████████▉| 24073/24156 [37:22:55<19:46, 14.30s/it]
{'loss': 0.3282, 'learning_rate': 1.5526892778802059e-06, 'rewards/chosen': -2.0295047760009766, 'rewards/rejected': -3.1946840286254883, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1651793718338013, 'policy_logps/rejected': -358.0459899902344, 'policy_logps/chosen': -398.7575378417969, 'referece_logps/rejected': -326.09918212890625, 'referece_logps/chosen': -378.4625244140625, 'logits/rejected': -0.4315590262413025, 'logits/chosen': -0.33641183376312256, 'epoch': 8.97}


100%|█████████▉| 24075/24156 [37:23:18<17:04, 12.65s/it]

100%|█████████▉| 24076/24156 [37:23:39<20:28, 15.36s/it]
{'loss': 0.4483, 'learning_rate': 1.5523540152526652e-06, 'rewards/chosen': -1.9234981536865234, 'rewards/rejected': -3.059462070465088, 'rewards/accuracies': 0.875, 'rewards/margins': 1.135963797569275, 'policy_logps/rejected': -295.6596984863281, 'policy_logps/chosen': -279.21453857421875, 'referece_logps/rejected': -265.0650939941406, 'referece_logps/chosen': -259.9795227050781, 'logits/rejected': -0.6947161555290222, 'logits/chosen': -0.7035207748413086, 'epoch': 8.97}


100%|█████████▉| 24078/24156 [37:24:08<18:53, 14.54s/it]
{'loss': 0.4087, 'learning_rate': 1.5521304571825153e-06, 'rewards/chosen': -1.4569458961486816, 'rewards/rejected': -2.978137254714966, 'rewards/accuracies': 0.75, 'rewards/margins': 1.521191120147705, 'policy_logps/rejected': -410.0705871582031, 'policy_logps/chosen': -350.31402587890625, 'referece_logps/rejected': -380.2892150878906, 'referece_logps/chosen': -335.74456787109375, 'logits/rejected': -0.18755607306957245, 'logits/chosen': -0.03224167227745056, 'epoch': 8.97}

100%|█████████▉| 24079/24156 [37:24:27<20:13, 15.76s/it]

100%|█████████▉| 24080/24156 [37:24:46<21:25, 16.91s/it]


100%|█████████▉| 24082/24156 [37:25:20<20:55, 16.96s/it]

100%|█████████▉| 24083/24156 [37:25:36<20:05, 16.52s/it]

100%|█████████▉| 24084/24156 [37:25:50<19:01, 15.85s/it]
{'loss': 0.4038, 'learning_rate': 1.5514595448203206e-06, 'rewards/chosen': -2.4986374378204346, 'rewards/rejected': -4.940268039703369, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4416303634643555, 'policy_logps/rejected': -285.4602355957031, 'policy_logps/chosen': -280.0774230957031, 'referece_logps/rejected': -236.0575714111328, 'referece_logps/chosen': -255.09107971191406, 'logits/rejected': -0.8021827340126038, 'logits/chosen': -0.8248288035392761, 'epoch': 8.97}


100%|█████████▉| 24086/24156 [37:26:24<19:26, 16.66s/it]

100%|█████████▉| 24087/24156 [37:26:40<19:02, 16.56s/it]
{'loss': 0.3746, 'learning_rate': 1.551123954778372e-06, 'rewards/chosen': -2.3119003772735596, 'rewards/rejected': -4.479836940765381, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1679368019104004, 'policy_logps/rejected': -332.2156982421875, 'policy_logps/chosen': -369.3890686035156, 'referece_logps/rejected': -287.41729736328125, 'referece_logps/chosen': -346.27008056640625, 'logits/rejected': 0.240433007478714, 'logits/chosen': 0.1724969446659088, 'epoch': 8.97}

100%|█████████▉| 24088/24156 [37:26:50<16:44, 14.77s/it]


100%|█████████▉| 24090/24156 [37:27:16<15:08, 13.76s/it]

100%|█████████▉| 24091/24156 [37:27:27<14:09, 13.07s/it]

100%|█████████▉| 24092/24156 [37:27:39<13:36, 12.75s/it]

100%|█████████▉| 24093/24156 [37:27:55<14:24, 13.73s/it]

100%|█████████▉| 24094/24156 [37:28:14<15:38, 15.13s/it]

100%|█████████▉| 24095/24156 [37:28:34<16:46, 16.50s/it]
{'loss': 0.3103, 'learning_rate': 1.5502286122140484e-06, 'rewards/chosen': -1.939922571182251, 'rewards/rejected': -3.0516459941864014, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1117231845855713, 'policy_logps/rejected': -496.87603759765625, 'policy_logps/chosen': -568.19140625, 'referece_logps/rejected': -466.359619140625, 'referece_logps/chosen': -548.7921752929688, 'logits/rejected': 0.3098039925098419, 'logits/chosen': 0.2945982813835144, 'epoch': 8.98}

100%|█████████▉| 24096/24156 [37:28:47<15:36, 15.61s/it]


100%|█████████▉| 24098/24156 [37:29:22<15:49, 16.38s/it]

100%|█████████▉| 24099/24156 [37:29:40<16:05, 16.94s/it]

100%|█████████▉| 24100/24156 [37:29:58<16:17, 17.45s/it]

100%|█████████▉| 24101/24156 [37:30:10<14:29, 15.81s/it]
{'loss': 0.2856, 'learning_rate': 1.5495566897923327e-06, 'rewards/chosen': -2.31441068649292, 'rewards/rejected': -5.04168176651001, 'rewards/accuracies': 0.875, 'rewards/margins': 2.72727108001709, 'policy_logps/rejected': -458.5750427246094, 'policy_logps/chosen': -388.0498352050781, 'referece_logps/rejected': -408.15826416015625, 'referece_logps/chosen': -364.9057312011719, 'logits/rejected': -0.5091031789779663, 'logits/chosen': -0.5742796659469604, 'epoch': 8.98}


100%|█████████▉| 24103/24156 [37:30:40<13:44, 15.56s/it]

100%|█████████▉| 24104/24156 [37:31:00<14:43, 16.99s/it]

100%|█████████▉| 24105/24156 [37:31:17<14:17, 16.82s/it]

100%|█████████▉| 24106/24156 [37:31:36<14:46, 17.74s/it]

100%|█████████▉| 24107/24156 [37:31:51<13:35, 16.64s/it]

100%|█████████▉| 24108/24156 [37:32:10<14:06, 17.63s/it]

100%|█████████▉| 24109/24156 [37:32:27<13:28, 17.20s/it]
{'loss': 0.2753, 'learning_rate': 1.5486602400588335e-06, 'rewards/chosen': -2.449908971786499, 'rewards/rejected': -4.5833740234375, 'rewards/accuracies': 0.875, 'rewards/margins': 2.133465528488159, 'policy_logps/rejected': -404.8392639160156, 'policy_logps/chosen': -308.0287170410156, 'referece_logps/rejected': -359.0055236816406, 'referece_logps/chosen': -283.52960205078125, 'logits/rejected': -0.9598547220230103, 'logits/chosen': -0.7958194017410278, 'epoch': 8.98}

100%|█████████▉| 24110/24156 [37:32:44<13:08, 17.13s/it]


100%|█████████▉| 24112/24156 [37:33:15<12:11, 16.63s/it]
{'loss': 0.2905, 'learning_rate': 1.5483239086091628e-06, 'rewards/chosen': -2.2822484970092773, 'rewards/rejected': -4.625306606292725, 'rewards/accuracies': 1.0, 'rewards/margins': 2.343057632446289, 'policy_logps/rejected': -381.5429992675781, 'policy_logps/chosen': -277.27813720703125, 'referece_logps/rejected': -335.2899475097656, 'referece_logps/chosen': -254.4556427001953, 'logits/rejected': -0.30577024817466736, 'logits/chosen': -0.4144997298717499, 'epoch': 8.98}

100%|█████████▉| 24113/24156 [37:33:26<10:42, 14.93s/it]


100%|█████████▉| 24115/24156 [37:34:01<11:03, 16.19s/it]

100%|█████████▉| 24116/24156 [37:34:16<10:40, 16.02s/it]

100%|█████████▉| 24117/24156 [37:34:30<10:02, 15.45s/it]
{'loss': 0.3178, 'learning_rate': 1.5477631590751487e-06, 'rewards/chosen': -2.354356288909912, 'rewards/rejected': -4.407817363739014, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0534605979919434, 'policy_logps/rejected': -307.9709167480469, 'policy_logps/chosen': -326.0189208984375, 'referece_logps/rejected': -263.89276123046875, 'referece_logps/chosen': -302.4753723144531, 'logits/rejected': 0.2540994882583618, 'logits/chosen': 0.28873613476753235, 'epoch': 8.99}

100%|█████████▉| 24118/24156 [37:34:49<10:25, 16.45s/it]


100%|█████████▉| 24120/24156 [37:35:22<09:50, 16.39s/it]
{'loss': 0.3592, 'learning_rate': 1.5474265911727126e-06, 'rewards/chosen': -2.684046506881714, 'rewards/rejected': -4.248049259185791, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5640027523040771, 'policy_logps/rejected': -318.23284912109375, 'policy_logps/chosen': -411.8988952636719, 'referece_logps/rejected': -275.75238037109375, 'referece_logps/chosen': -385.0584411621094, 'logits/rejected': -0.20241548120975494, 'logits/chosen': -0.3010309934616089, 'epoch': 8.99}

100%|█████████▉| 24121/24156 [37:35:37<09:14, 15.85s/it]

100%|█████████▉| 24122/24156 [37:35:58<09:47, 17.28s/it]


100%|█████████▉| 24124/24156 [37:36:26<08:16, 15.53s/it]

100%|█████████▉| 24125/24156 [37:36:42<08:04, 15.61s/it]

100%|█████████▉| 24126/24156 [37:36:55<07:19, 14.65s/it]

100%|█████████▉| 24127/24156 [37:37:16<08:04, 16.69s/it]
{'loss': 0.3194, 'learning_rate': 1.5466409217224067e-06, 'rewards/chosen': -2.3746767044067383, 'rewards/rejected': -3.462562084197998, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0878851413726807, 'policy_logps/rejected': -418.4329833984375, 'policy_logps/chosen': -326.0739440917969, 'referece_logps/rejected': -383.8073425292969, 'referece_logps/chosen': -302.3271789550781, 'logits/rejected': 0.24069932103157043, 'logits/chosen': 0.18063265085220337, 'epoch': 8.99}

100%|█████████▉| 24128/24156 [37:37:36<08:10, 17.51s/it]

100%|█████████▉| 24129/24156 [37:37:48<07:10, 15.95s/it]


100%|█████████▉| 24131/24156 [37:38:20<06:44, 16.18s/it]

100%|█████████▉| 24132/24156 [37:38:40<06:54, 17.27s/it]

100%|█████████▉| 24133/24156 [37:38:53<06:03, 15.82s/it]
{'loss': 0.2979, 'learning_rate': 1.5459671074864225e-06, 'rewards/chosen': -2.9609954357147217, 'rewards/rejected': -5.004828453063965, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0438332557678223, 'policy_logps/rejected': -595.6400146484375, 'policy_logps/chosen': -418.84228515625, 'referece_logps/rejected': -545.5916748046875, 'referece_logps/chosen': -389.23236083984375, 'logits/rejected': 0.7030614614486694, 'logits/chosen': 0.9033282995223999, 'epoch': 8.99}

100%|█████████▉| 24134/24156 [37:39:12<06:11, 16.88s/it]


100%|█████████▉| 24136/24156 [37:39:43<05:31, 16.56s/it]

100%|█████████▉| 24137/24156 [37:40:03<05:32, 17.52s/it]
{'loss': 0.3565, 'learning_rate': 1.5455177016714382e-06, 'rewards/chosen': -2.4845011234283447, 'rewards/rejected': -4.091766834259033, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6072660684585571, 'policy_logps/rejected': -276.3851318359375, 'policy_logps/chosen': -559.0987548828125, 'referece_logps/rejected': -235.46743774414062, 'referece_logps/chosen': -534.2537841796875, 'logits/rejected': -0.8104371428489685, 'logits/chosen': -0.9176546335220337, 'epoch': 8.99}


100%|█████████▉| 24139/24156 [37:40:26<04:06, 14.48s/it]
{'loss': 0.3027, 'learning_rate': 1.5452929399151174e-06, 'rewards/chosen': -2.44123911857605, 'rewards/rejected': -4.244521141052246, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8032822608947754, 'policy_logps/rejected': -372.6587219238281, 'policy_logps/chosen': -409.9009094238281, 'referece_logps/rejected': -330.2134704589844, 'referece_logps/chosen': -385.4884948730469, 'logits/rejected': -0.0823761373758316, 'logits/chosen': -0.06341557204723358, 'epoch': 8.99}


100%|█████████▉| 24141/24156 [37:40:51<03:21, 13.44s/it]
{'loss': 0.41, 'learning_rate': 1.5450681389477927e-06, 'rewards/chosen': -2.6353423595428467, 'rewards/rejected': -5.254593372344971, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6192514896392822, 'policy_logps/rejected': -402.2793884277344, 'policy_logps/chosen': -400.6025085449219, 'referece_logps/rejected': -349.73345947265625, 'referece_logps/chosen': -374.24908447265625, 'logits/rejected': 0.37314096093177795, 'logits/chosen': 0.2876279950141907, 'epoch': 8.99}

100%|█████████▉| 24142/24156 [37:41:02<02:59, 12.84s/it]

100%|█████████▉| 24143/24156 [37:41:21<03:11, 14.71s/it]


100%|█████████▉| 24145/24156 [37:41:53<02:51, 15.57s/it]

100%|█████████▉| 24146/24156 [37:42:11<02:42, 16.27s/it]

100%|█████████▉| 24147/24156 [37:42:31<02:35, 17.30s/it]

100%|█████████▉| 24148/24156 [37:42:51<02:26, 18.28s/it]
{'loss': 0.3542, 'learning_rate': 1.5442810270089085e-06, 'rewards/chosen': -1.5201213359832764, 'rewards/rejected': -3.756150722503662, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2360293865203857, 'policy_logps/rejected': -410.7541198730469, 'policy_logps/chosen': -413.1255798339844, 'referece_logps/rejected': -373.192626953125, 'referece_logps/chosen': -397.9243469238281, 'logits/rejected': -0.2909051775932312, 'logits/chosen': -0.30636829137802124, 'epoch': 9.0}


100%|█████████▉| 24150/24156 [37:43:25<01:45, 17.50s/it]
{'loss': 0.2455, 'learning_rate': 1.5440560497921493e-06, 'rewards/chosen': -2.0754551887512207, 'rewards/rejected': -5.1974639892578125, 'rewards/accuracies': 0.875, 'rewards/margins': 3.122009038925171, 'policy_logps/rejected': -355.1468505859375, 'policy_logps/chosen': -302.7657775878906, 'referece_logps/rejected': -303.17218017578125, 'referece_logps/chosen': -282.01123046875, 'logits/rejected': -0.17775335907936096, 'logits/chosen': -0.024258136749267578, 'epoch': 9.0}


100%|█████████▉| 24152/24156 [37:43:59<01:11, 17.83s/it]
{'loss': 0.187, 'learning_rate': 1.5438310334533288e-06, 'rewards/chosen': -2.001332998275757, 'rewards/rejected': -4.5581254959106445, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5567924976348877, 'policy_logps/rejected': -445.5815124511719, 'policy_logps/chosen': -397.37615966796875, 'referece_logps/rejected': -400.0002136230469, 'referece_logps/chosen': -377.3628234863281, 'logits/rejected': 0.2244102656841278, 'logits/chosen': 0.2101670354604721, 'epoch': 9.0}

100%|█████████▉| 24153/24156 [37:44:18<00:54, 18.06s/it]

100%|█████████▉| 24154/24156 [37:44:38<00:37, 18.74s/it]

100%|█████████▉| 24155/24156 [37:44:50<00:16, 16.56s/it]
{'loss': 0.4369, 'learning_rate': 1.5433808834742283e-06, 'rewards/chosen': -2.980855703353882, 'rewards/rejected': -4.7571330070495605, 'rewards/accuracies': 0.625, 'rewards/margins': 1.7762770652770996, 'policy_logps/rejected': -428.1482849121094, 'policy_logps/chosen': -621.6992797851562, 'referece_logps/rejected': -380.57696533203125, 'referece_logps/chosen': -591.8907470703125, 'logits/rejected': -0.7152144312858582, 'logits/chosen': -1.1398452520370483, 'epoch': 9.0}

100%|██████████| 24156/24156 [37:45:04<00:00,  5.63s/it]