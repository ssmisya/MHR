
  0%|          | 0/16110 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/code/mm/MAPO/m3apo/alignment/trainer/llava_dpo_trainer.py:179: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
 62%|██████▏   | 10001/16110 [00:22<00:13, 438.75it/s]

 62%|██████▏   | 10002/16110 [00:35<00:13, 438.75it/s]


 62%|██████▏   | 10003/16110 [00:51<00:39, 156.24it/s]

 62%|██████▏   | 10004/16110 [01:10<01:02, 98.47it/s]

 62%|██████▏   | 10005/16110 [01:29<01:35, 63.77it/s]

 62%|██████▏   | 10006/16110 [01:47<02:21, 43.13it/s]

 62%|██████▏   | 10007/16110 [02:03<03:21, 30.29it/s]
{'loss': 0.0921, 'learning_rate': 2.8925619834710743e-08, 'rewards/chosen': -2.4230735301971436, 'rewards/rejected': -6.378918170928955, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9558448791503906, 'policy_logps/rejected': -385.05987548828125, 'policy_logps/chosen': -375.5602111816406, 'referece_logps/rejected': -321.2707214355469, 'referece_logps/chosen': -351.3294677734375, 'logits/rejected': 0.40150874853134155, 'logits/chosen': 0.42787453532218933, 'epoch': 5.59}


 62%|██████▏   | 10009/16110 [02:45<07:45, 13.10it/s]
[2024-04-05 15:10:36,417] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 62%|██████▏   | 10010/16110 [03:04<11:12,  9.07it/s]

 62%|██████▏   | 10011/16110 [03:23<15:48,  6.43it/s]

 62%|██████▏   | 10012/16110 [03:40<21:45,  4.67it/s]

 62%|██████▏   | 10013/16110 [03:55<29:24,  3.45it/s]

 62%|██████▏   | 10014/16110 [04:17<44:30,  2.28it/s]
{'loss': 0.1637, 'learning_rate': 5.785123966942149e-08, 'rewards/chosen': -4.870157718658447, 'rewards/rejected': -8.097902297973633, 'rewards/accuracies': 0.625, 'rewards/margins': 3.2277443408966064, 'policy_logps/rejected': -376.85552978515625, 'policy_logps/chosen': -365.7355651855469, 'referece_logps/rejected': -295.87652587890625, 'referece_logps/chosen': -317.0339660644531, 'logits/rejected': -0.8160873055458069, 'logits/chosen': -0.796562910079956, 'epoch': 5.59}

 62%|██████▏   | 10015/16110 [04:28<55:59,  1.81it/s]


 62%|██████▏   | 10017/16110 [05:05<1:56:00,  1.14s/it]

 62%|██████▏   | 10018/16110 [05:28<2:57:30,  1.75s/it]

 62%|██████▏   | 10019/16110 [05:49<4:09:25,  2.46s/it]

 62%|██████▏   | 10020/16110 [06:02<5:07:20,  3.03s/it]

 62%|██████▏   | 10021/16110 [06:22<7:01:31,  4.15s/it]

 62%|██████▏   | 10022/16110 [06:33<8:00:28,  4.74s/it]
{'loss': 0.154, 'learning_rate': 9.09090909090909e-08, 'rewards/chosen': -4.130828380584717, 'rewards/rejected': -8.37233829498291, 'rewards/accuracies': 0.75, 'rewards/margins': 4.241509437561035, 'policy_logps/rejected': -302.33551025390625, 'policy_logps/chosen': -380.60882568359375, 'referece_logps/rejected': -218.61215209960938, 'referece_logps/chosen': -339.300537109375, 'logits/rejected': 0.3661498427391052, 'logits/chosen': 0.1686336100101471, 'epoch': 5.6}


 62%|██████▏   | 10024/16110 [07:04<12:11:51,  7.22s/it]

 62%|██████▏   | 10025/16110 [07:15<13:09:31,  7.79s/it]

 62%|██████▏   | 10026/16110 [07:26<14:23:45,  8.52s/it]

 62%|██████▏   | 10027/16110 [07:46<18:16:46, 10.82s/it]

 62%|██████▏   | 10028/16110 [07:58<18:48:07, 11.13s/it]

 62%|██████▏   | 10029/16110 [08:20<23:20:17, 13.82s/it]

 62%|██████▏   | 10030/16110 [08:33<23:07:02, 13.69s/it]

 62%|██████▏   | 10031/16110 [08:52<25:36:07, 15.16s/it]

 62%|██████▏   | 10032/16110 [09:12<27:54:29, 16.53s/it]
{'loss': 0.1417, 'learning_rate': 1.3223140495867768e-07, 'rewards/chosen': -2.9800491333007812, 'rewards/rejected': -8.406510353088379, 'rewards/accuracies': 1.0, 'rewards/margins': 5.426461219787598, 'policy_logps/rejected': -273.40509033203125, 'policy_logps/chosen': -391.48956298828125, 'referece_logps/rejected': -189.33999633789062, 'referece_logps/chosen': -361.6890563964844, 'logits/rejected': 0.2975522577762604, 'logits/chosen': 0.024025648832321167, 'epoch': 5.6}


 62%|██████▏   | 10034/16110 [09:47<29:00:00, 17.18s/it]
{'loss': 0.1537, 'learning_rate': 1.4049586776859503e-07, 'rewards/chosen': -3.606954574584961, 'rewards/rejected': -7.696832180023193, 'rewards/accuracies': 1.0, 'rewards/margins': 4.089878082275391, 'policy_logps/rejected': -430.5357666015625, 'policy_logps/chosen': -282.0096130371094, 'referece_logps/rejected': -353.56744384765625, 'referece_logps/chosen': -245.9400634765625, 'logits/rejected': -0.4443413317203522, 'logits/chosen': -0.34798097610473633, 'epoch': 5.61}

 62%|██████▏   | 10035/16110 [10:09<31:03:55, 18.41s/it]


 62%|██████▏   | 10037/16110 [10:48<31:44:23, 18.81s/it]

 62%|██████▏   | 10038/16110 [11:04<30:26:39, 18.05s/it]

 62%|██████▏   | 10039/16110 [11:25<31:57:41, 18.95s/it]

 62%|██████▏   | 10040/16110 [11:47<33:02:57, 19.60s/it]

 62%|██████▏   | 10041/16110 [12:08<34:09:26, 20.26s/it]
[2024-04-05 15:19:59,984] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1005, 'learning_rate': 1.6942148760330577e-07, 'rewards/chosen': -4.16343355178833, 'rewards/rejected': -9.682583808898926, 'rewards/accuracies': 0.875, 'rewards/margins': 5.519150733947754, 'policy_logps/rejected': -444.11572265625, 'policy_logps/chosen': -359.29071044921875, 'referece_logps/rejected': -347.2898864746094, 'referece_logps/chosen': -317.6564025878906, 'logits/rejected': -0.3037143647670746, 'logits/chosen': -0.24239099025726318, 'epoch': 5.61}


 62%|██████▏   | 10043/16110 [12:45<32:51:14, 19.49s/it]

 62%|██████▏   | 10044/16110 [13:09<34:50:21, 20.68s/it]

 62%|██████▏   | 10045/16110 [13:27<33:45:23, 20.04s/it]

 62%|██████▏   | 10046/16110 [13:49<34:48:06, 20.66s/it]
[2024-04-05 15:21:40,963] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 62%|██████▏   | 10047/16110 [14:10<34:46:20, 20.65s/it]

 62%|██████▏   | 10048/16110 [14:28<33:35:19, 19.95s/it]

 62%|██████▏   | 10049/16110 [14:45<31:52:10, 18.93s/it]

 62%|██████▏   | 10050/16110 [14:59<29:14:53, 17.38s/it]

 62%|██████▏   | 10051/16110 [15:09<25:54:19, 15.39s/it]

 62%|██████▏   | 10052/16110 [15:26<26:19:26, 15.64s/it]

 62%|██████▏   | 10053/16110 [15:41<26:02:23, 15.48s/it]

 62%|██████▏   | 10054/16110 [15:56<26:02:50, 15.48s/it]

 62%|██████▏   | 10055/16110 [16:13<26:56:01, 16.01s/it]

 62%|██████▏   | 10056/16110 [16:30<27:22:31, 16.28s/it]

 62%|██████▏   | 10057/16110 [16:52<30:17:18, 18.01s/it]

 62%|██████▏   | 10058/16110 [17:16<32:56:23, 19.59s/it]

 62%|██████▏   | 10059/16110 [17:35<32:35:23, 19.39s/it]

 62%|██████▏   | 10060/16110 [17:56<33:37:37, 20.01s/it]
[2024-04-05 15:25:47,635] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 62%|██████▏   | 10061/16110 [18:16<33:30:06, 19.94s/it]

 62%|██████▏   | 10062/16110 [18:28<29:40:45, 17.67s/it]
{'loss': 0.1466, 'learning_rate': 2.56198347107438e-07, 'rewards/chosen': -4.379441738128662, 'rewards/rejected': -8.442716598510742, 'rewards/accuracies': 0.875, 'rewards/margins': 4.06327486038208, 'policy_logps/rejected': -432.19232177734375, 'policy_logps/chosen': -413.2061767578125, 'referece_logps/rejected': -347.7651062011719, 'referece_logps/chosen': -369.4117126464844, 'logits/rejected': -0.2636571228504181, 'logits/chosen': -0.24165260791778564, 'epoch': 5.62}


 62%|██████▏   | 10064/16110 [19:13<34:02:21, 20.27s/it]

 62%|██████▏   | 10065/16110 [19:35<34:42:21, 20.67s/it]

 62%|██████▏   | 10066/16110 [19:52<32:56:13, 19.62s/it]
{'loss': 0.138, 'learning_rate': 2.727272727272727e-07, 'rewards/chosen': -4.8186235427856445, 'rewards/rejected': -6.679252624511719, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8606288433074951, 'policy_logps/rejected': -235.4110870361328, 'policy_logps/chosen': -493.4530334472656, 'referece_logps/rejected': -168.6185760498047, 'referece_logps/chosen': -445.26678466796875, 'logits/rejected': -0.3832967281341553, 'logits/chosen': -0.8512164950370789, 'epoch': 5.62}


 62%|██████▏   | 10068/16110 [20:21<28:29:23, 16.98s/it]
{'loss': 0.1459, 'learning_rate': 2.8099173553719007e-07, 'rewards/chosen': -3.1402347087860107, 'rewards/rejected': -9.636138916015625, 'rewards/accuracies': 1.0, 'rewards/margins': 6.495904445648193, 'policy_logps/rejected': -352.374267578125, 'policy_logps/chosen': -346.99176025390625, 'referece_logps/rejected': -256.01287841796875, 'referece_logps/chosen': -315.5894470214844, 'logits/rejected': -0.01829778403043747, 'logits/chosen': -0.08761037886142731, 'epoch': 5.62}


 63%|██████▎   | 10070/16110 [20:56<29:28:27, 17.57s/it]
{'loss': 0.252, 'learning_rate': 2.892561983471074e-07, 'rewards/chosen': -4.387279510498047, 'rewards/rejected': -6.338104248046875, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9508250951766968, 'policy_logps/rejected': -439.6744079589844, 'policy_logps/chosen': -357.4404602050781, 'referece_logps/rejected': -376.2933654785156, 'referece_logps/chosen': -313.56768798828125, 'logits/rejected': 0.06753894686698914, 'logits/chosen': 0.08425290882587433, 'epoch': 5.63}


 63%|██████▎   | 10072/16110 [21:39<32:11:07, 19.19s/it]

 63%|██████▎   | 10073/16110 [22:00<33:06:28, 19.74s/it]

 63%|██████▎   | 10074/16110 [22:21<33:53:35, 20.21s/it]

 63%|██████▎   | 10075/16110 [22:36<31:30:50, 18.80s/it]

 63%|██████▎   | 10076/16110 [22:55<31:24:08, 18.74s/it]

 63%|██████▎   | 10077/16110 [23:16<32:37:55, 19.47s/it]

 63%|██████▎   | 10078/16110 [23:34<32:03:56, 19.14s/it]
{'loss': 0.2474, 'learning_rate': 3.2231404958677684e-07, 'rewards/chosen': -4.153165340423584, 'rewards/rejected': -5.632784843444824, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4796193838119507, 'policy_logps/rejected': -267.6742248535156, 'policy_logps/chosen': -314.08502197265625, 'referece_logps/rejected': -211.34637451171875, 'referece_logps/chosen': -272.5533447265625, 'logits/rejected': -0.2726562023162842, 'logits/chosen': -0.35443049669265747, 'epoch': 5.63}

 63%|██████▎   | 10079/16110 [23:58<34:07:11, 20.37s/it]


 63%|██████▎   | 10081/16110 [24:34<31:42:19, 18.93s/it]
{'loss': 0.2787, 'learning_rate': 3.347107438016529e-07, 'rewards/chosen': -3.3917970657348633, 'rewards/rejected': -9.680273056030273, 'rewards/accuracies': 1.0, 'rewards/margins': 6.288475513458252, 'policy_logps/rejected': -345.05450439453125, 'policy_logps/chosen': -347.7659606933594, 'referece_logps/rejected': -248.25177001953125, 'referece_logps/chosen': -313.84796142578125, 'logits/rejected': -0.4796478748321533, 'logits/chosen': -0.603197455406189, 'epoch': 5.63}


 63%|██████▎   | 10083/16110 [25:03<27:19:18, 16.32s/it]
{'loss': 0.1892, 'learning_rate': 3.429752066115703e-07, 'rewards/chosen': -3.9849853515625, 'rewards/rejected': -7.8810133934021, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8960282802581787, 'policy_logps/rejected': -380.827880859375, 'policy_logps/chosen': -392.1569519042969, 'referece_logps/rejected': -302.01776123046875, 'referece_logps/chosen': -352.3070983886719, 'logits/rejected': 0.12721985578536987, 'logits/chosen': 0.1381872296333313, 'epoch': 5.63}


 63%|██████▎   | 10085/16110 [25:39<28:21:42, 16.95s/it]

 63%|██████▎   | 10086/16110 [25:59<30:04:07, 17.97s/it]

 63%|██████▎   | 10087/16110 [26:10<26:41:32, 15.95s/it]

 63%|██████▎   | 10088/16110 [26:29<27:56:27, 16.70s/it]

 63%|██████▎   | 10089/16110 [26:47<28:40:19, 17.14s/it]
{'loss': 0.334, 'learning_rate': 3.677685950413223e-07, 'rewards/chosen': -3.125187397003174, 'rewards/rejected': -5.683691501617432, 'rewards/accuracies': 0.875, 'rewards/margins': 2.558504343032837, 'policy_logps/rejected': -320.0150146484375, 'policy_logps/chosen': -404.0674133300781, 'referece_logps/rejected': -263.1781005859375, 'referece_logps/chosen': -372.8155212402344, 'logits/rejected': 0.42117974162101746, 'logits/chosen': 0.3832883834838867, 'epoch': 5.64}


 63%|██████▎   | 10091/16110 [27:27<30:37:56, 18.32s/it]

 63%|██████▎   | 10092/16110 [27:45<30:24:19, 18.19s/it]

 63%|██████▎   | 10093/16110 [28:07<32:18:53, 19.33s/it]

 63%|██████▎   | 10094/16110 [28:27<32:44:44, 19.60s/it]
{'loss': 0.166, 'learning_rate': 3.884297520661157e-07, 'rewards/chosen': -5.119178295135498, 'rewards/rejected': -10.252263069152832, 'rewards/accuracies': 1.0, 'rewards/margins': 5.133085250854492, 'policy_logps/rejected': -518.70654296875, 'policy_logps/chosen': -399.97381591796875, 'referece_logps/rejected': -416.18389892578125, 'referece_logps/chosen': -348.7820129394531, 'logits/rejected': 0.42856210470199585, 'logits/chosen': 0.42847341299057007, 'epoch': 5.64}


 63%|██████▎   | 10096/16110 [29:05<32:21:06, 19.37s/it]
{'loss': 0.1451, 'learning_rate': 3.9669421487603305e-07, 'rewards/chosen': -4.166390895843506, 'rewards/rejected': -8.737796783447266, 'rewards/accuracies': 1.0, 'rewards/margins': 4.57140588760376, 'policy_logps/rejected': -422.0411071777344, 'policy_logps/chosen': -521.9371948242188, 'referece_logps/rejected': -334.66314697265625, 'referece_logps/chosen': -480.2733459472656, 'logits/rejected': -0.6864720582962036, 'logits/chosen': -0.961197555065155, 'epoch': 5.64}

 63%|██████▎   | 10097/16110 [29:18<29:26:50, 17.63s/it]


 63%|██████▎   | 10099/16110 [29:48<26:40:16, 15.97s/it]

 63%|██████▎   | 10100/16110 [30:07<27:52:58, 16.70s/it]

 63%|██████▎   | 10101/16110 [30:25<28:32:49, 17.10s/it]

 63%|██████▎   | 10102/16110 [30:47<30:56:51, 18.54s/it]
{'loss': 0.2151, 'learning_rate': 4.2148760330578507e-07, 'rewards/chosen': -3.029742956161499, 'rewards/rejected': -6.169852256774902, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1401093006134033, 'policy_logps/rejected': -400.184814453125, 'policy_logps/chosen': -414.8407287597656, 'referece_logps/rejected': -338.48626708984375, 'referece_logps/chosen': -384.5433044433594, 'logits/rejected': 0.12937337160110474, 'logits/chosen': -0.11804813146591187, 'epoch': 5.64}


 63%|██████▎   | 10104/16110 [31:15<27:38:28, 16.57s/it]

 63%|██████▎   | 10105/16110 [31:33<28:28:47, 17.07s/it]
{'loss': 0.2027, 'learning_rate': 4.3388429752066114e-07, 'rewards/chosen': -3.545562982559204, 'rewards/rejected': -7.982392311096191, 'rewards/accuracies': 0.875, 'rewards/margins': 4.436829090118408, 'policy_logps/rejected': -352.29248046875, 'policy_logps/chosen': -410.34527587890625, 'referece_logps/rejected': -272.4685363769531, 'referece_logps/chosen': -374.8896484375, 'logits/rejected': 0.4457652270793915, 'logits/chosen': 0.12976014614105225, 'epoch': 5.65}


 63%|██████▎   | 10107/16110 [32:09<29:47:55, 17.87s/it]

 63%|██████▎   | 10108/16110 [32:29<30:33:11, 18.33s/it]

 63%|██████▎   | 10109/16110 [32:44<29:13:04, 17.53s/it]

 63%|██████▎   | 10110/16110 [33:08<32:05:47, 19.26s/it]

 63%|██████▎   | 10111/16110 [33:22<29:23:56, 17.64s/it]

 63%|██████▎   | 10112/16110 [33:37<28:26:00, 17.07s/it]

 63%|██████▎   | 10113/16110 [33:53<27:56:51, 16.78s/it]

 63%|██████▎   | 10114/16110 [34:05<25:17:57, 15.19s/it]

 63%|██████▎   | 10115/16110 [34:16<23:01:07, 13.82s/it]
{'loss': 0.2356, 'learning_rate': 4.7520661157024796e-07, 'rewards/chosen': -2.6425364017486572, 'rewards/rejected': -8.1961669921875, 'rewards/accuracies': 0.875, 'rewards/margins': 5.553630828857422, 'policy_logps/rejected': -265.6538391113281, 'policy_logps/chosen': -298.4278564453125, 'referece_logps/rejected': -183.69216918945312, 'referece_logps/chosen': -272.00250244140625, 'logits/rejected': 0.48792198300361633, 'logits/chosen': 0.3522028625011444, 'epoch': 5.65}

 63%|██████▎   | 10116/16110 [34:38<27:31:57, 16.54s/it]


 63%|██████▎   | 10118/16110 [35:16<29:45:58, 17.88s/it]

 63%|██████▎   | 10119/16110 [35:34<29:47:46, 17.90s/it]

 63%|██████▎   | 10120/16110 [35:49<28:31:00, 17.14s/it]

 63%|██████▎   | 10121/16110 [36:10<30:06:42, 18.10s/it]
{'loss': 0.2484, 'learning_rate': 5e-07, 'rewards/chosen': -4.812804698944092, 'rewards/rejected': -8.855337142944336, 'rewards/accuracies': 1.0, 'rewards/margins': 4.042532920837402, 'policy_logps/rejected': -357.2410583496094, 'policy_logps/chosen': -441.19134521484375, 'referece_logps/rejected': -268.6876525878906, 'referece_logps/chosen': -393.0632629394531, 'logits/rejected': -0.011856280267238617, 'logits/chosen': -0.3522050976753235, 'epoch': 5.65}


 63%|██████▎   | 10123/16110 [36:44<29:17:12, 17.61s/it]
{'loss': 0.3267, 'learning_rate': 5.082644628099174e-07, 'rewards/chosen': -4.169637680053711, 'rewards/rejected': -6.9709978103637695, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8013598918914795, 'policy_logps/rejected': -324.15228271484375, 'policy_logps/chosen': -332.43603515625, 'referece_logps/rejected': -254.44229125976562, 'referece_logps/chosen': -290.73968505859375, 'logits/rejected': -0.24404728412628174, 'logits/chosen': -0.15013086795806885, 'epoch': 5.66}


 63%|██████▎   | 10125/16110 [37:20<30:09:51, 18.14s/it]

 63%|██████▎   | 10126/16110 [37:39<30:58:46, 18.64s/it]

 63%|██████▎   | 10127/16110 [37:56<30:04:41, 18.10s/it]

 63%|██████▎   | 10128/16110 [38:20<32:39:02, 19.65s/it]

 63%|██████▎   | 10129/16110 [38:41<33:44:25, 20.31s/it]
{'loss': 0.0737, 'learning_rate': 5.330578512396694e-07, 'rewards/chosen': -3.6488466262817383, 'rewards/rejected': -8.693228721618652, 'rewards/accuracies': 1.0, 'rewards/margins': 5.0443830490112305, 'policy_logps/rejected': -357.1380615234375, 'policy_logps/chosen': -366.0244445800781, 'referece_logps/rejected': -270.205810546875, 'referece_logps/chosen': -329.5359802246094, 'logits/rejected': 0.10037780553102493, 'logits/chosen': 0.041162438690662384, 'epoch': 5.66}

 63%|██████▎   | 10130/16110 [39:03<34:17:43, 20.65s/it]

 63%|██████▎   | 10131/16110 [39:19<32:03:39, 19.30s/it]


 63%|██████▎   | 10133/16110 [39:50<28:14:22, 17.01s/it]

 63%|██████▎   | 10134/16110 [40:10<29:52:23, 18.00s/it]

 63%|██████▎   | 10135/16110 [40:30<30:31:46, 18.39s/it]

 63%|██████▎   | 10136/16110 [40:42<27:31:26, 16.59s/it]

 63%|██████▎   | 10137/16110 [40:54<25:06:25, 15.13s/it]

 63%|██████▎   | 10138/16110 [41:08<24:50:09, 14.97s/it]

 63%|██████▎   | 10139/16110 [41:28<27:04:02, 16.32s/it]
{'loss': 0.2255, 'learning_rate': 5.743801652892561e-07, 'rewards/chosen': -2.733433723449707, 'rewards/rejected': -7.019948482513428, 'rewards/accuracies': 1.0, 'rewards/margins': 4.286515235900879, 'policy_logps/rejected': -304.2757568359375, 'policy_logps/chosen': -626.4264526367188, 'referece_logps/rejected': -234.07627868652344, 'referece_logps/chosen': -599.092041015625, 'logits/rejected': 0.16215142607688904, 'logits/chosen': -0.20163607597351074, 'epoch': 5.66}

 63%|██████▎   | 10140/16110 [41:47<28:39:22, 17.28s/it]


 63%|██████▎   | 10142/16110 [42:10<23:30:16, 14.18s/it]

 63%|██████▎   | 10143/16110 [42:31<26:38:29, 16.07s/it]

 63%|██████▎   | 10144/16110 [42:52<29:29:34, 17.80s/it]
{'loss': 0.329, 'learning_rate': 5.950413223140495e-07, 'rewards/chosen': -4.024731636047363, 'rewards/rejected': -6.949997901916504, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9252655506134033, 'policy_logps/rejected': -296.29296875, 'policy_logps/chosen': -317.81939697265625, 'referece_logps/rejected': -226.79299926757812, 'referece_logps/chosen': -277.5720520019531, 'logits/rejected': 0.13470563292503357, 'logits/chosen': -0.08744943886995316, 'epoch': 5.67}

 63%|██████▎   | 10145/16110 [43:13<31:03:17, 18.74s/it]

 63%|██████▎   | 10146/16110 [43:31<30:40:48, 18.52s/it]


 63%|██████▎   | 10148/16110 [44:16<33:49:32, 20.42s/it]
{'loss': 0.2673, 'learning_rate': 6.115702479338843e-07, 'rewards/chosen': -2.3792684078216553, 'rewards/rejected': -8.924047470092773, 'rewards/accuracies': 0.875, 'rewards/margins': 6.544778823852539, 'policy_logps/rejected': -468.0989685058594, 'policy_logps/chosen': -311.4691162109375, 'referece_logps/rejected': -378.8584899902344, 'referece_logps/chosen': -287.6764221191406, 'logits/rejected': -0.16845375299453735, 'logits/chosen': 0.06528884917497635, 'epoch': 5.67}


 63%|██████▎   | 10150/16110 [44:54<31:50:05, 19.23s/it]

 63%|██████▎   | 10151/16110 [45:07<28:34:01, 17.26s/it]

 63%|██████▎   | 10152/16110 [45:24<28:55:04, 17.47s/it]

 63%|██████▎   | 10153/16110 [45:43<29:13:50, 17.67s/it]
{'loss': 0.1637, 'learning_rate': 6.322314049586777e-07, 'rewards/chosen': -2.997039318084717, 'rewards/rejected': -7.480113506317139, 'rewards/accuracies': 0.875, 'rewards/margins': 4.4830732345581055, 'policy_logps/rejected': -247.1285858154297, 'policy_logps/chosen': -374.371826171875, 'referece_logps/rejected': -172.32745361328125, 'referece_logps/chosen': -344.40142822265625, 'logits/rejected': 0.29300689697265625, 'logits/chosen': 0.008921794593334198, 'epoch': 5.67}


 63%|██████▎   | 10155/16110 [46:16<28:24:39, 17.18s/it]

 63%|██████▎   | 10156/16110 [46:32<27:55:48, 16.89s/it]

 63%|██████▎   | 10157/16110 [46:44<25:26:27, 15.39s/it]

 63%|██████▎   | 10158/16110 [47:02<26:43:59, 16.17s/it]
{'loss': 0.1652, 'learning_rate': 6.52892561983471e-07, 'rewards/chosen': -3.7753756046295166, 'rewards/rejected': -7.4573822021484375, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6820068359375, 'policy_logps/rejected': -589.9618530273438, 'policy_logps/chosen': -451.82025146484375, 'referece_logps/rejected': -515.3880615234375, 'referece_logps/chosen': -414.0664978027344, 'logits/rejected': 1.056191325187683, 'logits/chosen': 1.1400771141052246, 'epoch': 5.67}


 63%|██████▎   | 10160/16110 [47:32<25:07:34, 15.20s/it]
{'loss': 0.1613, 'learning_rate': 6.611570247933884e-07, 'rewards/chosen': -5.335272789001465, 'rewards/rejected': -8.195470809936523, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8601975440979004, 'policy_logps/rejected': -361.077880859375, 'policy_logps/chosen': -445.355712890625, 'referece_logps/rejected': -279.1231994628906, 'referece_logps/chosen': -392.0029602050781, 'logits/rejected': 0.38789865374565125, 'logits/chosen': 0.43367594480514526, 'epoch': 5.68}

 63%|██████▎   | 10161/16110 [47:47<25:11:52, 15.25s/it]


 63%|██████▎   | 10163/16110 [48:28<29:46:26, 18.02s/it]

 63%|██████▎   | 10164/16110 [48:50<31:42:57, 19.20s/it]

 63%|██████▎   | 10165/16110 [49:12<33:04:17, 20.03s/it]

 63%|██████▎   | 10166/16110 [49:33<33:26:13, 20.25s/it]
{'loss': 0.1397, 'learning_rate': 6.859504132231405e-07, 'rewards/chosen': -4.780800819396973, 'rewards/rejected': -8.454338073730469, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6735382080078125, 'policy_logps/rejected': -501.68768310546875, 'policy_logps/chosen': -438.7922058105469, 'referece_logps/rejected': -417.1442565917969, 'referece_logps/chosen': -390.98419189453125, 'logits/rejected': -0.026699021458625793, 'logits/chosen': 0.011560887098312378, 'epoch': 5.68}

 63%|██████▎   | 10167/16110 [49:50<31:40:33, 19.19s/it]


 63%|██████▎   | 10169/16110 [50:15<26:08:16, 15.84s/it]
{'loss': 0.1453, 'learning_rate': 6.983471074380165e-07, 'rewards/chosen': -3.783210277557373, 'rewards/rejected': -7.626605987548828, 'rewards/accuracies': 1.0, 'rewards/margins': 3.843395233154297, 'policy_logps/rejected': -294.21405029296875, 'policy_logps/chosen': -366.34332275390625, 'referece_logps/rejected': -217.94796752929688, 'referece_logps/chosen': -328.5111999511719, 'logits/rejected': 0.3593032658100128, 'logits/chosen': 0.1426062136888504, 'epoch': 5.68}

 63%|██████▎   | 10170/16110 [50:29<25:21:33, 15.37s/it]

 63%|██████▎   | 10171/16110 [50:47<26:40:53, 16.17s/it]


 63%|██████▎   | 10173/16110 [51:17<25:46:00, 15.62s/it]

 63%|██████▎   | 10174/16110 [51:30<24:46:09, 15.02s/it]
{'loss': 0.1101, 'learning_rate': 7.190082644628099e-07, 'rewards/chosen': -4.199818134307861, 'rewards/rejected': -8.899269104003906, 'rewards/accuracies': 1.0, 'rewards/margins': 4.699450492858887, 'policy_logps/rejected': -466.64703369140625, 'policy_logps/chosen': -559.4644775390625, 'referece_logps/rejected': -377.65435791015625, 'referece_logps/chosen': -517.46630859375, 'logits/rejected': 0.8788912296295166, 'logits/chosen': 0.6194653511047363, 'epoch': 5.68}


 63%|██████▎   | 10176/16110 [52:06<27:36:45, 16.75s/it]

 63%|██████▎   | 10177/16110 [52:21<26:32:49, 16.11s/it]

 63%|██████▎   | 10178/16110 [52:38<27:07:39, 16.46s/it]

 63%|██████▎   | 10179/16110 [52:55<27:19:42, 16.59s/it]

 63%|██████▎   | 10180/16110 [53:12<27:40:39, 16.80s/it]

 63%|██████▎   | 10181/16110 [53:29<27:41:33, 16.81s/it]
{'loss': 0.2039, 'learning_rate': 7.479338842975206e-07, 'rewards/chosen': -3.4901249408721924, 'rewards/rejected': -8.195295333862305, 'rewards/accuracies': 0.875, 'rewards/margins': 4.705169200897217, 'policy_logps/rejected': -305.0556945800781, 'policy_logps/chosen': -438.35565185546875, 'referece_logps/rejected': -223.10275268554688, 'referece_logps/chosen': -403.45440673828125, 'logits/rejected': -0.07089926302433014, 'logits/chosen': -0.3235178589820862, 'epoch': 5.69}

 63%|██████▎   | 10182/16110 [53:50<29:29:43, 17.91s/it]


 63%|██████▎   | 10184/16110 [54:33<32:21:17, 19.66s/it]

 63%|██████▎   | 10185/16110 [54:55<33:33:55, 20.39s/it]

 63%|██████▎   | 10186/16110 [55:09<30:42:05, 18.66s/it]

 63%|██████▎   | 10187/16110 [55:29<31:22:37, 19.07s/it]
{'loss': 0.1855, 'learning_rate': 7.727272727272727e-07, 'rewards/chosen': -3.8539249897003174, 'rewards/rejected': -6.697344779968262, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8434195518493652, 'policy_logps/rejected': -389.2291564941406, 'policy_logps/chosen': -387.76898193359375, 'referece_logps/rejected': -322.2557373046875, 'referece_logps/chosen': -349.22967529296875, 'logits/rejected': -0.10732806473970413, 'logits/chosen': -0.3765259087085724, 'epoch': 5.69}


 63%|██████▎   | 10189/16110 [55:57<26:32:40, 16.14s/it]
{'loss': 0.1609, 'learning_rate': 7.8099173553719e-07, 'rewards/chosen': -3.2987701892852783, 'rewards/rejected': -7.508426666259766, 'rewards/accuracies': 1.0, 'rewards/margins': 4.209656715393066, 'policy_logps/rejected': -484.3397216796875, 'policy_logps/chosen': -584.1237182617188, 'referece_logps/rejected': -409.25543212890625, 'referece_logps/chosen': -551.135986328125, 'logits/rejected': -0.5822852849960327, 'logits/chosen': -0.6103965044021606, 'epoch': 5.69}

 63%|██████▎   | 10190/16110 [56:12<25:51:01, 15.72s/it]

 63%|██████▎   | 10191/16110 [56:24<24:13:31, 14.73s/it]


 63%|██████▎   | 10193/16110 [56:53<24:26:01, 14.87s/it]

 63%|██████▎   | 10194/16110 [57:13<27:14:27, 16.58s/it]

 63%|██████▎   | 10195/16110 [57:29<26:53:40, 16.37s/it]

 63%|██████▎   | 10196/16110 [57:49<28:23:14, 17.28s/it]
{'loss': 0.1493, 'learning_rate': 8.099173553719008e-07, 'rewards/chosen': -3.4552054405212402, 'rewards/rejected': -7.704826354980469, 'rewards/accuracies': 1.0, 'rewards/margins': 4.2496209144592285, 'policy_logps/rejected': -475.14105224609375, 'policy_logps/chosen': -357.899169921875, 'referece_logps/rejected': -398.0928039550781, 'referece_logps/chosen': -323.34710693359375, 'logits/rejected': -0.0898490697145462, 'logits/chosen': -0.1353762447834015, 'epoch': 5.7}


 63%|██████▎   | 10198/16110 [58:25<28:29:21, 17.35s/it]
{'loss': 0.1752, 'learning_rate': 8.181818181818182e-07, 'rewards/chosen': -2.770237922668457, 'rewards/rejected': -6.381300449371338, 'rewards/accuracies': 0.875, 'rewards/margins': 3.611063003540039, 'policy_logps/rejected': -335.3075866699219, 'policy_logps/chosen': -446.2197570800781, 'referece_logps/rejected': -271.4945373535156, 'referece_logps/chosen': -418.5174255371094, 'logits/rejected': 0.5636264681816101, 'logits/chosen': 0.3316727578639984, 'epoch': 5.7}


 63%|██████▎   | 10200/16110 [58:54<25:42:25, 15.66s/it]
{'loss': 0.1913, 'learning_rate': 8.264462809917355e-07, 'rewards/chosen': -2.4940145015716553, 'rewards/rejected': -8.812657356262207, 'rewards/accuracies': 1.0, 'rewards/margins': 6.318643093109131, 'policy_logps/rejected': -420.894775390625, 'policy_logps/chosen': -353.541015625, 'referece_logps/rejected': -332.7681884765625, 'referece_logps/chosen': -328.6008605957031, 'logits/rejected': 0.11722880601882935, 'logits/chosen': 0.06776732206344604, 'epoch': 5.7}


 63%|██████▎   | 10202/16110 [59:26<25:21:06, 15.45s/it]

 63%|██████▎   | 10203/16110 [59:43<26:19:17, 16.04s/it]

 63%|██████▎   | 10204/16110 [1:00:03<28:10:45, 17.18s/it]

 63%|██████▎   | 10205/16110 [1:00:15<25:29:48, 15.54s/it]

 63%|██████▎   | 10206/16110 [1:00:33<27:04:06, 16.51s/it]

 63%|██████▎   | 10207/16110 [1:00:49<26:38:42, 16.25s/it]

 63%|██████▎   | 10208/16110 [1:01:03<25:42:22, 15.68s/it]

 63%|██████▎   | 10209/16110 [1:01:18<25:05:47, 15.31s/it]
{'loss': 0.1254, 'learning_rate': 8.636363636363636e-07, 'rewards/chosen': -2.7632298469543457, 'rewards/rejected': -6.603533744812012, 'rewards/accuracies': 1.0, 'rewards/margins': 3.840303897857666, 'policy_logps/rejected': -355.6859130859375, 'policy_logps/chosen': -364.9814453125, 'referece_logps/rejected': -289.6505432128906, 'referece_logps/chosen': -337.3491516113281, 'logits/rejected': 0.2701346278190613, 'logits/chosen': 0.23209325969219208, 'epoch': 5.7}

 63%|██████▎   | 10210/16110 [1:01:40<28:27:28, 17.36s/it]

 63%|██████▎   | 10211/16110 [1:02:04<31:42:40, 19.35s/it]

 63%|██████▎   | 10212/16110 [1:02:24<32:10:44, 19.64s/it]


 63%|██████▎   | 10214/16110 [1:02:59<30:15:38, 18.48s/it]
{'loss': 0.1575, 'learning_rate': 8.84297520661157e-07, 'rewards/chosen': -4.244558811187744, 'rewards/rejected': -9.687955856323242, 'rewards/accuracies': 1.0, 'rewards/margins': 5.443397045135498, 'policy_logps/rejected': -347.720458984375, 'policy_logps/chosen': -310.08868408203125, 'referece_logps/rejected': -250.8408966064453, 'referece_logps/chosen': -267.6430969238281, 'logits/rejected': 0.15003491938114166, 'logits/chosen': 0.039190664887428284, 'epoch': 5.71}


 63%|██████▎   | 10216/16110 [1:03:41<32:27:21, 19.82s/it]
{'loss': 0.2904, 'learning_rate': 8.925619834710744e-07, 'rewards/chosen': -4.405102729797363, 'rewards/rejected': -9.028362274169922, 'rewards/accuracies': 0.875, 'rewards/margins': 4.623260498046875, 'policy_logps/rejected': -414.9234619140625, 'policy_logps/chosen': -449.4312744140625, 'referece_logps/rejected': -324.6398010253906, 'referece_logps/chosen': -405.3802490234375, 'logits/rejected': -0.3255300521850586, 'logits/chosen': -0.4034709930419922, 'epoch': 5.71}


 63%|██████▎   | 10218/16110 [1:04:20<32:08:46, 19.64s/it]

 63%|██████▎   | 10219/16110 [1:04:42<33:21:58, 20.39s/it]

 63%|██████▎   | 10220/16110 [1:04:59<32:00:34, 19.56s/it]

 63%|██████▎   | 10221/16110 [1:05:17<31:13:42, 19.09s/it]
{'loss': 0.1325, 'learning_rate': 9.132231404958677e-07, 'rewards/chosen': -3.7021684646606445, 'rewards/rejected': -8.416815757751465, 'rewards/accuracies': 1.0, 'rewards/margins': 4.714646816253662, 'policy_logps/rejected': -343.1038513183594, 'policy_logps/chosen': -373.00860595703125, 'referece_logps/rejected': -258.9356994628906, 'referece_logps/chosen': -335.9869384765625, 'logits/rejected': 0.14098171889781952, 'logits/chosen': -0.07539552450180054, 'epoch': 5.71}

 63%|██████▎   | 10222/16110 [1:05:32<29:12:11, 17.86s/it]


 63%|██████▎   | 10224/16110 [1:06:13<31:23:43, 19.20s/it]

 63%|██████▎   | 10225/16110 [1:06:38<34:05:58, 20.86s/it]
{'loss': 0.1403, 'learning_rate': 9.297520661157024e-07, 'rewards/chosen': -5.302555561065674, 'rewards/rejected': -9.701395988464355, 'rewards/accuracies': 1.0, 'rewards/margins': 4.39884090423584, 'policy_logps/rejected': -529.525146484375, 'policy_logps/chosen': -449.4753112792969, 'referece_logps/rejected': -432.51116943359375, 'referece_logps/chosen': -396.44976806640625, 'logits/rejected': -0.6667397022247314, 'logits/chosen': -0.5767351984977722, 'epoch': 5.71}


 63%|██████▎   | 10227/16110 [1:07:20<33:47:47, 20.68s/it]

 63%|██████▎   | 10228/16110 [1:07:40<33:31:03, 20.51s/it]

 63%|██████▎   | 10229/16110 [1:07:58<32:14:09, 19.73s/it]
{'loss': 0.3059, 'learning_rate': 9.462809917355371e-07, 'rewards/chosen': -4.125307559967041, 'rewards/rejected': -9.028717994689941, 'rewards/accuracies': 0.875, 'rewards/margins': 4.9034104347229, 'policy_logps/rejected': -578.0332641601562, 'policy_logps/chosen': -521.5557861328125, 'referece_logps/rejected': -487.7460632324219, 'referece_logps/chosen': -480.302734375, 'logits/rejected': 0.387580931186676, 'logits/chosen': 0.1840958148241043, 'epoch': 5.71}


 64%|██████▎   | 10231/16110 [1:08:40<33:18:25, 20.40s/it]

 64%|██████▎   | 10232/16110 [1:09:00<33:06:02, 20.27s/it]
{'loss': 0.1886, 'learning_rate': 9.586776859504132e-07, 'rewards/chosen': -4.284518241882324, 'rewards/rejected': -9.243130683898926, 'rewards/accuracies': 1.0, 'rewards/margins': 4.958612442016602, 'policy_logps/rejected': -394.0801696777344, 'policy_logps/chosen': -342.9964904785156, 'referece_logps/rejected': -301.64886474609375, 'referece_logps/chosen': -300.1512756347656, 'logits/rejected': -0.44711506366729736, 'logits/chosen': -0.5571232438087463, 'epoch': 5.72}

 64%|██████▎   | 10233/16110 [1:09:10<28:22:41, 17.38s/it]


 64%|██████▎   | 10235/16110 [1:09:52<31:08:51, 19.09s/it]
{'loss': 0.2432, 'learning_rate': 9.710743801652893e-07, 'rewards/chosen': -3.272979974746704, 'rewards/rejected': -6.903629779815674, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6306495666503906, 'policy_logps/rejected': -581.8281860351562, 'policy_logps/chosen': -474.5908203125, 'referece_logps/rejected': -512.7918701171875, 'referece_logps/chosen': -441.86102294921875, 'logits/rejected': 0.3917214572429657, 'logits/chosen': 0.4733480215072632, 'epoch': 5.72}
[2024-04-05 16:17:57,445] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 64%|██████▎   | 10237/16110 [1:10:28<30:40:49, 18.81s/it]
{'loss': 0.1827, 'learning_rate': 9.793388429752066e-07, 'rewards/chosen': -3.1738641262054443, 'rewards/rejected': -7.17932653427124, 'rewards/accuracies': 1.0, 'rewards/margins': 4.005463123321533, 'policy_logps/rejected': -456.3990173339844, 'policy_logps/chosen': -283.4328918457031, 'referece_logps/rejected': -384.60577392578125, 'referece_logps/chosen': -251.69424438476562, 'logits/rejected': -0.2567155361175537, 'logits/chosen': -0.12038251757621765, 'epoch': 5.72}


 64%|██████▎   | 10239/16110 [1:11:12<33:47:48, 20.72s/it]
{'loss': 0.1137, 'learning_rate': 9.87603305785124e-07, 'rewards/chosen': -3.4618027210235596, 'rewards/rejected': -7.817793369293213, 'rewards/accuracies': 0.875, 'rewards/margins': 4.355990409851074, 'policy_logps/rejected': -420.89080810546875, 'policy_logps/chosen': -308.7906494140625, 'referece_logps/rejected': -342.71282958984375, 'referece_logps/chosen': -274.172607421875, 'logits/rejected': -0.0965585708618164, 'logits/chosen': -0.17484228312969208, 'epoch': 5.72}

 64%|██████▎   | 10240/16110 [1:11:35<34:40:28, 21.27s/it]
[2024-04-05 16:19:47,045] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▎   | 10241/16110 [1:11:55<34:27:15, 21.13s/it]

 64%|██████▎   | 10242/16110 [1:12:17<34:52:41, 21.40s/it]

 64%|██████▎   | 10243/16110 [1:12:31<30:53:40, 18.96s/it]

 64%|██████▎   | 10244/16110 [1:12:45<28:28:25, 17.47s/it]

 64%|██████▎   | 10245/16110 [1:13:03<29:00:09, 17.80s/it]

 64%|██████▎   | 10246/16110 [1:13:24<30:22:34, 18.65s/it]


 64%|██████▎   | 10248/16110 [1:14:02<30:32:14, 18.75s/it]
{'loss': 0.0926, 'learning_rate': 1.024793388429752e-06, 'rewards/chosen': -4.105542182922363, 'rewards/rejected': -8.893128395080566, 'rewards/accuracies': 1.0, 'rewards/margins': 4.787585735321045, 'policy_logps/rejected': -296.7128601074219, 'policy_logps/chosen': -323.0978698730469, 'referece_logps/rejected': -207.78158569335938, 'referece_logps/chosen': -282.04248046875, 'logits/rejected': -0.6825008988380432, 'logits/chosen': -0.6104931831359863, 'epoch': 5.73}

 64%|██████▎   | 10249/16110 [1:14:24<31:58:18, 19.64s/it]

 64%|██████▎   | 10250/16110 [1:14:40<30:11:52, 18.55s/it]
[2024-04-05 16:22:53,694] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▎   | 10251/16110 [1:15:02<31:53:38, 19.60s/it]

 64%|██████▎   | 10252/16110 [1:15:24<32:56:38, 20.25s/it]


 64%|██████▎   | 10254/16110 [1:16:09<34:32:59, 21.24s/it]
{'loss': 0.2425, 'learning_rate': 1.0495867768595042e-06, 'rewards/chosen': -3.9233477115631104, 'rewards/rejected': -6.02018928527832, 'rewards/accuracies': 1.0, 'rewards/margins': 2.096842050552368, 'policy_logps/rejected': -289.3807373046875, 'policy_logps/chosen': -251.4080352783203, 'referece_logps/rejected': -229.1788330078125, 'referece_logps/chosen': -212.174560546875, 'logits/rejected': 0.5047703981399536, 'logits/chosen': 0.5086685419082642, 'epoch': 5.73}

 64%|██████▎   | 10255/16110 [1:16:30<34:26:10, 21.17s/it]

 64%|██████▎   | 10256/16110 [1:16:50<34:01:52, 20.93s/it]

 64%|██████▎   | 10257/16110 [1:17:10<33:30:56, 20.61s/it]

 64%|██████▎   | 10258/16110 [1:17:30<33:05:57, 20.36s/it]

 64%|██████▎   | 10259/16110 [1:17:43<29:50:29, 18.36s/it]

 64%|██████▎   | 10260/16110 [1:18:04<30:44:58, 18.92s/it]

 64%|██████▎   | 10261/16110 [1:18:26<32:30:15, 20.01s/it]


 64%|██████▎   | 10263/16110 [1:19:05<31:54:37, 19.65s/it]
{'loss': 0.179, 'learning_rate': 1.0867768595041321e-06, 'rewards/chosen': -3.3909664154052734, 'rewards/rejected': -6.572444915771484, 'rewards/accuracies': 1.0, 'rewards/margins': 3.181478500366211, 'policy_logps/rejected': -466.1741943359375, 'policy_logps/chosen': -360.7773132324219, 'referece_logps/rejected': -400.4497375488281, 'referece_logps/chosen': -326.8676452636719, 'logits/rejected': -0.44745880365371704, 'logits/chosen': -0.20611199736595154, 'epoch': 5.73}

 64%|██████▎   | 10264/16110 [1:19:26<32:36:33, 20.08s/it]

 64%|██████▎   | 10265/16110 [1:19:40<29:39:45, 18.27s/it]

 64%|██████▎   | 10266/16110 [1:20:00<30:44:52, 18.94s/it]


 64%|██████▎   | 10268/16110 [1:20:41<31:58:18, 19.70s/it]

 64%|██████▎   | 10269/16110 [1:21:01<32:12:22, 19.85s/it]
{'loss': 0.2187, 'learning_rate': 1.1115702479338841e-06, 'rewards/chosen': -4.674722671508789, 'rewards/rejected': -8.885936737060547, 'rewards/accuracies': 1.0, 'rewards/margins': 4.211214065551758, 'policy_logps/rejected': -465.9258728027344, 'policy_logps/chosen': -441.5115661621094, 'referece_logps/rejected': -377.0664978027344, 'referece_logps/chosen': -394.7643127441406, 'logits/rejected': -0.25628018379211426, 'logits/chosen': -0.12026168406009674, 'epoch': 5.74}


 64%|██████▍   | 10271/16110 [1:21:31<28:19:17, 17.46s/it]

 64%|██████▍   | 10272/16110 [1:21:51<29:31:50, 18.21s/it]
{'loss': 0.1226, 'learning_rate': 1.1239669421487603e-06, 'rewards/chosen': -3.679720163345337, 'rewards/rejected': -6.649981498718262, 'rewards/accuracies': 1.0, 'rewards/margins': 2.970261335372925, 'policy_logps/rejected': -320.81634521484375, 'policy_logps/chosen': -267.557373046875, 'referece_logps/rejected': -254.3165283203125, 'referece_logps/chosen': -230.76016235351562, 'logits/rejected': -0.055693045258522034, 'logits/chosen': -0.04807402193546295, 'epoch': 5.74}


 64%|██████▍   | 10274/16110 [1:22:21<27:03:47, 16.69s/it]
{'loss': 0.1909, 'learning_rate': 1.1322314049586775e-06, 'rewards/chosen': -2.392969846725464, 'rewards/rejected': -8.05294418334961, 'rewards/accuracies': 0.875, 'rewards/margins': 5.659974098205566, 'policy_logps/rejected': -382.7513122558594, 'policy_logps/chosen': -453.2025146484375, 'referece_logps/rejected': -302.2218322753906, 'referece_logps/chosen': -429.2727966308594, 'logits/rejected': -0.15811845660209656, 'logits/chosen': -0.02934367209672928, 'epoch': 5.74}

 64%|██████▍   | 10275/16110 [1:22:42<29:18:55, 18.09s/it]


 64%|██████▍   | 10277/16110 [1:23:15<28:04:46, 17.33s/it]

 64%|██████▍   | 10278/16110 [1:23:27<25:35:53, 15.80s/it]
{'loss': 0.2182, 'learning_rate': 1.1487603305785123e-06, 'rewards/chosen': -4.372937202453613, 'rewards/rejected': -7.586214065551758, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2132768630981445, 'policy_logps/rejected': -492.1219787597656, 'policy_logps/chosen': -418.3748779296875, 'referece_logps/rejected': -416.2598876953125, 'referece_logps/chosen': -374.64544677734375, 'logits/rejected': 0.39001619815826416, 'logits/chosen': 0.3505854606628418, 'epoch': 5.74}

 64%|██████▍   | 10279/16110 [1:23:47<27:18:51, 16.86s/it]

 64%|██████▍   | 10280/16110 [1:24:01<25:56:54, 16.02s/it]

 64%|██████▍   | 10281/16110 [1:24:11<23:21:54, 14.43s/it]

 64%|██████▍   | 10282/16110 [1:24:28<24:37:51, 15.21s/it]

 64%|██████▍   | 10283/16110 [1:24:49<27:03:32, 16.72s/it]


 64%|██████▍   | 10285/16110 [1:25:27<28:49:52, 17.82s/it]
{'loss': 0.1382, 'learning_rate': 1.1776859504132231e-06, 'rewards/chosen': -1.4231016635894775, 'rewards/rejected': -5.981948375701904, 'rewards/accuracies': 1.0, 'rewards/margins': 4.558847427368164, 'policy_logps/rejected': -344.3826599121094, 'policy_logps/chosen': -231.6784210205078, 'referece_logps/rejected': -284.5631408691406, 'referece_logps/chosen': -217.4473876953125, 'logits/rejected': -0.2010233998298645, 'logits/chosen': -0.18984465301036835, 'epoch': 5.75}

 64%|██████▍   | 10286/16110 [1:25:48<30:16:13, 18.71s/it]


 64%|██████▍   | 10288/16110 [1:26:25<29:55:11, 18.50s/it]
{'loss': 0.2542, 'learning_rate': 1.190082644628099e-06, 'rewards/chosen': -3.136972427368164, 'rewards/rejected': -6.337103366851807, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2001309394836426, 'policy_logps/rejected': -406.71771240234375, 'policy_logps/chosen': -216.58001708984375, 'referece_logps/rejected': -343.3466796875, 'referece_logps/chosen': -185.21031188964844, 'logits/rejected': -0.9768019914627075, 'logits/chosen': -0.7649030685424805, 'epoch': 5.75}


 64%|██████▍   | 10290/16110 [1:27:05<31:24:16, 19.43s/it]

 64%|██████▍   | 10291/16110 [1:27:27<32:29:43, 20.10s/it]
{'loss': 0.2089, 'learning_rate': 1.2024793388429752e-06, 'rewards/chosen': -2.416092872619629, 'rewards/rejected': -6.6502275466918945, 'rewards/accuracies': 1.0, 'rewards/margins': 4.234135150909424, 'policy_logps/rejected': -373.6530456542969, 'policy_logps/chosen': -399.42279052734375, 'referece_logps/rejected': -307.1507263183594, 'referece_logps/chosen': -375.2618713378906, 'logits/rejected': 0.15064112842082977, 'logits/chosen': 0.03782327100634575, 'epoch': 5.75}

 64%|██████▍   | 10292/16110 [1:27:50<33:47:08, 20.91s/it]

 64%|██████▍   | 10293/16110 [1:28:03<29:53:53, 18.50s/it]

 64%|██████▍   | 10294/16110 [1:28:24<31:23:33, 19.43s/it]

 64%|██████▍   | 10295/16110 [1:28:38<28:40:30, 17.75s/it]

 64%|██████▍   | 10296/16110 [1:28:51<26:13:25, 16.24s/it]

 64%|██████▍   | 10297/16110 [1:29:09<27:06:01, 16.78s/it]

 64%|██████▍   | 10298/16110 [1:29:28<28:12:04, 17.47s/it]


 64%|██████▍   | 10300/16110 [1:30:06<28:59:42, 17.97s/it]
{'loss': 0.2931, 'learning_rate': 1.2396694214876033e-06, 'rewards/chosen': -3.908214569091797, 'rewards/rejected': -6.720381736755371, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8121676445007324, 'policy_logps/rejected': -342.8293151855469, 'policy_logps/chosen': -337.2620544433594, 'referece_logps/rejected': -275.6254577636719, 'referece_logps/chosen': -298.1799011230469, 'logits/rejected': -0.35389021039009094, 'logits/chosen': -0.46315521001815796, 'epoch': 5.75}

 64%|██████▍   | 10301/16110 [1:30:26<30:17:57, 18.78s/it]

 64%|██████▍   | 10302/16110 [1:30:46<30:44:45, 19.06s/it]

 64%|██████▍   | 10303/16110 [1:31:06<31:21:41, 19.44s/it]

 64%|██████▍   | 10304/16110 [1:31:18<27:44:28, 17.20s/it]

 64%|██████▍   | 10305/16110 [1:31:35<27:25:27, 17.01s/it]

 64%|██████▍   | 10306/16110 [1:31:47<24:58:34, 15.49s/it]

 64%|██████▍   | 10307/16110 [1:32:07<27:19:09, 16.95s/it]


 64%|██████▍   | 10309/16110 [1:32:46<29:04:35, 18.04s/it]
{'loss': 0.2411, 'learning_rate': 1.2768595041322314e-06, 'rewards/chosen': -4.163753986358643, 'rewards/rejected': -8.848556518554688, 'rewards/accuracies': 1.0, 'rewards/margins': 4.684803009033203, 'policy_logps/rejected': -548.4534912109375, 'policy_logps/chosen': -636.6141967773438, 'referece_logps/rejected': -459.9678955078125, 'referece_logps/chosen': -594.9766235351562, 'logits/rejected': 0.07578977942466736, 'logits/chosen': -0.2928946316242218, 'epoch': 5.76}


 64%|██████▍   | 10311/16110 [1:33:20<27:57:45, 17.36s/it]
{'loss': 0.3288, 'learning_rate': 1.2851239669421487e-06, 'rewards/chosen': -5.123284339904785, 'rewards/rejected': -6.430837154388428, 'rewards/accuracies': 0.75, 'rewards/margins': 1.307554006576538, 'policy_logps/rejected': -478.80291748046875, 'policy_logps/chosen': -422.3504943847656, 'referece_logps/rejected': -414.49456787109375, 'referece_logps/chosen': -371.1176452636719, 'logits/rejected': -0.3519074320793152, 'logits/chosen': -0.15958572924137115, 'epoch': 5.76}

 64%|██████▍   | 10312/16110 [1:33:35<27:02:32, 16.79s/it]

 64%|██████▍   | 10313/16110 [1:33:53<27:24:51, 17.02s/it]

 64%|██████▍   | 10314/16110 [1:34:09<27:13:50, 16.91s/it]

 64%|██████▍   | 10315/16110 [1:34:29<28:40:47, 17.82s/it]


 64%|██████▍   | 10317/16110 [1:35:12<31:40:01, 19.68s/it]
{'loss': 0.1587, 'learning_rate': 1.3099173553719007e-06, 'rewards/chosen': -3.959991455078125, 'rewards/rejected': -7.190724849700928, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2307333946228027, 'policy_logps/rejected': -385.2360534667969, 'policy_logps/chosen': -453.8216247558594, 'referece_logps/rejected': -313.3287658691406, 'referece_logps/chosen': -414.22174072265625, 'logits/rejected': 0.02387920394539833, 'logits/chosen': -0.2582341134548187, 'epoch': 5.76}

 64%|██████▍   | 10318/16110 [1:35:24<28:12:33, 17.53s/it]

 64%|██████▍   | 10319/16110 [1:35:44<29:23:02, 18.27s/it]

 64%|██████▍   | 10320/16110 [1:36:01<28:36:54, 17.79s/it]


 64%|██████▍   | 10322/16110 [1:36:37<28:54:10, 17.98s/it]

 64%|██████▍   | 10323/16110 [1:36:53<28:02:59, 17.45s/it]

 64%|██████▍   | 10324/16110 [1:37:13<28:55:15, 17.99s/it]

 64%|██████▍   | 10325/16110 [1:37:30<28:32:05, 17.76s/it]

 64%|██████▍   | 10326/16110 [1:37:50<29:34:23, 18.41s/it]

 64%|██████▍   | 10327/16110 [1:38:07<28:52:53, 17.98s/it]

 64%|██████▍   | 10328/16110 [1:38:31<31:46:01, 19.78s/it]

 64%|██████▍   | 10329/16110 [1:38:43<28:05:13, 17.49s/it]

 64%|██████▍   | 10330/16110 [1:38:59<27:34:20, 17.17s/it]

 64%|██████▍   | 10331/16110 [1:39:19<28:40:00, 17.86s/it]

 64%|██████▍   | 10332/16110 [1:39:40<30:26:07, 18.96s/it]

 64%|██████▍   | 10333/16110 [1:39:58<29:56:31, 18.66s/it]

 64%|██████▍   | 10334/16110 [1:40:16<29:22:55, 18.31s/it]

 64%|██████▍   | 10335/16110 [1:40:34<29:08:18, 18.16s/it]

 64%|██████▍   | 10336/16110 [1:40:49<27:35:13, 17.20s/it]

 64%|██████▍   | 10337/16110 [1:41:11<30:01:10, 18.72s/it]

 64%|██████▍   | 10338/16110 [1:41:25<27:48:42, 17.35s/it]

 64%|██████▍   | 10339/16110 [1:41:44<28:36:08, 17.84s/it]

 64%|██████▍   | 10340/16110 [1:42:02<28:36:34, 17.85s/it]

 64%|██████▍   | 10341/16110 [1:42:16<26:43:11, 16.67s/it]

 64%|██████▍   | 10342/16110 [1:42:38<29:28:32, 18.40s/it]

 64%|██████▍   | 10343/16110 [1:42:54<28:26:52, 17.76s/it]

 64%|██████▍   | 10344/16110 [1:43:16<30:23:44, 18.98s/it]

 64%|██████▍   | 10345/16110 [1:43:28<27:00:09, 16.86s/it]

 64%|██████▍   | 10346/16110 [1:43:41<25:16:46, 15.79s/it]

 64%|██████▍   | 10347/16110 [1:44:04<28:30:18, 17.81s/it]

 64%|██████▍   | 10348/16110 [1:44:17<26:21:22, 16.47s/it]

 64%|██████▍   | 10349/16110 [1:44:39<28:37:05, 17.88s/it]

 64%|██████▍   | 10350/16110 [1:44:59<30:01:41, 18.77s/it]

 64%|██████▍   | 10351/16110 [1:45:20<30:41:45, 19.19s/it]

 64%|██████▍   | 10352/16110 [1:45:36<29:13:01, 18.27s/it]

 64%|██████▍   | 10353/16110 [1:45:57<30:46:01, 19.24s/it]

 64%|██████▍   | 10354/16110 [1:46:09<27:08:26, 16.97s/it]

 64%|██████▍   | 10355/16110 [1:46:29<28:47:23, 18.01s/it]

 64%|██████▍   | 10356/16110 [1:46:47<28:25:42, 17.79s/it]

 64%|██████▍   | 10357/16110 [1:47:07<29:47:21, 18.64s/it]

 64%|██████▍   | 10358/16110 [1:47:24<28:56:13, 18.11s/it]

 64%|██████▍   | 10359/16110 [1:47:42<28:48:17, 18.03s/it]

 64%|██████▍   | 10360/16110 [1:47:55<26:34:20, 16.64s/it]

 64%|██████▍   | 10361/16110 [1:48:17<28:53:58, 18.10s/it]

 64%|██████▍   | 10362/16110 [1:48:32<27:36:12, 17.29s/it]

 64%|██████▍   | 10363/16110 [1:48:52<28:46:18, 18.02s/it]

 64%|██████▍   | 10364/16110 [1:49:10<28:52:49, 18.09s/it]

 64%|██████▍   | 10365/16110 [1:49:23<26:14:45, 16.45s/it]

 64%|██████▍   | 10366/16110 [1:49:43<28:12:38, 17.68s/it]

 64%|██████▍   | 10367/16110 [1:50:02<28:32:19, 17.89s/it]

 64%|██████▍   | 10368/16110 [1:50:18<27:49:05, 17.44s/it]

 64%|██████▍   | 10369/16110 [1:50:39<29:16:04, 18.35s/it]

 64%|██████▍   | 10370/16110 [1:50:57<29:10:57, 18.30s/it]

 64%|██████▍   | 10371/16110 [1:51:14<28:52:52, 18.12s/it]

 64%|██████▍   | 10372/16110 [1:51:36<30:26:26, 19.10s/it]

 64%|██████▍   | 10373/16110 [1:51:50<27:57:51, 17.55s/it]

 64%|██████▍   | 10374/16110 [1:52:11<29:39:09, 18.61s/it]

 64%|██████▍   | 10375/16110 [1:52:33<31:15:52, 19.63s/it]

 64%|██████▍   | 10376/16110 [1:52:46<28:09:52, 17.68s/it]

 64%|██████▍   | 10377/16110 [1:53:06<29:10:13, 18.32s/it]

 64%|██████▍   | 10378/16110 [1:53:24<28:52:38, 18.14s/it]

 64%|██████▍   | 10379/16110 [1:53:46<31:02:29, 19.50s/it]

 64%|██████▍   | 10380/16110 [1:54:07<31:32:49, 19.82s/it]

 64%|██████▍   | 10381/16110 [1:54:26<31:27:21, 19.77s/it]

 64%|██████▍   | 10382/16110 [1:54:46<31:22:06, 19.71s/it]

 64%|██████▍   | 10383/16110 [1:55:03<30:16:56, 19.04s/it]

 64%|██████▍   | 10384/16110 [1:55:21<29:43:57, 18.69s/it]

 64%|██████▍   | 10385/16110 [1:55:36<27:43:43, 17.44s/it]

 64%|██████▍   | 10386/16110 [1:55:51<26:35:34, 16.73s/it]

 64%|██████▍   | 10387/16110 [1:56:02<24:05:14, 15.15s/it]

 64%|██████▍   | 10388/16110 [1:56:23<26:32:05, 16.69s/it]

 64%|██████▍   | 10389/16110 [1:56:40<26:38:18, 16.76s/it]

 64%|██████▍   | 10390/16110 [1:56:56<26:21:23, 16.59s/it]

 65%|██████▍   | 10391/16110 [1:57:14<27:14:08, 17.14s/it]

 65%|██████▍   | 10392/16110 [1:57:31<27:00:02, 17.00s/it]

 65%|██████▍   | 10393/16110 [1:57:50<27:46:49, 17.49s/it]

 65%|██████▍   | 10394/16110 [1:58:08<28:17:46, 17.82s/it]

 65%|██████▍   | 10395/16110 [1:58:29<29:56:12, 18.86s/it]

 65%|██████▍   | 10396/16110 [1:58:51<31:10:25, 19.64s/it]

 65%|██████▍   | 10397/16110 [1:59:12<32:01:52, 20.18s/it]

 65%|██████▍   | 10398/16110 [1:59:32<31:48:06, 20.04s/it]

 65%|██████▍   | 10399/16110 [1:59:51<31:09:03, 19.64s/it]

 65%|██████▍   | 10400/16110 [2:00:13<32:17:41, 20.36s/it]

 65%|██████▍   | 10401/16110 [2:00:31<31:18:17, 19.74s/it]
{'loss': 0.1534, 'learning_rate': 1.6570247933884296e-06, 'rewards/chosen': -2.990405321121216, 'rewards/rejected': -7.991444110870361, 'rewards/accuracies': 1.0, 'rewards/margins': 5.001038551330566, 'policy_logps/rejected': -415.334716796875, 'policy_logps/chosen': -287.0415344238281, 'referece_logps/rejected': -335.4202880859375, 'referece_logps/chosen': -257.1374816894531, 'logits/rejected': -0.33824557065963745, 'logits/chosen': -0.23061935603618622, 'epoch': 5.81}


 65%|██████▍   | 10403/16110 [2:01:12<31:50:41, 20.09s/it]

 65%|██████▍   | 10404/16110 [2:01:33<32:17:45, 20.38s/it]
{'loss': 0.2624, 'learning_rate': 1.6694214876033058e-06, 'rewards/chosen': -3.8550710678100586, 'rewards/rejected': -7.237402439117432, 'rewards/accuracies': 1.0, 'rewards/margins': 3.382331132888794, 'policy_logps/rejected': -283.45281982421875, 'policy_logps/chosen': -345.6479187011719, 'referece_logps/rejected': -211.07876586914062, 'referece_logps/chosen': -307.09722900390625, 'logits/rejected': -0.10987263172864914, 'logits/chosen': -0.21283216774463654, 'epoch': 5.81}


 65%|██████▍   | 10406/16110 [2:02:06<29:19:58, 18.51s/it]
{'loss': 0.1219, 'learning_rate': 1.677685950413223e-06, 'rewards/chosen': -3.199760675430298, 'rewards/rejected': -7.109592437744141, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9098312854766846, 'policy_logps/rejected': -338.2793273925781, 'policy_logps/chosen': -327.1925964355469, 'referece_logps/rejected': -267.1833801269531, 'referece_logps/chosen': -295.19500732421875, 'logits/rejected': -0.2992549240589142, 'logits/chosen': -0.4584793150424957, 'epoch': 5.81}


 65%|██████▍   | 10408/16110 [2:02:49<31:32:20, 19.91s/it]

 65%|██████▍   | 10409/16110 [2:03:06<30:24:25, 19.20s/it]

 65%|██████▍   | 10410/16110 [2:03:24<29:41:13, 18.75s/it]

 65%|██████▍   | 10411/16110 [2:03:44<30:16:05, 19.12s/it]

 65%|██████▍   | 10412/16110 [2:04:05<31:08:47, 19.68s/it]

 65%|██████▍   | 10413/16110 [2:04:25<31:05:32, 19.65s/it]

 65%|██████▍   | 10414/16110 [2:04:45<31:26:45, 19.87s/it]

 65%|██████▍   | 10415/16110 [2:05:02<29:46:58, 18.83s/it]

 65%|██████▍   | 10416/16110 [2:05:20<29:39:59, 18.76s/it]

 65%|██████▍   | 10417/16110 [2:05:33<27:00:30, 17.08s/it]
{'loss': 0.2407, 'learning_rate': 1.7231404958677684e-06, 'rewards/chosen': -5.221554756164551, 'rewards/rejected': -8.554128646850586, 'rewards/accuracies': 0.75, 'rewards/margins': 3.332573652267456, 'policy_logps/rejected': -274.3153991699219, 'policy_logps/chosen': -325.96087646484375, 'referece_logps/rejected': -188.7740936279297, 'referece_logps/chosen': -273.7453308105469, 'logits/rejected': -0.21417468786239624, 'logits/chosen': -0.29356086254119873, 'epoch': 5.82}


 65%|██████▍   | 10419/16110 [2:06:05<25:30:44, 16.14s/it]

 65%|██████▍   | 10420/16110 [2:06:21<25:18:44, 16.01s/it]
{'loss': 0.2471, 'learning_rate': 1.7355371900826445e-06, 'rewards/chosen': -3.64888334274292, 'rewards/rejected': -7.5355448722839355, 'rewards/accuracies': 1.0, 'rewards/margins': 3.886662006378174, 'policy_logps/rejected': -280.58203125, 'policy_logps/chosen': -451.877685546875, 'referece_logps/rejected': -205.22659301757812, 'referece_logps/chosen': -415.38885498046875, 'logits/rejected': 0.04765474796295166, 'logits/chosen': 0.22913067042827606, 'epoch': 5.82}

 65%|██████▍   | 10421/16110 [2:06:42<27:41:39, 17.52s/it]


 65%|██████▍   | 10423/16110 [2:07:19<28:14:48, 17.88s/it]

 65%|██████▍   | 10424/16110 [2:07:35<27:09:30, 17.20s/it]

 65%|██████▍   | 10425/16110 [2:07:49<25:37:38, 16.23s/it]

 65%|██████▍   | 10426/16110 [2:08:08<27:18:35, 17.30s/it]

 65%|██████▍   | 10427/16110 [2:08:27<27:53:34, 17.67s/it]

 65%|██████▍   | 10428/16110 [2:08:41<26:22:15, 16.71s/it]

 65%|██████▍   | 10429/16110 [2:09:05<29:30:58, 18.70s/it]
{'loss': 0.3782, 'learning_rate': 1.7727272727272727e-06, 'rewards/chosen': -5.170135498046875, 'rewards/rejected': -6.989543437957764, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8194081783294678, 'policy_logps/rejected': -420.0855407714844, 'policy_logps/chosen': -391.6431884765625, 'referece_logps/rejected': -350.1900634765625, 'referece_logps/chosen': -339.9418029785156, 'logits/rejected': -0.6109817028045654, 'logits/chosen': -0.5850156545639038, 'epoch': 5.83}


 65%|██████▍   | 10431/16110 [2:09:40<27:58:55, 17.74s/it]

 65%|██████▍   | 10432/16110 [2:10:00<29:25:27, 18.66s/it]

 65%|██████▍   | 10433/16110 [2:10:22<30:36:14, 19.41s/it]
{'loss': 0.1742, 'learning_rate': 1.7892561983471072e-06, 'rewards/chosen': -4.1082892417907715, 'rewards/rejected': -6.79766845703125, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6893796920776367, 'policy_logps/rejected': -413.0666198730469, 'policy_logps/chosen': -466.451904296875, 'referece_logps/rejected': -345.0899353027344, 'referece_logps/chosen': -425.3689880371094, 'logits/rejected': -0.3137441575527191, 'logits/chosen': -0.41698983311653137, 'epoch': 5.83}


 65%|██████▍   | 10435/16110 [2:10:55<28:24:31, 18.02s/it]

 65%|██████▍   | 10436/16110 [2:11:13<28:16:01, 17.93s/it]

 65%|██████▍   | 10437/16110 [2:11:35<30:09:51, 19.14s/it]
{'loss': 0.2486, 'learning_rate': 1.8057851239669422e-06, 'rewards/chosen': -4.674604892730713, 'rewards/rejected': -9.600811958312988, 'rewards/accuracies': 1.0, 'rewards/margins': 4.926207065582275, 'policy_logps/rejected': -452.6185607910156, 'policy_logps/chosen': -481.68121337890625, 'referece_logps/rejected': -356.61041259765625, 'referece_logps/chosen': -434.9351806640625, 'logits/rejected': 0.11316578835248947, 'logits/chosen': 0.014416560530662537, 'epoch': 5.83}

 65%|██████▍   | 10438/16110 [2:11:50<28:34:31, 18.14s/it]

 65%|██████▍   | 10439/16110 [2:12:10<29:27:41, 18.70s/it]


 65%|██████▍   | 10441/16110 [2:12:52<31:07:30, 19.77s/it]

 65%|██████▍   | 10442/16110 [2:13:04<27:32:40, 17.49s/it]

 65%|██████▍   | 10443/16110 [2:13:21<27:30:34, 17.48s/it]

 65%|██████▍   | 10444/16110 [2:13:44<29:49:55, 18.95s/it]

 65%|██████▍   | 10445/16110 [2:13:56<26:30:44, 16.85s/it]

 65%|██████▍   | 10446/16110 [2:14:16<27:59:44, 17.79s/it]

 65%|██████▍   | 10447/16110 [2:14:27<25:02:01, 15.91s/it]

 65%|██████▍   | 10448/16110 [2:14:38<22:33:46, 14.35s/it]

 65%|██████▍   | 10449/16110 [2:14:55<23:58:13, 15.24s/it]
{'loss': 0.1573, 'learning_rate': 1.8553719008264462e-06, 'rewards/chosen': -4.367724895477295, 'rewards/rejected': -7.817373275756836, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4496474266052246, 'policy_logps/rejected': -511.9715270996094, 'policy_logps/chosen': -442.59112548828125, 'referece_logps/rejected': -433.79779052734375, 'referece_logps/chosen': -398.91387939453125, 'logits/rejected': -0.1550837606191635, 'logits/chosen': -0.08612010627985, 'epoch': 5.84}


 65%|██████▍   | 10451/16110 [2:15:27<24:37:09, 15.66s/it]

 65%|██████▍   | 10452/16110 [2:15:44<25:06:32, 15.98s/it]

 65%|██████▍   | 10453/16110 [2:16:03<26:46:13, 17.04s/it]

 65%|██████▍   | 10454/16110 [2:16:19<26:18:57, 16.75s/it]

 65%|██████▍   | 10455/16110 [2:16:42<29:08:11, 18.55s/it]
{'loss': 0.1592, 'learning_rate': 1.8801652892561982e-06, 'rewards/chosen': -3.6556308269500732, 'rewards/rejected': -6.002745628356934, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3471145629882812, 'policy_logps/rejected': -325.20135498046875, 'policy_logps/chosen': -270.360107421875, 'referece_logps/rejected': -265.1738586425781, 'referece_logps/chosen': -233.80380249023438, 'logits/rejected': -0.10312899947166443, 'logits/chosen': -0.21600691974163055, 'epoch': 5.84}


 65%|██████▍   | 10457/16110 [2:17:18<28:15:19, 17.99s/it]

 65%|██████▍   | 10458/16110 [2:17:32<26:18:40, 16.76s/it]

 65%|██████▍   | 10459/16110 [2:17:50<26:41:59, 17.01s/it]
[2024-04-05 17:25:41,176] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▍   | 10460/16110 [2:18:10<28:26:39, 18.12s/it]

 65%|██████▍   | 10461/16110 [2:18:26<27:30:12, 17.53s/it]

 65%|██████▍   | 10462/16110 [2:18:42<26:32:28, 16.92s/it]

 65%|██████▍   | 10463/16110 [2:18:56<25:13:06, 16.08s/it]

 65%|██████▍   | 10464/16110 [2:19:16<26:51:11, 17.12s/it]

 65%|██████▍   | 10465/16110 [2:19:32<26:27:51, 16.88s/it]
{'loss': 0.2298, 'learning_rate': 1.921487603305785e-06, 'rewards/chosen': -3.692741632461548, 'rewards/rejected': -8.544282913208008, 'rewards/accuracies': 0.875, 'rewards/margins': 4.851541519165039, 'policy_logps/rejected': -301.4591064453125, 'policy_logps/chosen': -379.3728332519531, 'referece_logps/rejected': -216.01626586914062, 'referece_logps/chosen': -342.44537353515625, 'logits/rejected': -0.3579183518886566, 'logits/chosen': -0.5615549683570862, 'epoch': 5.85}


 65%|██████▍   | 10467/16110 [2:20:02<25:25:59, 16.23s/it]

 65%|██████▍   | 10468/16110 [2:20:18<25:20:47, 16.17s/it]

 65%|██████▍   | 10469/16110 [2:20:38<27:12:37, 17.37s/it]

 65%|██████▍   | 10470/16110 [2:20:56<27:09:28, 17.33s/it]

 65%|██████▍   | 10471/16110 [2:21:18<29:25:16, 18.78s/it]
{'loss': 0.1064, 'learning_rate': 1.9462809917355372e-06, 'rewards/chosen': -4.4638166427612305, 'rewards/rejected': -9.006946563720703, 'rewards/accuracies': 1.0, 'rewards/margins': 4.543130397796631, 'policy_logps/rejected': -534.2267456054688, 'policy_logps/chosen': -500.17559814453125, 'referece_logps/rejected': -444.1573181152344, 'referece_logps/chosen': -455.53741455078125, 'logits/rejected': -0.31566691398620605, 'logits/chosen': -0.35693931579589844, 'epoch': 5.85}

 65%|██████▌   | 10472/16110 [2:21:42<31:45:14, 20.28s/it]

 65%|██████▌   | 10473/16110 [2:21:59<30:30:04, 19.48s/it]


 65%|██████▌   | 10475/16110 [2:22:36<29:21:19, 18.75s/it]
{'loss': 0.1932, 'learning_rate': 1.9628099173553718e-06, 'rewards/chosen': -3.6377100944519043, 'rewards/rejected': -8.639349937438965, 'rewards/accuracies': 0.875, 'rewards/margins': 5.001639366149902, 'policy_logps/rejected': -277.3201904296875, 'policy_logps/chosen': -376.4287109375, 'referece_logps/rejected': -190.92672729492188, 'referece_logps/chosen': -340.0516357421875, 'logits/rejected': -0.44690054655075073, 'logits/chosen': -0.6896912455558777, 'epoch': 5.85}
[2024-04-05 17:30:48,647] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10476/16110 [2:22:57<30:22:53, 19.41s/it]
[2024-04-05 17:31:06,688] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10477/16110 [2:23:15<29:43:53, 19.00s/it]

 65%|██████▌   | 10478/16110 [2:23:37<31:14:57, 19.97s/it]

 65%|██████▌   | 10479/16110 [2:23:54<29:32:26, 18.89s/it]
[2024-04-05 17:32:03,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10480/16110 [2:24:12<29:05:45, 18.60s/it]


 65%|██████▌   | 10482/16110 [2:24:47<28:11:03, 18.03s/it]

 65%|██████▌   | 10483/16110 [2:25:06<29:04:18, 18.60s/it]
{'loss': 0.1667, 'learning_rate': 1.9958677685950413e-06, 'rewards/chosen': -3.952031373977661, 'rewards/rejected': -9.126232147216797, 'rewards/accuracies': 1.0, 'rewards/margins': 5.174200057983398, 'policy_logps/rejected': -247.7495574951172, 'policy_logps/chosen': -493.1146240234375, 'referece_logps/rejected': -156.48724365234375, 'referece_logps/chosen': -453.59429931640625, 'logits/rejected': 0.2431982159614563, 'logits/chosen': -0.023378044366836548, 'epoch': 5.86}

 65%|██████▌   | 10484/16110 [2:25:19<26:12:51, 16.77s/it]


 65%|██████▌   | 10486/16110 [2:25:51<25:24:08, 16.26s/it]

 65%|██████▌   | 10487/16110 [2:26:06<25:04:50, 16.06s/it]

 65%|██████▌   | 10488/16110 [2:26:25<26:04:37, 16.70s/it]

 65%|██████▌   | 10489/16110 [2:26:40<25:34:54, 16.38s/it]
{'loss': 0.1537, 'learning_rate': 1.9999994947409725e-06, 'rewards/chosen': -3.9814884662628174, 'rewards/rejected': -9.361038208007812, 'rewards/accuracies': 1.0, 'rewards/margins': 5.379549026489258, 'policy_logps/rejected': -451.04339599609375, 'policy_logps/chosen': -311.0582275390625, 'referece_logps/rejected': -357.43304443359375, 'referece_logps/chosen': -271.2433166503906, 'logits/rejected': -0.05496440827846527, 'logits/chosen': -0.0620461069047451, 'epoch': 5.86}

 65%|██████▌   | 10490/16110 [2:26:55<24:50:44, 15.92s/it]

 65%|██████▌   | 10491/16110 [2:27:16<27:06:45, 17.37s/it]

 65%|██████▌   | 10492/16110 [2:27:33<27:12:01, 17.43s/it]

 65%|██████▌   | 10493/16110 [2:27:56<29:30:33, 18.91s/it]
[2024-04-05 17:36:11,375] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10494/16110 [2:28:20<31:52:16, 20.43s/it]
[2024-04-05 17:36:30,926] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10495/16110 [2:28:39<31:27:15, 20.17s/it]

 65%|██████▌   | 10496/16110 [2:29:02<32:34:41, 20.89s/it]


 65%|██████▌   | 10498/16110 [2:29:36<29:15:32, 18.77s/it]
{'loss': 0.1865, 'learning_rate': 1.9999960387715053e-06, 'rewards/chosen': -4.018486022949219, 'rewards/rejected': -7.105381011962891, 'rewards/accuracies': 1.0, 'rewards/margins': 3.086894989013672, 'policy_logps/rejected': -342.7882080078125, 'policy_logps/chosen': -484.48211669921875, 'referece_logps/rejected': -271.734375, 'referece_logps/chosen': -444.29730224609375, 'logits/rejected': -0.47630807757377625, 'logits/chosen': -0.6889911890029907, 'epoch': 5.86}
[2024-04-05 17:37:49,808] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10499/16110 [2:29:58<30:41:16, 19.69s/it]
[2024-04-05 17:38:09,857] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10500/16110 [2:30:18<30:51:01, 19.80s/it]/mnt/petrelfs/songmingyang/anaconda3/envs/vcd_origin/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.2648, 'learning_rate': 1.999994159210835e-06, 'rewards/chosen': -3.5780928134918213, 'rewards/rejected': -7.444876670837402, 'rewards/accuracies': 0.875, 'rewards/margins': 3.866783618927002, 'policy_logps/rejected': -296.22149658203125, 'policy_logps/chosen': -285.7721862792969, 'referece_logps/rejected': -221.77272033691406, 'referece_logps/chosen': -249.99124145507812, 'logits/rejected': -0.014989905059337616, 'logits/chosen': -0.21441665291786194, 'epoch': 5.87}

 65%|██████▌   | 10502/16110 [2:31:08<33:34:11, 21.55s/it]

 65%|██████▌   | 10503/16110 [2:31:28<32:50:53, 21.09s/it]

 65%|██████▌   | 10504/16110 [2:31:41<28:52:17, 18.54s/it]
{'loss': 0.2485, 'learning_rate': 1.9999919158657705e-06, 'rewards/chosen': -4.515196323394775, 'rewards/rejected': -7.758396148681641, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2432000637054443, 'policy_logps/rejected': -419.6791687011719, 'policy_logps/chosen': -383.4131774902344, 'referece_logps/rejected': -342.09527587890625, 'referece_logps/chosen': -338.26116943359375, 'logits/rejected': -0.5886393785476685, 'logits/chosen': -0.44742122292518616, 'epoch': 5.87}

 65%|██████▌   | 10505/16110 [2:32:02<29:54:09, 19.21s/it]


 65%|██████▌   | 10507/16110 [2:32:25<23:54:07, 15.36s/it]

 65%|██████▌   | 10508/16110 [2:32:47<26:38:52, 17.12s/it]
{'loss': 0.1847, 'learning_rate': 1.999988358853611e-06, 'rewards/chosen': -4.404229640960693, 'rewards/rejected': -9.986703872680664, 'rewards/accuracies': 1.0, 'rewards/margins': 5.5824737548828125, 'policy_logps/rejected': -446.4716491699219, 'policy_logps/chosen': -354.39404296875, 'referece_logps/rejected': -346.6046447753906, 'referece_logps/chosen': -310.3517761230469, 'logits/rejected': 0.2748870551586151, 'logits/chosen': 0.23876865208148956, 'epoch': 5.87}
[2024-04-05 17:40:59,444] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 65%|██████▌   | 10510/16110 [2:33:23<27:05:49, 17.42s/it]
{'loss': 0.2752, 'learning_rate': 1.9999863378258535e-06, 'rewards/chosen': -4.729086399078369, 'rewards/rejected': -6.832499980926514, 'rewards/accuracies': 0.75, 'rewards/margins': 2.103414297103882, 'policy_logps/rejected': -289.12994384765625, 'policy_logps/chosen': -296.5514831542969, 'referece_logps/rejected': -220.80490112304688, 'referece_logps/chosen': -249.26063537597656, 'logits/rejected': -0.6446887254714966, 'logits/chosen': -0.9824768304824829, 'epoch': 5.87}

 65%|██████▌   | 10511/16110 [2:33:44<28:35:08, 18.38s/it]


 65%|██████▌   | 10513/16110 [2:34:25<30:05:31, 19.36s/it]
{'loss': 0.1956, 'learning_rate': 1.9999830031330305e-06, 'rewards/chosen': -3.955305337905884, 'rewards/rejected': -7.522385597229004, 'rewards/accuracies': 1.0, 'rewards/margins': 3.567080497741699, 'policy_logps/rejected': -310.3597106933594, 'policy_logps/chosen': -602.2223510742188, 'referece_logps/rejected': -235.13587951660156, 'referece_logps/chosen': -562.6693115234375, 'logits/rejected': -0.23745112121105194, 'logits/chosen': -0.7148127555847168, 'epoch': 5.87}

 65%|██████▌   | 10514/16110 [2:34:42<28:56:45, 18.62s/it]


 65%|██████▌   | 10516/16110 [2:35:11<25:22:42, 16.33s/it]
{'loss': 0.193, 'learning_rate': 1.9999793046598715e-06, 'rewards/chosen': -3.3719494342803955, 'rewards/rejected': -8.338144302368164, 'rewards/accuracies': 1.0, 'rewards/margins': 4.9661946296691895, 'policy_logps/rejected': -371.5910949707031, 'policy_logps/chosen': -382.51318359375, 'referece_logps/rejected': -288.20965576171875, 'referece_logps/chosen': -348.793701171875, 'logits/rejected': -0.3243996202945709, 'logits/chosen': -0.36067208647727966, 'epoch': 5.87}

 65%|██████▌   | 10517/16110 [2:35:32<27:34:26, 17.75s/it]

 65%|██████▌   | 10518/16110 [2:35:46<26:00:02, 16.74s/it]

 65%|██████▌   | 10519/16110 [2:36:05<26:45:54, 17.23s/it]

 65%|██████▌   | 10520/16110 [2:36:18<25:00:32, 16.11s/it]
[2024-04-05 17:44:27,677] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 65%|██████▌   | 10522/16110 [2:36:57<28:00:37, 18.05s/it]
[2024-04-05 17:44:48,980] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10523/16110 [2:37:15<28:03:27, 18.08s/it]
[2024-04-05 17:45:07,138] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10524/16110 [2:37:37<29:39:52, 19.12s/it]
{'loss': 0.2247, 'learning_rate': 1.9999676635937883e-06, 'rewards/chosen': -3.869471788406372, 'rewards/rejected': -8.5496826171875, 'rewards/accuracies': 1.0, 'rewards/margins': 4.680211067199707, 'policy_logps/rejected': -381.26153564453125, 'policy_logps/chosen': -317.22900390625, 'referece_logps/rejected': -295.76470947265625, 'referece_logps/chosen': -278.5343017578125, 'logits/rejected': -0.32279127836227417, 'logits/chosen': -0.2549397051334381, 'epoch': 5.88}

 65%|██████▌   | 10525/16110 [2:37:58<30:24:00, 19.60s/it]
[2024-04-05 17:46:07,684] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10526/16110 [2:38:16<29:47:21, 19.21s/it]


 65%|██████▌   | 10528/16110 [2:38:48<26:47:47, 17.28s/it]

 65%|██████▌   | 10529/16110 [2:39:07<27:53:54, 18.00s/it]

 65%|██████▌   | 10530/16110 [2:39:25<27:53:51, 18.00s/it]
{'loss': 0.3035, 'learning_rate': 1.999957235177115e-06, 'rewards/chosen': -3.1740849018096924, 'rewards/rejected': -6.169682502746582, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9955973625183105, 'policy_logps/rejected': -289.03961181640625, 'policy_logps/chosen': -340.13134765625, 'referece_logps/rejected': -227.34280395507812, 'referece_logps/chosen': -308.3905029296875, 'logits/rejected': -0.5696374177932739, 'logits/chosen': -0.7130275964736938, 'epoch': 5.88}

 65%|██████▌   | 10531/16110 [2:39:46<29:12:42, 18.85s/it]
[2024-04-05 17:47:58,397] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10532/16110 [2:40:07<29:57:31, 19.34s/it]
[2024-04-05 17:48:16,141] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10533/16110 [2:40:24<29:12:50, 18.86s/it]

 65%|██████▌   | 10534/16110 [2:40:41<28:10:03, 18.19s/it]


 65%|██████▌   | 10536/16110 [2:41:09<25:20:15, 16.36s/it]

 65%|██████▌   | 10537/16110 [2:41:30<27:05:14, 17.50s/it]
{'loss': 0.1776, 'learning_rate': 1.999943229628037e-06, 'rewards/chosen': -3.597435712814331, 'rewards/rejected': -6.688018321990967, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0905823707580566, 'policy_logps/rejected': -373.1727600097656, 'policy_logps/chosen': -370.50836181640625, 'referece_logps/rejected': -306.2926025390625, 'referece_logps/chosen': -334.5340270996094, 'logits/rejected': -0.6435517072677612, 'logits/chosen': -0.6822218894958496, 'epoch': 5.89}
[2024-04-05 17:49:42,823] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 65%|██████▌   | 10539/16110 [2:42:11<29:42:32, 19.20s/it]

 65%|██████▌   | 10540/16110 [2:42:28<28:29:30, 18.41s/it]
{'loss': 0.2898, 'learning_rate': 1.9999366209717407e-06, 'rewards/chosen': -4.884642124176025, 'rewards/rejected': -6.739304542541504, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8546620607376099, 'policy_logps/rejected': -353.5531921386719, 'policy_logps/chosen': -437.133056640625, 'referece_logps/rejected': -286.16015625, 'referece_logps/chosen': -388.2866516113281, 'logits/rejected': 0.011866724118590355, 'logits/chosen': -0.12352810800075531, 'epoch': 5.89}
[2024-04-05 17:50:38,637] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10541/16110 [2:42:47<28:42:45, 18.56s/it]


 65%|██████▌   | 10543/16110 [2:43:28<30:04:55, 19.45s/it]

 65%|██████▌   | 10544/16110 [2:43:46<29:23:30, 19.01s/it]

 65%|██████▌   | 10545/16110 [2:44:02<28:15:56, 18.29s/it]
{'loss': 0.2409, 'learning_rate': 1.9999247981825763e-06, 'rewards/chosen': -4.586495876312256, 'rewards/rejected': -8.737929344177246, 'rewards/accuracies': 0.75, 'rewards/margins': 4.151433944702148, 'policy_logps/rejected': -317.68865966796875, 'policy_logps/chosen': -334.33673095703125, 'referece_logps/rejected': -230.30938720703125, 'referece_logps/chosen': -288.4717712402344, 'logits/rejected': 0.22258327901363373, 'logits/chosen': 0.1502525359392166, 'epoch': 5.89}


 65%|██████▌   | 10547/16110 [2:44:38<28:14:35, 18.28s/it]
[2024-04-05 17:52:29,388] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▌   | 10548/16110 [2:45:02<30:48:31, 19.94s/it]
{'loss': 0.3562, 'learning_rate': 1.9999172194960814e-06, 'rewards/chosen': -3.613656520843506, 'rewards/rejected': -6.945799350738525, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3321428298950195, 'policy_logps/rejected': -377.47576904296875, 'policy_logps/chosen': -361.8080749511719, 'referece_logps/rejected': -308.0177307128906, 'referece_logps/chosen': -325.6714782714844, 'logits/rejected': 0.06193119287490845, 'logits/chosen': -0.044449836015701294, 'epoch': 5.89}

 65%|██████▌   | 10549/16110 [2:45:23<31:34:25, 20.44s/it]
[2024-04-05 17:53:36,334] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 65%|██████▌   | 10551/16110 [2:46:02<30:22:20, 19.67s/it]
{'loss': 0.3136, 'learning_rate': 1.9999092770531803e-06, 'rewards/chosen': -4.692923069000244, 'rewards/rejected': -6.9190144538879395, 'rewards/accuracies': 0.875, 'rewards/margins': 2.226090908050537, 'policy_logps/rejected': -369.91021728515625, 'policy_logps/chosen': -272.55145263671875, 'referece_logps/rejected': -300.7200622558594, 'referece_logps/chosen': -225.6221923828125, 'logits/rejected': 0.2819020748138428, 'logits/chosen': 0.35582438111305237, 'epoch': 5.89}


 66%|██████▌   | 10553/16110 [2:46:46<32:13:32, 20.88s/it]

 66%|██████▌   | 10554/16110 [2:47:04<31:08:02, 20.17s/it]

 66%|██████▌   | 10555/16110 [2:47:24<30:51:03, 19.99s/it]
{'loss': 0.1566, 'learning_rate': 1.9998981212910295e-06, 'rewards/chosen': -2.7022321224212646, 'rewards/rejected': -7.706632137298584, 'rewards/accuracies': 1.0, 'rewards/margins': 5.00439977645874, 'policy_logps/rejected': -445.1834716796875, 'policy_logps/chosen': -412.096435546875, 'referece_logps/rejected': -368.11712646484375, 'referece_logps/chosen': -385.0741271972656, 'logits/rejected': -0.4703635573387146, 'logits/chosen': -0.2828909754753113, 'epoch': 5.9}

 66%|██████▌   | 10556/16110 [2:47:39<28:44:42, 18.63s/it]


 66%|██████▌   | 10558/16110 [2:48:14<27:42:25, 17.97s/it]
{'loss': 0.1723, 'learning_rate': 1.999889330094649e-06, 'rewards/chosen': -3.5271987915039062, 'rewards/rejected': -8.23896312713623, 'rewards/accuracies': 1.0, 'rewards/margins': 4.711764812469482, 'policy_logps/rejected': -560.4754638671875, 'policy_logps/chosen': -492.9642028808594, 'referece_logps/rejected': -478.0858154296875, 'referece_logps/chosen': -457.6922302246094, 'logits/rejected': -0.9584684371948242, 'logits/chosen': -0.8735562562942505, 'epoch': 5.9}


 66%|██████▌   | 10560/16110 [2:48:50<28:39:29, 18.59s/it]
{'loss': 0.2298, 'learning_rate': 1.999883267215604e-06, 'rewards/chosen': -5.16156005859375, 'rewards/rejected': -9.26011848449707, 'rewards/accuracies': 0.875, 'rewards/margins': 4.09855842590332, 'policy_logps/rejected': -661.818359375, 'policy_logps/chosen': -546.0379638671875, 'referece_logps/rejected': -569.2171630859375, 'referece_logps/chosen': -494.4223327636719, 'logits/rejected': -0.37147319316864014, 'logits/chosen': -0.4977804124355316, 'epoch': 5.9}

 66%|██████▌   | 10561/16110 [2:49:09<28:41:40, 18.62s/it]


 66%|██████▌   | 10563/16110 [2:49:48<29:43:18, 19.29s/it]

 66%|██████▌   | 10564/16110 [2:50:02<27:14:24, 17.68s/it]
{'loss': 0.259, 'learning_rate': 1.9998706564664407e-06, 'rewards/chosen': -5.364497661590576, 'rewards/rejected': -6.971280574798584, 'rewards/accuracies': 0.5, 'rewards/margins': 1.606783390045166, 'policy_logps/rejected': -411.66680908203125, 'policy_logps/chosen': -558.3894653320312, 'referece_logps/rejected': -341.9540100097656, 'referece_logps/chosen': -504.74444580078125, 'logits/rejected': 0.07866677641868591, 'logits/chosen': -0.05969572067260742, 'epoch': 5.9}
[2024-04-05 17:58:15,340] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10565/16110 [2:50:24<28:54:34, 18.77s/it]
[2024-04-05 17:58:29,496] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 66%|██████▌   | 10567/16110 [2:50:56<27:11:16, 17.66s/it]
{'loss': 0.193, 'learning_rate': 1.9998607740414063e-06, 'rewards/chosen': -4.684577941894531, 'rewards/rejected': -8.237586975097656, 'rewards/accuracies': 0.875, 'rewards/margins': 3.553008556365967, 'policy_logps/rejected': -402.6884765625, 'policy_logps/chosen': -538.283935546875, 'referece_logps/rejected': -320.3125915527344, 'referece_logps/chosen': -491.4381408691406, 'logits/rejected': -0.23166000843048096, 'logits/chosen': -0.5055751204490662, 'epoch': 5.9}

 66%|██████▌   | 10568/16110 [2:51:18<28:56:19, 18.80s/it]

 66%|██████▌   | 10569/16110 [2:51:35<28:17:31, 18.38s/it]

 66%|██████▌   | 10570/16110 [2:51:57<29:50:18, 19.39s/it]

 66%|██████▌   | 10571/16110 [2:52:16<29:37:10, 19.25s/it]

 66%|██████▌   | 10572/16110 [2:52:35<29:38:51, 19.27s/it]


 66%|██████▌   | 10574/16110 [2:53:11<28:46:48, 18.72s/it]

 66%|██████▌   | 10575/16110 [2:53:29<28:22:47, 18.46s/it]
{'loss': 0.1827, 'learning_rate': 1.999832642653907e-06, 'rewards/chosen': -3.8336477279663086, 'rewards/rejected': -9.42027473449707, 'rewards/accuracies': 1.0, 'rewards/margins': 5.586626052856445, 'policy_logps/rejected': -406.898681640625, 'policy_logps/chosen': -401.1882019042969, 'referece_logps/rejected': -312.6959228515625, 'referece_logps/chosen': -362.8516845703125, 'logits/rejected': 0.26123616099357605, 'logits/chosen': 0.24914085865020752, 'epoch': 5.91}

 66%|██████▌   | 10576/16110 [2:53:47<28:28:34, 18.52s/it]

 66%|██████▌   | 10577/16110 [2:54:09<30:08:27, 19.61s/it]


 66%|██████▌   | 10579/16110 [2:54:48<29:43:24, 19.35s/it]
{'loss': 0.2093, 'learning_rate': 1.9998176070204947e-06, 'rewards/chosen': -5.145174026489258, 'rewards/rejected': -7.543283462524414, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3981094360351562, 'policy_logps/rejected': -334.4591979980469, 'policy_logps/chosen': -302.15673828125, 'referece_logps/rejected': -259.0263977050781, 'referece_logps/chosen': -250.70498657226562, 'logits/rejected': -0.8612442016601562, 'logits/chosen': -0.8325179815292358, 'epoch': 5.91}

 66%|██████▌   | 10580/16110 [2:55:08<29:40:28, 19.32s/it]


 66%|██████▌   | 10582/16110 [2:55:41<28:06:21, 18.30s/it]

 66%|██████▌   | 10583/16110 [2:56:01<28:58:51, 18.88s/it]

 66%|██████▌   | 10584/16110 [2:56:25<31:12:49, 20.33s/it]

 66%|██████▌   | 10585/16110 [2:56:43<30:18:35, 19.75s/it]
{'loss': 0.2481, 'learning_rate': 1.9997938411730004e-06, 'rewards/chosen': -4.423295974731445, 'rewards/rejected': -9.054671287536621, 'rewards/accuracies': 0.75, 'rewards/margins': 4.631374835968018, 'policy_logps/rejected': -307.0284118652344, 'policy_logps/chosen': -232.86395263671875, 'referece_logps/rejected': -216.48171997070312, 'referece_logps/chosen': -188.63099670410156, 'logits/rejected': 0.027888525277376175, 'logits/chosen': 0.15965008735656738, 'epoch': 5.91}


 66%|██████▌   | 10587/16110 [2:57:21<30:03:09, 19.59s/it]
{'loss': 0.1172, 'learning_rate': 1.9997855959229663e-06, 'rewards/chosen': -3.7618629932403564, 'rewards/rejected': -6.644394397735596, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8825314044952393, 'policy_logps/rejected': -333.1126708984375, 'policy_logps/chosen': -295.37261962890625, 'referece_logps/rejected': -266.668701171875, 'referece_logps/chosen': -257.7540283203125, 'logits/rejected': -0.41306260228157043, 'logits/chosen': -0.419059157371521, 'epoch': 5.91}

 66%|██████▌   | 10588/16110 [2:57:38<29:00:06, 18.91s/it]

 66%|██████▌   | 10589/16110 [2:57:56<28:28:16, 18.56s/it]

 66%|██████▌   | 10590/16110 [2:58:17<29:43:29, 19.39s/it]

 66%|██████▌   | 10591/16110 [2:58:39<30:50:55, 20.12s/it]

 66%|██████▌   | 10592/16110 [2:59:03<32:19:04, 21.08s/it]


 66%|██████▌   | 10594/16110 [2:59:39<30:26:33, 19.87s/it]

 66%|██████▌   | 10595/16110 [2:59:55<28:42:12, 18.74s/it]

 66%|██████▌   | 10596/16110 [3:00:17<30:05:18, 19.64s/it]
[2024-04-05 18:08:08,474] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1745, 'learning_rate': 1.9997464919207658e-06, 'rewards/chosen': -4.0039873123168945, 'rewards/rejected': -7.112626552581787, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1086392402648926, 'policy_logps/rejected': -419.16607666015625, 'policy_logps/chosen': -505.7751770019531, 'referece_logps/rejected': -348.0398254394531, 'referece_logps/chosen': -465.7352600097656, 'logits/rejected': 0.7195538282394409, 'logits/chosen': 0.7516533136367798, 'epoch': 5.92}


 66%|██████▌   | 10598/16110 [3:00:51<28:16:01, 18.46s/it]
[2024-04-05 18:08:42,528] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1848, 'learning_rate': 1.9997373576226382e-06, 'rewards/chosen': -5.045305252075195, 'rewards/rejected': -7.493733882904053, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4484286308288574, 'policy_logps/rejected': -475.390625, 'policy_logps/chosen': -421.01922607421875, 'referece_logps/rejected': -400.4532775878906, 'referece_logps/chosen': -370.56622314453125, 'logits/rejected': 0.4047725200653076, 'logits/chosen': 0.3624745309352875, 'epoch': 5.92}


 66%|██████▌   | 10600/16110 [3:01:27<28:23:38, 18.55s/it]
[2024-04-05 18:09:18,556] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10601/16110 [3:01:43<27:18:02, 17.84s/it]
{'loss': 0.1777, 'learning_rate': 1.999723353100099e-06, 'rewards/chosen': -4.783364772796631, 'rewards/rejected': -13.296289443969727, 'rewards/accuracies': 1.0, 'rewards/margins': 8.512923240661621, 'policy_logps/rejected': -427.75103759765625, 'policy_logps/chosen': -490.62164306640625, 'referece_logps/rejected': -294.78814697265625, 'referece_logps/chosen': -442.7879333496094, 'logits/rejected': -0.5549842119216919, 'logits/chosen': -0.9867094159126282, 'epoch': 5.92}


 66%|██████▌   | 10603/16110 [3:02:15<26:16:36, 17.18s/it]
{'loss': 0.1057, 'learning_rate': 1.9997138147037086e-06, 'rewards/chosen': -4.4179368019104, 'rewards/rejected': -8.485465049743652, 'rewards/accuracies': 1.0, 'rewards/margins': 4.067528247833252, 'policy_logps/rejected': -278.729248046875, 'policy_logps/chosen': -434.54888916015625, 'referece_logps/rejected': -193.87457275390625, 'referece_logps/chosen': -390.3694763183594, 'logits/rejected': -0.14101868867874146, 'logits/chosen': -0.4409123659133911, 'epoch': 5.92}


 66%|██████▌   | 10605/16110 [3:02:57<29:12:17, 19.10s/it]

 66%|██████▌   | 10606/16110 [3:03:15<28:50:13, 18.86s/it]
{'loss': 0.1247, 'learning_rate': 1.9996992040409335e-06, 'rewards/chosen': -3.853525400161743, 'rewards/rejected': -8.016456604003906, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1629319190979, 'policy_logps/rejected': -487.7132568359375, 'policy_logps/chosen': -599.119384765625, 'referece_logps/rejected': -407.54864501953125, 'referece_logps/chosen': -560.5841674804688, 'logits/rejected': 0.3012995421886444, 'logits/chosen': 0.1863613724708557, 'epoch': 5.93}

 66%|██████▌   | 10607/16110 [3:03:35<29:25:47, 19.25s/it]

 66%|██████▌   | 10608/16110 [3:03:51<27:36:49, 18.07s/it]


 66%|██████▌   | 10610/16110 [3:04:25<26:34:09, 17.39s/it]
{'loss': 0.2531, 'learning_rate': 1.9996791574382466e-06, 'rewards/chosen': -3.9771201610565186, 'rewards/rejected': -9.360164642333984, 'rewards/accuracies': 0.875, 'rewards/margins': 5.383044719696045, 'policy_logps/rejected': -365.86944580078125, 'policy_logps/chosen': -334.28851318359375, 'referece_logps/rejected': -272.26776123046875, 'referece_logps/chosen': -294.517333984375, 'logits/rejected': -0.40298691391944885, 'logits/chosen': -0.41851288080215454, 'epoch': 5.93}

 66%|██████▌   | 10611/16110 [3:04:42<26:19:16, 17.23s/it]

 66%|██████▌   | 10612/16110 [3:05:03<27:58:46, 18.32s/it]

 66%|██████▌   | 10613/16110 [3:05:20<27:39:45, 18.12s/it]

 66%|██████▌   | 10614/16110 [3:05:37<26:57:30, 17.66s/it]


 66%|██████▌   | 10616/16110 [3:06:15<28:05:55, 18.41s/it]
{'loss': 0.1466, 'learning_rate': 1.9996478753057404e-06, 'rewards/chosen': -4.731203556060791, 'rewards/rejected': -9.016270637512207, 'rewards/accuracies': 1.0, 'rewards/margins': 4.285066604614258, 'policy_logps/rejected': -363.7095947265625, 'policy_logps/chosen': -484.5068664550781, 'referece_logps/rejected': -273.54693603515625, 'referece_logps/chosen': -437.1948547363281, 'logits/rejected': -0.09484604746103287, 'logits/chosen': -0.2708909213542938, 'epoch': 5.93}

 66%|██████▌   | 10617/16110 [3:06:39<30:21:31, 19.90s/it]
[2024-04-05 18:14:51,949] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10618/16110 [3:07:00<31:11:06, 20.44s/it]

 66%|██████▌   | 10619/16110 [3:07:22<31:36:59, 20.73s/it]
[2024-04-05 18:15:38,140] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 66%|██████▌   | 10621/16110 [3:08:07<32:59:25, 21.64s/it]
{'loss': 0.1403, 'learning_rate': 1.999620695681581e-06, 'rewards/chosen': -3.520620107650757, 'rewards/rejected': -8.287025451660156, 'rewards/accuracies': 1.0, 'rewards/margins': 4.76640510559082, 'policy_logps/rejected': -363.3619079589844, 'policy_logps/chosen': -482.7373046875, 'referece_logps/rejected': -280.4916687011719, 'referece_logps/chosen': -447.5310974121094, 'logits/rejected': -0.2106539011001587, 'logits/chosen': -0.5425812005996704, 'epoch': 5.93}

 66%|██████▌   | 10622/16110 [3:08:18<28:01:53, 18.39s/it]

 66%|██████▌   | 10623/16110 [3:08:39<29:06:27, 19.10s/it]

 66%|██████▌   | 10624/16110 [3:08:50<25:36:20, 16.80s/it]

 66%|██████▌   | 10625/16110 [3:09:04<24:06:02, 15.82s/it]

 66%|██████▌   | 10626/16110 [3:09:15<21:46:51, 14.30s/it]

 66%|██████▌   | 10627/16110 [3:09:35<24:34:17, 16.13s/it]

 66%|██████▌   | 10628/16110 [3:09:55<26:20:31, 17.30s/it]


 66%|██████▌   | 10630/16110 [3:10:34<27:30:17, 18.07s/it]

 66%|██████▌   | 10631/16110 [3:10:54<28:25:22, 18.68s/it]
{'loss': 0.2882, 'learning_rate': 1.9995633060574652e-06, 'rewards/chosen': -4.885278701782227, 'rewards/rejected': -6.421725749969482, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5364465713500977, 'policy_logps/rejected': -478.0877685546875, 'policy_logps/chosen': -424.9384460449219, 'referece_logps/rejected': -413.8705139160156, 'referece_logps/chosen': -376.0856628417969, 'logits/rejected': 0.20450282096862793, 'logits/chosen': 0.29826247692108154, 'epoch': 5.94}


 66%|██████▌   | 10633/16110 [3:11:32<28:23:49, 18.67s/it]

 66%|██████▌   | 10634/16110 [3:11:48<27:13:26, 17.90s/it]

 66%|██████▌   | 10635/16110 [3:12:08<28:04:07, 18.46s/it]

 66%|██████▌   | 10636/16110 [3:12:25<27:46:17, 18.26s/it]
{'loss': 0.3187, 'learning_rate': 1.999533096115502e-06, 'rewards/chosen': -4.592494964599609, 'rewards/rejected': -7.369508743286133, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7770137786865234, 'policy_logps/rejected': -503.9986877441406, 'policy_logps/chosen': -428.18701171875, 'referece_logps/rejected': -430.3035888671875, 'referece_logps/chosen': -382.2620849609375, 'logits/rejected': -0.562088131904602, 'logits/chosen': -0.5239115953445435, 'epoch': 5.94}

 66%|██████▌   | 10637/16110 [3:12:44<27:55:22, 18.37s/it]

 66%|██████▌   | 10638/16110 [3:13:02<27:55:05, 18.37s/it]
[2024-04-05 18:21:15,080] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10639/16110 [3:13:23<29:06:54, 19.16s/it]
[2024-04-05 18:21:36,183] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10640/16110 [3:13:45<29:59:46, 19.74s/it]
[2024-04-05 18:21:58,827] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10641/16110 [3:14:07<31:18:47, 20.61s/it]
[2024-04-05 18:22:17,813] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10642/16110 [3:14:26<30:34:00, 20.12s/it]

 66%|██████▌   | 10643/16110 [3:14:43<29:12:53, 19.24s/it]


 66%|██████▌   | 10645/16110 [3:15:24<30:12:56, 19.90s/it]
[2024-04-05 18:23:15,637] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1409, 'learning_rate': 1.999476172924609e-06, 'rewards/chosen': -6.225805282592773, 'rewards/rejected': -9.639179229736328, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4133737087249756, 'policy_logps/rejected': -568.8946533203125, 'policy_logps/chosen': -651.0001220703125, 'referece_logps/rejected': -472.5028076171875, 'referece_logps/chosen': -588.7420654296875, 'logits/rejected': 0.09888273477554321, 'logits/chosen': -0.12576279044151306, 'epoch': 5.95}
[2024-04-05 18:23:32,643] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 10646/16110 [3:15:41<28:53:24, 19.03s/it]


 66%|██████▌   | 10648/16110 [3:16:10<25:32:26, 16.83s/it]

 66%|██████▌   | 10649/16110 [3:16:28<25:57:08, 17.11s/it]

 66%|██████▌   | 10650/16110 [3:16:40<23:47:49, 15.69s/it]
{'loss': 0.2293, 'learning_rate': 1.9994431349333603e-06, 'rewards/chosen': -3.8903582096099854, 'rewards/rejected': -7.247701168060303, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3573434352874756, 'policy_logps/rejected': -344.74591064453125, 'policy_logps/chosen': -417.7698669433594, 'referece_logps/rejected': -272.2689208984375, 'referece_logps/chosen': -378.8663330078125, 'logits/rejected': 0.14690884947776794, 'logits/chosen': 0.11592984199523926, 'epoch': 5.95}

 66%|██████▌   | 10651/16110 [3:16:59<25:02:50, 16.52s/it]

 66%|██████▌   | 10652/16110 [3:17:12<23:27:41, 15.47s/it]

 66%|██████▌   | 10653/16110 [3:17:32<25:50:30, 17.05s/it]


 66%|██████▌   | 10655/16110 [3:18:00<23:42:54, 15.65s/it]

 66%|██████▌   | 10656/16110 [3:18:14<23:08:14, 15.27s/it]

 66%|██████▌   | 10657/16110 [3:18:34<25:08:40, 16.60s/it]
{'loss': 0.2302, 'learning_rate': 1.999395185028326e-06, 'rewards/chosen': -4.79885721206665, 'rewards/rejected': -6.904815673828125, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1059587001800537, 'policy_logps/rejected': -390.60260009765625, 'policy_logps/chosen': -376.134765625, 'referece_logps/rejected': -321.554443359375, 'referece_logps/chosen': -328.1462097167969, 'logits/rejected': -0.35055527091026306, 'logits/chosen': -0.36024534702301025, 'epoch': 5.95}


 66%|██████▌   | 10659/16110 [3:19:08<25:55:03, 17.12s/it]
{'loss': 0.2057, 'learning_rate': 1.999381121484761e-06, 'rewards/chosen': -2.8522331714630127, 'rewards/rejected': -8.383423805236816, 'rewards/accuracies': 0.875, 'rewards/margins': 5.531190872192383, 'policy_logps/rejected': -369.9985046386719, 'policy_logps/chosen': -299.8895568847656, 'referece_logps/rejected': -286.1642761230469, 'referece_logps/chosen': -271.3672180175781, 'logits/rejected': 0.1152724027633667, 'logits/chosen': 0.36527952551841736, 'epoch': 5.95}
[2024-04-05 18:27:20,433] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 66%|██████▌   | 10661/16110 [3:19:50<29:07:31, 19.24s/it]
{'loss': 0.3256, 'learning_rate': 1.9993668963583573e-06, 'rewards/chosen': -3.449157953262329, 'rewards/rejected': -7.995841979980469, 'rewards/accuracies': 0.875, 'rewards/margins': 4.5466837882995605, 'policy_logps/rejected': -448.6342468261719, 'policy_logps/chosen': -379.57684326171875, 'referece_logps/rejected': -368.67578125, 'referece_logps/chosen': -345.0852355957031, 'logits/rejected': -0.513099730014801, 'logits/chosen': -0.47377681732177734, 'epoch': 5.96}

 66%|██████▌   | 10662/16110 [3:20:11<29:31:51, 19.51s/it]

 66%|██████▌   | 10663/16110 [3:20:31<29:46:19, 19.68s/it]

 66%|██████▌   | 10664/16110 [3:20:55<32:05:17, 21.21s/it]

 66%|██████▌   | 10665/16110 [3:21:17<32:19:18, 21.37s/it]

 66%|██████▌   | 10666/16110 [3:21:37<31:36:45, 20.90s/it]

 66%|██████▌   | 10667/16110 [3:21:57<31:07:13, 20.58s/it]

 66%|██████▌   | 10668/16110 [3:22:16<30:27:02, 20.14s/it]

 66%|██████▌   | 10669/16110 [3:22:39<31:41:51, 20.97s/it]

 66%|██████▌   | 10670/16110 [3:22:53<28:36:16, 18.93s/it]

 66%|██████▌   | 10671/16110 [3:23:09<27:12:25, 18.01s/it]

 66%|██████▌   | 10672/16110 [3:23:30<28:44:42, 19.03s/it]

 66%|██████▋   | 10673/16110 [3:23:50<29:11:39, 19.33s/it]

 66%|██████▋   | 10674/16110 [3:24:07<28:01:34, 18.56s/it]

 66%|██████▋   | 10675/16110 [3:24:26<28:00:43, 18.55s/it]

 66%|██████▋   | 10676/16110 [3:24:47<29:16:42, 19.40s/it]
[2024-04-05 18:33:00,303] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10677/16110 [3:25:09<30:19:44, 20.10s/it]

 66%|██████▋   | 10678/16110 [3:25:28<30:10:20, 20.00s/it]

 66%|██████▋   | 10679/16110 [3:25:50<31:04:40, 20.60s/it]

 66%|██████▋   | 10680/16110 [3:26:09<29:58:37, 19.87s/it]

 66%|██████▋   | 10681/16110 [3:26:19<25:50:14, 17.13s/it]

 66%|██████▋   | 10682/16110 [3:26:35<25:02:02, 16.60s/it]

 66%|██████▋   | 10683/16110 [3:26:55<26:39:42, 17.69s/it]

 66%|██████▋   | 10684/16110 [3:27:12<26:33:03, 17.62s/it]

 66%|██████▋   | 10685/16110 [3:27:32<27:36:57, 18.33s/it]

 66%|██████▋   | 10686/16110 [3:27:51<27:45:27, 18.42s/it]

 66%|██████▋   | 10687/16110 [3:28:09<27:42:45, 18.40s/it]

 66%|██████▋   | 10688/16110 [3:28:32<29:33:45, 19.63s/it]
[2024-04-05 18:36:44,023] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10689/16110 [3:28:52<29:57:49, 19.90s/it]

 66%|██████▋   | 10690/16110 [3:29:14<30:47:56, 20.46s/it]

 66%|██████▋   | 10691/16110 [3:29:38<32:20:21, 21.48s/it]
[2024-04-05 18:37:52,204] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10692/16110 [3:30:01<32:48:36, 21.80s/it]

 66%|██████▋   | 10693/16110 [3:30:20<31:44:45, 21.10s/it]
[2024-04-05 18:38:30,218] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10694/16110 [3:30:39<30:35:36, 20.34s/it]

 66%|██████▋   | 10695/16110 [3:30:54<28:28:29, 18.93s/it]

 66%|██████▋   | 10696/16110 [3:31:09<26:34:08, 17.67s/it]
[2024-04-05 18:39:22,677] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10697/16110 [3:31:31<28:33:30, 18.99s/it]

 66%|██████▋   | 10698/16110 [3:31:52<29:21:24, 19.53s/it]

 66%|██████▋   | 10699/16110 [3:32:13<30:14:09, 20.12s/it]

 66%|██████▋   | 10700/16110 [3:32:31<29:18:38, 19.50s/it]
[2024-04-05 18:40:23,018] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10701/16110 [3:32:49<28:14:19, 18.79s/it]

 66%|██████▋   | 10702/16110 [3:33:05<27:11:23, 18.10s/it]

 66%|██████▋   | 10703/16110 [3:33:29<29:39:46, 19.75s/it]

 66%|██████▋   | 10704/16110 [3:33:46<28:44:14, 19.14s/it]

 66%|██████▋   | 10705/16110 [3:34:04<28:01:24, 18.67s/it]

 66%|██████▋   | 10706/16110 [3:34:26<29:22:31, 19.57s/it]

 66%|██████▋   | 10707/16110 [3:34:40<27:08:59, 18.09s/it]

 66%|██████▋   | 10708/16110 [3:35:03<29:03:37, 19.37s/it]
[2024-04-05 18:42:54,167] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▋   | 10709/16110 [3:35:21<28:43:33, 19.15s/it]

 66%|██████▋   | 10710/16110 [3:35:33<25:25:26, 16.95s/it]
{'loss': 0.2922, 'learning_rate': 1.998967913092187e-06, 'rewards/chosen': -5.932183265686035, 'rewards/rejected': -8.212215423583984, 'rewards/accuracies': 1.0, 'rewards/margins': 2.280031681060791, 'policy_logps/rejected': -431.31475830078125, 'policy_logps/chosen': -414.7297668457031, 'referece_logps/rejected': -349.1925354003906, 'referece_logps/chosen': -355.407958984375, 'logits/rejected': -0.10146579146385193, 'logits/chosen': -0.10650566220283508, 'epoch': 5.98}


 66%|██████▋   | 10712/16110 [3:36:14<28:17:12, 18.86s/it]

 66%|██████▋   | 10713/16110 [3:36:35<29:11:34, 19.47s/it]

 67%|██████▋   | 10714/16110 [3:36:54<28:43:46, 19.17s/it]
{'loss': 0.2224, 'learning_rate': 1.998931062299928e-06, 'rewards/chosen': -3.7206101417541504, 'rewards/rejected': -5.919326305389404, 'rewards/accuracies': 1.0, 'rewards/margins': 2.198716402053833, 'policy_logps/rejected': -275.1750793457031, 'policy_logps/chosen': -345.7747802734375, 'referece_logps/rejected': -215.9818115234375, 'referece_logps/chosen': -308.56866455078125, 'logits/rejected': -0.14910468459129333, 'logits/chosen': 0.04552130401134491, 'epoch': 5.99}


 67%|██████▋   | 10716/16110 [3:37:35<29:52:30, 19.94s/it]

 67%|██████▋   | 10717/16110 [3:37:48<26:42:27, 17.83s/it]

 67%|██████▋   | 10718/16110 [3:38:12<29:11:30, 19.49s/it]

 67%|██████▋   | 10719/16110 [3:38:30<28:46:12, 19.21s/it]
{'loss': 0.1108, 'learning_rate': 1.9988840903183223e-06, 'rewards/chosen': -3.654482364654541, 'rewards/rejected': -6.4561076164245605, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8016250133514404, 'policy_logps/rejected': -357.0728759765625, 'policy_logps/chosen': -453.5628662109375, 'referece_logps/rejected': -292.5118103027344, 'referece_logps/chosen': -417.01800537109375, 'logits/rejected': -0.23253655433654785, 'logits/chosen': -0.3800818622112274, 'epoch': 5.99}


 67%|██████▋   | 10721/16110 [3:39:03<26:58:37, 18.02s/it]

 67%|██████▋   | 10722/16110 [3:39:20<26:45:28, 17.88s/it]

 67%|██████▋   | 10723/16110 [3:39:37<25:56:39, 17.34s/it]

 67%|██████▋   | 10724/16110 [3:39:53<25:20:26, 16.94s/it]

 67%|██████▋   | 10725/16110 [3:40:13<26:50:23, 17.94s/it]

 67%|██████▋   | 10726/16110 [3:40:29<26:08:33, 17.48s/it]
{'loss': 0.1493, 'learning_rate': 1.998816633779015e-06, 'rewards/chosen': -4.7367987632751465, 'rewards/rejected': -8.050889015197754, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3140900135040283, 'policy_logps/rejected': -503.74578857421875, 'policy_logps/chosen': -399.783447265625, 'referece_logps/rejected': -423.23687744140625, 'referece_logps/chosen': -352.4154052734375, 'logits/rejected': 0.39223551750183105, 'logits/chosen': 0.5494431257247925, 'epoch': 5.99}

 67%|██████▋   | 10727/16110 [3:40:48<26:36:01, 17.79s/it]


 67%|██████▋   | 10729/16110 [3:41:30<29:19:02, 19.61s/it]

 67%|██████▋   | 10730/16110 [3:41:49<28:52:07, 19.32s/it]

 67%|██████▋   | 10731/16110 [3:42:07<28:11:10, 18.86s/it]

 67%|██████▋   | 10732/16110 [3:42:27<28:45:28, 19.25s/it]

 67%|██████▋   | 10733/16110 [3:42:45<28:25:55, 19.04s/it]

 67%|██████▋   | 10734/16110 [3:43:07<29:32:31, 19.78s/it]

 67%|██████▋   | 10735/16110 [3:43:22<27:34:28, 18.47s/it]

 67%|██████▋   | 10736/16110 [3:43:44<28:58:12, 19.41s/it]

 67%|██████▋   | 10737/16110 [3:44:05<29:46:23, 19.95s/it]

 67%|██████▋   | 10738/16110 [3:44:19<27:12:55, 18.24s/it]

 67%|██████▋   | 10739/16110 [3:44:36<26:24:11, 17.70s/it]

 67%|██████▋   | 10740/16110 [3:44:53<26:19:43, 17.65s/it]

 67%|██████▋   | 10741/16110 [3:45:05<23:40:55, 15.88s/it]
{'loss': 0.1669, 'learning_rate': 1.998665422702262e-06, 'rewards/chosen': -4.613077163696289, 'rewards/rejected': -7.198565483093262, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5854878425598145, 'policy_logps/rejected': -400.9229431152344, 'policy_logps/chosen': -395.1753234863281, 'referece_logps/rejected': -328.93731689453125, 'referece_logps/chosen': -349.0445861816406, 'logits/rejected': 0.5907418727874756, 'logits/chosen': 0.7507941722869873, 'epoch': 6.0}


 67%|██████▋   | 10743/16110 [3:45:40<25:27:12, 17.07s/it]

 67%|██████▋   | 10744/16110 [3:45:54<24:04:36, 16.15s/it]

 67%|██████▋   | 10745/16110 [3:46:13<25:18:01, 16.98s/it]

 67%|██████▋   | 10746/16110 [3:46:34<26:44:54, 17.95s/it]

 67%|██████▋   | 10747/16110 [3:46:54<27:43:22, 18.61s/it]

 67%|██████▋   | 10748/16110 [3:47:14<28:17:57, 19.00s/it]

 67%|██████▋   | 10749/16110 [3:47:34<28:46:55, 19.33s/it]

 67%|██████▋   | 10750/16110 [3:47:54<29:12:51, 19.62s/it]

 67%|██████▋   | 10751/16110 [3:48:08<26:35:11, 17.86s/it]

 67%|██████▋   | 10752/16110 [3:48:21<24:38:22, 16.56s/it]

 67%|██████▋   | 10753/16110 [3:48:35<23:13:13, 15.60s/it]

 67%|██████▋   | 10754/16110 [3:48:55<25:20:25, 17.03s/it]

 67%|██████▋   | 10755/16110 [3:49:08<23:19:58, 15.69s/it]

 67%|██████▋   | 10756/16110 [3:49:19<21:25:54, 14.41s/it]

 67%|██████▋   | 10757/16110 [3:49:35<22:09:31, 14.90s/it]

 67%|██████▋   | 10758/16110 [3:49:57<25:10:05, 16.93s/it]

 67%|██████▋   | 10759/16110 [3:50:18<26:57:29, 18.14s/it]

 67%|██████▋   | 10760/16110 [3:50:32<25:14:07, 16.98s/it]

 67%|██████▋   | 10761/16110 [3:50:53<27:00:08, 18.17s/it]

 67%|██████▋   | 10762/16110 [3:51:13<27:47:04, 18.70s/it]

 67%|██████▋   | 10763/16110 [3:51:33<28:38:00, 19.28s/it]

 67%|██████▋   | 10764/16110 [3:51:53<28:51:36, 19.43s/it]

 67%|██████▋   | 10765/16110 [3:52:08<26:49:39, 18.07s/it]

 67%|██████▋   | 10766/16110 [3:52:25<26:10:56, 17.64s/it]

 67%|██████▋   | 10767/16110 [3:52:37<23:34:26, 15.88s/it]

 67%|██████▋   | 10768/16110 [3:52:53<23:50:37, 16.07s/it]

 67%|██████▋   | 10769/16110 [3:53:11<24:50:58, 16.75s/it]

 67%|██████▋   | 10770/16110 [3:53:29<25:08:22, 16.95s/it]

 67%|██████▋   | 10771/16110 [3:53:47<25:35:38, 17.26s/it]

 67%|██████▋   | 10772/16110 [3:54:01<24:25:01, 16.47s/it]

 67%|██████▋   | 10773/16110 [3:54:17<23:58:35, 16.17s/it]

 67%|██████▋   | 10774/16110 [3:54:36<25:16:18, 17.05s/it]

 67%|██████▋   | 10775/16110 [3:54:50<23:50:52, 16.09s/it]

 67%|██████▋   | 10776/16110 [3:55:10<25:47:54, 17.41s/it]

 67%|██████▋   | 10777/16110 [3:55:31<27:01:22, 18.24s/it]

 67%|██████▋   | 10778/16110 [3:55:51<28:09:35, 19.01s/it]

 67%|██████▋   | 10779/16110 [3:56:14<29:52:21, 20.17s/it]
{'loss': 0.2273, 'learning_rate': 1.9982417086834216e-06, 'rewards/chosen': -5.05869722366333, 'rewards/rejected': -9.677562713623047, 'rewards/accuracies': 0.875, 'rewards/margins': 4.618865013122559, 'policy_logps/rejected': -413.31207275390625, 'policy_logps/chosen': -354.6902160644531, 'referece_logps/rejected': -316.5364685058594, 'referece_logps/chosen': -304.1032409667969, 'logits/rejected': -0.0023247599601745605, 'logits/chosen': 0.0337095782160759, 'epoch': 6.02}


 67%|██████▋   | 10781/16110 [3:56:42<25:06:37, 16.96s/it]

 67%|██████▋   | 10782/16110 [3:56:58<25:03:26, 16.93s/it]

 67%|██████▋   | 10783/16110 [3:57:18<26:16:54, 17.76s/it]
{'loss': 0.1205, 'learning_rate': 1.998193717393159e-06, 'rewards/chosen': -4.197838306427002, 'rewards/rejected': -8.38665771484375, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1888203620910645, 'policy_logps/rejected': -492.3240051269531, 'policy_logps/chosen': -290.0164794921875, 'referece_logps/rejected': -408.4573974609375, 'referece_logps/chosen': -248.03810119628906, 'logits/rejected': -0.5658961534500122, 'logits/chosen': -0.5008516907691956, 'epoch': 6.02}


 67%|██████▋   | 10785/16110 [3:57:56<27:15:35, 18.43s/it]
{'loss': 0.2189, 'learning_rate': 1.998169479659798e-06, 'rewards/chosen': -3.2513129711151123, 'rewards/rejected': -6.5254058837890625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.27409291267395, 'policy_logps/rejected': -242.11590576171875, 'policy_logps/chosen': -325.7984924316406, 'referece_logps/rejected': -176.86184692382812, 'referece_logps/chosen': -293.28533935546875, 'logits/rejected': -0.009459689259529114, 'logits/chosen': -0.044489286839962006, 'epoch': 6.03}

 67%|██████▋   | 10786/16110 [3:58:17<28:19:23, 19.15s/it]

 67%|██████▋   | 10787/16110 [3:58:33<26:59:23, 18.25s/it]

 67%|██████▋   | 10788/16110 [3:58:53<27:43:19, 18.75s/it]

 67%|██████▋   | 10789/16110 [3:59:07<25:41:10, 17.38s/it]


 67%|██████▋   | 10791/16110 [3:59:36<22:49:22, 15.45s/it]

 67%|██████▋   | 10792/16110 [3:59:46<20:26:22, 13.84s/it]
{'loss': 0.1342, 'learning_rate': 1.9980833766968644e-06, 'rewards/chosen': -4.332837104797363, 'rewards/rejected': -8.621028900146484, 'rewards/accuracies': 0.75, 'rewards/margins': 4.288192272186279, 'policy_logps/rejected': -470.9595642089844, 'policy_logps/chosen': -367.86663818359375, 'referece_logps/rejected': -384.749267578125, 'referece_logps/chosen': -324.5382385253906, 'logits/rejected': -0.0784049779176712, 'logits/chosen': 0.1166597306728363, 'epoch': 6.03}


 67%|██████▋   | 10794/16110 [4:00:15<21:20:13, 14.45s/it]

 67%|██████▋   | 10795/16110 [4:00:26<20:07:25, 13.63s/it]

 67%|██████▋   | 10796/16110 [4:00:40<19:54:42, 13.49s/it]
{'loss': 0.1404, 'learning_rate': 1.9980332874412118e-06, 'rewards/chosen': -3.8091020584106445, 'rewards/rejected': -7.900986671447754, 'rewards/accuracies': 1.0, 'rewards/margins': 4.091884613037109, 'policy_logps/rejected': -472.6069030761719, 'policy_logps/chosen': -475.5118713378906, 'referece_logps/rejected': -393.5970458984375, 'referece_logps/chosen': -437.42083740234375, 'logits/rejected': 0.7426098585128784, 'logits/chosen': 0.7179476618766785, 'epoch': 6.03}


 67%|██████▋   | 10798/16110 [4:01:22<25:55:03, 17.56s/it]

 67%|██████▋   | 10799/16110 [4:01:42<26:55:59, 18.26s/it]

 67%|██████▋   | 10800/16110 [4:02:00<26:51:23, 18.21s/it]
{'loss': 0.1859, 'learning_rate': 1.997982552725919e-06, 'rewards/chosen': -2.7733731269836426, 'rewards/rejected': -6.148276329040527, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3749029636383057, 'policy_logps/rejected': -296.8193359375, 'policy_logps/chosen': -272.2457275390625, 'referece_logps/rejected': -235.33656311035156, 'referece_logps/chosen': -244.51199340820312, 'logits/rejected': -0.44119930267333984, 'logits/chosen': -0.4417293667793274, 'epoch': 6.03}
[2024-04-05 19:10:13,228] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10801/16110 [4:02:22<28:20:43, 19.22s/it]
[2024-04-05 19:10:32,840] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 10803/16110 [4:02:59<27:49:38, 18.88s/it]

 67%|██████▋   | 10804/16110 [4:03:19<28:20:15, 19.23s/it]
{'loss': 0.1506, 'learning_rate': 1.9979311725837984e-06, 'rewards/chosen': -4.14517068862915, 'rewards/rejected': -9.348929405212402, 'rewards/accuracies': 1.0, 'rewards/margins': 5.2037577629089355, 'policy_logps/rejected': -484.3199462890625, 'policy_logps/chosen': -443.2038269042969, 'referece_logps/rejected': -390.8306579589844, 'referece_logps/chosen': -401.7521667480469, 'logits/rejected': 0.06523323059082031, 'logits/chosen': -0.01967516541481018, 'epoch': 6.04}

 67%|██████▋   | 10805/16110 [4:03:40<28:55:05, 19.62s/it]

 67%|██████▋   | 10806/16110 [4:04:02<29:58:37, 20.35s/it]


 67%|██████▋   | 10808/16110 [4:04:38<28:07:31, 19.10s/it]

 67%|██████▋   | 10809/16110 [4:04:58<28:33:06, 19.39s/it]
{'loss': 0.2032, 'learning_rate': 1.997866039825333e-06, 'rewards/chosen': -5.093022346496582, 'rewards/rejected': -7.088399410247803, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9953773021697998, 'policy_logps/rejected': -324.5408935546875, 'policy_logps/chosen': -341.3445739746094, 'referece_logps/rejected': -253.6569061279297, 'referece_logps/chosen': -290.41436767578125, 'logits/rejected': -0.48562073707580566, 'logits/chosen': -0.5678714513778687, 'epoch': 6.04}


 67%|██████▋   | 10811/16110 [4:05:35<28:02:20, 19.05s/it]

 67%|██████▋   | 10812/16110 [4:05:55<28:12:44, 19.17s/it]
{'loss': 0.2092, 'learning_rate': 1.9978264761524067e-06, 'rewards/chosen': -3.600470542907715, 'rewards/rejected': -7.4658708572387695, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8654000759124756, 'policy_logps/rejected': -240.4647674560547, 'policy_logps/chosen': -225.38034057617188, 'referece_logps/rejected': -165.8060760498047, 'referece_logps/chosen': -189.37564086914062, 'logits/rejected': -0.3543805480003357, 'logits/chosen': -0.39480358362197876, 'epoch': 6.04}


 67%|██████▋   | 10814/16110 [4:06:34<28:46:27, 19.56s/it]

 67%|██████▋   | 10815/16110 [4:06:54<29:04:25, 19.77s/it]

 67%|██████▋   | 10816/16110 [4:07:15<29:36:27, 20.13s/it]
[2024-04-05 19:15:06,940] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10817/16110 [4:07:31<27:35:48, 18.77s/it]
{'loss': 0.1817, 'learning_rate': 1.997759730047318e-06, 'rewards/chosen': -4.297593593597412, 'rewards/rejected': -7.094865798950195, 'rewards/accuracies': 1.0, 'rewards/margins': 2.797271966934204, 'policy_logps/rejected': -305.02703857421875, 'policy_logps/chosen': -330.1370544433594, 'referece_logps/rejected': -234.07838439941406, 'referece_logps/chosen': -287.1611328125, 'logits/rejected': -0.14194586873054504, 'logits/chosen': -0.1745675504207611, 'epoch': 6.04}


 67%|██████▋   | 10819/16110 [4:08:08<27:46:48, 18.90s/it]

 67%|██████▋   | 10820/16110 [4:08:27<27:37:28, 18.80s/it]

 67%|██████▋   | 10821/16110 [4:08:39<24:32:24, 16.70s/it]

 67%|██████▋   | 10822/16110 [4:08:59<25:54:24, 17.64s/it]

 67%|██████▋   | 10823/16110 [4:09:19<26:56:17, 18.34s/it]

 67%|██████▋   | 10824/16110 [4:09:38<27:36:45, 18.81s/it]
{'loss': 0.2818, 'learning_rate': 1.9976645916484032e-06, 'rewards/chosen': -4.172300815582275, 'rewards/rejected': -8.0032377243042, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8309366703033447, 'policy_logps/rejected': -509.1658935546875, 'policy_logps/chosen': -336.484130859375, 'referece_logps/rejected': -429.1335144042969, 'referece_logps/chosen': -294.7610778808594, 'logits/rejected': 0.031122535467147827, 'logits/chosen': 0.25490909814834595, 'epoch': 6.05}
[2024-04-05 19:17:51,716] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 10826/16110 [4:10:19<28:20:54, 19.31s/it]
[2024-04-05 19:18:10,225] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10827/16110 [4:10:33<26:15:36, 17.89s/it]
{'loss': 0.1862, 'learning_rate': 1.997623213142942e-06, 'rewards/chosen': -4.510035991668701, 'rewards/rejected': -8.349357604980469, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8393213748931885, 'policy_logps/rejected': -454.87799072265625, 'policy_logps/chosen': -469.9007873535156, 'referece_logps/rejected': -371.3843994140625, 'referece_logps/chosen': -424.8004150390625, 'logits/rejected': 0.17015112936496735, 'logits/chosen': 0.0910048708319664, 'epoch': 6.05}


 67%|██████▋   | 10829/16110 [4:11:07<25:56:09, 17.68s/it]
{'loss': 0.195, 'learning_rate': 1.9975954258484348e-06, 'rewards/chosen': -5.528047561645508, 'rewards/rejected': -7.626473903656006, 'rewards/accuracies': 0.875, 'rewards/margins': 2.098426342010498, 'policy_logps/rejected': -326.91156005859375, 'policy_logps/chosen': -392.8345031738281, 'referece_logps/rejected': -250.6468505859375, 'referece_logps/chosen': -337.5540771484375, 'logits/rejected': 0.1559670865535736, 'logits/chosen': -0.0017950758337974548, 'epoch': 6.05}

 67%|██████▋   | 10830/16110 [4:11:26<26:28:11, 18.05s/it]

 67%|██████▋   | 10831/16110 [4:11:48<28:06:23, 19.17s/it]
[2024-04-05 19:19:59,786] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10832/16110 [4:12:08<28:31:03, 19.45s/it]

 67%|██████▋   | 10833/16110 [4:12:24<27:03:32, 18.46s/it]


 67%|██████▋   | 10835/16110 [4:13:00<26:54:48, 18.37s/it]

 67%|██████▋   | 10836/16110 [4:13:17<26:39:52, 18.20s/it]

 67%|██████▋   | 10837/16110 [4:13:37<27:30:00, 18.77s/it]

 67%|██████▋   | 10838/16110 [4:13:55<26:56:42, 18.40s/it]

 67%|██████▋   | 10839/16110 [4:14:17<28:24:10, 19.40s/it]

 67%|██████▋   | 10840/16110 [4:14:27<24:34:21, 16.79s/it]
{'loss': 0.1125, 'learning_rate': 1.9974397127179928e-06, 'rewards/chosen': -5.0835700035095215, 'rewards/rejected': -9.618118286132812, 'rewards/accuracies': 1.0, 'rewards/margins': 4.534548759460449, 'policy_logps/rejected': -339.70758056640625, 'policy_logps/chosen': -423.3083801269531, 'referece_logps/rejected': -243.5263671875, 'referece_logps/chosen': -372.47265625, 'logits/rejected': -0.2950642704963684, 'logits/chosen': -0.21426406502723694, 'epoch': 6.06}


 67%|██████▋   | 10842/16110 [4:15:09<27:39:03, 18.90s/it]

 67%|██████▋   | 10843/16110 [4:15:31<29:04:04, 19.87s/it]

 67%|██████▋   | 10844/16110 [4:15:44<25:43:06, 17.58s/it]

 67%|██████▋   | 10845/16110 [4:16:01<25:37:10, 17.52s/it]
{'loss': 0.1074, 'learning_rate': 1.997367321286862e-06, 'rewards/chosen': -3.13938570022583, 'rewards/rejected': -6.528536796569824, 'rewards/accuracies': 0.875, 'rewards/margins': 3.389151096343994, 'policy_logps/rejected': -378.58807373046875, 'policy_logps/chosen': -512.39404296875, 'referece_logps/rejected': -313.3027038574219, 'referece_logps/chosen': -481.000244140625, 'logits/rejected': 0.43350750207901, 'logits/chosen': 0.40243974328041077, 'epoch': 6.06}


 67%|██████▋   | 10847/16110 [4:16:25<21:31:52, 14.73s/it]
{'loss': 0.124, 'learning_rate': 1.997338082510127e-06, 'rewards/chosen': -3.797913074493408, 'rewards/rejected': -7.041819095611572, 'rewards/accuracies': 1.0, 'rewards/margins': 3.243906021118164, 'policy_logps/rejected': -410.9773864746094, 'policy_logps/chosen': -493.9791259765625, 'referece_logps/rejected': -340.5592346191406, 'referece_logps/chosen': -456.0, 'logits/rejected': 0.5232936143875122, 'logits/chosen': 0.38497889041900635, 'epoch': 6.06}

 67%|██████▋   | 10848/16110 [4:16:46<24:13:50, 16.58s/it]

 67%|██████▋   | 10849/16110 [4:17:02<24:01:49, 16.44s/it]


 67%|██████▋   | 10851/16110 [4:17:37<24:26:21, 16.73s/it]

 67%|██████▋   | 10852/16110 [4:18:01<27:35:08, 18.89s/it]
[2024-04-05 19:25:52,887] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10853/16110 [4:18:19<27:09:20, 18.60s/it]
[2024-04-05 19:26:10,805] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10854/16110 [4:18:36<26:15:16, 17.98s/it]

 67%|██████▋   | 10855/16110 [4:18:49<24:11:51, 16.58s/it]
{'loss': 0.1113, 'learning_rate': 1.997219514925715e-06, 'rewards/chosen': -3.086357593536377, 'rewards/rejected': -8.25877571105957, 'rewards/accuracies': 0.875, 'rewards/margins': 5.172418117523193, 'policy_logps/rejected': -358.2308349609375, 'policy_logps/chosen': -450.09295654296875, 'referece_logps/rejected': -275.64306640625, 'referece_logps/chosen': -419.2294006347656, 'logits/rejected': -0.03757321834564209, 'logits/chosen': -0.15495547652244568, 'epoch': 6.06}


 67%|██████▋   | 10857/16110 [4:19:29<26:57:17, 18.47s/it]

 67%|██████▋   | 10858/16110 [4:19:54<29:32:39, 20.25s/it]
[2024-04-05 19:27:45,466] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1994, 'learning_rate': 1.997174386977466e-06, 'rewards/chosen': -4.576198101043701, 'rewards/rejected': -8.00839614868164, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4321980476379395, 'policy_logps/rejected': -418.6861572265625, 'policy_logps/chosen': -495.2266845703125, 'referece_logps/rejected': -338.6022033691406, 'referece_logps/chosen': -449.4647216796875, 'logits/rejected': 0.7021716237068176, 'logits/chosen': 0.5482500195503235, 'epoch': 6.07}


 67%|██████▋   | 10860/16110 [4:20:26<26:14:28, 17.99s/it]
{'loss': 0.1194, 'learning_rate': 1.9971441001450597e-06, 'rewards/chosen': -4.864320755004883, 'rewards/rejected': -9.780243873596191, 'rewards/accuracies': 1.0, 'rewards/margins': 4.915923118591309, 'policy_logps/rejected': -404.5880126953125, 'policy_logps/chosen': -358.11810302734375, 'referece_logps/rejected': -306.7856140136719, 'referece_logps/chosen': -309.4748840332031, 'logits/rejected': -0.1799391210079193, 'logits/chosen': -0.13637766242027283, 'epoch': 6.07}

 67%|██████▋   | 10861/16110 [4:20:40<24:47:23, 17.00s/it]


 67%|██████▋   | 10863/16110 [4:21:24<28:20:24, 19.44s/it]
[2024-04-05 19:29:15,168] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 10864/16110 [4:21:44<28:54:44, 19.84s/it]

 67%|██████▋   | 10865/16110 [4:22:03<28:34:38, 19.61s/it]
[2024-04-05 19:29:55,020] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1654, 'learning_rate': 1.9970676777322874e-06, 'rewards/chosen': -3.269662618637085, 'rewards/rejected': -7.809497833251953, 'rewards/accuracies': 1.0, 'rewards/margins': 4.539835453033447, 'policy_logps/rejected': -368.90045166015625, 'policy_logps/chosen': -390.8067626953125, 'referece_logps/rejected': -290.80548095703125, 'referece_logps/chosen': -358.1101379394531, 'logits/rejected': -0.17960451543331146, 'logits/chosen': -0.26685455441474915, 'epoch': 6.07}


 67%|██████▋   | 10867/16110 [4:22:37<26:31:07, 18.21s/it]

 67%|██████▋   | 10868/16110 [4:22:58<27:29:32, 18.88s/it]
{'loss': 0.1932, 'learning_rate': 1.9970213406532915e-06, 'rewards/chosen': -4.062995433807373, 'rewards/rejected': -7.449519157409668, 'rewards/accuracies': 0.875, 'rewards/margins': 3.386524200439453, 'policy_logps/rejected': -392.3106384277344, 'policy_logps/chosen': -267.93341064453125, 'referece_logps/rejected': -317.8154602050781, 'referece_logps/chosen': -227.303466796875, 'logits/rejected': -0.5494421720504761, 'logits/chosen': -0.4544534981250763, 'epoch': 6.07}


 67%|██████▋   | 10870/16110 [4:23:35<27:09:04, 18.65s/it]
{'loss': 0.1755, 'learning_rate': 1.9969902477646247e-06, 'rewards/chosen': -3.620964765548706, 'rewards/rejected': -6.953900337219238, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3329358100891113, 'policy_logps/rejected': -357.9686279296875, 'policy_logps/chosen': -373.5602111816406, 'referece_logps/rejected': -288.42962646484375, 'referece_logps/chosen': -337.35052490234375, 'logits/rejected': 0.08467750996351242, 'logits/chosen': 0.13543608784675598, 'epoch': 6.07}


 67%|██████▋   | 10872/16110 [4:24:12<26:40:58, 18.34s/it]

 67%|██████▋   | 10873/16110 [4:24:30<26:14:11, 18.04s/it]

 67%|██████▋   | 10874/16110 [4:24:44<24:23:09, 16.77s/it]
{'loss': 0.195, 'learning_rate': 1.996927578403519e-06, 'rewards/chosen': -4.679164886474609, 'rewards/rejected': -9.875738143920898, 'rewards/accuracies': 1.0, 'rewards/margins': 5.196572780609131, 'policy_logps/rejected': -626.1544189453125, 'policy_logps/chosen': -396.7774963378906, 'referece_logps/rejected': -527.3970336914062, 'referece_logps/chosen': -349.9858703613281, 'logits/rejected': -0.5586309432983398, 'logits/chosen': -0.19296357035636902, 'epoch': 6.07}

 68%|██████▊   | 10875/16110 [4:25:01<24:29:33, 16.84s/it]


 68%|██████▊   | 10877/16110 [4:25:34<24:20:44, 16.75s/it]

 68%|██████▊   | 10878/16110 [4:25:53<25:35:39, 17.61s/it]

 68%|██████▊   | 10879/16110 [4:26:08<24:11:07, 16.64s/it]

 68%|██████▊   | 10880/16110 [4:26:30<26:40:27, 18.36s/it]

 68%|██████▊   | 10881/16110 [4:26:52<28:03:03, 19.31s/it]

 68%|██████▊   | 10882/16110 [4:27:08<26:49:19, 18.47s/it]

 68%|██████▊   | 10883/16110 [4:27:28<27:30:02, 18.94s/it]

 68%|██████▊   | 10884/16110 [4:27:50<28:44:18, 19.80s/it]

 68%|██████▊   | 10885/16110 [4:28:10<28:46:07, 19.82s/it]

 68%|██████▊   | 10886/16110 [4:28:21<24:47:02, 17.08s/it]
{'loss': 0.1615, 'learning_rate': 1.996735702017152e-06, 'rewards/chosen': -2.8260159492492676, 'rewards/rejected': -8.530296325683594, 'rewards/accuracies': 1.0, 'rewards/margins': 5.704280853271484, 'policy_logps/rejected': -339.0439147949219, 'policy_logps/chosen': -286.6754150390625, 'referece_logps/rejected': -253.74095153808594, 'referece_logps/chosen': -258.4152526855469, 'logits/rejected': -0.12990444898605347, 'logits/chosen': -0.10190469026565552, 'epoch': 6.08}


 68%|██████▊   | 10888/16110 [4:29:02<27:24:09, 18.89s/it]

 68%|██████▊   | 10889/16110 [4:29:14<24:31:14, 16.91s/it]
{'loss': 0.1476, 'learning_rate': 1.9966868263790745e-06, 'rewards/chosen': -5.076064586639404, 'rewards/rejected': -9.20934772491455, 'rewards/accuracies': 0.875, 'rewards/margins': 4.1332831382751465, 'policy_logps/rejected': -580.5659790039062, 'policy_logps/chosen': -369.1739196777344, 'referece_logps/rejected': -488.4725036621094, 'referece_logps/chosen': -318.4132995605469, 'logits/rejected': -0.4670984745025635, 'logits/chosen': -0.2821568548679352, 'epoch': 6.08}


 68%|██████▊   | 10891/16110 [4:29:46<24:01:33, 16.57s/it]

 68%|██████▊   | 10892/16110 [4:30:02<24:06:06, 16.63s/it]
{'loss': 0.136, 'learning_rate': 1.9966375881597653e-06, 'rewards/chosen': -4.024778842926025, 'rewards/rejected': -7.680342197418213, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6555631160736084, 'policy_logps/rejected': -360.9375, 'policy_logps/chosen': -267.8005065917969, 'referece_logps/rejected': -284.13409423828125, 'referece_logps/chosen': -227.552734375, 'logits/rejected': -0.36020031571388245, 'logits/chosen': -0.22326727211475372, 'epoch': 6.08}

 68%|██████▊   | 10893/16110 [4:30:18<23:28:00, 16.19s/it]


 68%|██████▊   | 10895/16110 [4:30:56<26:01:42, 17.97s/it]
{'loss': 0.1516, 'learning_rate': 1.9965879873771366e-06, 'rewards/chosen': -2.8956313133239746, 'rewards/rejected': -7.154193878173828, 'rewards/accuracies': 1.0, 'rewards/margins': 4.258562088012695, 'policy_logps/rejected': -455.55548095703125, 'policy_logps/chosen': -348.1455383300781, 'referece_logps/rejected': -384.0135192871094, 'referece_logps/chosen': -319.189208984375, 'logits/rejected': -0.024127721786499023, 'logits/chosen': -0.027795519679784775, 'epoch': 6.09}

 68%|██████▊   | 10896/16110 [4:31:14<25:52:45, 17.87s/it]

 68%|██████▊   | 10897/16110 [4:31:27<23:53:15, 16.50s/it]

 68%|██████▊   | 10898/16110 [4:31:48<25:29:59, 17.61s/it]

 68%|██████▊   | 10899/16110 [4:31:59<22:55:25, 15.84s/it]


 68%|██████▊   | 10901/16110 [4:32:38<25:33:58, 17.67s/it]
{'loss': 0.2397, 'learning_rate': 1.9964876981942305e-06, 'rewards/chosen': -4.21021032333374, 'rewards/rejected': -7.304347038269043, 'rewards/accuracies': 0.875, 'rewards/margins': 3.094136953353882, 'policy_logps/rejected': -316.49163818359375, 'policy_logps/chosen': -312.8980712890625, 'referece_logps/rejected': -243.44815063476562, 'referece_logps/chosen': -270.7959899902344, 'logits/rejected': -0.15693923830986023, 'logits/chosen': -0.24414397776126862, 'epoch': 6.09}

 68%|██████▊   | 10902/16110 [4:32:56<25:33:56, 17.67s/it]


 68%|██████▊   | 10904/16110 [4:33:28<24:10:58, 16.72s/it]
{'loss': 0.201, 'learning_rate': 1.996437009830436e-06, 'rewards/chosen': -4.490606784820557, 'rewards/rejected': -9.682188034057617, 'rewards/accuracies': 1.0, 'rewards/margins': 5.1915812492370605, 'policy_logps/rejected': -432.77392578125, 'policy_logps/chosen': -364.7609558105469, 'referece_logps/rejected': -335.9520263671875, 'referece_logps/chosen': -319.85491943359375, 'logits/rejected': -0.2038765251636505, 'logits/chosen': -0.22680796682834625, 'epoch': 6.09}

 68%|██████▊   | 10905/16110 [4:33:44<23:43:02, 16.40s/it]
[2024-04-05 19:41:55,820] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 10906/16110 [4:34:04<25:20:23, 17.53s/it]

 68%|██████▊   | 10907/16110 [4:34:24<26:28:52, 18.32s/it]
[2024-04-05 19:42:37,306] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 10908/16110 [4:34:46<27:46:21, 19.22s/it]
[2024-04-05 19:42:57,493] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 68%|██████▊   | 10910/16110 [4:35:17<24:22:24, 16.87s/it]
{'loss': 0.1194, 'learning_rate': 1.996334545650364e-06, 'rewards/chosen': -3.669351100921631, 'rewards/rejected': -8.280564308166504, 'rewards/accuracies': 1.0, 'rewards/margins': 4.611212730407715, 'policy_logps/rejected': -306.1485595703125, 'policy_logps/chosen': -397.97454833984375, 'referece_logps/rejected': -223.34292602539062, 'referece_logps/chosen': -361.28106689453125, 'logits/rejected': -0.002827849704772234, 'logits/chosen': -0.1788795292377472, 'epoch': 6.09}

 68%|██████▊   | 10911/16110 [4:35:38<26:19:20, 18.23s/it]

 68%|██████▊   | 10912/16110 [4:35:57<26:48:32, 18.57s/it]

 68%|██████▊   | 10913/16110 [4:36:13<25:44:21, 17.83s/it]

 68%|██████▊   | 10914/16110 [4:36:34<27:03:16, 18.74s/it]

 68%|██████▊   | 10915/16110 [4:36:51<25:57:07, 17.98s/it]

 68%|██████▊   | 10916/16110 [4:37:12<27:30:33, 19.07s/it]

 68%|██████▊   | 10917/16110 [4:37:32<28:03:01, 19.45s/it]

 68%|██████▊   | 10918/16110 [4:37:46<25:40:50, 17.81s/it]

 68%|██████▊   | 10919/16110 [4:38:08<27:16:38, 18.92s/it]

 68%|██████▊   | 10920/16110 [4:38:28<27:46:04, 19.26s/it]

 68%|██████▊   | 10921/16110 [4:38:48<27:53:40, 19.35s/it]


 68%|██████▊   | 10923/16110 [4:39:23<25:59:57, 18.04s/it]
{'loss': 0.2345, 'learning_rate': 1.9961075664675126e-06, 'rewards/chosen': -3.507606267929077, 'rewards/rejected': -8.383033752441406, 'rewards/accuracies': 0.875, 'rewards/margins': 4.875427722930908, 'policy_logps/rejected': -350.01519775390625, 'policy_logps/chosen': -402.03948974609375, 'referece_logps/rejected': -266.18487548828125, 'referece_logps/chosen': -366.96343994140625, 'logits/rejected': -0.25810399651527405, 'logits/chosen': -0.2956109642982483, 'epoch': 6.1}


 68%|██████▊   | 10925/16110 [4:39:57<25:04:09, 17.41s/it]
{'loss': 0.148, 'learning_rate': 1.99607204260327e-06, 'rewards/chosen': -2.4332597255706787, 'rewards/rejected': -4.932747840881348, 'rewards/accuracies': 0.875, 'rewards/margins': 2.49948787689209, 'policy_logps/rejected': -282.7204284667969, 'policy_logps/chosen': -235.08453369140625, 'referece_logps/rejected': -233.3929443359375, 'referece_logps/chosen': -210.75192260742188, 'logits/rejected': 0.6509482264518738, 'logits/chosen': 0.559307873249054, 'epoch': 6.1}

 68%|██████▊   | 10926/16110 [4:40:19<26:48:28, 18.62s/it]

 68%|██████▊   | 10927/16110 [4:40:40<28:07:10, 19.53s/it]


 68%|██████▊   | 10929/16110 [4:41:11<24:32:08, 17.05s/it]
{'loss': 0.1457, 'learning_rate': 1.9960005117371036e-06, 'rewards/chosen': -5.761113166809082, 'rewards/rejected': -10.039322853088379, 'rewards/accuracies': 1.0, 'rewards/margins': 4.278210163116455, 'policy_logps/rejected': -290.8339538574219, 'policy_logps/chosen': -452.2048034667969, 'referece_logps/rejected': -190.4407196044922, 'referece_logps/chosen': -394.59368896484375, 'logits/rejected': 0.1526818871498108, 'logits/chosen': 0.14921191334724426, 'epoch': 6.11}


 68%|██████▊   | 10931/16110 [4:41:39<22:31:11, 15.65s/it]
{'loss': 0.1684, 'learning_rate': 1.9959645047467455e-06, 'rewards/chosen': -4.3699049949646, 'rewards/rejected': -9.519220352172852, 'rewards/accuracies': 0.875, 'rewards/margins': 5.149315357208252, 'policy_logps/rejected': -364.94580078125, 'policy_logps/chosen': -428.35162353515625, 'referece_logps/rejected': -269.7535705566406, 'referece_logps/chosen': -384.6525573730469, 'logits/rejected': -0.25168314576148987, 'logits/chosen': -0.5276432037353516, 'epoch': 6.11}

 68%|██████▊   | 10932/16110 [4:41:51<20:51:01, 14.50s/it]


 68%|██████▊   | 10934/16110 [4:42:27<23:21:42, 16.25s/it]
{'loss': 0.1234, 'learning_rate': 1.9959101923309793e-06, 'rewards/chosen': -4.107023239135742, 'rewards/rejected': -9.98378849029541, 'rewards/accuracies': 1.0, 'rewards/margins': 5.876765251159668, 'policy_logps/rejected': -349.7059326171875, 'policy_logps/chosen': -354.9620056152344, 'referece_logps/rejected': -249.86801147460938, 'referece_logps/chosen': -313.8917541503906, 'logits/rejected': 0.2784436345100403, 'logits/chosen': 0.40289223194122314, 'epoch': 6.11}

 68%|██████▊   | 10935/16110 [4:42:48<25:17:05, 17.59s/it]

 68%|██████▊   | 10936/16110 [4:43:10<27:16:18, 18.98s/it]


 68%|██████▊   | 10938/16110 [4:43:47<26:42:43, 18.59s/it]
{'loss': 0.2025, 'learning_rate': 1.9958372122042987e-06, 'rewards/chosen': -3.969463348388672, 'rewards/rejected': -9.742175102233887, 'rewards/accuracies': 1.0, 'rewards/margins': 5.772711753845215, 'policy_logps/rejected': -583.1219482421875, 'policy_logps/chosen': -509.75408935546875, 'referece_logps/rejected': -485.7001953125, 'referece_logps/chosen': -470.05950927734375, 'logits/rejected': -0.007265179418027401, 'logits/chosen': 0.1468154639005661, 'epoch': 6.11}


 68%|██████▊   | 10940/16110 [4:44:10<21:16:25, 14.81s/it]

 68%|██████▊   | 10941/16110 [4:44:30<23:29:28, 16.36s/it]

 68%|██████▊   | 10942/16110 [4:44:43<22:21:36, 15.58s/it]
{'loss': 0.1938, 'learning_rate': 1.9957635880382495e-06, 'rewards/chosen': -4.212197303771973, 'rewards/rejected': -8.815485954284668, 'rewards/accuracies': 0.875, 'rewards/margins': 4.60329008102417, 'policy_logps/rejected': -564.3701171875, 'policy_logps/chosen': -529.3259887695312, 'referece_logps/rejected': -476.21527099609375, 'referece_logps/chosen': -487.2040100097656, 'logits/rejected': -0.14157162606716156, 'logits/chosen': -0.062459126114845276, 'epoch': 6.11}


 68%|██████▊   | 10944/16110 [4:45:20<24:10:50, 16.85s/it]

 68%|██████▊   | 10945/16110 [4:45:41<26:11:14, 18.25s/it]

 68%|██████▊   | 10946/16110 [4:46:01<27:00:12, 18.83s/it]
{'loss': 0.2164, 'learning_rate': 1.995689319880447e-06, 'rewards/chosen': -4.143630027770996, 'rewards/rejected': -7.8290300369262695, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6854002475738525, 'policy_logps/rejected': -425.7718505859375, 'policy_logps/chosen': -350.5310974121094, 'referece_logps/rejected': -347.4815368652344, 'referece_logps/chosen': -309.0948181152344, 'logits/rejected': 0.051154375076293945, 'logits/chosen': 0.03863336890935898, 'epoch': 6.12}


 68%|██████▊   | 10948/16110 [4:46:28<23:01:08, 16.05s/it]

 68%|██████▊   | 10949/16110 [4:46:41<22:04:34, 15.40s/it]
{'loss': 0.2692, 'learning_rate': 1.9956331961713854e-06, 'rewards/chosen': -3.8577728271484375, 'rewards/rejected': -8.028419494628906, 'rewards/accuracies': 0.875, 'rewards/margins': 4.170647144317627, 'policy_logps/rejected': -367.0511779785156, 'policy_logps/chosen': -436.3536376953125, 'referece_logps/rejected': -286.7669982910156, 'referece_logps/chosen': -397.7759094238281, 'logits/rejected': -0.39623013138771057, 'logits/chosen': -0.47440841794013977, 'epoch': 6.12}

 68%|██████▊   | 10950/16110 [4:46:58<22:35:21, 15.76s/it]

 68%|██████▊   | 10951/16110 [4:47:11<21:18:51, 14.87s/it]

 68%|██████▊   | 10952/16110 [4:47:31<23:25:15, 16.35s/it]


 68%|██████▊   | 10954/16110 [4:48:04<24:06:07, 16.83s/it]
[2024-04-05 19:55:55,341] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1198, 'learning_rate': 1.995538851782124e-06, 'rewards/chosen': -6.681896686553955, 'rewards/rejected': -10.142563819885254, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4606666564941406, 'policy_logps/rejected': -496.0327453613281, 'policy_logps/chosen': -558.9451293945312, 'referece_logps/rejected': -394.60711669921875, 'referece_logps/chosen': -492.12615966796875, 'logits/rejected': 0.0005972683429718018, 'logits/chosen': -0.061487674713134766, 'epoch': 6.12}

 68%|██████▊   | 10955/16110 [4:48:19<23:13:51, 16.22s/it]

 68%|██████▊   | 10956/16110 [4:48:35<23:13:48, 16.23s/it]


 68%|██████▊   | 10958/16110 [4:49:14<25:37:24, 17.90s/it]
{'loss': 0.0846, 'learning_rate': 1.995462651938916e-06, 'rewards/chosen': -3.925412178039551, 'rewards/rejected': -9.672361373901367, 'rewards/accuracies': 1.0, 'rewards/margins': 5.7469482421875, 'policy_logps/rejected': -324.2834167480469, 'policy_logps/chosen': -322.45782470703125, 'referece_logps/rejected': -227.55978393554688, 'referece_logps/chosen': -283.2037048339844, 'logits/rejected': -0.470405638217926, 'logits/chosen': -0.5346900224685669, 'epoch': 6.12}


 68%|██████▊   | 10960/16110 [4:49:47<24:18:08, 16.99s/it]

 68%|██████▊   | 10961/16110 [4:50:10<26:30:10, 18.53s/it]
{'loss': 0.2096, 'learning_rate': 1.995405079561934e-06, 'rewards/chosen': -4.7960076332092285, 'rewards/rejected': -8.55961799621582, 'rewards/accuracies': 1.0, 'rewards/margins': 3.763610601425171, 'policy_logps/rejected': -414.6126708984375, 'policy_logps/chosen': -355.39349365234375, 'referece_logps/rejected': -329.01654052734375, 'referece_logps/chosen': -307.4334411621094, 'logits/rejected': -0.21899205446243286, 'logits/chosen': -0.31319352984428406, 'epoch': 6.12}


 68%|██████▊   | 10963/16110 [4:50:50<27:57:03, 19.55s/it]
{'loss': 0.2119, 'learning_rate': 1.995366496801009e-06, 'rewards/chosen': -4.642297744750977, 'rewards/rejected': -9.453851699829102, 'rewards/accuracies': 0.875, 'rewards/margins': 4.811554431915283, 'policy_logps/rejected': -390.0472412109375, 'policy_logps/chosen': -319.2545166015625, 'referece_logps/rejected': -295.50872802734375, 'referece_logps/chosen': -272.83154296875, 'logits/rejected': -0.2600284218788147, 'logits/chosen': -0.2022775113582611, 'epoch': 6.12}

 68%|██████▊   | 10964/16110 [4:51:05<26:11:44, 18.33s/it]


 68%|██████▊   | 10966/16110 [4:51:38<24:26:44, 17.11s/it]

 68%|██████▊   | 10967/16110 [4:51:52<23:07:11, 16.18s/it]
{'loss': 0.2878, 'learning_rate': 1.995288848484197e-06, 'rewards/chosen': -5.269209384918213, 'rewards/rejected': -8.499906539916992, 'rewards/accuracies': 0.75, 'rewards/margins': 3.230696678161621, 'policy_logps/rejected': -572.7907104492188, 'policy_logps/chosen': -505.650146484375, 'referece_logps/rejected': -487.7916564941406, 'referece_logps/chosen': -452.9580383300781, 'logits/rejected': -0.22401884198188782, 'logits/chosen': 0.14524677395820618, 'epoch': 6.13}


 68%|██████▊   | 10969/16110 [4:52:20<21:45:53, 15.24s/it]
{'loss': 0.2063, 'learning_rate': 1.9952497829408642e-06, 'rewards/chosen': -4.8003926277160645, 'rewards/rejected': -8.830568313598633, 'rewards/accuracies': 0.875, 'rewards/margins': 4.03017520904541, 'policy_logps/rejected': -377.4183349609375, 'policy_logps/chosen': -415.5359802246094, 'referece_logps/rejected': -289.1126403808594, 'referece_logps/chosen': -367.53204345703125, 'logits/rejected': -0.14375104010105133, 'logits/chosen': -0.13764770328998566, 'epoch': 6.13}

 68%|██████▊   | 10970/16110 [4:52:39<23:16:33, 16.30s/it]

 68%|██████▊   | 10971/16110 [4:52:59<24:50:52, 17.41s/it]


 68%|██████▊   | 10973/16110 [4:53:26<22:22:25, 15.68s/it]
{'loss': 0.2558, 'learning_rate': 1.9951711691159278e-06, 'rewards/chosen': -3.7305355072021484, 'rewards/rejected': -7.2989678382873535, 'rewards/accuracies': 0.875, 'rewards/margins': 3.568432331085205, 'policy_logps/rejected': -385.2566223144531, 'policy_logps/chosen': -322.3865051269531, 'referece_logps/rejected': -312.26690673828125, 'referece_logps/chosen': -285.0811462402344, 'logits/rejected': -0.0025904178619384766, 'logits/chosen': -0.010497510433197021, 'epoch': 6.13}


 68%|██████▊   | 10975/16110 [4:54:00<23:06:26, 16.20s/it]

 68%|██████▊   | 10976/16110 [4:54:20<24:45:23, 17.36s/it]
{'loss': 0.2272, 'learning_rate': 1.995111786376274e-06, 'rewards/chosen': -3.798384428024292, 'rewards/rejected': -8.670677185058594, 'rewards/accuracies': 1.0, 'rewards/margins': 4.872292995452881, 'policy_logps/rejected': -385.1733093261719, 'policy_logps/chosen': -357.9405517578125, 'referece_logps/rejected': -298.466552734375, 'referece_logps/chosen': -319.95672607421875, 'logits/rejected': -0.15583819150924683, 'logits/chosen': -0.20945793390274048, 'epoch': 6.13}

 68%|██████▊   | 10977/16110 [4:54:37<24:29:37, 17.18s/it]


 68%|██████▊   | 10979/16110 [4:55:18<27:02:36, 18.97s/it]

 68%|██████▊   | 10980/16110 [4:55:38<27:30:48, 19.31s/it]

 68%|██████▊   | 10981/16110 [4:56:00<28:27:23, 19.97s/it]
{'loss': 0.2203, 'learning_rate': 1.99501201069146e-06, 'rewards/chosen': -4.847168445587158, 'rewards/rejected': -7.229190826416016, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3820223808288574, 'policy_logps/rejected': -317.4510498046875, 'policy_logps/chosen': -393.38153076171875, 'referece_logps/rejected': -245.15911865234375, 'referece_logps/chosen': -344.9098205566406, 'logits/rejected': 0.15836754441261292, 'logits/chosen': -0.1811842918395996, 'epoch': 6.13}

 68%|██████▊   | 10982/16110 [4:56:20<28:19:51, 19.89s/it]


 68%|██████▊   | 10984/16110 [4:56:58<27:57:33, 19.64s/it]
{'loss': 0.1304, 'learning_rate': 1.9949516626448277e-06, 'rewards/chosen': -3.8441927433013916, 'rewards/rejected': -8.773538589477539, 'rewards/accuracies': 1.0, 'rewards/margins': 4.929346084594727, 'policy_logps/rejected': -419.9933776855469, 'policy_logps/chosen': -403.8321838378906, 'referece_logps/rejected': -332.2579345703125, 'referece_logps/chosen': -365.3902282714844, 'logits/rejected': 0.20707903802394867, 'logits/chosen': 0.13353802263736725, 'epoch': 6.14}

 68%|██████▊   | 10985/16110 [4:57:15<26:40:04, 18.73s/it]

 68%|██████▊   | 10986/16110 [4:57:29<24:55:58, 17.52s/it]

 68%|██████▊   | 10987/16110 [4:57:47<24:54:10, 17.50s/it]


 68%|██████▊   | 10989/16110 [4:58:30<27:58:50, 19.67s/it]
{'loss': 0.203, 'learning_rate': 1.9948502782446693e-06, 'rewards/chosen': -3.880091428756714, 'rewards/rejected': -7.091108798980713, 'rewards/accuracies': 0.875, 'rewards/margins': 3.211017608642578, 'policy_logps/rejected': -566.29296875, 'policy_logps/chosen': -308.4630432128906, 'referece_logps/rejected': -495.3818664550781, 'referece_logps/chosen': -269.662109375, 'logits/rejected': -0.2013157308101654, 'logits/chosen': -0.031284116208553314, 'epoch': 6.14}

 68%|██████▊   | 10990/16110 [4:58:53<29:33:47, 20.79s/it]

 68%|██████▊   | 10991/16110 [4:59:15<30:03:45, 21.14s/it]

 68%|██████▊   | 10992/16110 [4:59:36<29:47:09, 20.95s/it]

 68%|██████▊   | 10993/16110 [4:59:57<29:45:52, 20.94s/it]

 68%|██████▊   | 10994/16110 [5:00:19<30:28:34, 21.45s/it]

 68%|██████▊   | 10995/16110 [5:00:42<30:52:40, 21.73s/it]

 68%|██████▊   | 10996/16110 [5:00:59<29:03:50, 20.46s/it]

 68%|██████▊   | 10997/16110 [5:01:20<29:06:59, 20.50s/it]

 68%|██████▊   | 10998/16110 [5:01:41<29:32:08, 20.80s/it]


 68%|██████▊   | 11000/16110 [5:02:17<27:37:42, 19.46s/it]
{'loss': 0.2608, 'learning_rate': 1.994623694004202e-06, 'rewards/chosen': -4.932830810546875, 'rewards/rejected': -8.68188762664795, 'rewards/accuracies': 1.0, 'rewards/margins': 3.749056100845337, 'policy_logps/rejected': -397.6994934082031, 'policy_logps/chosen': -434.963623046875, 'referece_logps/rejected': -310.8806457519531, 'referece_logps/chosen': -385.6352844238281, 'logits/rejected': 0.2120315432548523, 'logits/chosen': 0.25680118799209595, 'epoch': 6.15}

 68%|██████▊   | 11001/16110 [5:02:53<34:42:49, 24.46s/it]

 68%|██████▊   | 11002/16110 [5:03:11<32:01:42, 22.57s/it]

 68%|██████▊   | 11003/16110 [5:03:31<30:44:54, 21.68s/it]

 68%|██████▊   | 11004/16110 [5:03:52<30:31:42, 21.52s/it]

 68%|██████▊   | 11005/16110 [5:04:14<30:54:15, 21.79s/it]

 68%|██████▊   | 11006/16110 [5:04:31<28:44:59, 20.28s/it]

 68%|██████▊   | 11007/16110 [5:04:46<26:26:37, 18.66s/it]

 68%|██████▊   | 11008/16110 [5:04:57<23:15:12, 16.41s/it]

 68%|██████▊   | 11009/16110 [5:05:14<23:35:30, 16.65s/it]

 68%|██████▊   | 11010/16110 [5:05:32<23:57:33, 16.91s/it]

 68%|██████▊   | 11011/16110 [5:05:55<26:28:24, 18.69s/it]

 68%|██████▊   | 11012/16110 [5:06:14<26:45:51, 18.90s/it]

 68%|██████▊   | 11013/16110 [5:06:26<23:56:00, 16.90s/it]

 68%|██████▊   | 11014/16110 [5:06:44<24:16:58, 17.15s/it]

 68%|██████▊   | 11015/16110 [5:07:01<24:05:48, 17.03s/it]

 68%|██████▊   | 11016/16110 [5:07:19<24:36:55, 17.40s/it]

 68%|██████▊   | 11017/16110 [5:07:36<24:15:21, 17.15s/it]

 68%|██████▊   | 11018/16110 [5:07:53<24:17:52, 17.18s/it]

 68%|██████▊   | 11019/16110 [5:08:10<24:21:24, 17.22s/it]

 68%|██████▊   | 11020/16110 [5:08:31<25:54:33, 18.32s/it]

 68%|██████▊   | 11021/16110 [5:08:48<25:32:25, 18.07s/it]

 68%|██████▊   | 11022/16110 [5:09:08<25:59:22, 18.39s/it]

 68%|██████▊   | 11023/16110 [5:09:24<25:08:41, 17.79s/it]

 68%|██████▊   | 11024/16110 [5:09:39<24:07:35, 17.08s/it]

 68%|██████▊   | 11025/16110 [5:09:59<25:00:47, 17.71s/it]

 68%|██████▊   | 11026/16110 [5:10:16<24:45:03, 17.53s/it]

 68%|██████▊   | 11027/16110 [5:10:33<24:31:23, 17.37s/it]

 68%|██████▊   | 11028/16110 [5:10:51<24:49:40, 17.59s/it]

 68%|██████▊   | 11029/16110 [5:11:11<25:46:30, 18.26s/it]

 68%|██████▊   | 11030/16110 [5:11:31<26:29:00, 18.77s/it]

 68%|██████▊   | 11031/16110 [5:11:52<27:25:20, 19.44s/it]

 68%|██████▊   | 11032/16110 [5:12:13<28:12:40, 20.00s/it]

 68%|██████▊   | 11033/16110 [5:12:24<24:28:45, 17.36s/it]

 68%|██████▊   | 11034/16110 [5:12:35<21:40:40, 15.37s/it]

 68%|██████▊   | 11035/16110 [5:12:49<21:08:21, 15.00s/it]

 69%|██████▊   | 11036/16110 [5:13:04<21:20:08, 15.14s/it]

 69%|██████▊   | 11037/16110 [5:13:24<23:17:07, 16.52s/it]

 69%|██████▊   | 11038/16110 [5:13:44<24:52:37, 17.66s/it]

 69%|██████▊   | 11039/16110 [5:13:59<23:40:04, 16.80s/it]

 69%|██████▊   | 11040/16110 [5:14:19<24:51:26, 17.65s/it]

 69%|██████▊   | 11041/16110 [5:14:41<26:43:57, 18.99s/it]

 69%|██████▊   | 11042/16110 [5:15:04<28:22:36, 20.16s/it]

 69%|██████▊   | 11043/16110 [5:15:24<28:18:49, 20.12s/it]

 69%|██████▊   | 11044/16110 [5:15:41<26:51:11, 19.08s/it]

 69%|██████▊   | 11045/16110 [5:15:54<24:29:14, 17.40s/it]

 69%|██████▊   | 11046/16110 [5:16:06<22:07:32, 15.73s/it]

 69%|██████▊   | 11047/16110 [5:16:25<23:41:33, 16.85s/it]

 69%|██████▊   | 11048/16110 [5:16:45<25:03:20, 17.82s/it]

 69%|██████▊   | 11049/16110 [5:17:04<25:11:34, 17.92s/it]

 69%|██████▊   | 11050/16110 [5:17:22<25:32:22, 18.17s/it]

 69%|██████▊   | 11051/16110 [5:17:37<23:51:22, 16.98s/it]

 69%|██████▊   | 11052/16110 [5:17:50<22:09:56, 15.78s/it]

 69%|██████▊   | 11053/16110 [5:18:09<23:46:56, 16.93s/it]

 69%|██████▊   | 11054/16110 [5:18:30<25:32:01, 18.18s/it]

 69%|██████▊   | 11055/16110 [5:18:45<24:01:13, 17.11s/it]

 69%|██████▊   | 11056/16110 [5:18:58<22:31:47, 16.05s/it]

 69%|██████▊   | 11057/16110 [5:19:17<23:28:13, 16.72s/it]

 69%|██████▊   | 11058/16110 [5:19:35<24:06:45, 17.18s/it]

 69%|██████▊   | 11059/16110 [5:19:48<22:11:32, 15.82s/it]


 69%|██████▊   | 11061/16110 [5:20:24<23:46:27, 16.95s/it]
{'loss': 0.2394, 'learning_rate': 1.993278926449896e-06, 'rewards/chosen': -3.760382890701294, 'rewards/rejected': -6.412292957305908, 'rewards/accuracies': 0.875, 'rewards/margins': 2.651909589767456, 'policy_logps/rejected': -522.085205078125, 'policy_logps/chosen': -377.7891845703125, 'referece_logps/rejected': -457.96221923828125, 'referece_logps/chosen': -340.18536376953125, 'logits/rejected': -0.08645348995923996, 'logits/chosen': 0.26219648122787476, 'epoch': 6.18}

 69%|██████▊   | 11062/16110 [5:20:42<24:19:24, 17.35s/it]

 69%|██████▊   | 11063/16110 [5:21:02<25:19:44, 18.07s/it]

 69%|██████▊   | 11064/16110 [5:21:23<26:24:52, 18.85s/it]

 69%|██████▊   | 11065/16110 [5:21:43<26:57:40, 19.24s/it]

 69%|██████▊   | 11066/16110 [5:22:02<27:08:20, 19.37s/it]


 69%|██████▊   | 11068/16110 [5:22:40<26:49:29, 19.15s/it]
{'loss': 0.144, 'learning_rate': 1.993115049464679e-06, 'rewards/chosen': -3.0367934703826904, 'rewards/rejected': -8.064598083496094, 'rewards/accuracies': 1.0, 'rewards/margins': 5.027804851531982, 'policy_logps/rejected': -378.1580810546875, 'policy_logps/chosen': -382.5147705078125, 'referece_logps/rejected': -297.51214599609375, 'referece_logps/chosen': -352.1468505859375, 'logits/rejected': -0.36484265327453613, 'logits/chosen': -0.5243845582008362, 'epoch': 6.18}

 69%|██████▊   | 11069/16110 [5:23:02<27:50:28, 19.88s/it]

 69%|██████▊   | 11070/16110 [5:23:16<25:38:44, 18.32s/it]

 69%|██████▊   | 11071/16110 [5:23:33<24:52:55, 17.78s/it]

 69%|██████▊   | 11072/16110 [5:23:57<27:28:11, 19.63s/it]


 69%|██████▊   | 11074/16110 [5:24:21<22:10:55, 15.86s/it]

 69%|██████▊   | 11075/16110 [5:24:34<20:38:35, 14.76s/it]

 69%|██████▉   | 11076/16110 [5:24:50<21:21:07, 15.27s/it]

 69%|██████▉   | 11077/16110 [5:25:07<22:06:10, 15.81s/it]

 69%|██████▉   | 11078/16110 [5:25:25<23:03:50, 16.50s/it]

 69%|██████▉   | 11079/16110 [5:25:43<23:43:48, 16.98s/it]

 69%|██████▉   | 11080/16110 [5:26:00<23:30:57, 16.83s/it]

 69%|██████▉   | 11081/16110 [5:26:13<22:00:24, 15.75s/it]

 69%|██████▉   | 11082/16110 [5:26:30<22:33:43, 16.15s/it]

 69%|██████▉   | 11083/16110 [5:26:49<23:36:02, 16.90s/it]

 69%|██████▉   | 11084/16110 [5:27:01<21:28:27, 15.38s/it]

 69%|██████▉   | 11085/16110 [5:27:23<24:23:33, 17.48s/it]

 69%|██████▉   | 11086/16110 [5:27:37<22:52:52, 16.40s/it]

 69%|██████▉   | 11087/16110 [5:27:54<23:12:02, 16.63s/it]

 69%|██████▉   | 11088/16110 [5:28:06<21:20:23, 15.30s/it]

 69%|██████▉   | 11089/16110 [5:28:30<24:47:45, 17.78s/it]

 69%|██████▉   | 11090/16110 [5:28:45<23:38:38, 16.96s/it]

 69%|██████▉   | 11091/16110 [5:29:04<24:35:52, 17.64s/it]

 69%|██████▉   | 11092/16110 [5:29:23<24:58:39, 17.92s/it]

 69%|██████▉   | 11093/16110 [5:29:35<22:31:02, 16.16s/it]

 69%|██████▉   | 11094/16110 [5:29:53<23:28:52, 16.85s/it]

 69%|██████▉   | 11095/16110 [5:30:12<24:09:15, 17.34s/it]

 69%|██████▉   | 11096/16110 [5:30:30<24:39:36, 17.71s/it]

 69%|██████▉   | 11097/16110 [5:30:48<24:48:31, 17.82s/it]

 69%|██████▉   | 11098/16110 [5:31:08<25:29:04, 18.31s/it]

 69%|██████▉   | 11099/16110 [5:31:22<23:59:25, 17.24s/it]

 69%|██████▉   | 11100/16110 [5:31:43<25:21:55, 18.23s/it]

 69%|██████▉   | 11101/16110 [5:31:56<23:19:10, 16.76s/it]

 69%|██████▉   | 11102/16110 [5:32:18<25:22:49, 18.24s/it]

 69%|██████▉   | 11103/16110 [5:32:32<23:41:03, 17.03s/it]

 69%|██████▉   | 11104/16110 [5:32:53<25:21:18, 18.23s/it]

 69%|██████▉   | 11105/16110 [5:33:04<22:17:38, 16.04s/it]

 69%|██████▉   | 11106/16110 [5:33:26<24:37:15, 17.71s/it]

 69%|██████▉   | 11107/16110 [5:33:40<23:05:38, 16.62s/it]
{'loss': 0.1428, 'learning_rate': 1.9921660220522335e-06, 'rewards/chosen': -4.990239143371582, 'rewards/rejected': -8.110976219177246, 'rewards/accuracies': 0.875, 'rewards/margins': 3.120737075805664, 'policy_logps/rejected': -564.2760620117188, 'policy_logps/chosen': -653.622802734375, 'referece_logps/rejected': -483.1662902832031, 'referece_logps/chosen': -603.7203979492188, 'logits/rejected': -0.27847039699554443, 'logits/chosen': -0.42863425612449646, 'epoch': 6.21}

 69%|██████▉   | 11108/16110 [5:33:59<24:14:02, 17.44s/it]

 69%|██████▉   | 11109/16110 [5:34:15<23:35:56, 16.99s/it]

 69%|██████▉   | 11110/16110 [5:34:33<24:06:40, 17.36s/it]

 69%|██████▉   | 11111/16110 [5:34:53<25:12:00, 18.15s/it]

 69%|██████▉   | 11112/16110 [5:35:15<26:38:02, 19.18s/it]

 69%|██████▉   | 11113/16110 [5:35:27<23:46:42, 17.13s/it]

 69%|██████▉   | 11114/16110 [5:35:47<25:00:09, 18.02s/it]

 69%|██████▉   | 11115/16110 [5:36:09<26:30:59, 19.11s/it]

 69%|██████▉   | 11116/16110 [5:36:29<26:59:41, 19.46s/it]
[2024-04-05 20:44:42,706] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 69%|██████▉   | 11118/16110 [5:37:11<27:39:15, 19.94s/it]

 69%|██████▉   | 11119/16110 [5:37:28<26:40:31, 19.24s/it]
{'loss': 0.162, 'learning_rate': 1.9918617390256246e-06, 'rewards/chosen': -5.236696720123291, 'rewards/rejected': -9.310304641723633, 'rewards/accuracies': 1.0, 'rewards/margins': 4.0736083984375, 'policy_logps/rejected': -469.2559814453125, 'policy_logps/chosen': -473.6275634765625, 'referece_logps/rejected': -376.1529235839844, 'referece_logps/chosen': -421.2606201171875, 'logits/rejected': 0.09869804978370667, 'logits/chosen': -0.04572691768407822, 'epoch': 6.21}


 69%|██████▉   | 11121/16110 [5:38:10<27:58:38, 20.19s/it]

 69%|██████▉   | 11122/16110 [5:38:28<27:02:46, 19.52s/it]

 69%|██████▉   | 11123/16110 [5:38:46<26:25:38, 19.08s/it]

 69%|██████▉   | 11124/16110 [5:39:01<24:49:42, 17.93s/it]

 69%|██████▉   | 11125/16110 [5:39:16<23:36:13, 17.05s/it]

 69%|██████▉   | 11126/16110 [5:39:27<21:03:33, 15.21s/it]
{'loss': 0.1917, 'learning_rate': 1.9916815743769544e-06, 'rewards/chosen': -3.3989579677581787, 'rewards/rejected': -8.635522842407227, 'rewards/accuracies': 0.75, 'rewards/margins': 5.2365641593933105, 'policy_logps/rejected': -374.7310791015625, 'policy_logps/chosen': -319.7367858886719, 'referece_logps/rejected': -288.3758850097656, 'referece_logps/chosen': -285.74725341796875, 'logits/rejected': -0.20029377937316895, 'logits/chosen': -0.1635800004005432, 'epoch': 6.22}
[2024-04-05 20:47:35,274] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 11127/16110 [5:39:44<21:40:07, 15.65s/it]

 69%|██████▉   | 11128/16110 [5:40:03<23:18:38, 16.84s/it]


 69%|██████▉   | 11130/16110 [5:40:40<24:38:23, 17.81s/it]
[2024-04-05 20:48:31,635] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 69%|██████▉   | 11131/16110 [5:41:01<25:56:00, 18.75s/it]
{'loss': 0.1407, 'learning_rate': 1.9915516827867636e-06, 'rewards/chosen': -5.016135215759277, 'rewards/rejected': -8.946918487548828, 'rewards/accuracies': 1.0, 'rewards/margins': 3.93078351020813, 'policy_logps/rejected': -304.74664306640625, 'policy_logps/chosen': -432.4549560546875, 'referece_logps/rejected': -215.27745056152344, 'referece_logps/chosen': -382.2936096191406, 'logits/rejected': 0.9625416994094849, 'logits/chosen': 0.857019305229187, 'epoch': 6.22}
[2024-04-05 20:49:13,292] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 69%|██████▉   | 11133/16110 [5:41:41<26:52:03, 19.43s/it]

 69%|██████▉   | 11134/16110 [5:42:02<27:25:37, 19.84s/it]
[2024-04-05 20:49:53,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.221, 'learning_rate': 1.9914732668733855e-06, 'rewards/chosen': -4.723938465118408, 'rewards/rejected': -9.690204620361328, 'rewards/accuracies': 0.875, 'rewards/margins': 4.966266632080078, 'policy_logps/rejected': -418.5797119140625, 'policy_logps/chosen': -327.63330078125, 'referece_logps/rejected': -321.6776428222656, 'referece_logps/chosen': -280.3939208984375, 'logits/rejected': 0.26395589113235474, 'logits/chosen': 0.34850332140922546, 'epoch': 6.22}


 69%|██████▉   | 11136/16110 [5:42:44<28:25:12, 20.57s/it]

 69%|██████▉   | 11137/16110 [5:43:04<28:09:16, 20.38s/it]

 69%|██████▉   | 11138/16110 [5:43:22<27:17:53, 19.77s/it]

 69%|██████▉   | 11139/16110 [5:43:41<26:41:00, 19.32s/it]
{'loss': 0.2173, 'learning_rate': 1.9913417721772148e-06, 'rewards/chosen': -4.041700839996338, 'rewards/rejected': -8.421531677246094, 'rewards/accuracies': 1.0, 'rewards/margins': 4.379831314086914, 'policy_logps/rejected': -369.9735412597656, 'policy_logps/chosen': -375.5122375488281, 'referece_logps/rejected': -285.75823974609375, 'referece_logps/chosen': -335.0952453613281, 'logits/rejected': -0.04155827313661575, 'logits/chosen': -0.04428546130657196, 'epoch': 6.22}

 69%|██████▉   | 11140/16110 [5:43:58<25:42:47, 18.63s/it]


 69%|██████▉   | 11142/16110 [5:44:38<26:39:54, 19.32s/it]
{'loss': 0.2408, 'learning_rate': 1.9912623945019615e-06, 'rewards/chosen': -5.426079750061035, 'rewards/rejected': -8.359115600585938, 'rewards/accuracies': 0.875, 'rewards/margins': 2.933034896850586, 'policy_logps/rejected': -448.2791748046875, 'policy_logps/chosen': -617.2098388671875, 'referece_logps/rejected': -364.6880798339844, 'referece_logps/chosen': -562.9489135742188, 'logits/rejected': 0.5548290014266968, 'logits/chosen': 0.3233886659145355, 'epoch': 6.22}


 69%|██████▉   | 11144/16110 [5:45:16<26:25:03, 19.15s/it]

 69%|██████▉   | 11145/16110 [5:45:34<25:53:36, 18.77s/it]

 69%|██████▉   | 11146/16110 [5:45:55<26:40:05, 19.34s/it]

 69%|██████▉   | 11147/16110 [5:46:13<26:13:14, 19.02s/it]

 69%|██████▉   | 11148/16110 [5:46:31<25:54:44, 18.80s/it]
{'loss': 0.1902, 'learning_rate': 1.991102557356774e-06, 'rewards/chosen': -5.2714738845825195, 'rewards/rejected': -11.0643949508667, 'rewards/accuracies': 1.0, 'rewards/margins': 5.79292106628418, 'policy_logps/rejected': -360.6871643066406, 'policy_logps/chosen': -389.9971923828125, 'referece_logps/rejected': -250.04324340820312, 'referece_logps/chosen': -337.2825012207031, 'logits/rejected': 0.5341655611991882, 'logits/chosen': 0.3373292088508606, 'epoch': 6.23}


 69%|██████▉   | 11150/16110 [5:47:03<23:45:56, 17.25s/it]
{'loss': 0.2275, 'learning_rate': 1.991048957808194e-06, 'rewards/chosen': -4.0448994636535645, 'rewards/rejected': -9.170302391052246, 'rewards/accuracies': 1.0, 'rewards/margins': 5.125402450561523, 'policy_logps/rejected': -481.36968994140625, 'policy_logps/chosen': -396.30181884765625, 'referece_logps/rejected': -389.6667175292969, 'referece_logps/chosen': -355.852783203125, 'logits/rejected': -0.49310654401779175, 'logits/chosen': -0.4805946350097656, 'epoch': 6.23}


 69%|██████▉   | 11152/16110 [5:47:36<23:06:58, 16.78s/it]
{'loss': 0.1675, 'learning_rate': 1.9909951980239446e-06, 'rewards/chosen': -4.485664367675781, 'rewards/rejected': -9.12643051147461, 'rewards/accuracies': 1.0, 'rewards/margins': 4.640765190124512, 'policy_logps/rejected': -414.1322021484375, 'policy_logps/chosen': -342.4163513183594, 'referece_logps/rejected': -322.8678894042969, 'referece_logps/chosen': -297.5596923828125, 'logits/rejected': -0.7388126850128174, 'logits/chosen': -0.6023707985877991, 'epoch': 6.23}


 69%|██████▉   | 11154/16110 [5:48:13<24:29:12, 17.79s/it]
{'loss': 0.1499, 'learning_rate': 1.9909412780127177e-06, 'rewards/chosen': -3.9083666801452637, 'rewards/rejected': -7.0307297706604, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1223630905151367, 'policy_logps/rejected': -392.0937805175781, 'policy_logps/chosen': -330.437255859375, 'referece_logps/rejected': -321.7864685058594, 'referece_logps/chosen': -291.35357666015625, 'logits/rejected': -0.14210206270217896, 'logits/chosen': -0.10423848778009415, 'epoch': 6.23}


 69%|██████▉   | 11156/16110 [5:48:55<26:51:09, 19.51s/it]
[2024-04-05 20:56:46,953] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1768, 'learning_rate': 1.990887197783231e-06, 'rewards/chosen': -3.9923269748687744, 'rewards/rejected': -7.439930438995361, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4476029872894287, 'policy_logps/rejected': -335.99993896484375, 'policy_logps/chosen': -343.6087646484375, 'referece_logps/rejected': -261.60064697265625, 'referece_logps/chosen': -303.68548583984375, 'logits/rejected': -0.07117428630590439, 'logits/chosen': 0.014841040596365929, 'epoch': 6.23}


 69%|██████▉   | 11158/16110 [5:49:31<25:53:53, 18.83s/it]

 69%|██████▉   | 11159/16110 [5:49:52<26:55:22, 19.58s/it]
{'loss': 0.1123, 'learning_rate': 1.9908057770488977e-06, 'rewards/chosen': -4.256762981414795, 'rewards/rejected': -7.924554347991943, 'rewards/accuracies': 1.0, 'rewards/margins': 3.667790412902832, 'policy_logps/rejected': -384.09039306640625, 'policy_logps/chosen': -352.23358154296875, 'referece_logps/rejected': -304.8448486328125, 'referece_logps/chosen': -309.66595458984375, 'logits/rejected': -0.18509766459465027, 'logits/chosen': -0.03525989502668381, 'epoch': 6.23}

 69%|██████▉   | 11160/16110 [5:50:10<26:06:53, 18.99s/it]


 69%|██████▉   | 11162/16110 [5:50:47<25:44:09, 18.72s/it]

 69%|██████▉   | 11163/16110 [5:51:05<25:15:35, 18.38s/it]
{'loss': 0.2424, 'learning_rate': 1.990696655387704e-06, 'rewards/chosen': -5.791544437408447, 'rewards/rejected': -8.9149169921875, 'rewards/accuracies': 0.875, 'rewards/margins': 3.123372793197632, 'policy_logps/rejected': -406.6556701660156, 'policy_logps/chosen': -391.7107238769531, 'referece_logps/rejected': -317.5065002441406, 'referece_logps/chosen': -333.7952575683594, 'logits/rejected': -0.06904569268226624, 'logits/chosen': -0.03854140266776085, 'epoch': 6.24}

 69%|██████▉   | 11164/16110 [5:51:26<26:29:54, 19.29s/it]


 69%|██████▉   | 11166/16110 [5:51:58<24:50:38, 18.09s/it]
{'loss': 0.1146, 'learning_rate': 1.990614393668841e-06, 'rewards/chosen': -6.406538486480713, 'rewards/rejected': -10.676319122314453, 'rewards/accuracies': 0.875, 'rewards/margins': 4.269781112670898, 'policy_logps/rejected': -458.31805419921875, 'policy_logps/chosen': -359.2447814941406, 'referece_logps/rejected': -351.55487060546875, 'referece_logps/chosen': -295.17938232421875, 'logits/rejected': -0.32571491599082947, 'logits/chosen': -0.16584303975105286, 'epoch': 6.24}


 69%|██████▉   | 11168/16110 [5:52:29<23:08:40, 16.86s/it]
{'loss': 0.1235, 'learning_rate': 1.9905593523143225e-06, 'rewards/chosen': -4.265158653259277, 'rewards/rejected': -6.777316093444824, 'rewards/accuracies': 1.0, 'rewards/margins': 2.512157440185547, 'policy_logps/rejected': -276.88818359375, 'policy_logps/chosen': -294.459716796875, 'referece_logps/rejected': -209.114990234375, 'referece_logps/chosen': -251.80813598632812, 'logits/rejected': -0.11557085067033768, 'logits/chosen': 0.08192289620637894, 'epoch': 6.24}

 69%|██████▉   | 11169/16110 [5:52:50<24:55:20, 18.16s/it]


 69%|██████▉   | 11171/16110 [5:53:23<23:02:42, 16.80s/it]

 69%|██████▉   | 11172/16110 [5:53:37<22:02:38, 16.07s/it]
{'loss': 0.1688, 'learning_rate': 1.990448789144684e-06, 'rewards/chosen': -3.310119152069092, 'rewards/rejected': -8.265741348266602, 'rewards/accuracies': 1.0, 'rewards/margins': 4.955621719360352, 'policy_logps/rejected': -317.33172607421875, 'policy_logps/chosen': -358.5454406738281, 'referece_logps/rejected': -234.67431640625, 'referece_logps/chosen': -325.4442443847656, 'logits/rejected': -0.2897208333015442, 'logits/chosen': -0.20584502816200256, 'epoch': 6.24}

 69%|██████▉   | 11173/16110 [5:53:59<24:15:44, 17.69s/it]


 69%|██████▉   | 11175/16110 [5:54:38<25:43:02, 18.76s/it]
{'loss': 0.1143, 'learning_rate': 1.9903654463996355e-06, 'rewards/chosen': -3.5320379734039307, 'rewards/rejected': -8.86181926727295, 'rewards/accuracies': 1.0, 'rewards/margins': 5.329780578613281, 'policy_logps/rejected': -456.9973449707031, 'policy_logps/chosen': -426.3844299316406, 'referece_logps/rejected': -368.379150390625, 'referece_logps/chosen': -391.0640563964844, 'logits/rejected': 0.09471336752176285, 'logits/chosen': 0.08481814712285995, 'epoch': 6.24}


 69%|██████▉   | 11177/16110 [5:55:16<26:18:29, 19.20s/it]
{'loss': 0.1487, 'learning_rate': 1.990309684411282e-06, 'rewards/chosen': -4.295500755310059, 'rewards/rejected': -9.52341365814209, 'rewards/accuracies': 1.0, 'rewards/margins': 5.227912902832031, 'policy_logps/rejected': -472.7764892578125, 'policy_logps/chosen': -412.2530822753906, 'referece_logps/rejected': -377.5423583984375, 'referece_logps/chosen': -369.29803466796875, 'logits/rejected': 0.23784573376178741, 'logits/chosen': 0.01943594217300415, 'epoch': 6.24}


 69%|██████▉   | 11179/16110 [5:55:56<26:57:33, 19.68s/it]

 69%|██████▉   | 11180/16110 [5:56:15<26:48:49, 19.58s/it]

 69%|██████▉   | 11181/16110 [5:56:33<26:03:50, 19.04s/it]
{'loss': 0.0685, 'learning_rate': 1.9901976800951913e-06, 'rewards/chosen': -3.6171135902404785, 'rewards/rejected': -6.193192958831787, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5760793685913086, 'policy_logps/rejected': -249.5686492919922, 'policy_logps/chosen': -406.3624267578125, 'referece_logps/rejected': -187.63670349121094, 'referece_logps/chosen': -370.1912841796875, 'logits/rejected': 0.6540571451187134, 'logits/chosen': 0.37321287393569946, 'epoch': 6.25}


 69%|██████▉   | 11183/16110 [5:57:00<22:07:35, 16.17s/it]
{'loss': 0.1518, 'learning_rate': 1.990141437785563e-06, 'rewards/chosen': -4.802824974060059, 'rewards/rejected': -11.435315132141113, 'rewards/accuracies': 1.0, 'rewards/margins': 6.632491111755371, 'policy_logps/rejected': -659.9506225585938, 'policy_logps/chosen': -479.3172607421875, 'referece_logps/rejected': -545.5974731445312, 'referece_logps/chosen': -431.28900146484375, 'logits/rejected': 0.4772360324859619, 'logits/chosen': 0.6542221903800964, 'epoch': 6.25}


 69%|██████▉   | 11185/16110 [5:57:35<23:31:24, 17.19s/it]

 69%|██████▉   | 11186/16110 [5:57:47<21:21:13, 15.61s/it]

 69%|██████▉   | 11187/16110 [5:57:58<19:21:27, 14.16s/it]
{'loss': 0.173, 'learning_rate': 1.9900284729086084e-06, 'rewards/chosen': -3.8265817165374756, 'rewards/rejected': -6.305196285247803, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4786148071289062, 'policy_logps/rejected': -301.5376892089844, 'policy_logps/chosen': -225.3124237060547, 'referece_logps/rejected': -238.48573303222656, 'referece_logps/chosen': -187.0465850830078, 'logits/rejected': -0.37842240929603577, 'logits/chosen': -0.2831174433231354, 'epoch': 6.25}


 69%|██████▉   | 11189/16110 [5:58:29<21:03:51, 15.41s/it]

 69%|██████▉   | 11190/16110 [5:58:41<19:39:19, 14.38s/it]

 69%|██████▉   | 11191/16110 [5:58:59<21:01:16, 15.38s/it]

 69%|██████▉   | 11192/16110 [5:59:20<23:10:30, 16.96s/it]

 69%|██████▉   | 11193/16110 [5:59:41<25:04:59, 18.36s/it]

 69%|██████▉   | 11194/16110 [6:00:02<25:56:04, 18.99s/it]
{'loss': 0.2407, 'learning_rate': 1.9898292437379362e-06, 'rewards/chosen': -5.564675331115723, 'rewards/rejected': -10.017610549926758, 'rewards/accuracies': 0.875, 'rewards/margins': 4.452935218811035, 'policy_logps/rejected': -554.227294921875, 'policy_logps/chosen': -512.8861694335938, 'referece_logps/rejected': -454.0511779785156, 'referece_logps/chosen': -457.2393798828125, 'logits/rejected': 0.8964389562606812, 'logits/chosen': 0.8223958015441895, 'epoch': 6.25}

 69%|██████▉   | 11195/16110 [6:00:21<26:02:19, 19.07s/it]


 70%|██████▉   | 11197/16110 [6:00:56<24:37:07, 18.04s/it]

 70%|██████▉   | 11198/16110 [6:01:12<23:23:21, 17.14s/it]
[2024-04-05 21:09:03,172] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.077, 'learning_rate': 1.9897145182606703e-06, 'rewards/chosen': -4.787802219390869, 'rewards/rejected': -12.963388442993164, 'rewards/accuracies': 1.0, 'rewards/margins': 8.17558479309082, 'policy_logps/rejected': -701.02587890625, 'policy_logps/chosen': -427.596435546875, 'referece_logps/rejected': -571.3919677734375, 'referece_logps/chosen': -379.7183837890625, 'logits/rejected': 0.04004712402820587, 'logits/chosen': -0.09531991183757782, 'epoch': 6.26}
[2024-04-05 21:09:26,318] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|██████▉   | 11200/16110 [6:01:51<24:54:05, 18.26s/it]

 70%|██████▉   | 11201/16110 [6:02:10<24:52:28, 18.24s/it]

 70%|██████▉   | 11202/16110 [6:02:26<23:59:52, 17.60s/it]

 70%|██████▉   | 11203/16110 [6:02:44<24:22:30, 17.88s/it]

 70%|██████▉   | 11204/16110 [6:03:05<25:40:41, 18.84s/it]
[2024-04-05 21:10:56,910] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 70%|██████▉   | 11205/16110 [6:03:18<23:20:51, 17.14s/it]

 70%|██████▉   | 11206/16110 [6:03:38<24:23:20, 17.90s/it]

 70%|██████▉   | 11207/16110 [6:03:52<22:52:34, 16.80s/it]
{'loss': 0.1257, 'learning_rate': 1.9894540457593822e-06, 'rewards/chosen': -4.1562323570251465, 'rewards/rejected': -10.157472610473633, 'rewards/accuracies': 1.0, 'rewards/margins': 6.00123929977417, 'policy_logps/rejected': -552.0838012695312, 'policy_logps/chosen': -356.3021545410156, 'referece_logps/rejected': -450.5090637207031, 'referece_logps/chosen': -314.7398681640625, 'logits/rejected': -0.7962421178817749, 'logits/chosen': -0.7246236801147461, 'epoch': 6.26}

 70%|██████▉   | 11208/16110 [6:04:05<21:06:13, 15.50s/it]

 70%|██████▉   | 11209/16110 [6:04:21<21:29:38, 15.79s/it]


 70%|██████▉   | 11211/16110 [6:04:58<22:55:10, 16.84s/it]

 70%|██████▉   | 11212/16110 [6:05:18<24:30:15, 18.01s/it]

 70%|██████▉   | 11213/16110 [6:05:32<22:39:06, 16.65s/it]
{'loss': 0.2015, 'learning_rate': 1.9892785976223307e-06, 'rewards/chosen': -4.067049980163574, 'rewards/rejected': -9.196184158325195, 'rewards/accuracies': 1.0, 'rewards/margins': 5.129134178161621, 'policy_logps/rejected': -484.0747375488281, 'policy_logps/chosen': -443.8862609863281, 'referece_logps/rejected': -392.1129150390625, 'referece_logps/chosen': -403.21575927734375, 'logits/rejected': 0.18119025230407715, 'logits/chosen': 0.1756693422794342, 'epoch': 6.26}


 70%|██████▉   | 11215/16110 [6:06:08<23:31:55, 17.31s/it]

 70%|██████▉   | 11216/16110 [6:06:31<25:40:58, 18.89s/it]

 70%|██████▉   | 11217/16110 [6:06:53<26:54:50, 19.80s/it]

 70%|██████▉   | 11218/16110 [6:07:12<26:55:31, 19.81s/it]
{'loss': 0.1909, 'learning_rate': 1.9891312911783244e-06, 'rewards/chosen': -5.147630214691162, 'rewards/rejected': -5.862963676452637, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7153342962265015, 'policy_logps/rejected': -290.51556396484375, 'policy_logps/chosen': -272.3898620605469, 'referece_logps/rejected': -231.8859100341797, 'referece_logps/chosen': -220.91354370117188, 'logits/rejected': -0.8200230598449707, 'logits/chosen': -0.8429206609725952, 'epoch': 6.27}

 70%|██████▉   | 11219/16110 [6:07:28<25:02:38, 18.43s/it]

 70%|██████▉   | 11220/16110 [6:07:41<23:01:13, 16.95s/it]

 70%|██████▉   | 11221/16110 [6:08:01<24:19:40, 17.91s/it]


 70%|██████▉   | 11223/16110 [6:08:32<22:20:49, 16.46s/it]

 70%|██████▉   | 11224/16110 [6:08:48<22:04:18, 16.26s/it]
{'loss': 0.1009, 'learning_rate': 1.9889532040724493e-06, 'rewards/chosen': -3.753859043121338, 'rewards/rejected': -9.68990421295166, 'rewards/accuracies': 1.0, 'rewards/margins': 5.936044692993164, 'policy_logps/rejected': -581.904296875, 'policy_logps/chosen': -413.3479919433594, 'referece_logps/rejected': -485.0052490234375, 'referece_logps/chosen': -375.8094177246094, 'logits/rejected': -0.36890819668769836, 'logits/chosen': -0.19636587798595428, 'epoch': 6.27}


 70%|██████▉   | 11226/16110 [6:09:20<22:05:08, 16.28s/it]
{'loss': 0.1505, 'learning_rate': 1.988893521897357e-06, 'rewards/chosen': -3.831470489501953, 'rewards/rejected': -9.876363754272461, 'rewards/accuracies': 1.0, 'rewards/margins': 6.044895172119141, 'policy_logps/rejected': -493.5345153808594, 'policy_logps/chosen': -379.8796081542969, 'referece_logps/rejected': -394.7708740234375, 'referece_logps/chosen': -341.56488037109375, 'logits/rejected': 0.14104661345481873, 'logits/chosen': -0.0911092758178711, 'epoch': 6.27}

 70%|██████▉   | 11227/16110 [6:09:40<23:27:10, 17.29s/it]


 70%|██████▉   | 11229/16110 [6:10:05<20:14:53, 14.93s/it]
{'loss': 0.2075, 'learning_rate': 1.988803698849292e-06, 'rewards/chosen': -4.931888580322266, 'rewards/rejected': -9.295004844665527, 'rewards/accuracies': 1.0, 'rewards/margins': 4.363115310668945, 'policy_logps/rejected': -399.1015319824219, 'policy_logps/chosen': -407.6302490234375, 'referece_logps/rejected': -306.1514587402344, 'referece_logps/chosen': -358.3113708496094, 'logits/rejected': 0.3109772503376007, 'logits/chosen': 0.2302655577659607, 'epoch': 6.27}


 70%|██████▉   | 11231/16110 [6:10:49<25:20:17, 18.70s/it]

 70%|██████▉   | 11232/16110 [6:11:03<23:17:50, 17.19s/it]

 70%|██████▉   | 11233/16110 [6:11:23<24:20:32, 17.97s/it]
{'loss': 0.233, 'learning_rate': 1.9886833752366073e-06, 'rewards/chosen': -4.511327266693115, 'rewards/rejected': -8.627510070800781, 'rewards/accuracies': 0.875, 'rewards/margins': 4.116183280944824, 'policy_logps/rejected': -516.8412475585938, 'policy_logps/chosen': -522.49755859375, 'referece_logps/rejected': -430.5661315917969, 'referece_logps/chosen': -477.38433837890625, 'logits/rejected': 0.4445079565048218, 'logits/chosen': 0.3792395293712616, 'epoch': 6.28}

 70%|██████▉   | 11234/16110 [6:11:38<23:23:44, 17.27s/it]


 70%|██████▉   | 11236/16110 [6:12:21<26:15:32, 19.40s/it]
{'loss': 0.1895, 'learning_rate': 1.988592712908201e-06, 'rewards/chosen': -3.573117733001709, 'rewards/rejected': -7.691842555999756, 'rewards/accuracies': 0.875, 'rewards/margins': 4.118724822998047, 'policy_logps/rejected': -376.0442810058594, 'policy_logps/chosen': -338.5389709472656, 'referece_logps/rejected': -299.1258544921875, 'referece_logps/chosen': -302.8078308105469, 'logits/rejected': 0.043251365423202515, 'logits/chosen': 0.002139762043952942, 'epoch': 6.28}

 70%|██████▉   | 11237/16110 [6:12:40<25:48:04, 19.06s/it]

 70%|██████▉   | 11238/16110 [6:13:02<27:11:01, 20.09s/it]
[2024-04-05 21:21:13,630] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 70%|██████▉   | 11240/16110 [6:13:35<24:24:25, 18.04s/it]
{'loss': 0.1533, 'learning_rate': 1.988471270374509e-06, 'rewards/chosen': -5.31689977645874, 'rewards/rejected': -8.515719413757324, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1988186836242676, 'policy_logps/rejected': -432.52001953125, 'policy_logps/chosen': -393.18963623046875, 'referece_logps/rejected': -347.3628234863281, 'referece_logps/chosen': -340.0206298828125, 'logits/rejected': -0.25810515880584717, 'logits/chosen': -0.19788625836372375, 'epoch': 6.28}


 70%|██████▉   | 11242/16110 [6:14:15<25:57:20, 19.19s/it]

 70%|██████▉   | 11243/16110 [6:14:33<25:26:57, 18.82s/it]
{'loss': 0.2207, 'learning_rate': 1.9883797689453288e-06, 'rewards/chosen': -3.3983559608459473, 'rewards/rejected': -7.546835899353027, 'rewards/accuracies': 1.0, 'rewards/margins': 4.148480415344238, 'policy_logps/rejected': -377.4469299316406, 'policy_logps/chosen': -587.1348266601562, 'referece_logps/rejected': -301.9786071777344, 'referece_logps/chosen': -553.1513671875, 'logits/rejected': 0.4606804847717285, 'logits/chosen': 0.018323808908462524, 'epoch': 6.28}

 70%|██████▉   | 11244/16110 [6:14:48<24:03:10, 17.79s/it]

 70%|██████▉   | 11245/16110 [6:15:10<25:41:35, 19.01s/it]

 70%|██████▉   | 11246/16110 [6:15:32<26:49:09, 19.85s/it]


 70%|██████▉   | 11248/16110 [6:16:11<26:27:32, 19.59s/it]

 70%|██████▉   | 11249/16110 [6:16:33<27:17:28, 20.21s/it]

 70%|██████▉   | 11250/16110 [6:16:51<26:35:11, 19.69s/it]
{'loss': 0.2457, 'learning_rate': 1.9881648673824354e-06, 'rewards/chosen': -3.5736818313598633, 'rewards/rejected': -7.184342384338379, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6106603145599365, 'policy_logps/rejected': -412.75628662109375, 'policy_logps/chosen': -310.8005065917969, 'referece_logps/rejected': -340.9128723144531, 'referece_logps/chosen': -275.0636901855469, 'logits/rejected': 0.38354235887527466, 'logits/chosen': 0.43052005767822266, 'epoch': 6.28}


 70%|██████▉   | 11252/16110 [6:17:29<26:14:12, 19.44s/it]
{'loss': 0.1275, 'learning_rate': 1.988103107436174e-06, 'rewards/chosen': -4.2059102058410645, 'rewards/rejected': -9.584644317626953, 'rewards/accuracies': 1.0, 'rewards/margins': 5.3787336349487305, 'policy_logps/rejected': -368.3345031738281, 'policy_logps/chosen': -370.5863342285156, 'referece_logps/rejected': -272.4880676269531, 'referece_logps/chosen': -328.5272521972656, 'logits/rejected': -0.4396985173225403, 'logits/chosen': -0.5663017630577087, 'epoch': 6.29}


 70%|██████▉   | 11254/16110 [6:18:11<27:09:12, 20.13s/it]

 70%|██████▉   | 11255/16110 [6:18:27<25:30:40, 18.92s/it]
{'loss': 0.1724, 'learning_rate': 1.9880101679710796e-06, 'rewards/chosen': -3.7058908939361572, 'rewards/rejected': -10.007646560668945, 'rewards/accuracies': 0.875, 'rewards/margins': 6.301756858825684, 'policy_logps/rejected': -354.3233642578125, 'policy_logps/chosen': -459.640869140625, 'referece_logps/rejected': -254.2469024658203, 'referece_logps/chosen': -422.5819396972656, 'logits/rejected': -0.00602809339761734, 'logits/chosen': -0.07009255886077881, 'epoch': 6.29}

 70%|██████▉   | 11256/16110 [6:18:42<23:54:30, 17.73s/it]


 70%|██████▉   | 11258/16110 [6:19:18<23:49:26, 17.68s/it]
{'loss': 0.1802, 'learning_rate': 1.9879168690812048e-06, 'rewards/chosen': -4.953606605529785, 'rewards/rejected': -9.791661262512207, 'rewards/accuracies': 1.0, 'rewards/margins': 4.838054656982422, 'policy_logps/rejected': -539.7852172851562, 'policy_logps/chosen': -528.5440673828125, 'referece_logps/rejected': -441.8686828613281, 'referece_logps/chosen': -479.00799560546875, 'logits/rejected': 0.20269504189491272, 'logits/chosen': 0.26015037298202515, 'epoch': 6.29}


 70%|██████▉   | 11260/16110 [6:19:49<22:50:13, 16.95s/it]
{'loss': 0.1326, 'learning_rate': 1.9878544701576107e-06, 'rewards/chosen': -4.879406929016113, 'rewards/rejected': -10.717537879943848, 'rewards/accuracies': 1.0, 'rewards/margins': 5.838132381439209, 'policy_logps/rejected': -467.8112487792969, 'policy_logps/chosen': -429.5409851074219, 'referece_logps/rejected': -360.6358642578125, 'referece_logps/chosen': -380.7469177246094, 'logits/rejected': 0.004486456513404846, 'logits/chosen': 0.10669014602899551, 'epoch': 6.29}

 70%|██████▉   | 11261/16110 [6:20:08<23:51:40, 17.72s/it]

 70%|██████▉   | 11262/16110 [6:20:20<21:30:05, 15.97s/it]


 70%|██████▉   | 11264/16110 [6:20:57<23:03:12, 17.13s/it]

 70%|██████▉   | 11265/16110 [6:21:15<23:13:24, 17.26s/it]

 70%|██████▉   | 11266/16110 [6:21:37<25:08:48, 18.69s/it]
{'loss': 0.1123, 'learning_rate': 1.987666315112258e-06, 'rewards/chosen': -5.3841071128845215, 'rewards/rejected': -8.761630058288574, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3775229454040527, 'policy_logps/rejected': -377.7408447265625, 'policy_logps/chosen': -491.87664794921875, 'referece_logps/rejected': -290.12457275390625, 'referece_logps/chosen': -438.03558349609375, 'logits/rejected': 0.7211337089538574, 'logits/chosen': 0.526694655418396, 'epoch': 6.29}

 70%|██████▉   | 11267/16110 [6:21:59<26:25:32, 19.64s/it]


 70%|██████▉   | 11269/16110 [6:22:37<26:06:14, 19.41s/it]

 70%|██████▉   | 11270/16110 [6:22:57<26:15:19, 19.53s/it]
{'loss': 0.1657, 'learning_rate': 1.9875400799546835e-06, 'rewards/chosen': -3.948362112045288, 'rewards/rejected': -9.235239028930664, 'rewards/accuracies': 1.0, 'rewards/margins': 5.286877632141113, 'policy_logps/rejected': -499.131591796875, 'policy_logps/chosen': -433.8462829589844, 'referece_logps/rejected': -406.7791748046875, 'referece_logps/chosen': -394.3626708984375, 'logits/rejected': 0.06062095984816551, 'logits/chosen': -0.04669925570487976, 'epoch': 6.3}

 70%|██████▉   | 11271/16110 [6:23:14<25:17:37, 18.82s/it]

 70%|██████▉   | 11272/16110 [6:23:36<26:29:42, 19.72s/it]


 70%|██████▉   | 11274/16110 [6:24:10<24:58:48, 18.60s/it]
{'loss': 0.1735, 'learning_rate': 1.987413206123758e-06, 'rewards/chosen': -4.8494977951049805, 'rewards/rejected': -7.7398576736450195, 'rewards/accuracies': 1.0, 'rewards/margins': 2.890359878540039, 'policy_logps/rejected': -434.56488037109375, 'policy_logps/chosen': -416.8382568359375, 'referece_logps/rejected': -357.16632080078125, 'referece_logps/chosen': -368.3432312011719, 'logits/rejected': 0.6939736008644104, 'logits/chosen': 0.6084032654762268, 'epoch': 6.3}

 70%|██████▉   | 11275/16110 [6:24:24<23:22:06, 17.40s/it]

 70%|██████▉   | 11276/16110 [6:24:43<23:45:39, 17.70s/it]

 70%|███████   | 11277/16110 [6:25:05<25:39:22, 19.11s/it]


 70%|███████   | 11279/16110 [6:25:48<27:12:12, 20.27s/it]

 70%|███████   | 11280/16110 [6:26:10<27:43:41, 20.67s/it]

 70%|███████   | 11281/16110 [6:26:24<25:10:14, 18.76s/it]
{'loss': 0.2302, 'learning_rate': 1.987189640358921e-06, 'rewards/chosen': -4.125748157501221, 'rewards/rejected': -8.312243461608887, 'rewards/accuracies': 0.875, 'rewards/margins': 4.186494827270508, 'policy_logps/rejected': -416.1186828613281, 'policy_logps/chosen': -548.6028442382812, 'referece_logps/rejected': -332.9961853027344, 'referece_logps/chosen': -507.3453674316406, 'logits/rejected': -0.022439252585172653, 'logits/chosen': -0.04262332618236542, 'epoch': 6.3}

 70%|███████   | 11282/16110 [6:26:45<26:02:19, 19.42s/it]


 70%|███████   | 11284/16110 [6:27:26<26:57:03, 20.10s/it]
{'loss': 0.1483, 'learning_rate': 1.9870932278900238e-06, 'rewards/chosen': -4.196922779083252, 'rewards/rejected': -9.350894927978516, 'rewards/accuracies': 1.0, 'rewards/margins': 5.153972625732422, 'policy_logps/rejected': -387.7793884277344, 'policy_logps/chosen': -392.0050964355469, 'referece_logps/rejected': -294.27044677734375, 'referece_logps/chosen': -350.0358581542969, 'logits/rejected': -1.3732900619506836, 'logits/chosen': -1.3490724563598633, 'epoch': 6.3}

 70%|███████   | 11285/16110 [6:27:47<27:19:18, 20.39s/it]


 70%|███████   | 11287/16110 [6:28:22<25:03:11, 18.70s/it]

 70%|███████   | 11288/16110 [6:28:36<23:01:28, 17.19s/it]
{'loss': 0.2531, 'learning_rate': 1.986964119351249e-06, 'rewards/chosen': -3.6256425380706787, 'rewards/rejected': -7.290787220001221, 'rewards/accuracies': 1.0, 'rewards/margins': 3.665144920349121, 'policy_logps/rejected': -429.6270751953125, 'policy_logps/chosen': -387.771240234375, 'referece_logps/rejected': -356.7192077636719, 'referece_logps/chosen': -351.51483154296875, 'logits/rejected': 0.8061386942863464, 'logits/chosen': 0.812447190284729, 'epoch': 6.31}

 70%|███████   | 11289/16110 [6:28:57<24:38:30, 18.40s/it]


 70%|███████   | 11291/16110 [6:29:28<22:47:47, 17.03s/it]

 70%|███████   | 11292/16110 [6:29:44<22:11:22, 16.58s/it]

 70%|███████   | 11293/16110 [6:30:04<23:41:59, 17.71s/it]

 70%|███████   | 11294/16110 [6:30:24<24:33:56, 18.36s/it]

 70%|███████   | 11295/16110 [6:30:36<22:11:17, 16.59s/it]
{'loss': 0.1107, 'learning_rate': 1.9867366435474137e-06, 'rewards/chosen': -5.219636917114258, 'rewards/rejected': -8.770069122314453, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5504324436187744, 'policy_logps/rejected': -455.5364990234375, 'policy_logps/chosen': -441.3954772949219, 'referece_logps/rejected': -367.8358154296875, 'referece_logps/chosen': -389.1990966796875, 'logits/rejected': 0.25632014870643616, 'logits/chosen': 0.19055268168449402, 'epoch': 6.31}

 70%|███████   | 11296/16110 [6:30:57<23:43:02, 17.74s/it]

 70%|███████   | 11297/16110 [6:31:15<23:52:44, 17.86s/it]


 70%|███████   | 11299/16110 [6:31:48<23:15:10, 17.40s/it]

 70%|███████   | 11300/16110 [6:32:08<24:16:58, 18.17s/it]
{'loss': 0.3332, 'learning_rate': 1.986572964265828e-06, 'rewards/chosen': -5.213388919830322, 'rewards/rejected': -11.917600631713867, 'rewards/accuracies': 0.875, 'rewards/margins': 6.704211711883545, 'policy_logps/rejected': -505.0364990234375, 'policy_logps/chosen': -396.7897644042969, 'referece_logps/rejected': -385.8605041503906, 'referece_logps/chosen': -344.65582275390625, 'logits/rejected': -0.15606442093849182, 'logits/chosen': -0.19566477835178375, 'epoch': 6.31}


 70%|███████   | 11302/16110 [6:32:42<23:01:03, 17.23s/it]

 70%|███████   | 11303/16110 [6:32:56<21:38:10, 16.20s/it]
{'loss': 0.1492, 'learning_rate': 1.986474278150349e-06, 'rewards/chosen': -5.063729763031006, 'rewards/rejected': -11.125157356262207, 'rewards/accuracies': 0.875, 'rewards/margins': 6.061427593231201, 'policy_logps/rejected': -465.46136474609375, 'policy_logps/chosen': -441.3989562988281, 'referece_logps/rejected': -354.2098388671875, 'referece_logps/chosen': -390.7616271972656, 'logits/rejected': 0.2694581151008606, 'logits/chosen': 0.1938350796699524, 'epoch': 6.31}

 70%|███████   | 11304/16110 [6:33:17<23:31:05, 17.62s/it]


 70%|███████   | 11306/16110 [6:33:52<24:17:48, 18.21s/it]

 70%|███████   | 11307/16110 [6:34:15<25:52:31, 19.39s/it]

 70%|███████   | 11308/16110 [6:34:26<22:41:44, 17.01s/it]
{'loss': 0.2155, 'learning_rate': 1.9863090038289057e-06, 'rewards/chosen': -3.7994298934936523, 'rewards/rejected': -6.5705742835998535, 'rewards/accuracies': 0.875, 'rewards/margins': 2.771144390106201, 'policy_logps/rejected': -372.7403259277344, 'policy_logps/chosen': -414.46905517578125, 'referece_logps/rejected': -307.0346374511719, 'referece_logps/chosen': -376.4747619628906, 'logits/rejected': 0.20807436108589172, 'logits/chosen': 0.17447221279144287, 'epoch': 6.32}


 70%|███████   | 11310/16110 [6:34:56<21:18:53, 15.99s/it]

 70%|███████   | 11311/16110 [6:35:14<22:17:11, 16.72s/it]
{'loss': 0.1645, 'learning_rate': 1.9862093608174425e-06, 'rewards/chosen': -3.363826274871826, 'rewards/rejected': -7.707770824432373, 'rewards/accuracies': 1.0, 'rewards/margins': 4.343944549560547, 'policy_logps/rejected': -368.53369140625, 'policy_logps/chosen': -457.76580810546875, 'referece_logps/rejected': -291.45599365234375, 'referece_logps/chosen': -424.12750244140625, 'logits/rejected': 0.12665611505508423, 'logits/chosen': 0.08078957349061966, 'epoch': 6.32}

 70%|███████   | 11312/16110 [6:35:33<23:02:21, 17.29s/it]


 70%|███████   | 11314/16110 [6:36:10<23:46:45, 17.85s/it]

 70%|███████   | 11315/16110 [6:36:28<23:50:34, 17.90s/it]
{'loss': 0.183, 'learning_rate': 1.986075945388954e-06, 'rewards/chosen': -4.2708282470703125, 'rewards/rejected': -9.156391143798828, 'rewards/accuracies': 1.0, 'rewards/margins': 4.885562896728516, 'policy_logps/rejected': -457.20111083984375, 'policy_logps/chosen': -370.0315246582031, 'referece_logps/rejected': -365.63720703125, 'referece_logps/chosen': -327.3232727050781, 'logits/rejected': -0.8378879427909851, 'logits/chosen': -0.77713942527771, 'epoch': 6.32}

 70%|███████   | 11316/16110 [6:36:44<22:52:19, 17.18s/it]


 70%|███████   | 11318/16110 [6:37:21<23:33:09, 17.69s/it]
{'loss': 0.2388, 'learning_rate': 1.9859754653048713e-06, 'rewards/chosen': -5.330691337585449, 'rewards/rejected': -9.145411491394043, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8147199153900146, 'policy_logps/rejected': -522.7687377929688, 'policy_logps/chosen': -382.8617248535156, 'referece_logps/rejected': -431.3146667480469, 'referece_logps/chosen': -329.5548095703125, 'logits/rejected': 0.4767445921897888, 'logits/chosen': 0.4464670419692993, 'epoch': 6.32}

 70%|███████   | 11319/16110 [6:37:40<24:18:43, 18.27s/it]

 70%|███████   | 11320/16110 [6:38:02<25:30:28, 19.17s/it]

 70%|███████   | 11321/16110 [6:38:23<26:21:17, 19.81s/it]

 70%|███████   | 11322/16110 [6:38:41<25:48:57, 19.41s/it]

 70%|███████   | 11323/16110 [6:38:57<24:22:28, 18.33s/it]


 70%|███████   | 11325/16110 [6:39:37<25:41:31, 19.33s/it]

 70%|███████   | 11326/16110 [6:39:53<24:16:42, 18.27s/it]

 70%|███████   | 11327/16110 [6:40:13<25:04:01, 18.87s/it]
{'loss': 0.2466, 'learning_rate': 1.985671873091992e-06, 'rewards/chosen': -5.768811225891113, 'rewards/rejected': -8.510443687438965, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7416324615478516, 'policy_logps/rejected': -444.6748046875, 'policy_logps/chosen': -307.05828857421875, 'referece_logps/rejected': -359.57037353515625, 'referece_logps/chosen': -249.37017822265625, 'logits/rejected': -0.12687808275222778, 'logits/chosen': 0.10848119109869003, 'epoch': 6.33}


 70%|███████   | 11329/16110 [6:40:49<24:45:59, 18.65s/it]
{'loss': 0.1842, 'learning_rate': 1.9856039698634175e-06, 'rewards/chosen': -3.2416279315948486, 'rewards/rejected': -7.420317649841309, 'rewards/accuracies': 0.875, 'rewards/margins': 4.178689479827881, 'policy_logps/rejected': -504.9777526855469, 'policy_logps/chosen': -384.71954345703125, 'referece_logps/rejected': -430.7745361328125, 'referece_logps/chosen': -352.30328369140625, 'logits/rejected': -0.2553531527519226, 'logits/chosen': -0.3869777321815491, 'epoch': 6.33}

 70%|███████   | 11330/16110 [6:41:10<25:41:07, 19.34s/it]

 70%|███████   | 11331/16110 [6:41:31<26:19:23, 19.83s/it]

 70%|███████   | 11332/16110 [6:41:50<26:06:30, 19.67s/it]

 70%|███████   | 11333/16110 [6:42:12<26:57:11, 20.31s/it]

 70%|███████   | 11334/16110 [6:42:31<26:41:25, 20.12s/it]

 70%|███████   | 11335/16110 [6:42:52<26:51:51, 20.25s/it]

 70%|███████   | 11336/16110 [6:43:12<26:34:01, 20.03s/it]

 70%|███████   | 11337/16110 [6:43:32<26:39:51, 20.11s/it]

 70%|███████   | 11338/16110 [6:43:52<26:36:53, 20.08s/it]


 70%|███████   | 11340/16110 [6:44:25<24:36:48, 18.58s/it]
{'loss': 0.1509, 'learning_rate': 1.985227653925786e-06, 'rewards/chosen': -5.725877285003662, 'rewards/rejected': -9.649685859680176, 'rewards/accuracies': 0.875, 'rewards/margins': 3.923809051513672, 'policy_logps/rejected': -554.32275390625, 'policy_logps/chosen': -476.11688232421875, 'referece_logps/rejected': -457.82586669921875, 'referece_logps/chosen': -418.85809326171875, 'logits/rejected': 0.4219500422477722, 'logits/chosen': 0.49699169397354126, 'epoch': 6.34}

 70%|███████   | 11341/16110 [6:44:45<25:18:27, 19.10s/it]


 70%|███████   | 11343/16110 [6:45:17<23:25:28, 17.69s/it]
{'loss': 0.1051, 'learning_rate': 1.985124185932822e-06, 'rewards/chosen': -3.8258562088012695, 'rewards/rejected': -7.035616874694824, 'rewards/accuracies': 0.75, 'rewards/margins': 3.2097604274749756, 'policy_logps/rejected': -359.107421875, 'policy_logps/chosen': -408.9284973144531, 'referece_logps/rejected': -288.7512512207031, 'referece_logps/chosen': -370.669921875, 'logits/rejected': -1.0398118495941162, 'logits/chosen': -1.1190404891967773, 'epoch': 6.34}

 70%|███████   | 11344/16110 [6:45:39<24:55:53, 18.83s/it]

 70%|███████   | 11345/16110 [6:46:00<25:54:37, 19.58s/it]

 70%|███████   | 11346/16110 [6:46:16<24:33:44, 18.56s/it]

 70%|███████   | 11347/16110 [6:46:36<25:10:41, 19.03s/it]

 70%|███████   | 11348/16110 [6:46:50<22:58:53, 17.37s/it]


 70%|███████   | 11350/16110 [6:47:27<23:40:57, 17.91s/it]
{'loss': 0.2011, 'learning_rate': 1.984881367001045e-06, 'rewards/chosen': -2.947608709335327, 'rewards/rejected': -7.016631126403809, 'rewards/accuracies': 1.0, 'rewards/margins': 4.069023132324219, 'policy_logps/rejected': -301.20513916015625, 'policy_logps/chosen': -239.0887451171875, 'referece_logps/rejected': -231.03878784179688, 'referece_logps/chosen': -209.61265563964844, 'logits/rejected': -0.4480843245983124, 'logits/chosen': -0.5798503756523132, 'epoch': 6.34}

 70%|███████   | 11351/16110 [6:47:44<23:03:43, 17.45s/it]

 70%|███████   | 11352/16110 [6:48:06<24:53:08, 18.83s/it]

 70%|███████   | 11353/16110 [6:48:23<24:12:34, 18.32s/it]

 70%|███████   | 11354/16110 [6:48:40<23:53:19, 18.08s/it]

 70%|███████   | 11355/16110 [6:48:57<23:18:02, 17.64s/it]

 70%|███████   | 11356/16110 [6:49:17<24:18:27, 18.41s/it]


 71%|███████   | 11358/16110 [6:49:56<25:06:33, 19.02s/it]

 71%|███████   | 11359/16110 [6:50:16<25:28:21, 19.30s/it]
{'loss': 0.1283, 'learning_rate': 1.9845663050072333e-06, 'rewards/chosen': -3.9673516750335693, 'rewards/rejected': -8.948236465454102, 'rewards/accuracies': 0.875, 'rewards/margins': 4.9808855056762695, 'policy_logps/rejected': -331.40997314453125, 'policy_logps/chosen': -353.57080078125, 'referece_logps/rejected': -241.9276123046875, 'referece_logps/chosen': -313.8973083496094, 'logits/rejected': 0.3574206531047821, 'logits/chosen': 0.28659701347351074, 'epoch': 6.35}

 71%|███████   | 11360/16110 [6:50:29<23:15:51, 17.63s/it]

 71%|███████   | 11361/16110 [6:50:46<22:59:01, 17.42s/it]

 71%|███████   | 11362/16110 [6:51:06<23:56:10, 18.15s/it]

 71%|███████   | 11363/16110 [6:51:21<22:40:29, 17.20s/it]

 71%|███████   | 11364/16110 [6:51:42<24:16:58, 18.42s/it]

 71%|███████   | 11365/16110 [6:52:03<25:12:54, 19.13s/it]

 71%|███████   | 11366/16110 [6:52:22<25:11:21, 19.11s/it]

 71%|███████   | 11367/16110 [6:52:37<23:34:15, 17.89s/it]

 71%|███████   | 11368/16110 [6:52:58<24:42:04, 18.75s/it]


 71%|███████   | 11370/16110 [6:53:32<23:23:27, 17.77s/it]

 71%|███████   | 11371/16110 [6:53:50<23:34:32, 17.91s/it]
{'loss': 0.2738, 'learning_rate': 1.984141208122555e-06, 'rewards/chosen': -4.648916721343994, 'rewards/rejected': -7.968379974365234, 'rewards/accuracies': 0.875, 'rewards/margins': 3.319462537765503, 'policy_logps/rejected': -376.8656005859375, 'policy_logps/chosen': -442.1195373535156, 'referece_logps/rejected': -297.1817626953125, 'referece_logps/chosen': -395.63037109375, 'logits/rejected': -0.5043803453445435, 'logits/chosen': -0.588876485824585, 'epoch': 6.35}

 71%|███████   | 11372/16110 [6:54:09<24:15:10, 18.43s/it]

 71%|███████   | 11373/16110 [6:54:29<24:52:31, 18.90s/it]

 71%|███████   | 11374/16110 [6:54:45<23:23:20, 17.78s/it]

 71%|███████   | 11375/16110 [6:55:05<24:16:24, 18.46s/it]

 71%|███████   | 11376/16110 [6:55:20<22:52:12, 17.39s/it]

 71%|███████   | 11377/16110 [6:55:39<23:33:44, 17.92s/it]

 71%|███████   | 11378/16110 [6:55:59<24:21:33, 18.53s/it]

 71%|███████   | 11379/16110 [6:56:17<24:06:22, 18.34s/it]

 71%|███████   | 11380/16110 [6:56:35<23:59:49, 18.26s/it]

 71%|███████   | 11381/16110 [6:56:51<23:07:45, 17.61s/it]

 71%|███████   | 11382/16110 [6:57:11<24:02:50, 18.31s/it]

 71%|███████   | 11383/16110 [6:57:30<24:19:09, 18.52s/it]

 71%|███████   | 11384/16110 [6:57:50<24:53:49, 18.97s/it]

 71%|███████   | 11385/16110 [6:58:08<24:48:05, 18.90s/it]

 71%|███████   | 11386/16110 [6:58:24<23:20:57, 17.79s/it]

 71%|███████   | 11387/16110 [6:58:39<22:33:26, 17.19s/it]

 71%|███████   | 11388/16110 [6:58:57<22:44:20, 17.34s/it]

 71%|███████   | 11389/16110 [6:59:19<24:42:33, 18.84s/it]

 71%|███████   | 11390/16110 [6:59:41<25:40:31, 19.58s/it]

 71%|███████   | 11391/16110 [6:59:56<23:54:21, 18.24s/it]

 71%|███████   | 11392/16110 [7:00:12<22:53:59, 17.47s/it]

 71%|███████   | 11393/16110 [7:00:31<23:45:37, 18.13s/it]

 71%|███████   | 11394/16110 [7:00:44<21:50:36, 16.67s/it]

 71%|███████   | 11395/16110 [7:01:07<23:58:57, 18.31s/it]

 71%|███████   | 11396/16110 [7:01:27<24:47:58, 18.94s/it]

 71%|███████   | 11397/16110 [7:01:47<25:16:05, 19.30s/it]

 71%|███████   | 11398/16110 [7:02:05<24:37:46, 18.82s/it]


 71%|███████   | 11400/16110 [7:02:40<23:56:06, 18.29s/it]
{'loss': 0.1411, 'learning_rate': 1.9830902465256184e-06, 'rewards/chosen': -3.477954864501953, 'rewards/rejected': -10.06785774230957, 'rewards/accuracies': 1.0, 'rewards/margins': 6.589902877807617, 'policy_logps/rejected': -433.437744140625, 'policy_logps/chosen': -496.3421325683594, 'referece_logps/rejected': -332.7591857910156, 'referece_logps/chosen': -461.5625915527344, 'logits/rejected': 0.11064757406711578, 'logits/chosen': -0.006645113229751587, 'epoch': 6.37}


 71%|███████   | 11402/16110 [7:03:18<24:13:24, 18.52s/it]
{'loss': 0.3095, 'learning_rate': 1.9830165341497037e-06, 'rewards/chosen': -2.998507022857666, 'rewards/rejected': -7.261242389678955, 'rewards/accuracies': 0.875, 'rewards/margins': 4.262735843658447, 'policy_logps/rejected': -289.4363098144531, 'policy_logps/chosen': -478.48553466796875, 'referece_logps/rejected': -216.82388305664062, 'referece_logps/chosen': -448.5003967285156, 'logits/rejected': -0.14687249064445496, 'logits/chosen': -0.5152089595794678, 'epoch': 6.37}

 71%|███████   | 11403/16110 [7:03:31<21:50:49, 16.71s/it]


 71%|███████   | 11405/16110 [7:04:04<21:30:31, 16.46s/it]

 71%|███████   | 11406/16110 [7:04:20<21:24:06, 16.38s/it]

 71%|███████   | 11407/16110 [7:04:34<20:25:24, 15.63s/it]
{'loss': 0.2259, 'learning_rate': 1.9828315578868324e-06, 'rewards/chosen': -2.5961194038391113, 'rewards/rejected': -8.328544616699219, 'rewards/accuracies': 1.0, 'rewards/margins': 5.732424259185791, 'policy_logps/rejected': -453.2020568847656, 'policy_logps/chosen': -327.77313232421875, 'referece_logps/rejected': -369.9165954589844, 'referece_logps/chosen': -301.81195068359375, 'logits/rejected': -0.7245017290115356, 'logits/chosen': -0.6567326188087463, 'epoch': 6.37}

 71%|███████   | 11408/16110 [7:04:53<21:38:49, 16.57s/it]

 71%|███████   | 11409/16110 [7:05:11<22:02:55, 16.88s/it]

 71%|███████   | 11410/16110 [7:05:31<23:28:09, 17.98s/it]

 71%|███████   | 11411/16110 [7:05:44<21:30:46, 16.48s/it]

 71%|███████   | 11412/16110 [7:06:06<23:29:32, 18.00s/it]

 71%|███████   | 11413/16110 [7:06:29<25:40:11, 19.67s/it]

 71%|███████   | 11414/16110 [7:06:50<25:53:39, 19.85s/it]


 71%|███████   | 11416/16110 [7:07:17<21:20:58, 16.37s/it]

 71%|███████   | 11417/16110 [7:07:34<21:55:10, 16.81s/it]
{'loss': 0.183, 'learning_rate': 1.982458626041912e-06, 'rewards/chosen': -5.61922025680542, 'rewards/rejected': -11.048898696899414, 'rewards/accuracies': 0.875, 'rewards/margins': 5.429678916931152, 'policy_logps/rejected': -349.19866943359375, 'policy_logps/chosen': -469.85198974609375, 'referece_logps/rejected': -238.70965576171875, 'referece_logps/chosen': -413.6598205566406, 'logits/rejected': 0.33247819542884827, 'logits/chosen': 0.3976341187953949, 'epoch': 6.38}

 71%|███████   | 11418/16110 [7:07:50<21:28:28, 16.48s/it]

 71%|███████   | 11419/16110 [7:08:13<23:48:43, 18.27s/it]

 71%|███████   | 11420/16110 [7:08:31<23:38:50, 18.15s/it]

 71%|███████   | 11421/16110 [7:08:45<22:12:56, 17.06s/it]

 71%|███████   | 11422/16110 [7:09:06<23:36:13, 18.13s/it]

 71%|███████   | 11423/16110 [7:09:21<22:32:15, 17.31s/it]

 71%|███████   | 11424/16110 [7:09:36<21:27:02, 16.48s/it]

 71%|███████   | 11425/16110 [7:09:56<23:09:34, 17.80s/it]

 71%|███████   | 11426/16110 [7:10:17<24:04:48, 18.51s/it]

 71%|███████   | 11427/16110 [7:10:36<24:35:41, 18.91s/it]

 71%|███████   | 11428/16110 [7:10:49<22:02:56, 16.95s/it]

 71%|███████   | 11429/16110 [7:11:03<20:49:11, 16.01s/it]

 71%|███████   | 11430/16110 [7:11:17<20:17:49, 15.61s/it]

 71%|███████   | 11431/16110 [7:11:36<21:37:19, 16.64s/it]

 71%|███████   | 11432/16110 [7:11:51<21:00:36, 16.17s/it]

 71%|███████   | 11433/16110 [7:12:13<23:05:04, 17.77s/it]

 71%|███████   | 11434/16110 [7:12:24<20:39:01, 15.90s/it]

 71%|███████   | 11435/16110 [7:12:45<22:24:32, 17.26s/it]

 71%|███████   | 11436/16110 [7:13:02<22:24:54, 17.26s/it]

 71%|███████   | 11437/16110 [7:13:23<23:43:44, 18.28s/it]

 71%|███████   | 11438/16110 [7:13:43<24:29:31, 18.87s/it]

 71%|███████   | 11439/16110 [7:13:58<22:52:12, 17.63s/it]

 71%|███████   | 11440/16110 [7:14:19<24:04:02, 18.55s/it]

 71%|███████   | 11441/16110 [7:14:41<25:34:29, 19.72s/it]

 71%|███████   | 11442/16110 [7:15:01<25:31:52, 19.69s/it]


 71%|███████   | 11444/16110 [7:15:34<23:28:56, 18.12s/it]
{'loss': 0.2491, 'learning_rate': 1.981431878398008e-06, 'rewards/chosen': -3.7499194145202637, 'rewards/rejected': -7.721560001373291, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9716408252716064, 'policy_logps/rejected': -344.50567626953125, 'policy_logps/chosen': -436.811279296875, 'referece_logps/rejected': -267.2900390625, 'referece_logps/chosen': -399.31201171875, 'logits/rejected': 0.8899385333061218, 'logits/chosen': 0.9444950222969055, 'epoch': 6.39}

 71%|███████   | 11445/16110 [7:15:51<23:18:27, 17.99s/it]


 71%|███████   | 11447/16110 [7:16:34<25:16:08, 19.51s/it]

 71%|███████   | 11448/16110 [7:16:54<25:26:06, 19.64s/it]

 71%|███████   | 11449/16110 [7:17:13<25:13:06, 19.48s/it]
{'loss': 0.2567, 'learning_rate': 1.9812385654224854e-06, 'rewards/chosen': -4.345475196838379, 'rewards/rejected': -7.80343770980835, 'rewards/accuracies': 1.0, 'rewards/margins': 3.45796275138855, 'policy_logps/rejected': -254.755859375, 'policy_logps/chosen': -354.99200439453125, 'referece_logps/rejected': -176.72146606445312, 'referece_logps/chosen': -311.5372314453125, 'logits/rejected': -0.32718783617019653, 'logits/chosen': -0.6002522110939026, 'epoch': 6.4}


 71%|███████   | 11451/16110 [7:17:43<22:01:04, 17.01s/it]

 71%|███████   | 11452/16110 [7:18:04<23:40:52, 18.30s/it]

 71%|███████   | 11453/16110 [7:18:24<24:15:50, 18.76s/it]

 71%|███████   | 11454/16110 [7:18:43<24:25:45, 18.89s/it]

 71%|███████   | 11455/16110 [7:18:56<22:07:07, 17.11s/it]

 71%|███████   | 11456/16110 [7:19:15<22:34:56, 17.47s/it]

 71%|███████   | 11457/16110 [7:19:29<21:31:09, 16.65s/it]

 71%|███████   | 11458/16110 [7:19:45<21:03:34, 16.30s/it]

 71%|███████   | 11459/16110 [7:20:07<23:17:48, 18.03s/it]

 71%|███████   | 11460/16110 [7:20:30<25:22:40, 19.65s/it]

 71%|███████   | 11461/16110 [7:20:50<25:31:02, 19.76s/it]

 71%|███████   | 11462/16110 [7:21:06<24:04:39, 18.65s/it]

 71%|███████   | 11463/16110 [7:21:23<23:19:58, 18.08s/it]

 71%|███████   | 11464/16110 [7:21:41<23:16:51, 18.04s/it]

 71%|███████   | 11465/16110 [7:21:54<21:29:03, 16.65s/it]

 71%|███████   | 11466/16110 [7:22:07<20:02:59, 15.54s/it]

 71%|███████   | 11467/16110 [7:22:30<22:53:34, 17.75s/it]

 71%|███████   | 11468/16110 [7:22:49<23:21:57, 18.12s/it]

 71%|███████   | 11469/16110 [7:23:05<22:33:41, 17.50s/it]

 71%|███████   | 11470/16110 [7:23:24<23:04:31, 17.90s/it]

 71%|███████   | 11471/16110 [7:23:47<25:08:55, 19.52s/it]

 71%|███████   | 11472/16110 [7:24:04<23:48:36, 18.48s/it]

 71%|███████   | 11473/16110 [7:24:24<24:38:43, 19.13s/it]

 71%|███████   | 11474/16110 [7:24:45<25:08:33, 19.52s/it]

 71%|███████   | 11475/16110 [7:25:03<24:37:43, 19.13s/it]

 71%|███████   | 11476/16110 [7:25:22<24:43:12, 19.20s/it]

 71%|███████   | 11477/16110 [7:25:37<22:55:15, 17.81s/it]

 71%|███████   | 11478/16110 [7:25:49<20:57:16, 16.29s/it]

 71%|███████▏  | 11479/16110 [7:26:10<22:23:53, 17.41s/it]

 71%|███████▏  | 11480/16110 [7:26:24<21:06:10, 16.41s/it]
{'loss': 0.1434, 'learning_rate': 1.980017901007976e-06, 'rewards/chosen': -3.538259506225586, 'rewards/rejected': -7.733400821685791, 'rewards/accuracies': 1.0, 'rewards/margins': 4.195141315460205, 'policy_logps/rejected': -383.0789794921875, 'policy_logps/chosen': -284.8517150878906, 'referece_logps/rejected': -305.7449951171875, 'referece_logps/chosen': -249.4691162109375, 'logits/rejected': -0.7975579500198364, 'logits/chosen': -0.7127402424812317, 'epoch': 6.41}


 71%|███████▏  | 11482/16110 [7:27:02<22:40:11, 17.63s/it]

 71%|███████▏  | 11483/16110 [7:27:21<23:14:30, 18.08s/it]

 71%|███████▏  | 11484/16110 [7:27:42<24:16:21, 18.89s/it]

 71%|███████▏  | 11485/16110 [7:28:02<24:43:20, 19.24s/it]

 71%|███████▏  | 11486/16110 [7:28:19<23:48:27, 18.54s/it]

 71%|███████▏  | 11487/16110 [7:28:39<24:40:15, 19.21s/it]

 71%|███████▏  | 11488/16110 [7:28:57<24:13:23, 18.87s/it]

 71%|███████▏  | 11489/16110 [7:29:11<22:03:45, 17.19s/it]

 71%|███████▏  | 11490/16110 [7:29:31<23:09:12, 18.04s/it]

 71%|███████▏  | 11491/16110 [7:29:53<24:37:32, 19.19s/it]

 71%|███████▏  | 11492/16110 [7:30:08<23:19:44, 18.19s/it]

 71%|███████▏  | 11493/16110 [7:30:21<21:11:38, 16.53s/it]

 71%|███████▏  | 11494/16110 [7:30:41<22:25:10, 17.48s/it]
{'loss': 0.2182, 'learning_rate': 1.9794541522469927e-06, 'rewards/chosen': -4.325125217437744, 'rewards/rejected': -8.892766952514648, 'rewards/accuracies': 1.0, 'rewards/margins': 4.567641735076904, 'policy_logps/rejected': -425.38751220703125, 'policy_logps/chosen': -424.64117431640625, 'referece_logps/rejected': -336.4598388671875, 'referece_logps/chosen': -381.3900146484375, 'logits/rejected': 0.18492160737514496, 'logits/chosen': 0.13232865929603577, 'epoch': 6.42}


 71%|███████▏  | 11496/16110 [7:31:11<20:29:43, 15.99s/it]

 71%|███████▏  | 11497/16110 [7:31:29<21:00:16, 16.39s/it]

 71%|███████▏  | 11498/16110 [7:31:46<21:11:54, 16.55s/it]

 71%|███████▏  | 11499/16110 [7:32:03<21:41:30, 16.94s/it]

 71%|███████▏  | 11500/16110 [7:32:23<22:43:33, 17.75s/it]
{'loss': 0.1432, 'learning_rate': 1.9792101699592584e-06, 'rewards/chosen': -3.9353103637695312, 'rewards/rejected': -6.487887859344482, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5525777339935303, 'policy_logps/rejected': -319.11175537109375, 'policy_logps/chosen': -658.2482299804688, 'referece_logps/rejected': -254.23287963867188, 'referece_logps/chosen': -618.8950805664062, 'logits/rejected': -0.051963284611701965, 'logits/chosen': -0.42329534888267517, 'epoch': 6.42}


 71%|███████▏  | 11502/16110 [7:33:20<28:21:00, 22.15s/it]

 71%|███████▏  | 11503/16110 [7:33:42<28:25:28, 22.21s/it]

 71%|███████▏  | 11504/16110 [7:34:02<27:27:51, 21.47s/it]
{'loss': 0.2867, 'learning_rate': 1.9790467234711517e-06, 'rewards/chosen': -4.251341819763184, 'rewards/rejected': -8.452824592590332, 'rewards/accuracies': 1.0, 'rewards/margins': 4.20148229598999, 'policy_logps/rejected': -313.4419250488281, 'policy_logps/chosen': -316.3736267089844, 'referece_logps/rejected': -228.91368103027344, 'referece_logps/chosen': -273.8602294921875, 'logits/rejected': -0.25369688868522644, 'logits/chosen': -0.35263508558273315, 'epoch': 6.43}


 71%|███████▏  | 11506/16110 [7:34:42<26:19:10, 20.58s/it]

 71%|███████▏  | 11507/16110 [7:34:58<24:32:19, 19.19s/it]

 71%|███████▏  | 11508/16110 [7:35:10<21:47:44, 17.05s/it]

 71%|███████▏  | 11509/16110 [7:35:26<21:29:44, 16.82s/it]

 71%|███████▏  | 11510/16110 [7:35:41<21:00:42, 16.44s/it]

 71%|███████▏  | 11511/16110 [7:35:59<21:35:12, 16.90s/it]

 71%|███████▏  | 11512/16110 [7:36:14<20:49:29, 16.30s/it]

 71%|███████▏  | 11513/16110 [7:36:30<20:26:48, 16.01s/it]

 71%|███████▏  | 11514/16110 [7:36:50<21:58:04, 17.21s/it]

 71%|███████▏  | 11515/16110 [7:37:10<22:59:14, 18.01s/it]

 71%|███████▏  | 11516/16110 [7:37:30<24:04:53, 18.87s/it]

 71%|███████▏  | 11517/16110 [7:37:48<23:30:59, 18.43s/it]

 71%|███████▏  | 11518/16110 [7:38:08<24:11:52, 18.97s/it]

 72%|███████▏  | 11519/16110 [7:38:28<24:23:37, 19.13s/it]
{'loss': 0.1693, 'learning_rate': 1.97842816074535e-06, 'rewards/chosen': -5.350231647491455, 'rewards/rejected': -9.627927780151367, 'rewards/accuracies': 1.0, 'rewards/margins': 4.27769660949707, 'policy_logps/rejected': -531.1417236328125, 'policy_logps/chosen': -535.7051391601562, 'referece_logps/rejected': -434.86248779296875, 'referece_logps/chosen': -482.2028503417969, 'logits/rejected': 0.3967018723487854, 'logits/chosen': 0.14138050377368927, 'epoch': 6.44}

 72%|███████▏  | 11520/16110 [7:38:45<23:41:45, 18.59s/it]


 72%|███████▏  | 11522/16110 [7:39:18<22:41:56, 17.81s/it]

 72%|███████▏  | 11523/16110 [7:39:38<23:23:33, 18.36s/it]

 72%|███████▏  | 11524/16110 [7:39:56<23:19:27, 18.31s/it]
{'loss': 0.1244, 'learning_rate': 1.978219995452631e-06, 'rewards/chosen': -3.308107614517212, 'rewards/rejected': -7.606943130493164, 'rewards/accuracies': 0.875, 'rewards/margins': 4.298835277557373, 'policy_logps/rejected': -389.8800048828125, 'policy_logps/chosen': -326.5094909667969, 'referece_logps/rejected': -313.8105773925781, 'referece_logps/chosen': -293.42840576171875, 'logits/rejected': -0.0105266273021698, 'logits/chosen': -0.008013397455215454, 'epoch': 6.44}


 72%|███████▏  | 11526/16110 [7:40:22<19:56:03, 15.66s/it]
{'loss': 0.1192, 'learning_rate': 1.978136452541219e-06, 'rewards/chosen': -4.149973392486572, 'rewards/rejected': -7.594454288482666, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4444808959960938, 'policy_logps/rejected': -457.9547119140625, 'policy_logps/chosen': -520.0543212890625, 'referece_logps/rejected': -382.01019287109375, 'referece_logps/chosen': -478.55462646484375, 'logits/rejected': -0.5929890275001526, 'logits/chosen': -0.6328767538070679, 'epoch': 6.44}

 72%|███████▏  | 11527/16110 [7:40:37<19:39:20, 15.44s/it]

 72%|███████▏  | 11528/16110 [7:40:53<19:52:45, 15.62s/it]
[2024-04-05 22:49:06,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 72%|███████▏  | 11529/16110 [7:41:15<22:14:08, 17.47s/it]


 72%|███████▏  | 11531/16110 [7:41:55<23:55:08, 18.81s/it]

 72%|███████▏  | 11532/16110 [7:42:12<23:21:16, 18.37s/it]

 72%|███████▏  | 11533/16110 [7:42:27<21:59:06, 17.29s/it]
{'loss': 0.1015, 'learning_rate': 1.977842807025135e-06, 'rewards/chosen': -4.201982021331787, 'rewards/rejected': -9.982560157775879, 'rewards/accuracies': 1.0, 'rewards/margins': 5.780578136444092, 'policy_logps/rejected': -363.04656982421875, 'policy_logps/chosen': -282.3426208496094, 'referece_logps/rejected': -263.2209777832031, 'referece_logps/chosen': -240.32281494140625, 'logits/rejected': -0.46011975407600403, 'logits/chosen': -0.469039648771286, 'epoch': 6.44}

 72%|███████▏  | 11534/16110 [7:42:43<21:39:53, 17.04s/it]

 72%|███████▏  | 11535/16110 [7:43:03<22:51:24, 17.99s/it]


 72%|███████▏  | 11537/16110 [7:43:38<22:10:24, 17.46s/it]
{'loss': 0.2422, 'learning_rate': 1.977674139997507e-06, 'rewards/chosen': -5.409546375274658, 'rewards/rejected': -7.4570465087890625, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0475003719329834, 'policy_logps/rejected': -374.4512634277344, 'policy_logps/chosen': -350.5230712890625, 'referece_logps/rejected': -299.8808288574219, 'referece_logps/chosen': -296.4276123046875, 'logits/rejected': -0.32621634006500244, 'logits/chosen': -0.46330589056015015, 'epoch': 6.45}


 72%|███████▏  | 11539/16110 [7:44:05<19:31:44, 15.38s/it]

 72%|███████▏  | 11540/16110 [7:44:22<20:06:41, 15.84s/it]
{'loss': 0.1639, 'learning_rate': 1.977547224778697e-06, 'rewards/chosen': -4.1331095695495605, 'rewards/rejected': -7.807108402252197, 'rewards/accuracies': 0.875, 'rewards/margins': 3.673999309539795, 'policy_logps/rejected': -292.11822509765625, 'policy_logps/chosen': -354.9203796386719, 'referece_logps/rejected': -214.047119140625, 'referece_logps/chosen': -313.5892639160156, 'logits/rejected': 0.4525339901447296, 'logits/chosen': 0.4724861979484558, 'epoch': 6.45}

 72%|███████▏  | 11541/16110 [7:44:40<20:51:30, 16.43s/it]

 72%|███████▏  | 11542/16110 [7:44:58<21:24:45, 16.88s/it]


 72%|███████▏  | 11544/16110 [7:45:37<23:05:40, 18.21s/it]

 72%|███████▏  | 11545/16110 [7:45:48<20:37:40, 16.27s/it]

 72%|███████▏  | 11546/16110 [7:45:59<18:28:22, 14.57s/it]
{'loss': 0.1717, 'learning_rate': 1.977292327531869e-06, 'rewards/chosen': -5.803267955780029, 'rewards/rejected': -8.446855545043945, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6435868740081787, 'policy_logps/rejected': -374.51678466796875, 'policy_logps/chosen': -381.66156005859375, 'referece_logps/rejected': -290.0482482910156, 'referece_logps/chosen': -323.62884521484375, 'logits/rejected': -0.7113234996795654, 'logits/chosen': -0.731619119644165, 'epoch': 6.45}


 72%|███████▏  | 11548/16110 [7:46:38<21:45:16, 17.17s/it]
{'loss': 0.1561, 'learning_rate': 1.977207045741668e-06, 'rewards/chosen': -4.147491455078125, 'rewards/rejected': -7.053350925445557, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9058592319488525, 'policy_logps/rejected': -337.648681640625, 'policy_logps/chosen': -426.70416259765625, 'referece_logps/rejected': -267.11517333984375, 'referece_logps/chosen': -385.22930908203125, 'logits/rejected': 0.25446414947509766, 'logits/chosen': 0.13463744521141052, 'epoch': 6.45}


 72%|███████▏  | 11550/16110 [7:47:08<20:48:25, 16.43s/it]

 72%|███████▏  | 11551/16110 [7:47:30<22:52:52, 18.07s/it]

 72%|███████▏  | 11552/16110 [7:47:44<21:28:28, 16.96s/it]

 72%|███████▏  | 11553/16110 [7:48:01<21:10:32, 16.73s/it]

 72%|███████▏  | 11554/16110 [7:48:19<21:43:35, 17.17s/it]

 72%|███████▏  | 11555/16110 [7:48:36<21:55:32, 17.33s/it]

 72%|███████▏  | 11556/16110 [7:48:57<23:11:37, 18.33s/it]

 72%|███████▏  | 11557/16110 [7:49:19<24:37:08, 19.47s/it]
{'loss': 0.156, 'learning_rate': 1.9768213226642796e-06, 'rewards/chosen': -3.55818247795105, 'rewards/rejected': -6.37605619430542, 'rewards/accuracies': 1.0, 'rewards/margins': 2.817873001098633, 'policy_logps/rejected': -394.0994873046875, 'policy_logps/chosen': -409.759765625, 'referece_logps/rejected': -330.3388977050781, 'referece_logps/chosen': -374.17791748046875, 'logits/rejected': 0.010075345635414124, 'logits/chosen': -0.14800618588924408, 'epoch': 6.46}

 72%|███████▏  | 11558/16110 [7:49:42<25:50:51, 20.44s/it]

 72%|███████▏  | 11559/16110 [7:50:03<26:12:42, 20.73s/it]


 72%|███████▏  | 11561/16110 [7:50:46<26:34:42, 21.03s/it]

 72%|███████▏  | 11562/16110 [7:51:05<25:33:46, 20.23s/it]
{'loss': 0.1964, 'learning_rate': 1.976605650050903e-06, 'rewards/chosen': -3.4209275245666504, 'rewards/rejected': -7.976663112640381, 'rewards/accuracies': 1.0, 'rewards/margins': 4.5557355880737305, 'policy_logps/rejected': -272.9087219238281, 'policy_logps/chosen': -245.1042938232422, 'referece_logps/rejected': -193.14208984375, 'referece_logps/chosen': -210.89501953125, 'logits/rejected': 0.048864252865314484, 'logits/chosen': 0.09918034821748734, 'epoch': 6.46}


 72%|███████▏  | 11564/16110 [7:51:37<23:06:35, 18.30s/it]

 72%|███████▏  | 11565/16110 [7:51:57<23:38:53, 18.73s/it]

 72%|███████▏  | 11566/16110 [7:52:15<23:28:36, 18.60s/it]
{'loss': 0.1547, 'learning_rate': 1.9764324013978047e-06, 'rewards/chosen': -3.760417938232422, 'rewards/rejected': -8.448832511901855, 'rewards/accuracies': 1.0, 'rewards/margins': 4.688414573669434, 'policy_logps/rejected': -358.0394592285156, 'policy_logps/chosen': -495.984130859375, 'referece_logps/rejected': -273.5511169433594, 'referece_logps/chosen': -458.37994384765625, 'logits/rejected': -0.5644607543945312, 'logits/chosen': -0.8199787139892578, 'epoch': 6.46}

 72%|███████▏  | 11567/16110 [7:52:28<21:08:23, 16.75s/it]


 72%|███████▏  | 11569/16110 [7:52:59<20:11:07, 16.00s/it]
{'loss': 0.342, 'learning_rate': 1.9763020504867454e-06, 'rewards/chosen': -6.207907676696777, 'rewards/rejected': -10.852874755859375, 'rewards/accuracies': 1.0, 'rewards/margins': 4.6449666023254395, 'policy_logps/rejected': -449.3262939453125, 'policy_logps/chosen': -588.9501342773438, 'referece_logps/rejected': -340.79754638671875, 'referece_logps/chosen': -526.8710327148438, 'logits/rejected': 0.034940093755722046, 'logits/chosen': -0.09505781531333923, 'epoch': 6.46}


 72%|███████▏  | 11571/16110 [7:53:27<18:31:55, 14.70s/it]

 72%|███████▏  | 11572/16110 [7:53:44<19:17:52, 15.31s/it]
{'loss': 0.1082, 'learning_rate': 1.9761713444101614e-06, 'rewards/chosen': -3.468796968460083, 'rewards/rejected': -6.760775089263916, 'rewards/accuracies': 1.0, 'rewards/margins': 3.291978597640991, 'policy_logps/rejected': -386.56732177734375, 'policy_logps/chosen': -415.0354309082031, 'referece_logps/rejected': -318.9595947265625, 'referece_logps/chosen': -380.3474426269531, 'logits/rejected': -0.42894357442855835, 'logits/chosen': -0.5371536016464233, 'epoch': 6.46}


 72%|███████▏  | 11574/16110 [7:54:23<22:26:17, 17.81s/it]
{'loss': 0.1605, 'learning_rate': 1.9760840097350674e-06, 'rewards/chosen': -3.619652509689331, 'rewards/rejected': -9.142241477966309, 'rewards/accuracies': 1.0, 'rewards/margins': 5.522589683532715, 'policy_logps/rejected': -384.793212890625, 'policy_logps/chosen': -487.7559814453125, 'referece_logps/rejected': -293.3708190917969, 'referece_logps/chosen': -451.5594482421875, 'logits/rejected': 0.13232511281967163, 'logits/chosen': -0.1534791886806488, 'epoch': 6.47}

 72%|███████▏  | 11575/16110 [7:54:44<23:45:15, 18.86s/it]

 72%|███████▏  | 11576/16110 [7:55:04<24:10:22, 19.19s/it]


 72%|███████▏  | 11578/16110 [7:55:41<23:35:03, 18.73s/it]

 72%|███████▏  | 11579/16110 [7:55:55<21:47:55, 17.32s/it]

 72%|███████▏  | 11580/16110 [7:56:15<22:49:12, 18.14s/it]

 72%|███████▏  | 11581/16110 [7:56:36<23:39:12, 18.80s/it]
{'loss': 0.1929, 'learning_rate': 1.9757770956633973e-06, 'rewards/chosen': -3.52445912361145, 'rewards/rejected': -8.284793853759766, 'rewards/accuracies': 0.875, 'rewards/margins': 4.760334014892578, 'policy_logps/rejected': -435.03363037109375, 'policy_logps/chosen': -406.80194091796875, 'referece_logps/rejected': -352.1856994628906, 'referece_logps/chosen': -371.557373046875, 'logits/rejected': -0.1573282927274704, 'logits/chosen': 0.09240594506263733, 'epoch': 6.47}

 72%|███████▏  | 11582/16110 [7:56:51<22:13:37, 17.67s/it]

 72%|███████▏  | 11583/16110 [7:57:06<21:32:40, 17.13s/it]


 72%|███████▏  | 11585/16110 [7:57:40<21:16:29, 16.93s/it]

 72%|███████▏  | 11586/16110 [7:57:57<21:26:13, 17.06s/it]
{'loss': 0.2038, 'learning_rate': 1.9755566880425424e-06, 'rewards/chosen': -3.2316951751708984, 'rewards/rejected': -8.182783126831055, 'rewards/accuracies': 0.875, 'rewards/margins': 4.9510884284973145, 'policy_logps/rejected': -367.5577087402344, 'policy_logps/chosen': -360.81689453125, 'referece_logps/rejected': -285.7298583984375, 'referece_logps/chosen': -328.49993896484375, 'logits/rejected': -0.15378692746162415, 'logits/chosen': -0.22860626876354218, 'epoch': 6.47}


 72%|███████▏  | 11588/16110 [7:58:31<21:11:27, 16.87s/it]
{'loss': 0.1916, 'learning_rate': 1.975468248952753e-06, 'rewards/chosen': -2.5122594833374023, 'rewards/rejected': -5.000735282897949, 'rewards/accuracies': 0.875, 'rewards/margins': 2.488476037979126, 'policy_logps/rejected': -357.6797180175781, 'policy_logps/chosen': -332.29010009765625, 'referece_logps/rejected': -307.6723937988281, 'referece_logps/chosen': -307.16754150390625, 'logits/rejected': 0.3365035355091095, 'logits/chosen': 0.3804684281349182, 'epoch': 6.47}

 72%|███████▏  | 11589/16110 [7:58:46<20:39:35, 16.45s/it]

 72%|███████▏  | 11590/16110 [7:59:03<20:41:46, 16.48s/it]

 72%|███████▏  | 11591/16110 [7:59:14<18:52:07, 15.03s/it]


 72%|███████▏  | 11593/16110 [7:59:55<22:24:37, 17.86s/it]

 72%|███████▏  | 11594/16110 [8:00:12<22:05:52, 17.62s/it]

 72%|███████▏  | 11595/16110 [8:00:33<23:28:58, 18.72s/it]
{'loss': 0.0667, 'learning_rate': 1.97515747021484e-06, 'rewards/chosen': -3.756863594055176, 'rewards/rejected': -9.079856872558594, 'rewards/accuracies': 1.0, 'rewards/margins': 5.322992324829102, 'policy_logps/rejected': -416.58160400390625, 'policy_logps/chosen': -404.3201599121094, 'referece_logps/rejected': -325.7830810546875, 'referece_logps/chosen': -366.75152587890625, 'logits/rejected': -0.1985989660024643, 'logits/chosen': -0.3927760720252991, 'epoch': 6.48}


 72%|███████▏  | 11597/16110 [8:01:03<21:30:43, 17.16s/it]
{'loss': 0.1296, 'learning_rate': 1.975068321513614e-06, 'rewards/chosen': -3.8814966678619385, 'rewards/rejected': -7.803430557250977, 'rewards/accuracies': 1.0, 'rewards/margins': 3.921933650970459, 'policy_logps/rejected': -327.7934875488281, 'policy_logps/chosen': -323.84552001953125, 'referece_logps/rejected': -249.75917053222656, 'referece_logps/chosen': -285.03057861328125, 'logits/rejected': 0.5353360772132874, 'logits/chosen': 0.3399122357368469, 'epoch': 6.48}


 72%|███████▏  | 11599/16110 [8:01:36<21:22:11, 17.05s/it]

 72%|███████▏  | 11600/16110 [8:01:56<22:22:27, 17.86s/it]
{'loss': 0.1104, 'learning_rate': 1.9749343028690216e-06, 'rewards/chosen': -3.5583395957946777, 'rewards/rejected': -8.320563316345215, 'rewards/accuracies': 1.0, 'rewards/margins': 4.762223720550537, 'policy_logps/rejected': -573.3475341796875, 'policy_logps/chosen': -497.7798767089844, 'referece_logps/rejected': -490.1418762207031, 'referece_logps/chosen': -462.1965026855469, 'logits/rejected': -0.051646433770656586, 'logits/chosen': -0.07830943167209625, 'epoch': 6.48}

 72%|███████▏  | 11601/16110 [8:02:07<19:44:25, 15.76s/it]

 72%|███████▏  | 11602/16110 [8:02:21<19:01:00, 15.19s/it]


 72%|███████▏  | 11604/16110 [8:02:56<20:28:31, 16.36s/it]

 72%|███████▏  | 11605/16110 [8:03:12<20:19:34, 16.24s/it]

 72%|███████▏  | 11606/16110 [8:03:33<22:15:47, 17.79s/it]
{'loss': 0.1285, 'learning_rate': 1.974665201624851e-06, 'rewards/chosen': -4.71038293838501, 'rewards/rejected': -9.052286148071289, 'rewards/accuracies': 1.0, 'rewards/margins': 4.341903209686279, 'policy_logps/rejected': -357.5006103515625, 'policy_logps/chosen': -319.3564453125, 'referece_logps/rejected': -266.9777526855469, 'referece_logps/chosen': -272.25262451171875, 'logits/rejected': -0.33273130655288696, 'logits/chosen': -0.3643518388271332, 'epoch': 6.48}


 72%|███████▏  | 11608/16110 [8:04:06<21:14:50, 16.99s/it]

 72%|███████▏  | 11609/16110 [8:04:26<22:19:50, 17.86s/it]
{'loss': 0.1583, 'learning_rate': 1.9745301191231675e-06, 'rewards/chosen': -5.375919818878174, 'rewards/rejected': -10.384703636169434, 'rewards/accuracies': 1.0, 'rewards/margins': 5.008782863616943, 'policy_logps/rejected': -465.7397155761719, 'policy_logps/chosen': -526.6778564453125, 'referece_logps/rejected': -361.8927001953125, 'referece_logps/chosen': -472.9186096191406, 'logits/rejected': -0.13299895823001862, 'logits/chosen': -0.3978586196899414, 'epoch': 6.49}


 72%|███████▏  | 11611/16110 [8:05:00<21:50:13, 17.47s/it]
{'loss': 0.1056, 'learning_rate': 1.9744398671629395e-06, 'rewards/chosen': -4.909310817718506, 'rewards/rejected': -8.801312446594238, 'rewards/accuracies': 1.0, 'rewards/margins': 3.892001152038574, 'policy_logps/rejected': -290.49896240234375, 'policy_logps/chosen': -389.053955078125, 'referece_logps/rejected': -202.48585510253906, 'referece_logps/chosen': -339.9608154296875, 'logits/rejected': 0.34132301807403564, 'logits/chosen': 0.25334569811820984, 'epoch': 6.49}

 72%|███████▏  | 11612/16110 [8:05:17<21:38:47, 17.33s/it]

 72%|███████▏  | 11613/16110 [8:05:37<22:51:42, 18.30s/it]


 72%|███████▏  | 11615/16110 [8:06:17<24:01:30, 19.24s/it]
{'loss': 0.1148, 'learning_rate': 1.9742588906063096e-06, 'rewards/chosen': -3.811319351196289, 'rewards/rejected': -8.859707832336426, 'rewards/accuracies': 0.875, 'rewards/margins': 5.048388481140137, 'policy_logps/rejected': -407.9000244140625, 'policy_logps/chosen': -488.1656799316406, 'referece_logps/rejected': -319.3029479980469, 'referece_logps/chosen': -450.052490234375, 'logits/rejected': 0.038950078189373016, 'logits/chosen': 0.11656016856431961, 'epoch': 6.49}

 72%|███████▏  | 11616/16110 [8:06:37<24:09:45, 19.36s/it]

 72%|███████▏  | 11617/16110 [8:06:51<22:04:17, 17.68s/it]


 72%|███████▏  | 11619/16110 [8:07:23<21:16:08, 17.05s/it]

 72%|███████▏  | 11620/16110 [8:07:40<21:02:26, 16.87s/it]

 72%|███████▏  | 11621/16110 [8:07:56<20:56:02, 16.79s/it]

 72%|███████▏  | 11622/16110 [8:08:16<21:57:17, 17.61s/it]

 72%|███████▏  | 11623/16110 [8:08:36<22:49:40, 18.32s/it]

 72%|███████▏  | 11624/16110 [8:08:58<24:10:03, 19.39s/it]

 72%|███████▏  | 11625/16110 [8:09:16<23:33:23, 18.91s/it]

 72%|███████▏  | 11626/16110 [8:09:36<24:13:05, 19.44s/it]

 72%|███████▏  | 11627/16110 [8:09:52<22:49:44, 18.33s/it]
{'loss': 0.1541, 'learning_rate': 1.9737121809027118e-06, 'rewards/chosen': -3.1904165744781494, 'rewards/rejected': -9.835261344909668, 'rewards/accuracies': 1.0, 'rewards/margins': 6.644845008850098, 'policy_logps/rejected': -376.7052307128906, 'policy_logps/chosen': -529.3984375, 'referece_logps/rejected': -278.35260009765625, 'referece_logps/chosen': -497.4942932128906, 'logits/rejected': -0.8785662055015564, 'logits/chosen': -0.9507465362548828, 'epoch': 6.5}


 72%|███████▏  | 11629/16110 [8:10:18<19:16:10, 15.48s/it]
{'loss': 0.1582, 'learning_rate': 1.9736205115184846e-06, 'rewards/chosen': -4.660782814025879, 'rewards/rejected': -7.301130294799805, 'rewards/accuracies': 1.0, 'rewards/margins': 2.640347480773926, 'policy_logps/rejected': -392.9844970703125, 'policy_logps/chosen': -545.3756713867188, 'referece_logps/rejected': -319.97320556640625, 'referece_logps/chosen': -498.7679138183594, 'logits/rejected': 0.11411350220441818, 'logits/chosen': -0.00200657919049263, 'epoch': 6.5}

 72%|███████▏  | 11630/16110 [8:10:29<17:42:23, 14.23s/it]

 72%|███████▏  | 11631/16110 [8:10:43<17:31:04, 14.08s/it]


 72%|███████▏  | 11633/16110 [8:11:16<18:44:13, 15.07s/it]
{'loss': 0.2141, 'learning_rate': 1.973436700511513e-06, 'rewards/chosen': -3.9205148220062256, 'rewards/rejected': -8.110098838806152, 'rewards/accuracies': 1.0, 'rewards/margins': 4.189583778381348, 'policy_logps/rejected': -356.508544921875, 'policy_logps/chosen': -445.8036193847656, 'referece_logps/rejected': -275.4076232910156, 'referece_logps/chosen': -406.5985412597656, 'logits/rejected': 0.06396855413913727, 'logits/chosen': -0.054394908249378204, 'epoch': 6.5}


 72%|███████▏  | 11635/16110 [8:11:51<20:05:10, 16.16s/it]
{'loss': 0.1692, 'learning_rate': 1.9733445589184877e-06, 'rewards/chosen': -4.926112174987793, 'rewards/rejected': -9.868858337402344, 'rewards/accuracies': 1.0, 'rewards/margins': 4.942746639251709, 'policy_logps/rejected': -367.30755615234375, 'policy_logps/chosen': -444.82623291015625, 'referece_logps/rejected': -268.61895751953125, 'referece_logps/chosen': -395.5651550292969, 'logits/rejected': 0.6054093837738037, 'logits/chosen': 0.2525182366371155, 'epoch': 6.5}

 72%|███████▏  | 11636/16110 [8:12:10<21:04:52, 16.96s/it]


 72%|███████▏  | 11638/16110 [8:12:43<20:50:19, 16.78s/it]

 72%|███████▏  | 11639/16110 [8:13:02<21:55:39, 17.66s/it]
{'loss': 0.1034, 'learning_rate': 1.973159803627847e-06, 'rewards/chosen': -3.328896999359131, 'rewards/rejected': -7.298568248748779, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9696717262268066, 'policy_logps/rejected': -447.91217041015625, 'policy_logps/chosen': -310.38800048828125, 'referece_logps/rejected': -374.9264221191406, 'referece_logps/chosen': -277.0990295410156, 'logits/rejected': -0.12502942979335785, 'logits/chosen': -0.014424830675125122, 'epoch': 6.5}

 72%|███████▏  | 11640/16110 [8:13:22<22:28:24, 18.10s/it]

 72%|███████▏  | 11641/16110 [8:13:41<23:09:33, 18.66s/it]

 72%|███████▏  | 11642/16110 [8:14:01<23:31:37, 18.96s/it]

 72%|███████▏  | 11643/16110 [8:14:23<24:46:21, 19.96s/it]


 72%|███████▏  | 11645/16110 [8:15:00<23:39:13, 19.07s/it]
{'loss': 0.225, 'learning_rate': 1.972881490654638e-06, 'rewards/chosen': -5.119534492492676, 'rewards/rejected': -8.179178237915039, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0596444606781006, 'policy_logps/rejected': -534.0654907226562, 'policy_logps/chosen': -552.7861328125, 'referece_logps/rejected': -452.2737121582031, 'referece_logps/chosen': -501.59075927734375, 'logits/rejected': 0.7196429967880249, 'logits/chosen': 0.6791726350784302, 'epoch': 6.51}


 72%|███████▏  | 11647/16110 [8:15:42<24:50:03, 20.03s/it]

 72%|███████▏  | 11648/16110 [8:16:02<24:50:35, 20.04s/it]
{'loss': 0.2337, 'learning_rate': 1.972741803260933e-06, 'rewards/chosen': -4.137576580047607, 'rewards/rejected': -6.782812595367432, 'rewards/accuracies': 0.875, 'rewards/margins': 2.645235300064087, 'policy_logps/rejected': -417.252197265625, 'policy_logps/chosen': -443.9759826660156, 'referece_logps/rejected': -349.4240417480469, 'referece_logps/chosen': -402.6002502441406, 'logits/rejected': -0.4532383978366852, 'logits/chosen': -0.5497367978096008, 'epoch': 6.51}


 72%|███████▏  | 11650/16110 [8:16:34<22:18:16, 18.00s/it]

 72%|███████▏  | 11651/16110 [8:16:51<21:42:57, 17.53s/it]

 72%|███████▏  | 11652/16110 [8:17:11<22:35:29, 18.24s/it]
{'loss': 0.1367, 'learning_rate': 1.9725550029464645e-06, 'rewards/chosen': -4.619001865386963, 'rewards/rejected': -9.107638359069824, 'rewards/accuracies': 1.0, 'rewards/margins': 4.488635540008545, 'policy_logps/rejected': -432.2275695800781, 'policy_logps/chosen': -512.978271484375, 'referece_logps/rejected': -341.1512451171875, 'referece_logps/chosen': -466.7882995605469, 'logits/rejected': 0.28996050357818604, 'logits/chosen': 0.09676221013069153, 'epoch': 6.51}

 72%|███████▏  | 11653/16110 [8:17:25<21:09:37, 17.09s/it]

 72%|███████▏  | 11654/16110 [8:17:39<20:04:43, 16.22s/it]

 72%|███████▏  | 11655/16110 [8:18:00<21:48:18, 17.62s/it]


 72%|███████▏  | 11657/16110 [8:18:29<19:50:46, 16.04s/it]

 72%|███████▏  | 11658/16110 [8:18:47<20:24:20, 16.50s/it]
{'loss': 0.1224, 'learning_rate': 1.972273623171322e-06, 'rewards/chosen': -4.809898853302002, 'rewards/rejected': -9.707563400268555, 'rewards/accuracies': 1.0, 'rewards/margins': 4.897663593292236, 'policy_logps/rejected': -699.4562377929688, 'policy_logps/chosen': -395.6607666015625, 'referece_logps/rejected': -602.380615234375, 'referece_logps/chosen': -347.5617370605469, 'logits/rejected': 0.09578079730272293, 'logits/chosen': 0.3264424800872803, 'epoch': 6.51}

 72%|███████▏  | 11659/16110 [8:19:03<20:25:59, 16.53s/it]


 72%|███████▏  | 11661/16110 [8:19:37<20:40:06, 16.72s/it]
{'loss': 0.2315, 'learning_rate': 1.972132402708072e-06, 'rewards/chosen': -3.958810567855835, 'rewards/rejected': -6.639206886291504, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6803958415985107, 'policy_logps/rejected': -331.59515380859375, 'policy_logps/chosen': -303.7376403808594, 'referece_logps/rejected': -265.203125, 'referece_logps/chosen': -264.1495361328125, 'logits/rejected': 0.16472797095775604, 'logits/chosen': 0.13753870129585266, 'epoch': 6.51}

 72%|███████▏  | 11662/16110 [8:19:56<21:36:38, 17.49s/it]

 72%|███████▏  | 11663/16110 [8:20:16<22:22:07, 18.11s/it]

 72%|███████▏  | 11664/16110 [8:20:36<23:13:14, 18.80s/it]

 72%|███████▏  | 11665/16110 [8:20:54<22:38:19, 18.34s/it]


 72%|███████▏  | 11667/16110 [8:21:25<21:26:24, 17.37s/it]
{'loss': 0.0918, 'learning_rate': 1.971848900887085e-06, 'rewards/chosen': -3.9171175956726074, 'rewards/rejected': -8.153823852539062, 'rewards/accuracies': 0.875, 'rewards/margins': 4.236705780029297, 'policy_logps/rejected': -276.357666015625, 'policy_logps/chosen': -328.49945068359375, 'referece_logps/rejected': -194.8194122314453, 'referece_logps/chosen': -289.3282775878906, 'logits/rejected': 0.09295424818992615, 'logits/chosen': -0.06705647706985474, 'epoch': 6.52}

 72%|███████▏  | 11668/16110 [8:21:42<21:22:47, 17.33s/it]

 72%|███████▏  | 11669/16110 [8:22:02<22:17:52, 18.08s/it]

 72%|███████▏  | 11670/16110 [8:22:24<23:41:11, 19.21s/it]


 72%|███████▏  | 11672/16110 [8:22:53<21:16:38, 17.26s/it]

 72%|███████▏  | 11673/16110 [8:23:13<22:09:05, 17.97s/it]
{'loss': 0.2321, 'learning_rate': 1.9715639848841107e-06, 'rewards/chosen': -5.184398651123047, 'rewards/rejected': -8.230844497680664, 'rewards/accuracies': 0.875, 'rewards/margins': 3.046445608139038, 'policy_logps/rejected': -369.41583251953125, 'policy_logps/chosen': -365.6047058105469, 'referece_logps/rejected': -287.1073913574219, 'referece_logps/chosen': -313.7607116699219, 'logits/rejected': 0.595798671245575, 'logits/chosen': 0.5734066367149353, 'epoch': 6.52}

 72%|███████▏  | 11674/16110 [8:23:32<22:23:04, 18.17s/it]

 72%|███████▏  | 11675/16110 [8:23:52<23:09:23, 18.80s/it]

 72%|███████▏  | 11676/16110 [8:24:12<23:39:52, 19.21s/it]

 72%|███████▏  | 11677/16110 [8:24:26<21:32:51, 17.50s/it]


 72%|███████▏  | 11679/16110 [8:24:57<21:02:34, 17.10s/it]

 73%|███████▎  | 11680/16110 [8:25:15<21:21:39, 17.36s/it]
{'loss': 0.2153, 'learning_rate': 1.971229796064966e-06, 'rewards/chosen': -5.663740634918213, 'rewards/rejected': -8.154181480407715, 'rewards/accuracies': 0.75, 'rewards/margins': 2.490441083908081, 'policy_logps/rejected': -268.39935302734375, 'policy_logps/chosen': -350.4485168457031, 'referece_logps/rejected': -186.85752868652344, 'referece_logps/chosen': -293.81109619140625, 'logits/rejected': -0.555182158946991, 'logits/chosen': -0.6227542757987976, 'epoch': 6.53}


 73%|███████▎  | 11682/16110 [8:25:50<21:24:04, 17.40s/it]
{'loss': 0.2901, 'learning_rate': 1.97113396019591e-06, 'rewards/chosen': -4.01284646987915, 'rewards/rejected': -7.776042938232422, 'rewards/accuracies': 0.75, 'rewards/margins': 3.7631967067718506, 'policy_logps/rejected': -314.2676086425781, 'policy_logps/chosen': -398.57452392578125, 'referece_logps/rejected': -236.50718688964844, 'referece_logps/chosen': -358.44610595703125, 'logits/rejected': -0.8657822608947754, 'logits/chosen': -0.860933244228363, 'epoch': 6.53}

 73%|███████▎  | 11683/16110 [8:26:03<19:48:03, 16.10s/it]

 73%|███████▎  | 11684/16110 [8:26:18<19:32:41, 15.90s/it]


 73%|███████▎  | 11686/16110 [8:26:51<19:50:37, 16.15s/it]
{'loss': 0.1744, 'learning_rate': 1.970941817426052e-06, 'rewards/chosen': -4.391915798187256, 'rewards/rejected': -9.39145565032959, 'rewards/accuracies': 1.0, 'rewards/margins': 4.999539375305176, 'policy_logps/rejected': -417.29364013671875, 'policy_logps/chosen': -412.6734619140625, 'referece_logps/rejected': -323.3790588378906, 'referece_logps/chosen': -368.7542724609375, 'logits/rejected': 0.03052731230854988, 'logits/chosen': -0.1852319836616516, 'epoch': 6.53}

 73%|███████▎  | 11687/16110 [8:27:11<21:00:38, 17.10s/it]

 73%|███████▎  | 11688/16110 [8:27:29<21:15:21, 17.30s/it]

 73%|███████▎  | 11689/16110 [8:27:47<21:34:38, 17.57s/it]

 73%|███████▎  | 11690/16110 [8:28:02<20:39:49, 16.83s/it]

 73%|███████▎  | 11691/16110 [8:28:16<19:47:43, 16.13s/it]

 73%|███████▎  | 11692/16110 [8:28:30<18:59:50, 15.48s/it]

 73%|███████▎  | 11693/16110 [8:28:51<20:43:41, 16.89s/it]


 73%|███████▎  | 11695/16110 [8:29:30<22:33:58, 18.40s/it]
{'loss': 0.1884, 'learning_rate': 1.9705072004828544e-06, 'rewards/chosen': -4.030508518218994, 'rewards/rejected': -9.790210723876953, 'rewards/accuracies': 0.875, 'rewards/margins': 5.759702205657959, 'policy_logps/rejected': -600.587646484375, 'policy_logps/chosen': -473.73663330078125, 'referece_logps/rejected': -502.68560791015625, 'referece_logps/chosen': -433.43157958984375, 'logits/rejected': 0.10475999116897583, 'logits/chosen': 0.038414642214775085, 'epoch': 6.53}

 73%|███████▎  | 11696/16110 [8:29:41<19:50:57, 16.19s/it]


 73%|███████▎  | 11698/16110 [8:30:21<22:18:33, 18.20s/it]
{'loss': 0.1327, 'learning_rate': 1.9703616219831836e-06, 'rewards/chosen': -3.2542808055877686, 'rewards/rejected': -7.6477556228637695, 'rewards/accuracies': 0.875, 'rewards/margins': 4.393474578857422, 'policy_logps/rejected': -479.6280517578125, 'policy_logps/chosen': -468.66656494140625, 'referece_logps/rejected': -403.1505432128906, 'referece_logps/chosen': -436.123779296875, 'logits/rejected': -0.1735685169696808, 'logits/chosen': -0.42446666955947876, 'epoch': 6.54}


 73%|███████▎  | 11700/16110 [8:31:00<23:03:02, 18.82s/it]
{'loss': 0.1801, 'learning_rate': 1.970264373533197e-06, 'rewards/chosen': -4.243522644042969, 'rewards/rejected': -8.08290958404541, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8393876552581787, 'policy_logps/rejected': -260.5054626464844, 'policy_logps/chosen': -367.2970275878906, 'referece_logps/rejected': -179.67636108398438, 'referece_logps/chosen': -324.86181640625, 'logits/rejected': 0.6903900504112244, 'logits/chosen': 0.6547688245773315, 'epoch': 6.54}

 73%|███████▎  | 11701/16110 [8:31:16<22:15:20, 18.17s/it]

 73%|███████▎  | 11702/16110 [8:31:33<21:41:57, 17.72s/it]

 73%|███████▎  | 11703/16110 [8:31:54<23:07:49, 18.89s/it]

 73%|███████▎  | 11704/16110 [8:32:10<22:01:37, 18.00s/it]

 73%|███████▎  | 11705/16110 [8:32:31<22:50:44, 18.67s/it]

 73%|███████▎  | 11706/16110 [8:32:47<22:04:50, 18.05s/it]

 73%|███████▎  | 11707/16110 [8:33:07<22:33:33, 18.45s/it]

 73%|███████▎  | 11708/16110 [8:33:27<23:13:25, 18.99s/it]

 73%|███████▎  | 11709/16110 [8:33:47<23:32:57, 19.26s/it]

 73%|███████▎  | 11710/16110 [8:34:01<21:48:30, 17.84s/it]

 73%|███████▎  | 11711/16110 [8:34:19<21:44:26, 17.79s/it]

 73%|███████▎  | 11712/16110 [8:34:32<20:06:08, 16.45s/it]

 73%|███████▎  | 11713/16110 [8:34:54<21:50:48, 17.89s/it]

 73%|███████▎  | 11714/16110 [8:35:11<21:37:11, 17.71s/it]


 73%|███████▎  | 11716/16110 [8:35:44<21:04:09, 17.26s/it]
{'loss': 0.1477, 'learning_rate': 1.9694807397537107e-06, 'rewards/chosen': -3.623469829559326, 'rewards/rejected': -9.036392211914062, 'rewards/accuracies': 1.0, 'rewards/margins': 5.4129228591918945, 'policy_logps/rejected': -284.65191650390625, 'policy_logps/chosen': -233.09495544433594, 'referece_logps/rejected': -194.2880401611328, 'referece_logps/chosen': -196.86026000976562, 'logits/rejected': 0.3891436457633972, 'logits/chosen': 0.47788017988204956, 'epoch': 6.55}

 73%|███████▎  | 11717/16110 [8:36:05<22:28:20, 18.42s/it]

 73%|███████▎  | 11718/16110 [8:36:23<22:20:20, 18.31s/it]


 73%|███████▎  | 11720/16110 [8:37:00<22:21:35, 18.34s/it]
{'loss': 0.159, 'learning_rate': 1.96928326350656e-06, 'rewards/chosen': -4.3457136154174805, 'rewards/rejected': -9.883100509643555, 'rewards/accuracies': 1.0, 'rewards/margins': 5.537387847900391, 'policy_logps/rejected': -388.179443359375, 'policy_logps/chosen': -522.4542236328125, 'referece_logps/rejected': -289.34844970703125, 'referece_logps/chosen': -478.9970703125, 'logits/rejected': -0.2592085003852844, 'logits/chosen': -0.6759120225906372, 'epoch': 6.55}


 73%|███████▎  | 11722/16110 [8:37:36<22:13:10, 18.23s/it]
{'loss': 0.2108, 'learning_rate': 1.9691842903002026e-06, 'rewards/chosen': -3.447807788848877, 'rewards/rejected': -7.244052886962891, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7962450981140137, 'policy_logps/rejected': -358.56707763671875, 'policy_logps/chosen': -367.022216796875, 'referece_logps/rejected': -286.1265563964844, 'referece_logps/chosen': -332.54412841796875, 'logits/rejected': -0.14112091064453125, 'logits/chosen': -0.16196072101593018, 'epoch': 6.55}


 73%|███████▎  | 11724/16110 [8:38:18<23:52:17, 19.59s/it]

 73%|███████▎  | 11725/16110 [8:38:36<23:27:03, 19.25s/it]

 73%|███████▎  | 11726/16110 [8:38:56<23:36:53, 19.39s/it]

 73%|███████▎  | 11727/16110 [8:39:16<23:49:17, 19.57s/it]
{'loss': 0.1096, 'learning_rate': 1.968936171754571e-06, 'rewards/chosen': -5.228436470031738, 'rewards/rejected': -10.265570640563965, 'rewards/accuracies': 1.0, 'rewards/margins': 5.037134170532227, 'policy_logps/rejected': -411.2786865234375, 'policy_logps/chosen': -446.4278564453125, 'referece_logps/rejected': -308.62298583984375, 'referece_logps/chosen': -394.1435546875, 'logits/rejected': 0.2869924008846283, 'logits/chosen': 0.1353325992822647, 'epoch': 6.55}

 73%|███████▎  | 11728/16110 [8:39:37<24:14:23, 19.91s/it]


 73%|███████▎  | 11730/16110 [8:40:10<22:36:40, 18.58s/it]
{'loss': 0.057, 'learning_rate': 1.968786830629906e-06, 'rewards/chosen': -4.622745037078857, 'rewards/rejected': -10.795562744140625, 'rewards/accuracies': 1.0, 'rewards/margins': 6.172817230224609, 'policy_logps/rejected': -462.6525573730469, 'policy_logps/chosen': -310.03643798828125, 'referece_logps/rejected': -354.6968994140625, 'referece_logps/chosen': -263.80902099609375, 'logits/rejected': -0.43983566761016846, 'logits/chosen': -0.2717640995979309, 'epoch': 6.55}


 73%|███████▎  | 11732/16110 [8:40:42<21:12:53, 17.44s/it]

 73%|███████▎  | 11733/16110 [8:41:02<22:07:24, 18.20s/it]

 73%|███████▎  | 11734/16110 [8:41:22<22:49:46, 18.78s/it]
{'loss': 0.1878, 'learning_rate': 1.9685871609128454e-06, 'rewards/chosen': -4.94694709777832, 'rewards/rejected': -10.310009956359863, 'rewards/accuracies': 0.875, 'rewards/margins': 5.363062858581543, 'policy_logps/rejected': -469.38494873046875, 'policy_logps/chosen': -295.318359375, 'referece_logps/rejected': -366.2848815917969, 'referece_logps/chosen': -245.84890747070312, 'logits/rejected': -0.8717137575149536, 'logits/chosen': -0.3139864206314087, 'epoch': 6.56}


 73%|███████▎  | 11736/16110 [8:41:58<22:24:19, 18.44s/it]
{'loss': 0.0693, 'learning_rate': 1.968487091140266e-06, 'rewards/chosen': -4.149815082550049, 'rewards/rejected': -8.977214813232422, 'rewards/accuracies': 1.0, 'rewards/margins': 4.827399253845215, 'policy_logps/rejected': -268.0628356933594, 'policy_logps/chosen': -431.435546875, 'referece_logps/rejected': -178.29067993164062, 'referece_logps/chosen': -389.9374084472656, 'logits/rejected': -0.1405123472213745, 'logits/chosen': -0.4486011266708374, 'epoch': 6.56}

 73%|███████▎  | 11737/16110 [8:42:19<23:21:49, 19.23s/it]

 73%|███████▎  | 11738/16110 [8:42:41<24:18:13, 20.01s/it]

 73%|███████▎  | 11739/16110 [8:43:04<25:06:25, 20.68s/it]

 73%|███████▎  | 11740/16110 [8:43:18<22:49:03, 18.80s/it]

 73%|███████▎  | 11741/16110 [8:43:38<23:25:50, 19.31s/it]

 73%|███████▎  | 11742/16110 [8:43:54<21:59:10, 18.12s/it]

 73%|███████▎  | 11743/16110 [8:44:12<22:11:44, 18.30s/it]

 73%|███████▎  | 11744/16110 [8:44:29<21:40:48, 17.88s/it]

 73%|███████▎  | 11745/16110 [8:44:50<22:49:55, 18.83s/it]

 73%|███████▎  | 11746/16110 [8:45:11<23:34:26, 19.45s/it]


 73%|███████▎  | 11748/16110 [8:45:43<20:52:25, 17.23s/it]
{'loss': 0.1042, 'learning_rate': 1.9678833847290083e-06, 'rewards/chosen': -4.241663932800293, 'rewards/rejected': -9.44162368774414, 'rewards/accuracies': 0.875, 'rewards/margins': 5.199959754943848, 'policy_logps/rejected': -311.1084289550781, 'policy_logps/chosen': -453.8743896484375, 'referece_logps/rejected': -216.6921844482422, 'referece_logps/chosen': -411.457763671875, 'logits/rejected': -0.17442718148231506, 'logits/chosen': -0.5291653871536255, 'epoch': 6.56}

 73%|███████▎  | 11749/16110 [8:46:02<21:26:00, 17.69s/it]

 73%|███████▎  | 11750/16110 [8:46:14<19:39:19, 16.23s/it]

 73%|███████▎  | 11751/16110 [8:46:36<21:25:07, 17.69s/it]

 73%|███████▎  | 11752/16110 [8:46:57<22:39:28, 18.72s/it]

 73%|███████▎  | 11753/16110 [8:47:17<23:09:00, 19.13s/it]

 73%|███████▎  | 11754/16110 [8:47:36<23:02:16, 19.04s/it]

 73%|███████▎  | 11755/16110 [8:47:50<21:21:11, 17.65s/it]

 73%|███████▎  | 11756/16110 [8:48:10<22:04:46, 18.26s/it]

 73%|███████▎  | 11757/16110 [8:48:25<21:11:32, 17.53s/it]

 73%|███████▎  | 11758/16110 [8:48:45<21:44:28, 17.98s/it]

 73%|███████▎  | 11759/16110 [8:49:03<21:50:27, 18.07s/it]

 73%|███████▎  | 11760/16110 [8:49:23<22:30:12, 18.62s/it]

 73%|███████▎  | 11761/16110 [8:49:42<22:38:30, 18.74s/it]

 73%|███████▎  | 11762/16110 [8:49:54<20:15:13, 16.77s/it]

 73%|███████▎  | 11763/16110 [8:50:14<21:36:57, 17.90s/it]


 73%|███████▎  | 11765/16110 [8:50:51<21:43:28, 18.00s/it]
{'loss': 0.2665, 'learning_rate': 1.9670184911182443e-06, 'rewards/chosen': -4.430026054382324, 'rewards/rejected': -8.375082969665527, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9450571537017822, 'policy_logps/rejected': -449.4028015136719, 'policy_logps/chosen': -478.967041015625, 'referece_logps/rejected': -365.6519470214844, 'referece_logps/chosen': -434.66680908203125, 'logits/rejected': -0.14578750729560852, 'logits/chosen': -0.22291791439056396, 'epoch': 6.57}

 73%|███████▎  | 11766/16110 [8:51:11<22:11:58, 18.40s/it]

 73%|███████▎  | 11767/16110 [8:51:24<20:18:13, 16.83s/it]

 73%|███████▎  | 11768/16110 [8:51:40<20:07:29, 16.69s/it]

 73%|███████▎  | 11769/16110 [8:52:02<22:02:12, 18.28s/it]

 73%|███████▎  | 11770/16110 [8:52:17<20:56:53, 17.38s/it]

 73%|███████▎  | 11771/16110 [8:52:39<22:33:15, 18.71s/it]

 73%|███████▎  | 11772/16110 [8:52:58<22:36:03, 18.76s/it]


 73%|███████▎  | 11774/16110 [8:53:33<21:35:25, 17.93s/it]
{'loss': 0.2481, 'learning_rate': 1.966556032375103e-06, 'rewards/chosen': -3.8040382862091064, 'rewards/rejected': -7.385060787200928, 'rewards/accuracies': 1.0, 'rewards/margins': 3.581022024154663, 'policy_logps/rejected': -309.09356689453125, 'policy_logps/chosen': -303.2350158691406, 'referece_logps/rejected': -235.2429656982422, 'referece_logps/chosen': -265.19464111328125, 'logits/rejected': 1.117127537727356, 'logits/chosen': 1.051905870437622, 'epoch': 6.58}

 73%|███████▎  | 11775/16110 [8:53:47<19:54:25, 16.53s/it]

 73%|███████▎  | 11776/16110 [8:54:04<20:09:28, 16.74s/it]

 73%|███████▎  | 11777/16110 [8:54:23<20:59:22, 17.44s/it]

 73%|███████▎  | 11778/16110 [8:54:37<19:41:55, 16.37s/it]

 73%|███████▎  | 11779/16110 [8:54:59<21:38:31, 17.99s/it]

 73%|███████▎  | 11780/16110 [8:55:21<23:14:06, 19.32s/it]

 73%|███████▎  | 11781/16110 [8:55:43<24:02:07, 19.99s/it]

 73%|███████▎  | 11782/16110 [8:56:04<24:25:10, 20.31s/it]

 73%|███████▎  | 11783/16110 [8:56:19<22:41:10, 18.87s/it]

 73%|███████▎  | 11784/16110 [8:56:37<22:10:38, 18.46s/it]

 73%|███████▎  | 11785/16110 [8:56:58<23:12:44, 19.32s/it]

 73%|███████▎  | 11786/16110 [8:57:19<23:43:07, 19.75s/it]

 73%|███████▎  | 11787/16110 [8:57:33<21:41:32, 18.06s/it]

 73%|███████▎  | 11788/16110 [8:57:45<19:40:38, 16.39s/it]

 73%|███████▎  | 11789/16110 [8:57:59<18:33:04, 15.46s/it]

 73%|███████▎  | 11790/16110 [8:58:20<20:46:36, 17.31s/it]

 73%|███████▎  | 11791/16110 [8:58:39<21:23:02, 17.82s/it]

 73%|███████▎  | 11792/16110 [8:58:55<20:32:40, 17.13s/it]

 73%|███████▎  | 11793/16110 [8:59:13<20:55:57, 17.46s/it]

 73%|███████▎  | 11794/16110 [8:59:28<19:58:10, 16.66s/it]

 73%|███████▎  | 11795/16110 [8:59:45<20:15:08, 16.90s/it]

 73%|███████▎  | 11796/16110 [9:00:05<21:17:31, 17.77s/it]

 73%|███████▎  | 11797/16110 [9:00:20<20:19:58, 16.97s/it]

 73%|███████▎  | 11798/16110 [9:00:38<20:42:43, 17.29s/it]

 73%|███████▎  | 11799/16110 [9:00:59<22:04:31, 18.43s/it]


 73%|███████▎  | 11801/16110 [9:01:38<22:17:27, 18.62s/it]
{'loss': 0.1907, 'learning_rate': 1.965149674775627e-06, 'rewards/chosen': -3.860285997390747, 'rewards/rejected': -9.008437156677246, 'rewards/accuracies': 1.0, 'rewards/margins': 5.148151397705078, 'policy_logps/rejected': -333.45355224609375, 'policy_logps/chosen': -264.76092529296875, 'referece_logps/rejected': -243.36920166015625, 'referece_logps/chosen': -226.1580810546875, 'logits/rejected': -0.5876337289810181, 'logits/chosen': -0.6328544616699219, 'epoch': 6.59}

 73%|███████▎  | 11802/16110 [9:01:59<23:11:58, 19.39s/it]

 73%|███████▎  | 11803/16110 [9:02:16<22:09:05, 18.52s/it]

 73%|███████▎  | 11804/16110 [9:02:32<21:15:14, 17.77s/it]

 73%|███████▎  | 11805/16110 [9:02:50<21:19:01, 17.83s/it]

 73%|███████▎  | 11806/16110 [9:03:07<21:14:37, 17.77s/it]

 73%|███████▎  | 11807/16110 [9:03:27<22:02:10, 18.44s/it]

 73%|███████▎  | 11808/16110 [9:03:48<22:56:11, 19.19s/it]

 73%|███████▎  | 11809/16110 [9:03:59<19:58:05, 16.71s/it]

 73%|███████▎  | 11810/16110 [9:04:14<19:25:38, 16.26s/it]

 73%|███████▎  | 11811/16110 [9:04:32<19:49:45, 16.61s/it]

 73%|███████▎  | 11812/16110 [9:04:49<20:14:30, 16.95s/it]

 73%|███████▎  | 11813/16110 [9:05:02<18:39:19, 15.63s/it]

 73%|███████▎  | 11814/16110 [9:05:18<18:51:50, 15.81s/it]

 73%|███████▎  | 11815/16110 [9:05:31<17:55:56, 15.03s/it]

 73%|███████▎  | 11816/16110 [9:05:48<18:33:36, 15.56s/it]

 73%|███████▎  | 11817/16110 [9:06:05<18:59:55, 15.93s/it]

 73%|███████▎  | 11818/16110 [9:06:26<20:55:33, 17.55s/it]


 73%|███████▎  | 11820/16110 [9:06:55<18:52:12, 15.84s/it]

 73%|███████▎  | 11821/16110 [9:07:08<18:00:27, 15.11s/it]

 73%|███████▎  | 11822/16110 [9:07:26<19:07:57, 16.06s/it]

 73%|███████▎  | 11823/16110 [9:07:42<18:55:28, 15.89s/it]

 73%|███████▎  | 11824/16110 [9:08:02<20:22:33, 17.11s/it]

 73%|███████▎  | 11825/16110 [9:08:20<20:51:49, 17.53s/it]

 73%|███████▎  | 11826/16110 [9:08:37<20:37:57, 17.34s/it]

 73%|███████▎  | 11827/16110 [9:08:55<20:43:53, 17.43s/it]

 73%|███████▎  | 11828/16110 [9:09:09<19:28:37, 16.37s/it]

 73%|███████▎  | 11829/16110 [9:09:29<20:54:10, 17.58s/it]

 73%|███████▎  | 11830/16110 [9:09:52<22:44:49, 19.13s/it]

 73%|███████▎  | 11831/16110 [9:10:10<22:24:44, 18.86s/it]

 73%|███████▎  | 11832/16110 [9:10:22<19:52:31, 16.73s/it]

 73%|███████▎  | 11833/16110 [9:10:42<21:13:55, 17.87s/it]

 73%|███████▎  | 11834/16110 [9:11:03<22:08:43, 18.64s/it]

 73%|███████▎  | 11835/16110 [9:11:15<19:43:13, 16.61s/it]

 73%|███████▎  | 11836/16110 [9:11:32<20:05:00, 16.92s/it]

 73%|███████▎  | 11837/16110 [9:11:53<21:22:11, 18.00s/it]

 73%|███████▎  | 11838/16110 [9:12:16<23:04:51, 19.45s/it]

 73%|███████▎  | 11839/16110 [9:12:38<24:04:46, 20.30s/it]

 73%|███████▎  | 11840/16110 [9:12:58<23:52:03, 20.12s/it]

 74%|███████▎  | 11841/16110 [9:13:11<21:23:09, 18.03s/it]

 74%|███████▎  | 11842/16110 [9:13:28<21:04:28, 17.78s/it]

 74%|███████▎  | 11843/16110 [9:13:48<21:53:15, 18.47s/it]

 74%|███████▎  | 11844/16110 [9:14:02<20:21:35, 17.18s/it]

 74%|███████▎  | 11845/16110 [9:14:24<21:50:07, 18.43s/it]

 74%|███████▎  | 11846/16110 [9:14:37<20:11:49, 17.05s/it]

 74%|███████▎  | 11847/16110 [9:14:55<20:15:03, 17.10s/it]
{'loss': 0.211, 'learning_rate': 1.962688179860647e-06, 'rewards/chosen': -4.737789154052734, 'rewards/rejected': -8.085326194763184, 'rewards/accuracies': 1.0, 'rewards/margins': 3.347537040710449, 'policy_logps/rejected': -452.45562744140625, 'policy_logps/chosen': -592.0391235351562, 'referece_logps/rejected': -371.60235595703125, 'referece_logps/chosen': -544.6611938476562, 'logits/rejected': 0.5577589273452759, 'logits/chosen': 0.613646388053894, 'epoch': 6.62}


 74%|███████▎  | 11849/16110 [9:15:35<21:59:16, 18.58s/it]
{'loss': 0.1156, 'learning_rate': 1.9625792890045395e-06, 'rewards/chosen': -4.395216464996338, 'rewards/rejected': -10.29926586151123, 'rewards/accuracies': 1.0, 'rewards/margins': 5.904047966003418, 'policy_logps/rejected': -486.3966064453125, 'policy_logps/chosen': -457.45660400390625, 'referece_logps/rejected': -383.4039306640625, 'referece_logps/chosen': -413.50439453125, 'logits/rejected': -0.3499452769756317, 'logits/chosen': -0.6285202503204346, 'epoch': 6.62}


 74%|███████▎  | 11851/16110 [9:16:09<20:28:19, 17.30s/it]

 74%|███████▎  | 11852/16110 [9:16:22<18:54:36, 15.99s/it]
{'loss': 0.0835, 'learning_rate': 1.9624156609147406e-06, 'rewards/chosen': -3.7113707065582275, 'rewards/rejected': -9.831082344055176, 'rewards/accuracies': 1.0, 'rewards/margins': 6.119710922241211, 'policy_logps/rejected': -417.60931396484375, 'policy_logps/chosen': -450.1207580566406, 'referece_logps/rejected': -319.2984619140625, 'referece_logps/chosen': -413.00701904296875, 'logits/rejected': 0.4957277774810791, 'logits/chosen': 0.512185275554657, 'epoch': 6.62}

 74%|███████▎  | 11853/16110 [9:16:41<20:12:33, 17.09s/it]


 74%|███████▎  | 11855/16110 [9:17:08<17:39:40, 14.94s/it]

 74%|███████▎  | 11856/16110 [9:17:22<17:17:44, 14.64s/it]
{'loss': 0.1928, 'learning_rate': 1.9621969455171143e-06, 'rewards/chosen': -2.020266532897949, 'rewards/rejected': -9.885740280151367, 'rewards/accuracies': 1.0, 'rewards/margins': 7.865474700927734, 'policy_logps/rejected': -523.210693359375, 'policy_logps/chosen': -293.3245849609375, 'referece_logps/rejected': -424.353271484375, 'referece_logps/chosen': -273.12188720703125, 'logits/rejected': 0.29009976983070374, 'logits/chosen': 0.2576391398906708, 'epoch': 6.62}


 74%|███████▎  | 11858/16110 [9:17:47<15:49:37, 13.40s/it]
{'loss': 0.1698, 'learning_rate': 1.9620873544532654e-06, 'rewards/chosen': -5.1670002937316895, 'rewards/rejected': -7.568667411804199, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4016666412353516, 'policy_logps/rejected': -336.7730407714844, 'policy_logps/chosen': -442.9907531738281, 'referece_logps/rejected': -261.08636474609375, 'referece_logps/chosen': -391.3207702636719, 'logits/rejected': -0.17057989537715912, 'logits/chosen': -0.46707573533058167, 'epoch': 6.62}


 74%|███████▎  | 11860/16110 [9:18:15<16:24:21, 13.90s/it]
{'loss': 0.131, 'learning_rate': 1.9619776078363423e-06, 'rewards/chosen': -3.8558363914489746, 'rewards/rejected': -9.85294246673584, 'rewards/accuracies': 1.0, 'rewards/margins': 5.997105121612549, 'policy_logps/rejected': -325.6835632324219, 'policy_logps/chosen': -430.89678955078125, 'referece_logps/rejected': -227.15414428710938, 'referece_logps/chosen': -392.33843994140625, 'logits/rejected': -0.9530752897262573, 'logits/chosen': -1.6023377180099487, 'epoch': 6.63}


 74%|███████▎  | 11862/16110 [9:18:55<19:53:19, 16.85s/it]

 74%|███████▎  | 11863/16110 [9:19:08<18:29:14, 15.67s/it]
{'loss': 0.1948, 'learning_rate': 1.9618126962877666e-06, 'rewards/chosen': -2.0414865016937256, 'rewards/rejected': -7.2223801612854, 'rewards/accuracies': 1.0, 'rewards/margins': 5.180893898010254, 'policy_logps/rejected': -453.5913391113281, 'policy_logps/chosen': -339.01043701171875, 'referece_logps/rejected': -381.3675537109375, 'referece_logps/chosen': -318.5955810546875, 'logits/rejected': -0.11063271760940552, 'logits/chosen': -0.14549925923347473, 'epoch': 6.63}


 74%|███████▎  | 11865/16110 [9:19:44<19:53:13, 16.87s/it]

 74%|███████▎  | 11866/16110 [9:20:02<20:15:55, 17.19s/it]
{'loss': 0.2471, 'learning_rate': 1.961647434844698e-06, 'rewards/chosen': -3.749225616455078, 'rewards/rejected': -8.325883865356445, 'rewards/accuracies': 1.0, 'rewards/margins': 4.576657295227051, 'policy_logps/rejected': -378.3978271484375, 'policy_logps/chosen': -268.0064697265625, 'referece_logps/rejected': -295.1390380859375, 'referece_logps/chosen': -230.5142059326172, 'logits/rejected': -0.031152844429016113, 'logits/chosen': 0.0862964540719986, 'epoch': 6.63}

 74%|███████▎  | 11867/16110 [9:20:21<21:07:07, 17.92s/it]


 74%|███████▎  | 11869/16110 [9:20:57<21:16:04, 18.05s/it]

 74%|███████▎  | 11870/16110 [9:21:15<21:08:19, 17.95s/it]

 74%|███████▎  | 11871/16110 [9:21:28<19:28:38, 16.54s/it]

 74%|███████▎  | 11872/16110 [9:21:45<19:34:49, 16.63s/it]

 74%|███████▎  | 11873/16110 [9:22:03<20:05:17, 17.07s/it]

 74%|███████▎  | 11874/16110 [9:22:16<18:33:49, 15.78s/it]

 74%|███████▎  | 11875/16110 [9:22:37<20:25:23, 17.36s/it]

 74%|███████▎  | 11876/16110 [9:22:49<18:41:13, 15.89s/it]

 74%|███████▎  | 11877/16110 [9:23:09<20:07:13, 17.11s/it]

 74%|███████▎  | 11878/16110 [9:23:25<19:43:23, 16.78s/it]
{'loss': 0.1872, 'learning_rate': 1.9609828913317983e-06, 'rewards/chosen': -3.560474157333374, 'rewards/rejected': -10.094115257263184, 'rewards/accuracies': 1.0, 'rewards/margins': 6.5336408615112305, 'policy_logps/rejected': -419.47467041015625, 'policy_logps/chosen': -377.2619934082031, 'referece_logps/rejected': -318.53350830078125, 'referece_logps/chosen': -341.65728759765625, 'logits/rejected': -0.1664099097251892, 'logits/chosen': -0.08111128956079483, 'epoch': 6.64}


 74%|███████▎  | 11880/16110 [9:23:51<17:22:49, 14.79s/it]

 74%|███████▎  | 11881/16110 [9:24:10<19:05:37, 16.25s/it]

 74%|███████▍  | 11882/16110 [9:24:29<20:01:25, 17.05s/it]
{'loss': 0.1796, 'learning_rate': 1.9607601336401253e-06, 'rewards/chosen': -4.4273834228515625, 'rewards/rejected': -8.37569808959961, 'rewards/accuracies': 1.0, 'rewards/margins': 3.948315382003784, 'policy_logps/rejected': -471.8421936035156, 'policy_logps/chosen': -477.7975769042969, 'referece_logps/rejected': -388.0851745605469, 'referece_logps/chosen': -433.5238037109375, 'logits/rejected': -0.19188007712364197, 'logits/chosen': -0.20007175207138062, 'epoch': 6.64}


 74%|███████▍  | 11884/16110 [9:25:13<22:58:10, 19.57s/it]
{'loss': 0.2139, 'learning_rate': 1.960648521777551e-06, 'rewards/chosen': -3.4488682746887207, 'rewards/rejected': -6.798351287841797, 'rewards/accuracies': 1.0, 'rewards/margins': 3.349483013153076, 'policy_logps/rejected': -388.7445373535156, 'policy_logps/chosen': -427.9010009765625, 'referece_logps/rejected': -320.7610168457031, 'referece_logps/chosen': -393.412353515625, 'logits/rejected': -0.7643676996231079, 'logits/chosen': -0.7295254468917847, 'epoch': 6.64}
[2024-04-06 00:33:25,371] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 74%|███████▍  | 11886/16110 [9:25:55<23:47:32, 20.28s/it]

 74%|███████▍  | 11887/16110 [9:26:11<22:10:39, 18.91s/it]

 74%|███████▍  | 11888/16110 [9:26:27<21:20:50, 18.20s/it]

 74%|███████▍  | 11889/16110 [9:26:50<23:04:19, 19.68s/it]

 74%|███████▍  | 11890/16110 [9:27:12<23:52:16, 20.36s/it]

 74%|███████▍  | 11891/16110 [9:27:32<23:37:09, 20.15s/it]

 74%|███████▍  | 11892/16110 [9:27:53<23:44:13, 20.26s/it]
{'loss': 0.2202, 'learning_rate': 1.9602005213036964e-06, 'rewards/chosen': -2.7863221168518066, 'rewards/rejected': -6.5825581550598145, 'rewards/accuracies': 1.0, 'rewards/margins': 3.796236515045166, 'policy_logps/rejected': -423.771240234375, 'policy_logps/chosen': -562.4473876953125, 'referece_logps/rejected': -357.9456787109375, 'referece_logps/chosen': -534.5841674804688, 'logits/rejected': -0.15510615706443787, 'logits/chosen': -0.29759520292282104, 'epoch': 6.64}

 74%|███████▍  | 11893/16110 [9:28:14<24:02:05, 20.52s/it]


 74%|███████▍  | 11895/16110 [9:28:51<22:55:26, 19.58s/it]
{'loss': 0.1248, 'learning_rate': 1.9600318806656564e-06, 'rewards/chosen': -3.978147506713867, 'rewards/rejected': -6.762537002563477, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7843899726867676, 'policy_logps/rejected': -500.1876220703125, 'policy_logps/chosen': -431.82000732421875, 'referece_logps/rejected': -432.5622863769531, 'referece_logps/chosen': -392.03857421875, 'logits/rejected': 0.11142781376838684, 'logits/chosen': 0.13911272585391998, 'epoch': 6.65}


 74%|███████▍  | 11897/16110 [9:29:32<23:33:41, 20.13s/it]
{'loss': 0.2055, 'learning_rate': 1.9599192595439177e-06, 'rewards/chosen': -5.317649841308594, 'rewards/rejected': -10.265312194824219, 'rewards/accuracies': 1.0, 'rewards/margins': 4.947662353515625, 'policy_logps/rejected': -449.30426025390625, 'policy_logps/chosen': -414.79473876953125, 'referece_logps/rejected': -346.651123046875, 'referece_logps/chosen': -361.6182556152344, 'logits/rejected': -0.026228658854961395, 'logits/chosen': -0.06869307160377502, 'epoch': 6.65}

 74%|███████▍  | 11898/16110 [9:29:50<23:00:34, 19.67s/it]

 74%|███████▍  | 11899/16110 [9:30:08<22:24:25, 19.16s/it]


 74%|███████▍  | 11901/16110 [9:30:48<22:37:57, 19.36s/it]

 74%|███████▍  | 11902/16110 [9:31:05<21:32:31, 18.43s/it]
{'loss': 0.1107, 'learning_rate': 1.9596370277683953e-06, 'rewards/chosen': -3.5283923149108887, 'rewards/rejected': -7.193614482879639, 'rewards/accuracies': 1.0, 'rewards/margins': 3.66522216796875, 'policy_logps/rejected': -528.942626953125, 'policy_logps/chosen': -589.8935546875, 'referece_logps/rejected': -457.0064697265625, 'referece_logps/chosen': -554.609619140625, 'logits/rejected': 0.8772845268249512, 'logits/chosen': 0.6789675951004028, 'epoch': 6.65}


 74%|███████▍  | 11904/16110 [9:31:33<18:44:36, 16.04s/it]

 74%|███████▍  | 11905/16110 [9:31:54<20:28:46, 17.53s/it]
{'loss': 0.115, 'learning_rate': 1.9594672232141276e-06, 'rewards/chosen': -3.642953872680664, 'rewards/rejected': -9.017960548400879, 'rewards/accuracies': 1.0, 'rewards/margins': 5.375007152557373, 'policy_logps/rejected': -468.49713134765625, 'policy_logps/chosen': -406.10137939453125, 'referece_logps/rejected': -378.3175354003906, 'referece_logps/chosen': -369.6718444824219, 'logits/rejected': -0.19874271750450134, 'logits/chosen': -0.16746850311756134, 'epoch': 6.65}

 74%|███████▍  | 11906/16110 [9:32:18<22:49:12, 19.54s/it]

 74%|███████▍  | 11907/16110 [9:32:38<23:03:41, 19.75s/it]


 74%|███████▍  | 11909/16110 [9:33:10<21:06:34, 18.09s/it]

 74%|███████▍  | 11910/16110 [9:33:28<21:03:05, 18.04s/it]

 74%|███████▍  | 11911/16110 [9:33:48<21:39:57, 18.58s/it]
{'loss': 0.2002, 'learning_rate': 1.9591265670437674e-06, 'rewards/chosen': -5.377942085266113, 'rewards/rejected': -7.565489292144775, 'rewards/accuracies': 0.75, 'rewards/margins': 2.187547445297241, 'policy_logps/rejected': -257.6380920410156, 'policy_logps/chosen': -408.2866516113281, 'referece_logps/rejected': -181.9832000732422, 'referece_logps/chosen': -354.5072326660156, 'logits/rejected': -0.31923502683639526, 'logits/chosen': -0.32813113927841187, 'epoch': 6.65}

 74%|███████▍  | 11912/16110 [9:34:04<20:59:28, 18.00s/it]


 74%|███████▍  | 11914/16110 [9:34:39<20:45:11, 17.81s/it]

 74%|███████▍  | 11915/16110 [9:35:00<21:50:05, 18.74s/it]

 74%|███████▍  | 11916/16110 [9:35:13<20:01:18, 17.19s/it]

 74%|███████▍  | 11917/16110 [9:35:28<19:01:10, 16.33s/it]

 74%|███████▍  | 11918/16110 [9:35:40<17:35:46, 15.11s/it]

 74%|███████▍  | 11919/16110 [9:36:00<19:07:48, 16.43s/it]

 74%|███████▍  | 11920/16110 [9:36:16<19:05:39, 16.41s/it]

 74%|███████▍  | 11921/16110 [9:36:27<17:21:00, 14.91s/it]
{'loss': 0.1174, 'learning_rate': 1.9585557055185776e-06, 'rewards/chosen': -4.263797283172607, 'rewards/rejected': -8.770995140075684, 'rewards/accuracies': 1.0, 'rewards/margins': 4.507197856903076, 'policy_logps/rejected': -470.67529296875, 'policy_logps/chosen': -377.3865966796875, 'referece_logps/rejected': -382.9653625488281, 'referece_logps/chosen': -334.74859619140625, 'logits/rejected': 0.5618141293525696, 'logits/chosen': 0.5661271810531616, 'epoch': 6.66}


 74%|███████▍  | 11923/16110 [9:37:06<19:32:52, 16.81s/it]

 74%|███████▍  | 11924/16110 [9:37:23<19:31:09, 16.79s/it]
{'loss': 0.1414, 'learning_rate': 1.9583836914182915e-06, 'rewards/chosen': -4.4735636711120605, 'rewards/rejected': -7.000927925109863, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5273635387420654, 'policy_logps/rejected': -288.8856506347656, 'policy_logps/chosen': -375.08355712890625, 'referece_logps/rejected': -218.87637329101562, 'referece_logps/chosen': -330.3479309082031, 'logits/rejected': 0.0029101669788360596, 'logits/chosen': -0.15605154633522034, 'epoch': 6.66}


 74%|███████▍  | 11926/16110 [9:37:52<18:07:57, 15.60s/it]

 74%|███████▍  | 11927/16110 [9:38:11<19:19:40, 16.63s/it]
{'loss': 0.1293, 'learning_rate': 1.958211328670938e-06, 'rewards/chosen': -3.856839656829834, 'rewards/rejected': -7.586860179901123, 'rewards/accuracies': 0.875, 'rewards/margins': 3.73002028465271, 'policy_logps/rejected': -361.18548583984375, 'policy_logps/chosen': -359.1190185546875, 'referece_logps/rejected': -285.3169250488281, 'referece_logps/chosen': -320.5506286621094, 'logits/rejected': 0.04699166119098663, 'logits/chosen': 0.2853550612926483, 'epoch': 6.66}

 74%|███████▍  | 11928/16110 [9:38:34<21:41:26, 18.67s/it]

 74%|███████▍  | 11929/16110 [9:38:54<22:08:49, 19.07s/it]


 74%|███████▍  | 11931/16110 [9:39:37<23:20:33, 20.11s/it]

 74%|███████▍  | 11932/16110 [9:39:56<22:49:01, 19.66s/it]

 74%|███████▍  | 11933/16110 [9:40:16<22:58:39, 19.80s/it]

 74%|███████▍  | 11934/16110 [9:40:37<23:32:42, 20.30s/it]
{'loss': 0.1994, 'learning_rate': 1.9578077934298657e-06, 'rewards/chosen': -3.2713077068328857, 'rewards/rejected': -7.5814409255981445, 'rewards/accuracies': 1.0, 'rewards/margins': 4.31013298034668, 'policy_logps/rejected': -438.0315856933594, 'policy_logps/chosen': -402.7493896484375, 'referece_logps/rejected': -362.2171936035156, 'referece_logps/chosen': -370.0362854003906, 'logits/rejected': 0.519134521484375, 'logits/chosen': 0.3156084418296814, 'epoch': 6.67}


 74%|███████▍  | 11936/16110 [9:41:12<21:44:34, 18.75s/it]

 74%|███████▍  | 11937/16110 [9:41:28<20:52:24, 18.01s/it]

 74%|███████▍  | 11938/16110 [9:41:41<19:14:49, 16.61s/it]

 74%|███████▍  | 11939/16110 [9:42:01<20:18:43, 17.53s/it]
{'loss': 0.1586, 'learning_rate': 1.9575183924668165e-06, 'rewards/chosen': -3.232264995574951, 'rewards/rejected': -8.415060997009277, 'rewards/accuracies': 1.0, 'rewards/margins': 5.182796478271484, 'policy_logps/rejected': -460.99835205078125, 'policy_logps/chosen': -485.87420654296875, 'referece_logps/rejected': -376.8477783203125, 'referece_logps/chosen': -453.5515441894531, 'logits/rejected': 0.2438662350177765, 'logits/chosen': 0.2952326536178589, 'epoch': 6.67}


 74%|███████▍  | 11941/16110 [9:42:38<20:52:51, 18.03s/it]

 74%|███████▍  | 11942/16110 [9:42:55<20:29:50, 17.70s/it]

 74%|███████▍  | 11943/16110 [9:43:13<20:38:58, 17.84s/it]
{'loss': 0.1377, 'learning_rate': 1.9572861750177778e-06, 'rewards/chosen': -3.4540493488311768, 'rewards/rejected': -8.234162330627441, 'rewards/accuracies': 0.875, 'rewards/margins': 4.7801127433776855, 'policy_logps/rejected': -517.4683837890625, 'policy_logps/chosen': -510.6151123046875, 'referece_logps/rejected': -435.12677001953125, 'referece_logps/chosen': -476.0745849609375, 'logits/rejected': 0.6925560235977173, 'logits/chosen': 0.5798052549362183, 'epoch': 6.67}


 74%|███████▍  | 11945/16110 [9:43:53<21:56:53, 18.97s/it]

 74%|███████▍  | 11946/16110 [9:44:08<20:19:50, 17.58s/it]

 74%|███████▍  | 11947/16110 [9:44:26<20:27:19, 17.69s/it]
{'loss': 0.1621, 'learning_rate': 1.9570533384615434e-06, 'rewards/chosen': -6.073515892028809, 'rewards/rejected': -10.686501502990723, 'rewards/accuracies': 0.875, 'rewards/margins': 4.612985610961914, 'policy_logps/rejected': -414.6900939941406, 'policy_logps/chosen': -463.8292236328125, 'referece_logps/rejected': -307.8250427246094, 'referece_logps/chosen': -403.0940856933594, 'logits/rejected': 0.3716652989387512, 'logits/chosen': 0.1071527972817421, 'epoch': 6.67}


 74%|███████▍  | 11949/16110 [9:45:08<22:41:32, 19.63s/it]

 74%|███████▍  | 11950/16110 [9:45:22<20:40:00, 17.88s/it]

 74%|███████▍  | 11951/16110 [9:45:42<21:27:18, 18.57s/it]

 74%|███████▍  | 11952/16110 [9:46:02<21:47:15, 18.86s/it]

 74%|███████▍  | 11953/16110 [9:46:15<20:01:01, 17.34s/it]
{'loss': 0.1273, 'learning_rate': 1.9567029231307094e-06, 'rewards/chosen': -4.449958801269531, 'rewards/rejected': -8.775700569152832, 'rewards/accuracies': 0.875, 'rewards/margins': 4.325741767883301, 'policy_logps/rejected': -393.0679931640625, 'policy_logps/chosen': -375.16168212890625, 'referece_logps/rejected': -305.31097412109375, 'referece_logps/chosen': -330.66204833984375, 'logits/rejected': 0.3655686676502228, 'logits/chosen': 0.2681446671485901, 'epoch': 6.68}

 74%|███████▍  | 11954/16110 [9:46:39<22:09:44, 19.20s/it]

 74%|███████▍  | 11955/16110 [9:46:51<19:42:57, 17.08s/it]

 74%|███████▍  | 11956/16110 [9:47:10<20:12:42, 17.52s/it]


 74%|███████▍  | 11958/16110 [9:47:49<21:21:14, 18.52s/it]
{'loss': 0.1737, 'learning_rate': 1.956409846891219e-06, 'rewards/chosen': -3.8022830486297607, 'rewards/rejected': -7.294342041015625, 'rewards/accuracies': 0.875, 'rewards/margins': 3.492058753967285, 'policy_logps/rejected': -534.686279296875, 'policy_logps/chosen': -363.0850830078125, 'referece_logps/rejected': -461.7428894042969, 'referece_logps/chosen': -325.0622253417969, 'logits/rejected': -0.1437833607196808, 'logits/chosen': 0.02282249927520752, 'epoch': 6.68}

 74%|███████▍  | 11959/16110 [9:48:01<19:13:10, 16.67s/it]

 74%|███████▍  | 11960/16110 [9:48:19<19:42:40, 17.10s/it]

 74%|███████▍  | 11961/16110 [9:48:33<18:31:34, 16.07s/it]


 74%|███████▍  | 11963/16110 [9:48:56<15:56:33, 13.84s/it]

 74%|███████▍  | 11964/16110 [9:49:19<18:56:31, 16.45s/it]

 74%|███████▍  | 11965/16110 [9:49:38<20:07:55, 17.49s/it]
{'loss': 0.1791, 'learning_rate': 1.9559979165538342e-06, 'rewards/chosen': -4.196922779083252, 'rewards/rejected': -6.871242046356201, 'rewards/accuracies': 1.0, 'rewards/margins': 2.674318790435791, 'policy_logps/rejected': -483.0782470703125, 'policy_logps/chosen': -588.6875, 'referece_logps/rejected': -414.36590576171875, 'referece_logps/chosen': -546.71826171875, 'logits/rejected': -0.1732081174850464, 'logits/chosen': -0.3387282192707062, 'epoch': 6.68}


 74%|███████▍  | 11967/16110 [9:50:17<20:55:53, 18.19s/it]
{'loss': 0.1617, 'learning_rate': 1.9558798743568434e-06, 'rewards/chosen': -3.179229259490967, 'rewards/rejected': -7.524682521820068, 'rewards/accuracies': 1.0, 'rewards/margins': 4.345453262329102, 'policy_logps/rejected': -348.80194091796875, 'policy_logps/chosen': -425.0622253417969, 'referece_logps/rejected': -273.5550842285156, 'referece_logps/chosen': -393.26995849609375, 'logits/rejected': 0.2957737147808075, 'logits/chosen': 0.1908719539642334, 'epoch': 6.69}


 74%|███████▍  | 11969/16110 [9:50:46<18:47:31, 16.34s/it]
{'loss': 0.166, 'learning_rate': 1.9557616776104217e-06, 'rewards/chosen': -3.6672797203063965, 'rewards/rejected': -6.100879192352295, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4335992336273193, 'policy_logps/rejected': -410.7662353515625, 'policy_logps/chosen': -372.6158447265625, 'referece_logps/rejected': -349.7574157714844, 'referece_logps/chosen': -335.9430236816406, 'logits/rejected': -0.7970032691955566, 'logits/chosen': -0.7705541849136353, 'epoch': 6.69}


 74%|███████▍  | 11971/16110 [9:51:26<20:50:32, 18.13s/it]

 74%|███████▍  | 11972/16110 [9:51:47<21:40:35, 18.86s/it]

 74%|███████▍  | 11973/16110 [9:52:03<20:37:04, 17.94s/it]

 74%|███████▍  | 11974/16110 [9:52:15<18:40:49, 16.26s/it]
{'loss': 0.1853, 'learning_rate': 1.9554655097160837e-06, 'rewards/chosen': -3.2033188343048096, 'rewards/rejected': -6.063825607299805, 'rewards/accuracies': 1.0, 'rewards/margins': 2.860506296157837, 'policy_logps/rejected': -469.619140625, 'policy_logps/chosen': -382.77764892578125, 'referece_logps/rejected': -408.9808654785156, 'referece_logps/chosen': -350.74444580078125, 'logits/rejected': -1.0714961290359497, 'logits/chosen': -0.9460846781730652, 'epoch': 6.69}


 74%|███████▍  | 11976/16110 [9:52:49<19:23:11, 16.88s/it]
{'loss': 0.1827, 'learning_rate': 1.955346772197314e-06, 'rewards/chosen': -3.274198055267334, 'rewards/rejected': -9.890127182006836, 'rewards/accuracies': 1.0, 'rewards/margins': 6.615927696228027, 'policy_logps/rejected': -404.391357421875, 'policy_logps/chosen': -276.5445251464844, 'referece_logps/rejected': -305.4900817871094, 'referece_logps/chosen': -243.8025360107422, 'logits/rejected': 0.2297634780406952, 'logits/chosen': 0.23446114361286163, 'epoch': 6.69}


 74%|███████▍  | 11978/16110 [9:53:29<21:15:56, 18.53s/it]
{'loss': 0.1098, 'learning_rate': 1.9552278802153076e-06, 'rewards/chosen': -3.055111885070801, 'rewards/rejected': -7.239756107330322, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1846442222595215, 'policy_logps/rejected': -476.6676940917969, 'policy_logps/chosen': -350.79730224609375, 'referece_logps/rejected': -404.2701416015625, 'referece_logps/chosen': -320.2461853027344, 'logits/rejected': 0.008851272985339165, 'logits/chosen': -0.09072217345237732, 'epoch': 6.69}


 74%|███████▍  | 11980/16110 [9:54:09<22:22:28, 19.50s/it]

 74%|███████▍  | 11981/16110 [9:54:26<21:34:52, 18.82s/it]
{'loss': 0.2731, 'learning_rate': 1.9550492526657855e-06, 'rewards/chosen': -6.3368821144104, 'rewards/rejected': -8.515010833740234, 'rewards/accuracies': 0.75, 'rewards/margins': 2.178128719329834, 'policy_logps/rejected': -500.7499694824219, 'policy_logps/chosen': -419.4720458984375, 'referece_logps/rejected': -415.599853515625, 'referece_logps/chosen': -356.10321044921875, 'logits/rejected': 0.36975520849227905, 'logits/chosen': 0.46712759137153625, 'epoch': 6.69}

 74%|███████▍  | 11982/16110 [9:54:39<19:33:23, 17.06s/it]


 74%|███████▍  | 11984/16110 [9:55:14<19:56:21, 17.40s/it]
{'loss': 0.1648, 'learning_rate': 1.95487027768222e-06, 'rewards/chosen': -4.044002532958984, 'rewards/rejected': -7.903980255126953, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8599774837493896, 'policy_logps/rejected': -382.094970703125, 'policy_logps/chosen': -412.0198974609375, 'referece_logps/rejected': -303.05517578125, 'referece_logps/chosen': -371.57989501953125, 'logits/rejected': 0.44745081663131714, 'logits/chosen': 0.3113493323326111, 'epoch': 6.69}

 74%|███████▍  | 11985/16110 [9:55:28<18:45:17, 16.37s/it]

 74%|███████▍  | 11986/16110 [9:55:40<17:09:51, 14.98s/it]

 74%|███████▍  | 11987/16110 [9:56:00<18:50:36, 16.45s/it]

 74%|███████▍  | 11988/16110 [9:56:24<21:21:55, 18.66s/it]


 74%|███████▍  | 11990/16110 [9:57:01<21:07:48, 18.46s/it]

 74%|███████▍  | 11991/16110 [9:57:21<21:34:35, 18.86s/it]

 74%|███████▍  | 11992/16110 [9:57:41<22:01:34, 19.26s/it]

 74%|███████▍  | 11993/16110 [9:58:01<22:14:04, 19.44s/it]
{'loss': 0.1572, 'learning_rate': 1.954331268778981e-06, 'rewards/chosen': -4.110256671905518, 'rewards/rejected': -9.750680923461914, 'rewards/accuracies': 1.0, 'rewards/margins': 5.640424728393555, 'policy_logps/rejected': -380.2803039550781, 'policy_logps/chosen': -467.56585693359375, 'referece_logps/rejected': -282.7734680175781, 'referece_logps/chosen': -426.4632873535156, 'logits/rejected': 0.08265960216522217, 'logits/chosen': 0.07001557946205139, 'epoch': 6.7}


 74%|███████▍  | 11995/16110 [9:58:37<21:31:46, 18.84s/it]
{'loss': 0.2531, 'learning_rate': 1.954211064638102e-06, 'rewards/chosen': -5.0191330909729, 'rewards/rejected': -6.994918346405029, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9757847785949707, 'policy_logps/rejected': -402.2314147949219, 'policy_logps/chosen': -331.173583984375, 'referece_logps/rejected': -332.28228759765625, 'referece_logps/chosen': -280.98223876953125, 'logits/rejected': 0.06116144731640816, 'logits/chosen': 0.06294562667608261, 'epoch': 6.7}


 74%|███████▍  | 11997/16110 [9:59:17<22:10:51, 19.41s/it]
{'loss': 0.2591, 'learning_rate': 1.95409070621761e-06, 'rewards/chosen': -4.262845516204834, 'rewards/rejected': -8.425247192382812, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1624016761779785, 'policy_logps/rejected': -427.8096923828125, 'policy_logps/chosen': -360.5992431640625, 'referece_logps/rejected': -343.55718994140625, 'referece_logps/chosen': -317.9707946777344, 'logits/rejected': -0.2785968780517578, 'logits/chosen': -0.23903705179691315, 'epoch': 6.7}


 74%|███████▍  | 11999/16110 [9:59:59<22:54:31, 20.06s/it]
{'loss': 0.2219, 'learning_rate': 1.953970193536966e-06, 'rewards/chosen': -3.727250337600708, 'rewards/rejected': -7.879292964935303, 'rewards/accuracies': 1.0, 'rewards/margins': 4.152042388916016, 'policy_logps/rejected': -571.073486328125, 'policy_logps/chosen': -558.7047119140625, 'referece_logps/rejected': -492.280517578125, 'referece_logps/chosen': -521.4321899414062, 'logits/rejected': 0.24391844868659973, 'logits/chosen': 0.20156970620155334, 'epoch': 6.7}


 74%|███████▍  | 12001/16110 [10:00:53<27:56:27, 24.48s/it]
{'loss': 0.1183, 'learning_rate': 1.953849526615654e-06, 'rewards/chosen': -3.611417770385742, 'rewards/rejected': -7.366471290588379, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7550535202026367, 'policy_logps/rejected': -425.4565734863281, 'policy_logps/chosen': -368.0224609375, 'referece_logps/rejected': -351.7918701171875, 'referece_logps/chosen': -331.90826416015625, 'logits/rejected': -0.44831517338752747, 'logits/chosen': -0.5020876526832581, 'epoch': 6.7}


 75%|███████▍  | 12003/16110 [10:01:25<23:19:21, 20.44s/it]
{'loss': 0.2206, 'learning_rate': 1.9537287054731842e-06, 'rewards/chosen': -5.798334121704102, 'rewards/rejected': -7.768559455871582, 'rewards/accuracies': 0.75, 'rewards/margins': 1.970225214958191, 'policy_logps/rejected': -422.9757080078125, 'policy_logps/chosen': -573.5217895507812, 'referece_logps/rejected': -345.29010009765625, 'referece_logps/chosen': -515.5384521484375, 'logits/rejected': 0.3698011338710785, 'logits/chosen': 0.043523505330085754, 'epoch': 6.71}


 75%|███████▍  | 12005/16110 [10:02:01<22:06:23, 19.39s/it]
{'loss': 0.1147, 'learning_rate': 1.9536077301290917e-06, 'rewards/chosen': -3.673909902572632, 'rewards/rejected': -9.220136642456055, 'rewards/accuracies': 1.0, 'rewards/margins': 5.5462260246276855, 'policy_logps/rejected': -363.5646057128906, 'policy_logps/chosen': -413.4105224609375, 'referece_logps/rejected': -271.3632507324219, 'referece_logps/chosen': -376.67144775390625, 'logits/rejected': -0.2657396197319031, 'logits/chosen': -0.4158228039741516, 'epoch': 6.71}


 75%|███████▍  | 12007/16110 [10:02:25<17:39:40, 15.50s/it]
{'loss': 0.162, 'learning_rate': 1.953486600602935e-06, 'rewards/chosen': -5.025090217590332, 'rewards/rejected': -9.631867408752441, 'rewards/accuracies': 1.0, 'rewards/margins': 4.606777191162109, 'policy_logps/rejected': -384.9667663574219, 'policy_logps/chosen': -450.437255859375, 'referece_logps/rejected': -288.6481018066406, 'referece_logps/chosen': -400.1864013671875, 'logits/rejected': -0.07932207733392715, 'logits/chosen': -0.1287550926208496, 'epoch': 6.71}

 75%|███████▍  | 12008/16110 [10:02:36<16:02:16, 14.08s/it]

 75%|███████▍  | 12009/16110 [10:02:51<16:21:48, 14.36s/it]

 75%|███████▍  | 12010/16110 [10:03:12<18:43:37, 16.44s/it]


 75%|███████▍  | 12012/16110 [10:03:43<18:26:20, 16.20s/it]

 75%|███████▍  | 12013/16110 [10:03:59<18:20:27, 16.12s/it]
{'loss': 0.2667, 'learning_rate': 1.9531222871280553e-06, 'rewards/chosen': -4.965575218200684, 'rewards/rejected': -8.780784606933594, 'rewards/accuracies': 0.875, 'rewards/margins': 3.81520938873291, 'policy_logps/rejected': -469.04986572265625, 'policy_logps/chosen': -362.7487487792969, 'referece_logps/rejected': -381.24200439453125, 'referece_logps/chosen': -313.093017578125, 'logits/rejected': 0.24179790914058685, 'logits/chosen': 0.3167959153652191, 'epoch': 6.71}

 75%|███████▍  | 12014/16110 [10:04:18<19:20:17, 17.00s/it]


 75%|███████▍  | 12016/16110 [10:04:59<20:55:07, 18.39s/it]
{'loss': 0.1511, 'learning_rate': 1.9529396102578918e-06, 'rewards/chosen': -5.207271099090576, 'rewards/rejected': -7.563233375549316, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3559622764587402, 'policy_logps/rejected': -479.3355712890625, 'policy_logps/chosen': -524.08251953125, 'referece_logps/rejected': -403.7032470703125, 'referece_logps/chosen': -472.009765625, 'logits/rejected': -0.06825476884841919, 'logits/chosen': -0.3051631450653076, 'epoch': 6.71}

 75%|███████▍  | 12017/16110 [10:05:18<21:05:31, 18.55s/it]

 75%|███████▍  | 12018/16110 [10:05:41<22:28:54, 19.78s/it]

 75%|███████▍  | 12019/16110 [10:06:01<22:31:58, 19.83s/it]

 75%|███████▍  | 12020/16110 [10:06:20<22:21:45, 19.68s/it]

 75%|███████▍  | 12021/16110 [10:06:36<21:03:43, 18.54s/it]


 75%|███████▍  | 12023/16110 [10:07:00<17:07:08, 15.08s/it]

 75%|███████▍  | 12024/16110 [10:07:14<16:43:41, 14.74s/it]

 75%|███████▍  | 12025/16110 [10:07:31<17:40:49, 15.58s/it]

 75%|███████▍  | 12026/16110 [10:07:52<19:19:21, 17.03s/it]

 75%|███████▍  | 12027/16110 [10:08:07<18:50:21, 16.61s/it]
{'loss': 0.0863, 'learning_rate': 1.9522668296488202e-06, 'rewards/chosen': -5.07091760635376, 'rewards/rejected': -10.809200286865234, 'rewards/accuracies': 1.0, 'rewards/margins': 5.738283634185791, 'policy_logps/rejected': -406.1768493652344, 'policy_logps/chosen': -410.134765625, 'referece_logps/rejected': -298.0848388671875, 'referece_logps/chosen': -359.42559814453125, 'logits/rejected': 0.20883706212043762, 'logits/chosen': 0.15670065581798553, 'epoch': 6.72}

 75%|███████▍  | 12028/16110 [10:08:22<18:21:51, 16.20s/it]

 75%|███████▍  | 12029/16110 [10:08:41<19:02:40, 16.80s/it]


 75%|███████▍  | 12031/16110 [10:09:18<20:12:31, 17.84s/it]
{'loss': 0.1757, 'learning_rate': 1.9520210272418337e-06, 'rewards/chosen': -4.905498504638672, 'rewards/rejected': -8.85519027709961, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9496922492980957, 'policy_logps/rejected': -474.31573486328125, 'policy_logps/chosen': -394.9230041503906, 'referece_logps/rejected': -385.76385498046875, 'referece_logps/chosen': -345.8680419921875, 'logits/rejected': -0.07779507339000702, 'logits/chosen': -0.08763739466667175, 'epoch': 6.72}

 75%|███████▍  | 12032/16110 [10:09:36<20:33:10, 18.14s/it]
[2024-04-06 01:17:48,673] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12033/16110 [10:09:57<21:22:57, 18.88s/it]

 75%|███████▍  | 12034/16110 [10:10:14<20:50:32, 18.41s/it]

 75%|███████▍  | 12035/16110 [10:10:31<20:08:07, 17.79s/it]

 75%|███████▍  | 12036/16110 [10:10:49<20:11:57, 17.85s/it]

 75%|███████▍  | 12037/16110 [10:11:09<20:55:46, 18.50s/it]

 75%|███████▍  | 12038/16110 [10:11:25<20:18:59, 17.96s/it]

 75%|███████▍  | 12039/16110 [10:11:42<19:59:00, 17.67s/it]
[2024-04-06 01:19:54,881] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12040/16110 [10:12:03<21:03:26, 18.63s/it]
[2024-04-06 01:20:14,899] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▍  | 12042/16110 [10:12:42<21:30:58, 19.04s/it]
{'loss': 0.1799, 'learning_rate': 1.9513418963884516e-06, 'rewards/chosen': -3.1864030361175537, 'rewards/rejected': -6.923407077789307, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7370047569274902, 'policy_logps/rejected': -453.3279113769531, 'policy_logps/chosen': -470.065185546875, 'referece_logps/rejected': -384.09381103515625, 'referece_logps/chosen': -438.2012023925781, 'logits/rejected': -0.4149685204029083, 'logits/chosen': -0.4479773938655853, 'epoch': 6.73}

 75%|███████▍  | 12043/16110 [10:12:59<20:48:29, 18.42s/it]

 75%|███████▍  | 12044/16110 [10:13:17<20:35:40, 18.23s/it]


 75%|███████▍  | 12046/16110 [10:13:58<21:53:06, 19.39s/it]

 75%|███████▍  | 12047/16110 [10:14:18<22:03:03, 19.54s/it]
{'loss': 0.23, 'learning_rate': 1.951031662188959e-06, 'rewards/chosen': -3.2662391662597656, 'rewards/rejected': -7.147243499755859, 'rewards/accuracies': 0.875, 'rewards/margins': 3.881004810333252, 'policy_logps/rejected': -455.29754638671875, 'policy_logps/chosen': -460.33966064453125, 'referece_logps/rejected': -383.82513427734375, 'referece_logps/chosen': -427.67730712890625, 'logits/rejected': -0.36250853538513184, 'logits/chosen': -0.2980915606021881, 'epoch': 6.73}

 75%|███████▍  | 12048/16110 [10:14:31<19:45:32, 17.51s/it]
[2024-04-06 01:22:43,231] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12049/16110 [10:14:52<20:47:30, 18.43s/it]

 75%|███████▍  | 12050/16110 [10:15:05<19:11:07, 17.01s/it]


 75%|███████▍  | 12052/16110 [10:15:34<17:23:58, 15.44s/it]
{'loss': 0.0914, 'learning_rate': 1.950720466954801e-06, 'rewards/chosen': -4.168760776519775, 'rewards/rejected': -8.714214324951172, 'rewards/accuracies': 1.0, 'rewards/margins': 4.545454025268555, 'policy_logps/rejected': -493.56390380859375, 'policy_logps/chosen': -397.1158752441406, 'referece_logps/rejected': -406.4217529296875, 'referece_logps/chosen': -355.4282531738281, 'logits/rejected': -0.7497254610061646, 'logits/chosen': -0.6920779347419739, 'epoch': 6.73}

 75%|███████▍  | 12053/16110 [10:15:58<20:02:30, 17.78s/it]


 75%|███████▍  | 12055/16110 [10:16:32<19:33:08, 17.36s/it]
{'loss': 0.148, 'learning_rate': 1.950533288648445e-06, 'rewards/chosen': -3.572974681854248, 'rewards/rejected': -8.969706535339355, 'rewards/accuracies': 1.0, 'rewards/margins': 5.396732330322266, 'policy_logps/rejected': -377.340576171875, 'policy_logps/chosen': -431.16265869140625, 'referece_logps/rejected': -287.6435241699219, 'referece_logps/chosen': -395.4328918457031, 'logits/rejected': -0.09533104300498962, 'logits/chosen': -0.035763680934906006, 'epoch': 6.73}

 75%|███████▍  | 12056/16110 [10:16:54<20:54:07, 18.56s/it]


 75%|███████▍  | 12058/16110 [10:17:32<21:15:28, 18.89s/it]
{'loss': 0.1447, 'learning_rate': 1.950345764550892e-06, 'rewards/chosen': -4.110714912414551, 'rewards/rejected': -8.566949844360352, 'rewards/accuracies': 1.0, 'rewards/margins': 4.456235885620117, 'policy_logps/rejected': -361.6013488769531, 'policy_logps/chosen': -459.04736328125, 'referece_logps/rejected': -275.93182373046875, 'referece_logps/chosen': -417.940185546875, 'logits/rejected': 0.9724096655845642, 'logits/chosen': 0.8016777634620667, 'epoch': 6.74}

 75%|███████▍  | 12059/16110 [10:17:50<20:45:11, 18.44s/it]

 75%|███████▍  | 12060/16110 [10:18:10<21:15:18, 18.89s/it]

 75%|███████▍  | 12061/16110 [10:18:29<21:28:32, 19.09s/it]
[2024-04-06 01:26:43,250] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▍  | 12063/16110 [10:19:11<22:11:13, 19.74s/it]
[2024-04-06 01:27:02,273] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12064/16110 [10:19:31<22:15:17, 19.80s/it]

 75%|███████▍  | 12065/16110 [10:19:51<22:22:03, 19.91s/it]
{'loss': 0.1802, 'learning_rate': 1.9499068639631664e-06, 'rewards/chosen': -5.807796478271484, 'rewards/rejected': -9.693778038024902, 'rewards/accuracies': 0.875, 'rewards/margins': 3.885981559753418, 'policy_logps/rejected': -303.0516052246094, 'policy_logps/chosen': -292.7654724121094, 'referece_logps/rejected': -206.1138153076172, 'referece_logps/chosen': -234.6875, 'logits/rejected': -0.3638627231121063, 'logits/chosen': -0.33087900280952454, 'epoch': 6.74}


 75%|███████▍  | 12067/16110 [10:20:31<22:27:46, 20.00s/it]

 75%|███████▍  | 12068/16110 [10:20:43<19:48:40, 17.64s/it]
{'loss': 0.2617, 'learning_rate': 1.9497181877216888e-06, 'rewards/chosen': -4.924753189086914, 'rewards/rejected': -11.173355102539062, 'rewards/accuracies': 0.875, 'rewards/margins': 6.248600959777832, 'policy_logps/rejected': -526.3165893554688, 'policy_logps/chosen': -461.8145446777344, 'referece_logps/rejected': -414.58306884765625, 'referece_logps/chosen': -412.5670166015625, 'logits/rejected': -0.13561025261878967, 'logits/chosen': -0.16594454646110535, 'epoch': 6.74}

 75%|███████▍  | 12069/16110 [10:21:00<19:36:12, 17.46s/it]

 75%|███████▍  | 12070/16110 [10:21:12<17:46:33, 15.84s/it]

 75%|███████▍  | 12071/16110 [10:21:24<16:33:58, 14.77s/it]

 75%|███████▍  | 12072/16110 [10:21:42<17:28:11, 15.57s/it]

 75%|███████▍  | 12073/16110 [10:22:02<18:54:26, 16.86s/it]

 75%|███████▍  | 12074/16110 [10:22:15<17:46:14, 15.85s/it]

 75%|███████▍  | 12075/16110 [10:22:33<18:29:39, 16.50s/it]

 75%|███████▍  | 12076/16110 [10:22:50<18:43:43, 16.71s/it]

 75%|███████▍  | 12077/16110 [10:23:09<19:25:05, 17.33s/it]
[2024-04-06 01:31:22,419] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12078/16110 [10:23:31<20:53:15, 18.65s/it]

 75%|███████▍  | 12079/16110 [10:23:50<20:57:14, 18.71s/it]

 75%|███████▍  | 12080/16110 [10:24:01<18:36:22, 16.62s/it]
[2024-04-06 01:32:15,486] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12081/16110 [10:24:24<20:33:51, 18.37s/it]
[2024-04-06 01:32:35,077] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▍  | 12082/16110 [10:24:43<20:58:05, 18.74s/it]

 75%|███████▌  | 12083/16110 [10:25:02<20:47:05, 18.58s/it]

 75%|███████▌  | 12084/16110 [10:25:22<21:26:54, 19.18s/it]

 75%|███████▌  | 12085/16110 [10:25:43<21:54:00, 19.59s/it]

 75%|███████▌  | 12086/16110 [10:26:05<22:37:47, 20.25s/it]

 75%|███████▌  | 12087/16110 [10:26:20<20:59:08, 18.78s/it]

 75%|███████▌  | 12088/16110 [10:26:33<19:00:42, 17.02s/it]

 75%|███████▌  | 12089/16110 [10:26:43<16:50:36, 15.08s/it]

 75%|███████▌  | 12090/16110 [10:27:03<18:15:18, 16.35s/it]


 75%|███████▌  | 12092/16110 [10:27:37<18:51:40, 16.90s/it]
{'loss': 0.2657, 'learning_rate': 1.9481963457735745e-06, 'rewards/chosen': -4.321168899536133, 'rewards/rejected': -7.878634452819824, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5574655532836914, 'policy_logps/rejected': -434.8881530761719, 'policy_logps/chosen': -530.22509765625, 'referece_logps/rejected': -356.101806640625, 'referece_logps/chosen': -487.013427734375, 'logits/rejected': -0.34999731183052063, 'logits/chosen': -0.6292828321456909, 'epoch': 6.76}

 75%|███████▌  | 12093/16110 [10:27:58<20:06:09, 18.02s/it]

 75%|███████▌  | 12094/16110 [10:28:16<19:59:53, 17.93s/it]

 75%|███████▌  | 12095/16110 [10:28:30<18:51:25, 16.91s/it]


 75%|███████▌  | 12097/16110 [10:29:05<19:37:44, 17.61s/it]
{'loss': 0.1159, 'learning_rate': 1.9478765154949354e-06, 'rewards/chosen': -2.7113358974456787, 'rewards/rejected': -5.579762935638428, 'rewards/accuracies': 0.875, 'rewards/margins': 2.868426561355591, 'policy_logps/rejected': -309.179443359375, 'policy_logps/chosen': -335.1930236816406, 'referece_logps/rejected': -253.38182067871094, 'referece_logps/chosen': -308.07965087890625, 'logits/rejected': 0.14613687992095947, 'logits/chosen': -0.06153688579797745, 'epoch': 6.76}


 75%|███████▌  | 12099/16110 [10:29:43<20:32:53, 18.44s/it]
{'loss': 0.2174, 'learning_rate': 1.9477483151683754e-06, 'rewards/chosen': -4.60597562789917, 'rewards/rejected': -9.050374031066895, 'rewards/accuracies': 1.0, 'rewards/margins': 4.444398880004883, 'policy_logps/rejected': -358.21038818359375, 'policy_logps/chosen': -322.81414794921875, 'referece_logps/rejected': -267.7066345214844, 'referece_logps/chosen': -276.7543640136719, 'logits/rejected': 0.11582191288471222, 'logits/chosen': 0.05390097200870514, 'epoch': 6.76}

 75%|███████▌  | 12100/16110 [10:30:05<21:37:33, 19.41s/it]
[2024-04-06 01:38:19,262] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 12101/16110 [10:30:28<22:40:17, 20.36s/it]

 75%|███████▌  | 12102/16110 [10:30:49<23:01:00, 20.67s/it]

 75%|███████▌  | 12103/16110 [10:31:11<23:22:05, 20.99s/it]
[2024-04-06 01:39:23,398] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▌  | 12105/16110 [10:31:52<22:57:05, 20.63s/it]
{'loss': 0.1972, 'learning_rate': 1.947362794863552e-06, 'rewards/chosen': -3.94858455657959, 'rewards/rejected': -7.598258972167969, 'rewards/accuracies': 1.0, 'rewards/margins': 3.649674892425537, 'policy_logps/rejected': -477.11749267578125, 'policy_logps/chosen': -338.2623596191406, 'referece_logps/rejected': -401.1348571777344, 'referece_logps/chosen': -298.7765197753906, 'logits/rejected': 0.31296437978744507, 'logits/chosen': 0.45433682203292847, 'epoch': 6.76}

 75%|███████▌  | 12106/16110 [10:32:14<23:42:40, 21.32s/it]


 75%|███████▌  | 12108/16110 [10:32:55<23:14:31, 20.91s/it]
{'loss': 0.1107, 'learning_rate': 1.9471695177193267e-06, 'rewards/chosen': -4.772224426269531, 'rewards/rejected': -9.122902870178223, 'rewards/accuracies': 0.875, 'rewards/margins': 4.350677490234375, 'policy_logps/rejected': -361.866943359375, 'policy_logps/chosen': -241.27638244628906, 'referece_logps/rejected': -270.637939453125, 'referece_logps/chosen': -193.5541534423828, 'logits/rejected': -0.25716155767440796, 'logits/chosen': -0.07876597344875336, 'epoch': 6.76}

 75%|███████▌  | 12109/16110 [10:33:14<22:30:17, 20.25s/it]
[2024-04-06 01:41:27,392] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 12110/16110 [10:33:36<22:55:52, 20.64s/it]
[2024-04-06 01:41:42,244] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 75%|███████▌  | 12112/16110 [10:34:07<20:18:41, 18.29s/it]
{'loss': 0.2149, 'learning_rate': 1.946911278878638e-06, 'rewards/chosen': -3.3368167877197266, 'rewards/rejected': -8.349440574645996, 'rewards/accuracies': 0.75, 'rewards/margins': 5.0126237869262695, 'policy_logps/rejected': -432.33197021484375, 'policy_logps/chosen': -273.589599609375, 'referece_logps/rejected': -348.8376159667969, 'referece_logps/chosen': -240.22145080566406, 'logits/rejected': 0.2723366320133209, 'logits/chosen': 0.28945857286453247, 'epoch': 6.77}

 75%|███████▌  | 12113/16110 [10:34:22<19:00:51, 17.13s/it]

 75%|███████▌  | 12114/16110 [10:34:35<17:47:47, 16.03s/it]

 75%|███████▌  | 12115/16110 [10:34:53<18:20:02, 16.52s/it]

 75%|███████▌  | 12116/16110 [10:35:05<16:54:09, 15.24s/it]
[2024-04-06 01:43:18,260] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 12117/16110 [10:35:27<18:56:20, 17.08s/it]


 75%|███████▌  | 12119/16110 [10:36:04<19:36:06, 17.68s/it]
{'loss': 0.171, 'learning_rate': 1.946457887426825e-06, 'rewards/chosen': -5.107660293579102, 'rewards/rejected': -9.047110557556152, 'rewards/accuracies': 0.75, 'rewards/margins': 3.9394500255584717, 'policy_logps/rejected': -405.08984375, 'policy_logps/chosen': -325.8830871582031, 'referece_logps/rejected': -314.61871337890625, 'referece_logps/chosen': -274.8064880371094, 'logits/rejected': -0.37874770164489746, 'logits/chosen': -0.28420233726501465, 'epoch': 6.77}

 75%|███████▌  | 12120/16110 [10:36:24<20:27:56, 18.47s/it]

 75%|███████▌  | 12121/16110 [10:36:47<22:07:13, 19.96s/it]

 75%|███████▌  | 12122/16110 [10:37:10<22:58:31, 20.74s/it]

 75%|███████▌  | 12123/16110 [10:37:29<22:20:59, 20.18s/it]

 75%|███████▌  | 12124/16110 [10:37:41<19:47:28, 17.87s/it]

 75%|███████▌  | 12125/16110 [10:37:59<19:47:46, 17.88s/it]

 75%|███████▌  | 12126/16110 [10:38:19<20:27:04, 18.48s/it]

 75%|███████▌  | 12127/16110 [10:38:37<20:18:37, 18.36s/it]


 75%|███████▌  | 12129/16110 [10:39:18<21:04:18, 19.06s/it]
{'loss': 0.096, 'learning_rate': 1.945806933772929e-06, 'rewards/chosen': -3.818783760070801, 'rewards/rejected': -7.336204528808594, 'rewards/accuracies': 1.0, 'rewards/margins': 3.517421245574951, 'policy_logps/rejected': -275.6288757324219, 'policy_logps/chosen': -319.51593017578125, 'referece_logps/rejected': -202.26683044433594, 'referece_logps/chosen': -281.3280944824219, 'logits/rejected': 0.02282850444316864, 'logits/chosen': -0.32673364877700806, 'epoch': 6.78}

 75%|███████▌  | 12130/16110 [10:39:36<20:48:35, 18.82s/it]

 75%|███████▌  | 12131/16110 [10:39:56<21:14:12, 19.21s/it]

 75%|███████▌  | 12132/16110 [10:40:19<22:14:32, 20.13s/it]

 75%|███████▌  | 12133/16110 [10:40:36<21:15:51, 19.25s/it]

 75%|███████▌  | 12134/16110 [10:40:52<20:12:01, 18.29s/it]

 75%|███████▌  | 12135/16110 [10:41:10<20:01:24, 18.13s/it]

 75%|███████▌  | 12136/16110 [10:41:24<18:45:11, 16.99s/it]

 75%|███████▌  | 12137/16110 [10:41:42<19:16:30, 17.47s/it]

 75%|███████▌  | 12138/16110 [10:41:55<17:41:41, 16.04s/it]

 75%|███████▌  | 12139/16110 [10:42:15<18:59:58, 17.22s/it]

 75%|███████▌  | 12140/16110 [10:42:32<18:43:32, 16.98s/it]


 75%|███████▌  | 12142/16110 [10:43:14<21:12:08, 19.24s/it]

 75%|███████▌  | 12143/16110 [10:43:34<21:27:38, 19.48s/it]

 75%|███████▌  | 12144/16110 [10:43:52<20:59:37, 19.06s/it]
{'loss': 0.1583, 'learning_rate': 1.9448233359579977e-06, 'rewards/chosen': -4.553543567657471, 'rewards/rejected': -10.298013687133789, 'rewards/accuracies': 1.0, 'rewards/margins': 5.744470596313477, 'policy_logps/rejected': -512.2498779296875, 'policy_logps/chosen': -376.5129089355469, 'referece_logps/rejected': -409.269775390625, 'referece_logps/chosen': -330.97747802734375, 'logits/rejected': 0.5483385920524597, 'logits/chosen': 0.5266852378845215, 'epoch': 6.78}

 75%|███████▌  | 12145/16110 [10:44:07<19:39:41, 17.85s/it]


 75%|███████▌  | 12147/16110 [10:44:40<18:34:25, 16.87s/it]
{'loss': 0.2179, 'learning_rate': 1.9446255849661517e-06, 'rewards/chosen': -4.436445713043213, 'rewards/rejected': -7.359426975250244, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9229812622070312, 'policy_logps/rejected': -489.00323486328125, 'policy_logps/chosen': -366.8001708984375, 'referece_logps/rejected': -415.40899658203125, 'referece_logps/chosen': -322.4356994628906, 'logits/rejected': -0.38417690992355347, 'logits/chosen': -0.28138619661331177, 'epoch': 6.79}

 75%|███████▌  | 12148/16110 [10:44:59<19:08:06, 17.39s/it]

 75%|███████▌  | 12149/16110 [10:45:16<19:00:48, 17.28s/it]

 75%|███████▌  | 12150/16110 [10:45:32<18:37:57, 16.94s/it]

 75%|███████▌  | 12151/16110 [10:45:51<19:10:21, 17.43s/it]
[2024-04-06 01:53:57,763] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 75%|███████▌  | 12152/16110 [10:46:06<18:32:24, 16.86s/it]


 75%|███████▌  | 12154/16110 [10:46:44<20:06:33, 18.30s/it]
{'loss': 0.2223, 'learning_rate': 1.944162829723986e-06, 'rewards/chosen': -6.064962863922119, 'rewards/rejected': -9.674638748168945, 'rewards/accuracies': 0.75, 'rewards/margins': 3.6096765995025635, 'policy_logps/rejected': -507.4638671875, 'policy_logps/chosen': -476.39202880859375, 'referece_logps/rejected': -410.7174987792969, 'referece_logps/chosen': -415.742431640625, 'logits/rejected': 0.36312445998191833, 'logits/chosen': 0.4251902997493744, 'epoch': 6.79}


 75%|███████▌  | 12156/16110 [10:47:25<21:01:03, 19.14s/it]
{'loss': 0.1372, 'learning_rate': 1.9440302704266724e-06, 'rewards/chosen': -6.238470554351807, 'rewards/rejected': -8.955876350402832, 'rewards/accuracies': 0.875, 'rewards/margins': 2.717405080795288, 'policy_logps/rejected': -598.1275634765625, 'policy_logps/chosen': -512.0, 'referece_logps/rejected': -508.5688171386719, 'referece_logps/chosen': -449.6153564453125, 'logits/rejected': -0.5375384092330933, 'logits/chosen': -0.5483167171478271, 'epoch': 6.79}

 75%|███████▌  | 12157/16110 [10:47:44<21:11:19, 19.30s/it]

 75%|███████▌  | 12158/16110 [10:48:01<20:24:35, 18.59s/it]

 75%|███████▌  | 12159/16110 [10:48:18<19:46:35, 18.02s/it]

 75%|███████▌  | 12160/16110 [10:48:36<19:49:08, 18.06s/it]

 75%|███████▌  | 12161/16110 [10:48:48<17:50:02, 16.26s/it]

 75%|███████▌  | 12162/16110 [10:49:10<19:42:27, 17.97s/it]

 75%|███████▌  | 12163/16110 [10:49:30<20:20:43, 18.56s/it]

 76%|███████▌  | 12164/16110 [10:49:48<20:03:42, 18.30s/it]

 76%|███████▌  | 12165/16110 [10:50:04<19:29:43, 17.79s/it]

 76%|███████▌  | 12166/16110 [10:50:24<20:04:33, 18.32s/it]

 76%|███████▌  | 12167/16110 [10:50:40<19:21:07, 17.67s/it]

 76%|███████▌  | 12168/16110 [10:51:03<21:07:40, 19.29s/it]

 76%|███████▌  | 12169/16110 [10:51:20<20:27:06, 18.68s/it]

 76%|███████▌  | 12170/16110 [10:51:37<19:46:32, 18.07s/it]

 76%|███████▌  | 12171/16110 [10:51:57<20:26:08, 18.68s/it]

 76%|███████▌  | 12172/16110 [10:52:14<20:01:47, 18.31s/it]

 76%|███████▌  | 12173/16110 [10:52:31<19:33:45, 17.89s/it]

 76%|███████▌  | 12174/16110 [10:52:48<19:12:06, 17.56s/it]

 76%|███████▌  | 12175/16110 [10:53:02<17:48:16, 16.29s/it]

 76%|███████▌  | 12176/16110 [10:53:18<17:53:09, 16.37s/it]

 76%|███████▌  | 12177/16110 [10:53:29<16:09:24, 14.79s/it]

 76%|███████▌  | 12178/16110 [10:53:51<18:21:07, 16.80s/it]

 76%|███████▌  | 12179/16110 [10:54:04<17:05:51, 15.66s/it]

 76%|███████▌  | 12180/16110 [10:54:25<19:06:46, 17.51s/it]

 76%|███████▌  | 12181/16110 [10:54:42<18:44:19, 17.17s/it]

 76%|███████▌  | 12182/16110 [10:55:02<19:44:11, 18.09s/it]

 76%|███████▌  | 12183/16110 [10:55:25<21:13:37, 19.46s/it]

 76%|███████▌  | 12184/16110 [10:55:36<18:36:31, 17.06s/it]

 76%|███████▌  | 12185/16110 [10:55:48<16:51:51, 15.47s/it]

 76%|███████▌  | 12186/16110 [10:56:04<16:56:52, 15.55s/it]

 76%|███████▌  | 12187/16110 [10:56:21<17:27:39, 16.02s/it]

 76%|███████▌  | 12188/16110 [10:56:38<17:49:54, 16.37s/it]

 76%|███████▌  | 12189/16110 [10:56:58<19:08:44, 17.58s/it]

 76%|███████▌  | 12190/16110 [10:57:18<19:54:31, 18.28s/it]


 76%|███████▌  | 12192/16110 [10:57:52<19:16:06, 17.70s/it]

 76%|███████▌  | 12193/16110 [10:58:10<19:16:06, 17.71s/it]

 76%|███████▌  | 12194/16110 [10:58:30<20:01:51, 18.41s/it]

 76%|███████▌  | 12195/16110 [10:58:52<21:20:12, 19.62s/it]
[2024-04-06 02:06:44,014] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12196/16110 [10:59:07<19:33:33, 17.99s/it]
[2024-04-06 02:06:58,201] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12197/16110 [10:59:28<20:45:14, 19.09s/it]

 76%|███████▌  | 12198/16110 [10:59:44<19:41:46, 18.13s/it]

 76%|███████▌  | 12199/16110 [11:00:07<21:09:47, 19.48s/it]

 76%|███████▌  | 12200/16110 [11:00:27<21:17:46, 19.61s/it]

 76%|███████▌  | 12201/16110 [11:00:42<19:55:42, 18.35s/it]

 76%|███████▌  | 12202/16110 [11:01:04<21:04:56, 19.42s/it]

 76%|███████▌  | 12203/16110 [11:01:23<20:52:33, 19.24s/it]

 76%|███████▌  | 12204/16110 [11:01:41<20:24:05, 18.80s/it]

 76%|███████▌  | 12205/16110 [11:02:03<21:32:42, 19.86s/it]

 76%|███████▌  | 12206/16110 [11:02:19<20:14:28, 18.67s/it]
[2024-04-06 02:10:10,424] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12207/16110 [11:02:32<18:25:59, 17.00s/it]

 76%|███████▌  | 12208/16110 [11:02:44<16:58:26, 15.66s/it]
{'loss': 0.0884, 'learning_rate': 1.9405302175149627e-06, 'rewards/chosen': -3.922104835510254, 'rewards/rejected': -9.669031143188477, 'rewards/accuracies': 1.0, 'rewards/margins': 5.7469258308410645, 'policy_logps/rejected': -444.37835693359375, 'policy_logps/chosen': -437.76824951171875, 'referece_logps/rejected': -347.6880798339844, 'referece_logps/chosen': -398.5472412109375, 'logits/rejected': -0.7865725755691528, 'logits/chosen': -0.8149838447570801, 'epoch': 6.82}


 76%|███████▌  | 12210/16110 [11:03:26<19:52:24, 18.34s/it]

 76%|███████▌  | 12211/16110 [11:03:44<19:43:57, 18.22s/it]

 76%|███████▌  | 12212/16110 [11:03:56<17:35:21, 16.24s/it]

 76%|███████▌  | 12213/16110 [11:04:11<17:16:28, 15.96s/it]

 76%|███████▌  | 12214/16110 [11:04:29<17:43:59, 16.39s/it]

 76%|███████▌  | 12215/16110 [11:04:49<19:09:32, 17.71s/it]
[2024-04-06 02:12:41,049] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12216/16110 [11:05:09<19:55:43, 18.42s/it]
[2024-04-06 02:13:01,144] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12217/16110 [11:05:30<20:43:01, 19.16s/it]

 76%|███████▌  | 12218/16110 [11:05:51<21:13:46, 19.64s/it]
{'loss': 0.2864, 'learning_rate': 1.9398453332826914e-06, 'rewards/chosen': -3.8322908878326416, 'rewards/rejected': -5.139791011810303, 'rewards/accuracies': 0.75, 'rewards/margins': 1.307499885559082, 'policy_logps/rejected': -305.4765319824219, 'policy_logps/chosen': -228.1409912109375, 'referece_logps/rejected': -254.07864379882812, 'referece_logps/chosen': -189.81808471679688, 'logits/rejected': -0.11299817264080048, 'logits/chosen': -0.04863470792770386, 'epoch': 6.83}
[2024-04-06 02:14:03,447] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▌  | 12220/16110 [11:06:25<19:29:30, 18.04s/it]
[2024-04-06 02:14:17,027] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12221/16110 [11:06:42<19:09:02, 17.73s/it]

 76%|███████▌  | 12222/16110 [11:07:05<20:35:48, 19.07s/it]

 76%|███████▌  | 12223/16110 [11:07:25<21:00:50, 19.46s/it]

 76%|███████▌  | 12224/16110 [11:07:37<18:40:05, 17.29s/it]

 76%|███████▌  | 12225/16110 [11:07:56<19:11:56, 17.79s/it]

 76%|███████▌  | 12226/16110 [11:08:13<18:53:03, 17.50s/it]

 76%|███████▌  | 12227/16110 [11:08:32<19:30:00, 18.08s/it]
[2024-04-06 02:16:24,050] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2273, 'learning_rate': 1.9392256893163324e-06, 'rewards/chosen': -3.185464382171631, 'rewards/rejected': -8.378993034362793, 'rewards/accuracies': 1.0, 'rewards/margins': 5.193528175354004, 'policy_logps/rejected': -522.8131713867188, 'policy_logps/chosen': -493.9876708984375, 'referece_logps/rejected': -439.0232849121094, 'referece_logps/chosen': -462.1329650878906, 'logits/rejected': -0.06522837281227112, 'logits/chosen': 0.08887428045272827, 'epoch': 6.83}


 76%|███████▌  | 12229/16110 [11:09:06<18:47:03, 17.42s/it]

 76%|███████▌  | 12230/16110 [11:09:18<17:04:37, 15.84s/it]

 76%|███████▌  | 12231/16110 [11:09:40<18:47:02, 17.43s/it]

 76%|███████▌  | 12232/16110 [11:09:54<17:58:28, 16.69s/it]

 76%|███████▌  | 12233/16110 [11:10:14<19:01:22, 17.66s/it]

 76%|███████▌  | 12234/16110 [11:10:36<20:25:24, 18.97s/it]
[2024-04-06 02:18:28,058] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 76%|███████▌  | 12235/16110 [11:11:00<21:45:41, 20.22s/it]

 76%|███████▌  | 12236/16110 [11:11:17<21:00:46, 19.53s/it]

 76%|███████▌  | 12237/16110 [11:11:36<20:38:55, 19.19s/it]

 76%|███████▌  | 12238/16110 [11:11:56<20:47:05, 19.32s/it]

 76%|███████▌  | 12239/16110 [11:12:08<18:29:44, 17.20s/it]

 76%|███████▌  | 12240/16110 [11:12:27<19:13:09, 17.88s/it]

 76%|███████▌  | 12241/16110 [11:12:43<18:36:31, 17.31s/it]

 76%|███████▌  | 12242/16110 [11:13:03<19:24:14, 18.06s/it]
[2024-04-06 02:20:54,652] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0925, 'learning_rate': 1.9381861168275846e-06, 'rewards/chosen': -4.554713249206543, 'rewards/rejected': -9.115608215332031, 'rewards/accuracies': 1.0, 'rewards/margins': 4.560895919799805, 'policy_logps/rejected': -331.40374755859375, 'policy_logps/chosen': -402.5927734375, 'referece_logps/rejected': -240.24765014648438, 'referece_logps/chosen': -357.0456848144531, 'logits/rejected': 0.1742924153804779, 'logits/chosen': -0.028102785348892212, 'epoch': 6.84}


 76%|███████▌  | 12244/16110 [11:13:43<20:05:56, 18.72s/it]

 76%|███████▌  | 12245/16110 [11:14:03<20:25:43, 19.03s/it]

 76%|███████▌  | 12246/16110 [11:14:25<21:22:37, 19.92s/it]
[2024-04-06 02:22:16,326] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1372, 'learning_rate': 1.9379074560639896e-06, 'rewards/chosen': -4.128525257110596, 'rewards/rejected': -9.184745788574219, 'rewards/accuracies': 1.0, 'rewards/margins': 5.056220531463623, 'policy_logps/rejected': -388.4657897949219, 'policy_logps/chosen': -283.8044128417969, 'referece_logps/rejected': -296.61834716796875, 'referece_logps/chosen': -242.51918029785156, 'logits/rejected': 0.3428521752357483, 'logits/chosen': 0.4544083774089813, 'epoch': 6.84}


 76%|███████▌  | 12248/16110 [11:15:06<21:44:15, 20.26s/it]
{'loss': 0.2006, 'learning_rate': 1.937767898205526e-06, 'rewards/chosen': -3.306549549102783, 'rewards/rejected': -7.5512919425964355, 'rewards/accuracies': 0.875, 'rewards/margins': 4.244742393493652, 'policy_logps/rejected': -351.3420715332031, 'policy_logps/chosen': -303.9926452636719, 'referece_logps/rejected': -275.8291320800781, 'referece_logps/chosen': -270.92718505859375, 'logits/rejected': -0.24146416783332825, 'logits/chosen': -0.17601588368415833, 'epoch': 6.84}

 76%|███████▌  | 12249/16110 [11:15:24<21:04:35, 19.65s/it]


 76%|███████▌  | 12251/16110 [11:16:04<21:06:55, 19.70s/it]

 76%|███████▌  | 12252/16110 [11:16:20<19:53:32, 18.56s/it]

 76%|███████▌  | 12253/16110 [11:16:38<19:47:22, 18.47s/it]

 76%|███████▌  | 12254/16110 [11:16:56<19:38:45, 18.34s/it]

 76%|███████▌  | 12255/16110 [11:17:09<17:57:45, 16.77s/it]
{'loss': 0.1872, 'learning_rate': 1.937278251833562e-06, 'rewards/chosen': -4.798027992248535, 'rewards/rejected': -9.423332214355469, 'rewards/accuracies': 1.0, 'rewards/margins': 4.625303745269775, 'policy_logps/rejected': -482.3851013183594, 'policy_logps/chosen': -352.4871826171875, 'referece_logps/rejected': -388.15179443359375, 'referece_logps/chosen': -304.50689697265625, 'logits/rejected': -0.5671612024307251, 'logits/chosen': -0.490254282951355, 'epoch': 6.85}


 76%|███████▌  | 12257/16110 [11:17:42<18:04:09, 16.88s/it]

 76%|███████▌  | 12258/16110 [11:17:59<18:03:20, 16.87s/it]
{'loss': 0.1811, 'learning_rate': 1.9370678350498444e-06, 'rewards/chosen': -3.9327900409698486, 'rewards/rejected': -7.398724555969238, 'rewards/accuracies': 0.875, 'rewards/margins': 3.465934991836548, 'policy_logps/rejected': -385.9268493652344, 'policy_logps/chosen': -390.27935791015625, 'referece_logps/rejected': -311.9395446777344, 'referece_logps/chosen': -350.95147705078125, 'logits/rejected': -0.26058804988861084, 'logits/chosen': -0.40356048941612244, 'epoch': 6.85}


 76%|███████▌  | 12260/16110 [11:18:30<17:08:52, 16.03s/it]

 76%|███████▌  | 12261/16110 [11:18:50<18:34:51, 17.38s/it]

 76%|███████▌  | 12262/16110 [11:19:08<18:47:08, 17.58s/it]

 76%|███████▌  | 12263/16110 [11:19:26<19:01:05, 17.80s/it]

 76%|███████▌  | 12264/16110 [11:19:49<20:32:50, 19.23s/it]
[2024-04-06 02:27:40,726] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1968, 'learning_rate': 1.9366459788811416e-06, 'rewards/chosen': -4.730505466461182, 'rewards/rejected': -9.03991413116455, 'rewards/accuracies': 0.875, 'rewards/margins': 4.309408664703369, 'policy_logps/rejected': -430.4047546386719, 'policy_logps/chosen': -457.9450988769531, 'referece_logps/rejected': -340.0055847167969, 'referece_logps/chosen': -410.6400146484375, 'logits/rejected': 0.5388372540473938, 'logits/chosen': 0.7055567502975464, 'epoch': 6.85}
[2024-04-06 02:28:02,345] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▌  | 12266/16110 [11:20:29<20:36:56, 19.31s/it]

 76%|███████▌  | 12267/16110 [11:20:50<21:26:15, 20.08s/it]

 76%|███████▌  | 12268/16110 [11:21:02<18:50:19, 17.65s/it]

 76%|███████▌  | 12269/16110 [11:21:20<18:41:31, 17.52s/it]

 76%|███████▌  | 12270/16110 [11:21:42<20:05:40, 18.84s/it]

 76%|███████▌  | 12271/16110 [11:21:58<19:21:41, 18.16s/it]

 76%|███████▌  | 12272/16110 [11:22:16<19:16:12, 18.08s/it]

 76%|███████▌  | 12273/16110 [11:22:34<19:21:38, 18.16s/it]

 76%|███████▌  | 12274/16110 [11:22:54<19:47:30, 18.57s/it]

 76%|███████▌  | 12275/16110 [11:23:14<20:25:24, 19.17s/it]
{'loss': 0.1251, 'learning_rate': 1.9358690364482637e-06, 'rewards/chosen': -5.2723517417907715, 'rewards/rejected': -10.168741226196289, 'rewards/accuracies': 0.875, 'rewards/margins': 4.896389007568359, 'policy_logps/rejected': -386.24517822265625, 'policy_logps/chosen': -476.34503173828125, 'referece_logps/rejected': -284.5577697753906, 'referece_logps/chosen': -423.62152099609375, 'logits/rejected': -0.27312251925468445, 'logits/chosen': -0.6405447721481323, 'epoch': 6.86}

 76%|███████▌  | 12276/16110 [11:23:29<19:01:37, 17.87s/it]

 76%|███████▌  | 12277/16110 [11:23:43<17:44:12, 16.66s/it]


 76%|███████▌  | 12279/16110 [11:24:15<17:14:06, 16.20s/it]

 76%|███████▌  | 12280/16110 [11:24:32<17:35:06, 16.53s/it]
{'loss': 0.342, 'learning_rate': 1.935514367428128e-06, 'rewards/chosen': -5.276344299316406, 'rewards/rejected': -7.352187156677246, 'rewards/accuracies': 0.75, 'rewards/margins': 2.075843334197998, 'policy_logps/rejected': -430.3298645019531, 'policy_logps/chosen': -428.8150634765625, 'referece_logps/rejected': -356.8080139160156, 'referece_logps/chosen': -376.0516052246094, 'logits/rejected': 0.03962770849466324, 'logits/chosen': 0.16514727473258972, 'epoch': 6.86}


 76%|███████▌  | 12282/16110 [11:25:06<17:54:49, 16.85s/it]
{'loss': 0.0584, 'learning_rate': 1.935372235100807e-06, 'rewards/chosen': -4.674557209014893, 'rewards/rejected': -10.684460639953613, 'rewards/accuracies': 0.875, 'rewards/margins': 6.0099029541015625, 'policy_logps/rejected': -489.1235046386719, 'policy_logps/chosen': -406.1256103515625, 'referece_logps/rejected': -382.2789001464844, 'referece_logps/chosen': -359.3800048828125, 'logits/rejected': 0.06616388261318207, 'logits/chosen': 0.1354680061340332, 'epoch': 6.86}

 76%|███████▌  | 12283/16110 [11:25:28<19:28:12, 18.32s/it]

 76%|███████▋  | 12284/16110 [11:25:39<17:18:35, 16.29s/it]

 76%|███████▋  | 12285/16110 [11:26:00<18:40:36, 17.58s/it]


 76%|███████▋  | 12287/16110 [11:26:41<20:13:39, 19.05s/it]

 76%|███████▋  | 12288/16110 [11:26:58<19:38:12, 18.50s/it]

 76%|███████▋  | 12289/16110 [11:27:19<20:17:01, 19.11s/it]

 76%|███████▋  | 12290/16110 [11:27:32<18:29:40, 17.43s/it]
{'loss': 0.1173, 'learning_rate': 1.9348021936847346e-06, 'rewards/chosen': -3.9394731521606445, 'rewards/rejected': -9.385089874267578, 'rewards/accuracies': 0.875, 'rewards/margins': 5.445615768432617, 'policy_logps/rejected': -304.77191162109375, 'policy_logps/chosen': -344.4450378417969, 'referece_logps/rejected': -210.9210205078125, 'referece_logps/chosen': -305.0502624511719, 'logits/rejected': -0.11584612727165222, 'logits/chosen': -0.22774092853069305, 'epoch': 6.87}
[2024-04-06 02:35:43,006] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▋  | 12292/16110 [11:28:07<18:09:30, 17.12s/it]
{'loss': 0.1411, 'learning_rate': 1.9346593054192275e-06, 'rewards/chosen': -3.030941963195801, 'rewards/rejected': -9.018253326416016, 'rewards/accuracies': 1.0, 'rewards/margins': 5.987311840057373, 'policy_logps/rejected': -335.0372314453125, 'policy_logps/chosen': -593.1467895507812, 'referece_logps/rejected': -244.85470581054688, 'referece_logps/chosen': -562.8373413085938, 'logits/rejected': 1.0277988910675049, 'logits/chosen': 0.8945538401603699, 'epoch': 6.87}


 76%|███████▋  | 12294/16110 [11:28:37<16:50:56, 15.90s/it]
{'loss': 0.1401, 'learning_rate': 1.9345162660352927e-06, 'rewards/chosen': -4.557244300842285, 'rewards/rejected': -8.702073097229004, 'rewards/accuracies': 1.0, 'rewards/margins': 4.144828796386719, 'policy_logps/rejected': -479.2091369628906, 'policy_logps/chosen': -393.80950927734375, 'referece_logps/rejected': -392.1883850097656, 'referece_logps/chosen': -348.237060546875, 'logits/rejected': -0.408143013715744, 'logits/chosen': -0.3578070104122162, 'epoch': 6.87}

 76%|███████▋  | 12295/16110 [11:28:57<18:22:27, 17.34s/it]

 76%|███████▋  | 12296/16110 [11:29:20<20:00:16, 18.88s/it]


 76%|███████▋  | 12298/16110 [11:29:54<18:52:26, 17.82s/it]

 76%|███████▋  | 12299/16110 [11:30:09<17:52:07, 16.88s/it]
{'loss': 0.1191, 'learning_rate': 1.9341580065841677e-06, 'rewards/chosen': -3.0604941844940186, 'rewards/rejected': -6.064213752746582, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0037195682525635, 'policy_logps/rejected': -367.53143310546875, 'policy_logps/chosen': -412.66058349609375, 'referece_logps/rejected': -306.8892822265625, 'referece_logps/chosen': -382.0556640625, 'logits/rejected': 0.32985323667526245, 'logits/chosen': 0.2228853702545166, 'epoch': 6.87}

 76%|███████▋  | 12300/16110 [11:30:22<16:35:35, 15.68s/it]


 76%|███████▋  | 12302/16110 [11:31:01<18:50:16, 17.81s/it]
{'loss': 0.1439, 'learning_rate': 1.9339425977781883e-06, 'rewards/chosen': -3.957615613937378, 'rewards/rejected': -8.610846519470215, 'rewards/accuracies': 1.0, 'rewards/margins': 4.653231620788574, 'policy_logps/rejected': -393.23681640625, 'policy_logps/chosen': -393.05218505859375, 'referece_logps/rejected': -307.1283874511719, 'referece_logps/chosen': -353.4760437011719, 'logits/rejected': -0.2047078013420105, 'logits/chosen': -0.30360063910484314, 'epoch': 6.87}

 76%|███████▋  | 12303/16110 [11:31:22<19:54:46, 18.83s/it]
{'loss': 0.104, 'learning_rate': 1.933870719337716e-06, 'rewards/chosen': -3.6257951259613037, 'rewards/rejected': -8.357144355773926, 'rewards/accuracies': 0.875, 'rewards/margins': 4.731349468231201, 'policy_logps/rejected': -514.5989990234375, 'policy_logps/chosen': -540.6599731445312, 'referece_logps/rejected': -431.0275573730469, 'referece_logps/chosen': -504.40203857421875, 'logits/rejected': -0.5040656328201294, 'logits/chosen': -0.41923174262046814, 'epoch': 6.87}

 76%|███████▋  | 12304/16110 [11:31:42<20:11:56, 19.11s/it]


 76%|███████▋  | 12306/16110 [11:32:17<19:20:29, 18.30s/it]
{'loss': 0.2412, 'learning_rate': 1.933654857541535e-06, 'rewards/chosen': -5.262687683105469, 'rewards/rejected': -7.567869186401367, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3051822185516357, 'policy_logps/rejected': -433.70184326171875, 'policy_logps/chosen': -481.0632629394531, 'referece_logps/rejected': -358.0231628417969, 'referece_logps/chosen': -428.4364318847656, 'logits/rejected': -0.33700913190841675, 'logits/chosen': -0.07608196139335632, 'epoch': 6.87}

 76%|███████▋  | 12307/16110 [11:32:34<18:48:44, 17.81s/it]

 76%|███████▋  | 12308/16110 [11:32:56<20:10:25, 19.10s/it]

 76%|███████▋  | 12309/16110 [11:33:12<19:06:23, 18.10s/it]

 76%|███████▋  | 12310/16110 [11:33:32<19:46:00, 18.73s/it]


 76%|███████▋  | 12312/16110 [11:34:05<18:34:49, 17.61s/it]

 76%|███████▋  | 12313/16110 [11:34:21<17:57:12, 17.02s/it]

 76%|███████▋  | 12314/16110 [11:34:41<18:53:55, 17.92s/it]

 76%|███████▋  | 12315/16110 [11:35:01<19:32:04, 18.53s/it]
{'loss': 0.1574, 'learning_rate': 1.9330052345614165e-06, 'rewards/chosen': -4.204209327697754, 'rewards/rejected': -8.696345329284668, 'rewards/accuracies': 1.0, 'rewards/margins': 4.492136001586914, 'policy_logps/rejected': -439.3948669433594, 'policy_logps/chosen': -339.2282409667969, 'referece_logps/rejected': -352.4314270019531, 'referece_logps/chosen': -297.1861572265625, 'logits/rejected': -0.401192843914032, 'logits/chosen': -0.43122661113739014, 'epoch': 6.88}

 76%|███████▋  | 12316/16110 [11:35:14<17:47:03, 16.88s/it]

 76%|███████▋  | 12317/16110 [11:35:34<18:44:32, 17.79s/it]
[2024-04-06 02:43:47,656] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 76%|███████▋  | 12319/16110 [11:36:15<20:11:29, 19.17s/it]
[2024-04-06 02:44:07,139] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0943, 'learning_rate': 1.932715532578932e-06, 'rewards/chosen': -4.632843494415283, 'rewards/rejected': -7.9880757331848145, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3552322387695312, 'policy_logps/rejected': -543.827392578125, 'policy_logps/chosen': -420.47052001953125, 'referece_logps/rejected': -463.94659423828125, 'referece_logps/chosen': -374.1421203613281, 'logits/rejected': 0.20530202984809875, 'logits/chosen': 0.2795751392841339, 'epoch': 6.88}

 76%|███████▋  | 12320/16110 [11:36:30<18:44:39, 17.80s/it]

 76%|███████▋  | 12321/16110 [11:36:44<17:24:51, 16.55s/it]


 76%|███████▋  | 12323/16110 [11:37:19<17:53:05, 17.00s/it]

 76%|███████▋  | 12324/16110 [11:37:33<16:51:19, 16.03s/it]

 77%|███████▋  | 12325/16110 [11:37:55<18:51:58, 17.94s/it]

 77%|███████▋  | 12326/16110 [11:38:15<19:30:07, 18.55s/it]
[2024-04-06 02:46:06,721] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 77%|███████▋  | 12327/16110 [11:38:37<20:34:43, 19.58s/it]
[2024-04-06 02:46:28,705] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1544, 'learning_rate': 1.932134319151956e-06, 'rewards/chosen': -5.514852046966553, 'rewards/rejected': -8.773741722106934, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2588906288146973, 'policy_logps/rejected': -420.22479248046875, 'policy_logps/chosen': -378.1815185546875, 'referece_logps/rejected': -332.48736572265625, 'referece_logps/chosen': -323.032958984375, 'logits/rejected': 0.3235698342323303, 'logits/chosen': 0.288504958152771, 'epoch': 6.89}
[2024-04-06 02:46:47,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 77%|███████▋  | 12329/16110 [11:39:13<19:43:54, 18.79s/it]

 77%|███████▋  | 12330/16110 [11:39:35<20:36:23, 19.63s/it]
{'loss': 0.1568, 'learning_rate': 1.9319157423565164e-06, 'rewards/chosen': -5.213597774505615, 'rewards/rejected': -7.388596534729004, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1749987602233887, 'policy_logps/rejected': -415.2193298339844, 'policy_logps/chosen': -270.35308837890625, 'referece_logps/rejected': -341.3333435058594, 'referece_logps/chosen': -218.2170867919922, 'logits/rejected': -0.6550740003585815, 'logits/chosen': -0.6112870573997498, 'epoch': 6.89}

 77%|███████▋  | 12331/16110 [11:39:53<19:59:25, 19.04s/it]

 77%|███████▋  | 12332/16110 [11:40:12<20:06:54, 19.17s/it]

 77%|███████▋  | 12333/16110 [11:40:34<20:59:35, 20.01s/it]

 77%|███████▋  | 12334/16110 [11:40:56<21:34:00, 20.56s/it]


 77%|███████▋  | 12336/16110 [11:41:26<18:07:43, 17.29s/it]

 77%|███████▋  | 12337/16110 [11:41:42<17:44:50, 16.93s/it]
{'loss': 0.1411, 'learning_rate': 1.9314044115666607e-06, 'rewards/chosen': -5.401455402374268, 'rewards/rejected': -11.572097778320312, 'rewards/accuracies': 1.0, 'rewards/margins': 6.170642375946045, 'policy_logps/rejected': -538.5879516601562, 'policy_logps/chosen': -467.7747497558594, 'referece_logps/rejected': -422.8670349121094, 'referece_logps/chosen': -413.7602233886719, 'logits/rejected': -0.2690102458000183, 'logits/chosen': -0.3966192901134491, 'epoch': 6.89}

 77%|███████▋  | 12338/16110 [11:41:57<17:11:17, 16.40s/it]

 77%|███████▋  | 12339/16110 [11:42:18<18:49:48, 17.98s/it]


 77%|███████▋  | 12341/16110 [11:42:58<19:46:48, 18.89s/it]

 77%|███████▋  | 12342/16110 [11:43:14<18:50:44, 18.01s/it]

 77%|███████▋  | 12343/16110 [11:43:34<19:21:27, 18.50s/it]

 77%|███████▋  | 12344/16110 [11:43:48<18:05:17, 17.29s/it]

 77%|███████▋  | 12345/16110 [11:44:01<16:53:02, 16.14s/it]
{'loss': 0.1922, 'learning_rate': 1.9308177746981746e-06, 'rewards/chosen': -4.5159101486206055, 'rewards/rejected': -10.41860294342041, 'rewards/accuracies': 0.875, 'rewards/margins': 5.902692794799805, 'policy_logps/rejected': -345.00390625, 'policy_logps/chosen': -300.45526123046875, 'referece_logps/rejected': -240.81785583496094, 'referece_logps/chosen': -255.296142578125, 'logits/rejected': -0.2655254602432251, 'logits/chosen': -0.3310444951057434, 'epoch': 6.9}


 77%|███████▋  | 12347/16110 [11:44:43<19:18:52, 18.48s/it]
{'loss': 0.1499, 'learning_rate': 1.930670739178418e-06, 'rewards/chosen': -5.834046840667725, 'rewards/rejected': -10.595420837402344, 'rewards/accuracies': 1.0, 'rewards/margins': 4.7613749504089355, 'policy_logps/rejected': -539.651123046875, 'policy_logps/chosen': -399.22381591796875, 'referece_logps/rejected': -433.6969299316406, 'referece_logps/chosen': -340.88336181640625, 'logits/rejected': -0.9378306269645691, 'logits/chosen': -0.8410512208938599, 'epoch': 6.9}

 77%|███████▋  | 12348/16110 [11:45:03<19:40:13, 18.82s/it]


 77%|███████▋  | 12350/16110 [11:45:40<19:29:58, 18.67s/it]

 77%|███████▋  | 12351/16110 [11:45:58<19:10:35, 18.37s/it]
[2024-04-06 02:53:49,523] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1902, 'learning_rate': 1.9303762167420694e-06, 'rewards/chosen': -4.339093208312988, 'rewards/rejected': -8.523650169372559, 'rewards/accuracies': 0.875, 'rewards/margins': 4.184557914733887, 'policy_logps/rejected': -464.31951904296875, 'policy_logps/chosen': -447.41107177734375, 'referece_logps/rejected': -379.0830078125, 'referece_logps/chosen': -404.0201416015625, 'logits/rejected': 0.4710270166397095, 'logits/chosen': 0.4305613338947296, 'epoch': 6.9}

 77%|███████▋  | 12352/16110 [11:46:20<20:29:59, 19.64s/it]


 77%|███████▋  | 12354/16110 [11:46:59<20:29:33, 19.64s/it]
[2024-04-06 02:54:50,955] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1467, 'learning_rate': 1.9301549300363394e-06, 'rewards/chosen': -4.820957660675049, 'rewards/rejected': -10.749152183532715, 'rewards/accuracies': 1.0, 'rewards/margins': 5.928194046020508, 'policy_logps/rejected': -495.8524169921875, 'policy_logps/chosen': -534.7827758789062, 'referece_logps/rejected': -388.36090087890625, 'referece_logps/chosen': -486.5732116699219, 'logits/rejected': -0.07027082145214081, 'logits/chosen': -0.2778438329696655, 'epoch': 6.9}


 77%|███████▋  | 12356/16110 [11:47:38<20:20:46, 19.51s/it]
{'loss': 0.1658, 'learning_rate': 1.930007217573198e-06, 'rewards/chosen': -5.237038612365723, 'rewards/rejected': -9.001382827758789, 'rewards/accuracies': 1.0, 'rewards/margins': 3.764343023300171, 'policy_logps/rejected': -500.07269287109375, 'policy_logps/chosen': -451.63238525390625, 'referece_logps/rejected': -410.058837890625, 'referece_logps/chosen': -399.26202392578125, 'logits/rejected': 0.5916382074356079, 'logits/chosen': 0.39952099323272705, 'epoch': 6.9}


 77%|███████▋  | 12358/16110 [11:48:14<19:48:08, 19.00s/it]
{'loss': 0.2042, 'learning_rate': 1.9298593547437924e-06, 'rewards/chosen': -4.430990219116211, 'rewards/rejected': -7.931889533996582, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5008997917175293, 'policy_logps/rejected': -340.8302307128906, 'policy_logps/chosen': -395.535400390625, 'referece_logps/rejected': -261.51129150390625, 'referece_logps/chosen': -351.2254943847656, 'logits/rejected': 0.08182469010353088, 'logits/chosen': 0.0645495280623436, 'epoch': 6.9}


 77%|███████▋  | 12360/16110 [11:48:48<18:50:07, 18.08s/it]

 77%|███████▋  | 12361/16110 [11:49:08<19:20:18, 18.57s/it]
{'loss': 0.2713, 'learning_rate': 1.929637278615242e-06, 'rewards/chosen': -5.280064582824707, 'rewards/rejected': -6.842779159545898, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5627145767211914, 'policy_logps/rejected': -299.17181396484375, 'policy_logps/chosen': -365.5065612792969, 'referece_logps/rejected': -230.7440185546875, 'referece_logps/chosen': -312.7059631347656, 'logits/rejected': 0.06564047932624817, 'logits/chosen': 0.01586124300956726, 'epoch': 6.91}

 77%|███████▋  | 12362/16110 [11:49:29<20:09:00, 19.35s/it]

 77%|███████▋  | 12363/16110 [11:49:45<19:02:01, 18.29s/it]

 77%|███████▋  | 12364/16110 [11:50:05<19:29:38, 18.73s/it]


 77%|███████▋  | 12366/16110 [11:50:42<19:20:14, 18.59s/it]

 77%|███████▋  | 12367/16110 [11:51:02<19:52:56, 19.12s/it]
{'loss': 0.1142, 'learning_rate': 1.929192111870522e-06, 'rewards/chosen': -6.1926188468933105, 'rewards/rejected': -10.193336486816406, 'rewards/accuracies': 0.75, 'rewards/margins': 4.000718116760254, 'policy_logps/rejected': -436.3619384765625, 'policy_logps/chosen': -346.37213134765625, 'referece_logps/rejected': -334.4285583496094, 'referece_logps/chosen': -284.4459533691406, 'logits/rejected': -0.3314853310585022, 'logits/chosen': -0.359062135219574, 'epoch': 6.91}


 77%|███████▋  | 12369/16110 [11:51:31<17:13:49, 16.58s/it]

 77%|███████▋  | 12370/16110 [11:51:42<15:41:19, 15.10s/it]
{'loss': 0.1385, 'learning_rate': 1.9289690214162987e-06, 'rewards/chosen': -4.3519206047058105, 'rewards/rejected': -9.440193176269531, 'rewards/accuracies': 1.0, 'rewards/margins': 5.088273525238037, 'policy_logps/rejected': -322.7079162597656, 'policy_logps/chosen': -419.6298828125, 'referece_logps/rejected': -228.3059844970703, 'referece_logps/chosen': -376.11065673828125, 'logits/rejected': -0.08318611234426498, 'logits/chosen': -0.4074196219444275, 'epoch': 6.91}

 77%|███████▋  | 12371/16110 [11:51:57<15:41:33, 15.11s/it]

 77%|███████▋  | 12372/16110 [11:52:09<14:34:10, 14.03s/it]


 77%|███████▋  | 12374/16110 [11:52:48<17:39:18, 17.01s/it]

 77%|███████▋  | 12375/16110 [11:53:06<17:51:30, 17.21s/it]

 77%|███████▋  | 12376/16110 [11:53:24<18:12:46, 17.56s/it]

 77%|███████▋  | 12377/16110 [11:53:40<17:43:59, 17.10s/it]
{'loss': 0.1216, 'learning_rate': 1.928447162927586e-06, 'rewards/chosen': -4.2733588218688965, 'rewards/rejected': -9.615097045898438, 'rewards/accuracies': 1.0, 'rewards/margins': 5.341738700866699, 'policy_logps/rejected': -335.5938720703125, 'policy_logps/chosen': -382.822021484375, 'referece_logps/rejected': -239.44288635253906, 'referece_logps/chosen': -340.08843994140625, 'logits/rejected': 0.6057040691375732, 'logits/chosen': 0.6614211201667786, 'epoch': 6.91}


 77%|███████▋  | 12379/16110 [11:54:16<18:17:05, 17.64s/it]

 77%|███████▋  | 12380/16110 [11:54:37<19:07:08, 18.45s/it]

 77%|███████▋  | 12381/16110 [11:54:52<18:07:15, 17.49s/it]

 77%|███████▋  | 12382/16110 [11:55:04<16:37:25, 16.05s/it]
{'loss': 0.1598, 'learning_rate': 1.928073280948752e-06, 'rewards/chosen': -5.259912014007568, 'rewards/rejected': -9.041866302490234, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7819554805755615, 'policy_logps/rejected': -358.728515625, 'policy_logps/chosen': -424.2822265625, 'referece_logps/rejected': -268.3098449707031, 'referece_logps/chosen': -371.68310546875, 'logits/rejected': 0.2276492565870285, 'logits/chosen': 0.06090209633111954, 'epoch': 6.92}

 77%|███████▋  | 12383/16110 [11:55:27<18:35:48, 17.96s/it]


 77%|███████▋  | 12385/16110 [11:56:02<18:15:35, 17.65s/it]
{'loss': 0.1809, 'learning_rate': 1.927848501576525e-06, 'rewards/chosen': -5.175762176513672, 'rewards/rejected': -9.180813789367676, 'rewards/accuracies': 1.0, 'rewards/margins': 4.005052089691162, 'policy_logps/rejected': -388.010986328125, 'policy_logps/chosen': -411.980712890625, 'referece_logps/rejected': -296.20281982421875, 'referece_logps/chosen': -360.22308349609375, 'logits/rejected': -0.5235508680343628, 'logits/chosen': -0.7050334215164185, 'epoch': 6.92}

 77%|███████▋  | 12386/16110 [11:56:23<19:20:24, 18.70s/it]

 77%|███████▋  | 12387/16110 [11:56:45<20:22:36, 19.70s/it]

 77%|███████▋  | 12388/16110 [11:57:01<19:12:26, 18.58s/it]

 77%|███████▋  | 12389/16110 [11:57:14<17:20:35, 16.78s/it]

 77%|███████▋  | 12390/16110 [11:57:27<16:19:33, 15.80s/it]

 77%|███████▋  | 12391/16110 [11:57:48<17:44:21, 17.17s/it]

 77%|███████▋  | 12392/16110 [11:58:10<19:15:15, 18.64s/it]

 77%|███████▋  | 12393/16110 [11:58:26<18:38:09, 18.05s/it]

 77%|███████▋  | 12394/16110 [11:58:48<19:47:05, 19.17s/it]

 77%|███████▋  | 12395/16110 [11:59:06<19:31:36, 18.92s/it]


 77%|███████▋  | 12397/16110 [11:59:41<18:29:31, 17.93s/it]

 77%|███████▋  | 12398/16110 [12:00:03<19:46:43, 19.18s/it]
{'loss': 0.1876, 'learning_rate': 1.926870558234523e-06, 'rewards/chosen': -3.8038411140441895, 'rewards/rejected': -8.791271209716797, 'rewards/accuracies': 1.0, 'rewards/margins': 4.987429618835449, 'policy_logps/rejected': -413.83172607421875, 'policy_logps/chosen': -361.3435974121094, 'referece_logps/rejected': -325.9190368652344, 'referece_logps/chosen': -323.3051452636719, 'logits/rejected': 0.8240653872489929, 'logits/chosen': 0.7697070837020874, 'epoch': 6.93}

 77%|███████▋  | 12399/16110 [12:00:20<19:17:03, 18.71s/it]

 77%|███████▋  | 12400/16110 [12:00:40<19:30:18, 18.93s/it]

 77%|███████▋  | 12401/16110 [12:00:51<17:13:36, 16.72s/it]

 77%|███████▋  | 12402/16110 [12:01:10<17:45:15, 17.24s/it]


 77%|███████▋  | 12404/16110 [12:01:43<17:43:26, 17.22s/it]

 77%|███████▋  | 12405/16110 [12:02:03<18:28:18, 17.95s/it]
{'loss': 0.1721, 'learning_rate': 1.9263413503974697e-06, 'rewards/chosen': -4.957064151763916, 'rewards/rejected': -7.23271369934082, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2756505012512207, 'policy_logps/rejected': -342.2999572753906, 'policy_logps/chosen': -405.19940185546875, 'referece_logps/rejected': -269.9727783203125, 'referece_logps/chosen': -355.6287841796875, 'logits/rejected': 0.3247497081756592, 'logits/chosen': 0.0983160138130188, 'epoch': 6.93}

 77%|███████▋  | 12406/16110 [12:02:18<17:34:13, 17.08s/it]


 77%|███████▋  | 12408/16110 [12:02:57<18:52:56, 18.36s/it]
{'loss': 0.181, 'learning_rate': 1.9261139853267365e-06, 'rewards/chosen': -5.645702838897705, 'rewards/rejected': -9.531420707702637, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8857176303863525, 'policy_logps/rejected': -408.6833801269531, 'policy_logps/chosen': -349.72882080078125, 'referece_logps/rejected': -313.36920166015625, 'referece_logps/chosen': -293.27178955078125, 'logits/rejected': 0.3233284056186676, 'logits/chosen': 0.4260137677192688, 'epoch': 6.93}

 77%|███████▋  | 12409/16110 [12:03:14<18:35:49, 18.09s/it]

 77%|███████▋  | 12410/16110 [12:03:32<18:30:35, 18.01s/it]

 77%|███████▋  | 12411/16110 [12:03:52<19:00:41, 18.50s/it]


 77%|███████▋  | 12413/16110 [12:04:25<18:05:27, 17.62s/it]
{'loss': 0.1462, 'learning_rate': 1.925734294899119e-06, 'rewards/chosen': -4.22430419921875, 'rewards/rejected': -9.703009605407715, 'rewards/accuracies': 1.0, 'rewards/margins': 5.478706359863281, 'policy_logps/rejected': -367.1611328125, 'policy_logps/chosen': -344.596923828125, 'referece_logps/rejected': -270.1310119628906, 'referece_logps/chosen': -302.3538818359375, 'logits/rejected': 0.15888731181621552, 'logits/chosen': -0.1868607997894287, 'epoch': 6.93}

 77%|███████▋  | 12414/16110 [12:04:40<17:15:14, 16.81s/it]

 77%|███████▋  | 12415/16110 [12:04:58<17:27:46, 17.01s/it]

 77%|███████▋  | 12416/16110 [12:05:12<16:46:45, 16.35s/it]

 77%|███████▋  | 12417/16110 [12:05:30<17:11:01, 16.75s/it]

 77%|███████▋  | 12418/16110 [12:05:42<15:40:36, 15.29s/it]

 77%|███████▋  | 12419/16110 [12:06:00<16:34:04, 16.16s/it]

 77%|███████▋  | 12420/16110 [12:06:12<15:14:42, 14.87s/it]

 77%|███████▋  | 12421/16110 [12:06:26<15:03:46, 14.70s/it]

 77%|███████▋  | 12422/16110 [12:06:44<16:03:57, 15.68s/it]

 77%|███████▋  | 12423/16110 [12:07:03<16:58:59, 16.58s/it]

 77%|███████▋  | 12424/16110 [12:07:23<17:55:01, 17.50s/it]

 77%|███████▋  | 12425/16110 [12:07:44<19:06:13, 18.66s/it]

 77%|███████▋  | 12426/16110 [12:07:56<17:03:59, 16.68s/it]

 77%|███████▋  | 12427/16110 [12:08:10<16:09:14, 15.79s/it]


 77%|███████▋  | 12429/16110 [12:08:42<16:00:25, 15.65s/it]
{'loss': 0.0877, 'learning_rate': 1.924513001061e-06, 'rewards/chosen': -4.949758052825928, 'rewards/rejected': -11.853347778320312, 'rewards/accuracies': 1.0, 'rewards/margins': 6.903589725494385, 'policy_logps/rejected': -478.2880859375, 'policy_logps/chosen': -489.03289794921875, 'referece_logps/rejected': -359.754638671875, 'referece_logps/chosen': -439.53533935546875, 'logits/rejected': -0.15488798916339874, 'logits/chosen': -0.597831130027771, 'epoch': 6.94}


 77%|███████▋  | 12431/16110 [12:09:24<18:41:59, 18.30s/it]

 77%|███████▋  | 12432/16110 [12:09:41<18:31:51, 18.14s/it]
{'loss': 0.1493, 'learning_rate': 1.924282943054237e-06, 'rewards/chosen': -3.337111234664917, 'rewards/rejected': -8.838930130004883, 'rewards/accuracies': 1.0, 'rewards/margins': 5.501819133758545, 'policy_logps/rejected': -559.739013671875, 'policy_logps/chosen': -394.67657470703125, 'referece_logps/rejected': -471.34979248046875, 'referece_logps/chosen': -361.305419921875, 'logits/rejected': 0.3535528779029846, 'logits/chosen': 0.5670915842056274, 'epoch': 6.95}


 77%|███████▋  | 12434/16110 [12:10:22<19:33:29, 19.15s/it]
{'loss': 0.0953, 'learning_rate': 1.924129384243627e-06, 'rewards/chosen': -3.757363796234131, 'rewards/rejected': -9.683863639831543, 'rewards/accuracies': 1.0, 'rewards/margins': 5.926499366760254, 'policy_logps/rejected': -500.97698974609375, 'policy_logps/chosen': -609.2218017578125, 'referece_logps/rejected': -404.1383056640625, 'referece_logps/chosen': -571.648193359375, 'logits/rejected': 0.34645891189575195, 'logits/chosen': 0.24384543299674988, 'epoch': 6.95}

 77%|███████▋  | 12435/16110 [12:10:43<20:13:09, 19.81s/it]

 77%|███████▋  | 12436/16110 [12:11:05<20:52:35, 20.46s/it]

 77%|███████▋  | 12437/16110 [12:11:25<20:43:29, 20.31s/it]

 77%|███████▋  | 12438/16110 [12:11:42<19:44:57, 19.36s/it]

 77%|███████▋  | 12439/16110 [12:12:01<19:38:22, 19.26s/it]

 77%|███████▋  | 12440/16110 [12:12:20<19:34:28, 19.20s/it]

 77%|███████▋  | 12441/16110 [12:12:41<20:01:06, 19.64s/it]

 77%|███████▋  | 12442/16110 [12:12:59<19:35:37, 19.23s/it]

 77%|███████▋  | 12443/16110 [12:13:19<19:48:50, 19.45s/it]

 77%|███████▋  | 12444/16110 [12:13:38<19:45:16, 19.40s/it]

 77%|███████▋  | 12445/16110 [12:13:55<18:59:02, 18.65s/it]

 77%|███████▋  | 12446/16110 [12:14:15<19:17:01, 18.95s/it]

 77%|███████▋  | 12447/16110 [12:14:33<19:10:03, 18.84s/it]

 77%|███████▋  | 12448/16110 [12:14:49<18:12:13, 17.90s/it]


 77%|███████▋  | 12450/16110 [12:15:28<18:48:10, 18.49s/it]
{'loss': 0.1335, 'learning_rate': 1.922895536876269e-06, 'rewards/chosen': -4.457927703857422, 'rewards/rejected': -7.743730545043945, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2858033180236816, 'policy_logps/rejected': -326.60516357421875, 'policy_logps/chosen': -412.23388671875, 'referece_logps/rejected': -249.16787719726562, 'referece_logps/chosen': -367.6546325683594, 'logits/rejected': -0.014112979173660278, 'logits/chosen': 0.03294980525970459, 'epoch': 6.96}

 77%|███████▋  | 12451/16110 [12:15:47<19:08:36, 18.83s/it]

 77%|███████▋  | 12452/16110 [12:16:08<19:42:26, 19.39s/it]

 77%|███████▋  | 12453/16110 [12:16:27<19:24:32, 19.11s/it]

 77%|███████▋  | 12454/16110 [12:16:49<20:16:53, 19.97s/it]

 77%|███████▋  | 12455/16110 [12:17:05<19:09:05, 18.86s/it]

 77%|███████▋  | 12456/16110 [12:17:21<18:12:54, 17.95s/it]

 77%|███████▋  | 12457/16110 [12:17:39<18:15:57, 18.00s/it]

 77%|███████▋  | 12458/16110 [12:17:51<16:32:05, 16.30s/it]

 77%|███████▋  | 12459/16110 [12:18:13<18:13:21, 17.97s/it]


 77%|███████▋  | 12461/16110 [12:18:48<17:50:53, 17.61s/it]
{'loss': 0.2112, 'learning_rate': 1.922041726380573e-06, 'rewards/chosen': -4.44271183013916, 'rewards/rejected': -7.524715900421143, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0820040702819824, 'policy_logps/rejected': -355.15069580078125, 'policy_logps/chosen': -374.446044921875, 'referece_logps/rejected': -279.9035339355469, 'referece_logps/chosen': -330.0189514160156, 'logits/rejected': -0.3140111565589905, 'logits/chosen': -0.3286784887313843, 'epoch': 6.96}

 77%|███████▋  | 12462/16110 [12:19:07<18:15:55, 18.02s/it]

 77%|███████▋  | 12463/16110 [12:19:28<19:21:33, 19.11s/it]

 77%|███████▋  | 12464/16110 [12:19:51<20:22:54, 20.12s/it]


 77%|███████▋  | 12466/16110 [12:20:24<18:11:54, 17.98s/it]

 77%|███████▋  | 12467/16110 [12:20:42<18:15:46, 18.05s/it]
{'loss': 0.1712, 'learning_rate': 1.92157411054856e-06, 'rewards/chosen': -4.940250873565674, 'rewards/rejected': -11.744271278381348, 'rewards/accuracies': 0.875, 'rewards/margins': 6.804019927978516, 'policy_logps/rejected': -386.65802001953125, 'policy_logps/chosen': -457.1883239746094, 'referece_logps/rejected': -269.21533203125, 'referece_logps/chosen': -407.78582763671875, 'logits/rejected': 0.4583720564842224, 'logits/chosen': 0.2666305601596832, 'epoch': 6.96}

 77%|███████▋  | 12468/16110 [12:20:58<17:33:02, 17.35s/it]

 77%|███████▋  | 12469/16110 [12:21:17<18:07:53, 17.93s/it]

 77%|███████▋  | 12470/16110 [12:21:32<17:05:30, 16.90s/it]

 77%|███████▋  | 12471/16110 [12:21:52<17:59:05, 17.79s/it]

 77%|███████▋  | 12472/16110 [12:22:11<18:24:21, 18.21s/it]

 77%|███████▋  | 12473/16110 [12:22:27<17:54:10, 17.72s/it]

 77%|███████▋  | 12474/16110 [12:22:44<17:27:15, 17.28s/it]

 77%|███████▋  | 12475/16110 [12:23:00<17:05:11, 16.92s/it]

 77%|███████▋  | 12476/16110 [12:23:20<17:58:50, 17.81s/it]

 77%|███████▋  | 12477/16110 [12:23:41<19:01:39, 18.85s/it]

 77%|███████▋  | 12478/16110 [12:23:54<17:19:02, 17.16s/it]

 77%|███████▋  | 12479/16110 [12:24:10<16:53:34, 16.75s/it]

 77%|███████▋  | 12480/16110 [12:24:29<17:33:45, 17.42s/it]

 77%|███████▋  | 12481/16110 [12:24:41<15:53:24, 15.76s/it]


 77%|███████▋  | 12483/16110 [12:25:11<15:38:02, 15.52s/it]

 77%|███████▋  | 12484/16110 [12:25:31<17:05:03, 16.96s/it]
{'loss': 0.2074, 'learning_rate': 1.9202419187806925e-06, 'rewards/chosen': -4.655977249145508, 'rewards/rejected': -9.25369930267334, 'rewards/accuracies': 0.875, 'rewards/margins': 4.597722053527832, 'policy_logps/rejected': -446.193359375, 'policy_logps/chosen': -446.6256103515625, 'referece_logps/rejected': -353.6563720703125, 'referece_logps/chosen': -400.0658264160156, 'logits/rejected': 0.1573205590248108, 'logits/chosen': 0.0693727359175682, 'epoch': 6.97}


 78%|███████▊  | 12486/16110 [12:26:03<16:09:34, 16.05s/it]
{'loss': 0.1718, 'learning_rate': 1.920084483295824e-06, 'rewards/chosen': -3.726475954055786, 'rewards/rejected': -8.10430908203125, 'rewards/accuracies': 1.0, 'rewards/margins': 4.377833366394043, 'policy_logps/rejected': -305.107666015625, 'policy_logps/chosen': -449.60870361328125, 'referece_logps/rejected': -224.0645751953125, 'referece_logps/chosen': -412.34393310546875, 'logits/rejected': -0.6719973087310791, 'logits/chosen': -0.9827733635902405, 'epoch': 6.98}

 78%|███████▊  | 12487/16110 [12:26:17<15:48:22, 15.71s/it]

 78%|███████▊  | 12488/16110 [12:26:39<17:34:11, 17.46s/it]

 78%|███████▊  | 12489/16110 [12:26:56<17:25:43, 17.33s/it]

 78%|███████▊  | 12490/16110 [12:27:13<17:22:17, 17.28s/it]

 78%|███████▊  | 12491/16110 [12:27:29<17:01:34, 16.94s/it]

 78%|███████▊  | 12492/16110 [12:27:49<17:53:21, 17.80s/it]

 78%|███████▊  | 12493/16110 [12:28:05<17:14:04, 17.15s/it]

 78%|███████▊  | 12494/16110 [12:28:23<17:39:44, 17.58s/it]

 78%|███████▊  | 12495/16110 [12:28:43<18:19:18, 18.25s/it]

 78%|███████▊  | 12496/16110 [12:29:00<17:56:06, 17.87s/it]

 78%|███████▊  | 12497/16110 [12:29:20<18:26:57, 18.38s/it]

 78%|███████▊  | 12498/16110 [12:29:39<18:50:02, 18.77s/it]

 78%|███████▊  | 12499/16110 [12:30:01<19:32:28, 19.48s/it]

 78%|███████▊  | 12500/16110 [12:30:22<20:01:31, 19.97s/it]

 78%|███████▊  | 12501/16110 [12:30:56<24:13:30, 24.16s/it]

 78%|███████▊  | 12502/16110 [12:31:15<22:51:36, 22.81s/it]

 78%|███████▊  | 12503/16110 [12:31:34<21:44:23, 21.70s/it]

 78%|███████▊  | 12504/16110 [12:31:52<20:23:50, 20.36s/it]

 78%|███████▊  | 12505/16110 [12:32:12<20:17:38, 20.27s/it]

 78%|███████▊  | 12506/16110 [12:32:33<20:37:10, 20.60s/it]

 78%|███████▊  | 12507/16110 [12:32:53<20:22:14, 20.35s/it]

 78%|███████▊  | 12508/16110 [12:33:11<19:43:32, 19.71s/it]

 78%|███████▊  | 12509/16110 [12:33:32<20:14:54, 20.24s/it]


 78%|███████▊  | 12511/16110 [12:34:12<19:42:50, 19.72s/it]
{'loss': 0.2589, 'learning_rate': 1.918103996208487e-06, 'rewards/chosen': -3.6641457080841064, 'rewards/rejected': -7.028108596801758, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3639628887176514, 'policy_logps/rejected': -263.0697326660156, 'policy_logps/chosen': -464.5532531738281, 'referece_logps/rejected': -192.78863525390625, 'referece_logps/chosen': -427.91180419921875, 'logits/rejected': -0.511372447013855, 'logits/chosen': -0.7366153001785278, 'epoch': 6.99}

 78%|███████▊  | 12512/16110 [12:34:32<20:04:15, 20.08s/it]

 78%|███████▊  | 12513/16110 [12:34:50<19:19:26, 19.34s/it]

 78%|███████▊  | 12514/16110 [12:35:11<19:43:31, 19.75s/it]


 78%|███████▊  | 12516/16110 [12:35:52<20:04:46, 20.11s/it]
{'loss': 0.1446, 'learning_rate': 1.917705113903279e-06, 'rewards/chosen': -5.016724109649658, 'rewards/rejected': -8.038335800170898, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0216124057769775, 'policy_logps/rejected': -271.712890625, 'policy_logps/chosen': -208.9847869873047, 'referece_logps/rejected': -191.3295440673828, 'referece_logps/chosen': -158.8175506591797, 'logits/rejected': -0.2530190944671631, 'logits/chosen': -0.20547431707382202, 'epoch': 6.99}

 78%|███████▊  | 12517/16110 [12:36:10<19:43:54, 19.77s/it]

 78%|███████▊  | 12518/16110 [12:36:27<18:40:15, 18.71s/it]

 78%|███████▊  | 12519/16110 [12:36:45<18:33:33, 18.61s/it]


 78%|███████▊  | 12521/16110 [12:37:20<18:18:51, 18.37s/it]

 78%|███████▊  | 12522/16110 [12:37:36<17:39:30, 17.72s/it]
{'loss': 0.2364, 'learning_rate': 1.917225231060551e-06, 'rewards/chosen': -5.238654613494873, 'rewards/rejected': -9.075084686279297, 'rewards/accuracies': 1.0, 'rewards/margins': 3.836430549621582, 'policy_logps/rejected': -347.86505126953125, 'policy_logps/chosen': -363.5070495605469, 'referece_logps/rejected': -257.1142272949219, 'referece_logps/chosen': -311.1205139160156, 'logits/rejected': -0.47047433257102966, 'logits/chosen': -0.5696065425872803, 'epoch': 7.0}

 78%|███████▊  | 12523/16110 [12:37:58<19:05:54, 19.17s/it]

 78%|███████▊  | 12524/16110 [12:38:17<18:56:49, 19.02s/it]

 78%|███████▊  | 12525/16110 [12:38:37<19:12:07, 19.28s/it]

 78%|███████▊  | 12526/16110 [12:38:51<17:44:05, 17.81s/it]

 78%|███████▊  | 12527/16110 [12:39:08<17:30:25, 17.59s/it]

 78%|███████▊  | 12528/16110 [12:39:29<18:24:12, 18.50s/it]

 78%|███████▊  | 12529/16110 [12:39:46<17:52:21, 17.97s/it]

 78%|███████▊  | 12530/16110 [12:39:58<16:12:54, 16.31s/it]

 78%|███████▊  | 12531/16110 [12:40:17<17:05:31, 17.19s/it]

 78%|███████▊  | 12532/16110 [12:40:36<17:38:09, 17.74s/it]

 78%|███████▊  | 12533/16110 [12:40:52<16:58:43, 17.09s/it]

 78%|███████▊  | 12534/16110 [12:41:08<16:38:47, 16.76s/it]

 78%|███████▊  | 12535/16110 [12:41:21<15:25:38, 15.54s/it]

 78%|███████▊  | 12536/16110 [12:41:32<14:08:22, 14.24s/it]

 78%|███████▊  | 12537/16110 [12:41:50<15:19:47, 15.45s/it]

 78%|███████▊  | 12538/16110 [12:42:04<14:54:58, 15.03s/it]

 78%|███████▊  | 12539/16110 [12:42:24<16:13:25, 16.36s/it]

 78%|███████▊  | 12540/16110 [12:42:37<15:28:30, 15.61s/it]

 78%|███████▊  | 12541/16110 [12:42:52<15:13:21, 15.35s/it]

 78%|███████▊  | 12542/16110 [12:43:06<14:36:46, 14.74s/it]

 78%|███████▊  | 12543/16110 [12:43:21<14:52:46, 15.02s/it]

 78%|███████▊  | 12544/16110 [12:43:33<13:53:27, 14.02s/it]

 78%|███████▊  | 12545/16110 [12:43:44<12:57:55, 13.09s/it]

 78%|███████▊  | 12546/16110 [12:44:02<14:31:48, 14.68s/it]

 78%|███████▊  | 12547/16110 [12:44:21<15:44:13, 15.90s/it]

 78%|███████▊  | 12548/16110 [12:44:37<15:52:29, 16.04s/it]

 78%|███████▊  | 12549/16110 [12:44:51<15:17:42, 15.46s/it]

 78%|███████▊  | 12550/16110 [12:45:10<16:05:52, 16.28s/it]

 78%|███████▊  | 12551/16110 [12:45:29<17:06:13, 17.30s/it]

 78%|███████▊  | 12552/16110 [12:45:49<17:46:33, 17.99s/it]

 78%|███████▊  | 12553/16110 [12:46:05<17:05:07, 17.29s/it]

 78%|███████▊  | 12554/16110 [12:46:22<17:01:28, 17.24s/it]

 78%|███████▊  | 12555/16110 [12:46:35<15:54:46, 16.11s/it]

 78%|███████▊  | 12556/16110 [12:46:54<16:35:33, 16.81s/it]

 78%|███████▊  | 12557/16110 [12:47:06<15:22:45, 15.58s/it]

 78%|███████▊  | 12558/16110 [12:47:23<15:51:02, 16.06s/it]

 78%|███████▊  | 12559/16110 [12:47:41<16:10:03, 16.39s/it]

 78%|███████▊  | 12560/16110 [12:48:02<17:41:04, 17.93s/it]

 78%|███████▊  | 12561/16110 [12:48:19<17:20:53, 17.60s/it]

 78%|███████▊  | 12562/16110 [12:48:33<16:16:48, 16.52s/it]

 78%|███████▊  | 12563/16110 [12:48:53<17:15:50, 17.52s/it]

 78%|███████▊  | 12564/16110 [12:49:10<17:14:01, 17.50s/it]

 78%|███████▊  | 12565/16110 [12:49:29<17:26:35, 17.71s/it]


 78%|███████▊  | 12567/16110 [12:50:02<16:49:44, 17.10s/it]

 78%|███████▊  | 12568/16110 [12:50:23<17:56:59, 18.24s/it]

 78%|███████▊  | 12569/16110 [12:50:43<18:23:32, 18.70s/it]

 78%|███████▊  | 12570/16110 [12:51:02<18:24:28, 18.72s/it]

 78%|███████▊  | 12571/16110 [12:51:20<18:19:55, 18.65s/it]

 78%|███████▊  | 12572/16110 [12:51:41<19:03:01, 19.38s/it]

 78%|███████▊  | 12573/16110 [12:52:01<19:09:23, 19.50s/it]

 78%|███████▊  | 12574/16110 [12:52:18<18:28:17, 18.81s/it]

 78%|███████▊  | 12575/16110 [12:52:41<19:32:03, 19.89s/it]

 78%|███████▊  | 12576/16110 [12:52:59<19:03:07, 19.41s/it]

 78%|███████▊  | 12577/16110 [12:53:19<19:12:39, 19.58s/it]

 78%|███████▊  | 12578/16110 [12:53:36<18:23:49, 18.75s/it]

 78%|███████▊  | 12579/16110 [12:53:47<16:02:14, 16.35s/it]

 78%|███████▊  | 12580/16110 [12:53:58<14:26:17, 14.72s/it]

 78%|███████▊  | 12581/16110 [12:54:11<14:07:48, 14.41s/it]

 78%|███████▊  | 12582/16110 [12:54:34<16:30:45, 16.85s/it]

 78%|███████▊  | 12583/16110 [12:54:45<14:48:04, 15.11s/it]

 78%|███████▊  | 12584/16110 [12:54:56<13:31:16, 13.81s/it]

 78%|███████▊  | 12585/16110 [12:55:18<16:00:08, 16.34s/it]

 78%|███████▊  | 12586/16110 [12:55:38<16:57:54, 17.33s/it]

 78%|███████▊  | 12587/16110 [12:55:59<18:01:24, 18.42s/it]

 78%|███████▊  | 12588/16110 [12:56:11<16:17:55, 16.66s/it]

 78%|███████▊  | 12589/16110 [12:56:26<15:43:17, 16.07s/it]

 78%|███████▊  | 12590/16110 [12:56:40<15:11:35, 15.54s/it]

 78%|███████▊  | 12591/16110 [12:56:55<14:57:01, 15.29s/it]

 78%|███████▊  | 12592/16110 [12:57:13<15:43:45, 16.10s/it]

 78%|███████▊  | 12593/16110 [12:57:30<16:07:02, 16.50s/it]

 78%|███████▊  | 12594/16110 [12:57:46<15:50:01, 16.21s/it]

 78%|███████▊  | 12595/16110 [12:58:07<17:27:06, 17.87s/it]

 78%|███████▊  | 12596/16110 [12:58:30<18:42:41, 19.17s/it]

 78%|███████▊  | 12597/16110 [12:58:43<17:02:28, 17.46s/it]

 78%|███████▊  | 12598/16110 [12:59:05<18:20:58, 18.81s/it]

 78%|███████▊  | 12599/16110 [12:59:25<18:35:48, 19.07s/it]

 78%|███████▊  | 12600/16110 [12:59:41<17:39:08, 18.11s/it]

 78%|███████▊  | 12601/16110 [13:00:03<18:57:21, 19.45s/it]

 78%|███████▊  | 12602/16110 [13:00:25<19:40:56, 20.20s/it]

 78%|███████▊  | 12603/16110 [13:00:45<19:40:24, 20.20s/it]

 78%|███████▊  | 12604/16110 [13:01:05<19:32:37, 20.07s/it]

 78%|███████▊  | 12605/16110 [13:01:23<18:56:24, 19.45s/it]

 78%|███████▊  | 12606/16110 [13:01:39<17:57:50, 18.46s/it]

 78%|███████▊  | 12607/16110 [13:01:57<17:48:54, 18.31s/it]

 78%|███████▊  | 12608/16110 [13:02:17<18:11:35, 18.70s/it]

 78%|███████▊  | 12609/16110 [13:02:30<16:36:46, 17.08s/it]

 78%|███████▊  | 12610/16110 [13:02:50<17:26:12, 17.93s/it]

 78%|███████▊  | 12611/16110 [13:03:06<16:51:41, 17.35s/it]

 78%|███████▊  | 12612/16110 [13:03:26<17:39:43, 18.18s/it]

 78%|███████▊  | 12613/16110 [13:03:45<17:49:20, 18.35s/it]

 78%|███████▊  | 12614/16110 [13:04:07<18:54:49, 19.48s/it]

 78%|███████▊  | 12615/16110 [13:04:26<18:53:48, 19.46s/it]

 78%|███████▊  | 12616/16110 [13:04:46<18:56:36, 19.52s/it]

 78%|███████▊  | 12617/16110 [13:05:05<18:37:53, 19.20s/it]

 78%|███████▊  | 12618/16110 [13:05:17<16:37:45, 17.14s/it]

 78%|███████▊  | 12619/16110 [13:05:32<16:01:24, 16.52s/it]

 78%|███████▊  | 12620/16110 [13:05:50<16:30:07, 17.02s/it]

 78%|███████▊  | 12621/16110 [13:06:10<17:25:21, 17.98s/it]

 78%|███████▊  | 12622/16110 [13:06:32<18:21:27, 18.95s/it]

 78%|███████▊  | 12623/16110 [13:06:46<17:05:55, 17.65s/it]

 78%|███████▊  | 12624/16110 [13:07:08<18:10:24, 18.77s/it]

 78%|███████▊  | 12625/16110 [13:07:27<18:29:41, 19.11s/it]
{'loss': 0.1996, 'learning_rate': 1.9087797168651086e-06, 'rewards/chosen': -4.987004280090332, 'rewards/rejected': -12.197965621948242, 'rewards/accuracies': 1.0, 'rewards/margins': 7.2109599113464355, 'policy_logps/rejected': -496.13800048828125, 'policy_logps/chosen': -409.7257080078125, 'referece_logps/rejected': -374.1584167480469, 'referece_logps/chosen': -359.8556823730469, 'logits/rejected': -0.21596726775169373, 'logits/chosen': -0.1266176849603653, 'epoch': 7.05}


 78%|███████▊  | 12627/16110 [13:07:55<15:50:23, 16.37s/it]
{'loss': 0.1161, 'learning_rate': 1.908611857425194e-06, 'rewards/chosen': -6.063278675079346, 'rewards/rejected': -10.6023588180542, 'rewards/accuracies': 1.0, 'rewards/margins': 4.539079666137695, 'policy_logps/rejected': -467.61553955078125, 'policy_logps/chosen': -572.778076171875, 'referece_logps/rejected': -361.5918884277344, 'referece_logps/chosen': -512.145263671875, 'logits/rejected': 0.4465722441673279, 'logits/chosen': 0.12458661943674088, 'epoch': 7.05}


 78%|███████▊  | 12629/16110 [13:08:28<16:03:36, 16.61s/it]

 78%|███████▊  | 12630/16110 [13:08:43<15:40:58, 16.22s/it]

 78%|███████▊  | 12631/16110 [13:09:03<16:45:26, 17.34s/it]

 78%|███████▊  | 12632/16110 [13:09:25<18:06:04, 18.74s/it]
{'loss': 0.157, 'learning_rate': 1.9081915661667095e-06, 'rewards/chosen': -4.772490978240967, 'rewards/rejected': -10.789836883544922, 'rewards/accuracies': 0.875, 'rewards/margins': 6.017346382141113, 'policy_logps/rejected': -387.9917297363281, 'policy_logps/chosen': -409.38702392578125, 'referece_logps/rejected': -280.0933837890625, 'referece_logps/chosen': -361.6620788574219, 'logits/rejected': 0.39789897203445435, 'logits/chosen': 0.22767625749111176, 'epoch': 7.06}


 78%|███████▊  | 12634/16110 [13:10:05<18:44:42, 19.41s/it]

 78%|███████▊  | 12635/16110 [13:10:19<17:04:14, 17.68s/it]

 78%|███████▊  | 12636/16110 [13:10:39<17:41:26, 18.33s/it]

 78%|███████▊  | 12637/16110 [13:11:00<18:22:49, 19.05s/it]

 78%|███████▊  | 12638/16110 [13:11:21<19:02:34, 19.74s/it]

 78%|███████▊  | 12639/16110 [13:11:33<16:51:43, 17.49s/it]

 78%|███████▊  | 12640/16110 [13:11:53<17:33:25, 18.21s/it]

 78%|███████▊  | 12641/16110 [13:12:13<17:57:49, 18.64s/it]

 78%|███████▊  | 12642/16110 [13:12:31<17:53:32, 18.57s/it]

 78%|███████▊  | 12643/16110 [13:12:47<16:57:26, 17.61s/it]

 78%|███████▊  | 12644/16110 [13:13:02<16:11:38, 16.82s/it]
{'loss': 0.1463, 'learning_rate': 1.9071791235615215e-06, 'rewards/chosen': -4.3579020500183105, 'rewards/rejected': -8.657585144042969, 'rewards/accuracies': 1.0, 'rewards/margins': 4.2996826171875, 'policy_logps/rejected': -253.38623046875, 'policy_logps/chosen': -358.0513916015625, 'referece_logps/rejected': -166.81039428710938, 'referece_logps/chosen': -314.4723815917969, 'logits/rejected': 0.32079601287841797, 'logits/chosen': 0.17033764719963074, 'epoch': 7.06}


 78%|███████▊  | 12646/16110 [13:13:26<13:57:15, 14.50s/it]
{'loss': 0.1419, 'learning_rate': 1.9070098696042373e-06, 'rewards/chosen': -5.505524635314941, 'rewards/rejected': -10.971120834350586, 'rewards/accuracies': 0.875, 'rewards/margins': 5.4655961990356445, 'policy_logps/rejected': -432.863037109375, 'policy_logps/chosen': -486.4743957519531, 'referece_logps/rejected': -323.15185546875, 'referece_logps/chosen': -431.419189453125, 'logits/rejected': -0.20580117404460907, 'logits/chosen': -0.39975079894065857, 'epoch': 7.06}


 79%|███████▊  | 12648/16110 [13:14:10<17:41:58, 18.41s/it]

 79%|███████▊  | 12649/16110 [13:14:30<18:11:09, 18.92s/it]

 79%|███████▊  | 12650/16110 [13:14:52<18:59:00, 19.75s/it]

 79%|███████▊  | 12651/16110 [13:15:12<19:01:50, 19.81s/it]

 79%|███████▊  | 12652/16110 [13:15:30<18:37:56, 19.40s/it]

 79%|███████▊  | 12653/16110 [13:15:50<18:45:19, 19.53s/it]

 79%|███████▊  | 12654/16110 [13:16:09<18:26:57, 19.22s/it]

 79%|███████▊  | 12655/16110 [13:16:28<18:34:25, 19.35s/it]

 79%|███████▊  | 12656/16110 [13:16:45<17:43:16, 18.47s/it]

 79%|███████▊  | 12657/16110 [13:17:03<17:37:16, 18.37s/it]

 79%|███████▊  | 12658/16110 [13:17:23<18:05:09, 18.86s/it]

 79%|███████▊  | 12659/16110 [13:17:44<18:41:41, 19.50s/it]

 79%|███████▊  | 12660/16110 [13:18:06<19:26:22, 20.28s/it]

 79%|███████▊  | 12661/16110 [13:18:21<17:53:24, 18.67s/it]

 79%|███████▊  | 12662/16110 [13:18:38<17:34:49, 18.36s/it]

 79%|███████▊  | 12663/16110 [13:18:58<17:46:15, 18.56s/it]

 79%|███████▊  | 12664/16110 [13:19:16<17:36:51, 18.40s/it]

 79%|███████▊  | 12665/16110 [13:19:34<17:36:29, 18.40s/it]

 79%|███████▊  | 12666/16110 [13:19:54<18:00:23, 18.82s/it]

 79%|███████▊  | 12667/16110 [13:20:10<17:09:34, 17.94s/it]

 79%|███████▊  | 12668/16110 [13:20:29<17:41:36, 18.51s/it]
{'loss': 0.1334, 'learning_rate': 1.9051384033443664e-06, 'rewards/chosen': -3.5352461338043213, 'rewards/rejected': -7.441973686218262, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9067275524139404, 'policy_logps/rejected': -375.5975646972656, 'policy_logps/chosen': -287.6468811035156, 'referece_logps/rejected': -301.1778564453125, 'referece_logps/chosen': -252.29440307617188, 'logits/rejected': 0.1565963625907898, 'logits/chosen': 0.13058243691921234, 'epoch': 7.08}


 79%|███████▊  | 12670/16110 [13:21:10<18:40:40, 19.55s/it]

 79%|███████▊  | 12671/16110 [13:21:27<17:58:28, 18.82s/it]

 79%|███████▊  | 12672/16110 [13:21:41<16:36:02, 17.38s/it]

 79%|███████▊  | 12673/16110 [13:21:53<15:01:03, 15.73s/it]

 79%|███████▊  | 12674/16110 [13:22:06<14:06:49, 14.79s/it]
{'loss': 0.1713, 'learning_rate': 1.9046249286581206e-06, 'rewards/chosen': -4.7061991691589355, 'rewards/rejected': -8.232938766479492, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5267391204833984, 'policy_logps/rejected': -557.9262084960938, 'policy_logps/chosen': -625.9417114257812, 'referece_logps/rejected': -475.5967712402344, 'referece_logps/chosen': -578.8797607421875, 'logits/rejected': -0.24881988763809204, 'logits/chosen': -0.32303082942962646, 'epoch': 7.08}


 79%|███████▊  | 12676/16110 [13:22:34<13:43:40, 14.39s/it]

 79%|███████▊  | 12677/16110 [13:22:52<14:39:38, 15.37s/it]

 79%|███████▊  | 12678/16110 [13:23:14<16:38:13, 17.45s/it]

 79%|███████▊  | 12679/16110 [13:23:34<17:15:35, 18.11s/it]

 79%|███████▊  | 12680/16110 [13:23:49<16:32:20, 17.36s/it]

 79%|███████▊  | 12681/16110 [13:24:03<15:18:34, 16.07s/it]

 79%|███████▊  | 12682/16110 [13:24:22<16:20:25, 17.16s/it]

 79%|███████▊  | 12683/16110 [13:24:40<16:35:06, 17.42s/it]

 79%|███████▊  | 12684/16110 [13:25:00<17:14:04, 18.11s/it]

 79%|███████▊  | 12685/16110 [13:25:18<17:17:10, 18.17s/it]
{'loss': 0.1812, 'learning_rate': 1.9036801400575686e-06, 'rewards/chosen': -3.6130404472351074, 'rewards/rejected': -9.284363746643066, 'rewards/accuracies': 1.0, 'rewards/margins': 5.671323299407959, 'policy_logps/rejected': -432.5290832519531, 'policy_logps/chosen': -461.96368408203125, 'referece_logps/rejected': -339.6854553222656, 'referece_logps/chosen': -425.8332824707031, 'logits/rejected': -0.4395281672477722, 'logits/chosen': -0.5540522336959839, 'epoch': 7.09}


 79%|███████▉  | 12687/16110 [13:25:57<17:53:46, 18.82s/it]
{'loss': 0.1638, 'learning_rate': 1.9035078853201734e-06, 'rewards/chosen': -4.584770202636719, 'rewards/rejected': -10.392813682556152, 'rewards/accuracies': 0.875, 'rewards/margins': 5.808043956756592, 'policy_logps/rejected': -465.0766906738281, 'policy_logps/chosen': -411.935546875, 'referece_logps/rejected': -361.1485290527344, 'referece_logps/chosen': -366.0877990722656, 'logits/rejected': -0.31954479217529297, 'logits/chosen': -0.2743394374847412, 'epoch': 7.09}


 79%|███████▉  | 12689/16110 [13:26:29<16:50:32, 17.72s/it]

 79%|███████▉  | 12690/16110 [13:26:44<16:02:49, 16.89s/it]
{'loss': 0.1758, 'learning_rate': 1.9032492293194617e-06, 'rewards/chosen': -4.94455099105835, 'rewards/rejected': -7.588492393493652, 'rewards/accuracies': 0.625, 'rewards/margins': 2.6439414024353027, 'policy_logps/rejected': -299.73065185546875, 'policy_logps/chosen': -488.8445129394531, 'referece_logps/rejected': -223.84573364257812, 'referece_logps/chosen': -439.39898681640625, 'logits/rejected': 0.3519788086414337, 'logits/chosen': 0.08016851544380188, 'epoch': 7.09}


 79%|███████▉  | 12692/16110 [13:27:21<16:38:01, 17.52s/it]

 79%|███████▉  | 12693/16110 [13:27:33<15:06:54, 15.92s/it]

 79%|███████▉  | 12694/16110 [13:27:44<13:39:38, 14.40s/it]

 79%|███████▉  | 12695/16110 [13:28:00<14:14:27, 15.01s/it]

 79%|███████▉  | 12696/16110 [13:28:17<14:41:28, 15.49s/it]

 79%|███████▉  | 12697/16110 [13:28:34<15:14:59, 16.09s/it]

 79%|███████▉  | 12698/16110 [13:28:51<15:26:55, 16.30s/it]

 79%|███████▉  | 12699/16110 [13:29:06<15:06:02, 15.94s/it]

 79%|███████▉  | 12700/16110 [13:29:18<14:03:05, 14.83s/it]

 79%|███████▉  | 12701/16110 [13:29:30<13:17:57, 14.04s/it]

 79%|███████▉  | 12702/16110 [13:29:46<13:44:43, 14.52s/it]

 79%|███████▉  | 12703/16110 [13:29:57<12:39:44, 13.38s/it]
{'loss': 0.1738, 'learning_rate': 1.902124590821514e-06, 'rewards/chosen': -4.051197528839111, 'rewards/rejected': -8.543947219848633, 'rewards/accuracies': 1.0, 'rewards/margins': 4.49275016784668, 'policy_logps/rejected': -287.4236755371094, 'policy_logps/chosen': -325.27703857421875, 'referece_logps/rejected': -201.9842071533203, 'referece_logps/chosen': -284.7650451660156, 'logits/rejected': 0.035802215337753296, 'logits/chosen': -0.11586727946996689, 'epoch': 7.1}

 79%|███████▉  | 12704/16110 [13:30:17<14:43:04, 15.56s/it]


 79%|███████▉  | 12706/16110 [13:30:53<16:01:18, 16.94s/it]

 79%|███████▉  | 12707/16110 [13:31:14<17:13:41, 18.23s/it]

 79%|███████▉  | 12708/16110 [13:31:27<15:40:16, 16.58s/it]

 79%|███████▉  | 12709/16110 [13:31:47<16:34:20, 17.54s/it]

 79%|███████▉  | 12710/16110 [13:32:04<16:25:03, 17.38s/it]

 79%|███████▉  | 12711/16110 [13:32:25<17:20:51, 18.37s/it]

 79%|███████▉  | 12712/16110 [13:32:37<15:37:47, 16.56s/it]

 79%|███████▉  | 12713/16110 [13:32:57<16:29:57, 17.49s/it]
{'loss': 0.1502, 'learning_rate': 1.9012552904623444e-06, 'rewards/chosen': -3.7962348461151123, 'rewards/rejected': -9.45079517364502, 'rewards/accuracies': 1.0, 'rewards/margins': 5.654560089111328, 'policy_logps/rejected': -376.2945861816406, 'policy_logps/chosen': -392.6120910644531, 'referece_logps/rejected': -281.7866516113281, 'referece_logps/chosen': -354.6497802734375, 'logits/rejected': 0.056441910564899445, 'logits/chosen': 0.05032965540885925, 'epoch': 7.1}

 79%|███████▉  | 12714/16110 [13:33:10<15:14:41, 16.16s/it]

 79%|███████▉  | 12715/16110 [13:33:28<15:47:59, 16.75s/it]

 79%|███████▉  | 12716/16110 [13:33:46<16:14:35, 17.23s/it]

 79%|███████▉  | 12717/16110 [13:34:06<16:53:46, 17.93s/it]


 79%|███████▉  | 12719/16110 [13:34:37<15:16:04, 16.21s/it]

 79%|███████▉  | 12720/16110 [13:34:53<15:23:30, 16.35s/it]
{'loss': 0.0536, 'learning_rate': 1.9006446124531236e-06, 'rewards/chosen': -4.391341209411621, 'rewards/rejected': -8.13963794708252, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7482967376708984, 'policy_logps/rejected': -574.8733520507812, 'policy_logps/chosen': -526.53076171875, 'referece_logps/rejected': -493.4769592285156, 'referece_logps/chosen': -482.617431640625, 'logits/rejected': 0.7608110904693604, 'logits/chosen': 0.7639686465263367, 'epoch': 7.11}

 79%|███████▉  | 12721/16110 [13:35:04<13:49:07, 14.68s/it]


 79%|███████▉  | 12723/16110 [13:35:35<14:33:00, 15.47s/it]

 79%|███████▉  | 12724/16110 [13:35:56<15:57:46, 16.97s/it]

 79%|███████▉  | 12725/16110 [13:36:12<15:38:51, 16.64s/it]

 79%|███████▉  | 12726/16110 [13:36:25<14:38:42, 15.58s/it]

 79%|███████▉  | 12727/16110 [13:36:44<15:37:05, 16.62s/it]
{'loss': 0.1541, 'learning_rate': 1.9000321506134678e-06, 'rewards/chosen': -3.8468432426452637, 'rewards/rejected': -9.283525466918945, 'rewards/accuracies': 1.0, 'rewards/margins': 5.436683177947998, 'policy_logps/rejected': -675.5821533203125, 'policy_logps/chosen': -455.7040710449219, 'referece_logps/rejected': -582.7469482421875, 'referece_logps/chosen': -417.23565673828125, 'logits/rejected': -0.7187469005584717, 'logits/chosen': -0.48769551515579224, 'epoch': 7.11}
[2024-04-06 04:44:55,806] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 12729/16110 [13:37:21<16:18:51, 17.37s/it]
[2024-04-06 04:45:12,188] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 12730/16110 [13:37:42<17:20:03, 18.46s/it]

 79%|███████▉  | 12731/16110 [13:38:00<17:15:31, 18.39s/it]

 79%|███████▉  | 12732/16110 [13:38:14<15:57:51, 17.01s/it]
{'loss': 0.0889, 'learning_rate': 1.8995935864015874e-06, 'rewards/chosen': -4.772235870361328, 'rewards/rejected': -9.845626831054688, 'rewards/accuracies': 1.0, 'rewards/margins': 5.073390007019043, 'policy_logps/rejected': -396.8839416503906, 'policy_logps/chosen': -360.5987854003906, 'referece_logps/rejected': -298.42767333984375, 'referece_logps/chosen': -312.8764343261719, 'logits/rejected': -0.21271169185638428, 'logits/chosen': -0.20581091940402985, 'epoch': 7.11}


 79%|███████▉  | 12734/16110 [13:38:50<16:35:56, 17.70s/it]

 79%|███████▉  | 12735/16110 [13:39:09<17:07:25, 18.27s/it]

 79%|███████▉  | 12736/16110 [13:39:29<17:33:34, 18.74s/it]

 79%|███████▉  | 12737/16110 [13:39:49<17:47:08, 18.98s/it]

 79%|███████▉  | 12738/16110 [13:40:05<16:56:09, 18.08s/it]

 79%|███████▉  | 12739/16110 [13:40:18<15:31:29, 16.58s/it]
{'loss': 0.0996, 'learning_rate': 1.8989780693910585e-06, 'rewards/chosen': -4.277508735656738, 'rewards/rejected': -9.173972129821777, 'rewards/accuracies': 1.0, 'rewards/margins': 4.896462440490723, 'policy_logps/rejected': -415.7703552246094, 'policy_logps/chosen': -312.45086669921875, 'referece_logps/rejected': -324.0306091308594, 'referece_logps/chosen': -269.67578125, 'logits/rejected': -0.010324269533157349, 'logits/chosen': -0.11151736229658127, 'epoch': 7.12}


 79%|███████▉  | 12741/16110 [13:40:58<17:23:17, 18.58s/it]
{'loss': 0.1366, 'learning_rate': 1.8988018802985897e-06, 'rewards/chosen': -4.34987735748291, 'rewards/rejected': -8.817116737365723, 'rewards/accuracies': 0.875, 'rewards/margins': 4.467238426208496, 'policy_logps/rejected': -377.46661376953125, 'policy_logps/chosen': -310.8203430175781, 'referece_logps/rejected': -289.2954406738281, 'referece_logps/chosen': -267.321533203125, 'logits/rejected': -0.4510168433189392, 'logits/chosen': -0.583814263343811, 'epoch': 7.12}


 79%|███████▉  | 12743/16110 [13:41:32<16:42:24, 17.86s/it]
{'loss': 0.1145, 'learning_rate': 1.8986255458852258e-06, 'rewards/chosen': -5.124416351318359, 'rewards/rejected': -10.916348457336426, 'rewards/accuracies': 0.875, 'rewards/margins': 5.791932106018066, 'policy_logps/rejected': -575.350830078125, 'policy_logps/chosen': -489.47003173828125, 'referece_logps/rejected': -466.1873474121094, 'referece_logps/chosen': -438.2258605957031, 'logits/rejected': -0.08226606249809265, 'logits/chosen': -0.15349501371383667, 'epoch': 7.12}

 79%|███████▉  | 12744/16110 [13:41:48<16:21:36, 17.50s/it]

 79%|███████▉  | 12745/16110 [13:42:08<17:03:25, 18.25s/it]


 79%|███████▉  | 12747/16110 [13:42:42<16:19:43, 17.48s/it]
{'loss': 0.1426, 'learning_rate': 1.898272441209879e-06, 'rewards/chosen': -5.063088417053223, 'rewards/rejected': -10.901833534240723, 'rewards/accuracies': 0.875, 'rewards/margins': 5.8387451171875, 'policy_logps/rejected': -394.87628173828125, 'policy_logps/chosen': -384.3052978515625, 'referece_logps/rejected': -285.8579406738281, 'referece_logps/chosen': -333.67437744140625, 'logits/rejected': 0.3589681386947632, 'logits/chosen': 0.47885751724243164, 'epoch': 7.12}


 79%|███████▉  | 12749/16110 [13:43:12<15:16:31, 16.36s/it]

 79%|███████▉  | 12750/16110 [13:43:32<16:26:56, 17.62s/it]
{'loss': 0.1059, 'learning_rate': 1.8980072314482358e-06, 'rewards/chosen': -5.699182033538818, 'rewards/rejected': -11.656320571899414, 'rewards/accuracies': 1.0, 'rewards/margins': 5.95713996887207, 'policy_logps/rejected': -523.2134399414062, 'policy_logps/chosen': -656.2220458984375, 'referece_logps/rejected': -406.6501770019531, 'referece_logps/chosen': -599.2302856445312, 'logits/rejected': -0.05452708899974823, 'logits/chosen': -0.2522122859954834, 'epoch': 7.12}


 79%|███████▉  | 12752/16110 [13:44:06<16:18:29, 17.48s/it]

 79%|███████▉  | 12753/16110 [13:44:26<16:55:02, 18.14s/it]

 79%|███████▉  | 12754/16110 [13:44:39<15:36:33, 16.74s/it]
{'loss': 0.2467, 'learning_rate': 1.8976531102759655e-06, 'rewards/chosen': -4.067400932312012, 'rewards/rejected': -6.1941375732421875, 'rewards/accuracies': 0.875, 'rewards/margins': 2.126736879348755, 'policy_logps/rejected': -384.728515625, 'policy_logps/chosen': -529.6410522460938, 'referece_logps/rejected': -322.787109375, 'referece_logps/chosen': -488.967041015625, 'logits/rejected': 1.0137993097305298, 'logits/chosen': 0.6510255336761475, 'epoch': 7.13}


 79%|███████▉  | 12756/16110 [13:45:19<17:10:39, 18.44s/it]
{'loss': 0.1282, 'learning_rate': 1.8974758319727728e-06, 'rewards/chosen': -3.1400389671325684, 'rewards/rejected': -7.512513160705566, 'rewards/accuracies': 1.0, 'rewards/margins': 4.372474193572998, 'policy_logps/rejected': -392.97967529296875, 'policy_logps/chosen': -338.7467041015625, 'referece_logps/rejected': -317.85455322265625, 'referece_logps/chosen': -307.3463439941406, 'logits/rejected': -0.30549126863479614, 'logits/chosen': -0.45205122232437134, 'epoch': 7.13}

 79%|███████▉  | 12757/16110 [13:45:40<17:56:44, 19.27s/it]


 79%|███████▉  | 12759/16110 [13:46:16<17:29:52, 18.80s/it]
{'loss': 0.2207, 'learning_rate': 1.897209642452269e-06, 'rewards/chosen': -4.653818130493164, 'rewards/rejected': -6.570396423339844, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9165782928466797, 'policy_logps/rejected': -413.4537658691406, 'policy_logps/chosen': -433.86993408203125, 'referece_logps/rejected': -347.7498474121094, 'referece_logps/chosen': -387.3317565917969, 'logits/rejected': 0.021585112437605858, 'logits/chosen': -0.029447060078382492, 'epoch': 7.13}


 79%|███████▉  | 12761/16110 [13:46:56<18:25:21, 19.80s/it]
{'loss': 0.1352, 'learning_rate': 1.8970320014366323e-06, 'rewards/chosen': -4.413260459899902, 'rewards/rejected': -9.00289249420166, 'rewards/accuracies': 0.875, 'rewards/margins': 4.589632034301758, 'policy_logps/rejected': -329.104248046875, 'policy_logps/chosen': -463.8293762207031, 'referece_logps/rejected': -239.0753173828125, 'referece_logps/chosen': -419.6968078613281, 'logits/rejected': -0.07227693498134613, 'logits/chosen': -0.2471097707748413, 'epoch': 7.13}


 79%|███████▉  | 12763/16110 [13:47:34<18:05:20, 19.46s/it]
[2024-04-06 04:55:25,313] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 12764/16110 [13:47:54<18:15:01, 19.64s/it]

 79%|███████▉  | 12765/16110 [13:48:13<18:17:04, 19.68s/it]

 79%|███████▉  | 12766/16110 [13:48:28<16:48:11, 18.09s/it]

 79%|███████▉  | 12767/16110 [13:48:46<16:45:33, 18.05s/it]

 79%|███████▉  | 12768/16110 [13:49:02<16:15:40, 17.52s/it]
{'loss': 0.1481, 'learning_rate': 1.896409115922057e-06, 'rewards/chosen': -3.8332653045654297, 'rewards/rejected': -7.863041400909424, 'rewards/accuracies': 0.875, 'rewards/margins': 4.029776096343994, 'policy_logps/rejected': -358.9527587890625, 'policy_logps/chosen': -414.59375, 'referece_logps/rejected': -280.3223571777344, 'referece_logps/chosen': -376.2610778808594, 'logits/rejected': -0.10151621699333191, 'logits/chosen': -0.47077709436416626, 'epoch': 7.13}
[2024-04-06 04:57:16,515] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 12770/16110 [13:49:44<17:39:35, 19.03s/it]

 79%|███████▉  | 12771/16110 [13:50:00<17:00:34, 18.34s/it]

 79%|███████▉  | 12772/16110 [13:50:22<17:56:23, 19.35s/it]
[2024-04-06 04:58:13,840] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 12773/16110 [13:50:42<18:06:31, 19.54s/it]
[2024-04-06 04:58:33,814] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 12774/16110 [13:51:02<18:04:49, 19.51s/it]
{'loss': 0.1452, 'learning_rate': 1.8958738008990316e-06, 'rewards/chosen': -3.595296621322632, 'rewards/rejected': -8.55766773223877, 'rewards/accuracies': 1.0, 'rewards/margins': 4.962371826171875, 'policy_logps/rejected': -473.0150146484375, 'policy_logps/chosen': -345.625244140625, 'referece_logps/rejected': -387.43829345703125, 'referece_logps/chosen': -309.6722717285156, 'logits/rejected': 0.05379803478717804, 'logits/chosen': 0.025720715522766113, 'epoch': 7.14}

 79%|███████▉  | 12775/16110 [13:51:21<18:05:48, 19.53s/it]


 79%|███████▉  | 12777/16110 [13:51:48<15:28:03, 16.71s/it]

 79%|███████▉  | 12778/16110 [13:52:08<16:16:11, 17.58s/it]

 79%|███████▉  | 12779/16110 [13:52:28<16:54:13, 18.27s/it]

 79%|███████▉  | 12780/16110 [13:52:46<16:52:38, 18.25s/it]

 79%|███████▉  | 12781/16110 [13:53:06<17:25:43, 18.85s/it]
{'loss': 0.1291, 'learning_rate': 1.8952476191204594e-06, 'rewards/chosen': -4.893482685089111, 'rewards/rejected': -7.670095443725586, 'rewards/accuracies': 1.0, 'rewards/margins': 2.776613473892212, 'policy_logps/rejected': -355.547607421875, 'policy_logps/chosen': -356.28662109375, 'referece_logps/rejected': -278.8466796875, 'referece_logps/chosen': -307.351806640625, 'logits/rejected': 0.3433934450149536, 'logits/chosen': 0.3483739495277405, 'epoch': 7.14}

 79%|███████▉  | 12782/16110 [13:53:25<17:17:07, 18.70s/it]

 79%|███████▉  | 12783/16110 [13:53:47<18:15:05, 19.75s/it]


 79%|███████▉  | 12785/16110 [13:54:24<17:54:22, 19.39s/it]

 79%|███████▉  | 12786/16110 [13:54:40<16:55:01, 18.32s/it]
{'loss': 0.1438, 'learning_rate': 1.894799260752452e-06, 'rewards/chosen': -3.5765304565429688, 'rewards/rejected': -6.673240661621094, 'rewards/accuracies': 0.875, 'rewards/margins': 3.096710205078125, 'policy_logps/rejected': -368.4385986328125, 'policy_logps/chosen': -413.7885437011719, 'referece_logps/rejected': -301.7061767578125, 'referece_logps/chosen': -378.02325439453125, 'logits/rejected': -0.3914158046245575, 'logits/chosen': -0.36448904871940613, 'epoch': 7.14}
[2024-04-06 05:02:52,952] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 12788/16110 [13:55:23<18:15:44, 19.79s/it]
{'loss': 0.1233, 'learning_rate': 1.89461966420082e-06, 'rewards/chosen': -3.2021877765655518, 'rewards/rejected': -9.99341869354248, 'rewards/accuracies': 1.0, 'rewards/margins': 6.791232109069824, 'policy_logps/rejected': -306.07940673828125, 'policy_logps/chosen': -261.4080505371094, 'referece_logps/rejected': -206.14520263671875, 'referece_logps/chosen': -229.3861846923828, 'logits/rejected': -0.0428057461977005, 'logits/chosen': -0.1534094661474228, 'epoch': 7.14}
[2024-04-06 05:03:35,217] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 79%|███████▉  | 12790/16110 [13:56:00<17:32:48, 19.03s/it]
[2024-04-06 05:03:51,578] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 79%|███████▉  | 12791/16110 [13:56:18<17:18:22, 18.77s/it]

 79%|███████▉  | 12792/16110 [13:56:35<16:44:00, 18.16s/it]

 79%|███████▉  | 12793/16110 [13:56:56<17:41:13, 19.20s/it]

 79%|███████▉  | 12794/16110 [13:57:18<18:17:19, 19.86s/it]
{'loss': 0.2519, 'learning_rate': 1.8940800067939796e-06, 'rewards/chosen': -4.471279144287109, 'rewards/rejected': -10.341357231140137, 'rewards/accuracies': 0.875, 'rewards/margins': 5.870079040527344, 'policy_logps/rejected': -475.78009033203125, 'policy_logps/chosen': -399.92388916015625, 'referece_logps/rejected': -372.366455078125, 'referece_logps/chosen': -355.2110595703125, 'logits/rejected': 0.079073965549469, 'logits/chosen': 0.02003021165728569, 'epoch': 7.15}


 79%|███████▉  | 12796/16110 [13:57:54<17:25:02, 18.92s/it]

 79%|███████▉  | 12797/16110 [13:58:11<16:46:49, 18.23s/it]

 79%|███████▉  | 12798/16110 [13:58:31<17:15:37, 18.76s/it]

 79%|███████▉  | 12799/16110 [13:58:50<17:22:50, 18.90s/it]

 79%|███████▉  | 12800/16110 [13:59:09<17:15:11, 18.76s/it]

 79%|███████▉  | 12801/16110 [13:59:22<15:49:17, 17.21s/it]
{'loss': 0.2515, 'learning_rate': 1.8934487622004434e-06, 'rewards/chosen': -4.498039245605469, 'rewards/rejected': -10.33511734008789, 'rewards/accuracies': 1.0, 'rewards/margins': 5.837077617645264, 'policy_logps/rejected': -633.0956420898438, 'policy_logps/chosen': -600.0247802734375, 'referece_logps/rejected': -529.7444458007812, 'referece_logps/chosen': -555.0443725585938, 'logits/rejected': -0.33590230345726013, 'logits/chosen': -0.6224009990692139, 'epoch': 7.15}

 79%|███████▉  | 12802/16110 [13:59:42<16:28:54, 17.94s/it]


 79%|███████▉  | 12804/16110 [14:00:18<16:29:04, 17.95s/it]
{'loss': 0.0611, 'learning_rate': 1.8931776870225692e-06, 'rewards/chosen': -4.211132526397705, 'rewards/rejected': -9.439131736755371, 'rewards/accuracies': 1.0, 'rewards/margins': 5.227999210357666, 'policy_logps/rejected': -564.235595703125, 'policy_logps/chosen': -365.5370788574219, 'referece_logps/rejected': -469.8442687988281, 'referece_logps/chosen': -323.4257507324219, 'logits/rejected': 0.75017249584198, 'logits/chosen': 0.6317448616027832, 'epoch': 7.15}


 79%|███████▉  | 12806/16110 [14:00:55<16:48:21, 18.31s/it]
[2024-04-06 05:08:46,251] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1286, 'learning_rate': 1.8929967897167815e-06, 'rewards/chosen': -4.298905372619629, 'rewards/rejected': -11.351907730102539, 'rewards/accuracies': 1.0, 'rewards/margins': 7.053003787994385, 'policy_logps/rejected': -493.6627502441406, 'policy_logps/chosen': -461.22064208984375, 'referece_logps/rejected': -380.1436767578125, 'referece_logps/chosen': -418.2315673828125, 'logits/rejected': 0.07646383345127106, 'logits/chosen': -0.04708130657672882, 'epoch': 7.15}


 80%|███████▉  | 12808/16110 [14:01:35<17:30:28, 19.09s/it]

 80%|███████▉  | 12809/16110 [14:01:55<17:41:59, 19.30s/it]
{'loss': 0.2057, 'learning_rate': 1.8927251730504142e-06, 'rewards/chosen': -5.7090935707092285, 'rewards/rejected': -8.623771667480469, 'rewards/accuracies': 0.875, 'rewards/margins': 2.914677858352661, 'policy_logps/rejected': -366.9985656738281, 'policy_logps/chosen': -412.2487487792969, 'referece_logps/rejected': -280.76092529296875, 'referece_logps/chosen': -355.1578369140625, 'logits/rejected': 0.30441343784332275, 'logits/chosen': 0.22365820407867432, 'epoch': 7.16}


 80%|███████▉  | 12811/16110 [14:02:32<17:15:05, 18.83s/it]

 80%|███████▉  | 12812/16110 [14:02:53<17:48:04, 19.43s/it]

 80%|███████▉  | 12813/16110 [14:03:13<17:51:07, 19.49s/it]

 80%|███████▉  | 12814/16110 [14:03:30<17:19:52, 18.93s/it]
{'loss': 0.2265, 'learning_rate': 1.8922717569633538e-06, 'rewards/chosen': -4.6905622482299805, 'rewards/rejected': -7.553706169128418, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8631441593170166, 'policy_logps/rejected': -453.91851806640625, 'policy_logps/chosen': -372.8288269042969, 'referece_logps/rejected': -378.3815002441406, 'referece_logps/chosen': -325.9232177734375, 'logits/rejected': 0.38736140727996826, 'logits/chosen': 0.4604075253009796, 'epoch': 7.16}

 80%|███████▉  | 12815/16110 [14:03:52<18:05:16, 19.76s/it]

 80%|███████▉  | 12816/16110 [14:04:08<17:04:05, 18.65s/it]


 80%|███████▉  | 12818/16110 [14:04:39<15:41:45, 17.16s/it]
{'loss': 0.1494, 'learning_rate': 1.8919083748788434e-06, 'rewards/chosen': -4.889480113983154, 'rewards/rejected': -10.419351577758789, 'rewards/accuracies': 1.0, 'rewards/margins': 5.529871463775635, 'policy_logps/rejected': -521.1876831054688, 'policy_logps/chosen': -444.67559814453125, 'referece_logps/rejected': -416.9941711425781, 'referece_logps/chosen': -395.7807922363281, 'logits/rejected': 0.41996151208877563, 'logits/chosen': 0.5713277459144592, 'epoch': 7.16}

 80%|███████▉  | 12819/16110 [14:05:00<16:36:37, 18.17s/it]

 80%|███████▉  | 12820/16110 [14:05:22<17:37:07, 19.28s/it]


 80%|███████▉  | 12822/16110 [14:06:03<18:23:23, 20.13s/it]
{'loss': 0.1287, 'learning_rate': 1.8915444159690248e-06, 'rewards/chosen': -4.331785202026367, 'rewards/rejected': -8.184882164001465, 'rewards/accuracies': 0.75, 'rewards/margins': 3.8530960083007812, 'policy_logps/rejected': -333.00531005859375, 'policy_logps/chosen': -428.5496826171875, 'referece_logps/rejected': -251.156494140625, 'referece_logps/chosen': -385.2318420410156, 'logits/rejected': -0.38378605246543884, 'logits/chosen': -0.5464245676994324, 'epoch': 7.16}


 80%|███████▉  | 12824/16110 [14:06:35<16:54:01, 18.52s/it]
{'loss': 0.2071, 'learning_rate': 1.891362220278168e-06, 'rewards/chosen': -4.287506103515625, 'rewards/rejected': -11.268292427062988, 'rewards/accuracies': 1.0, 'rewards/margins': 6.980785369873047, 'policy_logps/rejected': -468.25634765625, 'policy_logps/chosen': -388.3898010253906, 'referece_logps/rejected': -355.5733947753906, 'referece_logps/chosen': -345.5147399902344, 'logits/rejected': 0.7355510592460632, 'logits/chosen': 0.6729941368103027, 'epoch': 7.16}


 80%|███████▉  | 12826/16110 [14:07:05<15:29:12, 16.98s/it]

 80%|███████▉  | 12827/16110 [14:07:25<16:16:40, 17.85s/it]
{'loss': 0.1811, 'learning_rate': 1.8910886565297902e-06, 'rewards/chosen': -5.4099955558776855, 'rewards/rejected': -8.524131774902344, 'rewards/accuracies': 0.875, 'rewards/margins': 3.114135503768921, 'policy_logps/rejected': -384.89013671875, 'policy_logps/chosen': -370.58001708984375, 'referece_logps/rejected': -299.6488342285156, 'referece_logps/chosen': -316.48004150390625, 'logits/rejected': -0.08820586651563644, 'logits/chosen': -0.24624717235565186, 'epoch': 7.17}

 80%|███████▉  | 12828/16110 [14:07:44<16:38:06, 18.25s/it]

 80%|███████▉  | 12829/16110 [14:08:03<16:38:45, 18.26s/it]

 80%|███████▉  | 12830/16110 [14:08:18<15:56:04, 17.49s/it]

 80%|███████▉  | 12831/16110 [14:08:33<15:04:55, 16.56s/it]


 80%|███████▉  | 12833/16110 [14:08:59<13:38:31, 14.99s/it]

 80%|███████▉  | 12834/16110 [14:09:15<13:57:53, 15.35s/it]
{'loss': 0.1512, 'learning_rate': 1.8904490806434233e-06, 'rewards/chosen': -4.09066104888916, 'rewards/rejected': -8.361756324768066, 'rewards/accuracies': 1.0, 'rewards/margins': 4.271095275878906, 'policy_logps/rejected': -322.9441223144531, 'policy_logps/chosen': -300.2110290527344, 'referece_logps/rejected': -239.32656860351562, 'referece_logps/chosen': -259.304443359375, 'logits/rejected': -0.03868086636066437, 'logits/chosen': -0.11933077871799469, 'epoch': 7.17}

 80%|███████▉  | 12835/16110 [14:09:32<14:25:21, 15.85s/it]


 80%|███████▉  | 12837/16110 [14:10:09<15:36:18, 17.16s/it]
{'loss': 0.1186, 'learning_rate': 1.8901744367292557e-06, 'rewards/chosen': -3.995250701904297, 'rewards/rejected': -11.119107246398926, 'rewards/accuracies': 1.0, 'rewards/margins': 7.123856544494629, 'policy_logps/rejected': -559.3677368164062, 'policy_logps/chosen': -379.2132263183594, 'referece_logps/rejected': -448.17669677734375, 'referece_logps/chosen': -339.26068115234375, 'logits/rejected': -0.260980486869812, 'logits/chosen': -0.2908715605735779, 'epoch': 7.17}

 80%|███████▉  | 12838/16110 [14:10:29<16:21:55, 18.01s/it]


 80%|███████▉  | 12840/16110 [14:11:04<16:19:31, 17.97s/it]
{'loss': 0.1056, 'learning_rate': 1.8898994689816285e-06, 'rewards/chosen': -4.118971824645996, 'rewards/rejected': -9.189373970031738, 'rewards/accuracies': 1.0, 'rewards/margins': 5.0704026222229, 'policy_logps/rejected': -484.8177795410156, 'policy_logps/chosen': -461.3823547363281, 'referece_logps/rejected': -392.924072265625, 'referece_logps/chosen': -420.1926574707031, 'logits/rejected': 0.3819424510002136, 'logits/chosen': 0.28898417949676514, 'epoch': 7.17}
[2024-04-06 05:19:18,644] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|███████▉  | 12842/16110 [14:11:39<15:47:46, 17.40s/it]
{'loss': 0.1637, 'learning_rate': 1.8897159772917918e-06, 'rewards/chosen': -4.180769443511963, 'rewards/rejected': -6.7432541847229, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5624849796295166, 'policy_logps/rejected': -425.7065734863281, 'policy_logps/chosen': -292.1212158203125, 'referece_logps/rejected': -358.2740478515625, 'referece_logps/chosen': -250.3135223388672, 'logits/rejected': -0.11677919328212738, 'logits/chosen': 0.048386622220277786, 'epoch': 7.17}
[2024-04-06 05:19:46,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 12843/16110 [14:11:55<15:18:58, 16.88s/it]

 80%|███████▉  | 12844/16110 [14:12:15<16:12:36, 17.87s/it]

 80%|███████▉  | 12845/16110 [14:12:35<16:43:05, 18.43s/it]

 80%|███████▉  | 12846/16110 [14:12:52<16:20:25, 18.02s/it]


 80%|███████▉  | 12848/16110 [14:13:28<16:07:29, 17.80s/it]

 80%|███████▉  | 12849/16110 [14:13:44<15:41:45, 17.33s/it]

 80%|███████▉  | 12850/16110 [14:14:01<15:46:01, 17.41s/it]
{'loss': 0.1414, 'learning_rate': 1.8889805723108715e-06, 'rewards/chosen': -4.144499778747559, 'rewards/rejected': -8.640436172485352, 'rewards/accuracies': 1.0, 'rewards/margins': 4.495936393737793, 'policy_logps/rejected': -482.75384521484375, 'policy_logps/chosen': -430.0838928222656, 'referece_logps/rejected': -396.3494873046875, 'referece_logps/chosen': -388.6388854980469, 'logits/rejected': 0.6144921183586121, 'logits/chosen': 0.5458898544311523, 'epoch': 7.18}

 80%|███████▉  | 12851/16110 [14:14:21<16:15:06, 17.95s/it]


 80%|███████▉  | 12853/16110 [14:14:48<14:21:56, 15.88s/it]
{'loss': 0.1008, 'learning_rate': 1.8887042024423265e-06, 'rewards/chosen': -2.4919469356536865, 'rewards/rejected': -7.082212924957275, 'rewards/accuracies': 1.0, 'rewards/margins': 4.59026575088501, 'policy_logps/rejected': -424.999267578125, 'policy_logps/chosen': -442.4137268066406, 'referece_logps/rejected': -354.1771545410156, 'referece_logps/chosen': -417.4942626953125, 'logits/rejected': -0.3133085370063782, 'logits/chosen': -0.3198460638523102, 'epoch': 7.18}

 80%|███████▉  | 12854/16110 [14:15:09<15:52:29, 17.55s/it]


 80%|███████▉  | 12856/16110 [14:15:50<17:21:14, 19.20s/it]
[2024-04-06 05:23:41,311] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2386, 'learning_rate': 1.8884275092751728e-06, 'rewards/chosen': -3.453270435333252, 'rewards/rejected': -8.919937133789062, 'rewards/accuracies': 1.0, 'rewards/margins': 5.466667175292969, 'policy_logps/rejected': -372.4581298828125, 'policy_logps/chosen': -399.8314514160156, 'referece_logps/rejected': -283.25872802734375, 'referece_logps/chosen': -365.29876708984375, 'logits/rejected': -0.1791897416114807, 'logits/chosen': -0.19606804847717285, 'epoch': 7.18}
[2024-04-06 05:24:00,562] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 12857/16110 [14:16:09<17:21:45, 19.21s/it]

 80%|███████▉  | 12858/16110 [14:16:29<17:35:57, 19.48s/it]


 80%|███████▉  | 12860/16110 [14:17:06<17:22:57, 19.25s/it]
{'loss': 0.078, 'learning_rate': 1.8880580823173466e-06, 'rewards/chosen': -4.440708637237549, 'rewards/rejected': -8.88144302368164, 'rewards/accuracies': 1.0, 'rewards/margins': 4.440733909606934, 'policy_logps/rejected': -343.55474853515625, 'policy_logps/chosen': -324.6153564453125, 'referece_logps/rejected': -254.74032592773438, 'referece_logps/chosen': -280.208251953125, 'logits/rejected': -0.3877445459365845, 'logits/chosen': -0.5094344019889832, 'epoch': 7.18}


 80%|███████▉  | 12862/16110 [14:17:46<17:45:48, 19.69s/it]

 80%|███████▉  | 12863/16110 [14:18:02<16:46:39, 18.60s/it]

 80%|███████▉  | 12864/16110 [14:18:14<14:59:48, 16.63s/it]
[2024-04-06 05:26:05,727] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1858, 'learning_rate': 1.8876880810243183e-06, 'rewards/chosen': -4.495203971862793, 'rewards/rejected': -6.60623836517334, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1110341548919678, 'policy_logps/rejected': -383.0592041015625, 'policy_logps/chosen': -694.63671875, 'referece_logps/rejected': -316.9967956542969, 'referece_logps/chosen': -649.6846313476562, 'logits/rejected': -0.9465739727020264, 'logits/chosen': -0.9680976867675781, 'epoch': 7.19}

 80%|███████▉  | 12865/16110 [14:18:33<15:38:15, 17.35s/it]

 80%|███████▉  | 12866/16110 [14:18:53<16:21:12, 18.15s/it]

 80%|███████▉  | 12867/16110 [14:19:13<16:49:27, 18.68s/it]

 80%|███████▉  | 12868/16110 [14:19:27<15:27:59, 17.17s/it]

 80%|███████▉  | 12869/16110 [14:19:42<14:53:07, 16.53s/it]


 80%|███████▉  | 12871/16110 [14:20:26<17:32:25, 19.50s/it]
[2024-04-06 05:28:17,708] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.117, 'learning_rate': 1.887039197487392e-06, 'rewards/chosen': -5.075427532196045, 'rewards/rejected': -9.738938331604004, 'rewards/accuracies': 1.0, 'rewards/margins': 4.663511753082275, 'policy_logps/rejected': -385.820556640625, 'policy_logps/chosen': -391.1221923828125, 'referece_logps/rejected': -288.4311828613281, 'referece_logps/chosen': -340.367919921875, 'logits/rejected': 0.1383625864982605, 'logits/chosen': 0.2832820415496826, 'epoch': 7.19}


 80%|███████▉  | 12873/16110 [14:21:04<17:28:32, 19.44s/it]
[2024-04-06 05:28:55,787] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1756, 'learning_rate': 1.8868534794419782e-06, 'rewards/chosen': -5.687198638916016, 'rewards/rejected': -9.160271644592285, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4730722904205322, 'policy_logps/rejected': -388.0772705078125, 'policy_logps/chosen': -251.40621948242188, 'referece_logps/rejected': -296.47454833984375, 'referece_logps/chosen': -194.53421020507812, 'logits/rejected': 0.05778568983078003, 'logits/chosen': 0.06866215914487839, 'epoch': 7.19}

 80%|███████▉  | 12874/16110 [14:21:25<17:57:55, 19.99s/it]
[2024-04-06 05:29:29,172] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 12875/16110 [14:21:38<15:50:15, 17.62s/it]

 80%|███████▉  | 12876/16110 [14:21:52<14:54:40, 16.60s/it]


 80%|███████▉  | 12878/16110 [14:22:17<13:05:23, 14.58s/it]
{'loss': 0.1201, 'learning_rate': 1.8863885570671237e-06, 'rewards/chosen': -3.563438653945923, 'rewards/rejected': -8.377328872680664, 'rewards/accuracies': 1.0, 'rewards/margins': 4.813889980316162, 'policy_logps/rejected': -343.3170166015625, 'policy_logps/chosen': -410.95941162109375, 'referece_logps/rejected': -259.5437316894531, 'referece_logps/chosen': -375.3250732421875, 'logits/rejected': -0.18759509921073914, 'logits/chosen': -0.21211230754852295, 'epoch': 7.19}

 80%|███████▉  | 12879/16110 [14:22:40<15:28:14, 17.24s/it]

 80%|███████▉  | 12880/16110 [14:23:00<16:09:52, 18.02s/it]

 80%|███████▉  | 12881/16110 [14:23:15<15:24:43, 17.18s/it]
[2024-04-06 05:31:27,410] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 12882/16110 [14:23:36<16:21:50, 18.25s/it]

 80%|███████▉  | 12883/16110 [14:23:56<16:48:53, 18.76s/it]

 80%|███████▉  | 12884/16110 [14:24:17<17:34:47, 19.62s/it]

 80%|███████▉  | 12885/16110 [14:24:31<15:59:46, 17.86s/it]
[2024-04-06 05:32:42,587] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|███████▉  | 12886/16110 [14:24:51<16:31:48, 18.46s/it]

 80%|███████▉  | 12887/16110 [14:25:10<16:42:13, 18.66s/it]

 80%|████████  | 12888/16110 [14:25:27<16:10:32, 18.07s/it]


 80%|████████  | 12890/16110 [14:26:04<16:18:34, 18.23s/it]
{'loss': 0.1355, 'learning_rate': 1.885269089760397e-06, 'rewards/chosen': -3.680190324783325, 'rewards/rejected': -8.121048927307129, 'rewards/accuracies': 1.0, 'rewards/margins': 4.440858840942383, 'policy_logps/rejected': -380.6446533203125, 'policy_logps/chosen': -403.9085693359375, 'referece_logps/rejected': -299.43414306640625, 'referece_logps/chosen': -367.106689453125, 'logits/rejected': -0.24871623516082764, 'logits/chosen': -0.2142973393201828, 'epoch': 7.2}

 80%|████████  | 12891/16110 [14:26:23<16:33:16, 18.51s/it]

 80%|████████  | 12892/16110 [14:26:41<16:25:51, 18.38s/it]

 80%|████████  | 12893/16110 [14:26:53<14:36:16, 16.34s/it]


 80%|████████  | 12895/16110 [14:27:30<15:32:01, 17.39s/it]

 80%|████████  | 12896/16110 [14:27:50<16:12:43, 18.16s/it]
{'loss': 0.0878, 'learning_rate': 1.8847074234057278e-06, 'rewards/chosen': -4.422475337982178, 'rewards/rejected': -8.480368614196777, 'rewards/accuracies': 0.875, 'rewards/margins': 4.057893753051758, 'policy_logps/rejected': -487.31842041015625, 'policy_logps/chosen': -434.3876647949219, 'referece_logps/rejected': -402.5147705078125, 'referece_logps/chosen': -390.16290283203125, 'logits/rejected': -0.1252455711364746, 'logits/chosen': -0.08824989199638367, 'epoch': 7.2}

 80%|████████  | 12897/16110 [14:28:12<17:11:07, 19.26s/it]

 80%|████████  | 12898/16110 [14:28:27<16:02:48, 17.99s/it]

 80%|████████  | 12899/16110 [14:28:47<16:35:17, 18.60s/it]


 80%|████████  | 12901/16110 [14:29:21<15:50:04, 17.76s/it]
{'loss': 0.1638, 'learning_rate': 1.8842383846613576e-06, 'rewards/chosen': -5.082971572875977, 'rewards/rejected': -7.648690700531006, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5657193660736084, 'policy_logps/rejected': -472.738525390625, 'policy_logps/chosen': -426.1775817871094, 'referece_logps/rejected': -396.2515869140625, 'referece_logps/chosen': -375.3478698730469, 'logits/rejected': 0.906277060508728, 'logits/chosen': 0.8394495248794556, 'epoch': 7.21}

 80%|████████  | 12902/16110 [14:29:40<16:14:33, 18.23s/it]
[2024-04-06 05:37:53,112] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|████████  | 12904/16110 [14:30:21<17:09:01, 19.26s/it]
{'loss': 0.2634, 'learning_rate': 1.8839565324857147e-06, 'rewards/chosen': -4.009398460388184, 'rewards/rejected': -10.721111297607422, 'rewards/accuracies': 1.0, 'rewards/margins': 6.711711883544922, 'policy_logps/rejected': -343.05804443359375, 'policy_logps/chosen': -318.895263671875, 'referece_logps/rejected': -235.84695434570312, 'referece_logps/chosen': -278.80126953125, 'logits/rejected': -0.5529454946517944, 'logits/chosen': -0.6757745742797852, 'epoch': 7.21}
[2024-04-06 05:38:29,064] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 80%|████████  | 12906/16110 [14:30:57<16:35:39, 18.65s/it]
{'loss': 0.1529, 'learning_rate': 1.8837684523781352e-06, 'rewards/chosen': -4.113553047180176, 'rewards/rejected': -8.939538955688477, 'rewards/accuracies': 0.875, 'rewards/margins': 4.825985908508301, 'policy_logps/rejected': -535.2542724609375, 'policy_logps/chosen': -307.685546875, 'referece_logps/rejected': -445.8588562011719, 'referece_logps/chosen': -266.550048828125, 'logits/rejected': -0.651416540145874, 'logits/chosen': -0.39019352197647095, 'epoch': 7.21}


 80%|████████  | 12908/16110 [14:31:33<16:24:53, 18.46s/it]

 80%|████████  | 12909/16110 [14:31:55<17:27:45, 19.64s/it]
{'loss': 0.1964, 'learning_rate': 1.8834860643070627e-06, 'rewards/chosen': -3.7095742225646973, 'rewards/rejected': -7.732527732849121, 'rewards/accuracies': 0.875, 'rewards/margins': 4.022953510284424, 'policy_logps/rejected': -250.9517364501953, 'policy_logps/chosen': -284.18377685546875, 'referece_logps/rejected': -173.62646484375, 'referece_logps/chosen': -247.08802795410156, 'logits/rejected': -0.5415374040603638, 'logits/chosen': -0.7057399153709412, 'epoch': 7.21}


 80%|████████  | 12911/16110 [14:32:31<16:44:57, 18.85s/it]
{'loss': 0.1313, 'learning_rate': 1.883297627030935e-06, 'rewards/chosen': -5.820091724395752, 'rewards/rejected': -9.202624320983887, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3825321197509766, 'policy_logps/rejected': -380.27490234375, 'policy_logps/chosen': -395.650634765625, 'referece_logps/rejected': -288.2486572265625, 'referece_logps/chosen': -337.44970703125, 'logits/rejected': 0.16423246264457703, 'logits/chosen': -0.023383960127830505, 'epoch': 7.21}
[2024-04-06 05:40:43,714] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 12912/16110 [14:32:52<17:22:28, 19.56s/it]

 80%|████████  | 12913/16110 [14:33:12<17:29:52, 19.70s/it]


 80%|████████  | 12915/16110 [14:33:43<15:20:38, 17.29s/it]
{'loss': 0.085, 'learning_rate': 1.8829203240668026e-06, 'rewards/chosen': -4.924283981323242, 'rewards/rejected': -11.086792945861816, 'rewards/accuracies': 1.0, 'rewards/margins': 6.162509441375732, 'policy_logps/rejected': -330.0919189453125, 'policy_logps/chosen': -356.1209411621094, 'referece_logps/rejected': -219.22398376464844, 'referece_logps/chosen': -306.87811279296875, 'logits/rejected': 0.04773058742284775, 'logits/chosen': 0.13472209870815277, 'epoch': 7.22}


 80%|████████  | 12917/16110 [14:34:17<15:06:50, 17.04s/it]

 80%|████████  | 12918/16110 [14:34:35<15:17:32, 17.25s/it]
{'loss': 0.1017, 'learning_rate': 1.8826369721034238e-06, 'rewards/chosen': -5.451843738555908, 'rewards/rejected': -10.599539756774902, 'rewards/accuracies': 1.0, 'rewards/margins': 5.1476969718933105, 'policy_logps/rejected': -464.6412658691406, 'policy_logps/chosen': -439.95965576171875, 'referece_logps/rejected': -358.6458740234375, 'referece_logps/chosen': -385.4412841796875, 'logits/rejected': 0.2927111089229584, 'logits/chosen': 0.29405394196510315, 'epoch': 7.22}


 80%|████████  | 12920/16110 [14:35:07<14:42:59, 16.61s/it]
{'loss': 0.0494, 'learning_rate': 1.8824478924040079e-06, 'rewards/chosen': -5.334493637084961, 'rewards/rejected': -9.992074966430664, 'rewards/accuracies': 1.0, 'rewards/margins': 4.657580852508545, 'policy_logps/rejected': -582.70458984375, 'policy_logps/chosen': -491.7955322265625, 'referece_logps/rejected': -482.783935546875, 'referece_logps/chosen': -438.45062255859375, 'logits/rejected': 0.6572085022926331, 'logits/chosen': 0.7023247480392456, 'epoch': 7.22}


 80%|████████  | 12922/16110 [14:35:45<15:34:28, 17.59s/it]
{'loss': 0.21, 'learning_rate': 1.8822586700278576e-06, 'rewards/chosen': -5.292646884918213, 'rewards/rejected': -10.278014183044434, 'rewards/accuracies': 0.875, 'rewards/margins': 4.985368251800537, 'policy_logps/rejected': -436.09149169921875, 'policy_logps/chosen': -403.8604736328125, 'referece_logps/rejected': -333.3113098144531, 'referece_logps/chosen': -350.93402099609375, 'logits/rejected': 0.1853068768978119, 'logits/chosen': 0.35057389736175537, 'epoch': 7.22}

 80%|████████  | 12923/16110 [14:36:05<16:06:33, 18.20s/it]

 80%|████████  | 12924/16110 [14:36:18<14:49:26, 16.75s/it]

 80%|████████  | 12925/16110 [14:36:34<14:36:37, 16.51s/it]

 80%|████████  | 12926/16110 [14:36:46<13:27:18, 15.21s/it]

 80%|████████  | 12927/16110 [14:37:04<14:02:01, 15.87s/it]

 80%|████████  | 12928/16110 [14:37:22<14:43:29, 16.66s/it]
[2024-04-06 05:45:37,011] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 12929/16110 [14:37:45<16:28:31, 18.65s/it]

 80%|████████  | 12930/16110 [14:38:01<15:47:19, 17.87s/it]

 80%|████████  | 12931/16110 [14:38:13<14:07:14, 15.99s/it]

 80%|████████  | 12932/16110 [14:38:31<14:35:02, 16.52s/it]

 80%|████████  | 12933/16110 [14:38:46<14:11:11, 16.08s/it]


 80%|████████  | 12935/16110 [14:39:24<15:32:00, 17.61s/it]
{'loss': 0.1099, 'learning_rate': 1.881025248952564e-06, 'rewards/chosen': -3.277416229248047, 'rewards/rejected': -9.395140647888184, 'rewards/accuracies': 1.0, 'rewards/margins': 6.117724418640137, 'policy_logps/rejected': -374.86102294921875, 'policy_logps/chosen': -295.84295654296875, 'referece_logps/rejected': -280.9096374511719, 'referece_logps/chosen': -263.0688171386719, 'logits/rejected': 0.05178961157798767, 'logits/chosen': 0.034852564334869385, 'epoch': 7.23}

 80%|████████  | 12936/16110 [14:39:39<15:04:53, 17.11s/it]

 80%|████████  | 12937/16110 [14:39:57<15:05:56, 17.13s/it]

 80%|████████  | 12938/16110 [14:40:15<15:27:55, 17.55s/it]

 80%|████████  | 12939/16110 [14:40:36<16:24:44, 18.63s/it]

 80%|████████  | 12940/16110 [14:40:55<16:18:24, 18.52s/it]

 80%|████████  | 12941/16110 [14:41:16<17:06:42, 19.44s/it]

 80%|████████  | 12942/16110 [14:41:36<17:15:21, 19.61s/it]


 80%|████████  | 12944/16110 [14:42:15<17:17:37, 19.66s/it]

 80%|████████  | 12945/16110 [14:42:35<17:16:40, 19.65s/it]

 80%|████████  | 12946/16110 [14:42:52<16:42:58, 19.02s/it]

 80%|████████  | 12947/16110 [14:43:11<16:34:11, 18.86s/it]

 80%|████████  | 12948/16110 [14:43:29<16:27:18, 18.73s/it]

 80%|████████  | 12949/16110 [14:43:47<16:17:30, 18.55s/it]

 80%|████████  | 12950/16110 [14:44:07<16:32:43, 18.85s/it]

 80%|████████  | 12951/16110 [14:44:26<16:37:41, 18.95s/it]

 80%|████████  | 12952/16110 [14:44:40<15:29:06, 17.65s/it]

 80%|████████  | 12953/16110 [14:45:03<16:44:18, 19.09s/it]

 80%|████████  | 12954/16110 [14:45:20<16:07:56, 18.40s/it]

 80%|████████  | 12955/16110 [14:45:35<15:24:12, 17.58s/it]

 80%|████████  | 12956/16110 [14:45:52<15:04:13, 17.20s/it]

 80%|████████  | 12957/16110 [14:46:12<15:53:04, 18.14s/it]

 80%|████████  | 12958/16110 [14:46:32<16:20:18, 18.66s/it]

 80%|████████  | 12959/16110 [14:46:53<16:59:47, 19.42s/it]

 80%|████████  | 12960/16110 [14:47:15<17:31:37, 20.03s/it]

 80%|████████  | 12961/16110 [14:47:30<16:22:30, 18.72s/it]

 80%|████████  | 12962/16110 [14:47:50<16:40:17, 19.07s/it]

 80%|████████  | 12963/16110 [14:48:12<17:28:53, 20.00s/it]
[2024-04-06 05:56:03,869] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 12964/16110 [14:48:29<16:33:38, 18.95s/it]

 80%|████████  | 12965/16110 [14:48:49<16:52:36, 19.32s/it]
[2024-04-06 05:56:40,552] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 80%|████████  | 12966/16110 [14:49:05<15:58:00, 18.28s/it]

 80%|████████  | 12967/16110 [14:49:23<15:50:28, 18.14s/it]

 80%|████████  | 12968/16110 [14:49:43<16:18:31, 18.69s/it]

 81%|████████  | 12969/16110 [14:50:03<16:48:33, 19.27s/it]

 81%|████████  | 12970/16110 [14:50:22<16:48:44, 19.28s/it]

 81%|████████  | 12971/16110 [14:50:36<15:13:24, 17.46s/it]

 81%|████████  | 12972/16110 [14:50:51<14:33:53, 16.71s/it]

 81%|████████  | 12973/16110 [14:51:02<13:14:45, 15.20s/it]

 81%|████████  | 12974/16110 [14:51:22<14:20:46, 16.47s/it]

 81%|████████  | 12975/16110 [14:51:33<12:50:57, 14.76s/it]

 81%|████████  | 12976/16110 [14:51:44<11:56:27, 13.72s/it]

 81%|████████  | 12977/16110 [14:52:00<12:41:26, 14.58s/it]

 81%|████████  | 12978/16110 [14:52:11<11:37:47, 13.37s/it]

 81%|████████  | 12979/16110 [14:52:30<13:08:48, 15.12s/it]

 81%|████████  | 12980/16110 [14:52:49<14:14:49, 16.39s/it]

 81%|████████  | 12981/16110 [14:53:11<15:40:34, 18.04s/it]

 81%|████████  | 12982/16110 [14:53:31<16:12:56, 18.66s/it]

 81%|████████  | 12983/16110 [14:53:51<16:30:49, 19.01s/it]

 81%|████████  | 12984/16110 [14:54:09<16:12:19, 18.66s/it]
[2024-04-06 06:02:00,813] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 12985/16110 [14:54:29<16:37:46, 19.16s/it]

 81%|████████  | 12986/16110 [14:54:43<15:16:41, 17.61s/it]

 81%|████████  | 12987/16110 [14:55:03<15:52:42, 18.30s/it]

 81%|████████  | 12988/16110 [14:55:23<16:14:36, 18.73s/it]

 81%|████████  | 12989/16110 [14:55:40<15:43:08, 18.13s/it]

 81%|████████  | 12990/16110 [14:55:54<14:33:57, 16.81s/it]

 81%|████████  | 12991/16110 [14:56:12<14:52:49, 17.18s/it]
[2024-04-06 06:04:03,253] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 12992/16110 [14:56:33<15:50:57, 18.30s/it]

 81%|████████  | 12993/16110 [14:56:52<16:11:09, 18.69s/it]

 81%|████████  | 12994/16110 [14:57:09<15:39:29, 18.09s/it]

 81%|████████  | 12995/16110 [14:57:25<15:06:08, 17.45s/it]

 81%|████████  | 12996/16110 [14:57:43<15:14:17, 17.62s/it]

 81%|████████  | 12997/16110 [14:57:57<14:23:24, 16.64s/it]

 81%|████████  | 12998/16110 [14:58:17<15:08:07, 17.51s/it]

 81%|████████  | 12999/16110 [14:58:32<14:30:18, 16.79s/it]

 81%|████████  | 13000/16110 [14:58:52<15:16:08, 17.67s/it]

 81%|████████  | 13001/16110 [14:59:22<18:27:12, 21.37s/it]

 81%|████████  | 13002/16110 [14:59:38<17:16:01, 20.00s/it]

 81%|████████  | 13003/16110 [14:59:50<15:03:04, 17.44s/it]

 81%|████████  | 13004/16110 [15:00:02<13:44:16, 15.92s/it]

 81%|████████  | 13005/16110 [15:00:19<13:50:32, 16.05s/it]

 81%|████████  | 13006/16110 [15:00:38<14:38:10, 16.98s/it]

 81%|████████  | 13007/16110 [15:00:58<15:28:51, 17.96s/it]

 81%|████████  | 13008/16110 [15:01:20<16:34:45, 19.24s/it]
[2024-04-06 06:09:11,794] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 13009/16110 [15:01:36<15:45:55, 18.30s/it]

 81%|████████  | 13010/16110 [15:01:57<16:26:52, 19.10s/it]

 81%|████████  | 13011/16110 [15:02:16<16:28:07, 19.13s/it]

 81%|████████  | 13012/16110 [15:02:35<16:15:49, 18.90s/it]

 81%|████████  | 13013/16110 [15:02:55<16:30:36, 19.19s/it]

 81%|████████  | 13014/16110 [15:03:07<14:42:06, 17.10s/it]

 81%|████████  | 13015/16110 [15:03:24<14:40:53, 17.08s/it]

 81%|████████  | 13016/16110 [15:03:46<15:56:25, 18.55s/it]
[2024-04-06 06:11:37,520] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 13017/16110 [15:04:02<15:19:30, 17.84s/it]

 81%|████████  | 13018/16110 [15:04:20<15:15:17, 17.76s/it]

 81%|████████  | 13019/16110 [15:04:38<15:19:08, 17.84s/it]

 81%|████████  | 13020/16110 [15:04:56<15:21:24, 17.89s/it]

 81%|████████  | 13021/16110 [15:05:14<15:28:01, 18.03s/it]

 81%|████████  | 13022/16110 [15:05:32<15:31:30, 18.10s/it]

 81%|████████  | 13023/16110 [15:05:48<14:55:37, 17.41s/it]

 81%|████████  | 13024/16110 [15:06:12<16:35:10, 19.35s/it]
[2024-04-06 06:14:03,603] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 13025/16110 [15:06:33<17:08:32, 20.00s/it]
[2024-04-06 06:14:25,137] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 13026/16110 [15:06:54<17:11:39, 20.07s/it]
[2024-04-06 06:14:45,365] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 13027/16110 [15:07:16<17:44:30, 20.72s/it]

 81%|████████  | 13028/16110 [15:07:29<15:43:12, 18.36s/it]

 81%|████████  | 13029/16110 [15:07:46<15:31:25, 18.14s/it]

 81%|████████  | 13030/16110 [15:08:06<15:55:42, 18.62s/it]
{'loss': 0.1173, 'learning_rate': 1.871829643283148e-06, 'rewards/chosen': -4.5916361808776855, 'rewards/rejected': -9.550297737121582, 'rewards/accuracies': 1.0, 'rewards/margins': 4.958661079406738, 'policy_logps/rejected': -370.9057312011719, 'policy_logps/chosen': -305.91131591796875, 'referece_logps/rejected': -275.4027404785156, 'referece_logps/chosen': -259.9949645996094, 'logits/rejected': 0.26175278425216675, 'logits/chosen': 0.3461584448814392, 'epoch': 7.28}


 81%|████████  | 13032/16110 [15:08:41<14:57:36, 17.50s/it]

 81%|████████  | 13033/16110 [15:08:56<14:27:01, 16.91s/it]
[2024-04-06 06:16:47,746] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2905, 'learning_rate': 1.8715340576667166e-06, 'rewards/chosen': -4.660263538360596, 'rewards/rejected': -7.913278579711914, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2530152797698975, 'policy_logps/rejected': -545.1231689453125, 'policy_logps/chosen': -550.4403686523438, 'referece_logps/rejected': -465.99041748046875, 'referece_logps/chosen': -503.83770751953125, 'logits/rejected': 0.7745827436447144, 'logits/chosen': 1.2099474668502808, 'epoch': 7.28}

 81%|████████  | 13034/16110 [15:09:08<13:03:49, 15.29s/it]


 81%|████████  | 13036/16110 [15:09:38<13:20:06, 15.62s/it]

 81%|████████  | 13037/16110 [15:09:59<14:41:52, 17.22s/it]
[2024-04-06 06:17:50,966] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0895, 'learning_rate': 1.8711394503375453e-06, 'rewards/chosen': -4.646617412567139, 'rewards/rejected': -10.288267135620117, 'rewards/accuracies': 1.0, 'rewards/margins': 5.6416497230529785, 'policy_logps/rejected': -539.1640625, 'policy_logps/chosen': -497.8922424316406, 'referece_logps/rejected': -436.2813720703125, 'referece_logps/chosen': -451.4260559082031, 'logits/rejected': 0.21218639612197876, 'logits/chosen': 0.19950628280639648, 'epoch': 7.28}


 81%|████████  | 13039/16110 [15:10:27<13:15:20, 15.54s/it]
{'loss': 0.1729, 'learning_rate': 1.870941935384474e-06, 'rewards/chosen': -2.783437967300415, 'rewards/rejected': -6.981841087341309, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1984028816223145, 'policy_logps/rejected': -337.68243408203125, 'policy_logps/chosen': -264.24053955078125, 'referece_logps/rejected': -267.864013671875, 'referece_logps/chosen': -236.40615844726562, 'logits/rejected': 0.1482992172241211, 'logits/chosen': 0.21405090391635895, 'epoch': 7.28}

 81%|████████  | 13040/16110 [15:10:42<13:10:45, 15.45s/it]


 81%|████████  | 13042/16110 [15:11:13<13:21:57, 15.68s/it]

 81%|████████  | 13043/16110 [15:11:32<14:13:57, 16.71s/it]

 81%|████████  | 13044/16110 [15:11:50<14:31:57, 17.06s/it]

 81%|████████  | 13045/16110 [15:12:05<13:52:35, 16.30s/it]

 81%|████████  | 13046/16110 [15:12:19<13:27:01, 15.80s/it]

 81%|████████  | 13047/16110 [15:12:37<13:59:45, 16.45s/it]

 81%|████████  | 13048/16110 [15:12:54<13:57:54, 16.42s/it]

 81%|████████  | 13049/16110 [15:13:14<14:53:16, 17.51s/it]

 81%|████████  | 13050/16110 [15:13:35<15:48:23, 18.60s/it]

 81%|████████  | 13051/16110 [15:13:55<16:04:06, 18.91s/it]

 81%|████████  | 13052/16110 [15:14:11<15:23:12, 18.11s/it]

 81%|████████  | 13053/16110 [15:14:31<15:48:35, 18.62s/it]

 81%|████████  | 13054/16110 [15:14:51<16:16:41, 19.18s/it]
[2024-04-06 06:22:42,707] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 81%|████████  | 13055/16110 [15:15:09<16:03:12, 18.92s/it]

 81%|████████  | 13056/16110 [15:15:27<15:47:11, 18.61s/it]

 81%|████████  | 13057/16110 [15:15:47<16:03:22, 18.93s/it]

 81%|████████  | 13058/16110 [15:16:03<15:20:29, 18.10s/it]

 81%|████████  | 13059/16110 [15:16:22<15:27:14, 18.23s/it]

 81%|████████  | 13060/16110 [15:16:44<16:27:07, 19.42s/it]

 81%|████████  | 13061/16110 [15:17:03<16:23:53, 19.36s/it]

 81%|████████  | 13062/16110 [15:17:21<16:04:56, 18.99s/it]

 81%|████████  | 13063/16110 [15:17:34<14:31:17, 17.16s/it]
{'loss': 0.1041, 'learning_rate': 1.8685607814231437e-06, 'rewards/chosen': -4.584456920623779, 'rewards/rejected': -8.037833213806152, 'rewards/accuracies': 1.0, 'rewards/margins': 3.453376531600952, 'policy_logps/rejected': -334.77117919921875, 'policy_logps/chosen': -441.9070129394531, 'referece_logps/rejected': -254.39280700683594, 'referece_logps/chosen': -396.06243896484375, 'logits/rejected': 0.24368400871753693, 'logits/chosen': -0.08824183791875839, 'epoch': 7.3}


 81%|████████  | 13065/16110 [15:18:07<14:12:12, 16.79s/it]
{'loss': 0.1278, 'learning_rate': 1.8683614383558337e-06, 'rewards/chosen': -3.7273900508880615, 'rewards/rejected': -9.736405372619629, 'rewards/accuracies': 1.0, 'rewards/margins': 6.009015083312988, 'policy_logps/rejected': -410.48046875, 'policy_logps/chosen': -376.9483642578125, 'referece_logps/rejected': -313.11639404296875, 'referece_logps/chosen': -339.6744384765625, 'logits/rejected': 0.015077009797096252, 'logits/chosen': -0.13089732825756073, 'epoch': 7.3}

 81%|████████  | 13066/16110 [15:18:27<14:56:15, 17.67s/it]


 81%|████████  | 13068/16110 [15:19:07<16:04:33, 19.02s/it]
{'loss': 0.1882, 'learning_rate': 1.8680621605164547e-06, 'rewards/chosen': -6.2447896003723145, 'rewards/rejected': -11.20910358428955, 'rewards/accuracies': 1.0, 'rewards/margins': 4.96431303024292, 'policy_logps/rejected': -478.79437255859375, 'policy_logps/chosen': -470.2700500488281, 'referece_logps/rejected': -366.7033996582031, 'referece_logps/chosen': -407.8221435546875, 'logits/rejected': 0.6084983348846436, 'logits/chosen': 0.5164849162101746, 'epoch': 7.3}

 81%|████████  | 13069/16110 [15:19:25<15:41:24, 18.57s/it]


 81%|████████  | 13071/16110 [15:20:05<16:24:20, 19.43s/it]
{'loss': 0.0954, 'learning_rate': 1.8677625668877639e-06, 'rewards/chosen': -4.220727920532227, 'rewards/rejected': -9.686375617980957, 'rewards/accuracies': 1.0, 'rewards/margins': 5.465648174285889, 'policy_logps/rejected': -568.9601440429688, 'policy_logps/chosen': -387.0870056152344, 'referece_logps/rejected': -472.09637451171875, 'referece_logps/chosen': -344.8797912597656, 'logits/rejected': 0.5822670459747314, 'logits/chosen': 0.5618879199028015, 'epoch': 7.3}


 81%|████████  | 13073/16110 [15:20:43<16:17:18, 19.31s/it]

 81%|████████  | 13074/16110 [15:20:58<15:08:37, 17.96s/it]

 81%|████████  | 13075/16110 [15:21:14<14:41:39, 17.43s/it]

 81%|████████  | 13076/16110 [15:21:33<15:12:22, 18.04s/it]
{'loss': 0.2, 'learning_rate': 1.8672625427153212e-06, 'rewards/chosen': -5.298373699188232, 'rewards/rejected': -8.04241943359375, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7440457344055176, 'policy_logps/rejected': -394.2044677734375, 'policy_logps/chosen': -326.0527648925781, 'referece_logps/rejected': -313.7802429199219, 'referece_logps/chosen': -273.0690612792969, 'logits/rejected': 0.35913288593292236, 'logits/chosen': 0.3431651294231415, 'epoch': 7.31}


 81%|████████  | 13078/16110 [15:22:08<15:06:35, 17.94s/it]
{'loss': 0.1389, 'learning_rate': 1.8670622876303704e-06, 'rewards/chosen': -5.6227922439575195, 'rewards/rejected': -9.871731758117676, 'rewards/accuracies': 1.0, 'rewards/margins': 4.248940467834473, 'policy_logps/rejected': -275.248046875, 'policy_logps/chosen': -311.1861572265625, 'referece_logps/rejected': -176.5307159423828, 'referece_logps/chosen': -254.95826721191406, 'logits/rejected': 0.4732968807220459, 'logits/chosen': 0.3779999911785126, 'epoch': 7.31}


 81%|████████  | 13080/16110 [15:22:46<15:37:28, 18.56s/it]

 81%|████████  | 13081/16110 [15:23:08<16:25:47, 19.53s/it]
{'loss': 0.1495, 'learning_rate': 1.8667616421584208e-06, 'rewards/chosen': -4.66534948348999, 'rewards/rejected': -9.217852592468262, 'rewards/accuracies': 1.0, 'rewards/margins': 4.55250358581543, 'policy_logps/rejected': -346.79425048828125, 'policy_logps/chosen': -273.7840270996094, 'referece_logps/rejected': -254.61572265625, 'referece_logps/chosen': -227.13052368164062, 'logits/rejected': -0.02528104931116104, 'logits/chosen': -0.04020599275827408, 'epoch': 7.31}


 81%|████████  | 13083/16110 [15:23:48<16:39:17, 19.81s/it]

 81%|████████  | 13084/16110 [15:24:10<17:20:16, 20.63s/it]

 81%|████████  | 13085/16110 [15:24:26<16:15:45, 19.35s/it]

 81%|████████  | 13086/16110 [15:24:41<15:04:41, 17.95s/it]

 81%|████████  | 13087/16110 [15:25:02<15:49:56, 18.85s/it]

 81%|████████  | 13088/16110 [15:25:24<16:29:33, 19.65s/it]

 81%|████████  | 13089/16110 [15:25:42<16:09:40, 19.26s/it]

 81%|████████▏ | 13090/16110 [15:25:56<14:50:50, 17.70s/it]
{'loss': 0.1548, 'learning_rate': 1.865857814283423e-06, 'rewards/chosen': -4.980340003967285, 'rewards/rejected': -11.391375541687012, 'rewards/accuracies': 1.0, 'rewards/margins': 6.411036014556885, 'policy_logps/rejected': -477.9317626953125, 'policy_logps/chosen': -347.7289733886719, 'referece_logps/rejected': -364.0180358886719, 'referece_logps/chosen': -297.9255676269531, 'logits/rejected': -0.21441757678985596, 'logits/chosen': -0.10522681474685669, 'epoch': 7.31}


 81%|████████▏ | 13092/16110 [15:26:30<14:39:08, 17.48s/it]

 81%|████████▏ | 13093/16110 [15:26:43<13:31:13, 16.13s/it]

 81%|████████▏ | 13094/16110 [15:27:04<14:38:07, 17.47s/it]

 81%|████████▏ | 13095/16110 [15:27:22<14:46:24, 17.64s/it]

 81%|████████▏ | 13096/16110 [15:27:42<15:16:56, 18.25s/it]

 81%|████████▏ | 13097/16110 [15:27:56<14:21:50, 17.16s/it]

 81%|████████▏ | 13098/16110 [15:28:12<13:56:00, 16.65s/it]

 81%|████████▏ | 13099/16110 [15:28:30<14:19:12, 17.12s/it]
{'loss': 0.0972, 'learning_rate': 1.864951151522507e-06, 'rewards/chosen': -3.9817075729370117, 'rewards/rejected': -8.285208702087402, 'rewards/accuracies': 1.0, 'rewards/margins': 4.303500652313232, 'policy_logps/rejected': -460.4882507324219, 'policy_logps/chosen': -422.1041259765625, 'referece_logps/rejected': -377.63616943359375, 'referece_logps/chosen': -382.287109375, 'logits/rejected': 0.20295464992523193, 'logits/chosen': 0.21317139267921448, 'epoch': 7.32}

 81%|████████▏ | 13100/16110 [15:28:41<12:50:01, 15.35s/it]


 81%|████████▏ | 13102/16110 [15:29:10<12:23:46, 14.84s/it]

 81%|████████▏ | 13103/16110 [15:29:30<13:40:03, 16.36s/it]

 81%|████████▏ | 13104/16110 [15:29:44<13:17:03, 15.91s/it]

 81%|████████▏ | 13105/16110 [15:30:01<13:23:12, 16.04s/it]

 81%|████████▏ | 13106/16110 [15:30:12<12:10:20, 14.59s/it]

 81%|████████▏ | 13107/16110 [15:30:26<11:56:17, 14.31s/it]
{'loss': 0.2655, 'learning_rate': 1.8641428515636864e-06, 'rewards/chosen': -3.9414095878601074, 'rewards/rejected': -8.191547393798828, 'rewards/accuracies': 1.0, 'rewards/margins': 4.2501373291015625, 'policy_logps/rejected': -314.3403015136719, 'policy_logps/chosen': -263.1260681152344, 'referece_logps/rejected': -232.4248046875, 'referece_logps/chosen': -223.7119598388672, 'logits/rejected': -0.17450715601444244, 'logits/chosen': -0.18737080693244934, 'epoch': 7.32}


 81%|████████▏ | 13109/16110 [15:31:05<13:48:44, 16.57s/it]

 81%|████████▏ | 13110/16110 [15:31:24<14:34:34, 17.49s/it]

 81%|████████▏ | 13111/16110 [15:31:46<15:42:10, 18.85s/it]
{'loss': 0.2125, 'learning_rate': 1.8637378631507963e-06, 'rewards/chosen': -3.8094558715820312, 'rewards/rejected': -8.481849670410156, 'rewards/accuracies': 1.0, 'rewards/margins': 4.672393798828125, 'policy_logps/rejected': -455.05975341796875, 'policy_logps/chosen': -472.98724365234375, 'referece_logps/rejected': -370.24127197265625, 'referece_logps/chosen': -434.8927001953125, 'logits/rejected': 1.136517882347107, 'logits/chosen': 0.9578208327293396, 'epoch': 7.32}


 81%|████████▏ | 13113/16110 [15:32:18<14:44:57, 17.72s/it]

 81%|████████▏ | 13114/16110 [15:32:40<15:36:12, 18.75s/it]
{'loss': 0.0962, 'learning_rate': 1.8634337552412443e-06, 'rewards/chosen': -4.162764549255371, 'rewards/rejected': -8.477984428405762, 'rewards/accuracies': 1.0, 'rewards/margins': 4.315218925476074, 'policy_logps/rejected': -399.2530517578125, 'policy_logps/chosen': -271.5503845214844, 'referece_logps/rejected': -314.4732360839844, 'referece_logps/chosen': -229.92276000976562, 'logits/rejected': -0.8413406610488892, 'logits/chosen': -0.7887850999832153, 'epoch': 7.33}

 81%|████████▏ | 13115/16110 [15:33:00<15:53:51, 19.11s/it]


 81%|████████▏ | 13117/16110 [15:33:38<16:01:20, 19.27s/it]

 81%|████████▏ | 13118/16110 [15:33:56<15:41:52, 18.89s/it]

 81%|████████▏ | 13119/16110 [15:34:11<14:42:23, 17.70s/it]
{'loss': 0.0987, 'learning_rate': 1.8629262107676511e-06, 'rewards/chosen': -3.5948948860168457, 'rewards/rejected': -9.145294189453125, 'rewards/accuracies': 1.0, 'rewards/margins': 5.550397872924805, 'policy_logps/rejected': -316.9547119140625, 'policy_logps/chosen': -254.63436889648438, 'referece_logps/rejected': -225.5017852783203, 'referece_logps/chosen': -218.68540954589844, 'logits/rejected': -0.04075451195240021, 'logits/chosen': 0.03486467897891998, 'epoch': 7.33}


 81%|████████▏ | 13121/16110 [15:34:47<14:38:15, 17.63s/it]

 81%|████████▏ | 13122/16110 [15:35:05<14:47:23, 17.82s/it]

 81%|████████▏ | 13123/16110 [15:35:23<14:43:59, 17.76s/it]

 81%|████████▏ | 13124/16110 [15:35:43<15:24:34, 18.58s/it]
{'loss': 0.1047, 'learning_rate': 1.8624177942915414e-06, 'rewards/chosen': -4.939846038818359, 'rewards/rejected': -10.213241577148438, 'rewards/accuracies': 1.0, 'rewards/margins': 5.27339506149292, 'policy_logps/rejected': -316.62933349609375, 'policy_logps/chosen': -288.4318542480469, 'referece_logps/rejected': -214.49693298339844, 'referece_logps/chosen': -239.03338623046875, 'logits/rejected': 0.15253369510173798, 'logits/chosen': 0.03724230080842972, 'epoch': 7.33}


 81%|████████▏ | 13126/16110 [15:36:25<16:20:04, 19.71s/it]

 81%|████████▏ | 13127/16110 [15:36:45<16:19:00, 19.69s/it]

 81%|████████▏ | 13128/16110 [15:37:05<16:20:32, 19.73s/it]

 81%|████████▏ | 13129/16110 [15:37:25<16:23:30, 19.80s/it]
{'loss': 0.0978, 'learning_rate': 1.8619085063266801e-06, 'rewards/chosen': -4.715059280395508, 'rewards/rejected': -9.281835556030273, 'rewards/accuracies': 1.0, 'rewards/margins': 4.566775798797607, 'policy_logps/rejected': -393.40216064453125, 'policy_logps/chosen': -219.85723876953125, 'referece_logps/rejected': -300.5837707519531, 'referece_logps/chosen': -172.70664978027344, 'logits/rejected': -1.0515844821929932, 'logits/chosen': -0.7681326866149902, 'epoch': 7.33}


 82%|████████▏ | 13131/16110 [15:37:58<14:46:59, 17.87s/it]

 82%|████████▏ | 13132/16110 [15:38:10<13:18:41, 16.09s/it]

 82%|████████▏ | 13133/16110 [15:38:25<13:03:51, 15.80s/it]

 82%|████████▏ | 13134/16110 [15:38:42<13:24:35, 16.22s/it]
{'loss': 0.0739, 'learning_rate': 1.861398347387711e-06, 'rewards/chosen': -3.879711627960205, 'rewards/rejected': -9.5618257522583, 'rewards/accuracies': 1.0, 'rewards/margins': 5.6821136474609375, 'policy_logps/rejected': -312.4518737792969, 'policy_logps/chosen': -455.27801513671875, 'referece_logps/rejected': -216.83364868164062, 'referece_logps/chosen': -416.4809265136719, 'logits/rejected': 0.10420062392950058, 'logits/chosen': 0.037486106157302856, 'epoch': 7.34}


 82%|████████▏ | 13136/16110 [15:39:15<13:33:17, 16.41s/it]

 82%|████████▏ | 13137/16110 [15:39:31<13:35:38, 16.46s/it]

 82%|████████▏ | 13138/16110 [15:39:51<14:26:35, 17.50s/it]
{'loss': 0.1877, 'learning_rate': 1.8609895934815815e-06, 'rewards/chosen': -5.232592582702637, 'rewards/rejected': -9.216078758239746, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9834868907928467, 'policy_logps/rejected': -424.462158203125, 'policy_logps/chosen': -389.06402587890625, 'referece_logps/rejected': -332.3013000488281, 'referece_logps/chosen': -336.73809814453125, 'logits/rejected': -0.33167120814323425, 'logits/chosen': -0.3920896351337433, 'epoch': 7.34}


 82%|████████▏ | 13140/16110 [15:40:23<13:38:10, 16.53s/it]
{'loss': 0.1919, 'learning_rate': 1.8607850077010472e-06, 'rewards/chosen': -4.02441930770874, 'rewards/rejected': -9.814556121826172, 'rewards/accuracies': 1.0, 'rewards/margins': 5.790136337280273, 'policy_logps/rejected': -388.7989807128906, 'policy_logps/chosen': -363.212646484375, 'referece_logps/rejected': -290.6534118652344, 'referece_logps/chosen': -322.96844482421875, 'logits/rejected': 0.12881901860237122, 'logits/chosen': 0.03885802999138832, 'epoch': 7.34}


 82%|████████▏ | 13142/16110 [15:41:01<14:24:52, 17.48s/it]

 82%|████████▏ | 13143/16110 [15:41:21<14:59:20, 18.19s/it]
{'loss': 0.2537, 'learning_rate': 1.8604778680889325e-06, 'rewards/chosen': -6.408456325531006, 'rewards/rejected': -9.629219055175781, 'rewards/accuracies': 0.875, 'rewards/margins': 3.2207629680633545, 'policy_logps/rejected': -515.975341796875, 'policy_logps/chosen': -433.891357421875, 'referece_logps/rejected': -419.6831359863281, 'referece_logps/chosen': -369.80682373046875, 'logits/rejected': 0.05772014707326889, 'logits/chosen': 0.10328322649002075, 'epoch': 7.34}

 82%|████████▏ | 13144/16110 [15:41:33<13:23:39, 16.26s/it]


 82%|████████▏ | 13146/16110 [15:42:05<13:36:11, 16.52s/it]

 82%|████████▏ | 13147/16110 [15:42:28<15:06:39, 18.36s/it]
{'loss': 0.0971, 'learning_rate': 1.8600678616894992e-06, 'rewards/chosen': -3.8769969940185547, 'rewards/rejected': -9.473113059997559, 'rewards/accuracies': 1.0, 'rewards/margins': 5.59611701965332, 'policy_logps/rejected': -435.5460205078125, 'policy_logps/chosen': -371.66448974609375, 'referece_logps/rejected': -340.8149108886719, 'referece_logps/chosen': -332.8945007324219, 'logits/rejected': 0.1554546058177948, 'logits/chosen': 0.16230028867721558, 'epoch': 7.34}

 82%|████████▏ | 13148/16110 [15:42:46<15:13:04, 18.50s/it]

 82%|████████▏ | 13149/16110 [15:43:03<14:38:59, 17.81s/it]

 82%|████████▏ | 13150/16110 [15:43:23<15:11:32, 18.48s/it]


 82%|████████▏ | 13152/16110 [15:43:56<14:00:18, 17.04s/it]
{'loss': 0.2672, 'learning_rate': 1.8595545715185991e-06, 'rewards/chosen': -3.682575225830078, 'rewards/rejected': -7.6344404220581055, 'rewards/accuracies': 0.75, 'rewards/margins': 3.9518654346466064, 'policy_logps/rejected': -333.4427490234375, 'policy_logps/chosen': -477.2214050292969, 'referece_logps/rejected': -257.0983581542969, 'referece_logps/chosen': -440.3956604003906, 'logits/rejected': 0.005078058689832687, 'logits/chosen': -0.09466855227947235, 'epoch': 7.35}


 82%|████████▏ | 13154/16110 [15:44:27<13:21:35, 16.27s/it]

 82%|████████▏ | 13155/16110 [15:44:49<14:46:34, 18.00s/it]
{'loss': 0.1263, 'learning_rate': 1.8592461804570278e-06, 'rewards/chosen': -6.1063551902771, 'rewards/rejected': -8.292872428894043, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1865172386169434, 'policy_logps/rejected': -514.5834350585938, 'policy_logps/chosen': -522.6255493164062, 'referece_logps/rejected': -431.65472412109375, 'referece_logps/chosen': -461.56201171875, 'logits/rejected': 0.2616443336009979, 'logits/chosen': 0.2011740356683731, 'epoch': 7.35}

 82%|████████▏ | 13156/16110 [15:45:00<13:10:00, 16.05s/it]

 82%|████████▏ | 13157/16110 [15:45:20<14:05:27, 17.18s/it]


 82%|████████▏ | 13159/16110 [15:45:58<14:46:18, 18.02s/it]
{'loss': 0.2153, 'learning_rate': 1.8588345061553974e-06, 'rewards/chosen': -4.47351598739624, 'rewards/rejected': -9.044029235839844, 'rewards/accuracies': 1.0, 'rewards/margins': 4.5705132484436035, 'policy_logps/rejected': -381.09197998046875, 'policy_logps/chosen': -409.76226806640625, 'referece_logps/rejected': -290.65167236328125, 'referece_logps/chosen': -365.0270690917969, 'logits/rejected': 0.4093376696109772, 'logits/chosen': 0.1377667337656021, 'epoch': 7.35}

 82%|████████▏ | 13160/16110 [15:46:13<14:02:57, 17.14s/it]


 82%|████████▏ | 13162/16110 [15:46:45<13:22:40, 16.34s/it]
{'loss': 0.2003, 'learning_rate': 1.8585253859101243e-06, 'rewards/chosen': -4.268599510192871, 'rewards/rejected': -7.945991039276123, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6773924827575684, 'policy_logps/rejected': -477.816162109375, 'policy_logps/chosen': -494.10528564453125, 'referece_logps/rejected': -398.3562316894531, 'referece_logps/chosen': -451.4193420410156, 'logits/rejected': -0.33507969975471497, 'logits/chosen': -0.45184752345085144, 'epoch': 7.35}

 82%|████████▏ | 13163/16110 [15:47:05<14:10:02, 17.31s/it]


 82%|████████▏ | 13165/16110 [15:47:42<14:26:06, 17.65s/it]

 82%|████████▏ | 13166/16110 [15:48:00<14:32:11, 17.78s/it]
{'loss': 0.1154, 'learning_rate': 1.8581127397714943e-06, 'rewards/chosen': -4.319301605224609, 'rewards/rejected': -8.062744140625, 'rewards/accuracies': 1.0, 'rewards/margins': 3.743441581726074, 'policy_logps/rejected': -521.1532592773438, 'policy_logps/chosen': -624.4389038085938, 'referece_logps/rejected': -440.5257873535156, 'referece_logps/chosen': -581.245849609375, 'logits/rejected': 0.4852710962295532, 'logits/chosen': 0.05866740271449089, 'epoch': 7.36}


 82%|████████▏ | 13168/16110 [15:48:36<14:40:11, 17.95s/it]

 82%|████████▏ | 13169/16110 [15:48:55<15:05:32, 18.47s/it]

 82%|████████▏ | 13170/16110 [15:49:17<15:52:43, 19.44s/it]

 82%|████████▏ | 13171/16110 [15:49:34<15:10:26, 18.59s/it]
{'loss': 0.1626, 'learning_rate': 1.8575961517049164e-06, 'rewards/chosen': -3.623746156692505, 'rewards/rejected': -6.513835430145264, 'rewards/accuracies': 0.75, 'rewards/margins': 2.890089750289917, 'policy_logps/rejected': -652.2489624023438, 'policy_logps/chosen': -419.132568359375, 'referece_logps/rejected': -587.1105346679688, 'referece_logps/chosen': -382.8951416015625, 'logits/rejected': -0.007348060607910156, 'logits/chosen': 0.07204559445381165, 'epoch': 7.36}


 82%|████████▏ | 13173/16110 [15:50:04<14:02:56, 17.22s/it]

 82%|████████▏ | 13174/16110 [15:50:24<14:42:03, 18.03s/it]

 82%|████████▏ | 13175/16110 [15:50:40<14:13:18, 17.44s/it]

 82%|████████▏ | 13176/16110 [15:50:58<14:20:24, 17.60s/it]
{'loss': 0.1788, 'learning_rate': 1.857078697021943e-06, 'rewards/chosen': -5.220111846923828, 'rewards/rejected': -10.789772987365723, 'rewards/accuracies': 0.875, 'rewards/margins': 5.5696611404418945, 'policy_logps/rejected': -491.8394775390625, 'policy_logps/chosen': -540.2645874023438, 'referece_logps/rejected': -383.94171142578125, 'referece_logps/chosen': -488.0635070800781, 'logits/rejected': -0.6327887773513794, 'logits/chosen': -0.7339394092559814, 'epoch': 7.36}

 82%|████████▏ | 13177/16110 [15:51:18<14:59:24, 18.40s/it]

 82%|████████▏ | 13178/16110 [15:51:29<13:05:12, 16.07s/it]


 82%|████████▏ | 13180/16110 [15:52:04<13:18:53, 16.36s/it]
{'loss': 0.2154, 'learning_rate': 1.8566641096631172e-06, 'rewards/chosen': -4.926011562347412, 'rewards/rejected': -8.503360748291016, 'rewards/accuracies': 0.75, 'rewards/margins': 3.577349901199341, 'policy_logps/rejected': -377.2565612792969, 'policy_logps/chosen': -396.4356994628906, 'referece_logps/rejected': -292.222900390625, 'referece_logps/chosen': -347.175537109375, 'logits/rejected': 0.1708366721868515, 'logits/chosen': -0.00941561907529831, 'epoch': 7.36}


 82%|████████▏ | 13182/16110 [15:52:41<14:17:57, 17.58s/it]

 82%|████████▏ | 13183/16110 [15:53:02<15:01:52, 18.49s/it]
{'loss': 0.1333, 'learning_rate': 1.8563528055459995e-06, 'rewards/chosen': -4.357726097106934, 'rewards/rejected': -11.040620803833008, 'rewards/accuracies': 1.0, 'rewards/margins': 6.682894229888916, 'policy_logps/rejected': -445.8979797363281, 'policy_logps/chosen': -444.2024230957031, 'referece_logps/rejected': -335.4917907714844, 'referece_logps/chosen': -400.6251525878906, 'logits/rejected': 0.5029634237289429, 'logits/chosen': 0.5012059211730957, 'epoch': 7.36}


 82%|████████▏ | 13185/16110 [15:53:40<15:24:25, 18.96s/it]

 82%|████████▏ | 13186/16110 [15:53:56<14:32:19, 17.90s/it]

 82%|████████▏ | 13187/16110 [15:54:12<14:13:42, 17.52s/it]
{'loss': 0.189, 'learning_rate': 1.855937248807822e-06, 'rewards/chosen': -3.693178653717041, 'rewards/rejected': -9.408495903015137, 'rewards/accuracies': 1.0, 'rewards/margins': 5.7153167724609375, 'policy_logps/rejected': -448.91552734375, 'policy_logps/chosen': -543.5579223632812, 'referece_logps/rejected': -354.83056640625, 'referece_logps/chosen': -506.62615966796875, 'logits/rejected': 0.06461423635482788, 'logits/chosen': -0.08011344075202942, 'epoch': 7.37}

 82%|████████▏ | 13188/16110 [15:54:25<13:10:49, 16.24s/it]


 82%|████████▏ | 13190/16110 [15:55:08<15:11:06, 18.72s/it]
{'loss': 0.1363, 'learning_rate': 1.8556252179646485e-06, 'rewards/chosen': -4.918333053588867, 'rewards/rejected': -7.934203624725342, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0158705711364746, 'policy_logps/rejected': -335.8511962890625, 'policy_logps/chosen': -502.9345703125, 'referece_logps/rejected': -256.5091552734375, 'referece_logps/chosen': -453.75128173828125, 'logits/rejected': 0.11675778031349182, 'logits/chosen': -0.09775623679161072, 'epoch': 7.37}

 82%|████████▏ | 13191/16110 [15:55:29<15:46:26, 19.45s/it]

 82%|████████▏ | 13192/16110 [15:55:51<16:20:21, 20.16s/it]

 82%|████████▏ | 13193/16110 [15:56:11<16:21:31, 20.19s/it]

 82%|████████▏ | 13194/16110 [15:56:31<16:22:33, 20.22s/it]

 82%|████████▏ | 13195/16110 [15:56:47<15:22:14, 18.98s/it]


 82%|████████▏ | 13197/16110 [15:57:26<15:28:15, 19.12s/it]
{'loss': 0.0925, 'learning_rate': 1.8548959357189618e-06, 'rewards/chosen': -3.7481491565704346, 'rewards/rejected': -8.376388549804688, 'rewards/accuracies': 1.0, 'rewards/margins': 4.628239154815674, 'policy_logps/rejected': -377.9892883300781, 'policy_logps/chosen': -416.6048889160156, 'referece_logps/rejected': -294.22540283203125, 'referece_logps/chosen': -379.1234130859375, 'logits/rejected': 0.04528022184967995, 'logits/chosen': -0.01598987728357315, 'epoch': 7.37}

 82%|████████▏ | 13198/16110 [15:57:37<13:28:28, 16.66s/it]

 82%|████████▏ | 13199/16110 [15:57:49<12:18:08, 15.21s/it]

 82%|████████▏ | 13200/16110 [15:58:09<13:29:27, 16.69s/it]

 82%|████████▏ | 13201/16110 [15:58:23<12:51:52, 15.92s/it]


 82%|████████▏ | 13203/16110 [15:59:02<14:24:27, 17.84s/it]
{'loss': 0.163, 'learning_rate': 1.8542694889315688e-06, 'rewards/chosen': -3.523421287536621, 'rewards/rejected': -7.5424957275390625, 'rewards/accuracies': 1.0, 'rewards/margins': 4.0190749168396, 'policy_logps/rejected': -444.492431640625, 'policy_logps/chosen': -392.3774719238281, 'referece_logps/rejected': -369.06744384765625, 'referece_logps/chosen': -357.1432800292969, 'logits/rejected': -0.061738692224025726, 'logits/chosen': -0.22505822777748108, 'epoch': 7.38}

 82%|████████▏ | 13204/16110 [15:59:19<14:08:31, 17.52s/it]


 82%|████████▏ | 13206/16110 [15:59:57<14:41:37, 18.22s/it]
{'loss': 0.1427, 'learning_rate': 1.853955799323285e-06, 'rewards/chosen': -2.46653151512146, 'rewards/rejected': -7.594210624694824, 'rewards/accuracies': 1.0, 'rewards/margins': 5.127678871154785, 'policy_logps/rejected': -331.8460693359375, 'policy_logps/chosen': -440.0909729003906, 'referece_logps/rejected': -255.9039764404297, 'referece_logps/chosen': -415.4256896972656, 'logits/rejected': -0.5001704692840576, 'logits/chosen': -0.797726035118103, 'epoch': 7.38}
[2024-04-06 07:08:03,361] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13207/16110 [16:00:12<13:55:01, 17.26s/it]

 82%|████████▏ | 13208/16110 [16:00:33<14:55:51, 18.52s/it]


 82%|████████▏ | 13210/16110 [16:01:07<14:26:58, 17.94s/it]

 82%|████████▏ | 13211/16110 [16:01:21<13:28:23, 16.73s/it]

 82%|████████▏ | 13212/16110 [16:01:34<12:40:18, 15.74s/it]

 82%|████████▏ | 13213/16110 [16:01:48<12:13:51, 15.20s/it]
{'loss': 0.1157, 'learning_rate': 1.8532226489884852e-06, 'rewards/chosen': -4.100594997406006, 'rewards/rejected': -10.333054542541504, 'rewards/accuracies': 0.875, 'rewards/margins': 6.232460021972656, 'policy_logps/rejected': -399.8436584472656, 'policy_logps/chosen': -459.54925537109375, 'referece_logps/rejected': -296.5130920410156, 'referece_logps/chosen': -418.5433044433594, 'logits/rejected': 0.11539791524410248, 'logits/chosen': 0.12994499504566193, 'epoch': 7.38}


 82%|████████▏ | 13215/16110 [16:02:25<13:39:39, 16.99s/it]

 82%|████████▏ | 13216/16110 [16:02:42<13:47:12, 17.15s/it]
{'loss': 0.1717, 'learning_rate': 1.8529079242992606e-06, 'rewards/chosen': -2.122063398361206, 'rewards/rejected': -7.556178569793701, 'rewards/accuracies': 1.0, 'rewards/margins': 5.434115886688232, 'policy_logps/rejected': -329.4826354980469, 'policy_logps/chosen': -275.2154541015625, 'referece_logps/rejected': -253.92083740234375, 'referece_logps/chosen': -253.9948272705078, 'logits/rejected': 0.0848550796508789, 'logits/chosen': -0.033153556287288666, 'epoch': 7.38}


 82%|████████▏ | 13218/16110 [16:03:18<14:18:20, 17.81s/it]
{'loss': 0.2088, 'learning_rate': 1.852697935456925e-06, 'rewards/chosen': -4.776875972747803, 'rewards/rejected': -10.041915893554688, 'rewards/accuracies': 0.875, 'rewards/margins': 5.265039443969727, 'policy_logps/rejected': -515.8406982421875, 'policy_logps/chosen': -397.99639892578125, 'referece_logps/rejected': -415.4215393066406, 'referece_logps/chosen': -350.2276611328125, 'logits/rejected': 0.2961433529853821, 'logits/chosen': 0.38229256868362427, 'epoch': 7.38}

 82%|████████▏ | 13219/16110 [16:03:36<14:18:13, 17.81s/it]


 82%|████████▏ | 13221/16110 [16:04:15<14:57:22, 18.64s/it]
{'loss': 0.1841, 'learning_rate': 1.8523826937040212e-06, 'rewards/chosen': -2.9309120178222656, 'rewards/rejected': -6.672635078430176, 'rewards/accuracies': 0.875, 'rewards/margins': 3.741722583770752, 'policy_logps/rejected': -263.5315246582031, 'policy_logps/chosen': -407.05126953125, 'referece_logps/rejected': -196.80517578125, 'referece_logps/chosen': -377.7421875, 'logits/rejected': 0.20241162180900574, 'logits/chosen': -0.07711157202720642, 'epoch': 7.39}

 82%|████████▏ | 13222/16110 [16:04:34<14:55:30, 18.60s/it]

 82%|████████▏ | 13223/16110 [16:04:53<15:10:02, 18.91s/it]


 82%|████████▏ | 13225/16110 [16:05:29<14:46:42, 18.44s/it]

 82%|████████▏ | 13226/16110 [16:05:48<15:06:38, 18.86s/it]
{'loss': 0.2077, 'learning_rate': 1.8518566017606799e-06, 'rewards/chosen': -4.993045330047607, 'rewards/rejected': -11.392454147338867, 'rewards/accuracies': 1.0, 'rewards/margins': 6.39940881729126, 'policy_logps/rejected': -528.500732421875, 'policy_logps/chosen': -416.7585144042969, 'referece_logps/rejected': -414.5762023925781, 'referece_logps/chosen': -366.82806396484375, 'logits/rejected': -0.11743228882551193, 'logits/chosen': -0.1508454829454422, 'epoch': 7.39}
[2024-04-06 07:14:00,856] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 82%|████████▏ | 13228/16110 [16:06:29<15:35:56, 19.49s/it]
{'loss': 0.2339, 'learning_rate': 1.8516459239249295e-06, 'rewards/chosen': -3.6717803478240967, 'rewards/rejected': -7.510430812835693, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8386502265930176, 'policy_logps/rejected': -349.0339660644531, 'policy_logps/chosen': -225.05001831054688, 'referece_logps/rejected': -273.92962646484375, 'referece_logps/chosen': -188.3322296142578, 'logits/rejected': -0.21861320734024048, 'logits/chosen': -0.17628124356269836, 'epoch': 7.39}

 82%|████████▏ | 13229/16110 [16:06:45<14:51:46, 18.57s/it]


 82%|████████▏ | 13231/16110 [16:07:25<15:30:05, 19.38s/it]
[2024-04-06 07:15:16,735] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13232/16110 [16:07:47<16:03:45, 20.09s/it]
[2024-04-06 07:15:38,481] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13233/16110 [16:08:07<16:05:33, 20.14s/it]
{'loss': 0.1205, 'learning_rate': 1.8511186269875711e-06, 'rewards/chosen': -4.264495372772217, 'rewards/rejected': -9.402803421020508, 'rewards/accuracies': 1.0, 'rewards/margins': 5.138308048248291, 'policy_logps/rejected': -585.6227416992188, 'policy_logps/chosen': -576.3361206054688, 'referece_logps/rejected': -491.59466552734375, 'referece_logps/chosen': -533.691162109375, 'logits/rejected': -0.541255533695221, 'logits/chosen': -0.3438017666339874, 'epoch': 7.39}

 82%|████████▏ | 13234/16110 [16:08:30<16:50:14, 21.08s/it]

 82%|████████▏ | 13235/16110 [16:08:48<15:54:25, 19.92s/it]

 82%|████████▏ | 13236/16110 [16:09:07<15:42:19, 19.67s/it]


 82%|████████▏ | 13238/16110 [16:09:43<14:49:23, 18.58s/it]
{'loss': 0.1449, 'learning_rate': 1.8505904699794733e-06, 'rewards/chosen': -3.693723678588867, 'rewards/rejected': -7.92232608795166, 'rewards/accuracies': 0.875, 'rewards/margins': 4.228602886199951, 'policy_logps/rejected': -420.64825439453125, 'policy_logps/chosen': -476.6145324707031, 'referece_logps/rejected': -341.4249572753906, 'referece_logps/chosen': -439.67730712890625, 'logits/rejected': 0.4446278214454651, 'logits/chosen': 0.3041197955608368, 'epoch': 7.4}


 82%|████████▏ | 13240/16110 [16:10:13<13:29:43, 16.93s/it]
{'loss': 0.1418, 'learning_rate': 1.8503789664759493e-06, 'rewards/chosen': -4.816908359527588, 'rewards/rejected': -10.130287170410156, 'rewards/accuracies': 1.0, 'rewards/margins': 5.313379764556885, 'policy_logps/rejected': -357.858642578125, 'policy_logps/chosen': -317.0948791503906, 'referece_logps/rejected': -256.5557556152344, 'referece_logps/chosen': -268.9258117675781, 'logits/rejected': -0.9215326309204102, 'logits/chosen': -1.103759527206421, 'epoch': 7.4}


 82%|████████▏ | 13242/16110 [16:10:49<14:14:21, 17.87s/it]
{'loss': 0.1707, 'learning_rate': 1.8501673254806878e-06, 'rewards/chosen': -4.6519951820373535, 'rewards/rejected': -7.55733585357666, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9053409099578857, 'policy_logps/rejected': -353.5244445800781, 'policy_logps/chosen': -419.9425354003906, 'referece_logps/rejected': -277.9510803222656, 'referece_logps/chosen': -373.422607421875, 'logits/rejected': -0.4306691884994507, 'logits/chosen': -0.44526830315589905, 'epoch': 7.4}
[2024-04-06 07:19:01,971] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13243/16110 [16:11:10<15:00:43, 18.85s/it]

 82%|████████▏ | 13244/16110 [16:11:30<15:07:52, 19.01s/it]

 82%|████████▏ | 13245/16110 [16:11:46<14:34:23, 18.31s/it]

 82%|████████▏ | 13246/16110 [16:12:08<15:27:04, 19.42s/it]
[2024-04-06 07:20:20,439] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13247/16110 [16:12:29<15:40:39, 19.71s/it]

 82%|████████▏ | 13248/16110 [16:12:51<16:16:14, 20.47s/it]

 82%|████████▏ | 13249/16110 [16:13:03<14:16:56, 17.97s/it]

 82%|████████▏ | 13250/16110 [16:13:19<13:40:47, 17.22s/it]

 82%|████████▏ | 13251/16110 [16:13:40<14:37:11, 18.41s/it]

 82%|████████▏ | 13252/16110 [16:13:52<13:09:47, 16.58s/it]

 82%|████████▏ | 13253/16110 [16:14:14<14:25:50, 18.18s/it]

 82%|████████▏ | 13254/16110 [16:14:34<14:50:27, 18.71s/it]

 82%|████████▏ | 13255/16110 [16:14:49<13:56:45, 17.59s/it]


 82%|████████▏ | 13257/16110 [16:15:32<15:31:34, 19.59s/it]
[2024-04-06 07:23:23,180] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1788, 'learning_rate': 1.8485756389246867e-06, 'rewards/chosen': -4.043780326843262, 'rewards/rejected': -10.530580520629883, 'rewards/accuracies': 1.0, 'rewards/margins': 6.486799716949463, 'policy_logps/rejected': -418.51788330078125, 'policy_logps/chosen': -366.2634582519531, 'referece_logps/rejected': -313.21209716796875, 'referece_logps/chosen': -325.8256530761719, 'logits/rejected': 0.037913188338279724, 'logits/chosen': -0.1375151127576828, 'epoch': 7.41}
[2024-04-06 07:23:42,706] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13258/16110 [16:15:51<15:30:18, 19.57s/it]

 82%|████████▏ | 13259/16110 [16:16:12<15:56:28, 20.13s/it]

 82%|████████▏ | 13260/16110 [16:16:30<15:25:49, 19.49s/it]

 82%|████████▏ | 13261/16110 [16:16:50<15:27:58, 19.54s/it]


 82%|████████▏ | 13263/16110 [16:17:18<13:06:43, 16.58s/it]

 82%|████████▏ | 13264/16110 [16:17:30<12:02:50, 15.24s/it]
{'loss': 0.2208, 'learning_rate': 1.847830209880568e-06, 'rewards/chosen': -5.753997802734375, 'rewards/rejected': -8.237032890319824, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4830355644226074, 'policy_logps/rejected': -403.2659912109375, 'policy_logps/chosen': -381.8150329589844, 'referece_logps/rejected': -320.8956604003906, 'referece_logps/chosen': -324.2750549316406, 'logits/rejected': -0.49558836221694946, 'logits/chosen': -0.5043055415153503, 'epoch': 7.41}

 82%|████████▏ | 13265/16110 [16:17:47<12:27:25, 15.76s/it]

 82%|████████▏ | 13266/16110 [16:18:07<13:28:34, 17.06s/it]

 82%|████████▏ | 13267/16110 [16:18:24<13:34:14, 17.18s/it]


 82%|████████▏ | 13269/16110 [16:18:58<13:24:51, 17.00s/it]
{'loss': 0.1384, 'learning_rate': 1.847296732379922e-06, 'rewards/chosen': -4.244946479797363, 'rewards/rejected': -7.711292743682861, 'rewards/accuracies': 0.875, 'rewards/margins': 3.46634578704834, 'policy_logps/rejected': -413.9303894042969, 'policy_logps/chosen': -275.3203125, 'referece_logps/rejected': -336.81744384765625, 'referece_logps/chosen': -232.87083435058594, 'logits/rejected': -0.400586873292923, 'logits/chosen': -0.22036795318126678, 'epoch': 7.41}

 82%|████████▏ | 13270/16110 [16:19:14<13:12:47, 16.75s/it]

 82%|████████▏ | 13271/16110 [16:19:35<14:16:05, 18.09s/it]
[2024-04-06 07:27:48,616] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13272/16110 [16:19:57<15:07:10, 19.18s/it]
[2024-04-06 07:28:06,228] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13273/16110 [16:20:15<14:44:36, 18.71s/it]
[2024-04-06 07:28:27,037] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13274/16110 [16:20:35<15:14:05, 19.34s/it]
[2024-04-06 07:28:48,514] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13275/16110 [16:20:57<15:44:05, 19.98s/it]

 82%|████████▏ | 13276/16110 [16:21:19<16:16:58, 20.68s/it]

 82%|████████▏ | 13277/16110 [16:21:39<16:02:13, 20.38s/it]

 82%|████████▏ | 13278/16110 [16:22:00<16:07:03, 20.49s/it]

 82%|████████▏ | 13279/16110 [16:22:12<14:17:34, 18.18s/it]

 82%|████████▏ | 13280/16110 [16:22:25<13:03:11, 16.60s/it]

 82%|████████▏ | 13281/16110 [16:22:45<13:41:25, 17.42s/it]

 82%|████████▏ | 13282/16110 [16:23:01<13:24:51, 17.08s/it]

 82%|████████▏ | 13283/16110 [16:23:19<13:32:45, 17.25s/it]

 82%|████████▏ | 13284/16110 [16:23:36<13:41:54, 17.45s/it]

 82%|████████▏ | 13285/16110 [16:23:56<14:07:11, 17.99s/it]

 82%|████████▏ | 13286/16110 [16:24:15<14:22:24, 18.32s/it]

 82%|████████▏ | 13287/16110 [16:24:31<13:47:36, 17.59s/it]

 82%|████████▏ | 13288/16110 [16:24:44<12:40:30, 16.17s/it]
[2024-04-06 07:32:57,180] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 82%|████████▏ | 13289/16110 [16:25:06<14:01:47, 17.90s/it]

 82%|████████▏ | 13290/16110 [16:25:18<12:51:31, 16.42s/it]

 83%|████████▎ | 13291/16110 [16:25:35<12:55:33, 16.51s/it]

 83%|████████▎ | 13292/16110 [16:25:55<13:41:25, 17.49s/it]


 83%|████████▎ | 13294/16110 [16:26:34<14:31:49, 18.58s/it]
{'loss': 0.1744, 'learning_rate': 1.8446165125590448e-06, 'rewards/chosen': -3.8866770267486572, 'rewards/rejected': -9.493868827819824, 'rewards/accuracies': 1.0, 'rewards/margins': 5.6071906089782715, 'policy_logps/rejected': -343.5242919921875, 'policy_logps/chosen': -253.42422485351562, 'referece_logps/rejected': -248.58560180664062, 'referece_logps/chosen': -214.55743408203125, 'logits/rejected': -0.1395331770181656, 'logits/chosen': -0.23862817883491516, 'epoch': 7.43}

 83%|████████▎ | 13295/16110 [16:26:47<13:14:55, 16.94s/it]

 83%|████████▎ | 13296/16110 [16:27:07<13:56:32, 17.84s/it]

 83%|████████▎ | 13297/16110 [16:27:29<14:54:17, 19.07s/it]

 83%|████████▎ | 13298/16110 [16:27:47<14:35:12, 18.67s/it]

 83%|████████▎ | 13299/16110 [16:28:07<14:47:44, 18.95s/it]

 83%|████████▎ | 13300/16110 [16:28:25<14:40:19, 18.80s/it]

 83%|████████▎ | 13301/16110 [16:28:45<14:50:48, 19.03s/it]

 83%|████████▎ | 13302/16110 [16:28:56<12:59:36, 16.66s/it]

 83%|████████▎ | 13303/16110 [16:29:18<14:22:01, 18.43s/it]
{'loss': 0.1323, 'learning_rate': 1.8436464064599234e-06, 'rewards/chosen': -4.1783013343811035, 'rewards/rejected': -9.713726043701172, 'rewards/accuracies': 1.0, 'rewards/margins': 5.535425186157227, 'policy_logps/rejected': -477.41290283203125, 'policy_logps/chosen': -484.4612121582031, 'referece_logps/rejected': -380.275634765625, 'referece_logps/chosen': -442.67822265625, 'logits/rejected': -0.3424658179283142, 'logits/chosen': -0.45902329683303833, 'epoch': 7.43}

 83%|████████▎ | 13304/16110 [16:29:35<13:58:11, 17.92s/it]

 83%|████████▎ | 13305/16110 [16:29:56<14:38:32, 18.79s/it]

 83%|████████▎ | 13306/16110 [16:30:12<14:01:45, 18.01s/it]

 83%|████████▎ | 13307/16110 [16:30:28<13:34:02, 17.43s/it]


 83%|████████▎ | 13309/16110 [16:31:08<14:31:40, 18.67s/it]
{'loss': 0.1181, 'learning_rate': 1.8429981343284982e-06, 'rewards/chosen': -4.1545000076293945, 'rewards/rejected': -8.721080780029297, 'rewards/accuracies': 1.0, 'rewards/margins': 4.566580772399902, 'policy_logps/rejected': -543.6824951171875, 'policy_logps/chosen': -559.9088745117188, 'referece_logps/rejected': -456.4716491699219, 'referece_logps/chosen': -518.3638916015625, 'logits/rejected': 0.28377991914749146, 'logits/chosen': 0.10644984245300293, 'epoch': 7.44}

 83%|████████▎ | 13310/16110 [16:31:26<14:20:54, 18.45s/it]

 83%|████████▎ | 13311/16110 [16:31:46<14:33:57, 18.73s/it]

 83%|████████▎ | 13312/16110 [16:31:56<12:39:51, 16.29s/it]

 83%|████████▎ | 13313/16110 [16:32:13<12:42:12, 16.35s/it]


 83%|████████▎ | 13315/16110 [16:32:40<11:37:06, 14.96s/it]

 83%|████████▎ | 13316/16110 [16:32:57<11:56:31, 15.39s/it]

 83%|████████▎ | 13317/16110 [16:33:14<12:27:08, 16.05s/it]

 83%|████████▎ | 13318/16110 [16:33:29<12:02:35, 15.53s/it]

 83%|████████▎ | 13319/16110 [16:33:47<12:46:43, 16.48s/it]

 83%|████████▎ | 13320/16110 [16:34:06<13:12:13, 17.04s/it]

 83%|████████▎ | 13321/16110 [16:34:26<13:58:49, 18.05s/it]

 83%|████████▎ | 13322/16110 [16:34:47<14:38:56, 18.92s/it]

 83%|████████▎ | 13323/16110 [16:35:03<13:58:45, 18.06s/it]

 83%|████████▎ | 13324/16110 [16:35:23<14:27:17, 18.68s/it]

 83%|████████▎ | 13325/16110 [16:35:43<14:44:56, 19.07s/it]

 83%|████████▎ | 13326/16110 [16:36:00<14:09:18, 18.30s/it]

 83%|████████▎ | 13327/16110 [16:36:20<14:32:05, 18.80s/it]

 83%|████████▎ | 13328/16110 [16:36:36<13:53:50, 17.98s/it]

 83%|████████▎ | 13329/16110 [16:36:48<12:33:41, 16.26s/it]

 83%|████████▎ | 13330/16110 [16:37:10<13:57:50, 18.08s/it]

 83%|████████▎ | 13331/16110 [16:37:31<14:41:43, 19.04s/it]
[2024-04-06 07:45:23,129] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 13332/16110 [16:37:51<14:46:03, 19.14s/it]

 83%|████████▎ | 13333/16110 [16:38:12<15:06:49, 19.59s/it]
[2024-04-06 07:46:03,157] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 13334/16110 [16:38:30<14:52:34, 19.29s/it]

 83%|████████▎ | 13335/16110 [16:38:52<15:32:07, 20.15s/it]

 83%|████████▎ | 13336/16110 [16:39:09<14:45:53, 19.16s/it]

 83%|████████▎ | 13337/16110 [16:39:20<12:48:08, 16.62s/it]

 83%|████████▎ | 13338/16110 [16:39:35<12:30:52, 16.25s/it]
{'loss': 0.1249, 'learning_rate': 1.839847543218325e-06, 'rewards/chosen': -3.4132564067840576, 'rewards/rejected': -9.622377395629883, 'rewards/accuracies': 1.0, 'rewards/margins': 6.209120750427246, 'policy_logps/rejected': -484.6087646484375, 'policy_logps/chosen': -480.6310729980469, 'referece_logps/rejected': -388.3849792480469, 'referece_logps/chosen': -446.49853515625, 'logits/rejected': -0.3618032932281494, 'logits/chosen': -0.3422280550003052, 'epoch': 7.45}


 83%|████████▎ | 13340/16110 [16:40:06<12:09:59, 15.81s/it]
{'loss': 0.0916, 'learning_rate': 1.8396292074801827e-06, 'rewards/chosen': -5.351946830749512, 'rewards/rejected': -9.344025611877441, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9920785427093506, 'policy_logps/rejected': -345.0989685058594, 'policy_logps/chosen': -406.2200927734375, 'referece_logps/rejected': -251.65869140625, 'referece_logps/chosen': -352.70062255859375, 'logits/rejected': 0.42286747694015503, 'logits/chosen': 0.23348882794380188, 'epoch': 7.45}


 83%|████████▎ | 13342/16110 [16:40:44<13:30:13, 17.56s/it]
{'loss': 0.1949, 'learning_rate': 1.8394107359883549e-06, 'rewards/chosen': -6.13065767288208, 'rewards/rejected': -10.76262092590332, 'rewards/accuracies': 0.875, 'rewards/margins': 4.631964683532715, 'policy_logps/rejected': -440.61444091796875, 'policy_logps/chosen': -473.6484375, 'referece_logps/rejected': -332.98822021484375, 'referece_logps/chosen': -412.34185791015625, 'logits/rejected': -0.3540370464324951, 'logits/chosen': -0.2786201238632202, 'epoch': 7.45}

 83%|████████▎ | 13343/16110 [16:41:05<14:20:53, 18.67s/it]


 83%|████████▎ | 13345/16110 [16:41:47<15:17:31, 19.91s/it]

 83%|████████▎ | 13346/16110 [16:42:09<15:39:50, 20.40s/it]

 83%|████████▎ | 13347/16110 [16:42:27<15:08:44, 19.73s/it]
{'loss': 0.3152, 'learning_rate': 1.8388639635682735e-06, 'rewards/chosen': -4.1624250411987305, 'rewards/rejected': -9.165289878845215, 'rewards/accuracies': 1.0, 'rewards/margins': 5.002864837646484, 'policy_logps/rejected': -346.3397521972656, 'policy_logps/chosen': -251.5967559814453, 'referece_logps/rejected': -254.68685913085938, 'referece_logps/chosen': -209.97250366210938, 'logits/rejected': -0.17121830582618713, 'logits/chosen': -0.20614871382713318, 'epoch': 7.46}
[2024-04-06 07:50:41,088] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 13349/16110 [16:43:07<15:06:34, 19.70s/it]

 83%|████████▎ | 13350/16110 [16:43:27<15:09:57, 19.78s/it]

 83%|████████▎ | 13351/16110 [16:43:43<14:15:56, 18.61s/it]

 83%|████████▎ | 13352/16110 [16:44:04<14:47:51, 19.32s/it]
[2024-04-06 07:51:55,659] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 13353/16110 [16:44:25<15:16:18, 19.94s/it]

 83%|████████▎ | 13354/16110 [16:44:44<14:50:41, 19.39s/it]
[2024-04-06 07:52:35,167] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 13355/16110 [16:45:00<14:12:07, 18.56s/it]

 83%|████████▎ | 13356/16110 [16:45:17<13:43:56, 17.95s/it]

 83%|████████▎ | 13357/16110 [16:45:35<13:48:14, 18.05s/it]

 83%|████████▎ | 13358/16110 [16:45:52<13:35:39, 17.78s/it]

 83%|████████▎ | 13359/16110 [16:46:13<14:13:21, 18.61s/it]

 83%|████████▎ | 13360/16110 [16:46:23<12:25:02, 16.26s/it]

 83%|████████▎ | 13361/16110 [16:46:40<12:29:03, 16.35s/it]

 83%|████████▎ | 13362/16110 [16:47:00<13:22:33, 17.52s/it]

 83%|████████▎ | 13363/16110 [16:47:17<13:10:09, 17.26s/it]

 83%|████████▎ | 13364/16110 [16:47:39<14:13:50, 18.66s/it]

 83%|████████▎ | 13365/16110 [16:48:00<14:43:03, 19.30s/it]

 83%|████████▎ | 13366/16110 [16:48:18<14:36:43, 19.17s/it]
[2024-04-06 07:56:10,123] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1971, 'learning_rate': 1.8367785021785237e-06, 'rewards/chosen': -5.206645965576172, 'rewards/rejected': -8.205364227294922, 'rewards/accuracies': 0.75, 'rewards/margins': 2.9987175464630127, 'policy_logps/rejected': -490.879638671875, 'policy_logps/chosen': -439.4212646484375, 'referece_logps/rejected': -408.82598876953125, 'referece_logps/chosen': -387.3547668457031, 'logits/rejected': 1.0861592292785645, 'logits/chosen': 1.0277512073516846, 'epoch': 7.47}


 83%|████████▎ | 13368/16110 [16:48:55<14:08:44, 18.57s/it]

 83%|████████▎ | 13369/16110 [16:49:06<12:29:32, 16.41s/it]

 83%|████████▎ | 13370/16110 [16:49:26<13:18:56, 17.50s/it]
{'loss': 0.1306, 'learning_rate': 1.8363379007866485e-06, 'rewards/chosen': -2.908968925476074, 'rewards/rejected': -8.114546775817871, 'rewards/accuracies': 0.875, 'rewards/margins': 5.205577850341797, 'policy_logps/rejected': -328.99273681640625, 'policy_logps/chosen': -346.8036193847656, 'referece_logps/rejected': -247.84725952148438, 'referece_logps/chosen': -317.7138977050781, 'logits/rejected': 0.14681535959243774, 'logits/chosen': -0.06949949264526367, 'epoch': 7.47}

 83%|████████▎ | 13371/16110 [16:49:46<13:47:58, 18.14s/it]
[2024-04-06 07:57:57,576] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 13373/16110 [16:50:22<13:31:42, 17.79s/it]

 83%|████████▎ | 13374/16110 [16:50:39<13:28:27, 17.73s/it]

 83%|████████▎ | 13375/16110 [16:50:55<13:06:55, 17.26s/it]

 83%|████████▎ | 13376/16110 [16:51:14<13:23:14, 17.63s/it]

 83%|████████▎ | 13377/16110 [16:51:33<13:46:16, 18.14s/it]

 83%|████████▎ | 13378/16110 [16:51:53<14:11:44, 18.71s/it]

 83%|████████▎ | 13379/16110 [16:52:10<13:51:45, 18.27s/it]

 83%|████████▎ | 13380/16110 [16:52:32<14:35:11, 19.23s/it]
{'loss': 0.1281, 'learning_rate': 1.8352340315543434e-06, 'rewards/chosen': -4.711780071258545, 'rewards/rejected': -9.506651878356934, 'rewards/accuracies': 0.875, 'rewards/margins': 4.794872283935547, 'policy_logps/rejected': -372.6677551269531, 'policy_logps/chosen': -374.18865966796875, 'referece_logps/rejected': -277.6011962890625, 'referece_logps/chosen': -327.07086181640625, 'logits/rejected': 0.0055765509605407715, 'logits/chosen': -0.1309063881635666, 'epoch': 7.47}


 83%|████████▎ | 13382/16110 [16:53:10<14:29:46, 19.13s/it]

 83%|████████▎ | 13383/16110 [16:53:30<14:39:55, 19.36s/it]

 83%|████████▎ | 13384/16110 [16:53:51<15:06:35, 19.95s/it]

 83%|████████▎ | 13385/16110 [16:54:07<14:06:58, 18.65s/it]

 83%|████████▎ | 13386/16110 [16:54:28<14:40:06, 19.39s/it]

 83%|████████▎ | 13387/16110 [16:54:46<14:27:07, 19.11s/it]

 83%|████████▎ | 13388/16110 [16:54:58<12:49:23, 16.96s/it]

 83%|████████▎ | 13389/16110 [16:55:18<13:27:59, 17.82s/it]

 83%|████████▎ | 13390/16110 [16:55:32<12:38:47, 16.74s/it]

 83%|████████▎ | 13391/16110 [16:55:49<12:36:20, 16.69s/it]

 83%|████████▎ | 13392/16110 [16:56:02<11:46:46, 15.60s/it]

 83%|████████▎ | 13393/16110 [16:56:21<12:41:01, 16.81s/it]

 83%|████████▎ | 13394/16110 [16:56:39<12:46:50, 16.94s/it]

 83%|████████▎ | 13395/16110 [16:57:00<13:46:18, 18.26s/it]

 83%|████████▎ | 13396/16110 [16:57:22<14:41:11, 19.48s/it]

 83%|████████▎ | 13397/16110 [16:57:40<14:15:56, 18.93s/it]

 83%|████████▎ | 13398/16110 [16:58:01<14:49:35, 19.68s/it]

 83%|████████▎ | 13399/16110 [16:58:17<13:50:41, 18.38s/it]

 83%|████████▎ | 13400/16110 [16:58:36<14:06:05, 18.73s/it]

 83%|████████▎ | 13401/16110 [16:58:54<13:54:28, 18.48s/it]

 83%|████████▎ | 13402/16110 [16:59:13<14:01:07, 18.64s/it]

 83%|████████▎ | 13403/16110 [16:59:33<14:13:58, 18.93s/it]

 83%|████████▎ | 13404/16110 [16:59:51<13:56:55, 18.56s/it]

 83%|████████▎ | 13405/16110 [17:00:05<12:58:30, 17.27s/it]

 83%|████████▎ | 13406/16110 [17:00:23<13:12:03, 17.58s/it]

 83%|████████▎ | 13407/16110 [17:00:44<14:02:38, 18.70s/it]

 83%|████████▎ | 13408/16110 [17:01:05<14:29:29, 19.31s/it]

 83%|████████▎ | 13409/16110 [17:01:25<14:31:13, 19.35s/it]

 83%|████████▎ | 13410/16110 [17:01:46<14:53:05, 19.85s/it]

 83%|████████▎ | 13411/16110 [17:01:59<13:29:18, 17.99s/it]

 83%|████████▎ | 13412/16110 [17:02:14<12:41:34, 16.94s/it]

 83%|████████▎ | 13413/16110 [17:02:30<12:36:53, 16.84s/it]

 83%|████████▎ | 13414/16110 [17:02:50<13:18:45, 17.78s/it]

 83%|████████▎ | 13415/16110 [17:03:05<12:42:52, 16.98s/it]

 83%|████████▎ | 13416/16110 [17:03:19<11:53:02, 15.88s/it]

 83%|████████▎ | 13417/16110 [17:03:30<10:55:52, 14.61s/it]

 83%|████████▎ | 13418/16110 [17:03:50<12:00:43, 16.06s/it]

 83%|████████▎ | 13419/16110 [17:04:04<11:32:20, 15.44s/it]

 83%|████████▎ | 13420/16110 [17:04:17<11:06:49, 14.87s/it]

 83%|████████▎ | 13421/16110 [17:04:37<12:12:37, 16.35s/it]

 83%|████████▎ | 13422/16110 [17:04:56<12:39:30, 16.95s/it]

 83%|████████▎ | 13423/16110 [17:05:14<13:02:53, 17.48s/it]

 83%|████████▎ | 13424/16110 [17:05:34<13:35:43, 18.22s/it]

 83%|████████▎ | 13425/16110 [17:05:51<13:14:06, 17.75s/it]

 83%|████████▎ | 13426/16110 [17:06:06<12:37:02, 16.92s/it]
{'loss': 0.1538, 'learning_rate': 1.8301128185454617e-06, 'rewards/chosen': -5.645246505737305, 'rewards/rejected': -7.261831283569336, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6165845394134521, 'policy_logps/rejected': -372.1072998046875, 'policy_logps/chosen': -278.05731201171875, 'referece_logps/rejected': -299.489013671875, 'referece_logps/chosen': -221.60484313964844, 'logits/rejected': 0.17231358587741852, 'logits/chosen': 0.2097269594669342, 'epoch': 7.5}
[2024-04-06 08:14:20,695] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 83%|████████▎ | 13428/16110 [17:06:45<13:28:30, 18.09s/it]
{'loss': 0.1029, 'learning_rate': 1.8298885433506563e-06, 'rewards/chosen': -3.769897699356079, 'rewards/rejected': -7.171164512634277, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4012675285339355, 'policy_logps/rejected': -287.16705322265625, 'policy_logps/chosen': -268.24090576171875, 'referece_logps/rejected': -215.4553985595703, 'referece_logps/chosen': -230.54193115234375, 'logits/rejected': -0.43524450063705444, 'logits/chosen': -0.45269516110420227, 'epoch': 7.5}

 83%|████████▎ | 13429/16110 [17:07:05<13:51:33, 18.61s/it]

 83%|████████▎ | 13430/16110 [17:07:22<13:21:39, 17.95s/it]

 83%|████████▎ | 13431/16110 [17:07:39<13:18:03, 17.87s/it]

 83%|████████▎ | 13432/16110 [17:07:56<12:58:15, 17.44s/it]

 83%|████████▎ | 13433/16110 [17:08:16<13:31:38, 18.19s/it]

 83%|████████▎ | 13434/16110 [17:08:35<13:45:17, 18.50s/it]

 83%|████████▎ | 13435/16110 [17:08:55<14:01:47, 18.88s/it]
[2024-04-06 08:16:46,385] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 83%|████████▎ | 13436/16110 [17:09:09<12:53:37, 17.36s/it]

 83%|████████▎ | 13437/16110 [17:09:24<12:34:04, 16.93s/it]

 83%|████████▎ | 13438/16110 [17:09:38<11:51:12, 15.97s/it]

 83%|████████▎ | 13439/16110 [17:09:50<10:55:14, 14.72s/it]
{'loss': 0.1614, 'learning_rate': 1.8286526323067672e-06, 'rewards/chosen': -3.5620107650756836, 'rewards/rejected': -7.775688648223877, 'rewards/accuracies': 1.0, 'rewards/margins': 4.213678359985352, 'policy_logps/rejected': -347.35064697265625, 'policy_logps/chosen': -427.84490966796875, 'referece_logps/rejected': -269.5937194824219, 'referece_logps/chosen': -392.22479248046875, 'logits/rejected': -0.060392118990421295, 'logits/chosen': -0.16657927632331848, 'epoch': 7.51}

 83%|████████▎ | 13440/16110 [17:10:02<10:17:12, 13.87s/it]


 83%|████████▎ | 13442/16110 [17:10:40<12:17:05, 16.58s/it]

 83%|████████▎ | 13443/16110 [17:10:53<11:19:10, 15.28s/it]

 83%|████████▎ | 13444/16110 [17:11:09<11:35:10, 15.65s/it]
{'loss': 0.1559, 'learning_rate': 1.8280895144064132e-06, 'rewards/chosen': -5.0692949295043945, 'rewards/rejected': -8.356155395507812, 'rewards/accuracies': 1.0, 'rewards/margins': 3.286860466003418, 'policy_logps/rejected': -648.7054443359375, 'policy_logps/chosen': -477.7855529785156, 'referece_logps/rejected': -565.1438598632812, 'referece_logps/chosen': -427.09259033203125, 'logits/rejected': -0.3716996908187866, 'logits/chosen': -0.3331989645957947, 'epoch': 7.51}

 83%|████████▎ | 13445/16110 [17:11:27<12:07:48, 16.39s/it]

 83%|████████▎ | 13446/16110 [17:11:48<13:02:59, 17.63s/it]


 83%|████████▎ | 13448/16110 [17:12:29<14:09:08, 19.14s/it]

 83%|████████▎ | 13449/16110 [17:12:49<14:19:06, 19.37s/it]

 83%|████████▎ | 13450/16110 [17:13:05<13:34:30, 18.37s/it]

 83%|████████▎ | 13451/16110 [17:13:23<13:37:13, 18.44s/it]

 84%|████████▎ | 13452/16110 [17:13:43<13:49:34, 18.73s/it]

 84%|████████▎ | 13453/16110 [17:14:02<14:00:40, 18.98s/it]
{'loss': 0.0936, 'learning_rate': 1.8270737938342095e-06, 'rewards/chosen': -4.720266819000244, 'rewards/rejected': -9.400469779968262, 'rewards/accuracies': 1.0, 'rewards/margins': 4.680203914642334, 'policy_logps/rejected': -434.8367919921875, 'policy_logps/chosen': -306.3682861328125, 'referece_logps/rejected': -340.8321228027344, 'referece_logps/chosen': -259.1656188964844, 'logits/rejected': -0.2424788922071457, 'logits/chosen': -0.058351367712020874, 'epoch': 7.52}
[2024-04-06 08:22:15,588] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 13454/16110 [17:14:24<14:35:32, 19.78s/it]
[2024-04-06 08:22:33,831] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 13455/16110 [17:14:42<14:14:49, 19.32s/it]

 84%|████████▎ | 13456/16110 [17:15:02<14:17:44, 19.39s/it]


 84%|████████▎ | 13458/16110 [17:15:37<13:40:38, 18.57s/it]

 84%|████████▎ | 13459/16110 [17:15:57<13:56:58, 18.94s/it]

 84%|████████▎ | 13460/16110 [17:16:15<13:52:23, 18.85s/it]
[2024-04-06 08:24:06,958] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0745, 'learning_rate': 1.826281916642925e-06, 'rewards/chosen': -5.202092170715332, 'rewards/rejected': -11.141018867492676, 'rewards/accuracies': 1.0, 'rewards/margins': 5.93892765045166, 'policy_logps/rejected': -594.7235107421875, 'policy_logps/chosen': -417.12164306640625, 'referece_logps/rejected': -483.3133544921875, 'referece_logps/chosen': -365.1007080078125, 'logits/rejected': -0.11778968572616577, 'logits/chosen': 0.17293718457221985, 'epoch': 7.52}

 84%|████████▎ | 13461/16110 [17:16:32<13:22:17, 18.17s/it]


 84%|████████▎ | 13463/16110 [17:16:55<10:49:23, 14.72s/it]
{'loss': 0.1576, 'learning_rate': 1.8259420396285991e-06, 'rewards/chosen': -3.399226188659668, 'rewards/rejected': -9.532090187072754, 'rewards/accuracies': 1.0, 'rewards/margins': 6.132864952087402, 'policy_logps/rejected': -519.7405395507812, 'policy_logps/chosen': -350.1646423339844, 'referece_logps/rejected': -424.419677734375, 'referece_logps/chosen': -316.17236328125, 'logits/rejected': -0.5736938118934631, 'logits/chosen': -0.2845829129219055, 'epoch': 7.52}

 84%|████████▎ | 13464/16110 [17:17:10<10:59:50, 14.96s/it]

 84%|████████▎ | 13465/16110 [17:17:24<10:40:29, 14.53s/it]

 84%|████████▎ | 13466/16110 [17:17:42<11:24:48, 15.54s/it]

 84%|████████▎ | 13467/16110 [17:17:54<10:37:08, 14.46s/it]

 84%|████████▎ | 13468/16110 [17:18:14<11:55:58, 16.26s/it]


 84%|████████▎ | 13470/16110 [17:18:51<12:33:30, 17.13s/it]

 84%|████████▎ | 13471/16110 [17:19:13<13:44:56, 18.76s/it]
[2024-04-06 08:27:04,945] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1252, 'learning_rate': 1.825034232312173e-06, 'rewards/chosen': -3.849998950958252, 'rewards/rejected': -10.308938980102539, 'rewards/accuracies': 1.0, 'rewards/margins': 6.458939552307129, 'policy_logps/rejected': -449.92010498046875, 'policy_logps/chosen': -422.3431091308594, 'referece_logps/rejected': -346.83074951171875, 'referece_logps/chosen': -383.8431091308594, 'logits/rejected': 0.5124779939651489, 'logits/chosen': 0.46762603521347046, 'epoch': 7.53}


 84%|████████▎ | 13473/16110 [17:19:49<13:31:41, 18.47s/it]
[2024-04-06 08:27:40,900] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▎ | 13474/16110 [17:20:08<13:28:35, 18.40s/it]

 84%|████████▎ | 13475/16110 [17:20:25<13:19:52, 18.21s/it]
{'loss': 0.105, 'learning_rate': 1.8245795281435644e-06, 'rewards/chosen': -4.392022609710693, 'rewards/rejected': -10.134822845458984, 'rewards/accuracies': 1.0, 'rewards/margins': 5.742800712585449, 'policy_logps/rejected': -490.3875732421875, 'policy_logps/chosen': -502.19403076171875, 'referece_logps/rejected': -389.03936767578125, 'referece_logps/chosen': -458.2738342285156, 'logits/rejected': 0.055333204567432404, 'logits/chosen': 0.11398961395025253, 'epoch': 7.53}

 84%|████████▎ | 13476/16110 [17:20:40<12:36:33, 17.23s/it]
[2024-04-06 08:28:51,962] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▎ | 13478/16110 [17:21:16<12:38:04, 17.28s/it]

 84%|████████▎ | 13479/16110 [17:21:29<11:43:45, 16.05s/it]

 84%|████████▎ | 13480/16110 [17:21:45<11:40:56, 15.99s/it]
{'loss': 0.112, 'learning_rate': 1.8240103980400236e-06, 'rewards/chosen': -5.167156219482422, 'rewards/rejected': -8.509106636047363, 'rewards/accuracies': 0.75, 'rewards/margins': 3.341949939727783, 'policy_logps/rejected': -416.4020690917969, 'policy_logps/chosen': -412.5494384765625, 'referece_logps/rejected': -331.31103515625, 'referece_logps/chosen': -360.8778381347656, 'logits/rejected': -0.7447447180747986, 'logits/chosen': -0.7196303009986877, 'epoch': 7.53}

 84%|████████▎ | 13481/16110 [17:22:04<12:29:41, 17.11s/it]

 84%|████████▎ | 13482/16110 [17:22:17<11:23:27, 15.60s/it]


 84%|████████▎ | 13484/16110 [17:22:54<12:35:07, 17.25s/it]

 84%|████████▎ | 13485/16110 [17:23:12<12:41:38, 17.41s/it]
{'loss': 0.1297, 'learning_rate': 1.8234404352590983e-06, 'rewards/chosen': -4.427999019622803, 'rewards/rejected': -9.91964340209961, 'rewards/accuracies': 1.0, 'rewards/margins': 5.491644859313965, 'policy_logps/rejected': -312.8583679199219, 'policy_logps/chosen': -275.5592956542969, 'referece_logps/rejected': -213.66195678710938, 'referece_logps/chosen': -231.27932739257812, 'logits/rejected': -0.06585302948951721, 'logits/chosen': 0.008651770651340485, 'epoch': 7.53}


 84%|████████▎ | 13487/16110 [17:23:45<12:16:16, 16.84s/it]

 84%|████████▎ | 13488/16110 [17:24:03<12:38:02, 17.35s/it]
{'loss': 0.098, 'learning_rate': 1.823098058144962e-06, 'rewards/chosen': -4.8518195152282715, 'rewards/rejected': -10.1367826461792, 'rewards/accuracies': 1.0, 'rewards/margins': 5.284963607788086, 'policy_logps/rejected': -491.9231262207031, 'policy_logps/chosen': -324.75506591796875, 'referece_logps/rejected': -390.5552978515625, 'referece_logps/chosen': -276.2369079589844, 'logits/rejected': -0.1671038120985031, 'logits/chosen': -0.116307832300663, 'epoch': 7.54}

 84%|████████▎ | 13489/16110 [17:24:19<12:10:48, 16.73s/it]

 84%|████████▎ | 13490/16110 [17:24:38<12:48:15, 17.59s/it]
[2024-04-06 08:32:50,583] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▎ | 13492/16110 [17:25:13<12:30:04, 17.19s/it]
{'loss': 0.1621, 'learning_rate': 1.82264108956458e-06, 'rewards/chosen': -4.368423938751221, 'rewards/rejected': -7.330539226531982, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9621152877807617, 'policy_logps/rejected': -393.9041748046875, 'policy_logps/chosen': -385.0474853515625, 'referece_logps/rejected': -320.5988464355469, 'referece_logps/chosen': -341.36322021484375, 'logits/rejected': -0.33877235651016235, 'logits/chosen': -0.3054279386997223, 'epoch': 7.54}


 84%|████████▍ | 13494/16110 [17:25:50<13:11:33, 18.16s/it]

 84%|████████▍ | 13495/16110 [17:26:10<13:36:02, 18.72s/it]

 84%|████████▍ | 13496/16110 [17:26:30<13:51:28, 19.09s/it]
{'loss': 0.1089, 'learning_rate': 1.8221835889562306e-06, 'rewards/chosen': -5.490821838378906, 'rewards/rejected': -8.30352783203125, 'rewards/accuracies': 0.875, 'rewards/margins': 2.81270694732666, 'policy_logps/rejected': -326.68475341796875, 'policy_logps/chosen': -421.4615478515625, 'referece_logps/rejected': -243.6494903564453, 'referece_logps/chosen': -366.5533752441406, 'logits/rejected': 0.5608170032501221, 'logits/chosen': 0.43215930461883545, 'epoch': 7.54}

 84%|████████▍ | 13497/16110 [17:26:47<13:25:14, 18.49s/it]


 84%|████████▍ | 13499/16110 [17:27:26<13:53:10, 19.15s/it]

 84%|████████▍ | 13500/16110 [17:27:38<12:10:19, 16.79s/it]

 84%|████████▍ | 13501/16110 [17:28:16<16:46:26, 23.15s/it]
{'loss': 0.1239, 'learning_rate': 1.8216109654822524e-06, 'rewards/chosen': -4.198023796081543, 'rewards/rejected': -8.517607688903809, 'rewards/accuracies': 1.0, 'rewards/margins': 4.319584369659424, 'policy_logps/rejected': -606.1312866210938, 'policy_logps/chosen': -551.7672119140625, 'referece_logps/rejected': -520.9552612304688, 'referece_logps/chosen': -509.78692626953125, 'logits/rejected': -0.3126986622810364, 'logits/chosen': -0.3609214723110199, 'epoch': 7.54}

 84%|████████▍ | 13502/16110 [17:28:33<15:31:05, 21.42s/it]

 84%|████████▍ | 13503/16110 [17:28:53<15:11:15, 20.97s/it]

 84%|████████▍ | 13504/16110 [17:29:07<13:40:16, 18.89s/it]

 84%|████████▍ | 13505/16110 [17:29:27<13:57:36, 19.29s/it]


 84%|████████▍ | 13507/16110 [17:29:58<12:26:56, 17.22s/it]

 84%|████████▍ | 13508/16110 [17:30:16<12:38:10, 17.48s/it]

 84%|████████▍ | 13509/16110 [17:30:36<13:16:45, 18.38s/it]
[2024-04-06 08:38:27,895] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1408, 'learning_rate': 1.8206930412392714e-06, 'rewards/chosen': -5.185855388641357, 'rewards/rejected': -10.589143753051758, 'rewards/accuracies': 1.0, 'rewards/margins': 5.403288841247559, 'policy_logps/rejected': -530.9518432617188, 'policy_logps/chosen': -434.176025390625, 'referece_logps/rejected': -425.0604248046875, 'referece_logps/chosen': -382.3174743652344, 'logits/rejected': 0.7550228834152222, 'logits/chosen': 0.7012944221496582, 'epoch': 7.55}


 84%|████████▍ | 13511/16110 [17:31:16<13:57:33, 19.34s/it]
[2024-04-06 08:39:07,403] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 13512/16110 [17:31:34<13:42:52, 19.00s/it]
[2024-04-06 08:39:25,633] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1465, 'learning_rate': 1.8203482721659182e-06, 'rewards/chosen': -5.259734153747559, 'rewards/rejected': -8.044459342956543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.784724473953247, 'policy_logps/rejected': -335.6827087402344, 'policy_logps/chosen': -485.52899169921875, 'referece_logps/rejected': -255.2381134033203, 'referece_logps/chosen': -432.931640625, 'logits/rejected': -0.12065895646810532, 'logits/chosen': -0.3295915126800537, 'epoch': 7.55}


 84%|████████▍ | 13514/16110 [17:32:12<13:42:30, 19.01s/it]

 84%|████████▍ | 13515/16110 [17:32:32<13:51:19, 19.22s/it]

 84%|████████▍ | 13516/16110 [17:32:50<13:38:52, 18.94s/it]

 84%|████████▍ | 13517/16110 [17:33:10<13:46:11, 19.12s/it]
[2024-04-06 08:41:01,181] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1978, 'learning_rate': 1.819772993924221e-06, 'rewards/chosen': -4.617227554321289, 'rewards/rejected': -9.17529296875, 'rewards/accuracies': 1.0, 'rewards/margins': 4.558064937591553, 'policy_logps/rejected': -377.3259582519531, 'policy_logps/chosen': -408.2099304199219, 'referece_logps/rejected': -285.5730285644531, 'referece_logps/chosen': -362.03765869140625, 'logits/rejected': 0.23958882689476013, 'logits/chosen': 0.10572688281536102, 'epoch': 7.55}


 84%|████████▍ | 13519/16110 [17:33:50<14:06:35, 19.60s/it]
{'loss': 0.0958, 'learning_rate': 1.8195426506442444e-06, 'rewards/chosen': -3.545793294906616, 'rewards/rejected': -8.441926002502441, 'rewards/accuracies': 1.0, 'rewards/margins': 4.896132946014404, 'policy_logps/rejected': -580.5721435546875, 'policy_logps/chosen': -504.0321350097656, 'referece_logps/rejected': -496.1528625488281, 'referece_logps/chosen': -468.57421875, 'logits/rejected': 0.4056156575679779, 'logits/chosen': 0.4494982361793518, 'epoch': 7.55}

 84%|████████▍ | 13520/16110 [17:34:01<12:11:38, 16.95s/it]

 84%|████████▍ | 13521/16110 [17:34:17<12:00:10, 16.69s/it]

 84%|████████▍ | 13522/16110 [17:34:27<10:41:40, 14.88s/it]


 84%|████████▍ | 13524/16110 [17:35:05<11:51:16, 16.50s/it]

 84%|████████▍ | 13525/16110 [17:35:17<10:54:49, 15.20s/it]

 84%|████████▍ | 13526/16110 [17:35:36<11:51:21, 16.52s/it]
{'loss': 0.1768, 'learning_rate': 1.818735405923917e-06, 'rewards/chosen': -4.75739860534668, 'rewards/rejected': -7.5691070556640625, 'rewards/accuracies': 0.875, 'rewards/margins': 2.811708927154541, 'policy_logps/rejected': -460.80743408203125, 'policy_logps/chosen': -449.094482421875, 'referece_logps/rejected': -385.1163330078125, 'referece_logps/chosen': -401.5205078125, 'logits/rejected': 0.020887285470962524, 'logits/chosen': 0.13906234502792358, 'epoch': 7.56}


 84%|████████▍ | 13528/16110 [17:36:17<13:16:01, 18.50s/it]
{'loss': 0.1415, 'learning_rate': 1.818504466660388e-06, 'rewards/chosen': -5.340103626251221, 'rewards/rejected': -8.901347160339355, 'rewards/accuracies': 0.875, 'rewards/margins': 3.561244010925293, 'policy_logps/rejected': -352.59808349609375, 'policy_logps/chosen': -261.6549072265625, 'referece_logps/rejected': -263.5846252441406, 'referece_logps/chosen': -208.25384521484375, 'logits/rejected': -0.3761502802371979, 'logits/chosen': -0.29838261008262634, 'epoch': 7.56}
[2024-04-06 08:44:30,736] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▍ | 13530/16110 [17:36:56<13:31:33, 18.87s/it]

 84%|████████▍ | 13531/16110 [17:37:16<13:50:26, 19.32s/it]

 84%|████████▍ | 13532/16110 [17:37:32<13:03:52, 18.24s/it]
[2024-04-06 08:45:23,811] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1875, 'learning_rate': 1.818042191156162e-06, 'rewards/chosen': -5.209290981292725, 'rewards/rejected': -11.267647743225098, 'rewards/accuracies': 1.0, 'rewards/margins': 6.058357238769531, 'policy_logps/rejected': -504.33917236328125, 'policy_logps/chosen': -416.31146240234375, 'referece_logps/rejected': -391.6626892089844, 'referece_logps/chosen': -364.2185363769531, 'logits/rejected': 0.4498714208602905, 'logits/chosen': 0.3995722532272339, 'epoch': 7.56}

 84%|████████▍ | 13533/16110 [17:37:49<12:50:13, 17.93s/it]


 84%|████████▍ | 13535/16110 [17:38:29<13:29:00, 18.85s/it]
{'loss': 0.1734, 'learning_rate': 1.81769513732013e-06, 'rewards/chosen': -4.514847278594971, 'rewards/rejected': -11.587255477905273, 'rewards/accuracies': 1.0, 'rewards/margins': 7.0724077224731445, 'policy_logps/rejected': -515.018310546875, 'policy_logps/chosen': -429.54052734375, 'referece_logps/rejected': -399.1458435058594, 'referece_logps/chosen': -384.39208984375, 'logits/rejected': 0.265861451625824, 'logits/chosen': 0.23675402998924255, 'epoch': 7.56}


 84%|████████▍ | 13537/16110 [17:39:01<12:45:00, 17.84s/it]
{'loss': 0.1986, 'learning_rate': 1.8174636028291617e-06, 'rewards/chosen': -5.251830577850342, 'rewards/rejected': -8.062040328979492, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8102102279663086, 'policy_logps/rejected': -297.12603759765625, 'policy_logps/chosen': -303.3792724609375, 'referece_logps/rejected': -216.505615234375, 'referece_logps/chosen': -250.86099243164062, 'logits/rejected': 0.7383672595024109, 'logits/chosen': 0.5911613702774048, 'epoch': 7.56}


 84%|████████▍ | 13539/16110 [17:39:42<13:56:51, 19.53s/it]
[2024-04-06 08:47:33,977] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1296, 'learning_rate': 1.8172319361683073e-06, 'rewards/chosen': -5.353107452392578, 'rewards/rejected': -8.819906234741211, 'rewards/accuracies': 0.875, 'rewards/margins': 3.466798782348633, 'policy_logps/rejected': -408.3998718261719, 'policy_logps/chosen': -374.4239501953125, 'referece_logps/rejected': -320.2008056640625, 'referece_logps/chosen': -320.892822265625, 'logits/rejected': 0.552120566368103, 'logits/chosen': 0.353254497051239, 'epoch': 7.56}

 84%|████████▍ | 13540/16110 [17:39:54<12:13:54, 17.13s/it]

 84%|████████▍ | 13541/16110 [17:40:14<12:46:44, 17.91s/it]


 84%|████████▍ | 13543/16110 [17:40:50<12:59:43, 18.22s/it]
{'loss': 0.2512, 'learning_rate': 1.8167682064867875e-06, 'rewards/chosen': -3.956355571746826, 'rewards/rejected': -11.020713806152344, 'rewards/accuracies': 1.0, 'rewards/margins': 7.064358711242676, 'policy_logps/rejected': -395.93255615234375, 'policy_logps/chosen': -453.50146484375, 'referece_logps/rejected': -285.72540283203125, 'referece_logps/chosen': -413.9378967285156, 'logits/rejected': 0.5137922167778015, 'logits/chosen': 0.32270747423171997, 'epoch': 7.57}
[2024-04-06 08:49:03,318] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 84%|████████▍ | 13545/16110 [17:41:33<14:05:27, 19.78s/it]
[2024-04-06 08:49:24,641] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 13546/16110 [17:41:47<12:51:54, 18.06s/it]
{'loss': 0.1406, 'learning_rate': 1.8164200625584347e-06, 'rewards/chosen': -3.015831470489502, 'rewards/rejected': -8.330425262451172, 'rewards/accuracies': 1.0, 'rewards/margins': 5.31459379196167, 'policy_logps/rejected': -249.74798583984375, 'policy_logps/chosen': -403.4211120605469, 'referece_logps/rejected': -166.44374084472656, 'referece_logps/chosen': -373.2628173828125, 'logits/rejected': 0.10806101560592651, 'logits/chosen': -0.08561880141496658, 'epoch': 7.57}

 84%|████████▍ | 13547/16110 [17:41:58<11:18:55, 15.89s/it]

 84%|████████▍ | 13548/16110 [17:42:13<11:12:31, 15.75s/it]


 84%|████████▍ | 13550/16110 [17:42:45<11:19:54, 15.94s/it]
{'loss': 0.1229, 'learning_rate': 1.8159554086718058e-06, 'rewards/chosen': -3.7155771255493164, 'rewards/rejected': -7.857234001159668, 'rewards/accuracies': 0.875, 'rewards/margins': 4.141657829284668, 'policy_logps/rejected': -308.5851135253906, 'policy_logps/chosen': -407.582763671875, 'referece_logps/rejected': -230.0127716064453, 'referece_logps/chosen': -370.427001953125, 'logits/rejected': -0.5839205384254456, 'logits/chosen': -0.753548264503479, 'epoch': 7.57}

 84%|████████▍ | 13551/16110 [17:43:04<12:06:36, 17.04s/it]

 84%|████████▍ | 13552/16110 [17:43:24<12:38:41, 17.80s/it]

 84%|████████▍ | 13553/16110 [17:43:44<13:03:36, 18.39s/it]


 84%|████████▍ | 13555/16110 [17:44:09<11:03:50, 15.59s/it]

 84%|████████▍ | 13556/16110 [17:44:31<12:20:10, 17.39s/it]
{'loss': 0.1612, 'learning_rate': 1.8152574384906267e-06, 'rewards/chosen': -4.466579914093018, 'rewards/rejected': -8.865568161010742, 'rewards/accuracies': 1.0, 'rewards/margins': 4.398987770080566, 'policy_logps/rejected': -392.0399169921875, 'policy_logps/chosen': -413.025390625, 'referece_logps/rejected': -303.38427734375, 'referece_logps/chosen': -368.3595886230469, 'logits/rejected': 0.02470645308494568, 'logits/chosen': 0.17172101140022278, 'epoch': 7.57}

 84%|████████▍ | 13557/16110 [17:44:46<11:57:29, 16.86s/it]

 84%|████████▍ | 13558/16110 [17:45:06<12:34:00, 17.73s/it]

 84%|████████▍ | 13559/16110 [17:45:27<13:07:40, 18.53s/it]

 84%|████████▍ | 13560/16110 [17:45:42<12:24:23, 17.52s/it]


 84%|████████▍ | 13562/16110 [17:46:19<12:58:07, 18.32s/it]
{'loss': 0.1197, 'learning_rate': 1.8145582819908913e-06, 'rewards/chosen': -5.118104457855225, 'rewards/rejected': -9.67377758026123, 'rewards/accuracies': 1.0, 'rewards/margins': 4.555673599243164, 'policy_logps/rejected': -422.82574462890625, 'policy_logps/chosen': -399.58905029296875, 'referece_logps/rejected': -326.08795166015625, 'referece_logps/chosen': -348.40802001953125, 'logits/rejected': 0.34796759486198425, 'logits/chosen': 0.24296821653842926, 'epoch': 7.58}
[2024-04-06 08:54:30,012] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 13563/16110 [17:46:38<13:11:08, 18.64s/it]

 84%|████████▍ | 13564/16110 [17:47:01<13:57:13, 19.73s/it]

 84%|████████▍ | 13565/16110 [17:47:16<13:06:57, 18.55s/it]


 84%|████████▍ | 13567/16110 [17:47:49<12:05:10, 17.11s/it]

 84%|████████▍ | 13568/16110 [17:48:07<12:17:32, 17.41s/it]
{'loss': 0.1106, 'learning_rate': 1.8138579401899745e-06, 'rewards/chosen': -5.043608665466309, 'rewards/rejected': -10.441018104553223, 'rewards/accuracies': 0.875, 'rewards/margins': 5.397409439086914, 'policy_logps/rejected': -574.6787719726562, 'policy_logps/chosen': -467.3452453613281, 'referece_logps/rejected': -470.26861572265625, 'referece_logps/chosen': -416.90911865234375, 'logits/rejected': 0.377227783203125, 'logits/chosen': 0.35694727301597595, 'epoch': 7.58}


 84%|████████▍ | 13570/16110 [17:48:49<13:40:49, 19.39s/it]
{'loss': 0.1713, 'learning_rate': 1.813624229698831e-06, 'rewards/chosen': -6.045138835906982, 'rewards/rejected': -8.575085639953613, 'rewards/accuracies': 0.75, 'rewards/margins': 2.529946804046631, 'policy_logps/rejected': -348.8022155761719, 'policy_logps/chosen': -342.6092834472656, 'referece_logps/rejected': -263.0513610839844, 'referece_logps/chosen': -282.15789794921875, 'logits/rejected': -0.21806415915489197, 'logits/chosen': -0.3036820888519287, 'epoch': 7.58}
[2024-04-06 08:57:01,586] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 13571/16110 [17:49:10<13:56:25, 19.77s/it]

 84%|████████▍ | 13572/16110 [17:49:30<14:02:46, 19.92s/it]
[2024-04-06 08:57:35,733] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 13573/16110 [17:49:44<12:45:28, 18.10s/it]


 84%|████████▍ | 13575/16110 [17:50:19<12:28:54, 17.73s/it]
{'loss': 0.1976, 'learning_rate': 1.81303937802626e-06, 'rewards/chosen': -4.5445451736450195, 'rewards/rejected': -7.6551337242126465, 'rewards/accuracies': 0.875, 'rewards/margins': 3.110588550567627, 'policy_logps/rejected': -343.81524658203125, 'policy_logps/chosen': -272.6471252441406, 'referece_logps/rejected': -267.263916015625, 'referece_logps/chosen': -227.20167541503906, 'logits/rejected': -0.15573185682296753, 'logits/chosen': -0.21795673668384552, 'epoch': 7.58}

 84%|████████▍ | 13576/16110 [17:50:40<13:04:52, 18.58s/it]

 84%|████████▍ | 13577/16110 [17:50:53<11:51:15, 16.85s/it]

 84%|████████▍ | 13578/16110 [17:51:13<12:32:32, 17.83s/it]

 84%|████████▍ | 13579/16110 [17:51:33<12:56:05, 18.40s/it]


 84%|████████▍ | 13581/16110 [17:52:03<11:37:58, 16.56s/it]
{'loss': 0.0946, 'learning_rate': 1.8123364715711605e-06, 'rewards/chosen': -5.426253318786621, 'rewards/rejected': -9.389284133911133, 'rewards/accuracies': 0.875, 'rewards/margins': 3.96303129196167, 'policy_logps/rejected': -420.56646728515625, 'policy_logps/chosen': -359.57586669921875, 'referece_logps/rejected': -326.6736145019531, 'referece_logps/chosen': -305.3133544921875, 'logits/rejected': -0.03877685219049454, 'logits/chosen': -0.03608572483062744, 'epoch': 7.59}


 84%|████████▍ | 13583/16110 [17:52:42<12:44:21, 18.15s/it]

 84%|████████▍ | 13584/16110 [17:53:02<13:06:45, 18.69s/it]
{'loss': 0.0729, 'learning_rate': 1.81198457500407e-06, 'rewards/chosen': -5.576369762420654, 'rewards/rejected': -9.455451965332031, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8790817260742188, 'policy_logps/rejected': -374.0611267089844, 'policy_logps/chosen': -392.83001708984375, 'referece_logps/rejected': -279.5066223144531, 'referece_logps/chosen': -337.06634521484375, 'logits/rejected': 0.2648261487483978, 'logits/chosen': 0.043445587158203125, 'epoch': 7.59}


 84%|████████▍ | 13586/16110 [17:53:41<13:39:58, 19.49s/it]
{'loss': 0.1375, 'learning_rate': 1.8117498131797464e-06, 'rewards/chosen': -4.135186672210693, 'rewards/rejected': -9.405295372009277, 'rewards/accuracies': 1.0, 'rewards/margins': 5.270108222961426, 'policy_logps/rejected': -317.63409423828125, 'policy_logps/chosen': -566.9536743164062, 'referece_logps/rejected': -223.58116149902344, 'referece_logps/chosen': -525.601806640625, 'logits/rejected': 0.4877527952194214, 'logits/chosen': 0.10215063393115997, 'epoch': 7.59}


 84%|████████▍ | 13588/16110 [17:54:24<14:14:45, 20.34s/it]

 84%|████████▍ | 13589/16110 [17:54:44<14:09:32, 20.22s/it]
{'loss': 0.1173, 'learning_rate': 1.8113974243687582e-06, 'rewards/chosen': -3.947031259536743, 'rewards/rejected': -10.558168411254883, 'rewards/accuracies': 1.0, 'rewards/margins': 6.611137866973877, 'policy_logps/rejected': -387.3949890136719, 'policy_logps/chosen': -280.8000793457031, 'referece_logps/rejected': -281.81329345703125, 'referece_logps/chosen': -241.3297576904297, 'logits/rejected': -0.037992917001247406, 'logits/chosen': 0.023218147456645966, 'epoch': 7.59}

 84%|████████▍ | 13590/16110 [17:55:02<13:48:13, 19.72s/it]

 84%|████████▍ | 13591/16110 [17:55:25<14:23:29, 20.57s/it]

 84%|████████▍ | 13592/16110 [17:55:45<14:13:17, 20.33s/it]


 84%|████████▍ | 13594/16110 [17:56:24<14:12:09, 20.32s/it]
[2024-04-06 09:04:15,465] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 84%|████████▍ | 13595/16110 [17:56:38<12:55:50, 18.51s/it]
{'loss': 0.1281, 'learning_rate': 1.810691761348749e-06, 'rewards/chosen': -4.774287700653076, 'rewards/rejected': -8.839862823486328, 'rewards/accuracies': 1.0, 'rewards/margins': 4.065574645996094, 'policy_logps/rejected': -333.4879455566406, 'policy_logps/chosen': -344.611328125, 'referece_logps/rejected': -245.08932495117188, 'referece_logps/chosen': -296.8684387207031, 'logits/rejected': 0.07918392866849899, 'logits/chosen': 0.1395902931690216, 'epoch': 7.59}

 84%|████████▍ | 13596/16110 [17:56:51<11:44:53, 16.82s/it]


 84%|████████▍ | 13598/16110 [17:57:26<11:56:18, 17.11s/it]
{'loss': 0.272, 'learning_rate': 1.8103384873964385e-06, 'rewards/chosen': -4.1207475662231445, 'rewards/rejected': -9.047565460205078, 'rewards/accuracies': 0.875, 'rewards/margins': 4.926817893981934, 'policy_logps/rejected': -272.8417053222656, 'policy_logps/chosen': -318.12408447265625, 'referece_logps/rejected': -182.36605834960938, 'referece_logps/chosen': -276.9166259765625, 'logits/rejected': -0.6630323529243469, 'logits/chosen': -0.728279709815979, 'epoch': 7.6}


 84%|████████▍ | 13600/16110 [17:58:04<12:30:57, 17.95s/it]
{'loss': 0.0945, 'learning_rate': 1.8101028076479507e-06, 'rewards/chosen': -4.30926513671875, 'rewards/rejected': -8.39029598236084, 'rewards/accuracies': 0.875, 'rewards/margins': 4.081031322479248, 'policy_logps/rejected': -381.2736511230469, 'policy_logps/chosen': -378.29376220703125, 'referece_logps/rejected': -297.37066650390625, 'referece_logps/chosen': -335.20111083984375, 'logits/rejected': -0.8527781367301941, 'logits/chosen': -0.9175235033035278, 'epoch': 7.6}

 84%|████████▍ | 13601/16110 [17:58:20<12:05:10, 17.34s/it]

 84%|████████▍ | 13602/16110 [17:58:35<11:39:49, 16.74s/it]

 84%|████████▍ | 13603/16110 [17:58:57<12:47:40, 18.37s/it]

 84%|████████▍ | 13604/16110 [17:59:09<11:28:57, 16.50s/it]

 84%|████████▍ | 13605/16110 [17:59:29<12:11:55, 17.53s/it]


 84%|████████▍ | 13607/16110 [17:59:54<10:19:08, 14.84s/it]
{'loss': 0.1912, 'learning_rate': 1.809276897312801e-06, 'rewards/chosen': -6.326080799102783, 'rewards/rejected': -14.50989818572998, 'rewards/accuracies': 0.875, 'rewards/margins': 8.183818817138672, 'policy_logps/rejected': -632.2489013671875, 'policy_logps/chosen': -344.1136474609375, 'referece_logps/rejected': -487.1499328613281, 'referece_logps/chosen': -280.85284423828125, 'logits/rejected': -0.3277886211872101, 'logits/chosen': -0.29810619354248047, 'epoch': 7.6}

 84%|████████▍ | 13608/16110 [18:00:16<11:50:02, 17.03s/it]

 84%|████████▍ | 13609/16110 [18:00:31<11:16:14, 16.22s/it]

 84%|████████▍ | 13610/16110 [18:00:47<11:16:21, 16.23s/it]

 84%|████████▍ | 13611/16110 [18:01:07<11:56:40, 17.21s/it]

 84%|████████▍ | 13612/16110 [18:01:23<11:42:14, 16.87s/it]

 85%|████████▍ | 13613/16110 [18:01:41<12:05:55, 17.44s/it]

 85%|████████▍ | 13614/16110 [18:01:59<12:02:43, 17.37s/it]


 85%|████████▍ | 13616/16110 [18:02:30<11:42:05, 16.89s/it]

 85%|████████▍ | 13617/16110 [18:02:48<11:51:45, 17.13s/it]
{'loss': 0.1154, 'learning_rate': 1.8080942453294034e-06, 'rewards/chosen': -3.8401408195495605, 'rewards/rejected': -9.704647064208984, 'rewards/accuracies': 0.875, 'rewards/margins': 5.864506244659424, 'policy_logps/rejected': -473.1246032714844, 'policy_logps/chosen': -584.9303588867188, 'referece_logps/rejected': -376.078125, 'referece_logps/chosen': -546.5289306640625, 'logits/rejected': 0.18402282893657684, 'logits/chosen': -0.21102374792099, 'epoch': 7.61}

 85%|████████▍ | 13618/16110 [18:03:01<11:04:22, 16.00s/it]
[2024-04-06 09:11:14,779] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13619/16110 [18:03:23<12:17:03, 17.75s/it]
[2024-04-06 09:11:35,518] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13620/16110 [18:03:44<12:53:55, 18.65s/it]

 85%|████████▍ | 13621/16110 [18:03:57<11:45:54, 17.02s/it]

 85%|████████▍ | 13622/16110 [18:04:17<12:22:46, 17.91s/it]

 85%|████████▍ | 13623/16110 [18:04:38<12:55:25, 18.71s/it]

 85%|████████▍ | 13624/16110 [18:04:58<13:12:04, 19.12s/it]

 85%|████████▍ | 13625/16110 [18:05:11<12:04:06, 17.48s/it]

 85%|████████▍ | 13626/16110 [18:05:29<12:06:02, 17.54s/it]

 85%|████████▍ | 13627/16110 [18:05:49<12:33:18, 18.20s/it]


 85%|████████▍ | 13629/16110 [18:06:24<12:13:44, 17.74s/it]

 85%|████████▍ | 13630/16110 [18:06:39<11:29:51, 16.69s/it]
{'loss': 0.1365, 'learning_rate': 1.8065519152374394e-06, 'rewards/chosen': -5.996063709259033, 'rewards/rejected': -8.7678804397583, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7718164920806885, 'policy_logps/rejected': -529.5440673828125, 'policy_logps/chosen': -402.93328857421875, 'referece_logps/rejected': -441.865234375, 'referece_logps/chosen': -342.97265625, 'logits/rejected': -0.3584764301776886, 'logits/chosen': -0.2038704752922058, 'epoch': 7.61}

 85%|████████▍ | 13631/16110 [18:06:49<10:14:33, 14.87s/it]


 85%|████████▍ | 13633/16110 [18:07:18<10:15:44, 14.91s/it]
{'loss': 0.1167, 'learning_rate': 1.806195210090634e-06, 'rewards/chosen': -4.814554214477539, 'rewards/rejected': -9.581718444824219, 'rewards/accuracies': 1.0, 'rewards/margins': 4.7671637535095215, 'policy_logps/rejected': -454.4739990234375, 'policy_logps/chosen': -538.2553100585938, 'referece_logps/rejected': -358.6568298339844, 'referece_logps/chosen': -490.10980224609375, 'logits/rejected': 0.11045573651790619, 'logits/chosen': -0.13615117967128754, 'epoch': 7.62}

 85%|████████▍ | 13634/16110 [18:07:38<11:15:42, 16.37s/it]

 85%|████████▍ | 13635/16110 [18:07:59<12:06:25, 17.61s/it]

 85%|████████▍ | 13636/16110 [18:08:16<12:05:12, 17.59s/it]
{'loss': 0.1209, 'learning_rate': 1.805838211660879e-06, 'rewards/chosen': -5.209044456481934, 'rewards/rejected': -9.335787773132324, 'rewards/accuracies': 1.0, 'rewards/margins': 4.126745223999023, 'policy_logps/rejected': -469.32391357421875, 'policy_logps/chosen': -481.7004699707031, 'referece_logps/rejected': -375.966064453125, 'referece_logps/chosen': -429.61004638671875, 'logits/rejected': -0.0786917507648468, 'logits/chosen': -0.18094655871391296, 'epoch': 7.62}

 85%|████████▍ | 13637/16110 [18:08:36<12:29:29, 18.18s/it]

 85%|████████▍ | 13638/16110 [18:08:50<11:38:05, 16.94s/it]

 85%|████████▍ | 13639/16110 [18:09:04<11:02:43, 16.09s/it]

 85%|████████▍ | 13640/16110 [18:09:20<11:01:39, 16.07s/it]


 85%|████████▍ | 13642/16110 [18:09:56<11:48:03, 17.21s/it]
{'loss': 0.1373, 'learning_rate': 1.8051233354721126e-06, 'rewards/chosen': -4.455763816833496, 'rewards/rejected': -11.385725975036621, 'rewards/accuracies': 1.0, 'rewards/margins': 6.929962635040283, 'policy_logps/rejected': -428.7735290527344, 'policy_logps/chosen': -430.663818359375, 'referece_logps/rejected': -314.9162902832031, 'referece_logps/chosen': -386.1062316894531, 'logits/rejected': 0.5293619632720947, 'logits/chosen': 0.43289607763290405, 'epoch': 7.62}


 85%|████████▍ | 13644/16110 [18:10:29<11:10:44, 16.32s/it]
{'loss': 0.0968, 'learning_rate': 1.8048847830084469e-06, 'rewards/chosen': -3.6231417655944824, 'rewards/rejected': -9.437125205993652, 'rewards/accuracies': 0.875, 'rewards/margins': 5.81398344039917, 'policy_logps/rejected': -345.82598876953125, 'policy_logps/chosen': -409.510986328125, 'referece_logps/rejected': -251.45472717285156, 'referece_logps/chosen': -373.279541015625, 'logits/rejected': 0.2351730763912201, 'logits/chosen': -0.06155198812484741, 'epoch': 7.62}

 85%|████████▍ | 13645/16110 [18:10:46<11:19:48, 16.55s/it]


 85%|████████▍ | 13647/16110 [18:11:17<10:43:17, 15.67s/it]
{'loss': 0.1113, 'learning_rate': 1.8045267103198087e-06, 'rewards/chosen': -5.238362789154053, 'rewards/rejected': -10.908957481384277, 'rewards/accuracies': 1.0, 'rewards/margins': 5.670594215393066, 'policy_logps/rejected': -476.3351135253906, 'policy_logps/chosen': -406.5974426269531, 'referece_logps/rejected': -367.24554443359375, 'referece_logps/chosen': -354.21380615234375, 'logits/rejected': 0.10618910193443298, 'logits/chosen': 0.1045302301645279, 'epoch': 7.62}

 85%|████████▍ | 13648/16110 [18:11:32<10:35:46, 15.49s/it]

 85%|████████▍ | 13649/16110 [18:11:51<11:23:59, 16.68s/it]

 85%|████████▍ | 13650/16110 [18:12:04<10:37:03, 15.54s/it]


 85%|████████▍ | 13652/16110 [18:12:37<10:54:22, 15.97s/it]
{'loss': 0.0397, 'learning_rate': 1.8039292721787382e-06, 'rewards/chosen': -6.354048728942871, 'rewards/rejected': -12.034574508666992, 'rewards/accuracies': 1.0, 'rewards/margins': 5.680525302886963, 'policy_logps/rejected': -499.10223388671875, 'policy_logps/chosen': -518.5086059570312, 'referece_logps/rejected': -378.7564697265625, 'referece_logps/chosen': -454.9681396484375, 'logits/rejected': 0.7482752203941345, 'logits/chosen': 0.4800716042518616, 'epoch': 7.63}

 85%|████████▍ | 13653/16110 [18:12:57<11:48:10, 17.29s/it]

 85%|████████▍ | 13654/16110 [18:13:11<10:55:31, 16.01s/it]

 85%|████████▍ | 13655/16110 [18:13:30<11:31:44, 16.91s/it]

 85%|████████▍ | 13656/16110 [18:13:48<11:50:16, 17.37s/it]

 85%|████████▍ | 13657/16110 [18:14:03<11:26:33, 16.79s/it]
[2024-04-06 09:22:16,922] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13658/16110 [18:14:25<12:28:34, 18.32s/it]
[2024-04-06 09:22:37,159] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13659/16110 [18:14:46<12:51:47, 18.89s/it]

 85%|████████▍ | 13660/16110 [18:15:06<13:06:03, 19.25s/it]
[2024-04-06 09:23:09,307] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13661/16110 [18:15:18<11:37:44, 17.09s/it]

 85%|████████▍ | 13662/16110 [18:15:35<11:37:57, 17.11s/it]

 85%|████████▍ | 13663/16110 [18:15:56<12:31:52, 18.44s/it]

 85%|████████▍ | 13664/16110 [18:16:19<13:18:40, 19.59s/it]

 85%|████████▍ | 13665/16110 [18:16:35<12:44:02, 18.75s/it]

 85%|████████▍ | 13666/16110 [18:16:55<12:56:47, 19.07s/it]
[2024-04-06 09:25:02,293] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13667/16110 [18:17:11<12:11:55, 17.98s/it]

 85%|████████▍ | 13668/16110 [18:17:22<10:54:00, 16.07s/it]

 85%|████████▍ | 13669/16110 [18:17:35<10:11:48, 15.04s/it]

 85%|████████▍ | 13670/16110 [18:17:51<10:30:00, 15.49s/it]
[2024-04-06 09:26:06,297] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13671/16110 [18:18:15<12:03:45, 17.80s/it]

 85%|████████▍ | 13672/16110 [18:18:37<12:54:44, 19.07s/it]

 85%|████████▍ | 13673/16110 [18:18:56<13:02:59, 19.28s/it]

 85%|████████▍ | 13674/16110 [18:19:08<11:30:51, 17.02s/it]

 85%|████████▍ | 13675/16110 [18:19:25<11:27:21, 16.94s/it]

 85%|████████▍ | 13676/16110 [18:19:38<10:40:10, 15.78s/it]

 85%|████████▍ | 13677/16110 [18:19:51<10:06:10, 14.95s/it]


 85%|████████▍ | 13679/16110 [18:20:28<11:28:54, 17.00s/it]
[2024-04-06 09:28:19,611] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13680/16110 [18:20:44<11:22:38, 16.86s/it]
[2024-04-06 09:28:36,123] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13681/16110 [18:21:01<11:17:16, 16.73s/it]

 85%|████████▍ | 13682/16110 [18:21:19<11:32:04, 17.10s/it]

 85%|████████▍ | 13683/16110 [18:21:40<12:15:56, 18.19s/it]

 85%|████████▍ | 13684/16110 [18:22:00<12:39:13, 18.78s/it]

 85%|████████▍ | 13685/16110 [18:22:17<12:22:29, 18.37s/it]

 85%|████████▍ | 13686/16110 [18:22:31<11:23:33, 16.92s/it]
{'loss': 0.1157, 'learning_rate': 1.7998451794146461e-06, 'rewards/chosen': -3.2176947593688965, 'rewards/rejected': -7.925670623779297, 'rewards/accuracies': 1.0, 'rewards/margins': 4.7079758644104, 'policy_logps/rejected': -290.4436950683594, 'policy_logps/chosen': -248.82736206054688, 'referece_logps/rejected': -211.18695068359375, 'referece_logps/chosen': -216.65042114257812, 'logits/rejected': -0.2805706858634949, 'logits/chosen': -0.35603946447372437, 'epoch': 7.65}

 85%|████████▍ | 13687/16110 [18:22:54<12:37:35, 18.76s/it]

 85%|████████▍ | 13688/16110 [18:23:16<13:19:40, 19.81s/it]

 85%|████████▍ | 13689/16110 [18:23:37<13:33:13, 20.15s/it]
[2024-04-06 09:31:28,638] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 85%|████████▍ | 13690/16110 [18:23:53<12:42:37, 18.91s/it]

 85%|████████▍ | 13691/16110 [18:24:11<12:26:31, 18.52s/it]

 85%|████████▍ | 13692/16110 [18:24:33<13:10:42, 19.62s/it]

 85%|████████▍ | 13693/16110 [18:24:45<11:36:17, 17.29s/it]

 85%|████████▌ | 13694/16110 [18:25:03<11:49:15, 17.61s/it]

 85%|████████▌ | 13695/16110 [18:25:21<11:48:59, 17.61s/it]

 85%|████████▌ | 13696/16110 [18:25:38<11:42:05, 17.45s/it]

 85%|████████▌ | 13697/16110 [18:25:54<11:33:04, 17.23s/it]

 85%|████████▌ | 13698/16110 [18:26:07<10:39:12, 15.90s/it]

 85%|████████▌ | 13699/16110 [18:26:24<10:51:06, 16.20s/it]

 85%|████████▌ | 13700/16110 [18:26:45<11:50:47, 17.70s/it]

 85%|████████▌ | 13701/16110 [18:27:03<11:45:27, 17.57s/it]

 85%|████████▌ | 13702/16110 [18:27:25<12:40:39, 18.95s/it]

 85%|████████▌ | 13703/16110 [18:27:43<12:32:40, 18.76s/it]

 85%|████████▌ | 13704/16110 [18:28:01<12:20:29, 18.47s/it]

 85%|████████▌ | 13705/16110 [18:28:21<12:42:28, 19.02s/it]

 85%|████████▌ | 13706/16110 [18:28:41<12:54:17, 19.32s/it]

 85%|████████▌ | 13707/16110 [18:28:58<12:18:31, 18.44s/it]

 85%|████████▌ | 13708/16110 [18:29:20<13:04:21, 19.59s/it]

 85%|████████▌ | 13709/16110 [18:29:37<12:40:20, 19.00s/it]

 85%|████████▌ | 13710/16110 [18:29:51<11:39:02, 17.48s/it]

 85%|████████▌ | 13711/16110 [18:30:06<11:02:56, 16.58s/it]

 85%|████████▌ | 13712/16110 [18:30:24<11:17:09, 16.94s/it]

 85%|████████▌ | 13713/16110 [18:30:36<10:17:26, 15.46s/it]

 85%|████████▌ | 13714/16110 [18:30:57<11:25:11, 17.16s/it]

 85%|████████▌ | 13715/16110 [18:31:09<10:25:02, 15.66s/it]

 85%|████████▌ | 13716/16110 [18:31:22<9:53:57, 14.89s/it]

 85%|████████▌ | 13717/16110 [18:31:40<10:33:15, 15.88s/it]

 85%|████████▌ | 13718/16110 [18:32:00<11:20:07, 17.06s/it]

 85%|████████▌ | 13719/16110 [18:32:20<11:50:07, 17.82s/it]

 85%|████████▌ | 13720/16110 [18:32:41<12:31:26, 18.86s/it]

 85%|████████▌ | 13721/16110 [18:33:00<12:28:28, 18.80s/it]

 85%|████████▌ | 13722/16110 [18:33:19<12:32:43, 18.91s/it]

 85%|████████▌ | 13723/16110 [18:33:39<12:43:44, 19.20s/it]

 85%|████████▌ | 13724/16110 [18:33:52<11:28:21, 17.31s/it]

 85%|████████▌ | 13725/16110 [18:34:10<11:39:57, 17.61s/it]

 85%|████████▌ | 13726/16110 [18:34:29<11:58:27, 18.08s/it]

 85%|████████▌ | 13727/16110 [18:34:49<12:19:38, 18.62s/it]

 85%|████████▌ | 13728/16110 [18:35:04<11:35:32, 17.52s/it]

 85%|████████▌ | 13729/16110 [18:35:23<11:53:16, 17.97s/it]

 85%|████████▌ | 13730/16110 [18:35:43<12:13:08, 18.48s/it]

 85%|████████▌ | 13731/16110 [18:36:00<11:56:39, 18.07s/it]

 85%|████████▌ | 13732/16110 [18:36:15<11:21:33, 17.20s/it]

 85%|████████▌ | 13733/16110 [18:36:32<11:16:33, 17.08s/it]

 85%|████████▌ | 13734/16110 [18:36:50<11:33:43, 17.52s/it]

 85%|████████▌ | 13735/16110 [18:37:12<12:29:05, 18.92s/it]

 85%|████████▌ | 13736/16110 [18:37:32<12:41:15, 19.24s/it]

 85%|████████▌ | 13737/16110 [18:37:51<12:32:23, 19.02s/it]

 85%|████████▌ | 13738/16110 [18:38:12<13:01:21, 19.76s/it]

 85%|████████▌ | 13739/16110 [18:38:28<12:06:33, 18.39s/it]

 85%|████████▌ | 13740/16110 [18:38:43<11:33:38, 17.56s/it]

 85%|████████▌ | 13741/16110 [18:39:03<11:55:39, 18.13s/it]

 85%|████████▌ | 13742/16110 [18:39:15<10:52:44, 16.54s/it]

 85%|████████▌ | 13743/16110 [18:39:27<9:54:21, 15.07s/it]

 85%|████████▌ | 13744/16110 [18:39:47<10:54:22, 16.59s/it]

 85%|████████▌ | 13745/16110 [18:40:06<11:23:19, 17.34s/it]

 85%|████████▌ | 13746/16110 [18:40:26<11:52:23, 18.08s/it]

 85%|████████▌ | 13747/16110 [18:40:45<12:05:45, 18.43s/it]

 85%|████████▌ | 13748/16110 [18:41:03<12:01:36, 18.33s/it]

 85%|████████▌ | 13749/16110 [18:41:19<11:31:36, 17.58s/it]

 85%|████████▌ | 13750/16110 [18:41:34<11:01:58, 16.83s/it]

 85%|████████▌ | 13751/16110 [18:41:46<9:55:19, 15.14s/it]

 85%|████████▌ | 13752/16110 [18:42:05<10:48:34, 16.50s/it]

 85%|████████▌ | 13753/16110 [18:42:19<10:18:47, 15.75s/it]

 85%|████████▌ | 13754/16110 [18:42:33<9:56:43, 15.20s/it]

 85%|████████▌ | 13755/16110 [18:42:49<10:07:00, 15.47s/it]

 85%|████████▌ | 13756/16110 [18:43:08<10:48:30, 16.53s/it]

 85%|████████▌ | 13757/16110 [18:43:29<11:34:08, 17.70s/it]

 85%|████████▌ | 13758/16110 [18:43:48<11:55:28, 18.25s/it]

 85%|████████▌ | 13759/16110 [18:44:08<12:14:36, 18.75s/it]

 85%|████████▌ | 13760/16110 [18:44:23<11:27:04, 17.54s/it]

 85%|████████▌ | 13761/16110 [18:44:39<11:04:40, 16.98s/it]

 85%|████████▌ | 13762/16110 [18:44:59<11:42:46, 17.96s/it]

 85%|████████▌ | 13763/16110 [18:45:15<11:24:15, 17.49s/it]

 85%|████████▌ | 13764/16110 [18:45:33<11:27:24, 17.58s/it]

 85%|████████▌ | 13765/16110 [18:45:51<11:27:31, 17.59s/it]

 85%|████████▌ | 13766/16110 [18:46:10<11:46:33, 18.09s/it]

 85%|████████▌ | 13767/16110 [18:46:30<12:09:59, 18.69s/it]

 85%|████████▌ | 13768/16110 [18:46:49<12:17:11, 18.89s/it]

 85%|████████▌ | 13769/16110 [18:47:09<12:22:36, 19.03s/it]

 85%|████████▌ | 13770/16110 [18:47:29<12:38:18, 19.44s/it]

 85%|████████▌ | 13771/16110 [18:47:46<12:10:12, 18.73s/it]
{'loss': 0.0771, 'learning_rate': 1.789471859279788e-06, 'rewards/chosen': -4.162171840667725, 'rewards/rejected': -9.740886688232422, 'rewards/accuracies': 1.0, 'rewards/margins': 5.578714847564697, 'policy_logps/rejected': -220.87164306640625, 'policy_logps/chosen': -308.1395568847656, 'referece_logps/rejected': -123.46279907226562, 'referece_logps/chosen': -266.517822265625, 'logits/rejected': 0.5420278906822205, 'logits/chosen': 0.2797027826309204, 'epoch': 7.69}


 85%|████████▌ | 13773/16110 [18:48:26<12:35:22, 19.39s/it]

 85%|████████▌ | 13774/16110 [18:48:43<12:06:03, 18.65s/it]

 86%|████████▌ | 13775/16110 [18:48:55<10:46:21, 16.61s/it]

 86%|████████▌ | 13776/16110 [18:49:07<9:50:17, 15.17s/it]

 86%|████████▌ | 13777/16110 [18:49:24<10:16:13, 15.85s/it]
{'loss': 0.1067, 'learning_rate': 1.7887308775475191e-06, 'rewards/chosen': -5.253849983215332, 'rewards/rejected': -9.750800132751465, 'rewards/accuracies': 0.875, 'rewards/margins': 4.496951103210449, 'policy_logps/rejected': -584.48046875, 'policy_logps/chosen': -501.3685302734375, 'referece_logps/rejected': -486.972412109375, 'referece_logps/chosen': -448.830078125, 'logits/rejected': -0.19215375185012817, 'logits/chosen': -0.13089625537395477, 'epoch': 7.7}

 86%|████████▌ | 13778/16110 [18:49:46<11:21:32, 17.54s/it]


 86%|████████▌ | 13780/16110 [18:50:19<10:57:15, 16.93s/it]

 86%|████████▌ | 13781/16110 [18:50:41<11:57:52, 18.49s/it]

 86%|████████▌ | 13782/16110 [18:51:02<12:29:05, 19.31s/it]

 86%|████████▌ | 13783/16110 [18:51:23<12:50:05, 19.86s/it]

 86%|████████▌ | 13784/16110 [18:51:43<12:48:55, 19.83s/it]

 86%|████████▌ | 13785/16110 [18:52:02<12:40:53, 19.64s/it]

 86%|████████▌ | 13786/16110 [18:52:23<12:51:43, 19.92s/it]
{'loss': 0.1955, 'learning_rate': 1.7876172533142616e-06, 'rewards/chosen': -2.8759777545928955, 'rewards/rejected': -5.892572402954102, 'rewards/accuracies': 0.875, 'rewards/margins': 3.016594886779785, 'policy_logps/rejected': -527.0018310546875, 'policy_logps/chosen': -600.861083984375, 'referece_logps/rejected': -468.07611083984375, 'referece_logps/chosen': -572.101318359375, 'logits/rejected': 0.25361698865890503, 'logits/chosen': 0.17639638483524323, 'epoch': 7.7}


 86%|████████▌ | 13788/16110 [18:53:01<12:21:31, 19.16s/it]

 86%|████████▌ | 13789/16110 [18:53:19<12:09:20, 18.85s/it]

 86%|████████▌ | 13790/16110 [18:53:34<11:23:48, 17.68s/it]

 86%|████████▌ | 13791/16110 [18:53:44<10:02:23, 15.59s/it]
{'loss': 0.1326, 'learning_rate': 1.7869974586890987e-06, 'rewards/chosen': -4.5051093101501465, 'rewards/rejected': -8.902226448059082, 'rewards/accuracies': 0.875, 'rewards/margins': 4.397117614746094, 'policy_logps/rejected': -368.4010009765625, 'policy_logps/chosen': -427.6373596191406, 'referece_logps/rejected': -279.37872314453125, 'referece_logps/chosen': -382.5862731933594, 'logits/rejected': 0.5176682472229004, 'logits/chosen': 0.31235790252685547, 'epoch': 7.7}


 86%|████████▌ | 13793/16110 [18:54:13<9:29:04, 14.74s/it]
{'loss': 0.0541, 'learning_rate': 1.786749318126893e-06, 'rewards/chosen': -4.902846813201904, 'rewards/rejected': -9.17505931854248, 'rewards/accuracies': 1.0, 'rewards/margins': 4.272212028503418, 'policy_logps/rejected': -487.1298522949219, 'policy_logps/chosen': -534.2352905273438, 'referece_logps/rejected': -395.3792419433594, 'referece_logps/chosen': -485.206787109375, 'logits/rejected': 0.0692315399646759, 'logits/chosen': -0.1880953162908554, 'epoch': 7.71}


 86%|████████▌ | 13795/16110 [18:54:41<9:02:56, 14.07s/it]
{'loss': 0.0751, 'learning_rate': 1.7865010503607763e-06, 'rewards/chosen': -4.049057960510254, 'rewards/rejected': -10.874747276306152, 'rewards/accuracies': 1.0, 'rewards/margins': 6.825689315795898, 'policy_logps/rejected': -451.3518371582031, 'policy_logps/chosen': -435.4590148925781, 'referece_logps/rejected': -342.6043701171875, 'referece_logps/chosen': -394.9683837890625, 'logits/rejected': 0.16348448395729065, 'logits/chosen': 0.2905694544315338, 'epoch': 7.71}

 86%|████████▌ | 13796/16110 [18:55:00<10:06:08, 15.72s/it]


 86%|████████▌ | 13798/16110 [18:55:37<10:51:54, 16.92s/it]
{'loss': 0.1995, 'learning_rate': 1.7861284102920801e-06, 'rewards/chosen': -3.9073424339294434, 'rewards/rejected': -8.4486665725708, 'rewards/accuracies': 0.875, 'rewards/margins': 4.541323661804199, 'policy_logps/rejected': -270.4407653808594, 'policy_logps/chosen': -392.4007873535156, 'referece_logps/rejected': -185.95408630371094, 'referece_logps/chosen': -353.3273620605469, 'logits/rejected': -0.548672080039978, 'logits/chosen': -0.7938501238822937, 'epoch': 7.71}


 86%|████████▌ | 13800/16110 [18:56:09<10:49:35, 16.87s/it]

 86%|████████▌ | 13801/16110 [18:56:27<10:58:53, 17.12s/it]

 86%|████████▌ | 13802/16110 [18:56:41<10:31:08, 16.41s/it]

 86%|████████▌ | 13803/16110 [18:57:04<11:38:47, 18.17s/it]

 86%|████████▌ | 13804/16110 [18:57:23<11:51:24, 18.51s/it]

 86%|████████▌ | 13805/16110 [18:57:38<11:09:46, 17.43s/it]
{'loss': 0.1083, 'learning_rate': 1.7852578048771614e-06, 'rewards/chosen': -4.855068206787109, 'rewards/rejected': -10.118910789489746, 'rewards/accuracies': 1.0, 'rewards/margins': 5.263842582702637, 'policy_logps/rejected': -405.775146484375, 'policy_logps/chosen': -334.30426025390625, 'referece_logps/rejected': -304.5860290527344, 'referece_logps/chosen': -285.7535705566406, 'logits/rejected': 0.31395286321640015, 'logits/chosen': 0.3027533292770386, 'epoch': 7.71}


 86%|████████▌ | 13807/16110 [18:58:09<10:31:06, 16.44s/it]
{'loss': 0.2326, 'learning_rate': 1.7850087747312483e-06, 'rewards/chosen': -6.0058512687683105, 'rewards/rejected': -8.742514610290527, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7366631031036377, 'policy_logps/rejected': -320.8842468261719, 'policy_logps/chosen': -385.4321594238281, 'referece_logps/rejected': -233.45913696289062, 'referece_logps/chosen': -325.3736572265625, 'logits/rejected': 0.7072751522064209, 'logits/chosen': 0.4878447651863098, 'epoch': 7.71}


 86%|████████▌ | 13809/16110 [18:58:42<10:33:50, 16.53s/it]
{'loss': 0.1866, 'learning_rate': 1.7847596176628401e-06, 'rewards/chosen': -4.964202404022217, 'rewards/rejected': -9.064584732055664, 'rewards/accuracies': 1.0, 'rewards/margins': 4.1003828048706055, 'policy_logps/rejected': -431.97320556640625, 'policy_logps/chosen': -334.2464294433594, 'referece_logps/rejected': -341.32733154296875, 'referece_logps/chosen': -284.6044006347656, 'logits/rejected': -0.9557749629020691, 'logits/chosen': -1.1165900230407715, 'epoch': 7.71}


 86%|████████▌ | 13811/16110 [18:59:09<9:26:11, 14.78s/it]

 86%|████████▌ | 13812/16110 [18:59:27<10:01:33, 15.71s/it]

 86%|████████▌ | 13813/16110 [18:59:48<10:56:49, 17.16s/it]

 86%|████████▌ | 13814/16110 [19:00:00<9:58:29, 15.64s/it]

 86%|████████▌ | 13815/16110 [19:00:17<10:22:26, 16.27s/it]

 86%|████████▌ | 13816/16110 [19:00:30<9:36:42, 15.08s/it]

 86%|████████▌ | 13817/16110 [19:00:50<10:32:10, 16.54s/it]

 86%|████████▌ | 13818/16110 [19:01:09<11:08:29, 17.50s/it]

 86%|████████▌ | 13819/16110 [19:01:30<11:44:03, 18.44s/it]

 86%|████████▌ | 13820/16110 [19:01:52<12:22:20, 19.45s/it]

 86%|████████▌ | 13821/16110 [19:02:03<10:47:04, 16.96s/it]
{'loss': 0.1181, 'learning_rate': 1.7832620121373498e-06, 'rewards/chosen': -2.9270429611206055, 'rewards/rejected': -8.906649589538574, 'rewards/accuracies': 1.0, 'rewards/margins': 5.979606628417969, 'policy_logps/rejected': -321.8387756347656, 'policy_logps/chosen': -246.01632690429688, 'referece_logps/rejected': -232.77230834960938, 'referece_logps/chosen': -216.74588012695312, 'logits/rejected': -0.3495057225227356, 'logits/chosen': -0.6603665947914124, 'epoch': 7.72}

 86%|████████▌ | 13822/16110 [19:02:25<11:41:06, 18.39s/it]


 86%|████████▌ | 13824/16110 [19:03:01<11:30:03, 18.11s/it]

 86%|████████▌ | 13825/16110 [19:03:22<12:07:03, 19.09s/it]

 86%|████████▌ | 13826/16110 [19:03:42<12:10:05, 19.18s/it]

 86%|████████▌ | 13827/16110 [19:04:02<12:18:55, 19.42s/it]

 86%|████████▌ | 13828/16110 [19:04:17<11:34:25, 18.26s/it]

 86%|████████▌ | 13829/16110 [19:04:38<11:58:59, 18.91s/it]

 86%|████████▌ | 13830/16110 [19:04:54<11:28:05, 18.11s/it]

 86%|████████▌ | 13831/16110 [19:05:15<12:03:24, 19.05s/it]

 86%|████████▌ | 13832/16110 [19:05:38<12:43:53, 20.12s/it]
{'loss': 0.1626, 'learning_rate': 1.7818852018686897e-06, 'rewards/chosen': -4.843168258666992, 'rewards/rejected': -9.621091842651367, 'rewards/accuracies': 1.0, 'rewards/margins': 4.777923583984375, 'policy_logps/rejected': -405.2706604003906, 'policy_logps/chosen': -353.5749816894531, 'referece_logps/rejected': -309.0597229003906, 'referece_logps/chosen': -305.14324951171875, 'logits/rejected': -0.4055081307888031, 'logits/chosen': -0.33078232407569885, 'epoch': 7.73}


 86%|████████▌ | 13834/16110 [19:06:14<12:10:02, 19.25s/it]
{'loss': 0.0912, 'learning_rate': 1.7816344616747174e-06, 'rewards/chosen': -5.691502094268799, 'rewards/rejected': -9.755783081054688, 'rewards/accuracies': 0.875, 'rewards/margins': 4.064280986785889, 'policy_logps/rejected': -476.68792724609375, 'policy_logps/chosen': -550.2357788085938, 'referece_logps/rejected': -379.1300964355469, 'referece_logps/chosen': -493.32080078125, 'logits/rejected': 0.2848287522792816, 'logits/chosen': 0.2545928955078125, 'epoch': 7.73}


 86%|████████▌ | 13836/16110 [19:06:56<12:40:31, 20.07s/it]
[2024-04-06 10:14:47,425] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1083, 'learning_rate': 1.7813835951038186e-06, 'rewards/chosen': -5.934453964233398, 'rewards/rejected': -10.077885627746582, 'rewards/accuracies': 1.0, 'rewards/margins': 4.143430709838867, 'policy_logps/rejected': -548.1450805664062, 'policy_logps/chosen': -566.8382568359375, 'referece_logps/rejected': -447.3662109375, 'referece_logps/chosen': -507.4937438964844, 'logits/rejected': 0.5396662354469299, 'logits/chosen': 0.30501559376716614, 'epoch': 7.73}


 86%|████████▌ | 13838/16110 [19:07:30<11:29:54, 18.22s/it]
[2024-04-06 10:15:21,196] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 13839/16110 [19:07:46<11:09:16, 17.68s/it]

 86%|████████▌ | 13840/16110 [19:08:08<11:56:11, 18.93s/it]

 86%|████████▌ | 13841/16110 [19:08:24<11:28:20, 18.20s/it]
{'loss': 0.1601, 'learning_rate': 1.780755876043748e-06, 'rewards/chosen': -6.143178939819336, 'rewards/rejected': -12.466981887817383, 'rewards/accuracies': 0.875, 'rewards/margins': 6.323803424835205, 'policy_logps/rejected': -443.2340087890625, 'policy_logps/chosen': -449.8328552246094, 'referece_logps/rejected': -318.5641784667969, 'referece_logps/chosen': -388.4010009765625, 'logits/rejected': 0.005713250488042831, 'logits/chosen': -0.14088654518127441, 'epoch': 7.73}
[2024-04-06 10:16:34,778] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 13842/16110 [19:08:43<11:34:54, 18.38s/it]

 86%|████████▌ | 13843/16110 [19:08:59<11:08:16, 17.69s/it]


 86%|████████▌ | 13845/16110 [19:09:31<10:16:58, 16.34s/it]
{'loss': 0.1392, 'learning_rate': 1.7802531327083226e-06, 'rewards/chosen': -5.157639026641846, 'rewards/rejected': -10.538339614868164, 'rewards/accuracies': 1.0, 'rewards/margins': 5.380700588226318, 'policy_logps/rejected': -455.09478759765625, 'policy_logps/chosen': -393.91497802734375, 'referece_logps/rejected': -349.7113952636719, 'referece_logps/chosen': -342.3386535644531, 'logits/rejected': -0.06936787068843842, 'logits/chosen': 0.044230204075574875, 'epoch': 7.73}


 86%|████████▌ | 13847/16110 [19:10:05<10:39:20, 16.95s/it]
{'loss': 0.2237, 'learning_rate': 1.7800015717898993e-06, 'rewards/chosen': -4.600783348083496, 'rewards/rejected': -8.585798263549805, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9850149154663086, 'policy_logps/rejected': -425.3910827636719, 'policy_logps/chosen': -453.546142578125, 'referece_logps/rejected': -339.5330810546875, 'referece_logps/chosen': -407.53826904296875, 'logits/rejected': -0.35621118545532227, 'logits/chosen': -0.5774645805358887, 'epoch': 7.74}

 86%|████████▌ | 13848/16110 [19:10:25<11:12:46, 17.85s/it]


 86%|████████▌ | 13850/16110 [19:11:03<11:24:56, 18.18s/it]

 86%|████████▌ | 13851/16110 [19:11:24<12:00:23, 19.13s/it]

 86%|████████▌ | 13852/16110 [19:11:40<11:27:49, 18.28s/it]

 86%|████████▌ | 13853/16110 [19:11:52<10:13:00, 16.30s/it]

 86%|████████▌ | 13854/16110 [19:12:11<10:41:30, 17.06s/it]

 86%|████████▌ | 13855/16110 [19:12:32<11:31:06, 18.39s/it]
[2024-04-06 10:20:23,990] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 13856/16110 [19:12:52<11:44:22, 18.75s/it]
{'loss': 0.1054, 'learning_rate': 1.7788679875973514e-06, 'rewards/chosen': -3.89522385597229, 'rewards/rejected': -9.47529125213623, 'rewards/accuracies': 1.0, 'rewards/margins': 5.580066680908203, 'policy_logps/rejected': -399.91021728515625, 'policy_logps/chosen': -363.18780517578125, 'referece_logps/rejected': -305.15728759765625, 'referece_logps/chosen': -324.23553466796875, 'logits/rejected': 0.620819091796875, 'logits/chosen': 0.40091821551322937, 'epoch': 7.74}
[2024-04-06 10:20:59,040] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 86%|████████▌ | 13858/16110 [19:13:20<10:04:45, 16.11s/it]

 86%|████████▌ | 13859/16110 [19:13:38<10:32:15, 16.85s/it]
{'loss': 0.2089, 'learning_rate': 1.7784895593331024e-06, 'rewards/chosen': -4.353756904602051, 'rewards/rejected': -10.41021728515625, 'rewards/accuracies': 1.0, 'rewards/margins': 6.056459903717041, 'policy_logps/rejected': -612.9410400390625, 'policy_logps/chosen': -567.7521362304688, 'referece_logps/rejected': -508.8388671875, 'referece_logps/chosen': -524.214599609375, 'logits/rejected': 0.5535053610801697, 'logits/chosen': 0.49512630701065063, 'epoch': 7.74}

 86%|████████▌ | 13860/16110 [19:13:57<10:54:10, 17.44s/it]


 86%|████████▌ | 13862/16110 [19:14:23<9:31:06, 15.24s/it]

 86%|████████▌ | 13863/16110 [19:14:44<10:39:10, 17.07s/it]

 86%|████████▌ | 13864/16110 [19:15:04<11:10:29, 17.91s/it]

 86%|████████▌ | 13865/16110 [19:15:21<10:55:39, 17.52s/it]

 86%|████████▌ | 13866/16110 [19:15:34<10:10:16, 16.32s/it]
{'loss': 0.1333, 'learning_rate': 1.7776054589390741e-06, 'rewards/chosen': -3.8962466716766357, 'rewards/rejected': -9.73010540008545, 'rewards/accuracies': 0.875, 'rewards/margins': 5.833858489990234, 'policy_logps/rejected': -448.2724304199219, 'policy_logps/chosen': -404.39300537109375, 'referece_logps/rejected': -350.9714050292969, 'referece_logps/chosen': -365.43048095703125, 'logits/rejected': 0.16227105259895325, 'logits/chosen': 0.2039031684398651, 'epoch': 7.75}
[2024-04-06 10:23:45,732] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 86%|████████▌ | 13868/16110 [19:16:11<10:41:50, 17.18s/it]

 86%|████████▌ | 13869/16110 [19:16:30<11:08:53, 17.91s/it]

 86%|████████▌ | 13870/16110 [19:16:51<11:36:55, 18.67s/it]
[2024-04-06 10:24:42,545] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1073, 'learning_rate': 1.7770995671112986e-06, 'rewards/chosen': -3.927220344543457, 'rewards/rejected': -9.111841201782227, 'rewards/accuracies': 1.0, 'rewards/margins': 5.184620380401611, 'policy_logps/rejected': -470.45709228515625, 'policy_logps/chosen': -429.42919921875, 'referece_logps/rejected': -379.33868408203125, 'referece_logps/chosen': -390.1570129394531, 'logits/rejected': -0.3476438522338867, 'logits/chosen': -0.4610605537891388, 'epoch': 7.75}

 86%|████████▌ | 13871/16110 [19:17:12<11:58:38, 19.26s/it]


 86%|████████▌ | 13873/16110 [19:17:53<12:22:01, 19.90s/it]
{'loss': 0.1085, 'learning_rate': 1.7767198184078324e-06, 'rewards/chosen': -3.924027442932129, 'rewards/rejected': -8.990718841552734, 'rewards/accuracies': 0.875, 'rewards/margins': 5.066691875457764, 'policy_logps/rejected': -380.1123046875, 'policy_logps/chosen': -477.51470947265625, 'referece_logps/rejected': -290.205078125, 'referece_logps/chosen': -438.2744445800781, 'logits/rejected': -0.1124735027551651, 'logits/chosen': -0.28105252981185913, 'epoch': 7.75}

 86%|████████▌ | 13874/16110 [19:18:14<12:34:07, 20.24s/it]


 86%|████████▌ | 13876/16110 [19:18:55<12:37:20, 20.34s/it]
{'loss': 0.0807, 'learning_rate': 1.7763397871441665e-06, 'rewards/chosen': -4.954187393188477, 'rewards/rejected': -9.153345108032227, 'rewards/accuracies': 1.0, 'rewards/margins': 4.19915771484375, 'policy_logps/rejected': -505.39923095703125, 'policy_logps/chosen': -487.2574462890625, 'referece_logps/rejected': -413.8657531738281, 'referece_logps/chosen': -437.715576171875, 'logits/rejected': -0.023895110934972763, 'logits/chosen': -0.2610608637332916, 'epoch': 7.75}

 86%|████████▌ | 13877/16110 [19:19:10<11:32:53, 18.62s/it]

 86%|████████▌ | 13878/16110 [19:19:20<10:03:17, 16.22s/it]


 86%|████████▌ | 13880/16110 [19:19:57<10:40:31, 17.23s/it]
{'loss': 0.1194, 'learning_rate': 1.7758326394934904e-06, 'rewards/chosen': -3.7092790603637695, 'rewards/rejected': -7.073808670043945, 'rewards/accuracies': 0.75, 'rewards/margins': 3.364529609680176, 'policy_logps/rejected': -574.1502685546875, 'policy_logps/chosen': -362.06683349609375, 'referece_logps/rejected': -503.41217041015625, 'referece_logps/chosen': -324.97406005859375, 'logits/rejected': -0.43019843101501465, 'logits/chosen': -0.3441477417945862, 'epoch': 7.75}


 86%|████████▌ | 13882/16110 [19:20:35<11:10:29, 18.06s/it]
{'loss': 0.071, 'learning_rate': 1.775578877489341e-06, 'rewards/chosen': -3.6367475986480713, 'rewards/rejected': -9.647910118103027, 'rewards/accuracies': 1.0, 'rewards/margins': 6.011162757873535, 'policy_logps/rejected': -457.47021484375, 'policy_logps/chosen': -480.46044921875, 'referece_logps/rejected': -360.9910888671875, 'referece_logps/chosen': -444.0929260253906, 'logits/rejected': 0.11315275728702545, 'logits/chosen': -0.09208744764328003, 'epoch': 7.76}


 86%|████████▌ | 13884/16110 [19:21:01<9:27:32, 15.30s/it]
{'loss': 0.1662, 'learning_rate': 1.7753249900873492e-06, 'rewards/chosen': -4.993227005004883, 'rewards/rejected': -9.279409408569336, 'rewards/accuracies': 0.75, 'rewards/margins': 4.286182403564453, 'policy_logps/rejected': -493.2960205078125, 'policy_logps/chosen': -398.0596618652344, 'referece_logps/rejected': -400.501953125, 'referece_logps/chosen': -348.1274108886719, 'logits/rejected': -0.39463430643081665, 'logits/chosen': -0.349670946598053, 'epoch': 7.76}

 86%|████████▌ | 13885/16110 [19:21:14<9:08:07, 14.78s/it]


 86%|████████▌ | 13887/16110 [19:21:47<9:28:41, 15.35s/it]

 86%|████████▌ | 13888/16110 [19:22:05<9:57:39, 16.14s/it]

 86%|████████▌ | 13889/16110 [19:22:21<9:49:41, 15.93s/it]

 86%|████████▌ | 13890/16110 [19:22:39<10:15:23, 16.63s/it]

 86%|████████▌ | 13891/16110 [19:22:59<10:52:14, 17.64s/it]
{'loss': 0.194, 'learning_rate': 1.774435397265202e-06, 'rewards/chosen': -5.07017183303833, 'rewards/rejected': -10.398816108703613, 'rewards/accuracies': 1.0, 'rewards/margins': 5.328644275665283, 'policy_logps/rejected': -514.58203125, 'policy_logps/chosen': -410.4735107421875, 'referece_logps/rejected': -410.5939025878906, 'referece_logps/chosen': -359.7718200683594, 'logits/rejected': 0.124321848154068, 'logits/chosen': 0.18106848001480103, 'epoch': 7.76}
[2024-04-06 10:31:12,153] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 13892/16110 [19:23:21<11:36:08, 18.83s/it]
[2024-04-06 10:31:29,368] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▌ | 13893/16110 [19:23:38<11:17:54, 18.35s/it]


 86%|████████▋ | 13895/16110 [19:24:13<11:07:19, 18.08s/it]
{'loss': 0.2397, 'learning_rate': 1.7739263697253714e-06, 'rewards/chosen': -5.550350189208984, 'rewards/rejected': -8.869490623474121, 'rewards/accuracies': 0.875, 'rewards/margins': 3.319140672683716, 'policy_logps/rejected': -239.37149047851562, 'policy_logps/chosen': -315.7397766113281, 'referece_logps/rejected': -150.6765899658203, 'referece_logps/chosen': -260.23626708984375, 'logits/rejected': -0.20739109814167023, 'logits/chosen': -0.4314928948879242, 'epoch': 7.76}
[2024-04-06 10:32:22,353] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 13896/16110 [19:24:31<11:04:52, 18.02s/it]

 86%|████████▋ | 13897/16110 [19:24:50<11:23:56, 18.54s/it]

 86%|████████▋ | 13898/16110 [19:25:12<11:58:39, 19.49s/it]
[2024-04-06 10:33:21,469] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 13899/16110 [19:25:30<11:37:48, 18.94s/it]


 86%|████████▋ | 13901/16110 [19:26:03<11:02:09, 17.99s/it]

 86%|████████▋ | 13902/16110 [19:26:23<11:27:55, 18.69s/it]
{'loss': 0.0853, 'learning_rate': 1.7730343673463334e-06, 'rewards/chosen': -5.31190299987793, 'rewards/rejected': -9.140419960021973, 'rewards/accuracies': 1.0, 'rewards/margins': 3.828517436981201, 'policy_logps/rejected': -483.17999267578125, 'policy_logps/chosen': -473.11431884765625, 'referece_logps/rejected': -391.7757873535156, 'referece_logps/chosen': -419.99530029296875, 'logits/rejected': -0.3837255835533142, 'logits/chosen': -0.412201464176178, 'epoch': 7.77}

 86%|████████▋ | 13903/16110 [19:26:38<10:46:33, 17.58s/it]

 86%|████████▋ | 13904/16110 [19:26:58<11:08:49, 18.19s/it]

 86%|████████▋ | 13905/16110 [19:27:14<10:44:46, 17.55s/it]

 86%|████████▋ | 13906/16110 [19:27:34<11:11:47, 18.29s/it]

 86%|████████▋ | 13907/16110 [19:27:50<10:47:57, 17.65s/it]


 86%|████████▋ | 13909/16110 [19:28:25<10:37:26, 17.38s/it]

 86%|████████▋ | 13910/16110 [19:28:45<11:06:55, 18.19s/it]
{'loss': 0.1191, 'learning_rate': 1.7720130613628653e-06, 'rewards/chosen': -5.005578994750977, 'rewards/rejected': -10.84985637664795, 'rewards/accuracies': 0.875, 'rewards/margins': 5.844277381896973, 'policy_logps/rejected': -384.1082763671875, 'policy_logps/chosen': -347.98809814453125, 'referece_logps/rejected': -275.6097412109375, 'referece_logps/chosen': -297.93231201171875, 'logits/rejected': 0.2778317928314209, 'logits/chosen': 0.277654230594635, 'epoch': 7.77}
[2024-04-06 10:36:54,307] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 13911/16110 [19:29:03<10:57:17, 17.93s/it]

 86%|████████▋ | 13912/16110 [19:29:22<11:17:10, 18.49s/it]

 86%|████████▋ | 13913/16110 [19:29:41<11:12:25, 18.36s/it]
[2024-04-06 10:37:52,532] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 86%|████████▋ | 13914/16110 [19:30:01<11:34:11, 18.97s/it]

 86%|████████▋ | 13915/16110 [19:30:19<11:21:19, 18.62s/it]

 86%|████████▋ | 13916/16110 [19:30:40<11:51:52, 19.47s/it]

 86%|████████▋ | 13917/16110 [19:30:57<11:21:48, 18.65s/it]


 86%|████████▋ | 13919/16110 [19:31:37<11:45:08, 19.31s/it]
{'loss': 0.0925, 'learning_rate': 1.7708617050561637e-06, 'rewards/chosen': -5.544931888580322, 'rewards/rejected': -9.321821212768555, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7768893241882324, 'policy_logps/rejected': -298.77838134765625, 'policy_logps/chosen': -450.0013427734375, 'referece_logps/rejected': -205.5601806640625, 'referece_logps/chosen': -394.552001953125, 'logits/rejected': 0.16356982290744781, 'logits/chosen': -0.16024792194366455, 'epoch': 7.78}


 86%|████████▋ | 13921/16110 [19:32:18<12:04:04, 19.85s/it]

 86%|████████▋ | 13922/16110 [19:32:33<11:19:35, 18.64s/it]
{'loss': 0.1275, 'learning_rate': 1.7704773585761468e-06, 'rewards/chosen': -4.65298318862915, 'rewards/rejected': -9.307741165161133, 'rewards/accuracies': 0.875, 'rewards/margins': 4.654758930206299, 'policy_logps/rejected': -296.01220703125, 'policy_logps/chosen': -324.690185546875, 'referece_logps/rejected': -202.9347686767578, 'referece_logps/chosen': -278.1603698730469, 'logits/rejected': 0.1869506537914276, 'logits/chosen': 0.17227530479431152, 'epoch': 7.78}

 86%|████████▋ | 13923/16110 [19:32:53<11:31:58, 18.98s/it]

 86%|████████▋ | 13924/16110 [19:33:12<11:33:05, 19.02s/it]

 86%|████████▋ | 13925/16110 [19:33:30<11:20:52, 18.70s/it]

 86%|████████▋ | 13926/16110 [19:33:49<11:24:00, 18.79s/it]


 86%|████████▋ | 13928/16110 [19:34:26<11:14:04, 18.54s/it]

 86%|████████▋ | 13929/16110 [19:34:46<11:25:28, 18.86s/it]
{'loss': 0.1646, 'learning_rate': 1.7695794603507754e-06, 'rewards/chosen': -4.44319486618042, 'rewards/rejected': -9.017134666442871, 'rewards/accuracies': 0.875, 'rewards/margins': 4.573940753936768, 'policy_logps/rejected': -385.00018310546875, 'policy_logps/chosen': -355.48779296875, 'referece_logps/rejected': -294.8288269042969, 'referece_logps/chosen': -311.05584716796875, 'logits/rejected': 0.2038881927728653, 'logits/chosen': 0.17487986385822296, 'epoch': 7.78}

 86%|████████▋ | 13930/16110 [19:34:57<10:01:51, 16.56s/it]


 86%|████████▋ | 13932/16110 [19:35:38<11:10:30, 18.47s/it]

 86%|████████▋ | 13933/16110 [19:35:58<11:28:57, 18.99s/it]
{'loss': 0.1431, 'learning_rate': 1.7690656911833611e-06, 'rewards/chosen': -4.323668003082275, 'rewards/rejected': -8.117369651794434, 'rewards/accuracies': 0.875, 'rewards/margins': 3.793701171875, 'policy_logps/rejected': -376.154052734375, 'policy_logps/chosen': -477.0134582519531, 'referece_logps/rejected': -294.98040771484375, 'referece_logps/chosen': -433.77679443359375, 'logits/rejected': 0.6605547070503235, 'logits/chosen': 0.5458900332450867, 'epoch': 7.78}

 86%|████████▋ | 13934/16110 [19:36:17<11:32:43, 19.10s/it]


 87%|████████▋ | 13936/16110 [19:36:56<11:40:42, 19.34s/it]

 87%|████████▋ | 13937/16110 [19:37:12<11:03:12, 18.31s/it]

 87%|████████▋ | 13938/16110 [19:37:28<10:37:40, 17.62s/it]
{'loss': 0.1316, 'learning_rate': 1.768422780323751e-06, 'rewards/chosen': -4.62553071975708, 'rewards/rejected': -11.185741424560547, 'rewards/accuracies': 1.0, 'rewards/margins': 6.560210227966309, 'policy_logps/rejected': -368.2930908203125, 'policy_logps/chosen': -301.9178466796875, 'referece_logps/rejected': -256.4356689453125, 'referece_logps/chosen': -255.66253662109375, 'logits/rejected': 0.07272069156169891, 'logits/chosen': 0.2099686861038208, 'epoch': 7.79}

 87%|████████▋ | 13939/16110 [19:37:40<9:33:33, 15.85s/it]


 87%|████████▋ | 13941/16110 [19:38:14<10:00:24, 16.61s/it]
{'loss': 0.1021, 'learning_rate': 1.7680366610439284e-06, 'rewards/chosen': -4.858204364776611, 'rewards/rejected': -10.254578590393066, 'rewards/accuracies': 1.0, 'rewards/margins': 5.396374225616455, 'policy_logps/rejected': -434.3763427734375, 'policy_logps/chosen': -330.6160888671875, 'referece_logps/rejected': -331.83056640625, 'referece_logps/chosen': -282.0340576171875, 'logits/rejected': 0.24057962000370026, 'logits/chosen': 0.2323632389307022, 'epoch': 7.79}

 87%|████████▋ | 13942/16110 [19:38:28<9:22:40, 15.57s/it]

 87%|████████▋ | 13943/16110 [19:38:41<9:01:55, 15.00s/it]


 87%|████████▋ | 13945/16110 [19:39:22<10:38:32, 17.70s/it]
{'loss': 0.1282, 'learning_rate': 1.767521400737416e-06, 'rewards/chosen': -4.489630699157715, 'rewards/rejected': -9.412149429321289, 'rewards/accuracies': 1.0, 'rewards/margins': 4.922519207000732, 'policy_logps/rejected': -337.4936218261719, 'policy_logps/chosen': -373.05145263671875, 'referece_logps/rejected': -243.3721466064453, 'referece_logps/chosen': -328.1551513671875, 'logits/rejected': -0.4220685660839081, 'logits/chosen': -0.5751323103904724, 'epoch': 7.79}

 87%|████████▋ | 13946/16110 [19:39:39<10:29:49, 17.46s/it]


 87%|████████▋ | 13948/16110 [19:40:15<10:40:40, 17.78s/it]
{'loss': 0.1217, 'learning_rate': 1.7671346297397083e-06, 'rewards/chosen': -4.734638214111328, 'rewards/rejected': -10.282220840454102, 'rewards/accuracies': 1.0, 'rewards/margins': 5.547582626342773, 'policy_logps/rejected': -438.45904541015625, 'policy_logps/chosen': -397.8023681640625, 'referece_logps/rejected': -335.63690185546875, 'referece_logps/chosen': -350.4560241699219, 'logits/rejected': 0.7741008400917053, 'logits/chosen': 0.7567101716995239, 'epoch': 7.79}

 87%|████████▋ | 13949/16110 [19:40:35<11:09:53, 18.60s/it]

 87%|████████▋ | 13950/16110 [19:40:53<11:04:06, 18.45s/it]

 87%|████████▋ | 13951/16110 [19:41:15<11:37:58, 19.40s/it]

 87%|████████▋ | 13952/16110 [19:41:29<10:42:47, 17.87s/it]

 87%|████████▋ | 13953/16110 [19:41:51<11:25:16, 19.06s/it]


 87%|████████▋ | 13955/16110 [19:42:27<11:00:49, 18.40s/it]

 87%|████████▋ | 13956/16110 [19:42:42<10:31:54, 17.60s/it]
{'loss': 0.1102, 'learning_rate': 1.7661018764369154e-06, 'rewards/chosen': -6.4229607582092285, 'rewards/rejected': -11.678268432617188, 'rewards/accuracies': 1.0, 'rewards/margins': 5.255307674407959, 'policy_logps/rejected': -530.7485961914062, 'policy_logps/chosen': -492.57733154296875, 'referece_logps/rejected': -413.9659118652344, 'referece_logps/chosen': -428.3477783203125, 'logits/rejected': 0.5135920643806458, 'logits/chosen': 0.4629109799861908, 'epoch': 7.8}

 87%|████████▋ | 13957/16110 [19:42:53<9:15:58, 15.49s/it]


 87%|████████▋ | 13959/16110 [19:43:35<10:53:40, 18.23s/it]
{'loss': 0.0689, 'learning_rate': 1.7657140828593208e-06, 'rewards/chosen': -5.172752857208252, 'rewards/rejected': -11.609315872192383, 'rewards/accuracies': 1.0, 'rewards/margins': 6.436562538146973, 'policy_logps/rejected': -380.0583190917969, 'policy_logps/chosen': -330.4757080078125, 'referece_logps/rejected': -263.9651794433594, 'referece_logps/chosen': -278.7481994628906, 'logits/rejected': 0.098977230489254, 'logits/chosen': 0.23794342577457428, 'epoch': 7.8}

 87%|████████▋ | 13960/16110 [19:43:53<10:53:58, 18.25s/it]


 87%|████████▋ | 13962/16110 [19:44:32<11:22:52, 19.07s/it]

 87%|████████▋ | 13963/16110 [19:44:54<11:53:38, 19.94s/it]

 87%|████████▋ | 13964/16110 [19:45:10<11:12:24, 18.80s/it]
{'loss': 0.0984, 'learning_rate': 1.7650671412853508e-06, 'rewards/chosen': -6.387566566467285, 'rewards/rejected': -12.184767723083496, 'rewards/accuracies': 1.0, 'rewards/margins': 5.797202110290527, 'policy_logps/rejected': -451.41943359375, 'policy_logps/chosen': -384.2362365722656, 'referece_logps/rejected': -329.5717468261719, 'referece_logps/chosen': -320.36053466796875, 'logits/rejected': 0.29754242300987244, 'logits/chosen': 0.2961229085922241, 'epoch': 7.8}

 87%|████████▋ | 13965/16110 [19:45:27<10:52:00, 18.24s/it]

 87%|████████▋ | 13966/16110 [19:45:47<11:08:48, 18.72s/it]

 87%|████████▋ | 13967/16110 [19:46:04<10:44:45, 18.05s/it]

 87%|████████▋ | 13968/16110 [19:46:17<9:56:37, 16.71s/it]


 87%|████████▋ | 13970/16110 [19:46:55<10:40:29, 17.96s/it]
{'loss': 0.0931, 'learning_rate': 1.7642897909434806e-06, 'rewards/chosen': -4.895176887512207, 'rewards/rejected': -9.615389823913574, 'rewards/accuracies': 1.0, 'rewards/margins': 4.720212936401367, 'policy_logps/rejected': -356.7673034667969, 'policy_logps/chosen': -374.448974609375, 'referece_logps/rejected': -260.6134033203125, 'referece_logps/chosen': -325.4971618652344, 'logits/rejected': 0.3502800464630127, 'logits/chosen': 0.0902208536863327, 'epoch': 7.8}

 87%|████████▋ | 13971/16110 [19:47:17<11:27:16, 19.28s/it]

 87%|████████▋ | 13972/16110 [19:47:37<11:33:34, 19.46s/it]

 87%|████████▋ | 13973/16110 [19:47:50<10:26:09, 17.58s/it]

 87%|████████▋ | 13974/16110 [19:48:10<10:48:03, 18.20s/it]

 87%|████████▋ | 13975/16110 [19:48:30<11:02:31, 18.62s/it]

 87%|████████▋ | 13976/16110 [19:48:40<9:36:35, 16.21s/it]

 87%|████████▋ | 13977/16110 [19:48:55<9:19:23, 15.74s/it]

 87%|████████▋ | 13978/16110 [19:49:12<9:31:13, 16.08s/it]

 87%|████████▋ | 13979/16110 [19:49:32<10:17:17, 17.38s/it]

 87%|████████▋ | 13980/16110 [19:49:46<9:38:29, 16.30s/it]

 87%|████████▋ | 13981/16110 [19:50:06<10:16:11, 17.37s/it]

 87%|████████▋ | 13982/16110 [19:50:26<10:45:00, 18.19s/it]

 87%|████████▋ | 13983/16110 [19:50:45<10:55:17, 18.49s/it]

 87%|████████▋ | 13984/16110 [19:51:07<11:28:57, 19.44s/it]

 87%|████████▋ | 13985/16110 [19:51:26<11:31:21, 19.52s/it]

 87%|████████▋ | 13986/16110 [19:51:46<11:32:42, 19.57s/it]

 87%|████████▋ | 13987/16110 [19:52:00<10:38:24, 18.04s/it]

 87%|████████▋ | 13988/16110 [19:52:18<10:35:30, 17.97s/it]

 87%|████████▋ | 13989/16110 [19:52:40<11:17:51, 19.18s/it]

 87%|████████▋ | 13990/16110 [19:53:00<11:27:23, 19.45s/it]

 87%|████████▋ | 13991/16110 [19:53:20<11:25:27, 19.41s/it]

 87%|████████▋ | 13992/16110 [19:53:37<11:02:43, 18.77s/it]

 87%|████████▋ | 13993/16110 [19:53:50<10:05:58, 17.17s/it]

 87%|████████▋ | 13994/16110 [19:54:10<10:32:47, 17.94s/it]

 87%|████████▋ | 13995/16110 [19:54:29<10:41:13, 18.19s/it]

 87%|████████▋ | 13996/16110 [19:54:43<9:55:28, 16.90s/it]

 87%|████████▋ | 13997/16110 [19:54:56<9:12:04, 15.68s/it]

 87%|████████▋ | 13998/16110 [19:55:07<8:22:31, 14.28s/it]


 87%|████████▋ | 14000/16110 [19:55:41<9:08:23, 15.59s/it]
{'loss': 0.0858, 'learning_rate': 1.7603863796160447e-06, 'rewards/chosen': -4.501986980438232, 'rewards/rejected': -8.767648696899414, 'rewards/accuracies': 1.0, 'rewards/margins': 4.265661239624023, 'policy_logps/rejected': -444.8352966308594, 'policy_logps/chosen': -472.478515625, 'referece_logps/rejected': -357.15875244140625, 'referece_logps/chosen': -427.4586181640625, 'logits/rejected': 0.007995709776878357, 'logits/chosen': -0.2268868386745453, 'epoch': 7.82}

 87%|████████▋ | 14001/16110 [19:56:18<12:45:15, 21.77s/it]


 87%|████████▋ | 14003/16110 [19:56:42<9:51:46, 16.85s/it]
{'loss': 0.1346, 'learning_rate': 1.7599945147353966e-06, 'rewards/chosen': -5.0444560050964355, 'rewards/rejected': -8.57455825805664, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5301027297973633, 'policy_logps/rejected': -356.5496826171875, 'policy_logps/chosen': -369.4620361328125, 'referece_logps/rejected': -270.8041076660156, 'referece_logps/chosen': -319.0174560546875, 'logits/rejected': 0.7184865474700928, 'logits/chosen': 0.5452550053596497, 'epoch': 7.82}

 87%|████████▋ | 14004/16110 [19:56:58<9:49:46, 16.80s/it]


 87%|████████▋ | 14006/16110 [19:57:28<9:13:11, 15.78s/it]
{'loss': 0.1938, 'learning_rate': 1.759602373378989e-06, 'rewards/chosen': -4.973014831542969, 'rewards/rejected': -8.036619186401367, 'rewards/accuracies': 0.75, 'rewards/margins': 3.063605785369873, 'policy_logps/rejected': -487.3344421386719, 'policy_logps/chosen': -389.2430419921875, 'referece_logps/rejected': -406.9682312011719, 'referece_logps/chosen': -339.5129089355469, 'logits/rejected': -0.6654335856437683, 'logits/chosen': -0.7106943726539612, 'epoch': 7.82}

 87%|████████▋ | 14007/16110 [19:57:47<9:48:24, 16.79s/it]

 87%|████████▋ | 14008/16110 [19:58:05<9:57:49, 17.06s/it]

 87%|████████▋ | 14009/16110 [19:58:21<9:53:28, 16.95s/it]

 87%|████████▋ | 14010/16110 [19:58:38<9:56:34, 17.04s/it]
[2024-04-06 11:06:53,089] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 14011/16110 [19:59:01<10:58:30, 18.82s/it]


 87%|████████▋ | 14013/16110 [19:59:42<11:28:44, 19.71s/it]
[2024-04-06 11:07:33,441] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1506, 'learning_rate': 1.758686302498723e-06, 'rewards/chosen': -1.6387386322021484, 'rewards/rejected': -9.83840560913086, 'rewards/accuracies': 1.0, 'rewards/margins': 8.199666023254395, 'policy_logps/rejected': -644.2258911132812, 'policy_logps/chosen': -531.15087890625, 'referece_logps/rejected': -545.8418579101562, 'referece_logps/chosen': -514.7634887695312, 'logits/rejected': 0.20577850937843323, 'logits/chosen': 0.18719588220119476, 'epoch': 7.83}

 87%|████████▋ | 14014/16110 [19:59:59<10:58:20, 18.85s/it]

 87%|████████▋ | 14015/16110 [20:00:18<11:08:42, 19.15s/it]

 87%|████████▋ | 14016/16110 [20:00:35<10:44:33, 18.47s/it]

 87%|████████▋ | 14017/16110 [20:00:53<10:39:01, 18.32s/it]

 87%|████████▋ | 14018/16110 [20:01:14<11:08:01, 19.16s/it]

 87%|████████▋ | 14019/16110 [20:01:34<11:13:05, 19.31s/it]

 87%|████████▋ | 14020/16110 [20:01:53<11:13:03, 19.32s/it]

 87%|████████▋ | 14021/16110 [20:02:07<10:15:35, 17.68s/it]

 87%|████████▋ | 14022/16110 [20:02:26<10:27:21, 18.03s/it]
[2024-04-06 11:10:38,378] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 14023/16110 [20:02:47<10:53:33, 18.79s/it]

 87%|████████▋ | 14024/16110 [20:03:05<10:46:14, 18.59s/it]


 87%|████████▋ | 14026/16110 [20:03:38<10:09:25, 17.55s/it]
{'loss': 0.1036, 'learning_rate': 1.7569810427203565e-06, 'rewards/chosen': -4.46469783782959, 'rewards/rejected': -10.218643188476562, 'rewards/accuracies': 1.0, 'rewards/margins': 5.753945350646973, 'policy_logps/rejected': -440.9027404785156, 'policy_logps/chosen': -337.990966796875, 'referece_logps/rejected': -338.71630859375, 'referece_logps/chosen': -293.343994140625, 'logits/rejected': 0.11230787634849548, 'logits/chosen': 0.05813993513584137, 'epoch': 7.84}

 87%|████████▋ | 14027/16110 [20:03:55<10:07:43, 17.51s/it]

 87%|████████▋ | 14028/16110 [20:04:11<9:48:48, 16.97s/it]

 87%|████████▋ | 14029/16110 [20:04:27<9:39:46, 16.72s/it]

 87%|████████▋ | 14030/16110 [20:04:40<9:02:16, 15.64s/it]

 87%|████████▋ | 14031/16110 [20:04:56<9:00:31, 15.60s/it]

 87%|████████▋ | 14032/16110 [20:05:15<9:41:47, 16.80s/it]

 87%|████████▋ | 14033/16110 [20:05:34<9:57:32, 17.26s/it]

 87%|████████▋ | 14034/16110 [20:05:56<10:44:46, 18.64s/it]

 87%|████████▋ | 14035/16110 [20:06:17<11:11:58, 19.43s/it]

 87%|████████▋ | 14036/16110 [20:06:37<11:13:52, 19.50s/it]

 87%|████████▋ | 14037/16110 [20:06:51<10:25:36, 18.11s/it]

 87%|████████▋ | 14038/16110 [20:07:08<10:06:05, 17.55s/it]

 87%|████████▋ | 14039/16110 [20:07:22<9:29:24, 16.50s/it]

 87%|████████▋ | 14040/16110 [20:07:41<9:59:47, 17.39s/it]

 87%|████████▋ | 14041/16110 [20:07:57<9:40:55, 16.85s/it]

 87%|████████▋ | 14042/16110 [20:08:13<9:36:17, 16.72s/it]

 87%|████████▋ | 14043/16110 [20:08:33<10:06:14, 17.60s/it]

 87%|████████▋ | 14044/16110 [20:08:47<9:32:19, 16.62s/it]

 87%|████████▋ | 14045/16110 [20:09:03<9:29:13, 16.54s/it]

 87%|████████▋ | 14046/16110 [20:09:18<9:05:52, 15.87s/it]

 87%|████████▋ | 14047/16110 [20:09:34<9:09:18, 15.98s/it]


 87%|████████▋ | 14049/16110 [20:10:07<9:26:27, 16.49s/it]

 87%|████████▋ | 14050/16110 [20:10:27<10:01:00, 17.51s/it]

 87%|████████▋ | 14051/16110 [20:10:39<8:55:42, 15.61s/it]

 87%|████████▋ | 14052/16110 [20:10:51<8:23:28, 14.68s/it]

 87%|████████▋ | 14053/16110 [20:11:09<8:51:52, 15.51s/it]

 87%|████████▋ | 14054/16110 [20:11:20<8:11:48, 14.35s/it]

 87%|████████▋ | 14055/16110 [20:11:32<7:49:54, 13.72s/it]

 87%|████████▋ | 14056/16110 [20:11:54<9:07:03, 15.98s/it]

 87%|████████▋ | 14057/16110 [20:12:07<8:43:09, 15.29s/it]

 87%|████████▋ | 14058/16110 [20:12:23<8:48:04, 15.44s/it]

 87%|████████▋ | 14059/16110 [20:12:42<9:23:22, 16.48s/it]

 87%|████████▋ | 14060/16110 [20:13:02<9:59:38, 17.55s/it]

 87%|████████▋ | 14061/16110 [20:13:22<10:21:03, 18.19s/it]

 87%|████████▋ | 14062/16110 [20:13:33<9:09:31, 16.10s/it]

 87%|████████▋ | 14063/16110 [20:13:47<8:46:17, 15.43s/it]

 87%|████████▋ | 14064/16110 [20:14:10<10:07:19, 17.81s/it]
[2024-04-06 11:22:01,858] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 14065/16110 [20:14:25<9:39:05, 16.99s/it]

 87%|████████▋ | 14066/16110 [20:14:44<10:00:15, 17.62s/it]

 87%|████████▋ | 14067/16110 [20:15:04<10:24:54, 18.35s/it]
[2024-04-06 11:22:56,087] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 14068/16110 [20:15:23<10:25:19, 18.37s/it]

 87%|████████▋ | 14069/16110 [20:15:41<10:19:52, 18.22s/it]

 87%|████████▋ | 14070/16110 [20:15:58<10:08:19, 17.89s/it]

 87%|████████▋ | 14071/16110 [20:16:14<9:46:47, 17.27s/it]

 87%|████████▋ | 14072/16110 [20:16:34<10:15:43, 18.13s/it]

 87%|████████▋ | 14073/16110 [20:16:47<9:21:37, 16.54s/it]

 87%|████████▋ | 14074/16110 [20:17:09<10:18:15, 18.22s/it]

 87%|████████▋ | 14075/16110 [20:17:27<10:20:10, 18.29s/it]

 87%|████████▋ | 14076/16110 [20:17:47<10:33:09, 18.68s/it]

 87%|████████▋ | 14077/16110 [20:18:06<10:37:17, 18.81s/it]

 87%|████████▋ | 14078/16110 [20:18:28<11:12:22, 19.85s/it]
[2024-04-06 11:26:19,860] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 87%|████████▋ | 14079/16110 [20:18:43<10:22:05, 18.38s/it]

 87%|████████▋ | 14080/16110 [20:18:57<9:40:09, 17.15s/it]

 87%|████████▋ | 14081/16110 [20:19:19<10:20:40, 18.35s/it]

 87%|████████▋ | 14082/16110 [20:19:36<10:10:26, 18.06s/it]

 87%|████████▋ | 14083/16110 [20:19:56<10:28:27, 18.60s/it]

 87%|████████▋ | 14084/16110 [20:20:16<10:48:01, 19.19s/it]

 87%|████████▋ | 14085/16110 [20:20:33<10:18:03, 18.31s/it]

 87%|████████▋ | 14086/16110 [20:20:44<9:11:35, 16.35s/it]
{'loss': 0.1371, 'learning_rate': 1.7490437868264025e-06, 'rewards/chosen': -5.510585308074951, 'rewards/rejected': -9.624005317687988, 'rewards/accuracies': 0.875, 'rewards/margins': 4.113419055938721, 'policy_logps/rejected': -338.8559875488281, 'policy_logps/chosen': -370.7406005859375, 'referece_logps/rejected': -242.6159210205078, 'referece_logps/chosen': -315.634765625, 'logits/rejected': -0.17746004462242126, 'logits/chosen': -0.4997239410877228, 'epoch': 7.87}


 87%|████████▋ | 14088/16110 [20:21:20<9:49:45, 17.50s/it]
[2024-04-06 11:29:11,640] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1641, 'learning_rate': 1.7487773280309968e-06, 'rewards/chosen': -4.50266695022583, 'rewards/rejected': -9.111349105834961, 'rewards/accuracies': 1.0, 'rewards/margins': 4.608682632446289, 'policy_logps/rejected': -582.2685546875, 'policy_logps/chosen': -591.6583862304688, 'referece_logps/rejected': -491.155029296875, 'referece_logps/chosen': -546.6317749023438, 'logits/rejected': 0.8781563639640808, 'logits/chosen': 0.7758594751358032, 'epoch': 7.87}
[2024-04-06 11:29:23,155] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 87%|████████▋ | 14090/16110 [20:21:51<9:25:37, 16.80s/it]

 87%|████████▋ | 14091/16110 [20:22:08<9:25:08, 16.79s/it]
{'loss': 0.1619, 'learning_rate': 1.748377412855438e-06, 'rewards/chosen': -3.9346325397491455, 'rewards/rejected': -8.803979873657227, 'rewards/accuracies': 1.0, 'rewards/margins': 4.869346618652344, 'policy_logps/rejected': -564.4088134765625, 'policy_logps/chosen': -308.11187744140625, 'referece_logps/rejected': -476.3690185546875, 'referece_logps/chosen': -268.76556396484375, 'logits/rejected': 0.09469667077064514, 'logits/chosen': 0.3717612326145172, 'epoch': 7.87}


 87%|████████▋ | 14093/16110 [20:22:40<9:02:03, 16.12s/it]

 87%|████████▋ | 14094/16110 [20:22:53<8:31:50, 15.23s/it]

 87%|████████▋ | 14095/16110 [20:23:14<9:26:19, 16.86s/it]

 87%|████████▋ | 14096/16110 [20:23:31<9:30:31, 17.00s/it]

 88%|████████▊ | 14097/16110 [20:23:50<9:50:08, 17.59s/it]

 88%|████████▊ | 14098/16110 [20:24:12<10:38:17, 19.03s/it]
[2024-04-06 11:32:04,068] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14099/16110 [20:24:32<10:47:45, 19.33s/it]

 88%|████████▊ | 14100/16110 [20:24:47<10:03:21, 18.01s/it]
{'loss': 0.1482, 'learning_rate': 1.7471760344135055e-06, 'rewards/chosen': -4.028894901275635, 'rewards/rejected': -8.823179244995117, 'rewards/accuracies': 1.0, 'rewards/margins': 4.794284820556641, 'policy_logps/rejected': -407.92022705078125, 'policy_logps/chosen': -434.85113525390625, 'referece_logps/rejected': -319.68841552734375, 'referece_logps/chosen': -394.5621643066406, 'logits/rejected': 0.7498127818107605, 'logits/chosen': 0.5270551443099976, 'epoch': 7.88}


 88%|████████▊ | 14102/16110 [20:25:22<9:49:48, 17.62s/it]

 88%|████████▊ | 14103/16110 [20:25:38<9:31:17, 17.08s/it]

 88%|████████▊ | 14104/16110 [20:25:49<8:31:24, 15.30s/it]

 88%|████████▊ | 14105/16110 [20:26:01<8:01:03, 14.40s/it]

 88%|████████▊ | 14106/16110 [20:26:21<8:51:48, 15.92s/it]

 88%|████████▊ | 14107/16110 [20:26:40<9:28:41, 17.04s/it]

 88%|████████▊ | 14108/16110 [20:27:01<9:58:40, 17.94s/it]

 88%|████████▊ | 14109/16110 [20:27:20<10:17:02, 18.50s/it]

 88%|████████▊ | 14110/16110 [20:27:32<9:10:55, 16.53s/it]

 88%|████████▊ | 14111/16110 [20:27:52<9:46:54, 17.62s/it]
[2024-04-06 11:35:44,083] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14112/16110 [20:28:11<9:54:09, 17.84s/it]

 88%|████████▊ | 14113/16110 [20:28:29<9:58:36, 17.99s/it]

 88%|████████▊ | 14114/16110 [20:28:48<10:08:34, 18.29s/it]

 88%|████████▊ | 14115/16110 [20:29:10<10:46:56, 19.46s/it]
{'loss': 0.1512, 'learning_rate': 1.7451683027062728e-06, 'rewards/chosen': -4.530607223510742, 'rewards/rejected': -10.970765113830566, 'rewards/accuracies': 1.0, 'rewards/margins': 6.440156936645508, 'policy_logps/rejected': -311.62158203125, 'policy_logps/chosen': -275.369140625, 'referece_logps/rejected': -201.91395568847656, 'referece_logps/chosen': -230.06307983398438, 'logits/rejected': -0.010983770713210106, 'logits/chosen': 0.020909329876303673, 'epoch': 7.89}


 88%|████████▊ | 14117/16110 [20:29:49<10:41:17, 19.31s/it]
[2024-04-06 11:37:40,432] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14118/16110 [20:30:09<10:52:46, 19.66s/it]

 88%|████████▊ | 14119/16110 [20:30:28<10:46:53, 19.49s/it]

 88%|████████▊ | 14120/16110 [20:30:43<9:58:14, 18.04s/it]

 88%|████████▊ | 14121/16110 [20:31:03<10:15:00, 18.55s/it]

 88%|████████▊ | 14122/16110 [20:31:21<10:10:02, 18.41s/it]

 88%|████████▊ | 14123/16110 [20:31:41<10:26:28, 18.92s/it]
{'loss': 0.1675, 'learning_rate': 1.7440947402382267e-06, 'rewards/chosen': -4.922208786010742, 'rewards/rejected': -9.853678703308105, 'rewards/accuracies': 0.875, 'rewards/margins': 4.931469917297363, 'policy_logps/rejected': -432.2368469238281, 'policy_logps/chosen': -489.52459716796875, 'referece_logps/rejected': -333.70001220703125, 'referece_logps/chosen': -440.302490234375, 'logits/rejected': -0.34804806113243103, 'logits/chosen': -0.3653445541858673, 'epoch': 7.89}


 88%|████████▊ | 14125/16110 [20:32:18<10:20:36, 18.76s/it]

 88%|████████▊ | 14126/16110 [20:32:32<9:34:59, 17.39s/it]
{'loss': 0.1744, 'learning_rate': 1.7436916578954763e-06, 'rewards/chosen': -4.762768268585205, 'rewards/rejected': -11.327570915222168, 'rewards/accuracies': 1.0, 'rewards/margins': 6.564802169799805, 'policy_logps/rejected': -406.0549011230469, 'policy_logps/chosen': -363.35626220703125, 'referece_logps/rejected': -292.7792053222656, 'referece_logps/chosen': -315.72857666015625, 'logits/rejected': -0.43523547053337097, 'logits/chosen': -0.42038917541503906, 'epoch': 7.89}


 88%|████████▊ | 14128/16110 [20:33:01<8:37:20, 15.66s/it]

 88%|████████▊ | 14129/16110 [20:33:17<8:47:01, 15.96s/it]

 88%|████████▊ | 14130/16110 [20:33:35<9:00:19, 16.37s/it]

 88%|████████▊ | 14131/16110 [20:33:55<9:40:45, 17.61s/it]

 88%|████████▊ | 14132/16110 [20:34:09<9:01:26, 16.42s/it]

 88%|████████▊ | 14133/16110 [20:34:29<9:39:55, 17.60s/it]

 88%|████████▊ | 14134/16110 [20:34:49<10:02:15, 18.29s/it]

 88%|████████▊ | 14135/16110 [20:35:08<10:05:53, 18.41s/it]

 88%|████████▊ | 14136/16110 [20:35:26<10:03:13, 18.34s/it]

 88%|████████▊ | 14137/16110 [20:35:47<10:28:25, 19.11s/it]

 88%|████████▊ | 14138/16110 [20:36:03<9:59:48, 18.25s/it]

 88%|████████▊ | 14139/16110 [20:36:19<9:40:28, 17.67s/it]

 88%|████████▊ | 14140/16110 [20:36:39<10:01:37, 18.32s/it]

 88%|████████▊ | 14141/16110 [20:37:01<10:36:11, 19.39s/it]
[2024-04-06 11:44:52,647] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14142/16110 [20:37:19<10:26:11, 19.09s/it]

 88%|████████▊ | 14143/16110 [20:37:40<10:40:07, 19.53s/it]
[2024-04-06 11:45:31,590] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14144/16110 [20:38:00<10:42:00, 19.59s/it]

 88%|████████▊ | 14145/16110 [20:38:20<10:46:56, 19.75s/it]

 88%|████████▊ | 14146/16110 [20:38:37<10:23:54, 19.06s/it]
{'loss': 0.1449, 'learning_rate': 1.7409975354380075e-06, 'rewards/chosen': -3.2103376388549805, 'rewards/rejected': -8.300175666809082, 'rewards/accuracies': 0.875, 'rewards/margins': 5.089838981628418, 'policy_logps/rejected': -361.942138671875, 'policy_logps/chosen': -277.5689392089844, 'referece_logps/rejected': -278.94036865234375, 'referece_logps/chosen': -245.46554565429688, 'logits/rejected': -0.45647478103637695, 'logits/chosen': -0.5943678021430969, 'epoch': 7.9}


 88%|████████▊ | 14148/16110 [20:39:13<10:05:22, 18.51s/it]
{'loss': 0.1204, 'learning_rate': 1.7407274635362626e-06, 'rewards/chosen': -4.157901287078857, 'rewards/rejected': -7.565898895263672, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4079971313476562, 'policy_logps/rejected': -410.5981140136719, 'policy_logps/chosen': -446.4189453125, 'referece_logps/rejected': -334.93914794921875, 'referece_logps/chosen': -404.83990478515625, 'logits/rejected': 0.16817297041416168, 'logits/chosen': 0.09532636404037476, 'epoch': 7.9}
[2024-04-06 11:47:28,355] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 14150/16110 [20:39:55<10:39:21, 19.57s/it]
[2024-04-06 11:47:46,802] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14151/16110 [20:40:11<10:05:37, 18.55s/it]
{'loss': 0.1183, 'learning_rate': 1.7403221311417374e-06, 'rewards/chosen': -4.853916168212891, 'rewards/rejected': -10.810904502868652, 'rewards/accuracies': 1.0, 'rewards/margins': 5.956988334655762, 'policy_logps/rejected': -581.2357788085938, 'policy_logps/chosen': -340.17779541015625, 'referece_logps/rejected': -473.1267395019531, 'referece_logps/chosen': -291.6386413574219, 'logits/rejected': 0.11579452455043793, 'logits/chosen': 0.23714151978492737, 'epoch': 7.91}

 88%|████████▊ | 14152/16110 [20:40:25<9:14:30, 16.99s/it]
[2024-04-06 11:48:36,434] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14153/16110 [20:40:45<9:44:45, 17.93s/it]


 88%|████████▊ | 14155/16110 [20:41:24<10:10:51, 18.75s/it]

 88%|████████▊ | 14156/16110 [20:41:40<9:42:24, 17.88s/it]

 88%|████████▊ | 14157/16110 [20:42:00<10:02:59, 18.52s/it]

 88%|████████▊ | 14158/16110 [20:42:17<9:51:46, 18.19s/it]
{'loss': 0.1628, 'learning_rate': 1.7393753084570664e-06, 'rewards/chosen': -4.647087574005127, 'rewards/rejected': -10.394688606262207, 'rewards/accuracies': 0.875, 'rewards/margins': 5.747600078582764, 'policy_logps/rejected': -492.1793518066406, 'policy_logps/chosen': -394.2156677246094, 'referece_logps/rejected': -388.23248291015625, 'referece_logps/chosen': -347.7447814941406, 'logits/rejected': -0.7501592636108398, 'logits/chosen': -0.7314208745956421, 'epoch': 7.91}


 88%|████████▊ | 14160/16110 [20:42:49<9:24:45, 17.38s/it]

 88%|████████▊ | 14161/16110 [20:43:06<9:20:12, 17.25s/it]

 88%|████████▊ | 14162/16110 [20:43:22<9:05:27, 16.80s/it]

 88%|████████▊ | 14163/16110 [20:43:41<9:33:50, 17.68s/it]

 88%|████████▊ | 14164/16110 [20:43:56<9:06:29, 16.85s/it]

 88%|████████▊ | 14165/16110 [20:44:14<9:18:39, 17.23s/it]

 88%|████████▊ | 14166/16110 [20:44:28<8:41:41, 16.10s/it]

 88%|████████▊ | 14167/16110 [20:44:48<9:18:55, 17.26s/it]

 88%|████████▊ | 14168/16110 [20:45:06<9:31:10, 17.65s/it]

 88%|████████▊ | 14169/16110 [20:45:26<9:50:40, 18.26s/it]

 88%|████████▊ | 14170/16110 [20:45:46<10:07:32, 18.79s/it]

 88%|████████▊ | 14171/16110 [20:45:57<8:54:47, 16.55s/it]
{'loss': 0.131, 'learning_rate': 1.7376130397042737e-06, 'rewards/chosen': -3.690722942352295, 'rewards/rejected': -9.792994499206543, 'rewards/accuracies': 1.0, 'rewards/margins': 6.102272033691406, 'policy_logps/rejected': -367.68695068359375, 'policy_logps/chosen': -354.2383117675781, 'referece_logps/rejected': -269.7569885253906, 'referece_logps/chosen': -317.3310852050781, 'logits/rejected': -0.39288759231567383, 'logits/chosen': -0.4715835452079773, 'epoch': 7.92}


 88%|████████▊ | 14173/16110 [20:46:31<8:53:54, 16.54s/it]

 88%|████████▊ | 14174/16110 [20:46:48<8:58:49, 16.70s/it]

 88%|████████▊ | 14175/16110 [20:47:10<9:44:11, 18.11s/it]
[2024-04-06 11:55:01,530] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14176/16110 [20:47:30<10:06:09, 18.81s/it]

 88%|████████▊ | 14177/16110 [20:47:50<10:14:13, 19.07s/it]

 88%|████████▊ | 14178/16110 [20:48:10<10:26:50, 19.47s/it]

 88%|████████▊ | 14179/16110 [20:48:27<10:03:38, 18.76s/it]
{'loss': 0.1511, 'learning_rate': 1.7365260614107475e-06, 'rewards/chosen': -4.230295181274414, 'rewards/rejected': -10.235219955444336, 'rewards/accuracies': 0.875, 'rewards/margins': 6.00492525100708, 'policy_logps/rejected': -395.3592834472656, 'policy_logps/chosen': -408.62005615234375, 'referece_logps/rejected': -293.0071105957031, 'referece_logps/chosen': -366.3171081542969, 'logits/rejected': 0.59159916639328, 'logits/chosen': 0.4535166025161743, 'epoch': 7.92}

 88%|████████▊ | 14180/16110 [20:48:45<9:53:07, 18.44s/it]
[2024-04-06 11:56:58,809] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 14182/16110 [20:49:29<10:45:10, 20.08s/it]
[2024-04-06 11:57:20,227] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14183/16110 [20:49:48<10:41:18, 19.97s/it]

 88%|████████▊ | 14184/16110 [20:50:04<9:56:58, 18.60s/it]

 88%|████████▊ | 14185/16110 [20:50:20<9:31:53, 17.83s/it]

 88%|████████▊ | 14186/16110 [20:50:36<9:14:16, 17.29s/it]

 88%|████████▊ | 14187/16110 [20:50:47<8:15:49, 15.47s/it]

 88%|████████▊ | 14188/16110 [20:51:08<9:11:39, 17.22s/it]

 88%|████████▊ | 14189/16110 [20:51:29<9:41:11, 18.15s/it]
{'loss': 0.1071, 'learning_rate': 1.735164659491805e-06, 'rewards/chosen': -6.27118444442749, 'rewards/rejected': -8.93821907043457, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6670331954956055, 'policy_logps/rejected': -331.921875, 'policy_logps/chosen': -322.28863525390625, 'referece_logps/rejected': -242.5396728515625, 'referece_logps/chosen': -259.5767822265625, 'logits/rejected': -0.0979255884885788, 'logits/chosen': -0.3279861807823181, 'epoch': 7.93}


 88%|████████▊ | 14191/16110 [20:52:00<9:11:12, 17.23s/it]
{'loss': 0.2522, 'learning_rate': 1.7348920223411651e-06, 'rewards/chosen': -4.789615154266357, 'rewards/rejected': -9.2062406539917, 'rewards/accuracies': 1.0, 'rewards/margins': 4.416625022888184, 'policy_logps/rejected': -529.8369750976562, 'policy_logps/chosen': -571.9373779296875, 'referece_logps/rejected': -437.7745361328125, 'referece_logps/chosen': -524.0411987304688, 'logits/rejected': 0.4906761348247528, 'logits/chosen': 0.41873887181282043, 'epoch': 7.93}


 88%|████████▊ | 14193/16110 [20:52:32<9:00:25, 16.91s/it]

 88%|████████▊ | 14194/16110 [20:52:46<8:29:13, 15.95s/it]

 88%|████████▊ | 14195/16110 [20:53:01<8:17:44, 15.59s/it]

 88%|████████▊ | 14196/16110 [20:53:23<9:23:30, 17.66s/it]
{'loss': 0.1404, 'learning_rate': 1.7342099097258425e-06, 'rewards/chosen': -5.536744117736816, 'rewards/rejected': -7.975955009460449, 'rewards/accuracies': 0.75, 'rewards/margins': 2.439210891723633, 'policy_logps/rejected': -341.5008544921875, 'policy_logps/chosen': -339.3640441894531, 'referece_logps/rejected': -261.7413024902344, 'referece_logps/chosen': -283.99658203125, 'logits/rejected': 0.18884791433811188, 'logits/chosen': 0.2298438400030136, 'epoch': 7.93}

 88%|████████▊ | 14197/16110 [20:53:39<9:10:18, 17.26s/it]

 88%|████████▊ | 14198/16110 [20:53:56<8:59:38, 16.93s/it]


 88%|████████▊ | 14200/16110 [20:54:26<8:27:37, 15.95s/it]

 88%|████████▊ | 14201/16110 [20:54:43<8:30:50, 16.06s/it]

 88%|████████▊ | 14202/16110 [20:55:02<9:05:41, 17.16s/it]
{'loss': 0.1971, 'learning_rate': 1.733390395297437e-06, 'rewards/chosen': -5.195610523223877, 'rewards/rejected': -7.989839553833008, 'rewards/accuracies': 0.625, 'rewards/margins': 2.794229030609131, 'policy_logps/rejected': -415.8238525390625, 'policy_logps/chosen': -489.5793151855469, 'referece_logps/rejected': -335.92547607421875, 'referece_logps/chosen': -437.6231689453125, 'logits/rejected': -0.44186776876449585, 'logits/chosen': -0.6701375842094421, 'epoch': 7.93}

 88%|████████▊ | 14203/16110 [20:55:22<9:28:52, 17.90s/it]

 88%|████████▊ | 14204/16110 [20:55:42<9:45:19, 18.43s/it]


 88%|████████▊ | 14206/16110 [20:56:13<9:00:53, 17.05s/it]
{'loss': 0.0748, 'learning_rate': 1.7328434593881242e-06, 'rewards/chosen': -4.267837047576904, 'rewards/rejected': -9.753317832946777, 'rewards/accuracies': 0.875, 'rewards/margins': 5.485480785369873, 'policy_logps/rejected': -465.2030944824219, 'policy_logps/chosen': -439.0227355957031, 'referece_logps/rejected': -367.6698913574219, 'referece_logps/chosen': -396.3443603515625, 'logits/rejected': -0.13464173674583435, 'logits/chosen': -0.09595917910337448, 'epoch': 7.94}

 88%|████████▊ | 14207/16110 [20:56:28<8:44:19, 16.53s/it]

 88%|████████▊ | 14208/16110 [20:56:46<9:00:38, 17.05s/it]


 88%|████████▊ | 14210/16110 [20:57:23<9:18:56, 17.65s/it]
{'loss': 0.0873, 'learning_rate': 1.7322960495258068e-06, 'rewards/chosen': -4.588564872741699, 'rewards/rejected': -8.876741409301758, 'rewards/accuracies': 0.875, 'rewards/margins': 4.288176536560059, 'policy_logps/rejected': -329.71624755859375, 'policy_logps/chosen': -416.8219299316406, 'referece_logps/rejected': -240.9488525390625, 'referece_logps/chosen': -370.936279296875, 'logits/rejected': 0.04319387674331665, 'logits/chosen': -0.2922895550727844, 'epoch': 7.94}
[2024-04-06 12:05:37,544] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 14212/16110 [20:58:03<9:43:55, 18.46s/it]
[2024-04-06 12:05:54,307] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.189, 'learning_rate': 1.7320221669728926e-06, 'rewards/chosen': -5.474095821380615, 'rewards/rejected': -9.107646942138672, 'rewards/accuracies': 1.0, 'rewards/margins': 3.633552074432373, 'policy_logps/rejected': -311.4507751464844, 'policy_logps/chosen': -257.3456726074219, 'referece_logps/rejected': -220.3743133544922, 'referece_logps/chosen': -202.60472106933594, 'logits/rejected': 0.49812453985214233, 'logits/chosen': 0.6084700226783752, 'epoch': 7.94}


 88%|████████▊ | 14214/16110 [20:58:45<10:30:33, 19.95s/it]
[2024-04-06 12:06:36,572] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1082, 'learning_rate': 1.7317481660645116e-06, 'rewards/chosen': -5.586622714996338, 'rewards/rejected': -10.7084379196167, 'rewards/accuracies': 0.875, 'rewards/margins': 5.121813774108887, 'policy_logps/rejected': -612.9899291992188, 'policy_logps/chosen': -605.908935546875, 'referece_logps/rejected': -505.9055480957031, 'referece_logps/chosen': -550.042724609375, 'logits/rejected': 0.8160845041275024, 'logits/chosen': 0.8309416174888611, 'epoch': 7.94}

 88%|████████▊ | 14215/16110 [20:59:00<9:45:36, 18.54s/it]


 88%|████████▊ | 14217/16110 [20:59:37<9:47:14, 18.61s/it]

 88%|████████▊ | 14218/16110 [20:59:55<9:45:02, 18.55s/it]

 88%|████████▊ | 14219/16110 [21:00:14<9:42:35, 18.49s/it]

 88%|████████▊ | 14220/16110 [21:00:33<9:55:53, 18.92s/it]
{'loss': 0.1111, 'learning_rate': 1.730925453649675e-06, 'rewards/chosen': -3.4994869232177734, 'rewards/rejected': -8.862571716308594, 'rewards/accuracies': 0.875, 'rewards/margins': 5.36308479309082, 'policy_logps/rejected': -364.1014404296875, 'policy_logps/chosen': -245.3155975341797, 'referece_logps/rejected': -275.4757080078125, 'referece_logps/chosen': -210.32070922851562, 'logits/rejected': 0.5085618495941162, 'logits/chosen': 0.3836854100227356, 'epoch': 7.94}

 88%|████████▊ | 14221/16110 [21:00:54<10:10:27, 19.39s/it]

 88%|████████▊ | 14222/16110 [21:01:08<9:20:15, 17.80s/it]

 88%|████████▊ | 14223/16110 [21:01:26<9:22:20, 17.88s/it]


 88%|████████▊ | 14225/16110 [21:01:55<8:33:44, 16.35s/it]

 88%|████████▊ | 14226/16110 [21:02:14<8:55:50, 17.06s/it]
{'loss': 0.1883, 'learning_rate': 1.730101677631628e-06, 'rewards/chosen': -3.176936149597168, 'rewards/rejected': -6.221683502197266, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0447471141815186, 'policy_logps/rejected': -268.1343688964844, 'policy_logps/chosen': -300.3177795410156, 'referece_logps/rejected': -205.9175262451172, 'referece_logps/chosen': -268.5483703613281, 'logits/rejected': 0.10173153877258301, 'logits/chosen': 0.187773197889328, 'epoch': 7.95}


 88%|████████▊ | 14228/16110 [21:02:47<8:48:31, 16.85s/it]
{'loss': 0.1013, 'learning_rate': 1.729826849476484e-06, 'rewards/chosen': -4.080824375152588, 'rewards/rejected': -9.578798294067383, 'rewards/accuracies': 1.0, 'rewards/margins': 5.497974872589111, 'policy_logps/rejected': -331.86273193359375, 'policy_logps/chosen': -379.80755615234375, 'referece_logps/rejected': -236.07472229003906, 'referece_logps/chosen': -338.99932861328125, 'logits/rejected': 0.1039794310927391, 'logits/chosen': -0.08893819153308868, 'epoch': 7.95}


 88%|████████▊ | 14230/16110 [21:03:25<9:25:21, 18.04s/it]

 88%|████████▊ | 14231/16110 [21:03:44<9:29:20, 18.18s/it]

 88%|████████▊ | 14232/16110 [21:03:59<9:03:10, 17.35s/it]
{'loss': 0.1315, 'learning_rate': 1.729276839209085e-06, 'rewards/chosen': -4.2593607902526855, 'rewards/rejected': -9.022896766662598, 'rewards/accuracies': 1.0, 'rewards/margins': 4.76353645324707, 'policy_logps/rejected': -492.2589111328125, 'policy_logps/chosen': -524.1604614257812, 'referece_logps/rejected': -402.0299377441406, 'referece_logps/chosen': -481.5668640136719, 'logits/rejected': -0.16592806577682495, 'logits/chosen': -0.3285278081893921, 'epoch': 7.95}


 88%|████████▊ | 14234/16110 [21:04:41<10:02:49, 19.28s/it]
[2024-04-06 12:12:32,896] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1364, 'learning_rate': 1.7290016571857574e-06, 'rewards/chosen': -6.294494152069092, 'rewards/rejected': -11.08829402923584, 'rewards/accuracies': 0.875, 'rewards/margins': 4.79379940032959, 'policy_logps/rejected': -445.3228759765625, 'policy_logps/chosen': -360.03619384765625, 'referece_logps/rejected': -334.43994140625, 'referece_logps/chosen': -297.09124755859375, 'logits/rejected': 0.2084559202194214, 'logits/chosen': 0.2863881289958954, 'epoch': 7.95}
[2024-04-06 12:12:50,322] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14235/16110 [21:04:59<9:45:07, 18.72s/it]

 88%|████████▊ | 14236/16110 [21:05:11<8:43:59, 16.78s/it]

 88%|████████▊ | 14237/16110 [21:05:28<8:47:51, 16.91s/it]
[2024-04-06 12:13:40,249] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 88%|████████▊ | 14239/16110 [21:06:01<8:31:57, 16.42s/it]
[2024-04-06 12:13:53,024] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14240/16110 [21:06:18<8:30:39, 16.38s/it]

 88%|████████▊ | 14241/16110 [21:06:34<8:27:14, 16.28s/it]
{'loss': 0.0676, 'learning_rate': 1.7280375921928324e-06, 'rewards/chosen': -5.059829235076904, 'rewards/rejected': -9.827166557312012, 'rewards/accuracies': 0.875, 'rewards/margins': 4.767337322235107, 'policy_logps/rejected': -306.99139404296875, 'policy_logps/chosen': -357.76678466796875, 'referece_logps/rejected': -208.71974182128906, 'referece_logps/chosen': -307.16845703125, 'logits/rejected': -0.05370283126831055, 'logits/chosen': -0.33047017455101013, 'epoch': 7.96}

 88%|████████▊ | 14242/16110 [21:06:53<8:52:02, 17.09s/it]


 88%|████████▊ | 14244/16110 [21:07:34<9:50:30, 18.99s/it]
[2024-04-06 12:15:25,666] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1092, 'learning_rate': 1.7276239799530965e-06, 'rewards/chosen': -4.1130242347717285, 'rewards/rejected': -11.033834457397461, 'rewards/accuracies': 1.0, 'rewards/margins': 6.920808792114258, 'policy_logps/rejected': -682.4030151367188, 'policy_logps/chosen': -527.8236083984375, 'referece_logps/rejected': -572.064697265625, 'referece_logps/chosen': -486.693359375, 'logits/rejected': -0.4555669128894806, 'logits/chosen': -0.5356937646865845, 'epoch': 7.96}


 88%|████████▊ | 14246/16110 [21:08:07<9:01:59, 17.45s/it]
[2024-04-06 12:15:58,989] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1083, 'learning_rate': 1.7273480913952057e-06, 'rewards/chosen': -3.796198606491089, 'rewards/rejected': -8.401188850402832, 'rewards/accuracies': 1.0, 'rewards/margins': 4.604990005493164, 'policy_logps/rejected': -241.5535888671875, 'policy_logps/chosen': -305.40118408203125, 'referece_logps/rejected': -157.54171752929688, 'referece_logps/chosen': -267.4391784667969, 'logits/rejected': 0.06828899681568146, 'logits/chosen': -0.001427978277206421, 'epoch': 7.96}

 88%|████████▊ | 14247/16110 [21:08:23<8:43:38, 16.86s/it]


 88%|████████▊ | 14249/16110 [21:08:52<8:03:14, 15.58s/it]
{'loss': 0.1495, 'learning_rate': 1.7269340380727849e-06, 'rewards/chosen': -5.320759296417236, 'rewards/rejected': -8.772616386413574, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4518566131591797, 'policy_logps/rejected': -356.00164794921875, 'policy_logps/chosen': -308.085693359375, 'referece_logps/rejected': -268.27545166015625, 'referece_logps/chosen': -254.87808227539062, 'logits/rejected': -0.23252272605895996, 'logits/chosen': -0.29038381576538086, 'epoch': 7.96}

 88%|████████▊ | 14250/16110 [21:09:11<8:34:14, 16.59s/it]
[2024-04-06 12:17:24,383] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14251/16110 [21:09:33<9:22:02, 18.14s/it]

 88%|████████▊ | 14252/16110 [21:09:55<9:59:11, 19.35s/it]


 88%|████████▊ | 14254/16110 [21:10:36<10:21:35, 20.09s/it]
[2024-04-06 12:18:27,873] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0744, 'learning_rate': 1.7262433616125026e-06, 'rewards/chosen': -5.654496669769287, 'rewards/rejected': -10.395063400268555, 'rewards/accuracies': 1.0, 'rewards/margins': 4.740567684173584, 'policy_logps/rejected': -459.8518371582031, 'policy_logps/chosen': -485.80462646484375, 'referece_logps/rejected': -355.90118408203125, 'referece_logps/chosen': -429.2596435546875, 'logits/rejected': 0.51229327917099, 'logits/chosen': 0.4660927951335907, 'epoch': 7.96}
[2024-04-06 12:18:40,511] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 88%|████████▊ | 14255/16110 [21:10:49<9:12:05, 17.86s/it]

 88%|████████▊ | 14256/16110 [21:11:03<8:34:18, 16.64s/it]


 89%|████████▊ | 14258/16110 [21:11:44<9:41:10, 18.83s/it]
{'loss': 0.1127, 'learning_rate': 1.7256902920156896e-06, 'rewards/chosen': -4.841128826141357, 'rewards/rejected': -10.58864974975586, 'rewards/accuracies': 0.875, 'rewards/margins': 5.747520923614502, 'policy_logps/rejected': -381.15325927734375, 'policy_logps/chosen': -440.63214111328125, 'referece_logps/rejected': -275.2667236328125, 'referece_logps/chosen': -392.2208251953125, 'logits/rejected': 0.0142940953373909, 'logits/chosen': -0.3606777489185333, 'epoch': 7.97}

 89%|████████▊ | 14259/16110 [21:11:57<8:42:46, 16.95s/it]

 89%|████████▊ | 14260/16110 [21:12:13<8:32:39, 16.63s/it]

 89%|████████▊ | 14261/16110 [21:12:25<7:54:26, 15.40s/it]

 89%|████████▊ | 14262/16110 [21:12:37<7:21:01, 14.32s/it]

 89%|████████▊ | 14263/16110 [21:12:51<7:17:58, 14.23s/it]

 89%|████████▊ | 14264/16110 [21:13:11<8:11:35, 15.98s/it]

 89%|████████▊ | 14265/16110 [21:13:23<7:28:35, 14.59s/it]

 89%|████████▊ | 14266/16110 [21:13:35<7:11:54, 14.05s/it]


 89%|████████▊ | 14268/16110 [21:14:16<8:52:03, 17.33s/it]

 89%|████████▊ | 14269/16110 [21:14:36<9:20:51, 18.28s/it]

 89%|████████▊ | 14269/16110 [21:14:37<9:20:51, 18.28s/it]


 89%|████████▊ | 14271/16110 [21:15:14<9:37:04, 18.83s/it]
{'loss': 0.0704, 'learning_rate': 1.723889576392432e-06, 'rewards/chosen': -4.967763900756836, 'rewards/rejected': -10.571029663085938, 'rewards/accuracies': 1.0, 'rewards/margins': 5.603265762329102, 'policy_logps/rejected': -468.56512451171875, 'policy_logps/chosen': -658.3731689453125, 'referece_logps/rejected': -362.8548889160156, 'referece_logps/chosen': -608.6954345703125, 'logits/rejected': -0.06417752802371979, 'logits/chosen': -0.1604660451412201, 'epoch': 7.97}

 89%|████████▊ | 14272/16110 [21:15:27<8:40:24, 16.99s/it]

 89%|████████▊ | 14273/16110 [21:15:47<9:05:05, 17.80s/it]


 89%|████████▊ | 14275/16110 [21:16:22<9:16:41, 18.20s/it]
[2024-04-06 12:24:13,665] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1942, 'learning_rate': 1.7233345146305577e-06, 'rewards/chosen': -4.87904691696167, 'rewards/rejected': -10.610774993896484, 'rewards/accuracies': 1.0, 'rewards/margins': 5.731727600097656, 'policy_logps/rejected': -578.3516235351562, 'policy_logps/chosen': -562.9843139648438, 'referece_logps/rejected': -472.2439270019531, 'referece_logps/chosen': -514.19384765625, 'logits/rejected': -0.1797521412372589, 'logits/chosen': -0.317566454410553, 'epoch': 7.97}
[2024-04-06 12:24:30,642] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 89%|████████▊ | 14277/16110 [21:16:52<8:20:54, 16.40s/it]
{'loss': 0.1791, 'learning_rate': 1.7230568083009467e-06, 'rewards/chosen': -6.7680463790893555, 'rewards/rejected': -10.120328903198242, 'rewards/accuracies': 0.75, 'rewards/margins': 3.3522825241088867, 'policy_logps/rejected': -494.90625, 'policy_logps/chosen': -498.2256164550781, 'referece_logps/rejected': -393.70294189453125, 'referece_logps/chosen': -430.545166015625, 'logits/rejected': 0.6726445555686951, 'logits/chosen': 0.7343980669975281, 'epoch': 7.98}

 89%|████████▊ | 14278/16110 [21:17:11<8:48:27, 17.31s/it]

 89%|████████▊ | 14279/16110 [21:17:30<8:54:52, 17.53s/it]


 89%|████████▊ | 14281/16110 [21:18:00<8:24:07, 16.54s/it]
{'loss': 0.0753, 'learning_rate': 1.7225010449688782e-06, 'rewards/chosen': -5.006208419799805, 'rewards/rejected': -10.437402725219727, 'rewards/accuracies': 1.0, 'rewards/margins': 5.43119478225708, 'policy_logps/rejected': -387.3182678222656, 'policy_logps/chosen': -397.9354248046875, 'referece_logps/rejected': -282.94427490234375, 'referece_logps/chosen': -347.873291015625, 'logits/rejected': -0.2612822651863098, 'logits/chosen': -0.45892587304115295, 'epoch': 7.98}
[2024-04-06 12:26:12,996] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 89%|████████▊ | 14283/16110 [21:18:42<9:30:14, 18.73s/it]
[2024-04-06 12:26:33,605] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1323, 'learning_rate': 1.722222988056278e-06, 'rewards/chosen': -5.153224945068359, 'rewards/rejected': -13.357816696166992, 'rewards/accuracies': 1.0, 'rewards/margins': 8.204591751098633, 'policy_logps/rejected': -753.2537841796875, 'policy_logps/chosen': -461.43994140625, 'referece_logps/rejected': -619.6756591796875, 'referece_logps/chosen': -409.9076843261719, 'logits/rejected': -0.15887361764907837, 'logits/chosen': 0.024057865142822266, 'epoch': 7.98}


 89%|████████▊ | 14285/16110 [21:19:14<8:37:52, 17.03s/it]
{'loss': 0.1135, 'learning_rate': 1.7219448143725707e-06, 'rewards/chosen': -4.788277626037598, 'rewards/rejected': -9.171789169311523, 'rewards/accuracies': 0.875, 'rewards/margins': 4.383512496948242, 'policy_logps/rejected': -311.188232421875, 'policy_logps/chosen': -312.9815979003906, 'referece_logps/rejected': -219.47032165527344, 'referece_logps/chosen': -265.0988464355469, 'logits/rejected': -0.445790559053421, 'logits/chosen': -0.51308274269104, 'epoch': 7.98}


 89%|████████▊ | 14287/16110 [21:19:46<8:32:44, 16.88s/it]
{'loss': 0.0617, 'learning_rate': 1.7216665239627321e-06, 'rewards/chosen': -5.196963787078857, 'rewards/rejected': -11.09554672241211, 'rewards/accuracies': 1.0, 'rewards/margins': 5.898582935333252, 'policy_logps/rejected': -602.3304443359375, 'policy_logps/chosen': -512.4656372070312, 'referece_logps/rejected': -491.375, 'referece_logps/chosen': -460.4959716796875, 'logits/rejected': 0.7008663415908813, 'logits/chosen': 0.7714248299598694, 'epoch': 7.98}


 89%|████████▊ | 14289/16110 [21:20:23<8:54:11, 17.60s/it]

 89%|████████▊ | 14290/16110 [21:20:42<9:14:35, 18.28s/it]
{'loss': 0.0867, 'learning_rate': 1.721248869584909e-06, 'rewards/chosen': -4.354195594787598, 'rewards/rejected': -7.717754364013672, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3635590076446533, 'policy_logps/rejected': -564.24853515625, 'policy_logps/chosen': -409.4184265136719, 'referece_logps/rejected': -487.07098388671875, 'referece_logps/chosen': -365.87652587890625, 'logits/rejected': -0.09485229104757309, 'logits/chosen': -0.0020529180765151978, 'epoch': 7.98}
[2024-04-06 12:28:54,964] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 89%|████████▊ | 14292/16110 [21:21:14<8:26:08, 16.70s/it]
{'loss': 0.0914, 'learning_rate': 1.7209702875566351e-06, 'rewards/chosen': -6.781299114227295, 'rewards/rejected': -10.093690872192383, 'rewards/accuracies': 1.0, 'rewards/margins': 3.312391757965088, 'policy_logps/rejected': -374.3665771484375, 'policy_logps/chosen': -278.06268310546875, 'referece_logps/rejected': -273.4296875, 'referece_logps/chosen': -210.2496795654297, 'logits/rejected': -0.16951370239257812, 'logits/chosen': -0.1818736046552658, 'epoch': 7.98}


 89%|████████▊ | 14294/16110 [21:21:45<7:59:29, 15.84s/it]

 89%|████████▊ | 14295/16110 [21:22:01<8:00:20, 15.88s/it]

 89%|████████▊ | 14296/16110 [21:22:15<7:45:05, 15.38s/it]

 89%|████████▊ | 14297/16110 [21:22:33<8:06:53, 16.11s/it]

 89%|████████▉ | 14298/16110 [21:22:52<8:39:43, 17.21s/it]

 89%|████████▉ | 14299/16110 [21:23:05<7:57:02, 15.80s/it]

 89%|████████▉ | 14300/16110 [21:23:20<7:54:54, 15.74s/it]

 89%|████████▉ | 14301/16110 [21:23:33<7:23:40, 14.72s/it]

 89%|████████▉ | 14302/16110 [21:23:47<7:15:47, 14.46s/it]
{'loss': 0.0962, 'learning_rate': 1.7195756297882588e-06, 'rewards/chosen': -4.851738452911377, 'rewards/rejected': -9.974462509155273, 'rewards/accuracies': 1.0, 'rewards/margins': 5.122725009918213, 'policy_logps/rejected': -404.2189636230469, 'policy_logps/chosen': -380.5453186035156, 'referece_logps/rejected': -304.4742736816406, 'referece_logps/chosen': -332.02789306640625, 'logits/rejected': -0.15383151173591614, 'logits/chosen': -0.02023307979106903, 'epoch': 7.99}
[2024-04-06 12:32:01,511] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14303/16110 [21:24:10<8:34:24, 17.08s/it]

 89%|████████▉ | 14304/16110 [21:24:29<8:49:58, 17.61s/it]

 89%|████████▉ | 14305/16110 [21:24:50<9:19:03, 18.58s/it]

 89%|████████▉ | 14306/16110 [21:25:10<9:36:46, 19.18s/it]


 89%|████████▉ | 14308/16110 [21:25:47<9:20:31, 18.66s/it]
{'loss': 0.1399, 'learning_rate': 1.7187374386495104e-06, 'rewards/chosen': -3.755952835083008, 'rewards/rejected': -9.333033561706543, 'rewards/accuracies': 0.875, 'rewards/margins': 5.577081203460693, 'policy_logps/rejected': -355.6483154296875, 'policy_logps/chosen': -346.4842224121094, 'referece_logps/rejected': -262.3179626464844, 'referece_logps/chosen': -308.9246520996094, 'logits/rejected': -0.6911853551864624, 'logits/chosen': -0.8367260694503784, 'epoch': 7.99}

 89%|████████▉ | 14309/16110 [21:26:08<9:38:03, 19.26s/it]

 89%|████████▉ | 14310/16110 [21:26:30<10:01:55, 20.06s/it]

 89%|████████▉ | 14311/16110 [21:26:50<10:03:40, 20.13s/it]

 89%|████████▉ | 14312/16110 [21:27:09<9:57:04, 19.92s/it]


 89%|████████▉ | 14314/16110 [21:27:49<9:59:52, 20.04s/it]
{'loss': 0.0865, 'learning_rate': 1.7178982016428921e-06, 'rewards/chosen': -2.7780933380126953, 'rewards/rejected': -8.163705825805664, 'rewards/accuracies': 1.0, 'rewards/margins': 5.3856120109558105, 'policy_logps/rejected': -541.1774291992188, 'policy_logps/chosen': -580.1728515625, 'referece_logps/rejected': -459.5404052734375, 'referece_logps/chosen': -552.3919067382812, 'logits/rejected': 0.07187449187040329, 'logits/chosen': 0.036864347755908966, 'epoch': 8.0}


 89%|████████▉ | 14316/16110 [21:28:27<9:51:57, 19.80s/it]

 89%|████████▉ | 14317/16110 [21:28:41<8:58:50, 18.03s/it]
{'loss': 0.097, 'learning_rate': 1.7174781913207011e-06, 'rewards/chosen': -5.016456127166748, 'rewards/rejected': -9.982855796813965, 'rewards/accuracies': 1.0, 'rewards/margins': 4.966400146484375, 'policy_logps/rejected': -418.4297180175781, 'policy_logps/chosen': -432.1061096191406, 'referece_logps/rejected': -318.60113525390625, 'referece_logps/chosen': -381.9415588378906, 'logits/rejected': -0.15276648104190826, 'logits/chosen': -0.3543778955936432, 'epoch': 8.0}

 89%|████████▉ | 14318/16110 [21:29:01<9:12:22, 18.49s/it]

 89%|████████▉ | 14319/16110 [21:29:18<9:03:36, 18.21s/it]


 89%|████████▉ | 14321/16110 [21:29:45<7:50:51, 15.79s/it]
{'loss': 0.0917, 'learning_rate': 1.7169177715703672e-06, 'rewards/chosen': -3.0865542888641357, 'rewards/rejected': -6.54679536819458, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4602408409118652, 'policy_logps/rejected': -312.16259765625, 'policy_logps/chosen': -428.0838317871094, 'referece_logps/rejected': -246.69464111328125, 'referece_logps/chosen': -397.2182922363281, 'logits/rejected': 0.4062950909137726, 'logits/chosen': 0.1592758744955063, 'epoch': 8.0}

 89%|████████▉ | 14322/16110 [21:30:06<8:39:26, 17.43s/it]

 89%|████████▉ | 14323/16110 [21:30:22<8:20:44, 16.81s/it]

 89%|████████▉ | 14324/16110 [21:30:42<8:47:34, 17.72s/it]

 89%|████████▉ | 14325/16110 [21:31:03<9:18:21, 18.77s/it]

 89%|████████▉ | 14326/16110 [21:31:21<9:08:23, 18.44s/it]

 89%|████████▉ | 14327/16110 [21:31:38<9:01:42, 18.23s/it]


 89%|████████▉ | 14329/16110 [21:32:10<8:11:24, 16.56s/it]
{'loss': 0.1262, 'learning_rate': 1.7157955414723623e-06, 'rewards/chosen': -6.318394184112549, 'rewards/rejected': -10.079413414001465, 'rewards/accuracies': 0.75, 'rewards/margins': 3.761019706726074, 'policy_logps/rejected': -434.5941162109375, 'policy_logps/chosen': -565.7648315429688, 'referece_logps/rejected': -333.79998779296875, 'referece_logps/chosen': -502.580810546875, 'logits/rejected': 0.3842606246471405, 'logits/chosen': 0.2006838023662567, 'epoch': 8.01}

 89%|████████▉ | 14330/16110 [21:32:25<8:00:03, 16.18s/it]


 89%|████████▉ | 14332/16110 [21:33:01<8:27:20, 17.12s/it]
{'loss': 0.0678, 'learning_rate': 1.7153742276355408e-06, 'rewards/chosen': -5.1160383224487305, 'rewards/rejected': -9.447542190551758, 'rewards/accuracies': 0.875, 'rewards/margins': 4.3315043449401855, 'policy_logps/rejected': -362.02996826171875, 'policy_logps/chosen': -320.9761962890625, 'referece_logps/rejected': -267.5545654296875, 'referece_logps/chosen': -269.8158264160156, 'logits/rejected': 0.2269725203514099, 'logits/chosen': 0.22804884612560272, 'epoch': 8.01}

 89%|████████▉ | 14333/16110 [21:33:21<8:48:46, 17.85s/it]

 89%|████████▉ | 14334/16110 [21:33:32<7:45:08, 15.71s/it]

 89%|████████▉ | 14335/16110 [21:33:52<8:23:08, 17.01s/it]

 89%|████████▉ | 14336/16110 [21:34:06<7:55:54, 16.10s/it]

 89%|████████▉ | 14337/16110 [21:34:22<7:57:40, 16.17s/it]

 89%|████████▉ | 14338/16110 [21:34:33<7:10:27, 14.58s/it]

 89%|████████▉ | 14339/16110 [21:34:47<7:02:56, 14.33s/it]

 89%|████████▉ | 14340/16110 [21:35:00<6:55:10, 14.07s/it]

 89%|████████▉ | 14341/16110 [21:35:12<6:38:10, 13.51s/it]

 89%|████████▉ | 14342/16110 [21:35:32<7:32:05, 15.34s/it]

 89%|████████▉ | 14343/16110 [21:35:49<7:49:12, 15.93s/it]


 89%|████████▉ | 14345/16110 [21:36:28<8:36:33, 17.56s/it]
{'loss': 0.0624, 'learning_rate': 1.7135455290542335e-06, 'rewards/chosen': -4.6628923416137695, 'rewards/rejected': -9.784736633300781, 'rewards/accuracies': 1.0, 'rewards/margins': 5.12184476852417, 'policy_logps/rejected': -359.6810302734375, 'policy_logps/chosen': -530.084716796875, 'referece_logps/rejected': -261.8336486816406, 'referece_logps/chosen': -483.4557800292969, 'logits/rejected': 1.2503186464309692, 'logits/chosen': 0.8846743106842041, 'epoch': 8.01}

 89%|████████▉ | 14346/16110 [21:36:41<7:54:40, 16.15s/it]

 89%|████████▉ | 14347/16110 [21:36:56<7:48:54, 15.96s/it]

 89%|████████▉ | 14348/16110 [21:37:10<7:26:30, 15.20s/it]
[2024-04-06 12:45:22,713] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14349/16110 [21:37:31<8:22:12, 17.11s/it]
[2024-04-06 12:45:40,934] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14350/16110 [21:37:49<8:31:41, 17.44s/it]

 89%|████████▉ | 14351/16110 [21:38:00<7:32:33, 15.44s/it]

 89%|████████▉ | 14352/16110 [21:38:13<7:11:33, 14.73s/it]

 89%|████████▉ | 14353/16110 [21:38:27<7:06:08, 14.55s/it]

 89%|████████▉ | 14354/16110 [21:38:50<8:13:50, 16.87s/it]


 89%|████████▉ | 14356/16110 [21:39:18<7:27:33, 15.31s/it]

 89%|████████▉ | 14357/16110 [21:39:38<8:08:00, 16.70s/it]
{'loss': 0.1004, 'learning_rate': 1.7118531730079705e-06, 'rewards/chosen': -4.7034010887146, 'rewards/rejected': -9.47785472869873, 'rewards/accuracies': 1.0, 'rewards/margins': 4.774454116821289, 'policy_logps/rejected': -253.4497833251953, 'policy_logps/chosen': -302.0755920410156, 'referece_logps/rejected': -158.67123413085938, 'referece_logps/chosen': -255.0415802001953, 'logits/rejected': -0.08058798313140869, 'logits/chosen': -0.2388332188129425, 'epoch': 8.02}

 89%|████████▉ | 14358/16110 [21:39:50<7:32:13, 15.49s/it]

 89%|████████▉ | 14359/16110 [21:40:08<7:52:12, 16.18s/it]

 89%|████████▉ | 14360/16110 [21:40:28<8:20:56, 17.18s/it]

 89%|████████▉ | 14361/16110 [21:40:45<8:18:57, 17.12s/it]

 89%|████████▉ | 14362/16110 [21:41:06<8:52:15, 18.27s/it]

 89%|████████▉ | 14363/16110 [21:41:19<8:07:12, 16.73s/it]

 89%|████████▉ | 14364/16110 [21:41:37<8:17:48, 17.11s/it]

 89%|████████▉ | 14365/16110 [21:41:55<8:31:28, 17.59s/it]

 89%|████████▉ | 14366/16110 [21:42:12<8:25:07, 17.38s/it]

 89%|████████▉ | 14367/16110 [21:42:25<7:46:48, 16.07s/it]


 89%|████████▉ | 14369/16110 [21:42:58<7:41:18, 15.90s/it]
{'loss': 0.0772, 'learning_rate': 1.7101566735621815e-06, 'rewards/chosen': -4.789768695831299, 'rewards/rejected': -11.88026237487793, 'rewards/accuracies': 1.0, 'rewards/margins': 7.090494155883789, 'policy_logps/rejected': -524.7902221679688, 'policy_logps/chosen': -527.9765014648438, 'referece_logps/rejected': -405.9875793457031, 'referece_logps/chosen': -480.078857421875, 'logits/rejected': 0.0077926358208060265, 'logits/chosen': -0.04680704325437546, 'epoch': 8.03}

 89%|████████▉ | 14370/16110 [21:43:11<7:16:43, 15.06s/it]

 89%|████████▉ | 14371/16110 [21:43:31<7:58:05, 16.50s/it]

 89%|████████▉ | 14372/16110 [21:43:42<7:07:27, 14.76s/it]

 89%|████████▉ | 14373/16110 [21:44:02<7:51:05, 16.27s/it]


 89%|████████▉ | 14375/16110 [21:44:42<8:51:28, 18.38s/it]
[2024-04-06 12:52:33,791] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.0817, 'learning_rate': 1.7093068731493407e-06, 'rewards/chosen': -4.611833572387695, 'rewards/rejected': -9.575777053833008, 'rewards/accuracies': 1.0, 'rewards/margins': 4.9639434814453125, 'policy_logps/rejected': -466.79815673828125, 'policy_logps/chosen': -677.677734375, 'referece_logps/rejected': -371.0403747558594, 'referece_logps/chosen': -631.5594482421875, 'logits/rejected': 0.9358482956886292, 'logits/chosen': 0.8481640219688416, 'epoch': 8.03}

 89%|████████▉ | 14376/16110 [21:44:55<8:05:02, 16.78s/it]

 89%|████████▉ | 14377/16110 [21:45:15<8:31:58, 17.73s/it]


 89%|████████▉ | 14379/16110 [21:45:50<8:23:26, 17.45s/it]
{'loss': 0.1309, 'learning_rate': 1.7087397660504856e-06, 'rewards/chosen': -4.110275745391846, 'rewards/rejected': -8.022281646728516, 'rewards/accuracies': 0.875, 'rewards/margins': 3.912006139755249, 'policy_logps/rejected': -343.15716552734375, 'policy_logps/chosen': -324.0064392089844, 'referece_logps/rejected': -262.9343566894531, 'referece_logps/chosen': -282.9036865234375, 'logits/rejected': 0.1597132533788681, 'logits/chosen': 0.20030736923217773, 'epoch': 8.03}

 89%|████████▉ | 14380/16110 [21:46:08<8:25:58, 17.55s/it]

 89%|████████▉ | 14381/16110 [21:46:28<8:43:29, 18.17s/it]

 89%|████████▉ | 14382/16110 [21:46:44<8:26:59, 17.60s/it]

 89%|████████▉ | 14383/16110 [21:46:58<7:56:54, 16.57s/it]
[2024-04-06 12:55:11,612] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 89%|████████▉ | 14385/16110 [21:47:40<9:01:37, 18.84s/it]
{'loss': 0.0746, 'learning_rate': 1.7078882460836766e-06, 'rewards/chosen': -4.981767654418945, 'rewards/rejected': -13.072001457214355, 'rewards/accuracies': 1.0, 'rewards/margins': 8.090232849121094, 'policy_logps/rejected': -557.967041015625, 'policy_logps/chosen': -459.6382751464844, 'referece_logps/rejected': -427.2469482421875, 'referece_logps/chosen': -409.82061767578125, 'logits/rejected': 0.6148151755332947, 'logits/chosen': 0.49558746814727783, 'epoch': 8.04}

 89%|████████▉ | 14386/16110 [21:48:01<9:15:28, 19.33s/it]

 89%|████████▉ | 14387/16110 [21:48:17<8:46:59, 18.35s/it]

 89%|████████▉ | 14388/16110 [21:48:37<9:03:18, 18.93s/it]


 89%|████████▉ | 14390/16110 [21:49:15<9:03:07, 18.95s/it]

 89%|████████▉ | 14391/16110 [21:49:35<9:12:38, 19.29s/it]
{'loss': 0.0791, 'learning_rate': 1.7070356960361565e-06, 'rewards/chosen': -4.579965591430664, 'rewards/rejected': -8.823861122131348, 'rewards/accuracies': 1.0, 'rewards/margins': 4.243895530700684, 'policy_logps/rejected': -425.693359375, 'policy_logps/chosen': -376.928955078125, 'referece_logps/rejected': -337.4547119140625, 'referece_logps/chosen': -331.12933349609375, 'logits/rejected': -0.4067574441432953, 'logits/chosen': -0.5620144605636597, 'epoch': 8.04}

 89%|████████▉ | 14392/16110 [21:49:53<9:05:00, 19.03s/it]
[2024-04-06 12:58:03,336] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14393/16110 [21:50:12<9:00:35, 18.89s/it]
[2024-04-06 12:58:23,922] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14394/16110 [21:50:32<9:14:49, 19.40s/it]

 89%|████████▉ | 14395/16110 [21:50:52<9:16:41, 19.48s/it]

 89%|████████▉ | 14396/16110 [21:51:10<9:07:57, 19.18s/it]

 89%|████████▉ | 14397/16110 [21:51:25<8:32:12, 17.94s/it]


 89%|████████▉ | 14399/16110 [21:52:03<8:37:23, 18.14s/it]
{'loss': 0.1007, 'learning_rate': 1.7058973624361263e-06, 'rewards/chosen': -4.979392051696777, 'rewards/rejected': -9.239188194274902, 'rewards/accuracies': 1.0, 'rewards/margins': 4.259797096252441, 'policy_logps/rejected': -399.8822021484375, 'policy_logps/chosen': -372.3597717285156, 'referece_logps/rejected': -307.4903259277344, 'referece_logps/chosen': -322.56585693359375, 'logits/rejected': -0.2522561550140381, 'logits/chosen': -0.27131015062332153, 'epoch': 8.04}
[2024-04-06 13:00:16,172] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14400/16110 [21:52:25<9:07:21, 19.21s/it]
[2024-04-06 13:00:32,657] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14401/16110 [21:52:41<8:43:47, 18.39s/it]


 89%|████████▉ | 14403/16110 [21:53:19<8:52:46, 18.73s/it]
{'loss': 0.1029, 'learning_rate': 1.7053275106628188e-06, 'rewards/chosen': -4.038141250610352, 'rewards/rejected': -9.271544456481934, 'rewards/accuracies': 0.875, 'rewards/margins': 5.233404159545898, 'policy_logps/rejected': -486.0021667480469, 'policy_logps/chosen': -306.42218017578125, 'referece_logps/rejected': -393.2867126464844, 'referece_logps/chosen': -266.040771484375, 'logits/rejected': -0.4531291127204895, 'logits/chosen': -0.31158310174942017, 'epoch': 8.05}

 89%|████████▉ | 14404/16110 [21:53:36<8:35:36, 18.13s/it]

 89%|████████▉ | 14405/16110 [21:53:49<7:51:34, 16.59s/it]


 89%|████████▉ | 14407/16110 [21:54:27<8:25:30, 17.81s/it]
{'loss': 0.1607, 'learning_rate': 1.7047572027319397e-06, 'rewards/chosen': -5.049519062042236, 'rewards/rejected': -9.405555725097656, 'rewards/accuracies': 0.625, 'rewards/margins': 4.356037139892578, 'policy_logps/rejected': -382.8331604003906, 'policy_logps/chosen': -391.7633056640625, 'referece_logps/rejected': -288.777587890625, 'referece_logps/chosen': -341.2680969238281, 'logits/rejected': 0.2245466411113739, 'logits/chosen': 0.001740679144859314, 'epoch': 8.05}

 89%|████████▉ | 14408/16110 [21:54:43<8:15:27, 17.47s/it]

 89%|████████▉ | 14409/16110 [21:54:56<7:29:29, 15.86s/it]

 89%|████████▉ | 14410/16110 [21:55:15<7:57:20, 16.85s/it]

 89%|████████▉ | 14411/16110 [21:55:36<8:35:06, 18.19s/it]
[2024-04-06 13:03:48,394] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 89%|████████▉ | 14412/16110 [21:55:57<8:55:47, 18.93s/it]

 89%|████████▉ | 14413/16110 [21:56:16<9:01:50, 19.16s/it]

 89%|████████▉ | 14414/16110 [21:56:39<9:32:11, 20.24s/it]

 89%|████████▉ | 14415/16110 [21:56:59<9:25:33, 20.02s/it]

 89%|████████▉ | 14416/16110 [21:57:19<9:24:44, 20.00s/it]


 89%|████████▉ | 14418/16110 [21:57:46<7:52:54, 16.77s/it]

 90%|████████▉ | 14419/16110 [21:58:02<7:47:43, 16.60s/it]

 90%|████████▉ | 14420/16110 [21:58:19<7:43:52, 16.47s/it]
{'loss': 0.1141, 'learning_rate': 1.7029005560780864e-06, 'rewards/chosen': -5.030816555023193, 'rewards/rejected': -9.002561569213867, 'rewards/accuracies': 0.875, 'rewards/margins': 3.971745491027832, 'policy_logps/rejected': -399.7786560058594, 'policy_logps/chosen': -414.9043884277344, 'referece_logps/rejected': -309.7530212402344, 'referece_logps/chosen': -364.59625244140625, 'logits/rejected': 0.3186637759208679, 'logits/chosen': 0.13372662663459778, 'epoch': 8.06}
[2024-04-06 13:06:33,122] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 90%|████████▉ | 14422/16110 [21:58:58<8:20:30, 17.79s/it]
[2024-04-06 13:06:49,536] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14423/16110 [21:59:16<8:21:04, 17.82s/it]

 90%|████████▉ | 14424/16110 [21:59:28<7:36:36, 16.25s/it]

 90%|████████▉ | 14425/16110 [21:59:43<7:21:22, 15.72s/it]

 90%|████████▉ | 14426/16110 [21:59:59<7:27:37, 15.95s/it]

 90%|████████▉ | 14427/16110 [22:00:14<7:17:55, 15.61s/it]

 90%|████████▉ | 14428/16110 [22:00:30<7:19:50, 15.69s/it]

 90%|████████▉ | 14429/16110 [22:00:50<7:52:54, 16.88s/it]

 90%|████████▉ | 14430/16110 [22:01:10<8:23:31, 17.98s/it]

 90%|████████▉ | 14431/16110 [22:01:25<7:57:36, 17.07s/it]

 90%|████████▉ | 14432/16110 [22:01:45<8:18:55, 17.84s/it]

 90%|████████▉ | 14433/16110 [22:01:56<7:21:38, 15.80s/it]

 90%|████████▉ | 14434/16110 [22:02:10<7:08:57, 15.36s/it]

 90%|████████▉ | 14435/16110 [22:02:26<7:14:19, 15.56s/it]

 90%|████████▉ | 14436/16110 [22:02:37<6:33:36, 14.11s/it]

 90%|████████▉ | 14437/16110 [22:02:58<7:31:28, 16.19s/it]
[2024-04-06 13:10:49,628] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14438/16110 [22:03:18<8:02:21, 17.31s/it]
[2024-04-06 13:11:09,546] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14439/16110 [22:03:38<8:28:07, 18.25s/it]
[2024-04-06 13:11:29,974] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14440/16110 [22:04:01<9:01:43, 19.46s/it]

 90%|████████▉ | 14441/16110 [22:04:24<9:30:33, 20.51s/it]
{'loss': 0.1132, 'learning_rate': 1.6998912201566851e-06, 'rewards/chosen': -4.422272205352783, 'rewards/rejected': -9.87711238861084, 'rewards/accuracies': 1.0, 'rewards/margins': 5.454840660095215, 'policy_logps/rejected': -407.00921630859375, 'policy_logps/chosen': -340.2239074707031, 'referece_logps/rejected': -308.23809814453125, 'referece_logps/chosen': -296.00115966796875, 'logits/rejected': -0.43177494406700134, 'logits/chosen': -0.4103601574897766, 'epoch': 8.07}


 90%|████████▉ | 14443/16110 [22:04:56<8:32:54, 18.46s/it]
{'loss': 0.138, 'learning_rate': 1.699603965218426e-06, 'rewards/chosen': -4.565242767333984, 'rewards/rejected': -10.92088794708252, 'rewards/accuracies': 1.0, 'rewards/margins': 6.355644702911377, 'policy_logps/rejected': -378.281494140625, 'policy_logps/chosen': -390.14019775390625, 'referece_logps/rejected': -269.0726013183594, 'referece_logps/chosen': -344.4877624511719, 'logits/rejected': 0.02363867312669754, 'logits/chosen': -0.0046462491154670715, 'epoch': 8.07}


 90%|████████▉ | 14445/16110 [22:05:35<8:46:36, 18.98s/it]

 90%|████████▉ | 14446/16110 [22:05:54<8:45:05, 18.93s/it]

 90%|████████▉ | 14447/16110 [22:06:15<8:57:36, 19.40s/it]

 90%|████████▉ | 14448/16110 [22:06:33<8:50:02, 19.14s/it]

 90%|████████▉ | 14449/16110 [22:06:47<8:08:29, 17.65s/it]

 90%|████████▉ | 14450/16110 [22:06:58<7:11:08, 15.58s/it]
{'loss': 0.0996, 'learning_rate': 1.6985976824667384e-06, 'rewards/chosen': -3.6644833087921143, 'rewards/rejected': -9.480338096618652, 'rewards/accuracies': 1.0, 'rewards/margins': 5.815855026245117, 'policy_logps/rejected': -388.22344970703125, 'policy_logps/chosen': -297.93719482421875, 'referece_logps/rejected': -293.4200439453125, 'referece_logps/chosen': -261.2923889160156, 'logits/rejected': -0.19436705112457275, 'logits/chosen': -0.20820018649101257, 'epoch': 8.07}


 90%|████████▉ | 14452/16110 [22:07:35<7:47:24, 16.91s/it]

 90%|████████▉ | 14453/16110 [22:07:56<8:17:05, 18.00s/it]

 90%|████████▉ | 14454/16110 [22:08:10<7:48:47, 16.99s/it]

 90%|████████▉ | 14455/16110 [22:08:32<8:30:33, 18.51s/it]

 90%|████████▉ | 14456/16110 [22:08:53<8:51:28, 19.28s/it]

 90%|████████▉ | 14457/16110 [22:09:13<8:53:58, 19.38s/it]

 90%|████████▉ | 14458/16110 [22:09:29<8:22:15, 18.24s/it]

 90%|████████▉ | 14459/16110 [22:09:49<8:38:46, 18.85s/it]
[2024-04-06 13:17:40,446] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14460/16110 [22:10:07<8:29:18, 18.52s/it]
[2024-04-06 13:17:58,190] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14461/16110 [22:10:24<8:23:40, 18.33s/it]

 90%|████████▉ | 14462/16110 [22:10:35<7:19:04, 15.99s/it]

 90%|████████▉ | 14463/16110 [22:10:48<6:54:47, 15.11s/it]

 90%|████████▉ | 14464/16110 [22:11:08<7:34:17, 16.56s/it]
[2024-04-06 13:18:59,598] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.1662, 'learning_rate': 1.6965809679995396e-06, 'rewards/chosen': -5.2623114585876465, 'rewards/rejected': -8.364858627319336, 'rewards/accuracies': 0.875, 'rewards/margins': 3.102546215057373, 'policy_logps/rejected': -608.2803955078125, 'policy_logps/chosen': -559.972412109375, 'referece_logps/rejected': -524.6317749023438, 'referece_logps/chosen': -507.34930419921875, 'logits/rejected': 0.7071883678436279, 'logits/chosen': 0.7509886026382446, 'epoch': 8.08}

 90%|████████▉ | 14465/16110 [22:11:22<7:15:43, 15.89s/it]


 90%|████████▉ | 14467/16110 [22:12:02<8:10:42, 17.92s/it]

 90%|████████▉ | 14468/16110 [22:12:20<8:09:13, 17.88s/it]

 90%|████████▉ | 14469/16110 [22:12:40<8:25:06, 18.47s/it]
[2024-04-06 13:20:31,292] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14470/16110 [22:12:55<7:56:07, 17.42s/it]

 90%|████████▉ | 14471/16110 [22:13:14<8:08:41, 17.89s/it]

 90%|████████▉ | 14472/16110 [22:13:33<8:24:09, 18.47s/it]

 90%|████████▉ | 14473/16110 [22:13:55<8:45:39, 19.27s/it]

 90%|████████▉ | 14474/16110 [22:14:09<8:08:14, 17.91s/it]

 90%|████████▉ | 14475/16110 [22:14:26<7:58:35, 17.56s/it]
[2024-04-06 13:22:17,691] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14476/16110 [22:14:45<8:09:22, 17.97s/it]

 90%|████████▉ | 14477/16110 [22:14:58<7:29:29, 16.52s/it]

 90%|████████▉ | 14478/16110 [22:15:14<7:26:53, 16.43s/it]

 90%|████████▉ | 14479/16110 [22:15:25<6:43:49, 14.86s/it]

 90%|████████▉ | 14480/16110 [22:15:46<7:30:43, 16.59s/it]
[2024-04-06 13:23:37,784] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14481/16110 [22:16:06<7:55:39, 17.52s/it]

 90%|████████▉ | 14482/16110 [22:16:27<8:26:38, 18.67s/it]
[2024-04-06 13:24:18,831] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14483/16110 [22:16:45<8:18:02, 18.37s/it]

 90%|████████▉ | 14484/16110 [22:16:57<7:25:21, 16.43s/it]

 90%|████████▉ | 14485/16110 [22:17:14<7:35:12, 16.81s/it]

 90%|████████▉ | 14486/16110 [22:17:31<7:33:27, 16.75s/it]

 90%|████████▉ | 14487/16110 [22:17:51<8:01:43, 17.81s/it]
[2024-04-06 13:25:42,987] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|████████▉ | 14488/16110 [22:18:11<8:17:27, 18.40s/it]

 90%|████████▉ | 14489/16110 [22:18:31<8:33:04, 18.99s/it]

 90%|████████▉ | 14490/16110 [22:18:48<8:15:05, 18.34s/it]
{'loss': 0.0703, 'learning_rate': 1.6928210120043596e-06, 'rewards/chosen': -3.5672085285186768, 'rewards/rejected': -8.212737083435059, 'rewards/accuracies': 1.0, 'rewards/margins': 4.645528793334961, 'policy_logps/rejected': -497.48150634765625, 'policy_logps/chosen': -436.4183349609375, 'referece_logps/rejected': -415.3541259765625, 'referece_logps/chosen': -400.74627685546875, 'logits/rejected': 0.05975864827632904, 'logits/chosen': -0.10841318964958191, 'epoch': 8.09}


 90%|████████▉ | 14492/16110 [22:19:22<7:44:36, 17.23s/it]

 90%|████████▉ | 14493/16110 [22:19:42<8:10:54, 18.22s/it]
{'loss': 0.0995, 'learning_rate': 1.6923859507878628e-06, 'rewards/chosen': -3.696906566619873, 'rewards/rejected': -7.838596820831299, 'rewards/accuracies': 1.0, 'rewards/margins': 4.141690254211426, 'policy_logps/rejected': -365.1975402832031, 'policy_logps/chosen': -287.0234680175781, 'referece_logps/rejected': -286.81158447265625, 'referece_logps/chosen': -250.05438232421875, 'logits/rejected': 0.33851489424705505, 'logits/chosen': 0.3627483546733856, 'epoch': 8.1}


 90%|████████▉ | 14495/16110 [22:20:17<7:52:02, 17.54s/it]

 90%|████████▉ | 14496/16110 [22:20:32<7:31:33, 16.79s/it]
{'loss': 0.1424, 'learning_rate': 1.691950637690691e-06, 'rewards/chosen': -4.119933128356934, 'rewards/rejected': -10.181050300598145, 'rewards/accuracies': 1.0, 'rewards/margins': 6.061116695404053, 'policy_logps/rejected': -484.00262451171875, 'policy_logps/chosen': -454.42901611328125, 'referece_logps/rejected': -382.1921691894531, 'referece_logps/chosen': -413.22967529296875, 'logits/rejected': 0.3803006112575531, 'logits/chosen': 0.37652429938316345, 'epoch': 8.1}


 90%|████████▉ | 14498/16110 [22:21:08<7:56:05, 17.72s/it]
{'loss': 0.1219, 'learning_rate': 1.691660289103734e-06, 'rewards/chosen': -4.7815141677856445, 'rewards/rejected': -8.948019027709961, 'rewards/accuracies': 1.0, 'rewards/margins': 4.166504383087158, 'policy_logps/rejected': -360.22174072265625, 'policy_logps/chosen': -321.480712890625, 'referece_logps/rejected': -270.7415466308594, 'referece_logps/chosen': -273.66558837890625, 'logits/rejected': 0.4482969641685486, 'logits/chosen': 0.5170294642448425, 'epoch': 8.1}


 90%|█████████ | 14500/16110 [22:21:37<7:16:43, 16.28s/it]

 90%|█████████ | 14501/16110 [22:22:08<9:07:33, 20.42s/it]

 90%|█████████ | 14502/16110 [22:22:31<9:28:33, 21.21s/it]
[2024-04-06 13:30:22,273] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|█████████ | 14503/16110 [22:22:50<9:15:36, 20.74s/it]
{'loss': 0.0994, 'learning_rate': 1.690933928484395e-06, 'rewards/chosen': -3.5799336433410645, 'rewards/rejected': -8.175621032714844, 'rewards/accuracies': 1.0, 'rewards/margins': 4.595687389373779, 'policy_logps/rejected': -482.9472351074219, 'policy_logps/chosen': -478.79840087890625, 'referece_logps/rejected': -401.1910400390625, 'referece_logps/chosen': -442.99908447265625, 'logits/rejected': 0.03919956088066101, 'logits/chosen': 0.030871644616127014, 'epoch': 8.1}


 90%|█████████ | 14505/16110 [22:23:21<8:08:30, 18.26s/it]
{'loss': 0.14, 'learning_rate': 1.690643188699193e-06, 'rewards/chosen': -4.5914835929870605, 'rewards/rejected': -11.701515197753906, 'rewards/accuracies': 1.0, 'rewards/margins': 7.110030651092529, 'policy_logps/rejected': -493.236328125, 'policy_logps/chosen': -415.24163818359375, 'referece_logps/rejected': -376.22119140625, 'referece_logps/chosen': -369.3268737792969, 'logits/rejected': 0.42873987555503845, 'logits/chosen': 0.39146554470062256, 'epoch': 8.1}

 90%|█████████ | 14506/16110 [22:23:37<7:49:27, 17.56s/it]
[2024-04-06 13:31:44,749] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 90%|█████████ | 14508/16110 [22:24:14<8:07:16, 18.25s/it]
[2024-04-06 13:32:05,643] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 90%|█████████ | 14509/16110 [22:24:25<7:07:32, 16.02s/it]

 90%|█████████ | 14510/16110 [22:24:42<7:13:25, 16.25s/it]

 90%|█████████ | 14511/16110 [22:24:52<6:29:09, 14.60s/it]
{'loss': 0.0894, 'learning_rate': 1.6897702995405451e-06, 'rewards/chosen': -4.313540935516357, 'rewards/rejected': -8.680706024169922, 'rewards/accuracies': 0.875, 'rewards/margins': 4.367165565490723, 'policy_logps/rejected': -378.359130859375, 'policy_logps/chosen': -464.7247009277344, 'referece_logps/rejected': -291.5520935058594, 'referece_logps/chosen': -421.58929443359375, 'logits/rejected': 0.2893531918525696, 'logits/chosen': 0.22822973132133484, 'epoch': 8.11}

 90%|█████████ | 14512/16110 [22:25:03<5:58:00, 13.44s/it]


 90%|█████████ | 14514/16110 [22:25:47<7:55:45, 17.89s/it]

 90%|█████████ | 14515/16110 [22:25:59<7:05:18, 16.00s/it]
{'loss': 0.0597, 'learning_rate': 1.6891878157360608e-06, 'rewards/chosen': -4.760480880737305, 'rewards/rejected': -9.778873443603516, 'rewards/accuracies': 1.0, 'rewards/margins': 5.018392562866211, 'policy_logps/rejected': -469.1031494140625, 'policy_logps/chosen': -337.83721923828125, 'referece_logps/rejected': -371.31439208984375, 'referece_logps/chosen': -290.23236083984375, 'logits/rejected': 0.28507500886917114, 'logits/chosen': 0.18605613708496094, 'epoch': 8.11}


 90%|█████████ | 14517/16110 [22:26:30<6:48:32, 15.39s/it]

 90%|█████████ | 14518/16110 [22:26:48<7:11:00, 16.24s/it]

 90%|█████████ | 14519/16110 [22:27:03<6:59:59, 15.84s/it]

 90%|█████████ | 14520/16110 [22:27:17<6:44:32, 15.27s/it]

 90%|█████████ | 14521/16110 [22:27:29<6:17:24, 14.25s/it]

 90%|█████████ | 14522/16110 [22:27:40<5:54:44, 13.40s/it]

 90%|█████████ | 14523/16110 [22:28:00<6:48:41, 15.45s/it]

 90%|█████████ | 14524/16110 [22:28:22<7:42:49, 17.51s/it]

 90%|█████████ | 14525/16110 [22:28:42<7:54:35, 17.97s/it]

 90%|█████████ | 14526/16110 [22:28:56<7:29:48, 17.04s/it]

 90%|█████████ | 14527/16110 [22:29:19<8:14:58, 18.76s/it]

 90%|█████████ | 14528/16110 [22:29:39<8:20:23, 18.98s/it]

 90%|█████████ | 14529/16110 [22:29:58<8:26:44, 19.23s/it]
{'loss': 0.2415, 'learning_rate': 1.6871456148538988e-06, 'rewards/chosen': -5.746879577636719, 'rewards/rejected': -9.5656156539917, 'rewards/accuracies': 0.875, 'rewards/margins': 3.8187355995178223, 'policy_logps/rejected': -417.5144958496094, 'policy_logps/chosen': -297.1394958496094, 'referece_logps/rejected': -321.85833740234375, 'referece_logps/chosen': -239.67071533203125, 'logits/rejected': 0.5661321878433228, 'logits/chosen': 0.4488368630409241, 'epoch': 8.12}


 90%|█████████ | 14531/16110 [22:30:33<7:56:50, 18.12s/it]

 90%|█████████ | 14532/16110 [22:30:45<7:10:22, 16.36s/it]

 90%|█████████ | 14533/16110 [22:31:06<7:48:30, 17.83s/it]

 90%|█████████ | 14534/16110 [22:31:28<8:21:37, 19.10s/it]

 90%|█████████ | 14535/16110 [22:31:41<7:28:26, 17.08s/it]
{'loss': 0.0886, 'learning_rate': 1.6862687184651152e-06, 'rewards/chosen': -3.909782886505127, 'rewards/rejected': -8.646903038024902, 'rewards/accuracies': 1.0, 'rewards/margins': 4.737120151519775, 'policy_logps/rejected': -360.6802673339844, 'policy_logps/chosen': -361.5971984863281, 'referece_logps/rejected': -274.2112731933594, 'referece_logps/chosen': -322.4993896484375, 'logits/rejected': 0.2261292040348053, 'logits/chosen': -0.10690145194530487, 'epoch': 8.12}


 90%|█████████ | 14537/16110 [22:32:23<8:22:14, 19.16s/it]

 90%|█████████ | 14538/16110 [22:32:38<7:52:31, 18.04s/it]
{'loss': 0.1794, 'learning_rate': 1.6858298957079871e-06, 'rewards/chosen': -5.090681552886963, 'rewards/rejected': -10.250267028808594, 'rewards/accuracies': 1.0, 'rewards/margins': 5.159584999084473, 'policy_logps/rejected': -374.79205322265625, 'policy_logps/chosen': -478.742919921875, 'referece_logps/rejected': -272.28936767578125, 'referece_logps/chosen': -427.8360595703125, 'logits/rejected': 0.46248772740364075, 'logits/chosen': 0.21599304676055908, 'epoch': 8.12}

 90%|█████████ | 14539/16110 [22:32:54<7:32:40, 17.29s/it]

 90%|█████████ | 14540/16110 [22:33:10<7:24:55, 17.00s/it]

 90%|█████████ | 14541/16110 [22:33:30<7:48:29, 17.92s/it]

 90%|█████████ | 14542/16110 [22:33:48<7:45:53, 17.83s/it]


 90%|█████████ | 14544/16110 [22:34:19<7:21:07, 16.90s/it]

 90%|█████████ | 14545/16110 [22:34:35<7:07:09, 16.38s/it]

 90%|█████████ | 14546/16110 [22:34:51<7:08:48, 16.45s/it]
{'loss': 0.1068, 'learning_rate': 1.6846584823662385e-06, 'rewards/chosen': -4.699737548828125, 'rewards/rejected': -8.545110702514648, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8453731536865234, 'policy_logps/rejected': -480.3475646972656, 'policy_logps/chosen': -458.1015625, 'referece_logps/rejected': -394.8964538574219, 'referece_logps/chosen': -411.10418701171875, 'logits/rejected': -0.21963128447532654, 'logits/chosen': -0.2775819003582001, 'epoch': 8.13}


 90%|█████████ | 14548/16110 [22:35:29<7:42:45, 17.78s/it]

 90%|█████████ | 14549/16110 [22:35:48<7:57:38, 18.36s/it]

 90%|█████████ | 14550/16110 [22:36:09<8:12:48, 18.95s/it]
{'loss': 0.0826, 'learning_rate': 1.6840721113205072e-06, 'rewards/chosen': -4.656829357147217, 'rewards/rejected': -9.54426097869873, 'rewards/accuracies': 1.0, 'rewards/margins': 4.887431621551514, 'policy_logps/rejected': -363.3934326171875, 'policy_logps/chosen': -340.1485595703125, 'referece_logps/rejected': -267.9508056640625, 'referece_logps/chosen': -293.5802917480469, 'logits/rejected': -0.014948494732379913, 'logits/chosen': -0.11269006133079529, 'epoch': 8.13}


 90%|█████████ | 14552/16110 [22:36:45<7:59:34, 18.47s/it]

 90%|█████████ | 14553/16110 [22:37:05<8:09:42, 18.87s/it]

 90%|█████████ | 14554/16110 [22:37:20<7:36:16, 17.59s/it]
{'loss': 0.1268, 'learning_rate': 1.6834852978637421e-06, 'rewards/chosen': -4.630190372467041, 'rewards/rejected': -9.776357650756836, 'rewards/accuracies': 1.0, 'rewards/margins': 5.146166801452637, 'policy_logps/rejected': -423.2679748535156, 'policy_logps/chosen': -331.1702880859375, 'referece_logps/rejected': -325.50439453125, 'referece_logps/chosen': -284.8683776855469, 'logits/rejected': -0.36189770698547363, 'logits/chosen': -0.357649564743042, 'epoch': 8.13}


 90%|█████████ | 14556/16110 [22:37:54<7:30:10, 17.38s/it]

 90%|█████████ | 14557/16110 [22:38:13<7:48:02, 18.08s/it]

 90%|█████████ | 14558/16110 [22:38:27<7:13:07, 16.74s/it]

 90%|█████████ | 14559/16110 [22:38:47<7:36:54, 17.68s/it]
{'loss': 0.088, 'learning_rate': 1.6827511594804574e-06, 'rewards/chosen': -4.105528354644775, 'rewards/rejected': -8.449836730957031, 'rewards/accuracies': 1.0, 'rewards/margins': 4.344307899475098, 'policy_logps/rejected': -306.70751953125, 'policy_logps/chosen': -261.4881591796875, 'referece_logps/rejected': -222.20916748046875, 'referece_logps/chosen': -220.43289184570312, 'logits/rejected': -0.32910338044166565, 'logits/chosen': -0.19422294199466705, 'epoch': 8.13}


 90%|█████████ | 14561/16110 [22:39:20<7:20:00, 17.04s/it]

 90%|█████████ | 14562/16110 [22:39:37<7:26:56, 17.32s/it]

 90%|█████████ | 14563/16110 [22:39:53<7:17:39, 16.97s/it]
{'loss': 0.0514, 'learning_rate': 1.6821633519868877e-06, 'rewards/chosen': -5.565242767333984, 'rewards/rejected': -10.38357925415039, 'rewards/accuracies': 1.0, 'rewards/margins': 4.818336486816406, 'policy_logps/rejected': -461.3138427734375, 'policy_logps/chosen': -347.14080810546875, 'referece_logps/rejected': -357.4780578613281, 'referece_logps/chosen': -291.4883728027344, 'logits/rejected': 0.43405693769454956, 'logits/chosen': 0.4025663733482361, 'epoch': 8.14}

 90%|█████████ | 14564/16110 [22:40:08<7:01:41, 16.37s/it]


 90%|█████████ | 14566/16110 [22:40:42<7:17:02, 16.98s/it]

 90%|█████████ | 14567/16110 [22:40:57<7:00:41, 16.36s/it]

 90%|█████████ | 14568/16110 [22:41:17<7:33:03, 17.63s/it]

 90%|█████████ | 14569/16110 [22:41:38<7:52:04, 18.38s/it]

 90%|█████████ | 14570/16110 [22:41:55<7:48:19, 18.25s/it]

 90%|█████████ | 14571/16110 [22:42:11<7:30:52, 17.58s/it]

 90%|█████████ | 14572/16110 [22:42:32<7:53:10, 18.46s/it]

 90%|█████████ | 14573/16110 [22:42:52<8:04:28, 18.91s/it]

 90%|█████████ | 14574/16110 [22:43:10<7:55:08, 18.56s/it]

 90%|█████████ | 14575/16110 [22:43:32<8:21:56, 19.62s/it]

 90%|█████████ | 14576/16110 [22:43:51<8:20:34, 19.58s/it]

 90%|█████████ | 14577/16110 [22:44:11<8:22:29, 19.67s/it]
{'loss': 0.0906, 'learning_rate': 1.6801025539911673e-06, 'rewards/chosen': -4.540309906005859, 'rewards/rejected': -9.668473243713379, 'rewards/accuracies': 1.0, 'rewards/margins': 5.128162860870361, 'policy_logps/rejected': -399.59112548828125, 'policy_logps/chosen': -338.9410400390625, 'referece_logps/rejected': -302.9064025878906, 'referece_logps/chosen': -293.5379638671875, 'logits/rejected': -0.32604724168777466, 'logits/chosen': -0.28814923763275146, 'epoch': 8.14}

 90%|█████████ | 14578/16110 [22:44:33<8:36:10, 20.22s/it]

 90%|█████████ | 14579/16110 [22:44:52<8:31:10, 20.03s/it]

 91%|█████████ | 14580/16110 [22:45:09<8:05:30, 19.04s/it]


 91%|█████████ | 14582/16110 [22:45:46<8:01:53, 18.92s/it]

 91%|█████████ | 14583/16110 [22:46:04<7:56:24, 18.72s/it]

 91%|█████████ | 14584/16110 [22:46:24<8:03:21, 19.00s/it]

 91%|█████████ | 14585/16110 [22:46:46<8:25:29, 19.89s/it]

 91%|█████████ | 14586/16110 [22:47:08<8:41:28, 20.53s/it]

 91%|█████████ | 14587/16110 [22:47:24<8:11:50, 19.38s/it]

 91%|█████████ | 14588/16110 [22:47:46<8:27:21, 20.00s/it]
{'loss': 0.2071, 'learning_rate': 1.6784795748410249e-06, 'rewards/chosen': -4.7540130615234375, 'rewards/rejected': -8.175726890563965, 'rewards/accuracies': 1.0, 'rewards/margins': 3.42171311378479, 'policy_logps/rejected': -537.2470703125, 'policy_logps/chosen': -557.5324096679688, 'referece_logps/rejected': -455.48980712890625, 'referece_logps/chosen': -509.9923095703125, 'logits/rejected': -0.09396728873252869, 'logits/chosen': -0.17171472311019897, 'epoch': 8.15}


 91%|█████████ | 14590/16110 [22:48:18<7:25:09, 17.57s/it]
{'loss': 0.1158, 'learning_rate': 1.6781841309698365e-06, 'rewards/chosen': -3.397874355316162, 'rewards/rejected': -9.687828063964844, 'rewards/accuracies': 1.0, 'rewards/margins': 6.289953231811523, 'policy_logps/rejected': -279.9523010253906, 'policy_logps/chosen': -257.3095703125, 'referece_logps/rejected': -183.0740203857422, 'referece_logps/chosen': -223.33084106445312, 'logits/rejected': -0.43190205097198486, 'logits/chosen': -0.5696349143981934, 'epoch': 8.15}


 91%|█████████ | 14592/16110 [22:48:56<7:45:14, 18.39s/it]

 91%|█████████ | 14593/16110 [22:49:12<7:25:25, 17.62s/it]

 91%|█████████ | 14594/16110 [22:49:28<7:13:32, 17.16s/it]

 91%|█████████ | 14595/16110 [22:49:46<7:17:23, 17.32s/it]

 91%|█████████ | 14596/16110 [22:50:04<7:25:01, 17.64s/it]
{'loss': 0.102, 'learning_rate': 1.67729714164277e-06, 'rewards/chosen': -3.6933207511901855, 'rewards/rejected': -8.015521049499512, 'rewards/accuracies': 0.875, 'rewards/margins': 4.322200775146484, 'policy_logps/rejected': -425.44403076171875, 'policy_logps/chosen': -468.26495361328125, 'referece_logps/rejected': -345.288818359375, 'referece_logps/chosen': -431.3317565917969, 'logits/rejected': 0.5841718912124634, 'logits/chosen': 0.4753998816013336, 'epoch': 8.15}

 91%|█████████ | 14597/16110 [22:50:17<6:46:53, 16.14s/it]

 91%|█████████ | 14598/16110 [22:50:39<7:30:58, 17.90s/it]

 91%|█████████ | 14599/16110 [22:50:57<7:33:34, 18.01s/it]

 91%|█████████ | 14600/16110 [22:51:11<6:58:46, 16.64s/it]

 91%|█████████ | 14601/16110 [22:51:23<6:25:24, 15.32s/it]


 91%|█████████ | 14603/16110 [22:51:58<7:04:11, 16.89s/it]

 91%|█████████ | 14604/16110 [22:52:14<6:57:02, 16.62s/it]

 91%|█████████ | 14605/16110 [22:52:32<7:04:27, 16.92s/it]

 91%|█████████ | 14606/16110 [22:52:52<7:27:58, 17.87s/it]
{'loss': 0.0964, 'learning_rate': 1.6758166365886029e-06, 'rewards/chosen': -3.2805185317993164, 'rewards/rejected': -8.253888130187988, 'rewards/accuracies': 1.0, 'rewards/margins': 4.973370552062988, 'policy_logps/rejected': -485.0665588378906, 'policy_logps/chosen': -260.63665771484375, 'referece_logps/rejected': -402.5277099609375, 'referece_logps/chosen': -227.83148193359375, 'logits/rejected': -0.22582495212554932, 'logits/chosen': -0.10962174087762833, 'epoch': 8.16}


 91%|█████████ | 14608/16110 [22:53:29<7:35:01, 18.18s/it]
{'loss': 0.1014, 'learning_rate': 1.6755202075822016e-06, 'rewards/chosen': -4.801849842071533, 'rewards/rejected': -9.562396049499512, 'rewards/accuracies': 0.875, 'rewards/margins': 4.76054573059082, 'policy_logps/rejected': -400.0591125488281, 'policy_logps/chosen': -267.4051513671875, 'referece_logps/rejected': -304.4351501464844, 'referece_logps/chosen': -219.38662719726562, 'logits/rejected': -0.14724838733673096, 'logits/chosen': -0.03498431295156479, 'epoch': 8.16}


 91%|█████████ | 14610/16110 [22:54:00<7:08:40, 17.15s/it]

 91%|█████████ | 14611/16110 [22:54:20<7:28:37, 17.96s/it]

 91%|█████████ | 14612/16110 [22:54:42<8:02:21, 19.32s/it]
{'loss': 0.1579, 'learning_rate': 1.6749270219571452e-06, 'rewards/chosen': -4.619409561157227, 'rewards/rejected': -10.220498085021973, 'rewards/accuracies': 0.875, 'rewards/margins': 5.601088523864746, 'policy_logps/rejected': -372.6910400390625, 'policy_logps/chosen': -255.3285675048828, 'referece_logps/rejected': -270.4860534667969, 'referece_logps/chosen': -209.13449096679688, 'logits/rejected': 0.22048242390155792, 'logits/chosen': 0.3065676987171173, 'epoch': 8.16}


 91%|█████████ | 14614/16110 [22:55:06<6:28:43, 15.59s/it]

 91%|█████████ | 14615/16110 [22:55:22<6:32:18, 15.74s/it]
{'loss': 0.1164, 'learning_rate': 1.6744818462664584e-06, 'rewards/chosen': -5.383260250091553, 'rewards/rejected': -8.9840087890625, 'rewards/accuracies': 0.875, 'rewards/margins': 3.600748062133789, 'policy_logps/rejected': -410.8075256347656, 'policy_logps/chosen': -385.7662353515625, 'referece_logps/rejected': -320.9674377441406, 'referece_logps/chosen': -331.9336242675781, 'logits/rejected': 0.47492098808288574, 'logits/chosen': 0.43875032663345337, 'epoch': 8.16}

 91%|█████████ | 14616/16110 [22:55:35<6:12:16, 14.95s/it]


 91%|█████████ | 14618/16110 [22:56:12<6:52:59, 16.61s/it]

 91%|█████████ | 14619/16110 [22:56:33<7:20:35, 17.73s/it]
{'loss': 0.2243, 'learning_rate': 1.6738878970242523e-06, 'rewards/chosen': -5.9526801109313965, 'rewards/rejected': -8.979873657226562, 'rewards/accuracies': 1.0, 'rewards/margins': 3.027193546295166, 'policy_logps/rejected': -466.96759033203125, 'policy_logps/chosen': -446.23468017578125, 'referece_logps/rejected': -377.16888427734375, 'referece_logps/chosen': -386.7078857421875, 'logits/rejected': 0.7498997449874878, 'logits/chosen': 0.7071670293807983, 'epoch': 8.17}

 91%|█████████ | 14620/16110 [22:56:48<7:00:03, 16.92s/it]


 91%|█████████ | 14622/16110 [22:57:25<7:17:01, 17.62s/it]
{'loss': 0.1152, 'learning_rate': 1.673442149061699e-06, 'rewards/chosen': -4.771191596984863, 'rewards/rejected': -8.49415111541748, 'rewards/accuracies': 0.875, 'rewards/margins': 3.72295880317688, 'policy_logps/rejected': -563.8851318359375, 'policy_logps/chosen': -617.7698974609375, 'referece_logps/rejected': -478.9436340332031, 'referece_logps/chosen': -570.0579223632812, 'logits/rejected': 0.6681110262870789, 'logits/chosen': 0.6680751442909241, 'epoch': 8.17}


 91%|█████████ | 14624/16110 [22:57:57<7:01:47, 17.03s/it]

 91%|█████████ | 14625/16110 [22:58:17<7:20:21, 17.79s/it]
{'loss': 0.0812, 'learning_rate': 1.6729961561099706e-06, 'rewards/chosen': -3.3946306705474854, 'rewards/rejected': -7.156627655029297, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7619972229003906, 'policy_logps/rejected': -285.50555419921875, 'policy_logps/chosen': -364.4924621582031, 'referece_logps/rejected': -213.9392547607422, 'referece_logps/chosen': -330.5461730957031, 'logits/rejected': 0.5503420829772949, 'logits/chosen': 0.31155288219451904, 'epoch': 8.17}

 91%|█████████ | 14626/16110 [22:58:27<6:27:04, 15.65s/it]

 91%|█████████ | 14627/16110 [22:58:45<6:44:58, 16.38s/it]


 91%|█████████ | 14629/16110 [22:59:16<6:35:20, 16.02s/it]

 91%|█████████ | 14630/16110 [22:59:30<6:19:48, 15.40s/it]

 91%|█████████ | 14631/16110 [22:59:48<6:39:19, 16.20s/it]
{'loss': 0.0878, 'learning_rate': 1.672103435888061e-06, 'rewards/chosen': -4.981192588806152, 'rewards/rejected': -9.112874984741211, 'rewards/accuracies': 1.0, 'rewards/margins': 4.131681442260742, 'policy_logps/rejected': -346.9952392578125, 'policy_logps/chosen': -391.44732666015625, 'referece_logps/rejected': -255.86647033691406, 'referece_logps/chosen': -341.6354064941406, 'logits/rejected': 0.809983491897583, 'logits/chosen': 0.6249120235443115, 'epoch': 8.17}


 91%|█████████ | 14633/16110 [23:00:22<6:42:25, 16.35s/it]
{'loss': 0.0995, 'learning_rate': 1.6718056450813248e-06, 'rewards/chosen': -4.124344348907471, 'rewards/rejected': -7.479939937591553, 'rewards/accuracies': 0.75, 'rewards/margins': 3.355595588684082, 'policy_logps/rejected': -380.47381591796875, 'policy_logps/chosen': -318.3536376953125, 'referece_logps/rejected': -305.6744079589844, 'referece_logps/chosen': -277.1101989746094, 'logits/rejected': 0.19312119483947754, 'logits/chosen': 0.09036322683095932, 'epoch': 8.17}


 91%|█████████ | 14635/16110 [23:01:05<7:46:03, 18.96s/it]
{'loss': 0.0989, 'learning_rate': 1.671507745655103e-06, 'rewards/chosen': -4.7216081619262695, 'rewards/rejected': -10.221488952636719, 'rewards/accuracies': 1.0, 'rewards/margins': 5.499881267547607, 'policy_logps/rejected': -436.39093017578125, 'policy_logps/chosen': -497.33563232421875, 'referece_logps/rejected': -334.176025390625, 'referece_logps/chosen': -450.1195983886719, 'logits/rejected': -0.36787378787994385, 'logits/chosen': -0.5225851535797119, 'epoch': 8.18}


 91%|█████████ | 14637/16110 [23:01:39<7:17:11, 17.81s/it]
{'loss': 0.0998, 'learning_rate': 1.6712097376575619e-06, 'rewards/chosen': -4.577755928039551, 'rewards/rejected': -11.521781921386719, 'rewards/accuracies': 1.0, 'rewards/margins': 6.94402551651001, 'policy_logps/rejected': -382.7146911621094, 'policy_logps/chosen': -374.0378112792969, 'referece_logps/rejected': -267.496826171875, 'referece_logps/chosen': -328.26025390625, 'logits/rejected': -0.23433931171894073, 'logits/chosen': -0.22147014737129211, 'epoch': 8.18}


 91%|█████████ | 14639/16110 [23:02:19<7:43:34, 18.91s/it]
{'loss': 0.1261, 'learning_rate': 1.6709116211368834e-06, 'rewards/chosen': -4.967525482177734, 'rewards/rejected': -9.879891395568848, 'rewards/accuracies': 1.0, 'rewards/margins': 4.912364959716797, 'policy_logps/rejected': -340.2923278808594, 'policy_logps/chosen': -451.22216796875, 'referece_logps/rejected': -241.49342346191406, 'referece_logps/chosen': -401.54693603515625, 'logits/rejected': -0.030679479241371155, 'logits/chosen': -0.5568926334381104, 'epoch': 8.18}

 91%|█████████ | 14640/16110 [23:02:34<7:16:13, 17.81s/it]

 91%|█████████ | 14641/16110 [23:02:54<7:33:00, 18.50s/it]

 91%|█████████ | 14642/16110 [23:03:11<7:15:48, 17.81s/it]


 91%|█████████ | 14644/16110 [23:03:49<7:31:57, 18.50s/it]
{'loss': 0.1525, 'learning_rate': 1.6701658553628205e-06, 'rewards/chosen': -6.180805683135986, 'rewards/rejected': -12.057086944580078, 'rewards/accuracies': 1.0, 'rewards/margins': 5.876282215118408, 'policy_logps/rejected': -406.8009948730469, 'policy_logps/chosen': -377.6413879394531, 'referece_logps/rejected': -286.2301025390625, 'referece_logps/chosen': -315.8333740234375, 'logits/rejected': -0.3692229092121124, 'logits/chosen': -0.45310360193252563, 'epoch': 8.18}


 91%|█████████ | 14646/16110 [23:04:29<7:50:16, 19.27s/it]
{'loss': 0.1373, 'learning_rate': 1.6698673593908553e-06, 'rewards/chosen': -5.367240905761719, 'rewards/rejected': -8.89556884765625, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5283281803131104, 'policy_logps/rejected': -273.8648376464844, 'policy_logps/chosen': -371.7533874511719, 'referece_logps/rejected': -184.90914916992188, 'referece_logps/chosen': -318.0809631347656, 'logits/rejected': 0.9741436839103699, 'logits/chosen': 0.9767645597457886, 'epoch': 8.18}


 91%|█████████ | 14648/16110 [23:05:09<7:59:45, 19.69s/it]
{'loss': 0.1511, 'learning_rate': 1.669568755112792e-06, 'rewards/chosen': -5.681368827819824, 'rewards/rejected': -8.277385711669922, 'rewards/accuracies': 0.875, 'rewards/margins': 2.596017837524414, 'policy_logps/rejected': -385.1146240234375, 'policy_logps/chosen': -320.6934814453125, 'referece_logps/rejected': -302.3407897949219, 'referece_logps/chosen': -263.8797607421875, 'logits/rejected': 0.33637094497680664, 'logits/chosen': 0.4014502167701721, 'epoch': 8.18}

 91%|█████████ | 14649/16110 [23:05:30<8:07:54, 20.04s/it]


 91%|█████████ | 14651/16110 [23:06:05<7:42:11, 19.01s/it]

 91%|█████████ | 14652/16110 [23:06:25<7:50:39, 19.37s/it]
{'loss': 0.1436, 'learning_rate': 1.6689712218315087e-06, 'rewards/chosen': -6.337398052215576, 'rewards/rejected': -11.597970008850098, 'rewards/accuracies': 1.0, 'rewards/margins': 5.2605719566345215, 'policy_logps/rejected': -536.9515380859375, 'policy_logps/chosen': -693.460693359375, 'referece_logps/rejected': -420.9718017578125, 'referece_logps/chosen': -630.0867919921875, 'logits/rejected': 0.163701593875885, 'logits/chosen': -0.03061145916581154, 'epoch': 8.19}

 91%|█████████ | 14653/16110 [23:06:43<7:38:12, 18.87s/it]

 91%|█████████ | 14654/16110 [23:07:02<7:41:26, 19.02s/it]

 91%|█████████ | 14655/16110 [23:07:14<6:51:28, 16.97s/it]


 91%|█████████ | 14657/16110 [23:07:45<6:23:13, 15.82s/it]
{'loss': 0.1016, 'learning_rate': 1.6682236968684503e-06, 'rewards/chosen': -5.0216450691223145, 'rewards/rejected': -11.33303451538086, 'rewards/accuracies': 1.0, 'rewards/margins': 6.311388969421387, 'policy_logps/rejected': -413.5384826660156, 'policy_logps/chosen': -578.4757080078125, 'referece_logps/rejected': -300.2081298828125, 'referece_logps/chosen': -528.25927734375, 'logits/rejected': 0.2732747197151184, 'logits/chosen': 0.03455265238881111, 'epoch': 8.19}


 91%|█████████ | 14659/16110 [23:08:15<6:13:20, 15.44s/it]

 91%|█████████ | 14660/16110 [23:08:35<6:45:00, 16.76s/it]
{'loss': 0.0996, 'learning_rate': 1.6677748577212292e-06, 'rewards/chosen': -4.506313800811768, 'rewards/rejected': -8.568672180175781, 'rewards/accuracies': 1.0, 'rewards/margins': 4.062358856201172, 'policy_logps/rejected': -295.20355224609375, 'policy_logps/chosen': -397.6855163574219, 'referece_logps/rejected': -209.51681518554688, 'referece_logps/chosen': -352.62237548828125, 'logits/rejected': -0.07277724891901016, 'logits/chosen': -0.2502812147140503, 'epoch': 8.19}

 91%|█████████ | 14661/16110 [23:08:51<6:36:37, 16.42s/it]

 91%|█████████ | 14662/16110 [23:09:12<7:11:38, 17.89s/it]

 91%|█████████ | 14663/16110 [23:09:35<7:43:14, 19.21s/it]

 91%|█████████ | 14664/16110 [23:09:51<7:23:40, 18.41s/it]


 91%|█████████ | 14666/16110 [23:10:23<6:55:01, 17.24s/it]

 91%|█████████ | 14667/16110 [23:10:36<6:19:28, 15.78s/it]
{'loss': 0.0534, 'learning_rate': 1.6667266219420721e-06, 'rewards/chosen': -4.269527435302734, 'rewards/rejected': -11.614465713500977, 'rewards/accuracies': 1.0, 'rewards/margins': 7.344938278198242, 'policy_logps/rejected': -359.29071044921875, 'policy_logps/chosen': -529.8529052734375, 'referece_logps/rejected': -243.14602661132812, 'referece_logps/chosen': -487.15765380859375, 'logits/rejected': 0.2710742652416229, 'logits/chosen': 0.07400347292423248, 'epoch': 8.19}

 91%|█████████ | 14668/16110 [23:10:47<5:45:26, 14.37s/it]


 91%|█████████ | 14670/16110 [23:11:28<7:02:40, 17.61s/it]
{'loss': 0.1095, 'learning_rate': 1.6662769736720273e-06, 'rewards/chosen': -4.483630657196045, 'rewards/rejected': -9.880354881286621, 'rewards/accuracies': 1.0, 'rewards/margins': 5.396723747253418, 'policy_logps/rejected': -293.8392639160156, 'policy_logps/chosen': -387.76263427734375, 'referece_logps/rejected': -195.03567504882812, 'referece_logps/chosen': -342.92633056640625, 'logits/rejected': -0.254856139421463, 'logits/chosen': -0.6332295536994934, 'epoch': 8.2}

 91%|█████████ | 14671/16110 [23:11:49<7:24:36, 18.54s/it]

 91%|█████████ | 14672/16110 [23:12:11<7:49:24, 19.59s/it]

 91%|█████████ | 14673/16110 [23:12:28<7:35:18, 19.01s/it]


 91%|█████████ | 14675/16110 [23:12:58<6:40:06, 16.73s/it]
{'loss': 0.1303, 'learning_rate': 1.6655270213414789e-06, 'rewards/chosen': -4.893859386444092, 'rewards/rejected': -10.561543464660645, 'rewards/accuracies': 1.0, 'rewards/margins': 5.667684555053711, 'policy_logps/rejected': -540.0628051757812, 'policy_logps/chosen': -469.26165771484375, 'referece_logps/rejected': -434.4473571777344, 'referece_logps/chosen': -420.32305908203125, 'logits/rejected': 0.5535699129104614, 'logits/chosen': 0.585540235042572, 'epoch': 8.2}

 91%|█████████ | 14676/16110 [23:13:14<6:39:23, 16.71s/it]

 91%|█████████ | 14677/16110 [23:13:36<7:17:00, 18.30s/it]

 91%|█████████ | 14678/16110 [23:13:56<7:27:57, 18.77s/it]


 91%|█████████ | 14680/16110 [23:14:28<6:52:47, 17.32s/it]
{'loss': 0.2546, 'learning_rate': 1.6647763964838592e-06, 'rewards/chosen': -5.062251567840576, 'rewards/rejected': -10.46591854095459, 'rewards/accuracies': 0.875, 'rewards/margins': 5.403666019439697, 'policy_logps/rejected': -396.90521240234375, 'policy_logps/chosen': -357.1405029296875, 'referece_logps/rejected': -292.2460021972656, 'referece_logps/chosen': -306.51800537109375, 'logits/rejected': -0.01881246268749237, 'logits/chosen': 0.06990085542201996, 'epoch': 8.2}

 91%|█████████ | 14681/16110 [23:14:46<7:01:21, 17.69s/it]

 91%|█████████ | 14682/16110 [23:15:04<7:04:45, 17.85s/it]

 91%|█████████ | 14683/16110 [23:15:22<7:05:44, 17.90s/it]


 91%|█████████ | 14685/16110 [23:16:00<7:11:23, 18.16s/it]
{'loss': 0.1047, 'learning_rate': 1.6640250998576884e-06, 'rewards/chosen': -4.865604877471924, 'rewards/rejected': -11.683753967285156, 'rewards/accuracies': 1.0, 'rewards/margins': 6.818149089813232, 'policy_logps/rejected': -309.6095886230469, 'policy_logps/chosen': -497.650634765625, 'referece_logps/rejected': -192.77206420898438, 'referece_logps/chosen': -448.99456787109375, 'logits/rejected': 0.5219715237617493, 'logits/chosen': 0.264283150434494, 'epoch': 8.2}


 91%|█████████ | 14687/16110 [23:16:38<7:24:08, 18.73s/it]

 91%|█████████ | 14688/16110 [23:16:58<7:30:48, 19.02s/it]

 91%|█████████ | 14689/16110 [23:17:16<7:23:20, 18.72s/it]
{'loss': 0.1579, 'learning_rate': 1.6634235793935534e-06, 'rewards/chosen': -4.770695209503174, 'rewards/rejected': -8.511802673339844, 'rewards/accuracies': 0.875, 'rewards/margins': 3.74110746383667, 'policy_logps/rejected': -407.80450439453125, 'policy_logps/chosen': -405.1373596191406, 'referece_logps/rejected': -322.68646240234375, 'referece_logps/chosen': -357.4303894042969, 'logits/rejected': 0.18578551709651947, 'logits/chosen': 0.2522239089012146, 'epoch': 8.21}


 91%|█████████ | 14691/16110 [23:17:56<7:42:20, 19.55s/it]
{'loss': 0.1559, 'learning_rate': 1.6631226582407952e-06, 'rewards/chosen': -5.461284637451172, 'rewards/rejected': -9.321491241455078, 'rewards/accuracies': 0.875, 'rewards/margins': 3.86020565032959, 'policy_logps/rejected': -394.8951110839844, 'policy_logps/chosen': -394.94708251953125, 'referece_logps/rejected': -301.6802062988281, 'referece_logps/chosen': -340.3342590332031, 'logits/rejected': 0.1449107974767685, 'logits/chosen': 0.1938692331314087, 'epoch': 8.21}


 91%|█████████ | 14693/16110 [23:18:28<6:58:45, 17.73s/it]
{'loss': 0.1214, 'learning_rate': 1.6628216298724422e-06, 'rewards/chosen': -5.7592291831970215, 'rewards/rejected': -13.579362869262695, 'rewards/accuracies': 1.0, 'rewards/margins': 7.820133686065674, 'policy_logps/rejected': -522.5631103515625, 'policy_logps/chosen': -428.8623962402344, 'referece_logps/rejected': -386.76953125, 'referece_logps/chosen': -371.2701110839844, 'logits/rejected': 0.2232419103384018, 'logits/chosen': 0.36127769947052, 'epoch': 8.21}


 91%|█████████ | 14695/16110 [23:19:02<6:54:51, 17.59s/it]
{'loss': 0.1273, 'learning_rate': 1.662520494337166e-06, 'rewards/chosen': -4.087873458862305, 'rewards/rejected': -9.233858108520508, 'rewards/accuracies': 1.0, 'rewards/margins': 5.145984649658203, 'policy_logps/rejected': -379.5679931640625, 'policy_logps/chosen': -309.48626708984375, 'referece_logps/rejected': -287.22943115234375, 'referece_logps/chosen': -268.6075439453125, 'logits/rejected': -0.40283992886543274, 'logits/chosen': -0.3436341881752014, 'epoch': 8.21}


 91%|█████████ | 14697/16110 [23:19:38<7:00:30, 17.86s/it]

 91%|█████████ | 14698/16110 [23:19:56<7:01:56, 17.93s/it]
{'loss': 0.149, 'learning_rate': 1.6620685902027805e-06, 'rewards/chosen': -4.870762825012207, 'rewards/rejected': -10.66158390045166, 'rewards/accuracies': 1.0, 'rewards/margins': 5.790820598602295, 'policy_logps/rejected': -514.2462158203125, 'policy_logps/chosen': -428.4609069824219, 'referece_logps/rejected': -407.63037109375, 'referece_logps/chosen': -379.7532958984375, 'logits/rejected': -0.20328202843666077, 'logits/chosen': -0.041953541338443756, 'epoch': 8.21}

 91%|█████████ | 14699/16110 [23:20:15<7:11:10, 18.33s/it]


 91%|█████████▏| 14701/16110 [23:20:52<7:06:18, 18.15s/it]
{'loss': 0.155, 'learning_rate': 1.6616164452167673e-06, 'rewards/chosen': -4.792675495147705, 'rewards/rejected': -8.95329475402832, 'rewards/accuracies': 0.875, 'rewards/margins': 4.160619258880615, 'policy_logps/rejected': -349.1499328613281, 'policy_logps/chosen': -417.6572570800781, 'referece_logps/rejected': -259.61700439453125, 'referece_logps/chosen': -369.73052978515625, 'logits/rejected': 0.19695261120796204, 'logits/chosen': 0.11073452979326248, 'epoch': 8.21}

 91%|█████████▏| 14702/16110 [23:21:11<7:15:11, 18.54s/it]

 91%|█████████▏| 14703/16110 [23:21:30<7:12:15, 18.43s/it]

 91%|█████████▏| 14704/16110 [23:21:50<7:22:56, 18.90s/it]

 91%|█████████▏| 14705/16110 [23:22:04<6:49:59, 17.51s/it]

 91%|█████████▏| 14706/16110 [23:22:18<6:25:00, 16.45s/it]

 91%|█████████▏| 14707/16110 [23:22:33<6:18:21, 16.18s/it]

 91%|█████████▏| 14708/16110 [23:22:52<6:34:20, 16.88s/it]

 91%|█████████▏| 14709/16110 [23:23:07<6:19:20, 16.25s/it]

 91%|█████████▏| 14710/16110 [23:23:26<6:39:40, 17.13s/it]


 91%|█████████▏| 14712/16110 [23:23:59<6:31:54, 16.82s/it]

 91%|█████████▏| 14713/16110 [23:24:16<6:37:14, 17.06s/it]

 91%|█████████▏| 14714/16110 [23:24:34<6:43:27, 17.34s/it]
{'loss': 0.0899, 'learning_rate': 1.6596543711173902e-06, 'rewards/chosen': -6.017446041107178, 'rewards/rejected': -11.071961402893066, 'rewards/accuracies': 1.0, 'rewards/margins': 5.054516315460205, 'policy_logps/rejected': -508.6186218261719, 'policy_logps/chosen': -477.4405517578125, 'referece_logps/rejected': -397.8990173339844, 'referece_logps/chosen': -417.2660827636719, 'logits/rejected': -0.2455693930387497, 'logits/chosen': -0.37110692262649536, 'epoch': 8.22}

 91%|█████████▏| 14715/16110 [23:24:55<7:09:11, 18.46s/it]


 91%|█████████▏| 14717/16110 [23:25:32<7:12:40, 18.64s/it]
{'loss': 0.2092, 'learning_rate': 1.6592009443694516e-06, 'rewards/chosen': -6.613006114959717, 'rewards/rejected': -9.756476402282715, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1434693336486816, 'policy_logps/rejected': -396.3526611328125, 'policy_logps/chosen': -423.70404052734375, 'referece_logps/rejected': -298.7879333496094, 'referece_logps/chosen': -357.57391357421875, 'logits/rejected': 0.4889473617076874, 'logits/chosen': 0.47593557834625244, 'epoch': 8.22}

 91%|█████████▏| 14718/16110 [23:25:54<7:30:32, 19.42s/it]

 91%|█████████▏| 14719/16110 [23:26:14<7:38:53, 19.79s/it]

 91%|█████████▏| 14720/16110 [23:26:34<7:38:14, 19.78s/it]

 91%|█████████▏| 14721/16110 [23:26:50<7:09:03, 18.53s/it]

 91%|█████████▏| 14722/16110 [23:27:09<7:17:38, 18.92s/it]

 91%|█████████▏| 14723/16110 [23:27:32<7:40:13, 19.91s/it]

 91%|█████████▏| 14724/16110 [23:27:51<7:38:01, 19.83s/it]


 91%|█████████▏| 14726/16110 [23:28:29<7:21:57, 19.16s/it]
{'loss': 0.0927, 'learning_rate': 1.6578392259353708e-06, 'rewards/chosen': -6.100343704223633, 'rewards/rejected': -11.654507637023926, 'rewards/accuracies': 0.875, 'rewards/margins': 5.554163932800293, 'policy_logps/rejected': -357.23565673828125, 'policy_logps/chosen': -358.11480712890625, 'referece_logps/rejected': -240.69056701660156, 'referece_logps/chosen': -297.11138916015625, 'logits/rejected': 0.12373688071966171, 'logits/chosen': -0.021651282906532288, 'epoch': 8.23}

 91%|█████████▏| 14727/16110 [23:28:41<6:36:05, 17.18s/it]

 91%|█████████▏| 14728/16110 [23:28:56<6:20:34, 16.52s/it]

 91%|█████████▏| 14729/16110 [23:29:15<6:39:17, 17.35s/it]

 91%|█████████▏| 14730/16110 [23:29:35<6:54:55, 18.04s/it]


 91%|█████████▏| 14732/16110 [23:30:11<6:52:34, 17.96s/it]

 91%|█████████▏| 14733/16110 [23:30:31<7:07:46, 18.64s/it]
{'loss': 0.0934, 'learning_rate': 1.656778622310612e-06, 'rewards/chosen': -3.0172038078308105, 'rewards/rejected': -8.638981819152832, 'rewards/accuracies': 1.0, 'rewards/margins': 5.62177848815918, 'policy_logps/rejected': -297.61322021484375, 'policy_logps/chosen': -432.33441162109375, 'referece_logps/rejected': -211.223388671875, 'referece_logps/chosen': -402.162353515625, 'logits/rejected': 0.5950242280960083, 'logits/chosen': 0.4444006681442261, 'epoch': 8.23}


 91%|█████████▏| 14735/16110 [23:31:13<7:35:30, 19.88s/it]
[2024-04-06 14:39:04,357] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 91%|█████████▏| 14736/16110 [23:31:33<7:35:26, 19.89s/it]
{'loss': 0.221, 'learning_rate': 1.6563236795654611e-06, 'rewards/chosen': -3.9578657150268555, 'rewards/rejected': -8.770735740661621, 'rewards/accuracies': 0.875, 'rewards/margins': 4.812870025634766, 'policy_logps/rejected': -350.9429626464844, 'policy_logps/chosen': -369.9117126464844, 'referece_logps/rejected': -263.2356262207031, 'referece_logps/chosen': -330.33306884765625, 'logits/rejected': -0.3666914403438568, 'logits/chosen': -0.45733070373535156, 'epoch': 8.23}


 91%|█████████▏| 14738/16110 [23:32:03<6:40:25, 17.51s/it]
{'loss': 0.1921, 'learning_rate': 1.656020251746415e-06, 'rewards/chosen': -4.934741497039795, 'rewards/rejected': -9.689441680908203, 'rewards/accuracies': 0.625, 'rewards/margins': 4.75469970703125, 'policy_logps/rejected': -602.50341796875, 'policy_logps/chosen': -398.44537353515625, 'referece_logps/rejected': -505.60894775390625, 'referece_logps/chosen': -349.0979309082031, 'logits/rejected': -0.20002233982086182, 'logits/chosen': -0.06128387898206711, 'epoch': 8.23}

 91%|█████████▏| 14739/16110 [23:32:14<6:00:19, 15.77s/it]

 91%|█████████▏| 14740/16110 [23:32:34<6:27:15, 16.96s/it]

 92%|█████████▏| 14741/16110 [23:32:52<6:33:01, 17.23s/it]

 92%|█████████▏| 14742/16110 [23:33:12<6:49:28, 17.96s/it]

 92%|█████████▏| 14743/16110 [23:33:32<7:05:41, 18.68s/it]

 92%|█████████▏| 14744/16110 [23:33:50<6:58:25, 18.38s/it]

 92%|█████████▏| 14745/16110 [23:34:04<6:30:54, 17.18s/it]

 92%|█████████▏| 14746/16110 [23:34:19<6:15:59, 16.54s/it]

 92%|█████████▏| 14747/16110 [23:34:32<5:53:49, 15.58s/it]

 92%|█████████▏| 14748/16110 [23:34:49<5:57:33, 15.75s/it]

 92%|█████████▏| 14749/16110 [23:35:09<6:31:47, 17.27s/it]

 92%|█████████▏| 14750/16110 [23:35:29<6:47:04, 17.96s/it]


 92%|█████████▏| 14752/16110 [23:36:03<6:33:06, 17.37s/it]

 92%|█████████▏| 14753/16110 [23:36:17<6:09:58, 16.36s/it]

 92%|█████████▏| 14754/16110 [23:36:37<6:34:39, 17.46s/it]
{'loss': 0.1101, 'learning_rate': 1.6535890148973587e-06, 'rewards/chosen': -5.26364803314209, 'rewards/rejected': -11.570284843444824, 'rewards/accuracies': 1.0, 'rewards/margins': 6.306635856628418, 'policy_logps/rejected': -430.6879577636719, 'policy_logps/chosen': -317.3614196777344, 'referece_logps/rejected': -314.9850769042969, 'referece_logps/chosen': -264.72491455078125, 'logits/rejected': -0.1845526397228241, 'logits/chosen': -0.00233294814825058, 'epoch': 8.24}

 92%|█████████▏| 14755/16110 [23:37:00<7:06:24, 18.88s/it]

 92%|█████████▏| 14756/16110 [23:37:18<7:03:34, 18.77s/it]

 92%|█████████▏| 14757/16110 [23:37:33<6:37:15, 17.62s/it]

 92%|█████████▏| 14758/16110 [23:37:52<6:47:35, 18.09s/it]

 92%|█████████▏| 14759/16110 [23:38:10<6:45:56, 18.03s/it]

 92%|█████████▏| 14760/16110 [23:38:32<7:09:14, 19.08s/it]

 92%|█████████▏| 14761/16110 [23:38:51<7:12:23, 19.23s/it]

 92%|█████████▏| 14762/16110 [23:39:04<6:30:27, 17.38s/it]

 92%|█████████▏| 14763/16110 [23:39:24<6:46:10, 18.09s/it]

 92%|█████████▏| 14764/16110 [23:39:42<6:41:51, 17.91s/it]


 92%|█████████▏| 14766/16110 [23:40:06<5:31:31, 14.80s/it]
{'loss': 0.0841, 'learning_rate': 1.6517611475708532e-06, 'rewards/chosen': -4.637328147888184, 'rewards/rejected': -8.252094268798828, 'rewards/accuracies': 0.875, 'rewards/margins': 3.614766836166382, 'policy_logps/rejected': -338.1805114746094, 'policy_logps/chosen': -523.2113647460938, 'referece_logps/rejected': -255.6595458984375, 'referece_logps/chosen': -476.83807373046875, 'logits/rejected': 0.44425541162490845, 'logits/chosen': 0.16709405183792114, 'epoch': 8.25}

 92%|█████████▏| 14767/16110 [23:40:16<5:03:41, 13.57s/it]

 92%|█████████▏| 14768/16110 [23:40:32<5:16:36, 14.16s/it]

 92%|█████████▏| 14769/16110 [23:40:50<5:43:50, 15.38s/it]

 92%|█████████▏| 14770/16110 [23:41:10<6:13:07, 16.71s/it]

 92%|█████████▏| 14771/16110 [23:41:25<6:05:13, 16.37s/it]

 92%|█████████▏| 14772/16110 [23:41:45<6:25:09, 17.27s/it]

 92%|█████████▏| 14773/16110 [23:42:06<6:51:52, 18.48s/it]

 92%|█████████▏| 14774/16110 [23:42:21<6:27:38, 17.41s/it]

 92%|█████████▏| 14775/16110 [23:42:41<6:41:58, 18.07s/it]

 92%|█████████▏| 14776/16110 [23:42:55<6:17:38, 16.99s/it]

 92%|█████████▏| 14777/16110 [23:43:15<6:35:39, 17.81s/it]

 92%|█████████▏| 14778/16110 [23:43:33<6:35:26, 17.81s/it]

 92%|█████████▏| 14779/16110 [23:43:45<5:59:41, 16.21s/it]

 92%|█████████▏| 14780/16110 [23:43:57<5:32:49, 15.01s/it]

 92%|█████████▏| 14781/16110 [23:44:16<5:54:27, 16.00s/it]

 92%|█████████▏| 14782/16110 [23:44:31<5:46:36, 15.66s/it]

 92%|█████████▏| 14783/16110 [23:44:49<6:02:02, 16.37s/it]

 92%|█████████▏| 14784/16110 [23:45:07<6:16:50, 17.05s/it]

 92%|█████████▏| 14785/16110 [23:45:24<6:11:22, 16.82s/it]

 92%|█████████▏| 14786/16110 [23:45:37<5:48:18, 15.78s/it]

 92%|█████████▏| 14787/16110 [23:45:57<6:15:50, 17.04s/it]

 92%|█████████▏| 14788/16110 [23:46:19<6:48:30, 18.54s/it]

 92%|█████████▏| 14789/16110 [23:46:36<6:37:54, 18.07s/it]

 92%|█████████▏| 14790/16110 [23:46:57<6:56:22, 18.93s/it]

 92%|█████████▏| 14791/16110 [23:47:13<6:39:46, 18.19s/it]

 92%|█████████▏| 14792/16110 [23:47:33<6:50:15, 18.68s/it]

 92%|█████████▏| 14793/16110 [23:47:53<6:56:22, 18.97s/it]

 92%|█████████▏| 14794/16110 [23:48:13<7:02:24, 19.26s/it]


 92%|█████████▏| 14796/16110 [23:48:53<7:16:05, 19.91s/it]

 92%|█████████▏| 14797/16110 [23:49:13<7:14:01, 19.83s/it]

 92%|█████████▏| 14798/16110 [23:49:33<7:15:23, 19.91s/it]

 92%|█████████▏| 14799/16110 [23:49:49<6:52:08, 18.86s/it]

 92%|█████████▏| 14800/16110 [23:50:09<6:58:59, 19.19s/it]

 92%|█████████▏| 14801/16110 [23:50:20<6:08:09, 16.88s/it]

 92%|█████████▏| 14802/16110 [23:50:35<5:50:54, 16.10s/it]

 92%|█████████▏| 14803/16110 [23:50:54<6:13:38, 17.15s/it]

 92%|█████████▏| 14804/16110 [23:51:06<5:35:36, 15.42s/it]

 92%|█████████▏| 14805/16110 [23:51:16<5:04:23, 13.99s/it]

 92%|█████████▏| 14806/16110 [23:51:35<5:37:07, 15.51s/it]

 92%|█████████▏| 14807/16110 [23:51:55<6:04:21, 16.78s/it]

 92%|█████████▏| 14808/16110 [23:52:09<5:46:20, 15.96s/it]

 92%|█████████▏| 14809/16110 [23:52:29<6:08:50, 17.01s/it]

 92%|█████████▏| 14810/16110 [23:52:48<6:24:16, 17.74s/it]

 92%|█████████▏| 14811/16110 [23:53:08<6:38:38, 18.41s/it]

 92%|█████████▏| 14812/16110 [23:53:25<6:28:32, 17.96s/it]

 92%|█████████▏| 14813/16110 [23:53:44<6:35:25, 18.29s/it]

 92%|█████████▏| 14814/16110 [23:54:04<6:47:28, 18.86s/it]

 92%|█████████▏| 14815/16110 [23:54:23<6:44:33, 18.74s/it]

 92%|█████████▏| 14816/16110 [23:54:41<6:41:45, 18.63s/it]

 92%|█████████▏| 14817/16110 [23:55:01<6:47:23, 18.90s/it]

 92%|█████████▏| 14818/16110 [23:55:20<6:50:23, 19.06s/it]

 92%|█████████▏| 14819/16110 [23:55:40<6:57:36, 19.41s/it]

 92%|█████████▏| 14820/16110 [23:56:01<7:02:54, 19.67s/it]

 92%|█████████▏| 14821/16110 [23:56:14<6:21:36, 17.76s/it]

 92%|█████████▏| 14822/16110 [23:56:33<6:27:25, 18.05s/it]

 92%|█████████▏| 14823/16110 [23:56:52<6:36:19, 18.48s/it]

 92%|█████████▏| 14824/16110 [23:57:10<6:33:59, 18.38s/it]

 92%|█████████▏| 14825/16110 [23:57:31<6:49:45, 19.13s/it]

 92%|█████████▏| 14826/16110 [23:57:51<6:53:10, 19.31s/it]

 92%|█████████▏| 14827/16110 [23:58:09<6:42:55, 18.84s/it]

 92%|█████████▏| 14828/16110 [23:58:30<7:00:46, 19.69s/it]

 92%|█████████▏| 14829/16110 [23:58:45<6:25:54, 18.07s/it]

 92%|█████████▏| 14830/16110 [23:58:56<5:43:32, 16.10s/it]

 92%|█████████▏| 14831/16110 [23:59:16<6:06:02, 17.17s/it]

 92%|█████████▏| 14832/16110 [23:59:34<6:12:45, 17.50s/it]

 92%|█████████▏| 14833/16110 [23:59:45<5:33:03, 15.65s/it]

 92%|█████████▏| 14834/16110 [24:00:07<6:12:31, 17.52s/it]

 92%|█████████▏| 14835/16110 [24:00:21<5:47:18, 16.34s/it]

 92%|█████████▏| 14836/16110 [24:00:43<6:24:50, 18.12s/it]

 92%|█████████▏| 14837/16110 [24:01:03<6:34:47, 18.61s/it]

 92%|█████████▏| 14838/16110 [24:01:19<6:21:52, 18.01s/it]

 92%|█████████▏| 14839/16110 [24:01:38<6:23:20, 18.10s/it]

 92%|█████████▏| 14840/16110 [24:01:54<6:11:07, 17.53s/it]

 92%|█████████▏| 14841/16110 [24:02:09<5:56:00, 16.83s/it]

 92%|█████████▏| 14842/16110 [24:02:23<5:38:55, 16.04s/it]

 92%|█████████▏| 14843/16110 [24:02:43<6:02:58, 17.19s/it]

 92%|█████████▏| 14844/16110 [24:03:02<6:10:40, 17.57s/it]

 92%|█████████▏| 14845/16110 [24:03:19<6:08:48, 17.49s/it]

 92%|█████████▏| 14846/16110 [24:03:36<6:06:49, 17.41s/it]

 92%|█████████▏| 14847/16110 [24:03:50<5:41:54, 16.24s/it]
{'loss': 0.0968, 'learning_rate': 1.639324351799443e-06, 'rewards/chosen': -4.152595043182373, 'rewards/rejected': -9.20699691772461, 'rewards/accuracies': 1.0, 'rewards/margins': 5.0544023513793945, 'policy_logps/rejected': -314.3076477050781, 'policy_logps/chosen': -275.51190185546875, 'referece_logps/rejected': -222.23768615722656, 'referece_logps/chosen': -233.98594665527344, 'logits/rejected': -0.18206334114074707, 'logits/chosen': -0.2541705369949341, 'epoch': 8.29}


 92%|█████████▏| 14849/16110 [24:04:26<5:59:44, 17.12s/it]

 92%|█████████▏| 14850/16110 [24:04:46<6:15:16, 17.87s/it]

 92%|█████████▏| 14851/16110 [24:04:58<5:40:23, 16.22s/it]

 92%|█████████▏| 14852/16110 [24:05:14<5:36:36, 16.05s/it]

 92%|█████████▏| 14853/16110 [24:05:37<6:18:56, 18.09s/it]
{'loss': 0.0713, 'learning_rate': 1.6383963222841034e-06, 'rewards/chosen': -5.943617343902588, 'rewards/rejected': -10.416112899780273, 'rewards/accuracies': 0.875, 'rewards/margins': 4.472496032714844, 'policy_logps/rejected': -306.08355712890625, 'policy_logps/chosen': -462.4029541015625, 'referece_logps/rejected': -201.92239379882812, 'referece_logps/chosen': -402.966796875, 'logits/rejected': -0.10524503141641617, 'logits/chosen': -0.418758362531662, 'epoch': 8.3}

 92%|█████████▏| 14854/16110 [24:05:54<6:11:46, 17.76s/it]


 92%|█████████▏| 14856/16110 [24:06:25<5:47:30, 16.63s/it]
{'loss': 0.1069, 'learning_rate': 1.6379319590820466e-06, 'rewards/chosen': -3.391126871109009, 'rewards/rejected': -7.012260437011719, 'rewards/accuracies': 0.875, 'rewards/margins': 3.6211330890655518, 'policy_logps/rejected': -353.2366027832031, 'policy_logps/chosen': -444.7302551269531, 'referece_logps/rejected': -283.1140441894531, 'referece_logps/chosen': -410.8190002441406, 'logits/rejected': -0.6045059561729431, 'logits/chosen': -0.6935022473335266, 'epoch': 8.3}

 92%|█████████▏| 14857/16110 [24:06:47<6:23:33, 18.37s/it]

 92%|█████████▏| 14858/16110 [24:07:08<6:35:33, 18.96s/it]


 92%|█████████▏| 14860/16110 [24:07:45<6:30:12, 18.73s/it]

 92%|█████████▏| 14861/16110 [24:08:07<6:47:15, 19.56s/it]
{'loss': 0.089, 'learning_rate': 1.637157504781975e-06, 'rewards/chosen': -4.855505466461182, 'rewards/rejected': -12.732028007507324, 'rewards/accuracies': 1.0, 'rewards/margins': 7.876522064208984, 'policy_logps/rejected': -696.145263671875, 'policy_logps/chosen': -591.7835693359375, 'referece_logps/rejected': -568.824951171875, 'referece_logps/chosen': -543.228515625, 'logits/rejected': -0.16733938455581665, 'logits/chosen': 0.3530169427394867, 'epoch': 8.3}


 92%|█████████▏| 14863/16110 [24:08:41<6:22:54, 18.42s/it]

 92%|█████████▏| 14864/16110 [24:09:01<6:31:41, 18.86s/it]

 92%|█████████▏| 14865/16110 [24:09:21<6:38:34, 19.21s/it]

 92%|█████████▏| 14866/16110 [24:09:39<6:34:15, 19.02s/it]

 92%|█████████▏| 14867/16110 [24:09:59<6:36:55, 19.16s/it]

 92%|█████████▏| 14868/16110 [24:10:12<6:03:30, 17.56s/it]
{'loss': 0.1591, 'learning_rate': 1.6360721872539073e-06, 'rewards/chosen': -4.679563045501709, 'rewards/rejected': -9.444498062133789, 'rewards/accuracies': 0.875, 'rewards/margins': 4.764935493469238, 'policy_logps/rejected': -454.751708984375, 'policy_logps/chosen': -342.9775390625, 'referece_logps/rejected': -360.3067626953125, 'referece_logps/chosen': -296.181884765625, 'logits/rejected': -0.4238077998161316, 'logits/chosen': -0.19527152180671692, 'epoch': 8.31}

 92%|█████████▏| 14869/16110 [24:10:34<6:27:15, 18.72s/it]


 92%|█████████▏| 14871/16110 [24:11:11<6:28:04, 18.79s/it]

 92%|█████████▏| 14872/16110 [24:11:29<6:27:47, 18.79s/it]

 92%|█████████▏| 14873/16110 [24:11:49<6:33:28, 19.09s/it]

 92%|█████████▏| 14874/16110 [24:12:09<6:39:08, 19.38s/it]

 92%|█████████▏| 14875/16110 [24:12:29<6:39:25, 19.40s/it]
{'loss': 0.1227, 'learning_rate': 1.6349856099115798e-06, 'rewards/chosen': -5.545399188995361, 'rewards/rejected': -10.648106575012207, 'rewards/accuracies': 0.875, 'rewards/margins': 5.102707862854004, 'policy_logps/rejected': -346.9237060546875, 'policy_logps/chosen': -335.98529052734375, 'referece_logps/rejected': -240.44265747070312, 'referece_logps/chosen': -280.53131103515625, 'logits/rejected': 0.13097384572029114, 'logits/chosen': 0.13538643717765808, 'epoch': 8.31}


 92%|█████████▏| 14877/16110 [24:13:01<6:06:02, 17.81s/it]

 92%|█████████▏| 14878/16110 [24:13:14<5:37:31, 16.44s/it]

 92%|█████████▏| 14879/16110 [24:13:33<5:52:35, 17.19s/it]

 92%|█████████▏| 14880/16110 [24:13:46<5:27:24, 15.97s/it]

 92%|█████████▏| 14881/16110 [24:14:06<5:52:05, 17.19s/it]

 92%|█████████▏| 14882/16110 [24:14:19<5:23:21, 15.80s/it]

 92%|█████████▏| 14883/16110 [24:14:39<5:46:12, 16.93s/it]

 92%|█████████▏| 14884/16110 [24:14:53<5:27:40, 16.04s/it]

 92%|█████████▏| 14885/16110 [24:15:14<5:58:19, 17.55s/it]

 92%|█████████▏| 14886/16110 [24:15:34<6:14:43, 18.37s/it]

 92%|█████████▏| 14887/16110 [24:15:55<6:33:49, 19.32s/it]

 92%|█████████▏| 14888/16110 [24:16:15<6:33:43, 19.33s/it]
{'loss': 0.3815, 'learning_rate': 1.632964345539954e-06, 'rewards/chosen': -4.744588851928711, 'rewards/rejected': -9.854451179504395, 'rewards/accuracies': 0.75, 'rewards/margins': 5.10986328125, 'policy_logps/rejected': -319.29339599609375, 'policy_logps/chosen': -386.8914489746094, 'referece_logps/rejected': -220.74891662597656, 'referece_logps/chosen': -339.4455871582031, 'logits/rejected': -0.26607781648635864, 'logits/chosen': -0.5049240589141846, 'epoch': 8.32}


 92%|█████████▏| 14890/16110 [24:16:53<6:33:26, 19.35s/it]

 92%|█████████▏| 14891/16110 [24:17:17<6:56:14, 20.49s/it]

 92%|█████████▏| 14892/16110 [24:17:31<6:17:19, 18.59s/it]

 92%|█████████▏| 14893/16110 [24:17:51<6:27:52, 19.12s/it]
{'loss': 0.0737, 'learning_rate': 1.6321857840922661e-06, 'rewards/chosen': -3.2134506702423096, 'rewards/rejected': -7.988097190856934, 'rewards/accuracies': 1.0, 'rewards/margins': 4.774647235870361, 'policy_logps/rejected': -485.02008056640625, 'policy_logps/chosen': -599.1401977539062, 'referece_logps/rejected': -405.13909912109375, 'referece_logps/chosen': -567.0057373046875, 'logits/rejected': -0.8581507205963135, 'logits/chosen': -1.022911548614502, 'epoch': 8.32}

 92%|█████████▏| 14894/16110 [24:18:10<6:27:18, 19.11s/it]

 92%|█████████▏| 14895/16110 [24:18:28<6:19:26, 18.74s/it]


 92%|█████████▏| 14897/16110 [24:19:00<5:44:19, 17.03s/it]

 92%|█████████▏| 14898/16110 [24:19:20<6:00:57, 17.87s/it]

 92%|█████████▏| 14899/16110 [24:19:39<6:10:28, 18.36s/it]

 92%|█████████▏| 14900/16110 [24:20:00<6:25:32, 19.12s/it]
{'loss': 0.1477, 'learning_rate': 1.6310947249988078e-06, 'rewards/chosen': -4.418398380279541, 'rewards/rejected': -9.262960433959961, 'rewards/accuracies': 1.0, 'rewards/margins': 4.844561576843262, 'policy_logps/rejected': -460.5370788574219, 'policy_logps/chosen': -406.92901611328125, 'referece_logps/rejected': -367.9074401855469, 'referece_logps/chosen': -362.7450256347656, 'logits/rejected': 0.1446533203125, 'logits/chosen': 0.10738124698400497, 'epoch': 8.32}


 93%|█████████▎| 14902/16110 [24:20:36<6:17:01, 18.73s/it]

 93%|█████████▎| 14903/16110 [24:20:53<6:06:55, 18.24s/it]

 93%|█████████▎| 14904/16110 [24:21:14<6:20:58, 18.95s/it]

 93%|█████████▎| 14905/16110 [24:21:26<5:39:39, 16.91s/it]
{'loss': 0.1527, 'learning_rate': 1.6303146316697321e-06, 'rewards/chosen': -4.458378791809082, 'rewards/rejected': -8.486472129821777, 'rewards/accuracies': 0.875, 'rewards/margins': 4.028093338012695, 'policy_logps/rejected': -345.2996520996094, 'policy_logps/chosen': -366.81036376953125, 'referece_logps/rejected': -260.4349670410156, 'referece_logps/chosen': -322.2265625, 'logits/rejected': -0.12825416028499603, 'logits/chosen': -0.2528994381427765, 'epoch': 8.33}


 93%|█████████▎| 14907/16110 [24:22:10<6:32:12, 19.56s/it]

 93%|█████████▎| 14908/16110 [24:22:30<6:35:08, 19.72s/it]

 93%|█████████▎| 14909/16110 [24:22:50<6:37:37, 19.86s/it]

 93%|█████████▎| 14910/16110 [24:23:12<6:49:13, 20.46s/it]

 93%|█████████▎| 14911/16110 [24:23:32<6:44:16, 20.23s/it]

 93%|█████████▎| 14912/16110 [24:23:52<6:43:56, 20.23s/it]

 93%|█████████▎| 14913/16110 [24:24:06<6:04:55, 18.29s/it]

 93%|█████████▎| 14914/16110 [24:24:22<5:54:30, 17.78s/it]

 93%|█████████▎| 14915/16110 [24:24:34<5:18:32, 15.99s/it]

 93%|█████████▎| 14916/16110 [24:24:52<5:29:37, 16.56s/it]
{'loss': 0.0714, 'learning_rate': 1.6285961854126495e-06, 'rewards/chosen': -5.612059593200684, 'rewards/rejected': -10.09032917022705, 'rewards/accuracies': 1.0, 'rewards/margins': 4.478270053863525, 'policy_logps/rejected': -449.2969665527344, 'policy_logps/chosen': -470.9742736816406, 'referece_logps/rejected': -348.3936767578125, 'referece_logps/chosen': -414.8537292480469, 'logits/rejected': 0.838333010673523, 'logits/chosen': 0.7280828952789307, 'epoch': 8.33}


 93%|█████████▎| 14918/16110 [24:25:28<5:48:45, 17.55s/it]

 93%|█████████▎| 14919/16110 [24:25:48<6:00:54, 18.18s/it]

 93%|█████████▎| 14920/16110 [24:26:04<5:50:17, 17.66s/it]

 93%|█████████▎| 14921/16110 [24:26:26<6:10:30, 18.70s/it]
{'loss': 0.0964, 'learning_rate': 1.6278140566397446e-06, 'rewards/chosen': -5.645496845245361, 'rewards/rejected': -11.110639572143555, 'rewards/accuracies': 0.875, 'rewards/margins': 5.465142250061035, 'policy_logps/rejected': -385.2821960449219, 'policy_logps/chosen': -481.04638671875, 'referece_logps/rejected': -274.1757507324219, 'referece_logps/chosen': -424.5914306640625, 'logits/rejected': 0.7241845726966858, 'logits/chosen': 0.7779223918914795, 'epoch': 8.34}


 93%|█████████▎| 14923/16110 [24:27:00<5:48:07, 17.60s/it]

 93%|█████████▎| 14924/16110 [24:27:19<5:53:06, 17.86s/it]

 93%|█████████▎| 14925/16110 [24:27:36<5:52:16, 17.84s/it]
{'loss': 0.1336, 'learning_rate': 1.6271878968029055e-06, 'rewards/chosen': -4.831788063049316, 'rewards/rejected': -9.251721382141113, 'rewards/accuracies': 1.0, 'rewards/margins': 4.419933319091797, 'policy_logps/rejected': -369.4609375, 'policy_logps/chosen': -341.6679992675781, 'referece_logps/rejected': -276.9437561035156, 'referece_logps/chosen': -293.35009765625, 'logits/rejected': -0.14347341656684875, 'logits/chosen': -0.09186877310276031, 'epoch': 8.34}

 93%|█████████▎| 14926/16110 [24:27:53<5:44:13, 17.44s/it]

 93%|█████████▎| 14927/16110 [24:28:13<5:59:51, 18.25s/it]


 93%|█████████▎| 14929/16110 [24:28:53<6:14:28, 19.03s/it]

 93%|█████████▎| 14930/16110 [24:29:16<6:37:10, 20.20s/it]

 93%|█████████▎| 14931/16110 [24:29:32<6:12:23, 18.95s/it]

 93%|█████████▎| 14932/16110 [24:29:51<6:12:51, 18.99s/it]

 93%|█████████▎| 14933/16110 [24:30:07<5:55:24, 18.12s/it]

 93%|█████████▎| 14934/16110 [24:30:20<5:29:11, 16.80s/it]

 93%|█████████▎| 14935/16110 [24:30:40<5:45:37, 17.65s/it]

 93%|█████████▎| 14936/16110 [24:30:58<5:48:14, 17.80s/it]
{'loss': 0.1962, 'learning_rate': 1.625463866981018e-06, 'rewards/chosen': -4.022878170013428, 'rewards/rejected': -8.540619850158691, 'rewards/accuracies': 1.0, 'rewards/margins': 4.517741680145264, 'policy_logps/rejected': -471.9705505371094, 'policy_logps/chosen': -349.18695068359375, 'referece_logps/rejected': -386.5643310546875, 'referece_logps/chosen': -308.95819091796875, 'logits/rejected': 0.22541525959968567, 'logits/chosen': 0.3677007853984833, 'epoch': 8.34}


 93%|█████████▎| 14938/16110 [24:31:40<6:21:39, 19.54s/it]
{'loss': 0.095, 'learning_rate': 1.625150078104083e-06, 'rewards/chosen': -5.114919662475586, 'rewards/rejected': -9.093846321105957, 'rewards/accuracies': 1.0, 'rewards/margins': 3.978926658630371, 'policy_logps/rejected': -434.900390625, 'policy_logps/chosen': -462.4725341796875, 'referece_logps/rejected': -343.9619445800781, 'referece_logps/chosen': -411.3233642578125, 'logits/rejected': 0.8516886234283447, 'logits/chosen': 0.849242091178894, 'epoch': 8.35}


 93%|█████████▎| 14940/16110 [24:32:15<5:57:26, 18.33s/it]

 93%|█████████▎| 14941/16110 [24:32:28<5:29:38, 16.92s/it]

 93%|█████████▎| 14942/16110 [24:32:44<5:20:20, 16.46s/it]

 93%|█████████▎| 14943/16110 [24:32:58<5:09:43, 15.92s/it]

 93%|█████████▎| 14944/16110 [24:33:18<5:32:04, 17.09s/it]

 93%|█████████▎| 14945/16110 [24:33:31<5:04:28, 15.68s/it]

 93%|█████████▎| 14946/16110 [24:33:44<4:51:24, 15.02s/it]
{'loss': 0.0416, 'learning_rate': 1.6238939123431531e-06, 'rewards/chosen': -4.9455246925354, 'rewards/rejected': -10.388641357421875, 'rewards/accuracies': 1.0, 'rewards/margins': 5.443117141723633, 'policy_logps/rejected': -361.633544921875, 'policy_logps/chosen': -482.28070068359375, 'referece_logps/rejected': -257.7471008300781, 'referece_logps/chosen': -432.825439453125, 'logits/rejected': 0.8193928003311157, 'logits/chosen': 0.6493521332740784, 'epoch': 8.35}


 93%|█████████▎| 14948/16110 [24:34:16<4:56:23, 15.30s/it]

 93%|█████████▎| 14949/16110 [24:34:34<5:15:22, 16.30s/it]
{'loss': 0.1735, 'learning_rate': 1.6234224339071851e-06, 'rewards/chosen': -4.500701904296875, 'rewards/rejected': -8.995733261108398, 'rewards/accuracies': 1.0, 'rewards/margins': 4.495031833648682, 'policy_logps/rejected': -507.74908447265625, 'policy_logps/chosen': -367.27703857421875, 'referece_logps/rejected': -417.7917175292969, 'referece_logps/chosen': -322.27001953125, 'logits/rejected': 0.4516639709472656, 'logits/chosen': 0.46450328826904297, 'epoch': 8.35}

 93%|█████████▎| 14950/16110 [24:34:45<4:43:14, 14.65s/it]

 93%|█████████▎| 14951/16110 [24:35:06<5:18:59, 16.51s/it]


 93%|█████████▎| 14953/16110 [24:35:47<5:57:03, 18.52s/it]
{'loss': 0.1281, 'learning_rate': 1.6227934432336083e-06, 'rewards/chosen': -4.563050270080566, 'rewards/rejected': -10.021506309509277, 'rewards/accuracies': 1.0, 'rewards/margins': 5.458456993103027, 'policy_logps/rejected': -701.8533325195312, 'policy_logps/chosen': -462.3948669433594, 'referece_logps/rejected': -601.6382446289062, 'referece_logps/chosen': -416.764404296875, 'logits/rejected': 0.16075924038887024, 'logits/chosen': 0.41563859581947327, 'epoch': 8.35}


 93%|█████████▎| 14955/16110 [24:36:21<5:49:21, 18.15s/it]

 93%|█████████▎| 14956/16110 [24:36:41<6:01:02, 18.77s/it]
{'loss': 0.1569, 'learning_rate': 1.6223214358816737e-06, 'rewards/chosen': -4.516155242919922, 'rewards/rejected': -10.017202377319336, 'rewards/accuracies': 1.0, 'rewards/margins': 5.501047134399414, 'policy_logps/rejected': -349.17657470703125, 'policy_logps/chosen': -682.1146240234375, 'referece_logps/rejected': -249.00453186035156, 'referece_logps/chosen': -636.9530029296875, 'logits/rejected': -0.009339481592178345, 'logits/chosen': -0.40684744715690613, 'epoch': 8.36}

 93%|█████████▎| 14957/16110 [24:37:00<6:03:12, 18.90s/it]


 93%|█████████▎| 14959/16110 [24:37:36<5:51:49, 18.34s/it]

 93%|█████████▎| 14960/16110 [24:37:55<5:51:21, 18.33s/it]

 93%|█████████▎| 14961/16110 [24:38:13<5:49:52, 18.27s/it]
{'loss': 0.1768, 'learning_rate': 1.6215342539529507e-06, 'rewards/chosen': -5.1577582359313965, 'rewards/rejected': -10.340325355529785, 'rewards/accuracies': 1.0, 'rewards/margins': 5.182567596435547, 'policy_logps/rejected': -497.1723327636719, 'policy_logps/chosen': -490.6005859375, 'referece_logps/rejected': -393.7690734863281, 'referece_logps/chosen': -439.0229797363281, 'logits/rejected': 0.1344371736049652, 'logits/chosen': -0.09743847697973251, 'epoch': 8.36}

 93%|█████████▎| 14962/16110 [24:38:34<6:07:46, 19.22s/it]


 93%|█████████▎| 14964/16110 [24:39:13<6:06:13, 19.17s/it]
{'loss': 0.1097, 'learning_rate': 1.6210616432704196e-06, 'rewards/chosen': -5.044195652008057, 'rewards/rejected': -9.532532691955566, 'rewards/accuracies': 0.875, 'rewards/margins': 4.488337516784668, 'policy_logps/rejected': -402.3981628417969, 'policy_logps/chosen': -413.888427734375, 'referece_logps/rejected': -307.0728759765625, 'referece_logps/chosen': -363.4465026855469, 'logits/rejected': -0.1053793802857399, 'logits/chosen': -0.33193498849868774, 'epoch': 8.36}

 93%|█████████▎| 14965/16110 [24:39:30<5:57:59, 18.76s/it]


 93%|█████████▎| 14967/16110 [24:40:05<5:36:34, 17.67s/it]

 93%|█████████▎| 14968/16110 [24:40:21<5:29:36, 17.32s/it]

 93%|█████████▎| 14969/16110 [24:40:35<5:10:31, 16.33s/it]
{'loss': 0.0989, 'learning_rate': 1.6202734568092451e-06, 'rewards/chosen': -4.307950496673584, 'rewards/rejected': -10.408684730529785, 'rewards/accuracies': 1.0, 'rewards/margins': 6.100734710693359, 'policy_logps/rejected': -425.7703857421875, 'policy_logps/chosen': -377.3321228027344, 'referece_logps/rejected': -321.68353271484375, 'referece_logps/chosen': -334.2526550292969, 'logits/rejected': -0.025417406111955643, 'logits/chosen': -0.1545591950416565, 'epoch': 8.36}

 93%|█████████▎| 14970/16110 [24:40:48<4:52:25, 15.39s/it]

 93%|█████████▎| 14971/16110 [24:41:08<5:18:46, 16.79s/it]

 93%|█████████▎| 14972/16110 [24:41:26<5:21:11, 16.93s/it]


 93%|█████████▎| 14974/16110 [24:42:05<5:48:27, 18.40s/it]
{'loss': 0.1516, 'learning_rate': 1.6194846435505435e-06, 'rewards/chosen': -5.643892765045166, 'rewards/rejected': -11.280097961425781, 'rewards/accuracies': 0.875, 'rewards/margins': 5.636204242706299, 'policy_logps/rejected': -416.0050964355469, 'policy_logps/chosen': -367.92462158203125, 'referece_logps/rejected': -303.2041320800781, 'referece_logps/chosen': -311.4857177734375, 'logits/rejected': 0.4022936522960663, 'logits/chosen': 0.2669309675693512, 'epoch': 8.37}

 93%|█████████▎| 14975/16110 [24:42:26<5:59:09, 18.99s/it]


 93%|█████████▎| 14977/16110 [24:43:02<5:51:54, 18.64s/it]
{'loss': 0.1363, 'learning_rate': 1.6190110550640809e-06, 'rewards/chosen': -4.885747909545898, 'rewards/rejected': -8.259893417358398, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3741455078125, 'policy_logps/rejected': -392.9932861328125, 'policy_logps/chosen': -474.1349182128906, 'referece_logps/rejected': -310.39434814453125, 'referece_logps/chosen': -425.2774658203125, 'logits/rejected': 0.6971530318260193, 'logits/chosen': 0.5563643574714661, 'epoch': 8.37}


 93%|█████████▎| 14979/16110 [24:43:33<5:18:43, 16.91s/it]
{'loss': 0.1357, 'learning_rate': 1.6186952042914249e-06, 'rewards/chosen': -4.526396751403809, 'rewards/rejected': -10.50671100616455, 'rewards/accuracies': 1.0, 'rewards/margins': 5.980313301086426, 'policy_logps/rejected': -386.28363037109375, 'policy_logps/chosen': -403.122314453125, 'referece_logps/rejected': -281.2165222167969, 'referece_logps/chosen': -357.85833740234375, 'logits/rejected': 0.31102505326271057, 'logits/chosen': 0.26751378178596497, 'epoch': 8.37}

 93%|█████████▎| 14980/16110 [24:43:46<4:54:30, 15.64s/it]

 93%|█████████▎| 14981/16110 [24:44:04<5:09:44, 16.46s/it]


 93%|█████████▎| 14983/16110 [24:44:39<5:14:49, 16.76s/it]
{'loss': 0.1965, 'learning_rate': 1.6180632026998912e-06, 'rewards/chosen': -3.451798915863037, 'rewards/rejected': -9.332996368408203, 'rewards/accuracies': 1.0, 'rewards/margins': 5.881196022033691, 'policy_logps/rejected': -383.25762939453125, 'policy_logps/chosen': -296.16229248046875, 'referece_logps/rejected': -289.9276428222656, 'referece_logps/chosen': -261.6442565917969, 'logits/rejected': -0.43519994616508484, 'logits/chosen': -0.350541889667511, 'epoch': 8.37}

 93%|█████████▎| 14984/16110 [24:44:57<5:18:26, 16.97s/it]


 93%|█████████▎| 14986/16110 [24:45:31<5:20:34, 17.11s/it]

 93%|█████████▎| 14987/16110 [24:45:52<5:39:36, 18.14s/it]
{'loss': 0.0955, 'learning_rate': 1.617430801387369e-06, 'rewards/chosen': -4.27879524230957, 'rewards/rejected': -8.255122184753418, 'rewards/accuracies': 1.0, 'rewards/margins': 3.976327657699585, 'policy_logps/rejected': -539.6345825195312, 'policy_logps/chosen': -413.551025390625, 'referece_logps/rejected': -457.0833740234375, 'referece_logps/chosen': -370.7630920410156, 'logits/rejected': 0.6470559239387512, 'logits/chosen': 0.7421141862869263, 'epoch': 8.37}


 93%|█████████▎| 14989/16110 [24:46:25<5:27:04, 17.51s/it]

 93%|█████████▎| 14990/16110 [24:46:42<5:19:39, 17.12s/it]

 93%|█████████▎| 14991/16110 [24:47:04<5:46:44, 18.59s/it]
{'loss': 0.0533, 'learning_rate': 1.6167980007628523e-06, 'rewards/chosen': -4.696572303771973, 'rewards/rejected': -9.010398864746094, 'rewards/accuracies': 1.0, 'rewards/margins': 4.313826560974121, 'policy_logps/rejected': -687.1987915039062, 'policy_logps/chosen': -452.926513671875, 'referece_logps/rejected': -597.0948486328125, 'referece_logps/chosen': -405.96075439453125, 'logits/rejected': -0.4164420962333679, 'logits/chosen': -0.10780768096446991, 'epoch': 8.37}


 93%|█████████▎| 14993/16110 [24:47:31<4:57:31, 15.98s/it]

 93%|█████████▎| 14994/16110 [24:47:53<5:32:50, 17.90s/it]
{'loss': 0.1377, 'learning_rate': 1.6163231384921496e-06, 'rewards/chosen': -3.964921712875366, 'rewards/rejected': -8.41079330444336, 'rewards/accuracies': 1.0, 'rewards/margins': 4.445871829986572, 'policy_logps/rejected': -309.2232360839844, 'policy_logps/chosen': -309.58990478515625, 'referece_logps/rejected': -225.1152801513672, 'referece_logps/chosen': -269.940673828125, 'logits/rejected': 0.4484817087650299, 'logits/chosen': 0.3674757182598114, 'epoch': 8.38}


 93%|█████████▎| 14996/16110 [24:48:32<5:42:53, 18.47s/it]

 93%|█████████▎| 14997/16110 [24:48:49<5:37:10, 18.18s/it]
{'loss': 0.1273, 'learning_rate': 1.6158480520113971e-06, 'rewards/chosen': -4.922769069671631, 'rewards/rejected': -10.167638778686523, 'rewards/accuracies': 0.875, 'rewards/margins': 5.244869232177734, 'policy_logps/rejected': -530.7164916992188, 'policy_logps/chosen': -491.0546875, 'referece_logps/rejected': -429.0401306152344, 'referece_logps/chosen': -441.82696533203125, 'logits/rejected': 0.31503361463546753, 'logits/chosen': 0.32278281450271606, 'epoch': 8.38}


 93%|█████████▎| 14999/16110 [24:49:31<6:03:12, 19.62s/it]

 93%|█████████▎| 15000/16110 [24:49:45<5:32:43, 17.98s/it]

 93%|█████████▎| 15001/16110 [24:50:22<7:12:49, 23.42s/it]
{'loss': 0.0513, 'learning_rate': 1.6152142548979367e-06, 'rewards/chosen': -3.279237747192383, 'rewards/rejected': -10.016646385192871, 'rewards/accuracies': 1.0, 'rewards/margins': 6.737407684326172, 'policy_logps/rejected': -308.97320556640625, 'policy_logps/chosen': -350.3733215332031, 'referece_logps/rejected': -208.80673217773438, 'referece_logps/chosen': -317.5809326171875, 'logits/rejected': -0.11883477121591568, 'logits/chosen': -0.11279810965061188, 'epoch': 8.38}


 93%|█████████▎| 15003/16110 [24:50:50<5:47:15, 18.82s/it]
{'loss': 0.1097, 'learning_rate': 1.6148972071111459e-06, 'rewards/chosen': -4.43776273727417, 'rewards/rejected': -9.216824531555176, 'rewards/accuracies': 0.875, 'rewards/margins': 4.7790608406066895, 'policy_logps/rejected': -508.67230224609375, 'policy_logps/chosen': -462.4919738769531, 'referece_logps/rejected': -416.5040588378906, 'referece_logps/chosen': -418.1143493652344, 'logits/rejected': -0.05674894154071808, 'logits/chosen': 0.016890257596969604, 'epoch': 8.38}

 93%|█████████▎| 15004/16110 [24:51:05<5:27:48, 17.78s/it]

 93%|█████████▎| 15005/16110 [24:51:25<5:37:46, 18.34s/it]

 93%|█████████▎| 15006/16110 [24:51:43<5:39:30, 18.45s/it]

 93%|█████████▎| 15007/16110 [24:51:59<5:23:38, 17.61s/it]


 93%|█████████▎| 15009/16110 [24:52:38<5:42:04, 18.64s/it]
{'loss': 0.0926, 'learning_rate': 1.6139454674457174e-06, 'rewards/chosen': -4.996901988983154, 'rewards/rejected': -9.85766315460205, 'rewards/accuracies': 0.875, 'rewards/margins': 4.86076021194458, 'policy_logps/rejected': -450.4657897949219, 'policy_logps/chosen': -410.6307373046875, 'referece_logps/rejected': -351.8891906738281, 'referece_logps/chosen': -360.6617126464844, 'logits/rejected': 0.06055714935064316, 'logits/chosen': -0.03134682774543762, 'epoch': 8.38}


 93%|█████████▎| 15011/16110 [24:53:10<5:22:50, 17.63s/it]
{'loss': 0.113, 'learning_rate': 1.6136280222931995e-06, 'rewards/chosen': -4.401085376739502, 'rewards/rejected': -11.039361000061035, 'rewards/accuracies': 1.0, 'rewards/margins': 6.638275623321533, 'policy_logps/rejected': -323.88934326171875, 'policy_logps/chosen': -392.5380859375, 'referece_logps/rejected': -213.4957275390625, 'referece_logps/chosen': -348.5272216796875, 'logits/rejected': 0.19299572706222534, 'logits/chosen': -0.17738932371139526, 'epoch': 8.39}


 93%|█████████▎| 15013/16110 [24:53:40<4:58:26, 16.32s/it]

 93%|█████████▎| 15014/16110 [24:53:52<4:36:23, 15.13s/it]
{'loss': 0.079, 'learning_rate': 1.613151668555795e-06, 'rewards/chosen': -4.785611629486084, 'rewards/rejected': -10.086238861083984, 'rewards/accuracies': 1.0, 'rewards/margins': 5.300627708435059, 'policy_logps/rejected': -420.69793701171875, 'policy_logps/chosen': -324.119384765625, 'referece_logps/rejected': -319.83551025390625, 'referece_logps/chosen': -276.2632141113281, 'logits/rejected': 0.09473809599876404, 'logits/chosen': 0.07048395276069641, 'epoch': 8.39}

 93%|█████████▎| 15015/16110 [24:54:09<4:47:43, 15.77s/it]


 93%|█████████▎| 15017/16110 [24:54:46<5:17:02, 17.40s/it]
{'loss': 0.1242, 'learning_rate': 1.6126750917620787e-06, 'rewards/chosen': -5.2622857093811035, 'rewards/rejected': -11.765347480773926, 'rewards/accuracies': 1.0, 'rewards/margins': 6.503061294555664, 'policy_logps/rejected': -617.1097412109375, 'policy_logps/chosen': -579.0369873046875, 'referece_logps/rejected': -499.45623779296875, 'referece_logps/chosen': -526.4141235351562, 'logits/rejected': -0.16143444180488586, 'logits/chosen': -0.15018558502197266, 'epoch': 8.39}

 93%|█████████▎| 15018/16110 [24:55:05<5:26:39, 17.95s/it]

 93%|█████████▎| 15019/16110 [24:55:23<5:25:31, 17.90s/it]

 93%|█████████▎| 15020/16110 [24:55:43<5:35:26, 18.46s/it]

 93%|█████████▎| 15021/16110 [24:56:02<5:35:49, 18.50s/it]


 93%|█████████▎| 15023/16110 [24:56:40<5:40:50, 18.81s/it]
{'loss': 0.1183, 'learning_rate': 1.611721269699282e-06, 'rewards/chosen': -4.084904193878174, 'rewards/rejected': -10.35914421081543, 'rewards/accuracies': 1.0, 'rewards/margins': 6.274238586425781, 'policy_logps/rejected': -390.005615234375, 'policy_logps/chosen': -390.701904296875, 'referece_logps/rejected': -286.4142150878906, 'referece_logps/chosen': -349.85284423828125, 'logits/rejected': -0.0822676420211792, 'logits/chosen': -0.20803634822368622, 'epoch': 8.39}

 93%|█████████▎| 15024/16110 [24:56:59<5:40:39, 18.82s/it]

 93%|█████████▎| 15025/16110 [24:57:19<5:49:06, 19.31s/it]


 93%|█████████▎| 15027/16110 [24:57:54<5:35:56, 18.61s/it]

 93%|█████████▎| 15028/16110 [24:58:10<5:22:03, 17.86s/it]

 93%|█████████▎| 15029/16110 [24:58:26<5:10:29, 17.23s/it]

 93%|█████████▎| 15030/16110 [24:58:46<5:24:45, 18.04s/it]
{'loss': 0.0726, 'learning_rate': 1.6106073523474505e-06, 'rewards/chosen': -5.475102424621582, 'rewards/rejected': -8.578128814697266, 'rewards/accuracies': 1.0, 'rewards/margins': 3.103025197982788, 'policy_logps/rejected': -363.40972900390625, 'policy_logps/chosen': -325.8457946777344, 'referece_logps/rejected': -277.6284484863281, 'referece_logps/chosen': -271.0947570800781, 'logits/rejected': 0.0817771628499031, 'logits/chosen': -0.007902108132839203, 'epoch': 8.4}

 93%|█████████▎| 15031/16110 [24:59:01<5:07:23, 17.09s/it]


 93%|█████████▎| 15033/16110 [24:59:39<5:24:01, 18.05s/it]
{'loss': 0.1129, 'learning_rate': 1.610129588850114e-06, 'rewards/chosen': -3.676661252975464, 'rewards/rejected': -10.074959754943848, 'rewards/accuracies': 1.0, 'rewards/margins': 6.398299217224121, 'policy_logps/rejected': -484.15509033203125, 'policy_logps/chosen': -388.6915283203125, 'referece_logps/rejected': -383.405517578125, 'referece_logps/chosen': -351.9249267578125, 'logits/rejected': -0.4674229323863983, 'logits/chosen': -0.5135297775268555, 'epoch': 8.4}


 93%|█████████▎| 15035/16110 [25:00:16<5:26:56, 18.25s/it]

 93%|█████████▎| 15036/16110 [25:00:29<4:55:39, 16.52s/it]

 93%|█████████▎| 15037/16110 [25:00:45<4:52:35, 16.36s/it]

 93%|█████████▎| 15038/16110 [25:01:04<5:10:24, 17.37s/it]

 93%|█████████▎| 15039/16110 [25:01:24<5:22:58, 18.09s/it]

 93%|█████████▎| 15040/16110 [25:01:46<5:42:34, 19.21s/it]

 93%|█████████▎| 15041/16110 [25:02:02<5:26:23, 18.32s/it]

 93%|█████████▎| 15042/16110 [25:02:15<4:56:03, 16.63s/it]

 93%|█████████▎| 15043/16110 [25:02:35<5:13:23, 17.62s/it]
{'loss': 0.0938, 'learning_rate': 1.6085354418137073e-06, 'rewards/chosen': -6.804747104644775, 'rewards/rejected': -12.039589881896973, 'rewards/accuracies': 0.875, 'rewards/margins': 5.234842300415039, 'policy_logps/rejected': -563.5748291015625, 'policy_logps/chosen': -502.3290100097656, 'referece_logps/rejected': -443.178955078125, 'referece_logps/chosen': -434.28155517578125, 'logits/rejected': 0.6105157732963562, 'logits/chosen': 0.5937016010284424, 'epoch': 8.4}

 93%|█████████▎| 15044/16110 [25:02:55<5:27:16, 18.42s/it]

 93%|█████████▎| 15045/16110 [25:03:09<5:05:42, 17.22s/it]

 93%|█████████▎| 15046/16110 [25:03:29<5:18:24, 17.96s/it]

 93%|█████████▎| 15047/16110 [25:03:49<5:29:48, 18.62s/it]

 93%|█████████▎| 15048/16110 [25:04:06<5:17:10, 17.92s/it]


 93%|█████████▎| 15050/16110 [25:04:44<5:26:02, 18.45s/it]

 93%|█████████▎| 15051/16110 [25:05:04<5:34:54, 18.97s/it]
{'loss': 0.0621, 'learning_rate': 1.6072583528595722e-06, 'rewards/chosen': -5.383482456207275, 'rewards/rejected': -11.47850513458252, 'rewards/accuracies': 1.0, 'rewards/margins': 6.095022201538086, 'policy_logps/rejected': -417.2305908203125, 'policy_logps/chosen': -371.5914306640625, 'referece_logps/rejected': -302.445556640625, 'referece_logps/chosen': -317.7566223144531, 'logits/rejected': 0.024514421820640564, 'logits/chosen': 0.09884963929653168, 'epoch': 8.41}


 93%|█████████▎| 15053/16110 [25:05:33<4:51:48, 16.56s/it]
{'loss': 0.1336, 'learning_rate': 1.6069388350337174e-06, 'rewards/chosen': -4.9865312576293945, 'rewards/rejected': -8.849954605102539, 'rewards/accuracies': 0.75, 'rewards/margins': 3.863422393798828, 'policy_logps/rejected': -353.74237060546875, 'policy_logps/chosen': -395.15802001953125, 'referece_logps/rejected': -265.2428283691406, 'referece_logps/chosen': -345.292724609375, 'logits/rejected': 0.3185936212539673, 'logits/chosen': 0.15007460117340088, 'epoch': 8.41}


 93%|█████████▎| 15055/16110 [25:06:01<4:30:50, 15.40s/it]
{'loss': 0.1208, 'learning_rate': 1.6066192190762314e-06, 'rewards/chosen': -5.475327491760254, 'rewards/rejected': -12.659576416015625, 'rewards/accuracies': 0.875, 'rewards/margins': 7.184249401092529, 'policy_logps/rejected': -377.6899108886719, 'policy_logps/chosen': -374.5868225097656, 'referece_logps/rejected': -251.0941619873047, 'referece_logps/chosen': -319.8335266113281, 'logits/rejected': 0.3244036138057709, 'logits/chosen': 0.34429460763931274, 'epoch': 8.41}

 93%|█████████▎| 15056/16110 [25:06:19<4:45:43, 16.26s/it]


 93%|█████████▎| 15058/16110 [25:06:53<4:48:35, 16.46s/it]

 93%|█████████▎| 15059/16110 [25:07:13<5:06:49, 17.52s/it]
{'loss': 0.1932, 'learning_rate': 1.6059796929730875e-06, 'rewards/chosen': -4.172253608703613, 'rewards/rejected': -8.630654335021973, 'rewards/accuracies': 0.875, 'rewards/margins': 4.458400726318359, 'policy_logps/rejected': -695.1751098632812, 'policy_logps/chosen': -572.9268188476562, 'referece_logps/rejected': -608.8685302734375, 'referece_logps/chosen': -531.2042846679688, 'logits/rejected': 0.4148635268211365, 'logits/chosen': 0.29855528473854065, 'epoch': 8.41}


 93%|█████████▎| 15061/16110 [25:07:43<4:43:24, 16.21s/it]
{'loss': 0.1163, 'learning_rate': 1.6056597829308303e-06, 'rewards/chosen': -4.454732418060303, 'rewards/rejected': -9.372621536254883, 'rewards/accuracies': 0.875, 'rewards/margins': 4.91788911819458, 'policy_logps/rejected': -377.71044921875, 'policy_logps/chosen': -252.44944763183594, 'referece_logps/rejected': -283.9842529296875, 'referece_logps/chosen': -207.90213012695312, 'logits/rejected': -0.3265555799007416, 'logits/chosen': -0.18590348958969116, 'epoch': 8.41}

 93%|█████████▎| 15062/16110 [25:08:02<4:59:59, 17.18s/it]

 94%|█████████▎| 15063/16110 [25:08:22<5:13:08, 17.95s/it]


 94%|█████████▎| 15065/16110 [25:09:03<5:35:07, 19.24s/it]

 94%|█████████▎| 15066/16110 [25:09:23<5:38:24, 19.45s/it]
{'loss': 0.0498, 'learning_rate': 1.6048595795172397e-06, 'rewards/chosen': -5.254679203033447, 'rewards/rejected': -12.090524673461914, 'rewards/accuracies': 1.0, 'rewards/margins': 6.835845947265625, 'policy_logps/rejected': -416.1309814453125, 'policy_logps/chosen': -386.2603454589844, 'referece_logps/rejected': -295.2257080078125, 'referece_logps/chosen': -333.71356201171875, 'logits/rejected': 0.5998620390892029, 'logits/chosen': 0.4323222041130066, 'epoch': 8.42}

 94%|█████████▎| 15067/16110 [25:09:45<5:49:53, 20.13s/it]

 94%|█████████▎| 15068/16110 [25:10:03<5:37:53, 19.46s/it]

 94%|█████████▎| 15069/16110 [25:10:16<5:07:42, 17.74s/it]


 94%|█████████▎| 15071/16110 [25:10:57<5:27:26, 18.91s/it]
{'loss': 0.129, 'learning_rate': 1.6040587648821231e-06, 'rewards/chosen': -4.522065162658691, 'rewards/rejected': -11.052278518676758, 'rewards/accuracies': 0.875, 'rewards/margins': 6.530213832855225, 'policy_logps/rejected': -386.0041198730469, 'policy_logps/chosen': -370.05267333984375, 'referece_logps/rejected': -275.48138427734375, 'referece_logps/chosen': -324.8320617675781, 'logits/rejected': 0.6373366117477417, 'logits/chosen': 0.5237029194831848, 'epoch': 8.42}

 94%|█████████▎| 15072/16110 [25:11:14<5:18:12, 18.39s/it]

 94%|█████████▎| 15073/16110 [25:11:32<5:16:17, 18.30s/it]

 94%|█████████▎| 15074/16110 [25:11:48<5:02:01, 17.49s/it]

 94%|█████████▎| 15075/16110 [25:12:02<4:44:25, 16.49s/it]

 94%|█████████▎| 15076/16110 [25:12:14<4:18:51, 15.02s/it]

 94%|█████████▎| 15077/16110 [25:12:26<4:03:18, 14.13s/it]

 94%|█████████▎| 15078/16110 [25:12:46<4:32:34, 15.85s/it]

 94%|█████████▎| 15079/16110 [25:12:58<4:12:47, 14.71s/it]

 94%|█████████▎| 15080/16110 [25:13:13<4:16:11, 14.92s/it]

 94%|█████████▎| 15081/16110 [25:13:27<4:10:49, 14.62s/it]


 94%|█████████▎| 15083/16110 [25:14:03<4:38:53, 16.29s/it]

 94%|█████████▎| 15084/16110 [25:14:24<4:58:11, 17.44s/it]
{'loss': 0.0705, 'learning_rate': 1.6019737921229384e-06, 'rewards/chosen': -5.466644287109375, 'rewards/rejected': -10.673334121704102, 'rewards/accuracies': 1.0, 'rewards/margins': 5.206688404083252, 'policy_logps/rejected': -394.151123046875, 'policy_logps/chosen': -553.7702026367188, 'referece_logps/rejected': -287.41778564453125, 'referece_logps/chosen': -499.103759765625, 'logits/rejected': -0.038789987564086914, 'logits/chosen': -0.18315261602401733, 'epoch': 8.43}

 94%|█████████▎| 15085/16110 [25:14:39<4:46:26, 16.77s/it]

 94%|█████████▎| 15086/16110 [25:14:59<5:01:42, 17.68s/it]

 94%|█████████▎| 15087/16110 [25:15:18<5:12:29, 18.33s/it]

 94%|█████████▎| 15088/16110 [25:15:38<5:20:21, 18.81s/it]

 94%|█████████▎| 15089/16110 [25:15:54<5:04:07, 17.87s/it]


 94%|█████████▎| 15091/16110 [25:16:26<4:47:51, 16.95s/it]
{'loss': 0.1515, 'learning_rate': 1.6008494103203981e-06, 'rewards/chosen': -4.8678364753723145, 'rewards/rejected': -8.964715003967285, 'rewards/accuracies': 0.875, 'rewards/margins': 4.096878528594971, 'policy_logps/rejected': -526.4421997070312, 'policy_logps/chosen': -501.35357666015625, 'referece_logps/rejected': -436.7950439453125, 'referece_logps/chosen': -452.6752014160156, 'logits/rejected': 0.2229652851819992, 'logits/chosen': 0.32140296697616577, 'epoch': 8.43}

 94%|█████████▎| 15092/16110 [25:16:43<4:48:40, 17.01s/it]

 94%|█████████▎| 15093/16110 [25:16:58<4:38:14, 16.41s/it]

 94%|█████████▎| 15094/16110 [25:17:16<4:48:05, 17.01s/it]

 94%|█████████▎| 15095/16110 [25:17:30<4:33:54, 16.19s/it]

 94%|█████████▎| 15096/16110 [25:17:52<5:01:00, 17.81s/it]


 94%|█████████▎| 15098/16110 [25:18:32<5:17:04, 18.80s/it]
{'loss': 0.1966, 'learning_rate': 1.5997238384663666e-06, 'rewards/chosen': -4.894777774810791, 'rewards/rejected': -7.967672348022461, 'rewards/accuracies': 0.875, 'rewards/margins': 3.07289457321167, 'policy_logps/rejected': -512.770263671875, 'policy_logps/chosen': -546.9158325195312, 'referece_logps/rejected': -433.0935974121094, 'referece_logps/chosen': -497.96795654296875, 'logits/rejected': 0.3663328289985657, 'logits/chosen': 0.39439842104911804, 'epoch': 8.43}

 94%|█████████▎| 15099/16110 [25:18:52<5:25:55, 19.34s/it]

 94%|█████████▎| 15100/16110 [25:19:09<5:11:31, 18.51s/it]

 94%|█████████▎| 15101/16110 [25:19:29<5:19:27, 19.00s/it]

 94%|█████████▎| 15102/16110 [25:19:43<4:51:57, 17.38s/it]


 94%|█████████▍| 15104/16110 [25:20:16<4:49:39, 17.28s/it]
{'loss': 0.1346, 'learning_rate': 1.598758117097445e-06, 'rewards/chosen': -4.2895684242248535, 'rewards/rejected': -9.800762176513672, 'rewards/accuracies': 0.875, 'rewards/margins': 5.511194229125977, 'policy_logps/rejected': -262.9300842285156, 'policy_logps/chosen': -377.6138916015625, 'referece_logps/rejected': -164.92247009277344, 'referece_logps/chosen': -334.71820068359375, 'logits/rejected': 0.03833410143852234, 'logits/chosen': 0.1271703988313675, 'epoch': 8.44}

 94%|█████████▍| 15105/16110 [25:20:36<5:03:48, 18.14s/it]

 94%|█████████▍| 15106/16110 [25:20:55<5:09:23, 18.49s/it]

 94%|█████████▍| 15107/16110 [25:21:11<4:53:33, 17.56s/it]

 94%|█████████▍| 15108/16110 [25:21:32<5:12:31, 18.71s/it]

 94%|█████████▍| 15109/16110 [25:21:50<5:06:02, 18.34s/it]

 94%|█████████▍| 15110/16110 [25:22:10<5:18:23, 19.10s/it]

 94%|█████████▍| 15111/16110 [25:22:27<5:04:06, 18.27s/it]


 94%|█████████▍| 15113/16110 [25:23:02<4:53:23, 17.66s/it]
{'loss': 0.0966, 'learning_rate': 1.597307901832718e-06, 'rewards/chosen': -4.300853729248047, 'rewards/rejected': -9.583440780639648, 'rewards/accuracies': 1.0, 'rewards/margins': 5.282587051391602, 'policy_logps/rejected': -429.3255310058594, 'policy_logps/chosen': -465.71905517578125, 'referece_logps/rejected': -333.4911193847656, 'referece_logps/chosen': -422.71051025390625, 'logits/rejected': -0.17849962413311005, 'logits/chosen': -0.4087564945220947, 'epoch': 8.44}

 94%|█████████▍| 15114/16110 [25:23:24<5:16:59, 19.10s/it]

 94%|█████████▍| 15115/16110 [25:23:45<5:24:26, 19.56s/it]

 94%|█████████▍| 15116/16110 [25:23:59<4:57:02, 17.93s/it]

 94%|█████████▍| 15117/16110 [25:24:14<4:40:30, 16.95s/it]


 94%|█████████▍| 15119/16110 [25:24:52<4:58:57, 18.10s/it]
{'loss': 0.1392, 'learning_rate': 1.5963400049003265e-06, 'rewards/chosen': -4.273551940917969, 'rewards/rejected': -10.127500534057617, 'rewards/accuracies': 1.0, 'rewards/margins': 5.853947639465332, 'policy_logps/rejected': -467.374267578125, 'policy_logps/chosen': -504.7715148925781, 'referece_logps/rejected': -366.0992736816406, 'referece_logps/chosen': -462.0359802246094, 'logits/rejected': 0.7515875697135925, 'logits/chosen': 0.6813414096832275, 'epoch': 8.45}

 94%|█████████▍| 15120/16110 [25:25:09<4:51:23, 17.66s/it]


 94%|█████████▍| 15122/16110 [25:25:44<4:53:00, 17.79s/it]
{'loss': 0.1114, 'learning_rate': 1.5958557309354017e-06, 'rewards/chosen': -5.183302879333496, 'rewards/rejected': -11.89268970489502, 'rewards/accuracies': 1.0, 'rewards/margins': 6.709386825561523, 'policy_logps/rejected': -452.6685485839844, 'policy_logps/chosen': -363.6221618652344, 'referece_logps/rejected': -333.7416076660156, 'referece_logps/chosen': -311.7891540527344, 'logits/rejected': 0.6661794781684875, 'logits/chosen': 0.6575620174407959, 'epoch': 8.45}

 94%|█████████▍| 15123/16110 [25:26:00<4:45:12, 17.34s/it]

 94%|█████████▍| 15124/16110 [25:26:19<4:49:04, 17.59s/it]

 94%|█████████▍| 15125/16110 [25:26:31<4:21:15, 15.91s/it]

 94%|█████████▍| 15126/16110 [25:26:44<4:06:53, 15.05s/it]

 94%|█████████▍| 15127/16110 [25:27:00<4:11:11, 15.33s/it]

 94%|█████████▍| 15128/16110 [25:27:17<4:19:43, 15.87s/it]

 94%|█████████▍| 15129/16110 [25:27:33<4:19:38, 15.88s/it]

 94%|█████████▍| 15130/16110 [25:27:46<4:05:13, 15.01s/it]

 94%|█████████▍| 15131/16110 [25:27:59<3:54:15, 14.36s/it]

 94%|█████████▍| 15132/16110 [25:28:13<3:56:33, 14.51s/it]

 94%|█████████▍| 15133/16110 [25:28:35<4:30:57, 16.64s/it]

 94%|█████████▍| 15134/16110 [25:28:57<4:58:07, 18.33s/it]


 94%|█████████▍| 15136/16110 [25:29:36<5:09:49, 19.09s/it]
{'loss': 0.0879, 'learning_rate': 1.5935929225093953e-06, 'rewards/chosen': -4.424635887145996, 'rewards/rejected': -9.276923179626465, 'rewards/accuracies': 1.0, 'rewards/margins': 4.852287292480469, 'policy_logps/rejected': -373.4156494140625, 'policy_logps/chosen': -404.4800720214844, 'referece_logps/rejected': -280.64642333984375, 'referece_logps/chosen': -360.23370361328125, 'logits/rejected': 0.48947522044181824, 'logits/chosen': 0.5096831917762756, 'epoch': 8.46}


 94%|█████████▍| 15138/16110 [25:30:16<5:16:32, 19.54s/it]
{'loss': 0.2082, 'learning_rate': 1.5932692798491816e-06, 'rewards/chosen': -3.5633599758148193, 'rewards/rejected': -8.260119438171387, 'rewards/accuracies': 1.0, 'rewards/margins': 4.696759223937988, 'policy_logps/rejected': -385.5754089355469, 'policy_logps/chosen': -336.6449279785156, 'referece_logps/rejected': -302.9742126464844, 'referece_logps/chosen': -301.0113220214844, 'logits/rejected': 0.3354209065437317, 'logits/chosen': 0.36801111698150635, 'epoch': 8.46}

 94%|█████████▍| 15139/16110 [25:30:36<5:17:13, 19.60s/it]

 94%|█████████▍| 15140/16110 [25:30:56<5:18:44, 19.72s/it]

 94%|█████████▍| 15141/16110 [25:31:13<5:03:57, 18.82s/it]

 94%|█████████▍| 15142/16110 [25:31:33<5:09:03, 19.16s/it]

 94%|█████████▍| 15143/16110 [25:31:52<5:07:12, 19.06s/it]

 94%|█████████▍| 15144/16110 [25:32:03<4:31:26, 16.86s/it]

 94%|█████████▍| 15145/16110 [25:32:24<4:48:08, 17.92s/it]

 94%|█████████▍| 15146/16110 [25:32:40<4:38:31, 17.34s/it]

 94%|█████████▍| 15147/16110 [25:32:56<4:31:21, 16.91s/it]

 94%|█████████▍| 15148/16110 [25:33:13<4:35:40, 17.19s/it]

 94%|█████████▍| 15149/16110 [25:33:30<4:33:25, 17.07s/it]

 94%|█████████▍| 15150/16110 [25:33:43<4:14:40, 15.92s/it]

 94%|█████████▍| 15151/16110 [25:33:55<3:54:15, 14.66s/it]

 94%|█████████▍| 15152/16110 [25:34:09<3:50:20, 14.43s/it]

 94%|█████████▍| 15153/16110 [25:34:29<4:16:47, 16.10s/it]

 94%|█████████▍| 15154/16110 [25:34:48<4:30:30, 16.98s/it]

 94%|█████████▍| 15155/16110 [25:35:06<4:36:26, 17.37s/it]

 94%|█████████▍| 15156/16110 [25:35:26<4:46:33, 18.02s/it]

 94%|█████████▍| 15157/16110 [25:35:39<4:23:12, 16.57s/it]

 94%|█████████▍| 15158/16110 [25:36:00<4:43:16, 17.85s/it]

 94%|█████████▍| 15159/16110 [25:36:20<4:51:52, 18.41s/it]

 94%|█████████▍| 15160/16110 [25:36:41<5:07:14, 19.41s/it]

 94%|█████████▍| 15161/16110 [25:37:02<5:12:20, 19.75s/it]

 94%|█████████▍| 15162/16110 [25:37:16<4:44:53, 18.03s/it]

 94%|█████████▍| 15163/16110 [25:37:34<4:43:07, 17.94s/it]

 94%|█████████▍| 15164/16110 [25:37:51<4:39:43, 17.74s/it]

 94%|█████████▍| 15165/16110 [25:38:13<4:59:01, 18.99s/it]

 94%|█████████▍| 15166/16110 [25:38:33<5:01:56, 19.19s/it]

 94%|█████████▍| 15167/16110 [25:38:54<5:13:04, 19.92s/it]

 94%|█████████▍| 15168/16110 [25:39:15<5:15:00, 20.06s/it]


 94%|█████████▍| 15170/16110 [25:39:52<5:06:42, 19.58s/it]

 94%|█████████▍| 15171/16110 [25:40:11<5:01:00, 19.23s/it]

 94%|█████████▍| 15172/16110 [25:40:28<4:52:05, 18.68s/it]

 94%|█████████▍| 15173/16110 [25:40:48<4:56:30, 18.99s/it]

 94%|█████████▍| 15174/16110 [25:41:02<4:31:14, 17.39s/it]
{'loss': 0.1146, 'learning_rate': 1.5874273601695501e-06, 'rewards/chosen': -4.703689098358154, 'rewards/rejected': -10.945783615112305, 'rewards/accuracies': 1.0, 'rewards/margins': 6.242094993591309, 'policy_logps/rejected': -350.373779296875, 'policy_logps/chosen': -474.9678955078125, 'referece_logps/rejected': -240.91595458984375, 'referece_logps/chosen': -427.9310302734375, 'logits/rejected': -0.3881531357765198, 'logits/chosen': -0.6387032866477966, 'epoch': 8.48}


 94%|█████████▍| 15176/16110 [25:41:26<3:50:52, 14.83s/it]

 94%|█████████▍| 15177/16110 [25:41:39<3:40:35, 14.19s/it]

 94%|█████████▍| 15178/16110 [25:41:53<3:38:50, 14.09s/it]

 94%|█████████▍| 15179/16110 [25:42:12<4:04:20, 15.75s/it]
{'loss': 0.1489, 'learning_rate': 1.5866135416952803e-06, 'rewards/chosen': -5.038763999938965, 'rewards/rejected': -10.822277069091797, 'rewards/accuracies': 0.875, 'rewards/margins': 5.78351354598999, 'policy_logps/rejected': -500.735107421875, 'policy_logps/chosen': -414.7099304199219, 'referece_logps/rejected': -392.5123291015625, 'referece_logps/chosen': -364.3222961425781, 'logits/rejected': 0.4135291278362274, 'logits/chosen': 0.35686546564102173, 'epoch': 8.48}


 94%|█████████▍| 15181/16110 [25:42:50<4:28:26, 17.34s/it]

 94%|█████████▍| 15182/16110 [25:43:03<4:05:16, 15.86s/it]
{'loss': 0.145, 'learning_rate': 1.5861249660219454e-06, 'rewards/chosen': -6.028136730194092, 'rewards/rejected': -11.141626358032227, 'rewards/accuracies': 1.0, 'rewards/margins': 5.113489627838135, 'policy_logps/rejected': -574.0543823242188, 'policy_logps/chosen': -496.7290344238281, 'referece_logps/rejected': -462.6381530761719, 'referece_logps/chosen': -436.44769287109375, 'logits/rejected': -0.5491288304328918, 'logits/chosen': -0.6902422308921814, 'epoch': 8.48}


 94%|█████████▍| 15184/16110 [25:43:36<4:16:07, 16.60s/it]

 94%|█████████▍| 15185/16110 [25:43:52<4:14:35, 16.51s/it]

 94%|█████████▍| 15186/16110 [25:44:13<4:33:42, 17.77s/it]

 94%|█████████▍| 15187/16110 [25:44:33<4:42:25, 18.36s/it]

 94%|█████████▍| 15188/16110 [25:44:52<4:48:23, 18.77s/it]

 94%|█████████▍| 15189/16110 [25:45:04<4:15:54, 16.67s/it]

 94%|█████████▍| 15190/16110 [25:45:16<3:54:04, 15.27s/it]

 94%|█████████▍| 15191/16110 [25:45:32<3:57:57, 15.54s/it]

 94%|█████████▍| 15192/16110 [25:45:45<3:46:32, 14.81s/it]
{'loss': 0.1235, 'learning_rate': 1.5844948414894223e-06, 'rewards/chosen': -4.817533493041992, 'rewards/rejected': -11.531362533569336, 'rewards/accuracies': 1.0, 'rewards/margins': 6.713829517364502, 'policy_logps/rejected': -416.6429138183594, 'policy_logps/chosen': -345.12811279296875, 'referece_logps/rejected': -301.3293151855469, 'referece_logps/chosen': -296.9527893066406, 'logits/rejected': -0.7027439475059509, 'logits/chosen': -0.553402304649353, 'epoch': 8.49}


 94%|█████████▍| 15194/16110 [25:46:21<4:01:05, 15.79s/it]

 94%|█████████▍| 15195/16110 [25:46:43<4:29:30, 17.67s/it]

 94%|█████████▍| 15196/16110 [25:47:02<4:38:33, 18.29s/it]

 94%|█████████▍| 15197/16110 [25:47:23<4:48:26, 18.96s/it]

 94%|█████████▍| 15198/16110 [25:47:37<4:27:23, 17.59s/it]

 94%|█████████▍| 15199/16110 [25:47:57<4:38:19, 18.33s/it]

 94%|█████████▍| 15200/16110 [25:48:08<4:04:46, 16.14s/it]
{'loss': 0.0925, 'learning_rate': 1.5831890404965762e-06, 'rewards/chosen': -5.259335041046143, 'rewards/rejected': -9.381651878356934, 'rewards/accuracies': 1.0, 'rewards/margins': 4.122316360473633, 'policy_logps/rejected': -265.76324462890625, 'policy_logps/chosen': -265.0787353515625, 'referece_logps/rejected': -171.94671630859375, 'referece_logps/chosen': -212.48536682128906, 'logits/rejected': -0.049935318529605865, 'logits/chosen': -0.04641357809305191, 'epoch': 8.49}


 94%|█████████▍| 15202/16110 [25:48:44<4:13:15, 16.73s/it]
{'loss': 0.0835, 'learning_rate': 1.5828623543871344e-06, 'rewards/chosen': -4.524410247802734, 'rewards/rejected': -11.475971221923828, 'rewards/accuracies': 1.0, 'rewards/margins': 6.95156192779541, 'policy_logps/rejected': -426.7354431152344, 'policy_logps/chosen': -538.6969604492188, 'referece_logps/rejected': -311.9757080078125, 'referece_logps/chosen': -493.4528503417969, 'logits/rejected': 0.4924468696117401, 'logits/chosen': 0.2175300121307373, 'epoch': 8.49}


 94%|█████████▍| 15204/16110 [25:49:19<4:13:14, 16.77s/it]

 94%|█████████▍| 15205/16110 [25:49:31<3:50:34, 15.29s/it]

 94%|█████████▍| 15206/16110 [25:49:51<4:14:53, 16.92s/it]

 94%|█████████▍| 15207/16110 [25:50:11<4:26:42, 17.72s/it]
{'loss': 0.0867, 'learning_rate': 1.5820452269340279e-06, 'rewards/chosen': -4.201900005340576, 'rewards/rejected': -8.537873268127441, 'rewards/accuracies': 0.875, 'rewards/margins': 4.335972785949707, 'policy_logps/rejected': -576.342041015625, 'policy_logps/chosen': -396.4620056152344, 'referece_logps/rejected': -490.9633483886719, 'referece_logps/chosen': -354.4429626464844, 'logits/rejected': -0.6448402404785156, 'logits/chosen': -0.5998601317405701, 'epoch': 8.5}

 94%|█████████▍| 15208/16110 [25:50:22<3:56:21, 15.72s/it]

 94%|█████████▍| 15209/16110 [25:50:36<3:47:56, 15.18s/it]


 94%|█████████▍| 15211/16110 [25:51:06<3:40:20, 14.71s/it]

 94%|█████████▍| 15212/16110 [25:51:23<3:53:15, 15.59s/it]

 94%|█████████▍| 15213/16110 [25:51:45<4:22:46, 17.58s/it]

 94%|█████████▍| 15214/16110 [25:52:04<4:26:13, 17.83s/it]

 94%|█████████▍| 15215/16110 [25:52:23<4:34:01, 18.37s/it]

 94%|█████████▍| 15216/16110 [25:52:42<4:36:09, 18.53s/it]

 94%|█████████▍| 15217/16110 [25:52:59<4:25:58, 17.87s/it]
{'loss': 0.0762, 'learning_rate': 1.5804092083524992e-06, 'rewards/chosen': -4.474214553833008, 'rewards/rejected': -9.516027450561523, 'rewards/accuracies': 1.0, 'rewards/margins': 5.041812419891357, 'policy_logps/rejected': -406.5550537109375, 'policy_logps/chosen': -490.69097900390625, 'referece_logps/rejected': -311.394775390625, 'referece_logps/chosen': -445.9488525390625, 'logits/rejected': -0.04461289942264557, 'logits/chosen': -0.3023594617843628, 'epoch': 8.5}

 94%|█████████▍| 15218/16110 [25:53:18<4:33:05, 18.37s/it]


 94%|█████████▍| 15220/16110 [25:53:53<4:24:33, 17.84s/it]

 94%|█████████▍| 15221/16110 [25:54:11<4:25:04, 17.89s/it]

 94%|█████████▍| 15222/16110 [25:54:31<4:35:15, 18.60s/it]

 94%|█████████▍| 15223/16110 [25:54:51<4:41:03, 19.01s/it]

 95%|█████████▍| 15224/16110 [25:55:09<4:33:04, 18.49s/it]

 95%|█████████▍| 15225/16110 [25:55:23<4:13:35, 17.19s/it]

 95%|█████████▍| 15226/16110 [25:55:43<4:26:06, 18.06s/it]

 95%|█████████▍| 15227/16110 [25:56:05<4:44:54, 19.36s/it]

 95%|█████████▍| 15228/16110 [25:56:26<4:48:33, 19.63s/it]

 95%|█████████▍| 15229/16110 [25:56:46<4:50:22, 19.78s/it]

 95%|█████████▍| 15230/16110 [25:57:03<4:40:16, 19.11s/it]

 95%|█████████▍| 15231/16110 [25:57:17<4:17:16, 17.56s/it]

 95%|█████████▍| 15232/16110 [25:57:32<4:05:02, 16.74s/it]

 95%|█████████▍| 15233/16110 [25:57:43<3:38:56, 14.98s/it]

 95%|█████████▍| 15234/16110 [25:57:58<3:39:22, 15.03s/it]

 95%|█████████▍| 15235/16110 [25:58:19<4:07:00, 16.94s/it]

 95%|█████████▍| 15236/16110 [25:58:43<4:36:02, 18.95s/it]

 95%|█████████▍| 15237/16110 [25:59:01<4:31:14, 18.64s/it]

 95%|█████████▍| 15238/16110 [25:59:16<4:15:14, 17.56s/it]
{'loss': 0.0978, 'learning_rate': 1.5769659408252416e-06, 'rewards/chosen': -4.148556709289551, 'rewards/rejected': -10.781111717224121, 'rewards/accuracies': 1.0, 'rewards/margins': 6.632554531097412, 'policy_logps/rejected': -388.7569580078125, 'policy_logps/chosen': -524.8394775390625, 'referece_logps/rejected': -280.94586181640625, 'referece_logps/chosen': -483.3538513183594, 'logits/rejected': 0.013285130262374878, 'logits/chosen': -0.39465823769569397, 'epoch': 8.51}


 95%|█████████▍| 15240/16110 [25:59:55<4:26:53, 18.41s/it]

 95%|█████████▍| 15241/16110 [26:00:15<4:35:34, 19.03s/it]

 95%|█████████▍| 15242/16110 [26:00:36<4:40:27, 19.39s/it]

 95%|█████████▍| 15243/16110 [26:00:56<4:43:44, 19.64s/it]

 95%|█████████▍| 15244/16110 [26:01:18<4:52:56, 20.30s/it]

 95%|█████████▍| 15245/16110 [26:01:36<4:44:01, 19.70s/it]

 95%|█████████▍| 15246/16110 [26:01:52<4:27:37, 18.59s/it]

 95%|█████████▍| 15247/16110 [26:02:12<4:32:53, 18.97s/it]

 95%|█████████▍| 15248/16110 [26:02:27<4:18:02, 17.96s/it]
{'loss': 0.056, 'learning_rate': 1.5753226710392968e-06, 'rewards/chosen': -4.718392372131348, 'rewards/rejected': -11.407365798950195, 'rewards/accuracies': 0.875, 'rewards/margins': 6.688973426818848, 'policy_logps/rejected': -421.600830078125, 'policy_logps/chosen': -393.8450012207031, 'referece_logps/rejected': -307.5272216796875, 'referece_logps/chosen': -346.6611022949219, 'logits/rejected': 0.006546779535710812, 'logits/chosen': 0.05140572041273117, 'epoch': 8.52}

 95%|█████████▍| 15249/16110 [26:02:45<4:15:20, 17.79s/it]

 95%|█████████▍| 15250/16110 [26:03:07<4:33:56, 19.11s/it]


 95%|█████████▍| 15252/16110 [26:03:48<4:44:27, 19.89s/it]

 95%|█████████▍| 15253/16110 [26:04:01<4:16:31, 17.96s/it]

 95%|█████████▍| 15254/16110 [26:04:14<3:51:06, 16.20s/it]
{'loss': 0.1356, 'learning_rate': 1.5743355925045416e-06, 'rewards/chosen': -4.330681324005127, 'rewards/rejected': -10.340483665466309, 'rewards/accuracies': 1.0, 'rewards/margins': 6.00980281829834, 'policy_logps/rejected': -512.218505859375, 'policy_logps/chosen': -572.9818115234375, 'referece_logps/rejected': -408.8136901855469, 'referece_logps/chosen': -529.6749877929688, 'logits/rejected': -0.3225337564945221, 'logits/chosen': -0.3243798613548279, 'epoch': 8.52}


 95%|█████████▍| 15256/16110 [26:04:48<4:04:25, 17.17s/it]

 95%|█████████▍| 15257/16110 [26:05:04<3:57:57, 16.74s/it]

 95%|█████████▍| 15258/16110 [26:05:21<3:56:55, 16.69s/it]

 95%|█████████▍| 15259/16110 [26:05:39<4:05:07, 17.28s/it]

 95%|█████████▍| 15260/16110 [26:05:57<4:08:07, 17.51s/it]

 95%|█████████▍| 15261/16110 [26:06:16<4:12:30, 17.85s/it]

 95%|█████████▍| 15262/16110 [26:06:35<4:19:00, 18.33s/it]

 95%|█████████▍| 15263/16110 [26:06:53<4:13:26, 17.95s/it]

 95%|█████████▍| 15264/16110 [26:07:12<4:20:43, 18.49s/it]

 95%|█████████▍| 15265/16110 [26:07:26<4:01:00, 17.11s/it]

 95%|█████████▍| 15266/16110 [26:07:46<4:12:55, 17.98s/it]

 95%|█████████▍| 15267/16110 [26:08:06<4:19:59, 18.50s/it]
{'loss': 0.1483, 'learning_rate': 1.5721940572040935e-06, 'rewards/chosen': -5.348867416381836, 'rewards/rejected': -10.2232027053833, 'rewards/accuracies': 1.0, 'rewards/margins': 4.874335289001465, 'policy_logps/rejected': -387.2395324707031, 'policy_logps/chosen': -502.20989990234375, 'referece_logps/rejected': -285.00750732421875, 'referece_logps/chosen': -448.72125244140625, 'logits/rejected': 0.2657201886177063, 'logits/chosen': 0.03797642141580582, 'epoch': 8.53}

 95%|█████████▍| 15268/16110 [26:08:25<4:23:27, 18.77s/it]


 95%|█████████▍| 15270/16110 [26:08:55<3:57:37, 16.97s/it]

 95%|█████████▍| 15271/16110 [26:09:12<3:59:56, 17.16s/it]

 95%|█████████▍| 15272/16110 [26:09:32<4:10:23, 17.93s/it]

 95%|█████████▍| 15273/16110 [26:09:49<4:04:15, 17.51s/it]

 95%|█████████▍| 15274/16110 [26:10:02<3:48:19, 16.39s/it]
{'loss': 0.0957, 'learning_rate': 1.5710393028842464e-06, 'rewards/chosen': -5.467198371887207, 'rewards/rejected': -11.57841682434082, 'rewards/accuracies': 0.875, 'rewards/margins': 6.111218452453613, 'policy_logps/rejected': -427.11273193359375, 'policy_logps/chosen': -294.2398376464844, 'referece_logps/rejected': -311.3285217285156, 'referece_logps/chosen': -239.56787109375, 'logits/rejected': -0.3777233958244324, 'logits/chosen': -0.2439354509115219, 'epoch': 8.53}

 95%|█████████▍| 15275/16110 [26:10:24<4:08:58, 17.89s/it]

 95%|█████████▍| 15276/16110 [26:10:37<3:50:47, 16.60s/it]


 95%|█████████▍| 15278/16110 [26:11:16<4:09:54, 18.02s/it]

 95%|█████████▍| 15279/16110 [26:11:36<4:18:34, 18.67s/it]

 95%|█████████▍| 15280/16110 [26:11:57<4:24:20, 19.11s/it]

 95%|█████████▍| 15281/16110 [26:12:16<4:26:56, 19.32s/it]
{'loss': 0.0753, 'learning_rate': 1.5698834175552605e-06, 'rewards/chosen': -3.7328603267669678, 'rewards/rejected': -8.626922607421875, 'rewards/accuracies': 1.0, 'rewards/margins': 4.894062042236328, 'policy_logps/rejected': -425.2269287109375, 'policy_logps/chosen': -377.1952209472656, 'referece_logps/rejected': -338.95770263671875, 'referece_logps/chosen': -339.8666076660156, 'logits/rejected': 0.4547755718231201, 'logits/chosen': 0.43777957558631897, 'epoch': 8.54}


 95%|█████████▍| 15283/16110 [26:12:46<3:53:08, 16.92s/it]
{'loss': 0.0761, 'learning_rate': 1.5695529571880709e-06, 'rewards/chosen': -5.974112033843994, 'rewards/rejected': -9.96931266784668, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9952006340026855, 'policy_logps/rejected': -304.0839538574219, 'policy_logps/chosen': -464.6640625, 'referece_logps/rejected': -204.39080810546875, 'referece_logps/chosen': -404.92291259765625, 'logits/rejected': 0.4648648202419281, 'logits/chosen': 0.37515175342559814, 'epoch': 8.54}

 95%|█████████▍| 15284/16110 [26:13:08<4:10:26, 18.19s/it]


 95%|█████████▍| 15286/16110 [26:13:47<4:21:30, 19.04s/it]

 95%|█████████▍| 15287/16110 [26:14:05<4:15:20, 18.61s/it]

 95%|█████████▍| 15288/16110 [26:14:21<4:02:59, 17.74s/it]

 95%|█████████▍| 15289/16110 [26:14:41<4:13:57, 18.56s/it]

 95%|█████████▍| 15290/16110 [26:14:53<3:46:13, 16.55s/it]

 95%|█████████▍| 15291/16110 [26:15:09<3:42:31, 16.30s/it]

 95%|█████████▍| 15292/16110 [26:15:25<3:40:30, 16.17s/it]
{'loss': 0.2216, 'learning_rate': 1.5680647467311555e-06, 'rewards/chosen': -4.818600654602051, 'rewards/rejected': -7.900592803955078, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0819919109344482, 'policy_logps/rejected': -402.9040222167969, 'policy_logps/chosen': -364.05767822265625, 'referece_logps/rejected': -323.8980712890625, 'referece_logps/chosen': -315.87164306640625, 'logits/rejected': 0.3804990351200104, 'logits/chosen': 0.4206116199493408, 'epoch': 8.54}

 95%|█████████▍| 15293/16110 [26:15:42<3:45:32, 16.56s/it]

 95%|█████████▍| 15294/16110 [26:16:02<3:59:08, 17.58s/it]

 95%|█████████▍| 15295/16110 [26:16:24<4:18:12, 19.01s/it]

 95%|█████████▍| 15296/16110 [26:16:44<4:21:38, 19.29s/it]


 95%|█████████▍| 15298/16110 [26:17:21<4:10:52, 18.54s/it]
{'loss': 0.1547, 'learning_rate': 1.5670715728542878e-06, 'rewards/chosen': -5.720338821411133, 'rewards/rejected': -9.21251106262207, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4921727180480957, 'policy_logps/rejected': -369.2076721191406, 'policy_logps/chosen': -421.8143615722656, 'referece_logps/rejected': -277.08258056640625, 'referece_logps/chosen': -364.6109619140625, 'logits/rejected': -0.014079464599490166, 'logits/chosen': -0.1948992908000946, 'epoch': 8.55}


 95%|█████████▍| 15300/16110 [26:17:55<4:05:13, 18.17s/it]

 95%|█████████▍| 15301/16110 [26:18:11<3:57:39, 17.63s/it]

 95%|█████████▍| 15302/16110 [26:18:31<4:05:49, 18.25s/it]
{'loss': 0.1689, 'learning_rate': 1.5664089984182822e-06, 'rewards/chosen': -3.8913819789886475, 'rewards/rejected': -8.227775573730469, 'rewards/accuracies': 1.0, 'rewards/margins': 4.3363938331604, 'policy_logps/rejected': -482.68817138671875, 'policy_logps/chosen': -381.0181884765625, 'referece_logps/rejected': -400.410400390625, 'referece_logps/chosen': -342.1044006347656, 'logits/rejected': 0.564108669757843, 'logits/chosen': 0.6813613176345825, 'epoch': 8.55}


 95%|█████████▍| 15304/16110 [26:19:05<3:53:44, 17.40s/it]
{'loss': 0.1079, 'learning_rate': 1.5660775738055199e-06, 'rewards/chosen': -5.048664093017578, 'rewards/rejected': -9.667144775390625, 'rewards/accuracies': 0.875, 'rewards/margins': 4.618480682373047, 'policy_logps/rejected': -419.62481689453125, 'policy_logps/chosen': -371.1432189941406, 'referece_logps/rejected': -322.953369140625, 'referece_logps/chosen': -320.6566162109375, 'logits/rejected': 0.31465235352516174, 'logits/chosen': 0.3071849048137665, 'epoch': 8.55}

 95%|█████████▌| 15305/16110 [26:19:24<3:59:24, 17.84s/it]

 95%|█████████▌| 15306/16110 [26:19:42<4:02:17, 18.08s/it]


 95%|█████████▌| 15308/16110 [26:20:19<4:04:42, 18.31s/it]

 95%|█████████▌| 15309/16110 [26:20:31<3:39:25, 16.44s/it]
{'loss': 0.1607, 'learning_rate': 1.5652486119687138e-06, 'rewards/chosen': -4.575113773345947, 'rewards/rejected': -7.889484405517578, 'rewards/accuracies': 1.0, 'rewards/margins': 3.314370632171631, 'policy_logps/rejected': -510.27490234375, 'policy_logps/chosen': -475.2408447265625, 'referece_logps/rejected': -431.38006591796875, 'referece_logps/chosen': -429.4896545410156, 'logits/rejected': 0.6955651044845581, 'logits/chosen': 0.5504945516586304, 'epoch': 8.55}


 95%|█████████▌| 15311/16110 [26:20:56<3:10:39, 14.32s/it]

 95%|█████████▌| 15312/16110 [26:21:15<3:31:46, 15.92s/it]
{'loss': 0.2088, 'learning_rate': 1.5647509606399095e-06, 'rewards/chosen': -4.303179740905762, 'rewards/rejected': -8.489702224731445, 'rewards/accuracies': 0.75, 'rewards/margins': 4.186521530151367, 'policy_logps/rejected': -371.95794677734375, 'policy_logps/chosen': -437.0975646972656, 'referece_logps/rejected': -287.0608825683594, 'referece_logps/chosen': -394.06573486328125, 'logits/rejected': -0.1162073016166687, 'logits/chosen': -0.5535311102867126, 'epoch': 8.55}

 95%|█████████▌| 15313/16110 [26:21:28<3:18:37, 14.95s/it]


 95%|█████████▌| 15315/16110 [26:22:03<3:42:38, 16.80s/it]

 95%|█████████▌| 15316/16110 [26:22:25<4:00:38, 18.18s/it]

 95%|█████████▌| 15317/16110 [26:22:42<3:55:47, 17.84s/it]
{'loss': 0.1158, 'learning_rate': 1.5639210852951456e-06, 'rewards/chosen': -3.7213425636291504, 'rewards/rejected': -8.516939163208008, 'rewards/accuracies': 1.0, 'rewards/margins': 4.795596122741699, 'policy_logps/rejected': -268.9004821777344, 'policy_logps/chosen': -432.22076416015625, 'referece_logps/rejected': -183.7310791015625, 'referece_logps/chosen': -395.00738525390625, 'logits/rejected': 0.7109960317611694, 'logits/chosen': 0.6609675884246826, 'epoch': 8.56}


 95%|█████████▌| 15319/16110 [26:23:19<3:59:12, 18.15s/it]

 95%|█████████▌| 15320/16110 [26:23:39<4:05:35, 18.65s/it]

 95%|█████████▌| 15321/16110 [26:23:58<4:05:39, 18.68s/it]

 95%|█████████▌| 15322/16110 [26:24:11<3:43:33, 17.02s/it]
{'loss': 0.1247, 'learning_rate': 1.5630906400979435e-06, 'rewards/chosen': -4.409260272979736, 'rewards/rejected': -8.332322120666504, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9230616092681885, 'policy_logps/rejected': -346.0394592285156, 'policy_logps/chosen': -309.6282958984375, 'referece_logps/rejected': -262.7162170410156, 'referece_logps/chosen': -265.5356750488281, 'logits/rejected': -0.43633583188056946, 'logits/chosen': -0.43007713556289673, 'epoch': 8.56}

 95%|█████████▌| 15323/16110 [26:24:31<3:53:09, 17.78s/it]

 95%|█████████▌| 15324/16110 [26:24:50<4:00:12, 18.34s/it]

 95%|█████████▌| 15325/16110 [26:25:07<3:52:52, 17.80s/it]

 95%|█████████▌| 15326/16110 [26:25:19<3:30:01, 16.07s/it]

 95%|█████████▌| 15327/16110 [26:25:31<3:13:29, 14.83s/it]

 95%|█████████▌| 15328/16110 [26:25:52<3:38:36, 16.77s/it]


 95%|█████████▌| 15330/16110 [26:26:25<3:33:24, 16.42s/it]
{'loss': 0.1016, 'learning_rate': 1.5617607445841568e-06, 'rewards/chosen': -3.0569522380828857, 'rewards/rejected': -7.101008892059326, 'rewards/accuracies': 1.0, 'rewards/margins': 4.0440568923950195, 'policy_logps/rejected': -286.9842834472656, 'policy_logps/chosen': -282.1120300292969, 'referece_logps/rejected': -215.97418212890625, 'referece_logps/chosen': -251.5425262451172, 'logits/rejected': 0.00928308442234993, 'logits/chosen': 0.017097927629947662, 'epoch': 8.56}

 95%|█████████▌| 15331/16110 [26:26:36<3:10:32, 14.68s/it]

 95%|█████████▌| 15332/16110 [26:26:55<3:26:27, 15.92s/it]

 95%|█████████▌| 15333/16110 [26:27:16<3:47:40, 17.58s/it]

 95%|█████████▌| 15334/16110 [26:27:37<3:57:50, 18.39s/it]


 95%|█████████▌| 15336/16110 [26:28:16<4:04:33, 18.96s/it]
{'loss': 0.1348, 'learning_rate': 1.5607623690761069e-06, 'rewards/chosen': -5.785983562469482, 'rewards/rejected': -10.11800765991211, 'rewards/accuracies': 1.0, 'rewards/margins': 4.332023620605469, 'policy_logps/rejected': -618.2390747070312, 'policy_logps/chosen': -487.35400390625, 'referece_logps/rejected': -517.0590209960938, 'referece_logps/chosen': -429.494140625, 'logits/rejected': -0.42064177989959717, 'logits/chosen': -0.3541417419910431, 'epoch': 8.57}


 95%|█████████▌| 15338/16110 [26:28:50<3:53:13, 18.13s/it]
{'loss': 0.1461, 'learning_rate': 1.5604293958369608e-06, 'rewards/chosen': -5.410508632659912, 'rewards/rejected': -10.61507797241211, 'rewards/accuracies': 1.0, 'rewards/margins': 5.204568862915039, 'policy_logps/rejected': -506.30810546875, 'policy_logps/chosen': -512.1265869140625, 'referece_logps/rejected': -400.1573486328125, 'referece_logps/chosen': -458.0215148925781, 'logits/rejected': -0.41903138160705566, 'logits/chosen': -0.31368082761764526, 'epoch': 8.57}


 95%|█████████▌| 15340/16110 [26:29:14<3:15:14, 15.21s/it]

 95%|█████████▌| 15341/16110 [26:29:28<3:09:44, 14.80s/it]

 95%|█████████▌| 15342/16110 [26:29:48<3:28:51, 16.32s/it]

 95%|█████████▌| 15343/16110 [26:30:02<3:20:33, 15.69s/it]
{'loss': 0.0921, 'learning_rate': 1.559596566430054e-06, 'rewards/chosen': -4.476593494415283, 'rewards/rejected': -11.972597122192383, 'rewards/accuracies': 1.0, 'rewards/margins': 7.496003150939941, 'policy_logps/rejected': -532.03564453125, 'policy_logps/chosen': -540.3949584960938, 'referece_logps/rejected': -412.3096618652344, 'referece_logps/chosen': -495.6290283203125, 'logits/rejected': 0.92041015625, 'logits/chosen': 0.9440062046051025, 'epoch': 8.57}


 95%|█████████▌| 15345/16110 [26:30:44<3:54:29, 18.39s/it]

 95%|█████████▌| 15346/16110 [26:31:02<3:52:56, 18.29s/it]

 95%|█████████▌| 15347/16110 [26:31:17<3:40:57, 17.38s/it]
{'loss': 0.11, 'learning_rate': 1.5589298957167605e-06, 'rewards/chosen': -6.360925674438477, 'rewards/rejected': -10.640748023986816, 'rewards/accuracies': 1.0, 'rewards/margins': 4.279821872711182, 'policy_logps/rejected': -388.08544921875, 'policy_logps/chosen': -450.3936767578125, 'referece_logps/rejected': -281.677978515625, 'referece_logps/chosen': -386.78436279296875, 'logits/rejected': 0.4541405737400055, 'logits/chosen': 0.3496530055999756, 'epoch': 8.57}

 95%|█████████▌| 15348/16110 [26:31:37<3:48:13, 17.97s/it]


 95%|█████████▌| 15350/16110 [26:32:14<3:54:41, 18.53s/it]
{'loss': 0.1758, 'learning_rate': 1.55842965543852e-06, 'rewards/chosen': -4.667935371398926, 'rewards/rejected': -10.573759078979492, 'rewards/accuracies': 1.0, 'rewards/margins': 5.90582275390625, 'policy_logps/rejected': -452.650634765625, 'policy_logps/chosen': -520.82177734375, 'referece_logps/rejected': -346.91302490234375, 'referece_logps/chosen': -474.1424255371094, 'logits/rejected': 0.017245814204216003, 'logits/chosen': -0.11228173226118088, 'epoch': 8.58}


 95%|█████████▌| 15352/16110 [26:32:51<3:53:08, 18.45s/it]

 95%|█████████▌| 15353/16110 [26:33:08<3:46:31, 17.95s/it]

 95%|█████████▌| 15354/16110 [26:33:28<3:52:11, 18.43s/it]
{'loss': 0.1575, 'learning_rate': 1.5577623524224936e-06, 'rewards/chosen': -5.579301834106445, 'rewards/rejected': -9.148569107055664, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5692672729492188, 'policy_logps/rejected': -362.3475341796875, 'policy_logps/chosen': -417.43719482421875, 'referece_logps/rejected': -270.86187744140625, 'referece_logps/chosen': -361.6441650390625, 'logits/rejected': -0.018900245428085327, 'logits/chosen': -0.1535353660583496, 'epoch': 8.58}

 95%|█████████▌| 15355/16110 [26:33:47<3:54:49, 18.66s/it]

 95%|█████████▌| 15356/16110 [26:34:07<3:59:11, 19.03s/it]


 95%|█████████▌| 15358/16110 [26:34:48<4:09:03, 19.87s/it]

 95%|█████████▌| 15359/16110 [26:35:06<4:00:06, 19.18s/it]
{'loss': 0.1271, 'learning_rate': 1.5569277164370066e-06, 'rewards/chosen': -5.063357353210449, 'rewards/rejected': -9.839980125427246, 'rewards/accuracies': 1.0, 'rewards/margins': 4.776623725891113, 'policy_logps/rejected': -397.7598876953125, 'policy_logps/chosen': -385.55621337890625, 'referece_logps/rejected': -299.360107421875, 'referece_logps/chosen': -334.922607421875, 'logits/rejected': 0.5075360536575317, 'logits/chosen': 0.5455679893493652, 'epoch': 8.58}


 95%|█████████▌| 15361/16110 [26:35:48<4:15:06, 20.44s/it]
{'loss': 0.1193, 'learning_rate': 1.5565937044156182e-06, 'rewards/chosen': -5.276935577392578, 'rewards/rejected': -10.048455238342285, 'rewards/accuracies': 1.0, 'rewards/margins': 4.771520137786865, 'policy_logps/rejected': -458.6291809082031, 'policy_logps/chosen': -365.3490905761719, 'referece_logps/rejected': -358.1446228027344, 'referece_logps/chosen': -312.57977294921875, 'logits/rejected': -0.09073519706726074, 'logits/chosen': -0.05377735197544098, 'epoch': 8.58}


 95%|█████████▌| 15363/16110 [26:36:24<3:56:10, 18.97s/it]
{'loss': 0.1269, 'learning_rate': 1.556259602402545e-06, 'rewards/chosen': -5.14959716796875, 'rewards/rejected': -9.989892959594727, 'rewards/accuracies': 1.0, 'rewards/margins': 4.84029483795166, 'policy_logps/rejected': -606.44775390625, 'policy_logps/chosen': -363.3907470703125, 'referece_logps/rejected': -506.5487976074219, 'referece_logps/chosen': -311.894775390625, 'logits/rejected': -0.6782773733139038, 'logits/chosen': -0.3668408989906311, 'epoch': 8.58}


 95%|█████████▌| 15365/16110 [26:37:06<4:08:10, 19.99s/it]

 95%|█████████▌| 15366/16110 [26:37:24<4:02:02, 19.52s/it]

 95%|█████████▌| 15367/16110 [26:37:44<4:02:11, 19.56s/it]

 95%|█████████▌| 15368/16110 [26:38:02<3:56:08, 19.10s/it]
{'loss': 0.1385, 'learning_rate': 1.5554239540107754e-06, 'rewards/chosen': -5.461807727813721, 'rewards/rejected': -8.494566917419434, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0327587127685547, 'policy_logps/rejected': -341.88140869140625, 'policy_logps/chosen': -324.1455078125, 'referece_logps/rejected': -256.9356994628906, 'referece_logps/chosen': -269.5274353027344, 'logits/rejected': 0.3293384909629822, 'logits/chosen': 0.38134393095970154, 'epoch': 8.59}


 95%|█████████▌| 15370/16110 [26:38:31<3:24:19, 16.57s/it]
{'loss': 0.11, 'learning_rate': 1.5550895374522989e-06, 'rewards/chosen': -4.972311496734619, 'rewards/rejected': -11.980868339538574, 'rewards/accuracies': 1.0, 'rewards/margins': 7.008556842803955, 'policy_logps/rejected': -407.2122802734375, 'policy_logps/chosen': -423.44952392578125, 'referece_logps/rejected': -287.40362548828125, 'referece_logps/chosen': -373.7264099121094, 'logits/rejected': 0.5767633318901062, 'logits/chosen': 0.6046040058135986, 'epoch': 8.59}


 95%|█████████▌| 15372/16110 [26:39:11<3:43:12, 18.15s/it]

 95%|█████████▌| 15373/16110 [26:39:30<3:47:34, 18.53s/it]

 95%|█████████▌| 15374/16110 [26:39:41<3:17:48, 16.13s/it]

 95%|█████████▌| 15375/16110 [26:40:03<3:39:31, 17.92s/it]
{'loss': 0.1798, 'learning_rate': 1.5542531035247919e-06, 'rewards/chosen': -5.136798858642578, 'rewards/rejected': -8.284242630004883, 'rewards/accuracies': 0.875, 'rewards/margins': 3.1474437713623047, 'policy_logps/rejected': -328.221435546875, 'policy_logps/chosen': -359.4276123046875, 'referece_logps/rejected': -245.37899780273438, 'referece_logps/chosen': -308.0596008300781, 'logits/rejected': 0.618781328201294, 'logits/chosen': 0.5581925511360168, 'epoch': 8.59}

 95%|█████████▌| 15376/16110 [26:40:24<3:50:20, 18.83s/it]


 95%|█████████▌| 15378/16110 [26:40:59<3:41:32, 18.16s/it]
{'loss': 0.0954, 'learning_rate': 1.5537509742744407e-06, 'rewards/chosen': -3.909227132797241, 'rewards/rejected': -9.419048309326172, 'rewards/accuracies': 1.0, 'rewards/margins': 5.509820938110352, 'policy_logps/rejected': -375.7565612792969, 'policy_logps/chosen': -319.5798034667969, 'referece_logps/rejected': -281.5660705566406, 'referece_logps/chosen': -280.487548828125, 'logits/rejected': 0.5616222620010376, 'logits/chosen': 0.3474385440349579, 'epoch': 8.59}

 95%|█████████▌| 15379/16110 [26:41:16<3:37:26, 17.85s/it]

 95%|█████████▌| 15380/16110 [26:41:37<3:50:06, 18.91s/it]


 95%|█████████▌| 15382/16110 [26:42:17<3:51:59, 19.12s/it]

 95%|█████████▌| 15383/16110 [26:42:37<3:54:25, 19.35s/it]
{'loss': 0.0883, 'learning_rate': 1.5529136446204511e-06, 'rewards/chosen': -4.940982818603516, 'rewards/rejected': -10.289984703063965, 'rewards/accuracies': 1.0, 'rewards/margins': 5.349001407623291, 'policy_logps/rejected': -333.1482238769531, 'policy_logps/chosen': -325.88824462890625, 'referece_logps/rejected': -230.24838256835938, 'referece_logps/chosen': -276.4783935546875, 'logits/rejected': -0.10378807038068771, 'logits/chosen': -0.22507870197296143, 'epoch': 8.59}

 95%|█████████▌| 15384/16110 [26:42:56<3:53:54, 19.33s/it]

 95%|█████████▌| 15385/16110 [26:43:16<3:57:04, 19.62s/it]

 96%|█████████▌| 15386/16110 [26:43:37<4:02:34, 20.10s/it]

 96%|█████████▌| 15387/16110 [26:43:53<3:47:44, 18.90s/it]

 96%|█████████▌| 15388/16110 [26:44:10<3:39:40, 18.26s/it]

 96%|█████████▌| 15389/16110 [26:44:30<3:45:03, 18.73s/it]

 96%|█████████▌| 15390/16110 [26:44:50<3:48:44, 19.06s/it]


 96%|█████████▌| 15392/16110 [26:45:27<3:43:22, 18.67s/it]
{'loss': 0.1299, 'learning_rate': 1.551405043814598e-06, 'rewards/chosen': -5.560021877288818, 'rewards/rejected': -14.865172386169434, 'rewards/accuracies': 1.0, 'rewards/margins': 9.30515193939209, 'policy_logps/rejected': -574.8209228515625, 'policy_logps/chosen': -458.72821044921875, 'referece_logps/rejected': -426.169189453125, 'referece_logps/chosen': -403.12799072265625, 'logits/rejected': 0.3917733132839203, 'logits/chosen': 0.42287972569465637, 'epoch': 8.6}

 96%|█████████▌| 15393/16110 [26:45:43<3:34:59, 17.99s/it]

 96%|█████████▌| 15394/16110 [26:46:04<3:45:05, 18.86s/it]


 96%|█████████▌| 15396/16110 [26:46:45<3:54:26, 19.70s/it]
{'loss': 0.1635, 'learning_rate': 1.550733974780903e-06, 'rewards/chosen': -4.847927093505859, 'rewards/rejected': -8.194306373596191, 'rewards/accuracies': 0.875, 'rewards/margins': 3.3463785648345947, 'policy_logps/rejected': -296.79638671875, 'policy_logps/chosen': -278.80975341796875, 'referece_logps/rejected': -214.8533172607422, 'referece_logps/chosen': -230.3304901123047, 'logits/rejected': 0.16820424795150757, 'logits/chosen': 0.15602841973304749, 'epoch': 8.6}

 96%|█████████▌| 15397/16110 [26:47:01<3:39:16, 18.45s/it]

 96%|█████████▌| 15398/16110 [26:47:20<3:43:27, 18.83s/it]

 96%|█████████▌| 15399/16110 [26:47:38<3:38:31, 18.44s/it]

 96%|█████████▌| 15400/16110 [26:47:53<3:25:26, 17.36s/it]


 96%|█████████▌| 15402/16110 [26:48:31<3:36:25, 18.34s/it]
{'loss': 0.0785, 'learning_rate': 1.5497267035340784e-06, 'rewards/chosen': -4.755128860473633, 'rewards/rejected': -9.820352554321289, 'rewards/accuracies': 0.875, 'rewards/margins': 5.065225124359131, 'policy_logps/rejected': -454.1416015625, 'policy_logps/chosen': -521.738525390625, 'referece_logps/rejected': -355.9380798339844, 'referece_logps/chosen': -474.1871643066406, 'logits/rejected': 0.2846686840057373, 'logits/chosen': 0.2678554654121399, 'epoch': 8.6}

 96%|█████████▌| 15403/16110 [26:48:47<3:25:21, 17.43s/it]

 96%|█████████▌| 15404/16110 [26:49:02<3:17:37, 16.79s/it]

 96%|█████████▌| 15405/16110 [26:49:19<3:18:12, 16.87s/it]


 96%|█████████▌| 15407/16110 [26:50:02<3:44:51, 19.19s/it]

 96%|█████████▌| 15408/16110 [26:50:15<3:25:56, 17.60s/it]

 96%|█████████▌| 15409/16110 [26:50:35<3:32:44, 18.21s/it]
{'loss': 0.1279, 'learning_rate': 1.5485505428231939e-06, 'rewards/chosen': -4.347707748413086, 'rewards/rejected': -10.179919242858887, 'rewards/accuracies': 0.875, 'rewards/margins': 5.832211971282959, 'policy_logps/rejected': -368.9080810546875, 'policy_logps/chosen': -366.1148986816406, 'referece_logps/rejected': -267.1089172363281, 'referece_logps/chosen': -322.6378479003906, 'logits/rejected': 0.23502039909362793, 'logits/chosen': 0.20331916213035583, 'epoch': 8.61}

 96%|█████████▌| 15410/16110 [26:50:52<3:27:19, 17.77s/it]

 96%|█████████▌| 15411/16110 [26:51:12<3:34:21, 18.40s/it]

 96%|█████████▌| 15412/16110 [26:51:31<3:36:48, 18.64s/it]


 96%|█████████▌| 15414/16110 [26:52:08<3:30:45, 18.17s/it]
{'loss': 0.076, 'learning_rate': 1.5477097627095683e-06, 'rewards/chosen': -5.045210361480713, 'rewards/rejected': -11.259142875671387, 'rewards/accuracies': 1.0, 'rewards/margins': 6.213932991027832, 'policy_logps/rejected': -442.7525634765625, 'policy_logps/chosen': -420.2720642089844, 'referece_logps/rejected': -330.1611022949219, 'referece_logps/chosen': -369.82000732421875, 'logits/rejected': 0.24686285853385925, 'logits/chosen': 0.14110107719898224, 'epoch': 8.61}

 96%|█████████▌| 15415/16110 [26:52:23<3:19:33, 17.23s/it]

 96%|█████████▌| 15416/16110 [26:52:42<3:26:39, 17.87s/it]

 96%|█████████▌| 15417/16110 [26:53:02<3:34:14, 18.55s/it]

 96%|█████████▌| 15418/16110 [26:53:19<3:27:11, 17.96s/it]

 96%|█████████▌| 15419/16110 [26:53:39<3:33:33, 18.54s/it]

 96%|█████████▌| 15420/16110 [26:53:59<3:39:43, 19.11s/it]

 96%|█████████▌| 15421/16110 [26:54:14<3:25:58, 17.94s/it]


 96%|█████████▌| 15423/16110 [26:54:44<3:06:05, 16.25s/it]
{'loss': 0.0653, 'learning_rate': 1.546194964330424e-06, 'rewards/chosen': -3.6965019702911377, 'rewards/rejected': -8.831467628479004, 'rewards/accuracies': 1.0, 'rewards/margins': 5.134965419769287, 'policy_logps/rejected': -456.961181640625, 'policy_logps/chosen': -315.32623291015625, 'referece_logps/rejected': -368.6465148925781, 'referece_logps/chosen': -278.3612060546875, 'logits/rejected': 0.46329736709594727, 'logits/chosen': 0.4077502489089966, 'epoch': 8.62}

 96%|█████████▌| 15424/16110 [26:55:01<3:08:45, 16.51s/it]


 96%|█████████▌| 15426/16110 [26:55:31<3:03:13, 16.07s/it]
{'loss': 0.1218, 'learning_rate': 1.5456896338956565e-06, 'rewards/chosen': -3.9830222129821777, 'rewards/rejected': -8.799093246459961, 'rewards/accuracies': 1.0, 'rewards/margins': 4.816071510314941, 'policy_logps/rejected': -519.3995971679688, 'policy_logps/chosen': -449.668212890625, 'referece_logps/rejected': -431.4086608886719, 'referece_logps/chosen': -409.8379821777344, 'logits/rejected': 0.01206209883093834, 'logits/chosen': -0.2824477255344391, 'epoch': 8.62}


 96%|█████████▌| 15428/16110 [26:56:04<3:02:26, 16.05s/it]
{'loss': 0.0702, 'learning_rate': 1.5453526366419425e-06, 'rewards/chosen': -5.740065574645996, 'rewards/rejected': -9.674920082092285, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9348535537719727, 'policy_logps/rejected': -488.0061950683594, 'policy_logps/chosen': -345.4183044433594, 'referece_logps/rejected': -391.2570495605469, 'referece_logps/chosen': -288.0176696777344, 'logits/rejected': -0.0630401000380516, 'logits/chosen': 0.09129573404788971, 'epoch': 8.62}

 96%|█████████▌| 15429/16110 [26:56:24<3:17:48, 17.43s/it]


 96%|█████████▌| 15431/16110 [26:56:57<3:15:53, 17.31s/it]
{'loss': 0.1382, 'learning_rate': 1.5448469754517854e-06, 'rewards/chosen': -6.098573207855225, 'rewards/rejected': -10.98481559753418, 'rewards/accuracies': 1.0, 'rewards/margins': 4.886242866516113, 'policy_logps/rejected': -399.1349182128906, 'policy_logps/chosen': -361.0668640136719, 'referece_logps/rejected': -289.2867736816406, 'referece_logps/chosen': -300.0811462402344, 'logits/rejected': 0.6992820501327515, 'logits/chosen': 0.6369144916534424, 'epoch': 8.62}


 96%|█████████▌| 15433/16110 [26:57:28<3:00:01, 15.95s/it]
{'loss': 0.0676, 'learning_rate': 1.5445097578647755e-06, 'rewards/chosen': -4.071038722991943, 'rewards/rejected': -9.94184684753418, 'rewards/accuracies': 1.0, 'rewards/margins': 5.8708086013793945, 'policy_logps/rejected': -309.6865234375, 'policy_logps/chosen': -339.8372802734375, 'referece_logps/rejected': -210.26803588867188, 'referece_logps/chosen': -299.12689208984375, 'logits/rejected': -0.3578451871871948, 'logits/chosen': -0.5448067784309387, 'epoch': 8.62}

 96%|█████████▌| 15434/16110 [26:57:43<2:56:24, 15.66s/it]

 96%|█████████▌| 15435/16110 [26:58:02<3:09:42, 16.86s/it]

 96%|█████████▌| 15436/16110 [26:58:15<2:54:30, 15.53s/it]


 96%|█████████▌| 15438/16110 [26:58:48<3:03:13, 16.36s/it]
{'loss': 0.1165, 'learning_rate': 1.5436663288506667e-06, 'rewards/chosen': -4.359121322631836, 'rewards/rejected': -11.019658088684082, 'rewards/accuracies': 1.0, 'rewards/margins': 6.660537242889404, 'policy_logps/rejected': -491.8396911621094, 'policy_logps/chosen': -533.0469970703125, 'referece_logps/rejected': -381.6430969238281, 'referece_logps/chosen': -489.45574951171875, 'logits/rejected': 0.39992669224739075, 'logits/chosen': 0.2562442421913147, 'epoch': 8.62}


 96%|█████████▌| 15440/16110 [26:59:20<2:57:29, 15.90s/it]

 96%|█████████▌| 15441/16110 [26:59:40<3:10:53, 17.12s/it]

 96%|█████████▌| 15442/16110 [26:59:54<3:00:39, 16.23s/it]

 96%|█████████▌| 15443/16110 [27:00:14<3:12:26, 17.31s/it]
{'loss': 0.0701, 'learning_rate': 1.542822350451917e-06, 'rewards/chosen': -3.516955852508545, 'rewards/rejected': -12.333182334899902, 'rewards/accuracies': 1.0, 'rewards/margins': 8.816226959228516, 'policy_logps/rejected': -434.31854248046875, 'policy_logps/chosen': -407.9858093261719, 'referece_logps/rejected': -310.9866943359375, 'referece_logps/chosen': -372.8162536621094, 'logits/rejected': -0.025726526975631714, 'logits/chosen': -0.21223323047161102, 'epoch': 8.63}

 96%|█████████▌| 15444/16110 [27:00:30<3:09:55, 17.11s/it]

 96%|█████████▌| 15445/16110 [27:00:49<3:12:52, 17.40s/it]

 96%|█████████▌| 15446/16110 [27:01:09<3:22:33, 18.30s/it]

 96%|█████████▌| 15447/16110 [27:01:27<3:20:23, 18.13s/it]


 96%|█████████▌| 15449/16110 [27:01:52<2:47:17, 15.19s/it]

 96%|█████████▌| 15450/16110 [27:02:12<3:04:25, 16.77s/it]
{'loss': 0.1048, 'learning_rate': 1.541639859351412e-06, 'rewards/chosen': -5.053223609924316, 'rewards/rejected': -10.762041091918945, 'rewards/accuracies': 1.0, 'rewards/margins': 5.708817481994629, 'policy_logps/rejected': -419.7015380859375, 'policy_logps/chosen': -403.825439453125, 'referece_logps/rejected': -312.0810852050781, 'referece_logps/chosen': -353.2931823730469, 'logits/rejected': 0.5928525328636169, 'logits/chosen': 0.7277638912200928, 'epoch': 8.63}

 96%|█████████▌| 15451/16110 [27:02:31<3:09:18, 17.24s/it]

 96%|█████████▌| 15452/16110 [27:02:43<2:51:12, 15.61s/it]

 96%|█████████▌| 15453/16110 [27:02:59<2:53:59, 15.89s/it]

 96%|█████████▌| 15454/16110 [27:03:19<3:08:25, 17.23s/it]


 96%|█████████▌| 15456/16110 [27:04:00<3:24:36, 18.77s/it]

 96%|█████████▌| 15457/16110 [27:04:12<3:01:33, 16.68s/it]
{'loss': 0.1541, 'learning_rate': 1.5404562954707537e-06, 'rewards/chosen': -5.1883721351623535, 'rewards/rejected': -8.943740844726562, 'rewards/accuracies': 0.875, 'rewards/margins': 3.755368232727051, 'policy_logps/rejected': -411.8939208984375, 'policy_logps/chosen': -443.8155517578125, 'referece_logps/rejected': -322.45648193359375, 'referece_logps/chosen': -391.9317932128906, 'logits/rejected': 0.3627176880836487, 'logits/chosen': 0.24822473526000977, 'epoch': 8.64}


 96%|█████████▌| 15459/16110 [27:04:49<3:09:18, 17.45s/it]

 96%|█████████▌| 15460/16110 [27:05:08<3:15:38, 18.06s/it]

 96%|█████████▌| 15461/16110 [27:05:26<3:16:13, 18.14s/it]

 96%|█████████▌| 15462/16110 [27:05:48<3:28:30, 19.31s/it]
{'loss': 0.1232, 'learning_rate': 1.5396102371931235e-06, 'rewards/chosen': -5.483688831329346, 'rewards/rejected': -11.263143539428711, 'rewards/accuracies': 1.0, 'rewards/margins': 5.779454708099365, 'policy_logps/rejected': -427.2976989746094, 'policy_logps/chosen': -397.2002868652344, 'referece_logps/rejected': -314.666259765625, 'referece_logps/chosen': -342.3634033203125, 'logits/rejected': -0.009400814771652222, 'logits/chosen': -0.19491609930992126, 'epoch': 8.64}

 96%|█████████▌| 15463/16110 [27:06:05<3:20:17, 18.57s/it]

 96%|█████████▌| 15464/16110 [27:06:25<3:23:52, 18.94s/it]

 96%|█████████▌| 15465/16110 [27:06:45<3:26:19, 19.19s/it]

 96%|█████████▌| 15466/16110 [27:07:06<3:31:00, 19.66s/it]

 96%|█████████▌| 15467/16110 [27:07:23<3:25:00, 19.13s/it]


 96%|█████████▌| 15469/16110 [27:08:04<3:31:49, 19.83s/it]
{'loss': 0.0558, 'learning_rate': 1.5384248397157648e-06, 'rewards/chosen': -4.683920860290527, 'rewards/rejected': -10.723138809204102, 'rewards/accuracies': 1.0, 'rewards/margins': 6.039217472076416, 'policy_logps/rejected': -332.1690673828125, 'policy_logps/chosen': -556.9612426757812, 'referece_logps/rejected': -224.9376678466797, 'referece_logps/chosen': -510.1220397949219, 'logits/rejected': 0.7692736983299255, 'logits/chosen': 0.30447322130203247, 'epoch': 8.64}

 96%|█████████▌| 15470/16110 [27:08:24<3:30:52, 19.77s/it]


 96%|█████████▌| 15472/16110 [27:08:58<3:16:23, 18.47s/it]

 96%|█████████▌| 15473/16110 [27:09:14<3:08:12, 17.73s/it]
{'loss': 0.18, 'learning_rate': 1.5377469907806066e-06, 'rewards/chosen': -4.125132083892822, 'rewards/rejected': -8.997291564941406, 'rewards/accuracies': 0.875, 'rewards/margins': 4.8721604347229, 'policy_logps/rejected': -551.2891845703125, 'policy_logps/chosen': -474.33575439453125, 'referece_logps/rejected': -461.3162841796875, 'referece_logps/chosen': -433.0844421386719, 'logits/rejected': 0.1598382443189621, 'logits/chosen': 0.12038590759038925, 'epoch': 8.64}

 96%|█████████▌| 15474/16110 [27:09:36<3:19:29, 18.82s/it]

 96%|█████████▌| 15475/16110 [27:09:57<3:27:51, 19.64s/it]

 96%|█████████▌| 15476/16110 [27:10:14<3:18:52, 18.82s/it]

 96%|█████████▌| 15477/16110 [27:10:36<3:27:52, 19.70s/it]

 96%|█████████▌| 15478/16110 [27:10:51<3:12:54, 18.31s/it]

 96%|█████████▌| 15479/16110 [27:11:12<3:20:54, 19.10s/it]

 96%|█████████▌| 15480/16110 [27:11:34<3:30:54, 20.09s/it]

 96%|█████████▌| 15481/16110 [27:11:46<3:02:33, 17.41s/it]

 96%|█████████▌| 15482/16110 [27:12:08<3:16:42, 18.79s/it]

 96%|█████████▌| 15483/16110 [27:12:29<3:24:11, 19.54s/it]

 96%|█████████▌| 15484/16110 [27:12:46<3:17:53, 18.97s/it]

 96%|█████████▌| 15485/16110 [27:13:08<3:24:46, 19.66s/it]

 96%|█████████▌| 15486/16110 [27:13:27<3:24:34, 19.67s/it]

 96%|█████████▌| 15487/16110 [27:13:40<3:02:14, 17.55s/it]

 96%|█████████▌| 15488/16110 [27:13:52<2:45:47, 15.99s/it]

 96%|█████████▌| 15489/16110 [27:14:14<3:03:05, 17.69s/it]

 96%|█████████▌| 15490/16110 [27:14:32<3:04:35, 17.86s/it]

 96%|█████████▌| 15491/16110 [27:14:47<2:54:12, 16.89s/it]

 96%|█████████▌| 15492/16110 [27:15:06<3:02:11, 17.69s/it]


 96%|█████████▌| 15494/16110 [27:15:45<3:08:45, 18.38s/it]
{'loss': 0.1442, 'learning_rate': 1.5341825883374228e-06, 'rewards/chosen': -5.012852668762207, 'rewards/rejected': -9.277398109436035, 'rewards/accuracies': 0.875, 'rewards/margins': 4.264545440673828, 'policy_logps/rejected': -324.7071533203125, 'policy_logps/chosen': -355.4754638671875, 'referece_logps/rejected': -231.93316650390625, 'referece_logps/chosen': -305.3469543457031, 'logits/rejected': 0.03647354245185852, 'logits/chosen': 0.07290460914373398, 'epoch': 8.66}

 96%|█████████▌| 15495/16110 [27:16:06<3:16:01, 19.12s/it]

 96%|█████████▌| 15496/16110 [27:16:25<3:14:52, 19.04s/it]

 96%|█████████▌| 15497/16110 [27:16:38<2:54:46, 17.11s/it]

 96%|█████████▌| 15498/16110 [27:16:58<3:04:37, 18.10s/it]

 96%|█████████▌| 15499/16110 [27:17:11<2:50:03, 16.70s/it]

 96%|█████████▌| 15500/16110 [27:17:26<2:43:17, 16.06s/it]

 96%|█████████▌| 15501/16110 [27:18:00<3:37:12, 21.40s/it]

 96%|█████████▌| 15502/16110 [27:18:13<3:12:57, 19.04s/it]

 96%|█████████▌| 15503/16110 [27:18:29<3:03:31, 18.14s/it]

 96%|█████████▌| 15504/16110 [27:18:45<2:55:24, 17.37s/it]

 96%|█████████▌| 15505/16110 [27:18:57<2:40:29, 15.92s/it]

 96%|█████████▋| 15506/16110 [27:19:18<2:54:02, 17.29s/it]

 96%|█████████▋| 15507/16110 [27:19:33<2:48:04, 16.72s/it]


 96%|█████████▋| 15509/16110 [27:20:05<2:42:05, 16.18s/it]
{'loss': 0.165, 'learning_rate': 1.5316307530365113e-06, 'rewards/chosen': -5.508848667144775, 'rewards/rejected': -10.170273780822754, 'rewards/accuracies': 1.0, 'rewards/margins': 4.6614251136779785, 'policy_logps/rejected': -404.0340881347656, 'policy_logps/chosen': -299.73565673828125, 'referece_logps/rejected': -302.33135986328125, 'referece_logps/chosen': -244.64720153808594, 'logits/rejected': -0.10525844246149063, 'logits/chosen': -0.11871355772018433, 'epoch': 8.66}

 96%|█████████▋| 15510/16110 [27:20:22<2:43:39, 16.37s/it]

 96%|█████████▋| 15511/16110 [27:20:38<2:42:52, 16.31s/it]

 96%|█████████▋| 15512/16110 [27:20:55<2:44:13, 16.48s/it]

 96%|█████████▋| 15513/16110 [27:21:15<2:53:19, 17.42s/it]

 96%|█████████▋| 15514/16110 [27:21:37<3:07:23, 18.87s/it]

 96%|█████████▋| 15515/16110 [27:21:54<3:00:10, 18.17s/it]

 96%|█████████▋| 15516/16110 [27:22:17<3:16:09, 19.81s/it]

 96%|█████████▋| 15517/16110 [27:22:32<3:00:05, 18.22s/it]

 96%|█████████▋| 15518/16110 [27:22:53<3:09:06, 19.17s/it]

 96%|█████████▋| 15519/16110 [27:23:15<3:15:25, 19.84s/it]

 96%|█████████▋| 15520/16110 [27:23:31<3:03:11, 18.63s/it]

 96%|█████████▋| 15521/16110 [27:23:47<2:57:50, 18.12s/it]

 96%|█████████▋| 15522/16110 [27:24:00<2:42:28, 16.58s/it]

 96%|█████████▋| 15523/16110 [27:24:22<2:56:51, 18.08s/it]

 96%|█████████▋| 15524/16110 [27:24:40<2:56:17, 18.05s/it]

 96%|█████████▋| 15525/16110 [27:24:57<2:54:32, 17.90s/it]

 96%|█████████▋| 15526/16110 [27:25:17<2:59:41, 18.46s/it]

 96%|█████████▋| 15527/16110 [27:25:37<3:04:29, 18.99s/it]


 96%|█████████▋| 15529/16110 [27:26:18<3:10:27, 19.67s/it]
{'loss': 0.0614, 'learning_rate': 1.5282207888766599e-06, 'rewards/chosen': -3.5884182453155518, 'rewards/rejected': -9.434389114379883, 'rewards/accuracies': 1.0, 'rewards/margins': 5.845972061157227, 'policy_logps/rejected': -358.1995544433594, 'policy_logps/chosen': -344.3658142089844, 'referece_logps/rejected': -263.85565185546875, 'referece_logps/chosen': -308.4816589355469, 'logits/rejected': -0.5909639596939087, 'logits/chosen': -0.7094639539718628, 'epoch': 8.68}

 96%|█████████▋| 15530/16110 [27:26:33<2:57:56, 18.41s/it]

 96%|█████████▋| 15531/16110 [27:26:50<2:52:40, 17.89s/it]

 96%|█████████▋| 15532/16110 [27:27:05<2:44:09, 17.04s/it]

 96%|█████████▋| 15533/16110 [27:27:24<2:48:53, 17.56s/it]

 96%|█████████▋| 15534/16110 [27:27:44<2:57:04, 18.45s/it]

 96%|█████████▋| 15535/16110 [27:28:00<2:49:46, 17.72s/it]

 96%|█████████▋| 15536/16110 [27:28:17<2:46:32, 17.41s/it]

 96%|█████████▋| 15537/16110 [27:28:37<2:53:37, 18.18s/it]

 96%|█████████▋| 15538/16110 [27:28:57<2:58:37, 18.74s/it]

 96%|█████████▋| 15539/16110 [27:29:13<2:51:26, 18.02s/it]

 96%|█████████▋| 15540/16110 [27:29:32<2:53:49, 18.30s/it]

 96%|█████████▋| 15541/16110 [27:29:45<2:37:25, 16.60s/it]

 96%|█████████▋| 15542/16110 [27:29:58<2:27:51, 15.62s/it]

 96%|█████████▋| 15543/16110 [27:30:15<2:29:35, 15.83s/it]

 96%|█████████▋| 15544/16110 [27:30:28<2:20:56, 14.94s/it]

 96%|█████████▋| 15545/16110 [27:30:46<2:31:46, 16.12s/it]

 96%|█████████▋| 15546/16110 [27:31:06<2:41:20, 17.16s/it]

 97%|█████████▋| 15547/16110 [27:31:27<2:52:58, 18.43s/it]

 97%|█████████▋| 15548/16110 [27:31:47<2:56:21, 18.83s/it]


 97%|█████████▋| 15550/16110 [27:32:22<2:46:06, 17.80s/it]

 97%|█████████▋| 15551/16110 [27:32:41<2:48:50, 18.12s/it]

 97%|█████████▋| 15552/16110 [27:33:00<2:50:18, 18.31s/it]

 97%|█████████▋| 15553/16110 [27:33:14<2:38:05, 17.03s/it]

 97%|█████████▋| 15554/16110 [27:33:35<2:50:04, 18.35s/it]

 97%|█████████▋| 15555/16110 [27:33:48<2:33:31, 16.60s/it]

 97%|█████████▋| 15556/16110 [27:34:02<2:26:57, 15.92s/it]

 97%|█████████▋| 15557/16110 [27:34:21<2:36:57, 17.03s/it]

 97%|█████████▋| 15558/16110 [27:34:43<2:48:42, 18.34s/it]

 97%|█████████▋| 15559/16110 [27:35:03<2:53:03, 18.84s/it]

 97%|█████████▋| 15560/16110 [27:35:23<2:55:26, 19.14s/it]

 97%|█████████▋| 15561/16110 [27:35:42<2:56:18, 19.27s/it]

 97%|█████████▋| 15562/16110 [27:36:04<3:03:39, 20.11s/it]

 97%|█████████▋| 15563/16110 [27:36:16<2:40:40, 17.62s/it]

 97%|█████████▋| 15564/16110 [27:36:36<2:47:33, 18.41s/it]

 97%|█████████▋| 15565/16110 [27:36:53<2:41:53, 17.82s/it]

 97%|█████████▋| 15566/16110 [27:37:07<2:32:46, 16.85s/it]

 97%|█████████▋| 15567/16110 [27:37:27<2:39:18, 17.60s/it]

 97%|█████████▋| 15568/16110 [27:37:46<2:44:30, 18.21s/it]

 97%|█████████▋| 15569/16110 [27:38:02<2:37:40, 17.49s/it]

 97%|█████████▋| 15570/16110 [27:38:19<2:35:27, 17.27s/it]

 97%|█████████▋| 15571/16110 [27:38:39<2:41:14, 17.95s/it]

 97%|█████████▋| 15572/16110 [27:39:00<2:49:29, 18.90s/it]

 97%|█████████▋| 15573/16110 [27:39:17<2:46:09, 18.57s/it]

 97%|█████████▋| 15574/16110 [27:39:40<2:55:55, 19.69s/it]

 97%|█████████▋| 15575/16110 [27:39:52<2:36:48, 17.59s/it]

 97%|█████████▋| 15576/16110 [27:40:11<2:39:55, 17.97s/it]

 97%|█████████▋| 15577/16110 [27:40:27<2:34:13, 17.36s/it]

 97%|█████████▋| 15578/16110 [27:40:45<2:34:21, 17.41s/it]

 97%|█████████▋| 15579/16110 [27:41:05<2:41:02, 18.20s/it]
{'loss': 0.1054, 'learning_rate': 1.5196586351026358e-06, 'rewards/chosen': -3.890031337738037, 'rewards/rejected': -8.248495101928711, 'rewards/accuracies': 1.0, 'rewards/margins': 4.358464241027832, 'policy_logps/rejected': -569.1254272460938, 'policy_logps/chosen': -504.91357421875, 'referece_logps/rejected': -486.64044189453125, 'referece_logps/chosen': -466.01324462890625, 'logits/rejected': 0.12356051802635193, 'logits/chosen': 0.13983219861984253, 'epoch': 8.7}


 97%|█████████▋| 15581/16110 [27:41:34<2:24:01, 16.33s/it]

 97%|█████████▋| 15582/16110 [27:41:53<2:31:12, 17.18s/it]

 97%|█████████▋| 15583/16110 [27:42:13<2:37:04, 17.88s/it]

 97%|█████████▋| 15584/16110 [27:42:33<2:41:57, 18.47s/it]
{'loss': 0.0655, 'learning_rate': 1.5187995172426443e-06, 'rewards/chosen': -4.759697437286377, 'rewards/rejected': -9.68089771270752, 'rewards/accuracies': 1.0, 'rewards/margins': 4.921199798583984, 'policy_logps/rejected': -463.2689208984375, 'policy_logps/chosen': -451.9720458984375, 'referece_logps/rejected': -366.4599609375, 'referece_logps/chosen': -404.37506103515625, 'logits/rejected': 0.32664352655410767, 'logits/chosen': 0.4214305579662323, 'epoch': 8.71}


 97%|█████████▋| 15586/16110 [27:43:12<2:44:42, 18.86s/it]

 97%|█████████▋| 15587/16110 [27:43:28<2:35:31, 17.84s/it]

 97%|█████████▋| 15588/16110 [27:43:48<2:40:43, 18.47s/it]

 97%|█████████▋| 15589/16110 [27:44:10<2:49:27, 19.52s/it]

 97%|█████████▋| 15590/16110 [27:44:25<2:37:42, 18.20s/it]

 97%|█████████▋| 15591/16110 [27:44:44<2:39:34, 18.45s/it]

 97%|█████████▋| 15592/16110 [27:45:04<2:42:56, 18.87s/it]
{'loss': 0.1106, 'learning_rate': 1.5174238385749475e-06, 'rewards/chosen': -3.9384043216705322, 'rewards/rejected': -8.433187484741211, 'rewards/accuracies': 1.0, 'rewards/margins': 4.4947829246521, 'policy_logps/rejected': -363.78515625, 'policy_logps/chosen': -457.3096618652344, 'referece_logps/rejected': -279.45330810546875, 'referece_logps/chosen': -417.92559814453125, 'logits/rejected': 0.26994970440864563, 'logits/chosen': 0.24650032818317413, 'epoch': 8.71}


 97%|█████████▋| 15594/16110 [27:45:44<2:48:00, 19.54s/it]

 97%|█████████▋| 15595/16110 [27:46:02<2:44:19, 19.14s/it]

 97%|█████████▋| 15596/16110 [27:46:24<2:50:23, 19.89s/it]

 97%|█████████▋| 15597/16110 [27:46:43<2:48:29, 19.71s/it]
{'loss': 0.0819, 'learning_rate': 1.5165633594557124e-06, 'rewards/chosen': -4.548232078552246, 'rewards/rejected': -11.243208885192871, 'rewards/accuracies': 1.0, 'rewards/margins': 6.694976806640625, 'policy_logps/rejected': -750.6134033203125, 'policy_logps/chosen': -463.2310485839844, 'referece_logps/rejected': -638.1813354492188, 'referece_logps/chosen': -417.74871826171875, 'logits/rejected': -1.0409470796585083, 'logits/chosen': -0.9192853569984436, 'epoch': 8.71}


 97%|█████████▋| 15599/16110 [27:47:22<2:49:17, 19.88s/it]

 97%|█████████▋| 15600/16110 [27:47:37<2:36:24, 18.40s/it]

 97%|█████████▋| 15601/16110 [27:47:55<2:34:49, 18.25s/it]

 97%|█████████▋| 15602/16110 [27:48:09<2:24:03, 17.01s/it]

 97%|█████████▋| 15603/16110 [27:48:22<2:13:06, 15.75s/it]

 97%|█████████▋| 15604/16110 [27:48:33<2:01:02, 14.35s/it]
{'loss': 0.113, 'learning_rate': 1.5153578119293685e-06, 'rewards/chosen': -3.9723663330078125, 'rewards/rejected': -10.107890129089355, 'rewards/accuracies': 1.0, 'rewards/margins': 6.135523796081543, 'policy_logps/rejected': -364.9538269042969, 'policy_logps/chosen': -422.11798095703125, 'referece_logps/rejected': -263.8749084472656, 'referece_logps/chosen': -382.394287109375, 'logits/rejected': 1.0468978881835938, 'logits/chosen': 0.6294445395469666, 'epoch': 8.72}


 97%|█████████▋| 15606/16110 [27:49:15<2:28:40, 17.70s/it]

 97%|█████████▋| 15607/16110 [27:49:34<2:33:02, 18.26s/it]

 97%|█████████▋| 15608/16110 [27:49:54<2:37:32, 18.83s/it]

 97%|█████████▋| 15609/16110 [27:50:15<2:42:09, 19.42s/it]

 97%|█████████▋| 15610/16110 [27:50:27<2:22:42, 17.13s/it]

 97%|█████████▋| 15611/16110 [27:50:46<2:28:20, 17.84s/it]
{'loss': 0.1858, 'learning_rate': 1.5141512436774941e-06, 'rewards/chosen': -4.0680999755859375, 'rewards/rejected': -10.549661636352539, 'rewards/accuracies': 1.0, 'rewards/margins': 6.481561183929443, 'policy_logps/rejected': -547.6220703125, 'policy_logps/chosen': -544.4464111328125, 'referece_logps/rejected': -442.12542724609375, 'referece_logps/chosen': -503.7653503417969, 'logits/rejected': 0.6939390897750854, 'logits/chosen': 0.7389090061187744, 'epoch': 8.72}

 97%|█████████▋| 15612/16110 [27:51:04<2:26:46, 17.68s/it]


 97%|█████████▋| 15614/16110 [27:51:43<2:33:58, 18.63s/it]
{'loss': 0.0728, 'learning_rate': 1.5136338311232605e-06, 'rewards/chosen': -4.620119571685791, 'rewards/rejected': -13.11611270904541, 'rewards/accuracies': 1.0, 'rewards/margins': 8.495992660522461, 'policy_logps/rejected': -718.767333984375, 'policy_logps/chosen': -430.2310485839844, 'referece_logps/rejected': -587.606201171875, 'referece_logps/chosen': -384.02984619140625, 'logits/rejected': 0.19686011970043182, 'logits/chosen': 0.3848537504673004, 'epoch': 8.72}


 97%|█████████▋| 15616/16110 [27:52:17<2:27:45, 17.95s/it]
{'loss': 0.0907, 'learning_rate': 1.5132887856015586e-06, 'rewards/chosen': -3.40014386177063, 'rewards/rejected': -8.036016464233398, 'rewards/accuracies': 1.0, 'rewards/margins': 4.635872840881348, 'policy_logps/rejected': -311.00323486328125, 'policy_logps/chosen': -345.00860595703125, 'referece_logps/rejected': -230.64308166503906, 'referece_logps/chosen': -311.0071716308594, 'logits/rejected': -0.12497273087501526, 'logits/chosen': 0.06392683833837509, 'epoch': 8.72}

 97%|█████████▋| 15617/16110 [27:52:36<2:28:35, 18.08s/it]

 97%|█████████▋| 15618/16110 [27:52:56<2:33:17, 18.69s/it]

 97%|█████████▋| 15619/16110 [27:53:18<2:40:31, 19.62s/it]


 97%|█████████▋| 15621/16110 [27:53:57<2:39:49, 19.61s/it]

 97%|█████████▋| 15622/16110 [27:54:15<2:34:47, 19.03s/it]

 97%|█████████▋| 15623/16110 [27:54:32<2:28:59, 18.36s/it]

 97%|█████████▋| 15624/16110 [27:54:50<2:27:46, 18.24s/it]

 97%|█████████▋| 15625/16110 [27:55:08<2:26:58, 18.18s/it]

 97%|█████████▋| 15626/16110 [27:55:25<2:24:41, 17.94s/it]
{'loss': 0.1414, 'learning_rate': 1.5115623142589851e-06, 'rewards/chosen': -5.848452568054199, 'rewards/rejected': -9.502382278442383, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6539292335510254, 'policy_logps/rejected': -318.74481201171875, 'policy_logps/chosen': -377.01275634765625, 'referece_logps/rejected': -223.72097778320312, 'referece_logps/chosen': -318.5281982421875, 'logits/rejected': -0.24070017039775848, 'logits/chosen': -0.42392992973327637, 'epoch': 8.73}

 97%|█████████▋| 15627/16110 [27:55:42<2:21:49, 17.62s/it]

 97%|█████████▋| 15628/16110 [27:56:00<2:22:54, 17.79s/it]


 97%|█████████▋| 15630/16110 [27:56:33<2:14:07, 16.77s/it]

 97%|█████████▋| 15631/16110 [27:56:45<2:03:58, 15.53s/it]
{'loss': 0.1319, 'learning_rate': 1.5106983027369775e-06, 'rewards/chosen': -5.368838310241699, 'rewards/rejected': -8.913887023925781, 'rewards/accuracies': 0.875, 'rewards/margins': 3.5450496673583984, 'policy_logps/rejected': -344.95135498046875, 'policy_logps/chosen': -468.8985290527344, 'referece_logps/rejected': -255.81248474121094, 'referece_logps/chosen': -415.2101745605469, 'logits/rejected': 0.2161659449338913, 'logits/chosen': -0.08868061006069183, 'epoch': 8.73}


 97%|█████████▋| 15633/16110 [27:57:21<2:12:41, 16.69s/it]

 97%|█████████▋| 15634/16110 [27:57:38<2:13:12, 16.79s/it]

 97%|█████████▋| 15635/16110 [27:57:52<2:06:29, 15.98s/it]
{'loss': 0.3024, 'learning_rate': 1.510006721907149e-06, 'rewards/chosen': -4.83299446105957, 'rewards/rejected': -8.814929008483887, 'rewards/accuracies': 1.0, 'rewards/margins': 3.981933116912842, 'policy_logps/rejected': -484.46136474609375, 'policy_logps/chosen': -361.08209228515625, 'referece_logps/rejected': -396.31207275390625, 'referece_logps/chosen': -312.7521667480469, 'logits/rejected': -0.1482706516981125, 'logits/chosen': -0.24968719482421875, 'epoch': 8.73}


 97%|█████████▋| 15637/16110 [27:58:28<2:14:55, 17.11s/it]
{'loss': 0.1244, 'learning_rate': 1.5096608077752286e-06, 'rewards/chosen': -4.847944259643555, 'rewards/rejected': -10.82825756072998, 'rewards/accuracies': 1.0, 'rewards/margins': 5.980312347412109, 'policy_logps/rejected': -397.6595764160156, 'policy_logps/chosen': -440.7760925292969, 'referece_logps/rejected': -289.37701416015625, 'referece_logps/chosen': -392.2966003417969, 'logits/rejected': 0.09774109721183777, 'logits/chosen': -0.045680150389671326, 'epoch': 8.74}


 97%|█████████▋| 15639/16110 [27:59:02<2:16:28, 17.39s/it]
{'loss': 0.1033, 'learning_rate': 1.5093148112398701e-06, 'rewards/chosen': -4.721884250640869, 'rewards/rejected': -11.165107727050781, 'rewards/accuracies': 1.0, 'rewards/margins': 6.443222999572754, 'policy_logps/rejected': -476.5115661621094, 'policy_logps/chosen': -394.3185119628906, 'referece_logps/rejected': -364.8605041503906, 'referece_logps/chosen': -347.09967041015625, 'logits/rejected': -0.20004992187023163, 'logits/chosen': -0.2817232012748718, 'epoch': 8.74}

 97%|█████████▋| 15640/16110 [27:59:20<2:17:47, 17.59s/it]

 97%|█████████▋| 15641/16110 [27:59:34<2:10:12, 16.66s/it]


 97%|█████████▋| 15643/16110 [28:00:00<1:54:14, 14.68s/it]

 97%|█████████▋| 15644/16110 [28:00:19<2:05:06, 16.11s/it]

 97%|█████████▋| 15645/16110 [28:00:39<2:14:30, 17.36s/it]

 97%|█████████▋| 15646/16110 [28:01:01<2:24:26, 18.68s/it]

 97%|█████████▋| 15647/16110 [28:01:22<2:28:21, 19.22s/it]

 97%|█████████▋| 15648/16110 [28:01:41<2:29:14, 19.38s/it]

 97%|█████████▋| 15649/16110 [28:01:57<2:20:59, 18.35s/it]

 97%|█████████▋| 15650/16110 [28:02:17<2:23:51, 18.76s/it]

 97%|█████████▋| 15651/16110 [28:02:36<2:22:54, 18.68s/it]
{'loss': 0.1792, 'learning_rate': 1.5072371046892007e-06, 'rewards/chosen': -4.802309989929199, 'rewards/rejected': -9.416787147521973, 'rewards/accuracies': 0.875, 'rewards/margins': 4.614476680755615, 'policy_logps/rejected': -455.8660888671875, 'policy_logps/chosen': -628.5521240234375, 'referece_logps/rejected': -361.6982421875, 'referece_logps/chosen': -580.529052734375, 'logits/rejected': 0.5920522212982178, 'logits/chosen': 0.4261091351509094, 'epoch': 8.74}

 97%|█████████▋| 15652/16110 [28:02:53<2:18:58, 18.21s/it]


 97%|█████████▋| 15654/16110 [28:03:33<2:24:45, 19.05s/it]
{'loss': 0.2549, 'learning_rate': 1.506717216263912e-06, 'rewards/chosen': -5.362476348876953, 'rewards/rejected': -7.37078332901001, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0083067417144775, 'policy_logps/rejected': -261.6482849121094, 'policy_logps/chosen': -286.3532409667969, 'referece_logps/rejected': -187.94044494628906, 'referece_logps/chosen': -232.72850036621094, 'logits/rejected': 0.03169059753417969, 'logits/chosen': 0.05167916417121887, 'epoch': 8.75}


 97%|█████████▋| 15656/16110 [28:04:06<2:15:31, 17.91s/it]

 97%|█████████▋| 15657/16110 [28:04:27<2:22:40, 18.90s/it]

 97%|█████████▋| 15658/16110 [28:04:49<2:30:25, 19.97s/it]
{'loss': 0.1529, 'learning_rate': 1.506023744983282e-06, 'rewards/chosen': -3.9264132976531982, 'rewards/rejected': -9.893386840820312, 'rewards/accuracies': 0.875, 'rewards/margins': 5.966973304748535, 'policy_logps/rejected': -446.2476501464844, 'policy_logps/chosen': -350.9640808105469, 'referece_logps/rejected': -347.3138122558594, 'referece_logps/chosen': -311.6999206542969, 'logits/rejected': 0.024496790021657944, 'logits/chosen': 0.18391630053520203, 'epoch': 8.75}


 97%|█████████▋| 15660/16110 [28:05:30<2:28:57, 19.86s/it]

 97%|█████████▋| 15661/16110 [28:05:52<2:33:16, 20.48s/it]

 97%|█████████▋| 15662/16110 [28:06:08<2:23:54, 19.27s/it]

 97%|█████████▋| 15663/16110 [28:06:28<2:24:50, 19.44s/it]
{'loss': 0.0729, 'learning_rate': 1.50515644572354e-06, 'rewards/chosen': -3.9339094161987305, 'rewards/rejected': -10.359095573425293, 'rewards/accuracies': 1.0, 'rewards/margins': 6.425186634063721, 'policy_logps/rejected': -402.5111389160156, 'policy_logps/chosen': -449.2839660644531, 'referece_logps/rejected': -298.920166015625, 'referece_logps/chosen': -409.94488525390625, 'logits/rejected': 0.6958550214767456, 'logits/chosen': 0.5095271468162537, 'epoch': 8.75}

 97%|█████████▋| 15664/16110 [28:06:51<2:31:35, 20.39s/it]


 97%|█████████▋| 15666/16110 [28:07:28<2:25:22, 19.65s/it]
{'loss': 0.1135, 'learning_rate': 1.5046358210861225e-06, 'rewards/chosen': -3.710256576538086, 'rewards/rejected': -6.762434959411621, 'rewards/accuracies': 0.75, 'rewards/margins': 3.052178382873535, 'policy_logps/rejected': -231.87185668945312, 'policy_logps/chosen': -306.31439208984375, 'referece_logps/rejected': -164.24749755859375, 'referece_logps/chosen': -269.21185302734375, 'logits/rejected': -0.45637112855911255, 'logits/chosen': -0.680304765701294, 'epoch': 8.75}


 97%|█████████▋| 15668/16110 [28:08:02<2:14:57, 18.32s/it]

 97%|█████████▋| 15669/16110 [28:08:15<2:03:30, 16.80s/it]
{'loss': 0.107, 'learning_rate': 1.5041150128689956e-06, 'rewards/chosen': -5.377403736114502, 'rewards/rejected': -10.4430513381958, 'rewards/accuracies': 1.0, 'rewards/margins': 5.065647602081299, 'policy_logps/rejected': -460.6851501464844, 'policy_logps/chosen': -338.3593444824219, 'referece_logps/rejected': -356.2546691894531, 'referece_logps/chosen': -284.5853271484375, 'logits/rejected': -0.5931308269500732, 'logits/chosen': -0.3470191955566406, 'epoch': 8.75}

 97%|█████████▋| 15670/16110 [28:08:29<1:55:18, 15.72s/it]


 97%|█████████▋| 15672/16110 [28:09:06<2:06:46, 17.37s/it]

 97%|█████████▋| 15673/16110 [28:09:21<2:01:51, 16.73s/it]

 97%|█████████▋| 15674/16110 [28:09:40<2:04:55, 17.19s/it]

 97%|█████████▋| 15675/16110 [28:09:56<2:02:36, 16.91s/it]

 97%|█████████▋| 15676/16110 [28:10:09<1:53:03, 15.63s/it]
{'loss': 0.1631, 'learning_rate': 1.502899080839002e-06, 'rewards/chosen': -5.894197940826416, 'rewards/rejected': -7.7788543701171875, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8846566677093506, 'policy_logps/rejected': -442.009521484375, 'policy_logps/chosen': -409.9089050292969, 'referece_logps/rejected': -364.22100830078125, 'referece_logps/chosen': -350.9668884277344, 'logits/rejected': 0.6185870170593262, 'logits/chosen': 0.59846431016922, 'epoch': 8.76}

 97%|█████████▋| 15677/16110 [28:10:21<1:46:00, 14.69s/it]

 97%|█████████▋| 15678/16110 [28:10:35<1:43:53, 14.43s/it]


 97%|█████████▋| 15680/16110 [28:11:04<1:45:49, 14.77s/it]

 97%|█████████▋| 15681/16110 [28:11:24<1:56:11, 16.25s/it]

 97%|█████████▋| 15682/16110 [28:11:38<1:52:16, 15.74s/it]

 97%|█████████▋| 15683/16110 [28:11:58<2:00:54, 16.99s/it]
{'loss': 0.114, 'learning_rate': 1.5016821527594305e-06, 'rewards/chosen': -4.809299468994141, 'rewards/rejected': -11.280058860778809, 'rewards/accuracies': 1.0, 'rewards/margins': 6.470759391784668, 'policy_logps/rejected': -502.2662353515625, 'policy_logps/chosen': -359.3439636230469, 'referece_logps/rejected': -389.46563720703125, 'referece_logps/chosen': -311.2509765625, 'logits/rejected': -0.44022560119628906, 'logits/chosen': -0.4438493549823761, 'epoch': 8.76}

 97%|█████████▋| 15684/16110 [28:12:22<2:13:48, 18.85s/it]

 97%|█████████▋| 15685/16110 [28:12:41<2:14:28, 18.98s/it]

 97%|█████████▋| 15686/16110 [28:12:55<2:03:38, 17.50s/it]

 97%|█████████▋| 15687/16110 [28:13:11<2:01:28, 17.23s/it]


 97%|█████████▋| 15689/16110 [28:13:46<2:02:13, 17.42s/it]
{'loss': 0.1041, 'learning_rate': 1.5006382806011865e-06, 'rewards/chosen': -3.941922426223755, 'rewards/rejected': -10.557022094726562, 'rewards/accuracies': 1.0, 'rewards/margins': 6.615099906921387, 'policy_logps/rejected': -363.9201354980469, 'policy_logps/chosen': -433.2638244628906, 'referece_logps/rejected': -258.34991455078125, 'referece_logps/chosen': -393.8446350097656, 'logits/rejected': -0.23635779321193695, 'logits/chosen': -0.5055646896362305, 'epoch': 8.76}


 97%|█████████▋| 15691/16110 [28:14:25<2:08:18, 18.37s/it]
{'loss': 0.111, 'learning_rate': 1.5002901612507822e-06, 'rewards/chosen': -5.389271259307861, 'rewards/rejected': -10.703634262084961, 'rewards/accuracies': 0.75, 'rewards/margins': 5.3143630027771, 'policy_logps/rejected': -380.3427734375, 'policy_logps/chosen': -359.52362060546875, 'referece_logps/rejected': -273.3064270019531, 'referece_logps/chosen': -305.63092041015625, 'logits/rejected': 0.19746001064777374, 'logits/chosen': 0.18170911073684692, 'epoch': 8.77}

 97%|█████████▋| 15692/16110 [28:14:40<2:01:12, 17.40s/it]

 97%|█████████▋| 15693/16110 [28:15:01<2:09:31, 18.64s/it]


 97%|█████████▋| 15695/16110 [28:15:45<2:19:32, 20.18s/it]
{'loss': 0.0881, 'learning_rate': 1.4995936799411784e-06, 'rewards/chosen': -5.8166399002075195, 'rewards/rejected': -10.177363395690918, 'rewards/accuracies': 1.0, 'rewards/margins': 4.36072301864624, 'policy_logps/rejected': -395.99273681640625, 'policy_logps/chosen': -352.54827880859375, 'referece_logps/rejected': -294.2190856933594, 'referece_logps/chosen': -294.3819274902344, 'logits/rejected': 0.8174675107002258, 'logits/chosen': 0.7754542827606201, 'epoch': 8.77}


 97%|█████████▋| 15697/16110 [28:16:25<2:17:22, 19.96s/it]

 97%|█████████▋| 15698/16110 [28:16:38<2:03:46, 18.03s/it]
{'loss': 0.0775, 'learning_rate': 1.499071106897986e-06, 'rewards/chosen': -5.539403438568115, 'rewards/rejected': -10.933356285095215, 'rewards/accuracies': 1.0, 'rewards/margins': 5.3939528465271, 'policy_logps/rejected': -435.4969787597656, 'policy_logps/chosen': -370.7861022949219, 'referece_logps/rejected': -326.1634216308594, 'referece_logps/chosen': -315.3920593261719, 'logits/rejected': 0.0038597434759140015, 'logits/chosen': 0.029469288885593414, 'epoch': 8.77}

 97%|█████████▋| 15699/16110 [28:17:00<2:11:27, 19.19s/it]

 97%|█████████▋| 15700/16110 [28:17:20<2:12:06, 19.33s/it]


 97%|█████████▋| 15702/16110 [28:18:01<2:15:23, 19.91s/it]

 97%|█████████▋| 15703/16110 [28:18:21<2:14:26, 19.82s/it]
{'loss': 0.1138, 'learning_rate': 1.4981997484635943e-06, 'rewards/chosen': -3.5577738285064697, 'rewards/rejected': -9.677776336669922, 'rewards/accuracies': 0.875, 'rewards/margins': 6.120001792907715, 'policy_logps/rejected': -393.6403503417969, 'policy_logps/chosen': -428.4958190917969, 'referece_logps/rejected': -296.8625793457031, 'referece_logps/chosen': -392.9181213378906, 'logits/rejected': 0.3579403758049011, 'logits/chosen': 0.3372827172279358, 'epoch': 8.77}

 97%|█████████▋| 15704/16110 [28:18:41<2:15:50, 20.07s/it]

 97%|█████████▋| 15705/16110 [28:18:56<2:03:30, 18.30s/it]


 97%|█████████▋| 15707/16110 [28:19:31<1:59:45, 17.83s/it]
{'loss': 0.0507, 'learning_rate': 1.497502299197113e-06, 'rewards/chosen': -4.679520606994629, 'rewards/rejected': -10.410185813903809, 'rewards/accuracies': 0.875, 'rewards/margins': 5.7306647300720215, 'policy_logps/rejected': -342.5602111816406, 'policy_logps/chosen': -324.9189453125, 'referece_logps/rejected': -238.45834350585938, 'referece_logps/chosen': -278.1237487792969, 'logits/rejected': 0.5164424180984497, 'logits/chosen': 0.518616795539856, 'epoch': 8.77}


 98%|█████████▊| 15709/16110 [28:20:13<2:09:37, 19.39s/it]

 98%|█████████▊| 15710/16110 [28:20:33<2:10:56, 19.64s/it]
{'loss': 0.0871, 'learning_rate': 1.4969790010738473e-06, 'rewards/chosen': -5.030925750732422, 'rewards/rejected': -11.995451927185059, 'rewards/accuracies': 1.0, 'rewards/margins': 6.964524745941162, 'policy_logps/rejected': -595.5928955078125, 'policy_logps/chosen': -508.4892272949219, 'referece_logps/rejected': -475.638427734375, 'referece_logps/chosen': -458.17999267578125, 'logits/rejected': 0.3367804288864136, 'logits/chosen': 0.2697474956512451, 'epoch': 8.78}

 98%|█████████▊| 15711/16110 [28:20:54<2:13:38, 20.10s/it]


 98%|█████████▊| 15713/16110 [28:21:27<1:58:06, 17.85s/it]

 98%|█████████▊| 15714/16110 [28:21:47<2:02:13, 18.52s/it]

 98%|█████████▊| 15715/16110 [28:22:07<2:05:34, 19.07s/it]
{'loss': 0.1211, 'learning_rate': 1.4961064358640853e-06, 'rewards/chosen': -6.072053909301758, 'rewards/rejected': -12.92672348022461, 'rewards/accuracies': 1.0, 'rewards/margins': 6.854668617248535, 'policy_logps/rejected': -474.33917236328125, 'policy_logps/chosen': -378.0403747558594, 'referece_logps/rejected': -345.0719299316406, 'referece_logps/chosen': -317.31982421875, 'logits/rejected': -0.3802964687347412, 'logits/chosen': -0.5824412703514099, 'epoch': 8.78}

 98%|█████████▊| 15716/16110 [28:22:28<2:08:01, 19.50s/it]


 98%|█████████▊| 15718/16110 [28:22:59<1:55:09, 17.63s/it]
{'loss': 0.1726, 'learning_rate': 1.49558265604601e-06, 'rewards/chosen': -2.827829360961914, 'rewards/rejected': -6.390233516693115, 'rewards/accuracies': 0.875, 'rewards/margins': 3.562404155731201, 'policy_logps/rejected': -232.04489135742188, 'policy_logps/chosen': -321.3938903808594, 'referece_logps/rejected': -168.14254760742188, 'referece_logps/chosen': -293.1156005859375, 'logits/rejected': -0.3737803101539612, 'logits/chosen': -0.08699969947338104, 'epoch': 8.78}


 98%|█████████▊| 15720/16110 [28:23:33<1:51:31, 17.16s/it]

 98%|█████████▊| 15721/16110 [28:23:47<1:45:20, 16.25s/it]

 98%|█████████▊| 15722/16110 [28:24:05<1:48:12, 16.73s/it]

 98%|█████████▊| 15723/16110 [28:24:23<1:51:01, 17.21s/it]

 98%|█████████▊| 15724/16110 [28:24:43<1:55:46, 18.00s/it]

 98%|█████████▊| 15725/16110 [28:25:03<1:58:49, 18.52s/it]
{'loss': 0.1196, 'learning_rate': 1.4943598023532785e-06, 'rewards/chosen': -3.7741613388061523, 'rewards/rejected': -9.188016891479492, 'rewards/accuracies': 1.0, 'rewards/margins': 5.41385555267334, 'policy_logps/rejected': -336.09625244140625, 'policy_logps/chosen': -364.73016357421875, 'referece_logps/rejected': -244.21607971191406, 'referece_logps/chosen': -326.988525390625, 'logits/rejected': -0.23910759389400482, 'logits/chosen': -0.41565924882888794, 'epoch': 8.78}

 98%|█████████▊| 15726/16110 [28:25:18<1:51:32, 17.43s/it]

 98%|█████████▊| 15727/16110 [28:25:37<1:53:46, 17.82s/it]

 98%|█████████▊| 15728/16110 [28:25:54<1:52:29, 17.67s/it]

 98%|█████████▊| 15729/16110 [28:26:11<1:50:17, 17.37s/it]

 98%|█████████▊| 15730/16110 [28:26:27<1:47:52, 17.03s/it]

 98%|█████████▊| 15731/16110 [28:26:47<1:52:48, 17.86s/it]

 98%|█████████▊| 15732/16110 [28:27:07<1:56:43, 18.53s/it]

 98%|█████████▊| 15733/16110 [28:27:25<1:55:08, 18.33s/it]

 98%|█████████▊| 15734/16110 [28:27:44<1:57:01, 18.67s/it]


 98%|█████████▊| 15736/16110 [28:28:19<1:51:41, 17.92s/it]

 98%|█████████▊| 15737/16110 [28:28:39<1:55:16, 18.54s/it]
{'loss': 0.1414, 'learning_rate': 1.4922612050886357e-06, 'rewards/chosen': -3.6281638145446777, 'rewards/rejected': -9.462443351745605, 'rewards/accuracies': 1.0, 'rewards/margins': 5.834279537200928, 'policy_logps/rejected': -347.7242126464844, 'policy_logps/chosen': -260.0951232910156, 'referece_logps/rejected': -253.09979248046875, 'referece_logps/chosen': -223.81349182128906, 'logits/rejected': 0.16865845024585724, 'logits/chosen': 0.11507423967123032, 'epoch': 8.79}


 98%|█████████▊| 15739/16110 [28:29:09<1:45:27, 17.06s/it]
{'loss': 0.0798, 'learning_rate': 1.4919111599820993e-06, 'rewards/chosen': -5.287473201751709, 'rewards/rejected': -11.323200225830078, 'rewards/accuracies': 1.0, 'rewards/margins': 6.035727500915527, 'policy_logps/rejected': -353.32196044921875, 'policy_logps/chosen': -339.8442077636719, 'referece_logps/rejected': -240.0899658203125, 'referece_logps/chosen': -286.969482421875, 'logits/rejected': -0.07666747272014618, 'logits/chosen': -0.3519952893257141, 'epoch': 8.79}

 98%|█████████▊| 15740/16110 [28:29:29<1:50:03, 17.85s/it]


 98%|█████████▊| 15742/16110 [28:29:57<1:39:19, 16.19s/it]

 98%|█████████▊| 15743/16110 [28:30:17<1:46:08, 17.35s/it]
{'loss': 0.0445, 'learning_rate': 1.4912108312247666e-06, 'rewards/chosen': -4.759378433227539, 'rewards/rejected': -9.861797332763672, 'rewards/accuracies': 1.0, 'rewards/margins': 5.102417945861816, 'policy_logps/rejected': -511.1102294921875, 'policy_logps/chosen': -493.1991882324219, 'referece_logps/rejected': -412.4923095703125, 'referece_logps/chosen': -445.6053771972656, 'logits/rejected': -0.6949840784072876, 'logits/chosen': -0.8730367422103882, 'epoch': 8.79}


 98%|█████████▊| 15745/16110 [28:30:55<1:50:25, 18.15s/it]

 98%|█████████▊| 15746/16110 [28:31:06<1:36:12, 15.86s/it]
{'loss': 0.1277, 'learning_rate': 1.4906853761534702e-06, 'rewards/chosen': -3.5989654064178467, 'rewards/rejected': -11.598489761352539, 'rewards/accuracies': 1.0, 'rewards/margins': 7.999524116516113, 'policy_logps/rejected': -344.7666320800781, 'policy_logps/chosen': -368.5118408203125, 'referece_logps/rejected': -228.78173828125, 'referece_logps/chosen': -332.5221862792969, 'logits/rejected': 0.03896518796682358, 'logits/chosen': -0.13436663150787354, 'epoch': 8.8}

 98%|█████████▊| 15747/16110 [28:31:25<1:41:17, 16.74s/it]

 98%|█████████▊| 15748/16110 [28:31:38<1:35:37, 15.85s/it]

 98%|█████████▊| 15749/16110 [28:31:58<1:42:39, 17.06s/it]


 98%|█████████▊| 15751/16110 [28:32:36<1:48:57, 18.21s/it]
{'loss': 0.0842, 'learning_rate': 1.4898092211185685e-06, 'rewards/chosen': -4.4280476570129395, 'rewards/rejected': -10.030694007873535, 'rewards/accuracies': 1.0, 'rewards/margins': 5.602646350860596, 'policy_logps/rejected': -584.0316162109375, 'policy_logps/chosen': -729.0405883789062, 'referece_logps/rejected': -483.7246398925781, 'referece_logps/chosen': -684.760009765625, 'logits/rejected': 0.23422637581825256, 'logits/chosen': 0.08549010008573532, 'epoch': 8.8}

 98%|█████████▊| 15752/16110 [28:32:55<1:50:23, 18.50s/it]

 98%|█████████▊| 15753/16110 [28:33:15<1:52:21, 18.88s/it]


 98%|█████████▊| 15755/16110 [28:33:44<1:39:50, 16.87s/it]

 98%|█████████▊| 15756/16110 [28:34:04<1:44:51, 17.77s/it]
{'loss': 0.0718, 'learning_rate': 1.4889325711226048e-06, 'rewards/chosen': -4.385429382324219, 'rewards/rejected': -10.3408203125, 'rewards/accuracies': 1.0, 'rewards/margins': 5.955390453338623, 'policy_logps/rejected': -374.5077209472656, 'policy_logps/chosen': -381.4648742675781, 'referece_logps/rejected': -271.0995178222656, 'referece_logps/chosen': -337.6105651855469, 'logits/rejected': -0.2334119975566864, 'logits/chosen': -0.43961167335510254, 'epoch': 8.8}

 98%|█████████▊| 15757/16110 [28:34:23<1:46:55, 18.17s/it]

 98%|█████████▊| 15758/16110 [28:34:41<1:46:05, 18.08s/it]


 98%|█████████▊| 15760/16110 [28:35:14<1:43:20, 17.72s/it]
{'loss': 0.0221, 'learning_rate': 1.4882308953491586e-06, 'rewards/chosen': -5.5221052169799805, 'rewards/rejected': -9.610342979431152, 'rewards/accuracies': 1.0, 'rewards/margins': 4.088237285614014, 'policy_logps/rejected': -340.3329162597656, 'policy_logps/chosen': -302.7414245605469, 'referece_logps/rejected': -244.2294921875, 'referece_logps/chosen': -247.5203399658203, 'logits/rejected': 2.444535493850708e-05, 'logits/chosen': -0.010275259613990784, 'epoch': 8.8}

 98%|█████████▊| 15761/16110 [28:35:32<1:44:31, 17.97s/it]

 98%|█████████▊| 15762/16110 [28:35:51<1:45:23, 18.17s/it]


 98%|█████████▊| 15764/16110 [28:36:20<1:32:47, 16.09s/it]

 98%|█████████▊| 15765/16110 [28:36:38<1:36:38, 16.81s/it]
{'loss': 0.1135, 'learning_rate': 1.487353356656014e-06, 'rewards/chosen': -4.114638805389404, 'rewards/rejected': -8.454913139343262, 'rewards/accuracies': 1.0, 'rewards/margins': 4.340274810791016, 'policy_logps/rejected': -377.1902770996094, 'policy_logps/chosen': -432.57415771484375, 'referece_logps/rejected': -292.6411437988281, 'referece_logps/chosen': -391.42779541015625, 'logits/rejected': 0.46608322858810425, 'logits/chosen': 0.3642798364162445, 'epoch': 8.81}

 98%|█████████▊| 15766/16110 [28:36:57<1:40:04, 17.45s/it]

 98%|█████████▊| 15767/16110 [28:37:11<1:34:16, 16.49s/it]

 98%|█████████▊| 15768/16110 [28:37:31<1:39:27, 17.45s/it]

 98%|█████████▊| 15769/16110 [28:37:53<1:46:45, 18.78s/it]

 98%|█████████▊| 15770/16110 [28:38:05<1:35:22, 16.83s/it]

 98%|█████████▊| 15771/16110 [28:38:21<1:33:26, 16.54s/it]

 98%|█████████▊| 15772/16110 [28:38:40<1:37:50, 17.37s/it]

 98%|█████████▊| 15773/16110 [28:38:58<1:37:00, 17.27s/it]


 98%|█████████▊| 15775/16110 [28:39:36<1:42:19, 18.33s/it]

 98%|█████████▊| 15776/16110 [28:39:58<1:48:07, 19.42s/it]

 98%|█████████▊| 15777/16110 [28:40:18<1:48:45, 19.60s/it]
{'loss': 0.0794, 'learning_rate': 1.4852452561661032e-06, 'rewards/chosen': -6.077037811279297, 'rewards/rejected': -9.508694648742676, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4316563606262207, 'policy_logps/rejected': -504.30621337890625, 'policy_logps/chosen': -401.7950744628906, 'referece_logps/rejected': -409.2193298339844, 'referece_logps/chosen': -341.02471923828125, 'logits/rejected': 0.07716701924800873, 'logits/chosen': 0.4273892939090729, 'epoch': 8.81}

 98%|█████████▊| 15778/16110 [28:40:33<1:40:04, 18.09s/it]

 98%|█████████▊| 15779/16110 [28:40:48<1:34:07, 17.06s/it]


 98%|█████████▊| 15781/16110 [28:41:28<1:42:38, 18.72s/it]
{'loss': 0.1131, 'learning_rate': 1.4845419277497943e-06, 'rewards/chosen': -6.284085273742676, 'rewards/rejected': -11.33426284790039, 'rewards/accuracies': 1.0, 'rewards/margins': 5.050177574157715, 'policy_logps/rejected': -425.22021484375, 'policy_logps/chosen': -389.29388427734375, 'referece_logps/rejected': -311.87762451171875, 'referece_logps/chosen': -326.4530029296875, 'logits/rejected': -0.2947661280632019, 'logits/chosen': -0.38817378878593445, 'epoch': 8.82}

 98%|█████████▊| 15782/16110 [28:41:50<1:46:45, 19.53s/it]


 98%|█████████▊| 15784/16110 [28:42:27<1:44:24, 19.22s/it]

 98%|█████████▊| 15785/16110 [28:42:46<1:45:08, 19.41s/it]
{'loss': 0.1215, 'learning_rate': 1.4838382859649215e-06, 'rewards/chosen': -4.019671440124512, 'rewards/rejected': -8.621528625488281, 'rewards/accuracies': 0.875, 'rewards/margins': 4.6018571853637695, 'policy_logps/rejected': -395.7905578613281, 'policy_logps/chosen': -569.23779296875, 'referece_logps/rejected': -309.5752868652344, 'referece_logps/chosen': -529.0411376953125, 'logits/rejected': 0.0727660283446312, 'logits/chosen': 0.01360756903886795, 'epoch': 8.82}

 98%|█████████▊| 15786/16110 [28:43:01<1:37:36, 18.07s/it]

 98%|█████████▊| 15787/16110 [28:43:18<1:34:30, 17.56s/it]

 98%|█████████▊| 15788/16110 [28:43:31<1:27:44, 16.35s/it]

 98%|█████████▊| 15789/16110 [28:43:44<1:21:41, 15.27s/it]

 98%|█████████▊| 15790/16110 [28:44:04<1:28:36, 16.62s/it]

 98%|█████████▊| 15791/16110 [28:44:24<1:33:30, 17.59s/it]

 98%|█████████▊| 15792/16110 [28:44:40<1:30:37, 17.10s/it]

 98%|█████████▊| 15793/16110 [28:45:02<1:38:58, 18.73s/it]

 98%|█████████▊| 15794/16110 [28:45:22<1:40:30, 19.08s/it]

 98%|█████████▊| 15795/16110 [28:45:33<1:27:23, 16.65s/it]


 98%|█████████▊| 15797/16110 [28:46:13<1:35:17, 18.27s/it]
{'loss': 0.1323, 'learning_rate': 1.4817254849506035e-06, 'rewards/chosen': -4.413541793823242, 'rewards/rejected': -8.155155181884766, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7416136264801025, 'policy_logps/rejected': -238.45062255859375, 'policy_logps/chosen': -411.1809997558594, 'referece_logps/rejected': -156.89906311035156, 'referece_logps/chosen': -367.04559326171875, 'logits/rejected': -0.1434727907180786, 'logits/chosen': -0.42879435420036316, 'epoch': 8.83}

 98%|█████████▊| 15798/16110 [28:46:28<1:30:01, 17.31s/it]

 98%|█████████▊| 15799/16110 [28:46:43<1:27:03, 16.80s/it]

 98%|█████████▊| 15800/16110 [28:46:58<1:23:46, 16.21s/it]

 98%|█████████▊| 15801/16110 [28:47:18<1:28:55, 17.27s/it]


 98%|█████████▊| 15803/16110 [28:47:57<1:33:52, 18.35s/it]

 98%|█████████▊| 15804/16110 [28:48:09<1:24:09, 16.50s/it]

 98%|█████████▊| 15805/16110 [28:48:29<1:29:02, 17.52s/it]
{'loss': 0.0806, 'learning_rate': 1.4803153924465305e-06, 'rewards/chosen': -5.12723970413208, 'rewards/rejected': -10.624813079833984, 'rewards/accuracies': 0.875, 'rewards/margins': 5.4975738525390625, 'policy_logps/rejected': -435.98480224609375, 'policy_logps/chosen': -564.4388427734375, 'referece_logps/rejected': -329.7366943359375, 'referece_logps/chosen': -513.1664428710938, 'logits/rejected': 0.7686613202095032, 'logits/chosen': 0.6420938372612, 'epoch': 8.83}

 98%|█████████▊| 15806/16110 [28:48:48<1:30:57, 17.95s/it]

 98%|█████████▊| 15807/16110 [28:49:06<1:31:21, 18.09s/it]

 98%|█████████▊| 15808/16110 [28:49:20<1:23:50, 16.66s/it]


 98%|█████████▊| 15810/16110 [28:49:47<1:14:25, 14.88s/it]
{'loss': 0.1508, 'learning_rate': 1.479433453422295e-06, 'rewards/chosen': -5.079169750213623, 'rewards/rejected': -8.475481033325195, 'rewards/accuracies': 0.875, 'rewards/margins': 3.396310806274414, 'policy_logps/rejected': -352.754638671875, 'policy_logps/chosen': -319.63140869140625, 'referece_logps/rejected': -267.9998474121094, 'referece_logps/chosen': -268.8397216796875, 'logits/rejected': -0.05442051216959953, 'logits/chosen': -0.07388174533843994, 'epoch': 8.83}

 98%|█████████▊| 15811/16110 [28:50:06<1:19:54, 16.04s/it]

 98%|█████████▊| 15812/16110 [28:50:20<1:17:28, 15.60s/it]

 98%|█████████▊| 15813/16110 [28:50:38<1:21:11, 16.40s/it]

 98%|█████████▊| 15814/16110 [28:50:55<1:21:43, 16.56s/it]


 98%|█████████▊| 15816/16110 [28:51:25<1:18:00, 15.92s/it]

 98%|█████████▊| 15817/16110 [28:51:45<1:23:52, 17.17s/it]
{'loss': 0.105, 'learning_rate': 1.4781979250681399e-06, 'rewards/chosen': -5.105660438537598, 'rewards/rejected': -12.477961540222168, 'rewards/accuracies': 0.875, 'rewards/margins': 7.37230110168457, 'policy_logps/rejected': -614.0729370117188, 'policy_logps/chosen': -389.0171813964844, 'referece_logps/rejected': -489.2933349609375, 'referece_logps/chosen': -337.9606018066406, 'logits/rejected': -0.11106047034263611, 'logits/chosen': -0.10598982870578766, 'epoch': 8.84}

 98%|█████████▊| 15818/16110 [28:52:04<1:26:32, 17.78s/it]

 98%|█████████▊| 15819/16110 [28:52:18<1:20:34, 16.61s/it]

 98%|█████████▊| 15820/16110 [28:52:39<1:25:46, 17.75s/it]

 98%|█████████▊| 15821/16110 [28:52:59<1:28:41, 18.42s/it]

 98%|█████████▊| 15822/16110 [28:53:15<1:26:04, 17.93s/it]

 98%|█████████▊| 15823/16110 [28:53:30<1:21:33, 17.05s/it]

 98%|█████████▊| 15824/16110 [28:53:52<1:28:30, 18.57s/it]

 98%|█████████▊| 15825/16110 [28:54:12<1:30:13, 18.99s/it]

 98%|█████████▊| 15826/16110 [28:54:35<1:34:49, 20.03s/it]

 98%|█████████▊| 15827/16110 [28:54:54<1:33:08, 19.75s/it]


 98%|█████████▊| 15829/16110 [28:55:33<1:32:12, 19.69s/it]
{'loss': 0.1211, 'learning_rate': 1.4760776742996744e-06, 'rewards/chosen': -4.851985454559326, 'rewards/rejected': -10.259537696838379, 'rewards/accuracies': 1.0, 'rewards/margins': 5.4075517654418945, 'policy_logps/rejected': -347.3968505859375, 'policy_logps/chosen': -307.37310791015625, 'referece_logps/rejected': -244.80148315429688, 'referece_logps/chosen': -258.8532409667969, 'logits/rejected': -0.18303969502449036, 'logits/chosen': -0.31748244166374207, 'epoch': 8.84}


 98%|█████████▊| 15831/16110 [28:56:11<1:30:19, 19.43s/it]
{'loss': 0.1579, 'learning_rate': 1.475724029430535e-06, 'rewards/chosen': -5.032036781311035, 'rewards/rejected': -7.480073928833008, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4480371475219727, 'policy_logps/rejected': -404.4151916503906, 'policy_logps/chosen': -383.5959167480469, 'referece_logps/rejected': -329.61444091796875, 'referece_logps/chosen': -333.2755432128906, 'logits/rejected': 0.09324747323989868, 'logits/chosen': 0.11168966442346573, 'epoch': 8.84}

 98%|█████████▊| 15832/16110 [28:56:31<1:29:28, 19.31s/it]

 98%|█████████▊| 15833/16110 [28:56:44<1:20:54, 17.53s/it]

 98%|█████████▊| 15834/16110 [28:57:01<1:19:51, 17.36s/it]

 98%|█████████▊| 15835/16110 [28:57:23<1:25:47, 18.72s/it]

 98%|█████████▊| 15836/16110 [28:57:45<1:30:49, 19.89s/it]


 98%|█████████▊| 15838/16110 [28:58:26<1:30:39, 20.00s/it]
{'loss': 0.0582, 'learning_rate': 1.4744856670469201e-06, 'rewards/chosen': -3.3552756309509277, 'rewards/rejected': -8.769095420837402, 'rewards/accuracies': 1.0, 'rewards/margins': 5.413819789886475, 'policy_logps/rejected': -434.0184326171875, 'policy_logps/chosen': -310.74041748046875, 'referece_logps/rejected': -346.3275146484375, 'referece_logps/chosen': -277.18768310546875, 'logits/rejected': -0.0917125791311264, 'logits/chosen': -0.14016103744506836, 'epoch': 8.85}

 98%|█████████▊| 15839/16110 [28:58:39<1:21:38, 18.07s/it]

 98%|█████████▊| 15840/16110 [28:58:57<1:20:32, 17.90s/it]

 98%|█████████▊| 15841/16110 [28:59:18<1:25:28, 19.07s/it]

 98%|█████████▊| 15842/16110 [28:59:37<1:24:15, 18.87s/it]

 98%|█████████▊| 15843/16110 [28:59:59<1:28:10, 19.81s/it]

 98%|█████████▊| 15844/16110 [29:00:16<1:24:29, 19.06s/it]

 98%|█████████▊| 15845/16110 [29:00:35<1:23:33, 18.92s/it]

 98%|█████████▊| 15846/16110 [29:00:55<1:25:16, 19.38s/it]

 98%|█████████▊| 15847/16110 [29:01:15<1:25:05, 19.41s/it]

 98%|█████████▊| 15848/16110 [29:01:31<1:20:33, 18.45s/it]

 98%|█████████▊| 15849/16110 [29:01:50<1:21:28, 18.73s/it]

 98%|█████████▊| 15850/16110 [29:02:12<1:24:38, 19.53s/it]

 98%|█████████▊| 15851/16110 [29:02:28<1:19:39, 18.45s/it]

 98%|█████████▊| 15852/16110 [29:02:43<1:15:06, 17.47s/it]

 98%|█████████▊| 15853/16110 [29:02:59<1:13:22, 17.13s/it]

 98%|█████████▊| 15854/16110 [29:03:13<1:09:00, 16.17s/it]

 98%|█████████▊| 15855/16110 [29:03:31<1:10:56, 16.69s/it]


 98%|█████████▊| 15857/16110 [29:04:10<1:16:36, 18.17s/it]
{'loss': 0.0703, 'learning_rate': 1.4711196675654674e-06, 'rewards/chosen': -5.268118381500244, 'rewards/rejected': -12.13663101196289, 'rewards/accuracies': 1.0, 'rewards/margins': 6.868513107299805, 'policy_logps/rejected': -440.70001220703125, 'policy_logps/chosen': -361.65179443359375, 'referece_logps/rejected': -319.333740234375, 'referece_logps/chosen': -308.9705810546875, 'logits/rejected': 0.6382561922073364, 'logits/chosen': 0.4919871687889099, 'epoch': 8.86}

 98%|█████████▊| 15858/16110 [29:04:31<1:20:08, 19.08s/it]

 98%|█████████▊| 15859/16110 [29:04:51<1:20:57, 19.35s/it]

 98%|█████████▊| 15860/16110 [29:05:11<1:21:13, 19.49s/it]

 98%|█████████▊| 15861/16110 [29:05:31<1:21:26, 19.63s/it]

 98%|█████████▊| 15862/16110 [29:05:45<1:14:23, 18.00s/it]

 98%|█████████▊| 15863/16110 [29:06:04<1:14:44, 18.16s/it]

 98%|█████████▊| 15864/16110 [29:06:21<1:13:31, 17.93s/it]

 98%|█████████▊| 15865/16110 [29:06:38<1:11:53, 17.60s/it]

 98%|█████████▊| 15866/16110 [29:06:57<1:13:25, 18.05s/it]

 98%|█████████▊| 15867/16110 [29:07:19<1:17:52, 19.23s/it]

 98%|█████████▊| 15868/16110 [29:07:37<1:16:05, 18.87s/it]

 99%|█████████▊| 15869/16110 [29:07:59<1:19:41, 19.84s/it]

 99%|█████████▊| 15870/16110 [29:08:19<1:19:06, 19.78s/it]

 99%|█████████▊| 15871/16110 [29:08:39<1:19:43, 20.02s/it]

 99%|█████████▊| 15872/16110 [29:08:58<1:18:13, 19.72s/it]

 99%|█████████▊| 15873/16110 [29:09:19<1:19:26, 20.11s/it]

 99%|█████████▊| 15874/16110 [29:09:35<1:13:30, 18.69s/it]

 99%|█████████▊| 15875/16110 [29:09:55<1:15:17, 19.22s/it]

 99%|█████████▊| 15876/16110 [29:10:13<1:13:07, 18.75s/it]

 99%|█████████▊| 15877/16110 [29:10:31<1:12:25, 18.65s/it]

 99%|█████████▊| 15878/16110 [29:10:49<1:10:43, 18.29s/it]

 99%|█████████▊| 15879/16110 [29:11:04<1:06:46, 17.35s/it]

 99%|█████████▊| 15880/16110 [29:11:24<1:09:29, 18.13s/it]

 99%|█████████▊| 15881/16110 [29:11:44<1:11:02, 18.61s/it]

 99%|█████████▊| 15882/16110 [29:12:01<1:09:28, 18.28s/it]

 99%|█████████▊| 15883/16110 [29:12:12<1:01:12, 16.18s/it]

 99%|█████████▊| 15884/16110 [29:12:25<57:17, 15.21s/it]

 99%|█████████▊| 15885/16110 [29:12:46<1:03:42, 16.99s/it]

 99%|█████████▊| 15886/16110 [29:13:05<1:05:00, 17.41s/it]

 99%|█████████▊| 15887/16110 [29:13:21<1:03:37, 17.12s/it]

 99%|█████████▊| 15888/16110 [29:13:39<1:04:14, 17.36s/it]

 99%|█████████▊| 15889/16110 [29:13:54<1:01:37, 16.73s/it]

 99%|█████████▊| 15890/16110 [29:14:14<1:04:51, 17.69s/it]

 99%|█████████▊| 15891/16110 [29:14:33<1:05:36, 17.97s/it]

 99%|█████████▊| 15892/16110 [29:14:48<1:02:05, 17.09s/it]

 99%|█████████▊| 15893/16110 [29:15:01<57:03, 15.78s/it]

 99%|█████████▊| 15894/16110 [29:15:16<56:32, 15.71s/it]

 99%|█████████▊| 15895/16110 [29:15:34<58:43, 16.39s/it]

 99%|█████████▊| 15896/16110 [29:15:54<1:01:52, 17.35s/it]

 99%|█████████▊| 15897/16110 [29:16:07<56:47, 16.00s/it]

 99%|█████████▊| 15898/16110 [29:16:29<1:03:18, 17.92s/it]

 99%|█████████▊| 15899/16110 [29:16:47<1:03:06, 17.95s/it]

 99%|█████████▊| 15900/16110 [29:17:04<1:02:12, 17.77s/it]

 99%|█████████▊| 15901/16110 [29:17:25<1:04:49, 18.61s/it]

 99%|█████████▊| 15902/16110 [29:17:44<1:04:58, 18.74s/it]

 99%|█████████▊| 15903/16110 [29:18:04<1:05:59, 19.13s/it]

 99%|█████████▊| 15904/16110 [29:18:24<1:06:21, 19.33s/it]

 99%|█████████▊| 15905/16110 [29:18:45<1:08:21, 20.01s/it]

 99%|█████████▊| 15906/16110 [29:19:05<1:07:08, 19.75s/it]

 99%|█████████▊| 15907/16110 [29:19:24<1:06:30, 19.66s/it]

 99%|█████████▊| 15908/16110 [29:19:43<1:05:27, 19.44s/it]

 99%|█████████▉| 15909/16110 [29:19:55<57:34, 17.19s/it]

 99%|█████████▉| 15910/16110 [29:20:11<55:52, 16.76s/it]

 99%|█████████▉| 15911/16110 [29:20:27<55:12, 16.64s/it]

 99%|█████████▉| 15912/16110 [29:20:45<56:16, 17.05s/it]


 99%|█████████▉| 15914/16110 [29:21:18<53:48, 16.47s/it]

 99%|█████████▉| 15915/16110 [29:21:30<49:15, 15.16s/it]
{'loss': 0.0648, 'learning_rate': 1.4608021966965342e-06, 'rewards/chosen': -5.124563217163086, 'rewards/rejected': -11.890294075012207, 'rewards/accuracies': 1.0, 'rewards/margins': 6.7657294273376465, 'policy_logps/rejected': -344.7098693847656, 'policy_logps/chosen': -407.626708984375, 'referece_logps/rejected': -225.80694580078125, 'referece_logps/chosen': -356.38104248046875, 'logits/rejected': 0.17344878613948822, 'logits/chosen': 0.1728765368461609, 'epoch': 8.89}


 99%|█████████▉| 15917/16110 [29:22:03<51:46, 16.10s/it]

 99%|█████████▉| 15918/16110 [29:22:20<52:04, 16.27s/it]

 99%|█████████▉| 15919/16110 [29:22:39<54:17, 17.05s/it]

 99%|█████████▉| 15920/16110 [29:23:00<57:43, 18.23s/it]

 99%|█████████▉| 15921/16110 [29:23:16<55:21, 17.57s/it]

 99%|█████████▉| 15922/16110 [29:23:30<51:39, 16.49s/it]

 99%|█████████▉| 15923/16110 [29:23:45<49:39, 15.93s/it]

 99%|█████████▉| 15924/16110 [29:24:02<50:54, 16.42s/it]

 99%|█████████▉| 15925/16110 [29:24:16<47:57, 15.55s/it]
{'loss': 0.1541, 'learning_rate': 1.4590169504093642e-06, 'rewards/chosen': -3.8709335327148438, 'rewards/rejected': -9.526591300964355, 'rewards/accuracies': 0.875, 'rewards/margins': 5.655657768249512, 'policy_logps/rejected': -447.3610534667969, 'policy_logps/chosen': -393.1442565917969, 'referece_logps/rejected': -352.0951232910156, 'referece_logps/chosen': -354.4349670410156, 'logits/rejected': 0.27361416816711426, 'logits/chosen': 0.05550108104944229, 'epoch': 8.9}


 99%|█████████▉| 15927/16110 [29:24:54<52:38, 17.26s/it]

 99%|█████████▉| 15928/16110 [29:25:15<56:05, 18.49s/it]

 99%|█████████▉| 15929/16110 [29:25:32<54:09, 17.95s/it]

 99%|█████████▉| 15930/16110 [29:25:53<56:40, 18.89s/it]

 99%|█████████▉| 15931/16110 [29:26:09<54:02, 18.12s/it]

 99%|█████████▉| 15932/16110 [29:26:29<55:19, 18.65s/it]
{'loss': 0.1101, 'learning_rate': 1.4577661736280655e-06, 'rewards/chosen': -5.018067359924316, 'rewards/rejected': -11.459046363830566, 'rewards/accuracies': 1.0, 'rewards/margins': 6.44097900390625, 'policy_logps/rejected': -476.18701171875, 'policy_logps/chosen': -420.7326965332031, 'referece_logps/rejected': -361.5965576171875, 'referece_logps/chosen': -370.55206298828125, 'logits/rejected': -0.13234776258468628, 'logits/chosen': -0.3643825650215149, 'epoch': 8.9}


 99%|█████████▉| 15934/16110 [29:27:11<58:01, 19.78s/it]

 99%|█████████▉| 15935/16110 [29:27:27<54:41, 18.75s/it]

 99%|█████████▉| 15936/16110 [29:27:44<52:46, 18.20s/it]
{'loss': 0.1168, 'learning_rate': 1.4570510368085504e-06, 'rewards/chosen': -5.582501411437988, 'rewards/rejected': -9.248571395874023, 'rewards/accuracies': 0.875, 'rewards/margins': 3.666069507598877, 'policy_logps/rejected': -391.3026428222656, 'policy_logps/chosen': -365.77667236328125, 'referece_logps/rejected': -298.81695556640625, 'referece_logps/chosen': -309.95166015625, 'logits/rejected': 0.14059020578861237, 'logits/chosen': -0.029047459363937378, 'epoch': 8.9}


 99%|█████████▉| 15938/16110 [29:28:16<48:40, 16.98s/it]
{'loss': 0.1129, 'learning_rate': 1.456693357523878e-06, 'rewards/chosen': -5.336845397949219, 'rewards/rejected': -10.639165878295898, 'rewards/accuracies': 1.0, 'rewards/margins': 5.302321434020996, 'policy_logps/rejected': -399.38128662109375, 'policy_logps/chosen': -340.64410400390625, 'referece_logps/rejected': -292.9896240234375, 'referece_logps/chosen': -287.275634765625, 'logits/rejected': 0.5936094522476196, 'logits/chosen': 0.43520697951316833, 'epoch': 8.9}


 99%|█████████▉| 15940/16110 [29:28:51<48:56, 17.27s/it]
{'loss': 0.0808, 'learning_rate': 1.456335604399699e-06, 'rewards/chosen': -4.125360488891602, 'rewards/rejected': -9.403861045837402, 'rewards/accuracies': 1.0, 'rewards/margins': 5.278501510620117, 'policy_logps/rejected': -357.4088439941406, 'policy_logps/chosen': -458.7924499511719, 'referece_logps/rejected': -263.3702392578125, 'referece_logps/chosen': -417.53887939453125, 'logits/rejected': 0.3188215494155884, 'logits/chosen': 0.17065313458442688, 'epoch': 8.91}


 99%|█████████▉| 15942/16110 [29:29:29<51:39, 18.45s/it]

 99%|█████████▉| 15943/16110 [29:29:45<49:23, 17.75s/it]

 99%|█████████▉| 15944/16110 [29:29:57<44:07, 15.95s/it]
{'loss': 0.1476, 'learning_rate': 1.4556198768642031e-06, 'rewards/chosen': -6.355276107788086, 'rewards/rejected': -10.136347770690918, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7810721397399902, 'policy_logps/rejected': -432.4616394042969, 'policy_logps/chosen': -452.2996826171875, 'referece_logps/rejected': -331.09814453125, 'referece_logps/chosen': -388.74688720703125, 'logits/rejected': 0.5529981255531311, 'logits/chosen': 0.5775250792503357, 'epoch': 8.91}


 99%|█████████▉| 15946/16110 [29:30:26<41:03, 15.02s/it]
{'loss': 0.149, 'learning_rate': 1.4552619025686074e-06, 'rewards/chosen': -6.025121212005615, 'rewards/rejected': -11.177735328674316, 'rewards/accuracies': 1.0, 'rewards/margins': 5.152613162994385, 'policy_logps/rejected': -338.13128662109375, 'policy_logps/chosen': -295.279296875, 'referece_logps/rejected': -226.3539276123047, 'referece_logps/chosen': -235.028076171875, 'logits/rejected': 0.1532585322856903, 'logits/chosen': -0.008033757098019123, 'epoch': 8.91}


 99%|█████████▉| 15948/16110 [29:31:03<45:06, 16.71s/it]

 99%|█████████▉| 15949/16110 [29:31:16<41:52, 15.61s/it]

 99%|█████████▉| 15950/16110 [29:31:33<42:25, 15.91s/it]
{'loss': 0.1593, 'learning_rate': 1.4545457332111122e-06, 'rewards/chosen': -5.907339572906494, 'rewards/rejected': -10.40821361541748, 'rewards/accuracies': 0.875, 'rewards/margins': 4.500874042510986, 'policy_logps/rejected': -657.0385131835938, 'policy_logps/chosen': -529.6616821289062, 'referece_logps/rejected': -552.9563598632812, 'referece_logps/chosen': -470.5882568359375, 'logits/rejected': -0.3707907497882843, 'logits/chosen': -0.2163398414850235, 'epoch': 8.91}


 99%|█████████▉| 15952/16110 [29:32:05<42:28, 16.13s/it]

 99%|█████████▉| 15953/16110 [29:32:27<47:23, 18.11s/it]
{'loss': 0.1195, 'learning_rate': 1.4540084132504461e-06, 'rewards/chosen': -3.845479965209961, 'rewards/rejected': -11.713288307189941, 'rewards/accuracies': 1.0, 'rewards/margins': 7.867807865142822, 'policy_logps/rejected': -446.1207580566406, 'policy_logps/chosen': -476.61761474609375, 'referece_logps/rejected': -328.9878845214844, 'referece_logps/chosen': -438.1628112792969, 'logits/rejected': -0.08929336071014404, 'logits/chosen': -0.41918712854385376, 'epoch': 8.91}

 99%|█████████▉| 15954/16110 [29:32:40<42:58, 16.53s/it]


 99%|█████████▉| 15956/16110 [29:33:18<45:35, 17.76s/it]

 99%|█████████▉| 15957/16110 [29:33:38<46:55, 18.40s/it]

 99%|█████████▉| 15958/16110 [29:33:55<46:06, 18.20s/it]

 99%|█████████▉| 15959/16110 [29:34:12<44:34, 17.71s/it]

 99%|█████████▉| 15960/16110 [29:34:30<44:49, 17.93s/it]
{'loss': 0.0917, 'learning_rate': 1.4527540247163082e-06, 'rewards/chosen': -4.209722995758057, 'rewards/rejected': -10.627388954162598, 'rewards/accuracies': 1.0, 'rewards/margins': 6.417665958404541, 'policy_logps/rejected': -458.3853454589844, 'policy_logps/chosen': -468.2041931152344, 'referece_logps/rejected': -352.1114196777344, 'referece_logps/chosen': -426.1069641113281, 'logits/rejected': 0.6336350440979004, 'logits/chosen': 0.6328445076942444, 'epoch': 8.92}


 99%|█████████▉| 15962/16110 [29:34:59<39:57, 16.20s/it]

 99%|█████████▉| 15963/16110 [29:35:19<42:36, 17.39s/it]

 99%|█████████▉| 15964/16110 [29:35:36<41:54, 17.22s/it]

 99%|█████████▉| 15965/16110 [29:35:56<44:14, 18.31s/it]
{'loss': 0.1051, 'learning_rate': 1.451857483741977e-06, 'rewards/chosen': -4.104247093200684, 'rewards/rejected': -9.858148574829102, 'rewards/accuracies': 1.0, 'rewards/margins': 5.753900527954102, 'policy_logps/rejected': -361.2504577636719, 'policy_logps/chosen': -377.3625183105469, 'referece_logps/rejected': -262.6689453125, 'referece_logps/chosen': -336.3200378417969, 'logits/rejected': 0.48735132813453674, 'logits/chosen': 0.4711446166038513, 'epoch': 8.92}


 99%|█████████▉| 15967/16110 [29:36:36<45:42, 19.18s/it]

 99%|█████████▉| 15968/16110 [29:36:53<43:21, 18.32s/it]

 99%|█████████▉| 15969/16110 [29:37:13<44:10, 18.80s/it]

 99%|█████████▉| 15970/16110 [29:37:33<44:43, 19.17s/it]

 99%|█████████▉| 15971/16110 [29:37:48<41:34, 17.95s/it]

 99%|█████████▉| 15972/16110 [29:38:05<40:43, 17.71s/it]

 99%|█████████▉| 15973/16110 [29:38:25<42:01, 18.40s/it]
{'loss': 0.0815, 'learning_rate': 1.4504220688110011e-06, 'rewards/chosen': -5.442326545715332, 'rewards/rejected': -10.276270866394043, 'rewards/accuracies': 1.0, 'rewards/margins': 4.8339433670043945, 'policy_logps/rejected': -491.948974609375, 'policy_logps/chosen': -432.2719421386719, 'referece_logps/rejected': -389.18621826171875, 'referece_logps/chosen': -377.84869384765625, 'logits/rejected': 0.0890011116862297, 'logits/chosen': 0.09861258417367935, 'epoch': 8.92}


 99%|█████████▉| 15975/16110 [29:38:58<39:58, 17.76s/it]

 99%|█████████▉| 15976/16110 [29:39:18<41:03, 18.39s/it]

 99%|█████████▉| 15977/16110 [29:39:40<43:15, 19.51s/it]

 99%|█████████▉| 15978/16110 [29:40:00<43:03, 19.57s/it]

 99%|█████████▉| 15979/16110 [29:40:20<42:48, 19.61s/it]

 99%|█████████▉| 15980/16110 [29:40:39<42:37, 19.67s/it]

 99%|█████████▉| 15981/16110 [29:40:59<42:16, 19.66s/it]

 99%|█████████▉| 15982/16110 [29:41:17<41:09, 19.29s/it]
{'loss': 0.0775, 'learning_rate': 1.4488058344104173e-06, 'rewards/chosen': -7.52020263671875, 'rewards/rejected': -12.300596237182617, 'rewards/accuracies': 0.875, 'rewards/margins': 4.780394077301025, 'policy_logps/rejected': -501.4738464355469, 'policy_logps/chosen': -613.5421142578125, 'referece_logps/rejected': -378.46783447265625, 'referece_logps/chosen': -538.3401489257812, 'logits/rejected': 0.5587785840034485, 'logits/chosen': 0.397118479013443, 'epoch': 8.93}

 99%|█████████▉| 15983/16110 [29:41:35<39:26, 18.63s/it]


 99%|█████████▉| 15985/16110 [29:42:13<39:16, 18.85s/it]

 99%|█████████▉| 15986/16110 [29:42:29<37:19, 18.06s/it]

 99%|█████████▉| 15987/16110 [29:42:49<38:04, 18.58s/it]

 99%|█████████▉| 15988/16110 [29:43:04<35:27, 17.44s/it]

 99%|█████████▉| 15989/16110 [29:43:20<34:34, 17.14s/it]

 99%|█████████▉| 15990/16110 [29:43:38<34:34, 17.29s/it]

 99%|█████████▉| 15991/16110 [29:43:54<33:35, 16.94s/it]
{'loss': 0.069, 'learning_rate': 1.4471881305845777e-06, 'rewards/chosen': -4.662075996398926, 'rewards/rejected': -9.409971237182617, 'rewards/accuracies': 1.0, 'rewards/margins': 4.747895240783691, 'policy_logps/rejected': -255.74560546875, 'policy_logps/chosen': -326.1553039550781, 'referece_logps/rejected': -161.64588928222656, 'referece_logps/chosen': -279.5345764160156, 'logits/rejected': 0.6789401173591614, 'logits/chosen': 0.4726064205169678, 'epoch': 8.93}

 99%|█████████▉| 15992/16110 [29:44:15<35:53, 18.25s/it]

 99%|█████████▉| 15993/16110 [29:44:31<34:18, 17.60s/it]


 99%|█████████▉| 15995/16110 [29:45:00<29:42, 15.50s/it]

 99%|█████████▉| 15996/16110 [29:45:18<31:10, 16.40s/it]
{'loss': 0.1092, 'learning_rate': 1.4462887732494076e-06, 'rewards/chosen': -4.228832721710205, 'rewards/rejected': -8.047438621520996, 'rewards/accuracies': 1.0, 'rewards/margins': 3.818605899810791, 'policy_logps/rejected': -361.31390380859375, 'policy_logps/chosen': -330.8558349609375, 'referece_logps/rejected': -280.83953857421875, 'referece_logps/chosen': -288.5674743652344, 'logits/rejected': -0.3727107644081116, 'logits/chosen': -0.4108864665031433, 'epoch': 8.94}


 99%|█████████▉| 15998/16110 [29:45:45<27:20, 14.65s/it]
{'loss': 0.1243, 'learning_rate': 1.4459289039892285e-06, 'rewards/chosen': -5.273228645324707, 'rewards/rejected': -9.831513404846191, 'rewards/accuracies': 1.0, 'rewards/margins': 4.558284282684326, 'policy_logps/rejected': -406.20343017578125, 'policy_logps/chosen': -398.655029296875, 'referece_logps/rejected': -307.8883056640625, 'referece_logps/chosen': -345.9227294921875, 'logits/rejected': 0.41988787055015564, 'logits/chosen': 0.33604684472084045, 'epoch': 8.94}

 99%|█████████▉| 15999/16110 [29:45:57<25:41, 13.89s/it]


 99%|█████████▉| 16001/16110 [29:46:44<36:17, 19.97s/it]
{'loss': 0.1453, 'learning_rate': 1.445388964931374e-06, 'rewards/chosen': -4.953228950500488, 'rewards/rejected': -9.747562408447266, 'rewards/accuracies': 1.0, 'rewards/margins': 4.794332504272461, 'policy_logps/rejected': -335.9128112792969, 'policy_logps/chosen': -407.3854675292969, 'referece_logps/rejected': -238.43719482421875, 'referece_logps/chosen': -357.8531494140625, 'logits/rejected': -0.12344764173030853, 'logits/chosen': -0.46143046021461487, 'epoch': 8.94}

 99%|█████████▉| 16002/16110 [29:46:59<33:12, 18.45s/it]

 99%|█████████▉| 16003/16110 [29:47:19<33:51, 18.98s/it]


 99%|█████████▉| 16005/16110 [29:47:56<32:35, 18.62s/it]
{'loss': 0.1203, 'learning_rate': 1.444668794180302e-06, 'rewards/chosen': -4.349155902862549, 'rewards/rejected': -8.934432983398438, 'rewards/accuracies': 1.0, 'rewards/margins': 4.585276126861572, 'policy_logps/rejected': -519.830322265625, 'policy_logps/chosen': -383.724609375, 'referece_logps/rejected': -430.48602294921875, 'referece_logps/chosen': -340.2330322265625, 'logits/rejected': -1.0496821403503418, 'logits/chosen': -1.1149369478225708, 'epoch': 8.94}


 99%|█████████▉| 16007/16110 [29:48:35<32:48, 19.12s/it]
{'loss': 0.0722, 'learning_rate': 1.4443086009326427e-06, 'rewards/chosen': -4.520648002624512, 'rewards/rejected': -9.365303993225098, 'rewards/accuracies': 1.0, 'rewards/margins': 4.844655990600586, 'policy_logps/rejected': -343.2548522949219, 'policy_logps/chosen': -479.6498107910156, 'referece_logps/rejected': -249.60183715820312, 'referece_logps/chosen': -434.4433288574219, 'logits/rejected': 0.1524161994457245, 'logits/chosen': 0.05746285617351532, 'epoch': 8.94}

 99%|█████████▉| 16008/16110 [29:48:57<34:19, 20.19s/it]


 99%|█████████▉| 16010/16110 [29:49:37<32:48, 19.68s/it]
{'loss': 0.087, 'learning_rate': 1.4437681763847874e-06, 'rewards/chosen': -5.554157733917236, 'rewards/rejected': -9.908102035522461, 'rewards/accuracies': 0.875, 'rewards/margins': 4.353943824768066, 'policy_logps/rejected': -232.5251007080078, 'policy_logps/chosen': -406.57952880859375, 'referece_logps/rejected': -133.44407653808594, 'referece_logps/chosen': -351.0379333496094, 'logits/rejected': -0.09717357903718948, 'logits/chosen': -0.4164744019508362, 'epoch': 8.94}


 99%|█████████▉| 16012/16110 [29:50:07<28:40, 17.56s/it]
{'loss': 0.0953, 'learning_rate': 1.44340780365359e-06, 'rewards/chosen': -5.667078971862793, 'rewards/rejected': -10.921878814697266, 'rewards/accuracies': 1.0, 'rewards/margins': 5.254799842834473, 'policy_logps/rejected': -385.61199951171875, 'policy_logps/chosen': -507.7903747558594, 'referece_logps/rejected': -276.3932189941406, 'referece_logps/chosen': -451.11956787109375, 'logits/rejected': 0.3494277000427246, 'logits/chosen': 0.0920557975769043, 'epoch': 8.95}


 99%|█████████▉| 16014/16110 [29:50:46<29:29, 18.44s/it]

 99%|█████████▉| 16015/16110 [29:51:03<28:16, 17.86s/it]
{'loss': 0.0973, 'learning_rate': 1.4428671101535183e-06, 'rewards/chosen': -4.4870429039001465, 'rewards/rejected': -10.580122947692871, 'rewards/accuracies': 1.0, 'rewards/margins': 6.093080043792725, 'policy_logps/rejected': -515.16748046875, 'policy_logps/chosen': -607.1747436523438, 'referece_logps/rejected': -409.3662109375, 'referece_logps/chosen': -562.3043212890625, 'logits/rejected': 0.24576438963413239, 'logits/chosen': 0.2627490758895874, 'epoch': 8.95}

 99%|█████████▉| 16016/16110 [29:51:20<27:31, 17.57s/it]

 99%|█████████▉| 16017/16110 [29:51:40<28:24, 18.33s/it]


 99%|█████████▉| 16019/16110 [29:52:20<29:19, 19.33s/it]
{'loss': 0.1414, 'learning_rate': 1.4421459349066299e-06, 'rewards/chosen': -4.694179534912109, 'rewards/rejected': -10.433713912963867, 'rewards/accuracies': 1.0, 'rewards/margins': 5.739535331726074, 'policy_logps/rejected': -490.537109375, 'policy_logps/chosen': -405.91094970703125, 'referece_logps/rejected': -386.199951171875, 'referece_logps/chosen': -358.9691162109375, 'logits/rejected': 0.26971694827079773, 'logits/chosen': 0.46327587962150574, 'epoch': 8.95}

 99%|█████████▉| 16020/16110 [29:52:40<29:07, 19.42s/it]


 99%|█████████▉| 16022/16110 [29:53:11<26:09, 17.84s/it]
{'loss': 0.1114, 'learning_rate': 1.441604865791435e-06, 'rewards/chosen': -3.73113751411438, 'rewards/rejected': -9.751537322998047, 'rewards/accuracies': 1.0, 'rewards/margins': 6.020399570465088, 'policy_logps/rejected': -326.35003662109375, 'policy_logps/chosen': -490.2686462402344, 'referece_logps/rejected': -228.83465576171875, 'referece_logps/chosen': -452.957275390625, 'logits/rejected': 0.33613330125808716, 'logits/chosen': 0.0244656503200531, 'epoch': 8.95}


 99%|█████████▉| 16024/16110 [29:53:42<24:15, 16.93s/it]
{'loss': 0.1144, 'learning_rate': 1.4412440637858767e-06, 'rewards/chosen': -5.986727237701416, 'rewards/rejected': -11.634113311767578, 'rewards/accuracies': 1.0, 'rewards/margins': 5.64738655090332, 'policy_logps/rejected': -437.83221435546875, 'policy_logps/chosen': -300.56536865234375, 'referece_logps/rejected': -321.4910888671875, 'referece_logps/chosen': -240.69808959960938, 'logits/rejected': 0.30309486389160156, 'logits/chosen': 0.36109063029289246, 'epoch': 8.95}

 99%|█████████▉| 16025/16110 [29:54:01<25:01, 17.66s/it]


 99%|█████████▉| 16027/16110 [29:54:43<26:17, 19.00s/it]
{'loss': 0.1203, 'learning_rate': 1.4407027270302343e-06, 'rewards/chosen': -4.696298599243164, 'rewards/rejected': -10.405014038085938, 'rewards/accuracies': 1.0, 'rewards/margins': 5.708715915679932, 'policy_logps/rejected': -402.6907958984375, 'policy_logps/chosen': -326.29412841796875, 'referece_logps/rejected': -298.64068603515625, 'referece_logps/chosen': -279.33111572265625, 'logits/rejected': 0.5542886257171631, 'logits/chosen': 0.7273922562599182, 'epoch': 8.95}


 99%|█████████▉| 16029/16110 [29:55:18<24:43, 18.31s/it]
{'loss': 0.1878, 'learning_rate': 1.4403417467800303e-06, 'rewards/chosen': -4.745294094085693, 'rewards/rejected': -9.520317077636719, 'rewards/accuracies': 1.0, 'rewards/margins': 4.775022506713867, 'policy_logps/rejected': -402.48394775390625, 'policy_logps/chosen': -446.26116943359375, 'referece_logps/rejected': -307.28076171875, 'referece_logps/chosen': -398.8082580566406, 'logits/rejected': 0.26857131719589233, 'logits/chosen': 0.27992182970046997, 'epoch': 8.95}

100%|█████████▉| 16030/16110 [29:55:40<25:36, 19.21s/it]

100%|█████████▉| 16031/16110 [29:55:58<24:51, 18.87s/it]

100%|█████████▉| 16032/16110 [29:56:16<24:13, 18.63s/it]

100%|█████████▉| 16033/16110 [29:56:29<22:01, 17.16s/it]


100%|█████████▉| 16035/16110 [29:57:03<20:44, 16.59s/it]
{'loss': 0.1618, 'learning_rate': 1.4392583790885486e-06, 'rewards/chosen': -5.465800762176514, 'rewards/rejected': -10.711610794067383, 'rewards/accuracies': 1.0, 'rewards/margins': 5.245809555053711, 'policy_logps/rejected': -309.02105712890625, 'policy_logps/chosen': -266.7720642089844, 'referece_logps/rejected': -201.9049530029297, 'referece_logps/chosen': -212.1140594482422, 'logits/rejected': -0.15714909136295319, 'logits/chosen': -0.28087642788887024, 'epoch': 8.96}


100%|█████████▉| 16037/16110 [29:57:31<18:28, 15.19s/it]

100%|█████████▉| 16038/16110 [29:57:47<18:29, 15.41s/it]

100%|█████████▉| 16039/16110 [29:58:02<18:19, 15.49s/it]

100%|█████████▉| 16040/16110 [29:58:20<18:54, 16.21s/it]

100%|█████████▉| 16041/16110 [29:58:41<20:04, 17.46s/it]

100%|█████████▉| 16042/16110 [29:59:00<20:32, 18.12s/it]

100%|█████████▉| 16043/16110 [29:59:17<19:47, 17.72s/it]
{'loss': 0.0753, 'learning_rate': 1.437812894817993e-06, 'rewards/chosen': -4.513785362243652, 'rewards/rejected': -7.74229621887207, 'rewards/accuracies': 1.0, 'rewards/margins': 3.228511095046997, 'policy_logps/rejected': -347.8846435546875, 'policy_logps/chosen': -278.21722412109375, 'referece_logps/rejected': -270.461669921875, 'referece_logps/chosen': -233.07937622070312, 'logits/rejected': -0.520902156829834, 'logits/chosen': -0.3671223223209381, 'epoch': 8.96}

100%|█████████▉| 16044/16110 [29:59:32<18:31, 16.85s/it]


100%|█████████▉| 16046/16110 [30:00:11<19:31, 18.31s/it]
{'loss': 0.098, 'learning_rate': 1.4372705460198643e-06, 'rewards/chosen': -6.007754802703857, 'rewards/rejected': -12.103103637695312, 'rewards/accuracies': 1.0, 'rewards/margins': 6.0953474044799805, 'policy_logps/rejected': -439.3594055175781, 'policy_logps/chosen': -522.9801025390625, 'referece_logps/rejected': -318.328369140625, 'referece_logps/chosen': -462.9025573730469, 'logits/rejected': -0.4139806926250458, 'logits/chosen': -0.6070094108581543, 'epoch': 8.96}


100%|█████████▉| 16048/16110 [30:00:41<16:41, 16.16s/it]

100%|█████████▉| 16049/16110 [30:00:52<14:45, 14.51s/it]

100%|█████████▉| 16050/16110 [30:01:13<16:38, 16.65s/it]

100%|█████████▉| 16051/16110 [30:01:27<15:26, 15.70s/it]

100%|█████████▉| 16052/16110 [30:01:45<15:56, 16.50s/it]

100%|█████████▉| 16053/16110 [30:01:57<14:20, 15.10s/it]
{'loss': 0.1103, 'learning_rate': 1.4360044472142875e-06, 'rewards/chosen': -4.571748733520508, 'rewards/rejected': -10.326858520507812, 'rewards/accuracies': 1.0, 'rewards/margins': 5.755110740661621, 'policy_logps/rejected': -283.33392333984375, 'policy_logps/chosen': -493.44580078125, 'referece_logps/rejected': -180.0653533935547, 'referece_logps/chosen': -447.7283020019531, 'logits/rejected': 0.020844392478466034, 'logits/chosen': -0.16079212725162506, 'epoch': 8.97}


100%|█████████▉| 16055/16110 [30:02:31<14:29, 15.80s/it]

100%|█████████▉| 16056/16110 [30:02:46<13:56, 15.49s/it]

100%|█████████▉| 16057/16110 [30:03:05<14:49, 16.79s/it]

100%|█████████▉| 16058/16110 [30:03:22<14:25, 16.64s/it]

100%|█████████▉| 16059/16110 [30:03:41<14:56, 17.58s/it]

100%|█████████▉| 16060/16110 [30:03:54<13:17, 15.94s/it]
{'loss': 0.1773, 'learning_rate': 1.4347374848516637e-06, 'rewards/chosen': -4.742567539215088, 'rewards/rejected': -9.503451347351074, 'rewards/accuracies': 0.75, 'rewards/margins': 4.760883331298828, 'policy_logps/rejected': -376.54718017578125, 'policy_logps/chosen': -357.9400939941406, 'referece_logps/rejected': -281.5126953125, 'referece_logps/chosen': -310.514404296875, 'logits/rejected': -0.34360450506210327, 'logits/chosen': -0.30382218956947327, 'epoch': 8.97}


100%|█████████▉| 16062/16110 [30:04:19<11:28, 14.33s/it]

100%|█████████▉| 16063/16110 [30:04:36<11:42, 14.94s/it]
{'loss': 0.1447, 'learning_rate': 1.4341942372495634e-06, 'rewards/chosen': -4.868411064147949, 'rewards/rejected': -9.02547550201416, 'rewards/accuracies': 0.875, 'rewards/margins': 4.157063961029053, 'policy_logps/rejected': -511.2935485839844, 'policy_logps/chosen': -398.9342346191406, 'referece_logps/rejected': -421.038818359375, 'referece_logps/chosen': -350.2500915527344, 'logits/rejected': -0.13642658293247223, 'logits/chosen': -0.13878822326660156, 'epoch': 8.97}


100%|█████████▉| 16065/16110 [30:04:59<09:55, 13.23s/it]
{'loss': 0.053, 'learning_rate': 1.4338319844170673e-06, 'rewards/chosen': -4.638776779174805, 'rewards/rejected': -12.712894439697266, 'rewards/accuracies': 1.0, 'rewards/margins': 8.074117660522461, 'policy_logps/rejected': -522.6744995117188, 'policy_logps/chosen': -393.7761535644531, 'referece_logps/rejected': -395.5455627441406, 'referece_logps/chosen': -347.3883972167969, 'logits/rejected': -0.02538430131971836, 'logits/chosen': 0.07042285799980164, 'epoch': 8.97}

100%|█████████▉| 16066/16110 [30:05:15<10:12, 13.92s/it]

100%|█████████▉| 16067/16110 [30:05:26<09:29, 13.23s/it]

100%|█████████▉| 16068/16110 [30:05:38<09:03, 12.94s/it]


100%|█████████▉| 16070/16110 [30:06:13<09:55, 14.89s/it]

100%|█████████▉| 16071/16110 [30:06:29<09:55, 15.27s/it]

100%|█████████▉| 16072/16110 [30:06:46<09:57, 15.72s/it]

100%|█████████▉| 16073/16110 [30:07:03<10:01, 16.25s/it]

100%|█████████▉| 16074/16110 [30:07:23<10:27, 17.43s/it]
{'loss': 0.0728, 'learning_rate': 1.4322009794944382e-06, 'rewards/chosen': -6.040603160858154, 'rewards/rejected': -11.264805793762207, 'rewards/accuracies': 1.0, 'rewards/margins': 5.224202632904053, 'policy_logps/rejected': -389.2545166015625, 'policy_logps/chosen': -380.57708740234375, 'referece_logps/rejected': -276.6064147949219, 'referece_logps/chosen': -320.1710205078125, 'logits/rejected': 0.2824421525001526, 'logits/chosen': 0.5619206428527832, 'epoch': 8.98}

100%|█████████▉| 16075/16110 [30:07:34<08:59, 15.43s/it]


100%|█████████▉| 16077/16110 [30:07:59<07:36, 13.84s/it]

100%|█████████▉| 16078/16110 [30:08:12<07:09, 13.41s/it]
{'loss': 0.1319, 'learning_rate': 1.4314756338834395e-06, 'rewards/chosen': -5.551407814025879, 'rewards/rejected': -12.347643852233887, 'rewards/accuracies': 1.0, 'rewards/margins': 6.796236515045166, 'policy_logps/rejected': -468.1506652832031, 'policy_logps/chosen': -402.9532470703125, 'referece_logps/rejected': -344.6742248535156, 'referece_logps/chosen': -347.4391784667969, 'logits/rejected': 0.20835605263710022, 'logits/chosen': 0.07494223117828369, 'epoch': 8.98}

100%|█████████▉| 16079/16110 [30:08:30<07:41, 14.88s/it]


100%|█████████▉| 16081/16110 [30:09:01<07:35, 15.70s/it]

100%|█████████▉| 16082/16110 [30:09:21<07:53, 16.90s/it]
{'loss': 0.1087, 'learning_rate': 1.4307500092235243e-06, 'rewards/chosen': -5.3456854820251465, 'rewards/rejected': -11.301276206970215, 'rewards/accuracies': 1.0, 'rewards/margins': 5.955590724945068, 'policy_logps/rejected': -362.9422302246094, 'policy_logps/chosen': -406.315185546875, 'referece_logps/rejected': -249.92950439453125, 'referece_logps/chosen': -352.85833740234375, 'logits/rejected': 0.5786341428756714, 'logits/chosen': 0.5261512398719788, 'epoch': 8.98}


100%|█████████▉| 16084/16110 [30:10:01<08:02, 18.56s/it]
{'loss': 0.2056, 'learning_rate': 1.4303870923968674e-06, 'rewards/chosen': -3.376077890396118, 'rewards/rejected': -8.867366790771484, 'rewards/accuracies': 1.0, 'rewards/margins': 5.491289138793945, 'policy_logps/rejected': -300.59417724609375, 'policy_logps/chosen': -228.68711853027344, 'referece_logps/rejected': -211.92051696777344, 'referece_logps/chosen': -194.92636108398438, 'logits/rejected': -0.8069924712181091, 'logits/chosen': -0.7948226928710938, 'epoch': 8.99}

100%|█████████▉| 16085/16110 [30:10:20<07:48, 18.76s/it]


100%|█████████▉| 16087/16110 [30:10:56<06:52, 17.94s/it]

100%|█████████▉| 16088/16110 [30:11:14<06:39, 18.15s/it]

100%|█████████▉| 16089/16110 [30:11:34<06:28, 18.49s/it]
{'loss': 0.046, 'learning_rate': 1.429479496018839e-06, 'rewards/chosen': -4.3470916748046875, 'rewards/rejected': -8.785120964050293, 'rewards/accuracies': 1.0, 'rewards/margins': 4.4380292892456055, 'policy_logps/rejected': -375.20452880859375, 'policy_logps/chosen': -397.8633728027344, 'referece_logps/rejected': -287.3533020019531, 'referece_logps/chosen': -354.3924560546875, 'logits/rejected': 0.3165663778781891, 'logits/chosen': 0.05980899930000305, 'epoch': 8.99}


100%|█████████▉| 16091/16110 [30:12:04<05:27, 17.26s/it]

100%|█████████▉| 16092/16110 [30:12:24<05:24, 18.03s/it]

100%|█████████▉| 16093/16110 [30:12:42<05:07, 18.07s/it]

100%|█████████▉| 16094/16110 [30:13:03<05:00, 18.77s/it]
{'loss': 0.066, 'learning_rate': 1.4285714656440257e-06, 'rewards/chosen': -5.1588006019592285, 'rewards/rejected': -9.388261795043945, 'rewards/accuracies': 1.0, 'rewards/margins': 4.229461193084717, 'policy_logps/rejected': -353.03594970703125, 'policy_logps/chosen': -408.17950439453125, 'referece_logps/rejected': -259.1532897949219, 'referece_logps/chosen': -356.5914611816406, 'logits/rejected': 0.09090592712163925, 'logits/chosen': -0.04025407135486603, 'epoch': 8.99}


100%|█████████▉| 16096/16110 [30:13:37<04:11, 17.99s/it]
{'loss': 0.2195, 'learning_rate': 1.4282081321805241e-06, 'rewards/chosen': -3.1709234714508057, 'rewards/rejected': -8.206034660339355, 'rewards/accuracies': 1.0, 'rewards/margins': 5.035110950469971, 'policy_logps/rejected': -265.7377624511719, 'policy_logps/chosen': -218.58929443359375, 'referece_logps/rejected': -183.6774444580078, 'referece_logps/chosen': -186.88006591796875, 'logits/rejected': 0.2930046319961548, 'logits/chosen': 0.4402637481689453, 'epoch': 8.99}


100%|█████████▉| 16098/16110 [30:14:14<03:35, 17.98s/it]

100%|█████████▉| 16099/16110 [30:14:29<03:06, 16.93s/it]

100%|█████████▉| 16100/16110 [30:14:48<02:57, 17.73s/it]

100%|█████████▉| 16101/16110 [30:15:06<02:39, 17.72s/it]

100%|█████████▉| 16102/16110 [30:15:20<02:12, 16.61s/it]

100%|█████████▉| 16103/16110 [30:15:38<01:59, 17.12s/it]
{'loss': 0.1322, 'learning_rate': 1.4269359202266617e-06, 'rewards/chosen': -4.498318672180176, 'rewards/rejected': -10.15541934967041, 'rewards/accuracies': 1.0, 'rewards/margins': 5.657101154327393, 'policy_logps/rejected': -530.1146240234375, 'policy_logps/chosen': -454.46234130859375, 'referece_logps/rejected': -428.56036376953125, 'referece_logps/chosen': -409.4791259765625, 'logits/rejected': -0.14861205220222473, 'logits/chosen': -0.2260805368423462, 'epoch': 9.0}


100%|█████████▉| 16105/16110 [30:16:14<01:27, 17.41s/it]

100%|█████████▉| 16106/16110 [30:16:27<01:04, 16.06s/it]
{'loss': 0.0812, 'learning_rate': 1.4263904275293123e-06, 'rewards/chosen': -3.6679015159606934, 'rewards/rejected': -9.603055953979492, 'rewards/accuracies': 1.0, 'rewards/margins': 5.935155391693115, 'policy_logps/rejected': -382.3682861328125, 'policy_logps/chosen': -325.1632080078125, 'referece_logps/rejected': -286.3377380371094, 'referece_logps/chosen': -288.48419189453125, 'logits/rejected': -0.24650390446186066, 'logits/chosen': -0.31548407673835754, 'epoch': 9.0}


100%|█████████▉| 16108/16110 [30:16:55<00:29, 14.64s/it]
{'loss': 0.1237, 'learning_rate': 1.4260266795437779e-06, 'rewards/chosen': -5.2007365226745605, 'rewards/rejected': -9.505095481872559, 'rewards/accuracies': 0.875, 'rewards/margins': 4.304358005523682, 'policy_logps/rejected': -469.0265808105469, 'policy_logps/chosen': -311.48095703125, 'referece_logps/rejected': -373.97564697265625, 'referece_logps/chosen': -259.47357177734375, 'logits/rejected': 0.005165994167327881, 'logits/chosen': 0.04704974219202995, 'epoch': 9.0}

100%|█████████▉| 16109/16110 [30:17:14<00:16, 16.02s/it]
{'loss': 0.1296, 'learning_rate': 1.425662862677014e-06, 'rewards/chosen': -4.739458084106445, 'rewards/rejected': -12.417305946350098, 'rewards/accuracies': 1.0, 'rewards/margins': 7.677847862243652, 'policy_logps/rejected': -446.3124084472656, 'policy_logps/chosen': -552.6354370117188, 'referece_logps/rejected': -322.1393127441406, 'referece_logps/chosen': -505.2408447265625, 'logits/rejected': 0.8366045951843262, 'logits/chosen': 0.7586231827735901, 'epoch': 9.0}

100%|██████████| 16110/16110 [30:17:32<00:00,  6.77s/it]