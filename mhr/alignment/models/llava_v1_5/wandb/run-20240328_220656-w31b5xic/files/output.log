  0%|          | 0/2685 [00:00<?, ?it/s]/mnt/petrelfs/songmingyang/code/mm/MAPO/mhr/alignment/trainer/llava_dpo_trainer.py:179: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/2685 [00:22<16:44:46, 22.46s/it]
{'loss': 0.6931, 'learning_rate': 2.4691358024691355e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'policy_logps/rejected': -342.7065124511719, 'policy_logps/chosen': -338.5207824707031, 'referece_logps/rejected': -342.7065124511719, 'referece_logps/chosen': -338.5207824707031, 'logits/rejected': 0.2454776018857956, 'logits/chosen': 0.14770741760730743, 'epoch': 0.0}
[2024-03-28 22:07:48,052] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 2/2685 [00:44<16:44:06, 22.46s/it]
[2024-03-28 22:08:11,552] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  0%|          | 3/2685 [01:08<17:05:03, 22.93s/it]

  0%|          | 4/2685 [01:23<14:49:54, 19.92s/it]

  0%|          | 5/2685 [01:48<16:12:10, 21.77s/it]

  0%|          | 6/2685 [02:05<14:58:10, 20.12s/it]


  0%|          | 8/2685 [02:46<15:03:24, 20.25s/it]
{'loss': 0.6908, 'learning_rate': 1.9753086419753084e-07, 'rewards/chosen': 0.0020696637220680714, 'rewards/rejected': -0.016161536797881126, 'rewards/accuracies': 0.75, 'rewards/margins': 0.01823120377957821, 'policy_logps/rejected': -389.2480163574219, 'policy_logps/chosen': -358.81451416015625, 'referece_logps/rejected': -389.0863952636719, 'referece_logps/chosen': -358.83514404296875, 'logits/rejected': -0.7904117107391357, 'logits/chosen': -0.8211640119552612, 'epoch': 0.01}

  0%|          | 9/2685 [03:05<14:37:31, 19.68s/it]

  0%|          | 10/2685 [03:20<13:37:09, 18.33s/it]

  0%|          | 11/2685 [03:40<13:54:19, 18.72s/it]


  0%|          | 13/2685 [04:22<14:53:18, 20.06s/it]

  1%|          | 14/2685 [04:44<15:21:58, 20.71s/it]

  1%|          | 15/2685 [05:00<14:21:15, 19.35s/it]

  1%|          | 16/2685 [05:22<14:52:42, 20.07s/it]

  1%|          | 17/2685 [05:43<15:07:13, 20.40s/it]

  1%|          | 18/2685 [06:01<14:37:36, 19.74s/it]

  1%|          | 19/2685 [06:18<13:57:35, 18.85s/it]

  1%|          | 20/2685 [06:40<14:34:44, 19.69s/it]
[2024-03-28 22:13:43,447] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 21/2685 [06:59<14:25:27, 19.49s/it]
[2024-03-28 22:14:02,470] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|          | 22/2685 [07:20<14:52:54, 20.12s/it]

  1%|          | 23/2685 [07:41<14:58:21, 20.25s/it]

  1%|          | 24/2685 [07:59<14:30:33, 19.63s/it]

  1%|          | 25/2685 [08:19<14:35:24, 19.75s/it]

  1%|          | 26/2685 [08:38<14:21:08, 19.43s/it]

  1%|          | 27/2685 [08:57<14:11:37, 19.22s/it]

  1%|          | 28/2685 [09:21<15:20:30, 20.79s/it]

  1%|          | 29/2685 [09:39<14:48:30, 20.07s/it]

  1%|          | 30/2685 [10:02<15:14:53, 20.68s/it]

  1%|          | 31/2685 [10:22<15:06:21, 20.49s/it]

  1%|          | 32/2685 [10:43<15:23:06, 20.88s/it]

  1%|          | 33/2685 [11:02<14:52:36, 20.19s/it]

  1%|▏         | 34/2685 [11:23<15:01:06, 20.39s/it]

  1%|▏         | 35/2685 [11:44<15:16:12, 20.74s/it]
[2024-03-28 22:18:48,023] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|▏         | 36/2685 [12:05<15:13:28, 20.69s/it]
[2024-03-28 22:19:08,587] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  1%|▏         | 37/2685 [12:27<15:29:24, 21.06s/it]

  1%|▏         | 38/2685 [12:46<15:09:57, 20.63s/it]

  1%|▏         | 39/2685 [13:07<15:03:45, 20.49s/it]

  1%|▏         | 40/2685 [13:27<15:02:26, 20.47s/it]

  2%|▏         | 41/2685 [13:47<14:49:45, 20.19s/it]

  2%|▏         | 42/2685 [14:05<14:21:28, 19.56s/it]

  2%|▏         | 43/2685 [14:25<14:35:52, 19.89s/it]
[2024-03-28 22:21:29,011] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 44/2685 [14:46<14:40:59, 20.01s/it]
[2024-03-28 22:21:49,315] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 45/2685 [15:06<14:43:23, 20.08s/it]

  2%|▏         | 46/2685 [15:29<15:24:03, 21.01s/it]
[2024-03-28 22:22:32,721] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 47/2685 [15:52<15:47:51, 21.56s/it]

  2%|▏         | 48/2685 [16:10<14:59:51, 20.47s/it]

  2%|▏         | 49/2685 [16:31<15:02:25, 20.54s/it]

  2%|▏         | 50/2685 [16:51<15:00:05, 20.50s/it]

  2%|▏         | 51/2685 [17:12<15:06:37, 20.65s/it]

  2%|▏         | 52/2685 [17:32<15:03:13, 20.58s/it]

  2%|▏         | 53/2685 [17:55<15:34:47, 21.31s/it]
[2024-03-28 22:24:59,036] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 54/2685 [18:12<14:32:13, 19.89s/it]

  2%|▏         | 55/2685 [18:26<13:09:13, 18.01s/it]

  2%|▏         | 56/2685 [18:47<13:47:51, 18.89s/it]

  2%|▏         | 57/2685 [19:09<14:31:41, 19.90s/it]

  2%|▏         | 58/2685 [19:31<14:58:49, 20.53s/it]

  2%|▏         | 59/2685 [19:54<15:37:26, 21.42s/it]
[2024-03-28 22:26:57,931] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 60/2685 [20:12<14:54:33, 20.45s/it]

  2%|▏         | 61/2685 [20:32<14:44:06, 20.22s/it]

  2%|▏         | 62/2685 [20:53<14:46:28, 20.28s/it]

  2%|▏         | 63/2685 [21:13<14:54:37, 20.47s/it]
[2024-03-28 22:28:17,133] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 64/2685 [21:34<14:55:57, 20.51s/it]
[2024-03-28 22:28:37,734] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  2%|▏         | 65/2685 [21:53<14:35:54, 20.06s/it]

  2%|▏         | 66/2685 [22:16<15:08:00, 20.80s/it]

  2%|▏         | 67/2685 [22:34<14:33:58, 20.03s/it]

  3%|▎         | 68/2685 [22:57<15:15:16, 20.98s/it]

  3%|▎         | 69/2685 [23:12<14:01:24, 19.30s/it]

  3%|▎         | 70/2685 [23:30<13:37:28, 18.76s/it]

  3%|▎         | 71/2685 [23:51<14:07:40, 19.46s/it]
[2024-03-28 22:30:54,663] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 72/2685 [24:07<13:22:57, 18.44s/it]

  3%|▎         | 73/2685 [24:28<13:59:14, 19.28s/it]

  3%|▎         | 74/2685 [24:49<14:15:38, 19.66s/it]

  3%|▎         | 75/2685 [25:09<14:15:49, 19.67s/it]

  3%|▎         | 76/2685 [25:29<14:26:26, 19.93s/it]

  3%|▎         | 77/2685 [25:50<14:43:55, 20.34s/it]

  3%|▎         | 78/2685 [26:13<15:07:44, 20.89s/it]

  3%|▎         | 79/2685 [26:33<14:54:49, 20.60s/it]

  3%|▎         | 80/2685 [26:51<14:20:34, 19.82s/it]

  3%|▎         | 81/2685 [27:11<14:26:38, 19.97s/it]

  3%|▎         | 82/2685 [27:32<14:47:51, 20.47s/it]
[2024-03-28 22:34:36,079] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 83/2685 [27:53<14:47:51, 20.47s/it]

  3%|▎         | 84/2685 [28:15<15:08:52, 20.97s/it]
[2024-03-28 22:35:18,687] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 85/2685 [28:35<14:57:57, 20.72s/it]

  3%|▎         | 86/2685 [28:58<15:26:37, 21.39s/it]
[2024-03-28 22:36:01,794] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 87/2685 [29:20<15:28:41, 21.45s/it]
[2024-03-28 22:36:23,373] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  3%|▎         | 88/2685 [29:39<14:59:31, 20.78s/it]

  3%|▎         | 89/2685 [30:00<15:00:05, 20.80s/it]

  3%|▎         | 90/2685 [30:21<15:10:29, 21.05s/it]

  3%|▎         | 91/2685 [30:43<15:13:34, 21.13s/it]

  3%|▎         | 92/2685 [31:00<14:16:57, 19.83s/it]

  3%|▎         | 93/2685 [31:17<13:43:48, 19.07s/it]
{'loss': 0.6447, 'learning_rate': 1.9998952044849375e-06, 'rewards/chosen': 0.06261520087718964, 'rewards/rejected': 0.03127555921673775, 'rewards/accuracies': 0.625, 'rewards/margins': 0.03133963793516159, 'policy_logps/rejected': -463.76007080078125, 'policy_logps/chosen': -440.134033203125, 'referece_logps/rejected': -464.0728454589844, 'referece_logps/chosen': -440.7602233886719, 'logits/rejected': -0.06131863594055176, 'logits/chosen': -0.1445096731185913, 'epoch': 0.1}


  4%|▎         | 95/2685 [31:58<14:08:32, 19.66s/it]

  4%|▎         | 96/2685 [32:19<14:30:47, 20.18s/it]

  4%|▎         | 97/2685 [32:40<14:37:35, 20.35s/it]
[2024-03-28 22:39:43,460] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6161, 'learning_rate': 1.9998136993930044e-06, 'rewards/chosen': 0.033387091010808945, 'rewards/rejected': -0.23558273911476135, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2689698338508606, 'policy_logps/rejected': -350.2713623046875, 'policy_logps/chosen': -207.04400634765625, 'referece_logps/rejected': -347.91552734375, 'referece_logps/chosen': -207.37786865234375, 'logits/rejected': -0.023092634975910187, 'logits/chosen': -0.03717610985040665, 'epoch': 0.11}


  4%|▎         | 99/2685 [33:22<14:51:19, 20.68s/it]
{'loss': 0.5985, 'learning_rate': 1.9997642152390312e-06, 'rewards/chosen': 0.15820196270942688, 'rewards/rejected': -0.08083362132310867, 'rewards/accuracies': 0.75, 'rewards/margins': 0.23903560638427734, 'policy_logps/rejected': -266.799072265625, 'policy_logps/chosen': -344.84405517578125, 'referece_logps/rejected': -265.99072265625, 'referece_logps/chosen': -346.42608642578125, 'logits/rejected': -0.8109655380249023, 'logits/chosen': -1.0243980884552002, 'epoch': 0.11}


  4%|▍         | 101/2685 [34:03<14:55:35, 20.80s/it]

  4%|▍         | 102/2685 [34:23<14:49:27, 20.66s/it]
[2024-03-28 22:41:26,946] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 103/2685 [34:45<15:04:09, 21.01s/it]

  4%|▍         | 104/2685 [35:03<14:22:55, 20.06s/it]

  4%|▍         | 105/2685 [35:22<14:06:50, 19.69s/it]

  4%|▍         | 106/2685 [35:43<14:27:17, 20.18s/it]
[2024-03-28 22:42:46,759] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6182, 'learning_rate': 1.999545184890626e-06, 'rewards/chosen': -0.22102336585521698, 'rewards/rejected': -0.468121737241745, 'rewards/accuracies': 0.5, 'rewards/margins': 0.24709835648536682, 'policy_logps/rejected': -335.8604431152344, 'policy_logps/chosen': -454.9570007324219, 'referece_logps/rejected': -331.1792297363281, 'referece_logps/chosen': -452.7467956542969, 'logits/rejected': 0.4059383273124695, 'logits/chosen': 0.5342771410942078, 'epoch': 0.12}


  4%|▍         | 108/2685 [36:25<14:43:58, 20.58s/it]
{'loss': 0.63, 'learning_rate': 1.9994695103484374e-06, 'rewards/chosen': 0.021617695689201355, 'rewards/rejected': -0.17892053723335266, 'rewards/accuracies': 0.625, 'rewards/margins': 0.20053823292255402, 'policy_logps/rejected': -286.75250244140625, 'policy_logps/chosen': -251.59959411621094, 'referece_logps/rejected': -284.96331787109375, 'referece_logps/chosen': -251.81578063964844, 'logits/rejected': -0.17671515047550201, 'logits/chosen': -0.23021595180034637, 'epoch': 0.12}
[2024-03-28 22:43:50,317] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 110/2685 [37:06<14:32:04, 20.32s/it]
[2024-03-28 22:44:09,413] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6388, 'learning_rate': 1.9993880168228743e-06, 'rewards/chosen': -0.1543354094028473, 'rewards/rejected': -0.17907506227493286, 'rewards/accuracies': 0.5, 'rewards/margins': 0.02473965473473072, 'policy_logps/rejected': -483.7110595703125, 'policy_logps/chosen': -409.9040222167969, 'referece_logps/rejected': -481.9203186035156, 'referece_logps/chosen': -408.36065673828125, 'logits/rejected': -0.8584905862808228, 'logits/chosen': -0.8546766638755798, 'epoch': 0.12}

  4%|▍         | 111/2685 [37:22<13:44:35, 19.22s/it]


  4%|▍         | 113/2685 [38:01<13:51:26, 19.40s/it]

  4%|▍         | 114/2685 [38:23<14:24:22, 20.17s/it]
[2024-03-28 22:45:26,847] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  4%|▍         | 115/2685 [38:44<14:38:17, 20.51s/it]
[2024-03-28 22:45:48,129] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.602, 'learning_rate': 1.9991588281537217e-06, 'rewards/chosen': 0.016505811363458633, 'rewards/rejected': -0.14532414078712463, 'rewards/accuracies': 0.75, 'rewards/margins': 0.16182994842529297, 'policy_logps/rejected': -400.8942565917969, 'policy_logps/chosen': -438.3269958496094, 'referece_logps/rejected': -399.4410095214844, 'referece_logps/chosen': -438.4920959472656, 'logits/rejected': -0.48428794741630554, 'logits/chosen': -0.5450073480606079, 'epoch': 0.13}
[2024-03-28 22:46:10,524] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


  4%|▍         | 117/2685 [39:28<15:05:18, 21.15s/it]

  4%|▍         | 118/2685 [39:48<14:50:06, 20.80s/it]

  4%|▍         | 119/2685 [40:10<14:57:13, 20.98s/it]

  4%|▍         | 120/2685 [40:28<14:25:21, 20.24s/it]

  5%|▍         | 121/2685 [40:50<14:41:47, 20.63s/it]

  5%|▍         | 122/2685 [41:11<14:56:06, 20.98s/it]

  5%|▍         | 123/2685 [41:34<15:14:05, 21.41s/it]

  5%|▍         | 124/2685 [41:50<14:12:36, 19.98s/it]
{'loss': 0.5417, 'learning_rate': 1.9986546747321483e-06, 'rewards/chosen': -0.3000991940498352, 'rewards/rejected': -0.7728436589241028, 'rewards/accuracies': 0.875, 'rewards/margins': 0.4727444648742676, 'policy_logps/rejected': -425.7153015136719, 'policy_logps/chosen': -368.0662536621094, 'referece_logps/rejected': -417.9868469238281, 'referece_logps/chosen': -365.0653076171875, 'logits/rejected': -0.7996771335601807, 'logits/chosen': -0.8715686798095703, 'epoch': 0.14}


  5%|▍         | 126/2685 [42:28<13:38:56, 19.20s/it]

  5%|▍         | 127/2685 [42:50<14:07:12, 19.87s/it]
[2024-03-28 22:49:53,165] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

  5%|▍         | 128/2685 [43:09<14:07:31, 19.89s/it]

  5%|▍         | 129/2685 [43:30<14:12:09, 20.00s/it]

  5%|▍         | 130/2685 [43:48<13:54:43, 19.60s/it]
{'loss': 0.5998, 'learning_rate': 1.9982531586290957e-06, 'rewards/chosen': -0.4870491027832031, 'rewards/rejected': -0.9344198703765869, 'rewards/accuracies': 0.75, 'rewards/margins': 0.447370707988739, 'policy_logps/rejected': -410.3683166503906, 'policy_logps/chosen': -470.2197265625, 'referece_logps/rejected': -401.02410888671875, 'referece_logps/chosen': -465.3492126464844, 'logits/rejected': -0.4233541488647461, 'logits/chosen': -0.49019232392311096, 'epoch': 0.15}

  5%|▍         | 131/2685 [44:05<13:17:25, 18.73s/it]


  5%|▍         | 133/2685 [44:44<13:17:48, 18.76s/it]

  5%|▍         | 134/2685 [45:07<14:12:32, 20.05s/it]

  5%|▌         | 135/2685 [45:29<14:38:42, 20.68s/it]
{'loss': 0.5897, 'learning_rate': 1.997878604232291e-06, 'rewards/chosen': -0.28046950697898865, 'rewards/rejected': -0.26022589206695557, 'rewards/accuracies': 0.375, 'rewards/margins': -0.020243626087903976, 'policy_logps/rejected': -338.07489013671875, 'policy_logps/chosen': -353.9092712402344, 'referece_logps/rejected': -335.47265625, 'referece_logps/chosen': -351.10455322265625, 'logits/rejected': -0.2625802159309387, 'logits/chosen': -0.21025896072387695, 'epoch': 0.15}


  5%|▌         | 137/2685 [46:12<15:02:07, 21.24s/it]

  5%|▌         | 138/2685 [46:31<14:29:19, 20.48s/it]

  5%|▌         | 139/2685 [46:51<14:22:48, 20.33s/it]

  5%|▌         | 140/2685 [47:08<13:42:46, 19.40s/it]
{'loss': 0.5706, 'learning_rate': 1.9974677391716964e-06, 'rewards/chosen': -0.1820736974477768, 'rewards/rejected': -0.1944725066423416, 'rewards/accuracies': 0.5, 'rewards/margins': 0.012398818507790565, 'policy_logps/rejected': -302.0670166015625, 'policy_logps/chosen': -317.8867492675781, 'referece_logps/rejected': -300.1222839355469, 'referece_logps/chosen': -316.0660095214844, 'logits/rejected': -0.9002173542976379, 'logits/chosen': -0.9443073272705078, 'epoch': 0.16}


  5%|▌         | 142/2685 [47:53<14:53:58, 21.09s/it]
{'loss': 0.5529, 'learning_rate': 1.997293229466127e-06, 'rewards/chosen': -0.44310835003852844, 'rewards/rejected': -0.5087445378303528, 'rewards/accuracies': 0.625, 'rewards/margins': 0.06563616544008255, 'policy_logps/rejected': -397.9974670410156, 'policy_logps/chosen': -430.3866882324219, 'referece_logps/rejected': -392.90997314453125, 'referece_logps/chosen': -425.95562744140625, 'logits/rejected': -0.5597511529922485, 'logits/chosen': -0.562469482421875, 'epoch': 0.16}


  5%|▌         | 144/2685 [48:35<14:48:43, 20.99s/it]

  5%|▌         | 145/2685 [48:50<13:39:35, 19.36s/it]

  5%|▌         | 146/2685 [49:11<13:57:31, 19.79s/it]

  5%|▌         | 147/2685 [49:33<14:23:02, 20.40s/it]

  6%|▌         | 148/2685 [49:54<14:29:55, 20.57s/it]

  6%|▌         | 149/2685 [50:16<14:51:05, 21.08s/it]
{'loss': 0.5223, 'learning_rate': 1.9966367277550354e-06, 'rewards/chosen': -0.7442358732223511, 'rewards/rejected': -1.5604969263076782, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8162609934806824, 'policy_logps/rejected': -701.6510009765625, 'policy_logps/chosen': -558.652587890625, 'referece_logps/rejected': -686.0460205078125, 'referece_logps/chosen': -551.2102661132812, 'logits/rejected': -0.1991708129644394, 'logits/chosen': -0.18110428750514984, 'epoch': 0.17}


  6%|▌         | 151/2685 [50:53<13:43:45, 19.50s/it]

  6%|▌         | 152/2685 [51:11<13:25:55, 19.09s/it]

  6%|▌         | 153/2685 [51:33<14:06:51, 20.07s/it]

  6%|▌         | 154/2685 [51:55<14:26:27, 20.54s/it]

  6%|▌         | 155/2685 [52:15<14:27:57, 20.58s/it]

  6%|▌         | 156/2685 [52:34<14:02:42, 19.99s/it]

  6%|▌         | 157/2685 [52:52<13:41:34, 19.50s/it]

  6%|▌         | 158/2685 [53:14<14:12:19, 20.24s/it]
{'loss': 0.473, 'learning_rate': 1.99568821699889e-06, 'rewards/chosen': -0.10740087926387787, 'rewards/rejected': -0.9079252481460571, 'rewards/accuracies': 1.0, 'rewards/margins': 0.8005243539810181, 'policy_logps/rejected': -371.54913330078125, 'policy_logps/chosen': -434.9249267578125, 'referece_logps/rejected': -362.46990966796875, 'referece_logps/chosen': -433.85089111328125, 'logits/rejected': -0.6606218814849854, 'logits/chosen': -0.6374399662017822, 'epoch': 0.18}

  6%|▌         | 159/2685 [53:29<13:09:07, 18.74s/it]


  6%|▌         | 161/2685 [54:13<14:15:55, 20.35s/it]
{'loss': 0.4734, 'learning_rate': 1.9953459547115697e-06, 'rewards/chosen': -0.48210182785987854, 'rewards/rejected': -0.8492699265480042, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3671680986881256, 'policy_logps/rejected': -367.7187805175781, 'policy_logps/chosen': -385.4992370605469, 'referece_logps/rejected': -359.22607421875, 'referece_logps/chosen': -380.6781921386719, 'logits/rejected': -0.5791550278663635, 'logits/chosen': -0.5917288661003113, 'epoch': 0.18}

  6%|▌         | 162/2685 [54:32<13:55:59, 19.88s/it]

  6%|▌         | 163/2685 [54:52<14:02:50, 20.05s/it]


  6%|▌         | 165/2685 [55:31<13:50:11, 19.77s/it]

  6%|▌         | 166/2685 [55:54<14:33:37, 20.81s/it]

  6%|▌         | 167/2685 [56:11<13:46:08, 19.69s/it]

  6%|▋         | 168/2685 [56:32<13:52:01, 19.83s/it]
{'loss': 0.5666, 'learning_rate': 1.9944966447699727e-06, 'rewards/chosen': -0.31292724609375, 'rewards/rejected': -0.729387640953064, 'rewards/accuracies': 0.75, 'rewards/margins': 0.41646039485931396, 'policy_logps/rejected': -320.8642578125, 'policy_logps/chosen': -370.013427734375, 'referece_logps/rejected': -313.57037353515625, 'referece_logps/chosen': -366.8841857910156, 'logits/rejected': -1.1578699350357056, 'logits/chosen': -1.1477952003479004, 'epoch': 0.19}


  6%|▋         | 170/2685 [57:17<14:53:19, 21.31s/it]

  6%|▋         | 171/2685 [57:37<14:34:55, 20.88s/it]
{'loss': 0.5568, 'learning_rate': 1.994110938586216e-06, 'rewards/chosen': -0.4319165349006653, 'rewards/rejected': -1.0656663179397583, 'rewards/accuracies': 0.75, 'rewards/margins': 0.633749783039093, 'policy_logps/rejected': -421.0767822265625, 'policy_logps/chosen': -381.42340087890625, 'referece_logps/rejected': -410.42010498046875, 'referece_logps/chosen': -377.1042785644531, 'logits/rejected': 0.5578303337097168, 'logits/chosen': 0.4487290382385254, 'epoch': 0.19}


  6%|▋         | 173/2685 [58:09<12:38:37, 18.12s/it]

  6%|▋         | 174/2685 [58:29<12:58:03, 18.59s/it]

  7%|▋         | 175/2685 [58:50<13:28:24, 19.32s/it]

  7%|▋         | 176/2685 [59:09<13:31:55, 19.42s/it]
{'loss': 0.5421, 'learning_rate': 1.9934391586095946e-06, 'rewards/chosen': -0.3840958774089813, 'rewards/rejected': -0.6991639733314514, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3150680661201477, 'policy_logps/rejected': -339.7027282714844, 'policy_logps/chosen': -286.0518493652344, 'referece_logps/rejected': -332.71112060546875, 'referece_logps/chosen': -282.2109069824219, 'logits/rejected': -0.08052709698677063, 'logits/chosen': -0.15967421233654022, 'epoch': 0.2}


  7%|▋         | 178/2685 [59:46<13:11:14, 18.94s/it]
{'loss': 0.5094, 'learning_rate': 1.993160323451714e-06, 'rewards/chosen': -0.6963891983032227, 'rewards/rejected': -1.6344380378723145, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9380489587783813, 'policy_logps/rejected': -535.381591796875, 'policy_logps/chosen': -499.7081604003906, 'referece_logps/rejected': -519.0371704101562, 'referece_logps/chosen': -492.74432373046875, 'logits/rejected': 0.743463397026062, 'logits/chosen': 0.7751737833023071, 'epoch': 0.2}


  7%|▋         | 180/2685 [1:00:21<12:51:47, 18.49s/it]

  7%|▋         | 181/2685 [1:00:42<13:16:37, 19.09s/it]

  7%|▋         | 182/2685 [1:01:02<13:36:34, 19.57s/it]

  7%|▋         | 183/2685 [1:01:23<13:45:32, 19.80s/it]

  7%|▋         | 184/2685 [1:01:45<14:17:08, 20.56s/it]

  7%|▋         | 185/2685 [1:02:05<14:11:20, 20.43s/it]

  7%|▋         | 186/2685 [1:02:27<14:27:14, 20.82s/it]

  7%|▋         | 187/2685 [1:02:47<14:12:35, 20.48s/it]

  7%|▋         | 188/2685 [1:03:07<14:06:15, 20.33s/it]
{'loss': 0.4832, 'learning_rate': 1.9916794475458213e-06, 'rewards/chosen': 0.12902012467384338, 'rewards/rejected': -0.5951418280601501, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7241619825363159, 'policy_logps/rejected': -325.81201171875, 'policy_logps/chosen': -361.9303283691406, 'referece_logps/rejected': -319.860595703125, 'referece_logps/chosen': -363.22052001953125, 'logits/rejected': -0.7789732217788696, 'logits/chosen': -0.9521018266677856, 'epoch': 0.21}

  7%|▋         | 189/2685 [1:03:27<14:01:17, 20.22s/it]


  7%|▋         | 191/2685 [1:04:05<13:54:38, 20.08s/it]

  7%|▋         | 192/2685 [1:04:24<13:39:07, 19.71s/it]

  7%|▋         | 193/2685 [1:04:44<13:47:43, 19.93s/it]
{'loss': 0.4752, 'learning_rate': 1.990884868158239e-06, 'rewards/chosen': -0.5102968215942383, 'rewards/rejected': -0.7870210409164429, 'rewards/accuracies': 0.625, 'rewards/margins': 0.276724249124527, 'policy_logps/rejected': -304.7617492675781, 'policy_logps/chosen': -415.7601318359375, 'referece_logps/rejected': -296.89154052734375, 'referece_logps/chosen': -410.65716552734375, 'logits/rejected': -0.3788101375102997, 'logits/chosen': -0.5169097185134888, 'epoch': 0.22}


  7%|▋         | 195/2685 [1:05:30<14:48:39, 21.41s/it]

  7%|▋         | 196/2685 [1:05:50<14:31:54, 21.02s/it]
{'loss': 0.4117, 'learning_rate': 1.990390811655836e-06, 'rewards/chosen': -0.7695094347000122, 'rewards/rejected': -2.429652452468872, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6601427793502808, 'policy_logps/rejected': -628.1318969726562, 'policy_logps/chosen': -496.2755432128906, 'referece_logps/rejected': -603.8353881835938, 'referece_logps/chosen': -488.5804443359375, 'logits/rejected': 0.5200831294059753, 'logits/chosen': 0.532206654548645, 'epoch': 0.22}


  7%|▋         | 198/2685 [1:06:34<14:53:45, 21.56s/it]

  7%|▋         | 199/2685 [1:06:54<14:38:35, 21.20s/it]

  7%|▋         | 200/2685 [1:07:16<14:47:38, 21.43s/it]

  7%|▋         | 201/2685 [1:07:36<14:27:54, 20.96s/it]
{'loss': 0.4655, 'learning_rate': 1.9895385568095978e-06, 'rewards/chosen': -0.25161802768707275, 'rewards/rejected': -1.656569480895996, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4049514532089233, 'policy_logps/rejected': -421.223876953125, 'policy_logps/chosen': -339.33209228515625, 'referece_logps/rejected': -404.6582336425781, 'referece_logps/chosen': -336.81591796875, 'logits/rejected': -0.03987380862236023, 'logits/chosen': -0.006652355194091797, 'epoch': 0.22}


  8%|▊         | 203/2685 [1:08:16<14:17:52, 20.74s/it]

  8%|▊         | 204/2685 [1:08:36<14:04:40, 20.43s/it]
{'loss': 0.485, 'learning_rate': 1.9890099184126636e-06, 'rewards/chosen': -1.4247431755065918, 'rewards/rejected': -2.2024755477905273, 'rewards/accuracies': 1.0, 'rewards/margins': 0.777732253074646, 'policy_logps/rejected': -481.86834716796875, 'policy_logps/chosen': -450.66912841796875, 'referece_logps/rejected': -459.8436279296875, 'referece_logps/chosen': -436.4216613769531, 'logits/rejected': 0.0022823363542556763, 'logits/chosen': -0.10020454227924347, 'epoch': 0.23}

  8%|▊         | 205/2685 [1:08:55<13:56:10, 20.23s/it]


  8%|▊         | 207/2685 [1:09:38<14:07:03, 20.51s/it]
{'loss': 0.4718, 'learning_rate': 1.9884683243281113e-06, 'rewards/chosen': -0.4276585578918457, 'rewards/rejected': -0.7775983810424805, 'rewards/accuracies': 0.75, 'rewards/margins': 0.34993985295295715, 'policy_logps/rejected': -405.1107482910156, 'policy_logps/chosen': -339.87664794921875, 'referece_logps/rejected': -397.33477783203125, 'referece_logps/chosen': -335.6000671386719, 'logits/rejected': -0.1274113804101944, 'logits/chosen': -0.16800323128700256, 'epoch': 0.23}

  8%|▊         | 208/2685 [1:09:57<13:56:06, 20.25s/it]


  8%|▊         | 210/2685 [1:10:38<13:53:17, 20.20s/it]

  8%|▊         | 211/2685 [1:11:00<14:15:27, 20.75s/it]

  8%|▊         | 212/2685 [1:11:20<14:02:04, 20.43s/it]
{'loss': 0.5685, 'learning_rate': 1.987536896449924e-06, 'rewards/chosen': -0.9216333627700806, 'rewards/rejected': -1.2679752111434937, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3463418185710907, 'policy_logps/rejected': -512.15283203125, 'policy_logps/chosen': -403.53814697265625, 'referece_logps/rejected': -499.47308349609375, 'referece_logps/chosen': -394.32177734375, 'logits/rejected': -0.23656995594501495, 'logits/chosen': -0.18034473061561584, 'epoch': 0.24}

  8%|▊         | 213/2685 [1:11:41<14:14:00, 20.73s/it]

  8%|▊         | 214/2685 [1:12:04<14:32:36, 21.19s/it]

  8%|▊         | 215/2685 [1:12:26<14:41:59, 21.42s/it]


  8%|▊         | 217/2685 [1:13:05<14:06:53, 20.59s/it]
{'loss': 0.5344, 'learning_rate': 1.98656953422053e-06, 'rewards/chosen': -0.3492576777935028, 'rewards/rejected': -0.9377386569976807, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5884810090065002, 'policy_logps/rejected': -339.76177978515625, 'policy_logps/chosen': -360.03515625, 'referece_logps/rejected': -330.3843994140625, 'referece_logps/chosen': -356.5426025390625, 'logits/rejected': -0.3375331163406372, 'logits/chosen': -0.18482032418251038, 'epoch': 0.24}

  8%|▊         | 218/2685 [1:13:22<13:25:34, 19.59s/it]

  8%|▊         | 219/2685 [1:13:43<13:50:36, 20.21s/it]

  8%|▊         | 220/2685 [1:14:03<13:46:53, 20.13s/it]

  8%|▊         | 221/2685 [1:14:26<14:11:00, 20.72s/it]

  8%|▊         | 222/2685 [1:14:44<13:38:29, 19.94s/it]

  8%|▊         | 223/2685 [1:15:03<13:33:36, 19.83s/it]

  8%|▊         | 224/2685 [1:15:27<14:22:32, 21.03s/it]


  8%|▊         | 226/2685 [1:16:06<13:42:30, 20.07s/it]

  8%|▊         | 227/2685 [1:16:25<13:19:48, 19.52s/it]

  8%|▊         | 228/2685 [1:16:43<12:59:18, 19.03s/it]

  9%|▊         | 229/2685 [1:16:58<12:20:10, 18.08s/it]
{'loss': 0.469, 'learning_rate': 1.984101466090964e-06, 'rewards/chosen': -0.798675000667572, 'rewards/rejected': -1.6140092611312866, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8153343796730042, 'policy_logps/rejected': -525.1874389648438, 'policy_logps/chosen': -467.3061828613281, 'referece_logps/rejected': -509.04736328125, 'referece_logps/chosen': -459.3194580078125, 'logits/rejected': -1.3402900695800781, 'logits/chosen': -1.3177672624588013, 'epoch': 0.26}

  9%|▊         | 230/2685 [1:17:18<12:33:09, 18.41s/it]


  9%|▊         | 232/2685 [1:17:57<12:58:47, 19.05s/it]
{'loss': 0.4813, 'learning_rate': 1.983452199957626e-06, 'rewards/chosen': -0.2673414349555969, 'rewards/rejected': -2.039201498031616, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7718602418899536, 'policy_logps/rejected': -416.3331604003906, 'policy_logps/chosen': -408.77471923828125, 'referece_logps/rejected': -395.9411315917969, 'referece_logps/chosen': -406.1012878417969, 'logits/rejected': -0.16014505922794342, 'logits/chosen': -0.25799036026000977, 'epoch': 0.26}

  9%|▊         | 233/2685 [1:18:17<13:15:46, 19.47s/it]


  9%|▉         | 235/2685 [1:18:59<13:46:59, 20.25s/it]
{'loss': 0.4107, 'learning_rate': 1.982790050940858e-06, 'rewards/chosen': -0.8378170132637024, 'rewards/rejected': -2.1480002403259277, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3101834058761597, 'policy_logps/rejected': -433.6854553222656, 'policy_logps/chosen': -355.1562194824219, 'referece_logps/rejected': -412.2054748535156, 'referece_logps/chosen': -346.77801513671875, 'logits/rejected': 0.1763363629579544, 'logits/chosen': 0.19504062831401825, 'epoch': 0.26}

  9%|▉         | 236/2685 [1:19:22<14:15:13, 20.95s/it]

  9%|▉         | 237/2685 [1:19:41<14:01:08, 20.62s/it]

  9%|▉         | 238/2685 [1:20:02<13:59:17, 20.58s/it]


  9%|▉         | 240/2685 [1:20:43<13:58:59, 20.59s/it]
{'loss': 0.4242, 'learning_rate': 1.9816578642515498e-06, 'rewards/chosen': -0.3994661569595337, 'rewards/rejected': -1.8029065132141113, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4034403562545776, 'policy_logps/rejected': -363.0552673339844, 'policy_logps/chosen': -263.7908630371094, 'referece_logps/rejected': -345.0262145996094, 'referece_logps/chosen': -259.79620361328125, 'logits/rejected': -0.553115963935852, 'logits/chosen': -0.6021305918693542, 'epoch': 0.27}

  9%|▉         | 241/2685 [1:21:03<13:55:00, 20.50s/it]


  9%|▉         | 243/2685 [1:21:42<13:31:43, 19.94s/it]
{'loss': 0.4329, 'learning_rate': 1.9809614037428174e-06, 'rewards/chosen': -0.258257657289505, 'rewards/rejected': -0.7260355949401855, 'rewards/accuracies': 0.75, 'rewards/margins': 0.46777793765068054, 'policy_logps/rejected': -371.5644836425781, 'policy_logps/chosen': -247.267822265625, 'referece_logps/rejected': -364.30413818359375, 'referece_logps/chosen': -244.68524169921875, 'logits/rejected': -0.3739076256752014, 'logits/chosen': -0.4332141578197479, 'epoch': 0.27}

  9%|▉         | 244/2685 [1:22:04<13:48:21, 20.36s/it]

  9%|▉         | 245/2685 [1:22:24<13:42:43, 20.23s/it]


  9%|▉         | 247/2685 [1:23:03<13:36:38, 20.10s/it]
{'loss': 0.4657, 'learning_rate': 1.980012802046965e-06, 'rewards/chosen': -1.6430829763412476, 'rewards/rejected': -2.5189762115478516, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8758934140205383, 'policy_logps/rejected': -429.8360900878906, 'policy_logps/chosen': -407.1931457519531, 'referece_logps/rejected': -404.6463317871094, 'referece_logps/chosen': -390.76239013671875, 'logits/rejected': -0.32152149081230164, 'logits/chosen': -0.3690088987350464, 'epoch': 0.28}

  9%|▉         | 248/2685 [1:23:22<13:23:15, 19.78s/it]

  9%|▉         | 249/2685 [1:23:44<13:50:49, 20.46s/it]

  9%|▉         | 250/2685 [1:24:06<14:07:48, 20.89s/it]


  9%|▉         | 252/2685 [1:24:47<14:02:41, 20.78s/it]

  9%|▉         | 253/2685 [1:25:03<12:57:06, 19.17s/it]

  9%|▉         | 254/2685 [1:25:19<12:26:59, 18.44s/it]

  9%|▉         | 255/2685 [1:25:41<13:08:45, 19.48s/it]

 10%|▉         | 256/2685 [1:26:01<13:09:30, 19.50s/it]
{'loss': 0.4219, 'learning_rate': 1.9777950369318033e-06, 'rewards/chosen': -0.9089973568916321, 'rewards/rejected': -2.2224249839782715, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3134276866912842, 'policy_logps/rejected': -500.24737548828125, 'policy_logps/chosen': -490.1408386230469, 'referece_logps/rejected': -478.02313232421875, 'referece_logps/chosen': -481.0509033203125, 'logits/rejected': -0.2282930463552475, 'logits/chosen': -0.13246330618858337, 'epoch': 0.29}

 10%|▉         | 257/2685 [1:26:20<13:11:43, 19.56s/it]


 10%|▉         | 259/2685 [1:26:57<12:39:56, 18.80s/it]
{'loss': 0.4573, 'learning_rate': 1.977030151260355e-06, 'rewards/chosen': -0.5561699867248535, 'rewards/rejected': -1.2534785270690918, 'rewards/accuracies': 0.5, 'rewards/margins': 0.6973084211349487, 'policy_logps/rejected': -548.6624755859375, 'policy_logps/chosen': -546.4720458984375, 'referece_logps/rejected': -536.127685546875, 'referece_logps/chosen': -540.910400390625, 'logits/rejected': -0.8114087581634521, 'logits/chosen': -0.7324837446212769, 'epoch': 0.29}


 10%|▉         | 261/2685 [1:27:39<13:21:51, 19.85s/it]

 10%|▉         | 262/2685 [1:27:55<12:38:01, 18.77s/it]

 10%|▉         | 263/2685 [1:28:15<12:52:51, 19.15s/it]

 10%|▉         | 264/2685 [1:28:35<13:07:32, 19.52s/it]

 10%|▉         | 265/2685 [1:28:59<13:54:29, 20.69s/it]

 10%|▉         | 266/2685 [1:29:21<14:15:51, 21.23s/it]
{'loss': 0.4907, 'learning_rate': 1.975195662715787e-06, 'rewards/chosen': -0.994768500328064, 'rewards/rejected': -1.433548927307129, 'rewards/accuracies': 0.875, 'rewards/margins': 0.43878036737442017, 'policy_logps/rejected': -377.669921875, 'policy_logps/chosen': -396.338134765625, 'referece_logps/rejected': -363.33447265625, 'referece_logps/chosen': -386.39044189453125, 'logits/rejected': -0.9875469207763672, 'logits/chosen': -1.0796819925308228, 'epoch': 0.3}

 10%|▉         | 267/2685 [1:29:45<14:41:20, 21.87s/it]


 10%|█         | 269/2685 [1:30:17<12:49:06, 19.10s/it]

 10%|█         | 270/2685 [1:30:37<12:55:51, 19.28s/it]
{'loss': 0.4632, 'learning_rate': 1.974116147995387e-06, 'rewards/chosen': -0.5303272008895874, 'rewards/rejected': -2.0215935707092285, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4912664890289307, 'policy_logps/rejected': -465.36358642578125, 'policy_logps/chosen': -508.429443359375, 'referece_logps/rejected': -445.14764404296875, 'referece_logps/chosen': -503.1261291503906, 'logits/rejected': 0.3584558367729187, 'logits/chosen': 0.24551662802696228, 'epoch': 0.3}


 10%|█         | 272/2685 [1:31:15<12:45:08, 19.03s/it]
{'loss': 0.475, 'learning_rate': 1.9735678819942616e-06, 'rewards/chosen': -0.3682006001472473, 'rewards/rejected': -1.6562329530715942, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2880324125289917, 'policy_logps/rejected': -333.5010070800781, 'policy_logps/chosen': -236.43576049804688, 'referece_logps/rejected': -316.93865966796875, 'referece_logps/chosen': -232.7537841796875, 'logits/rejected': -0.4963919222354889, 'logits/chosen': -0.522944450378418, 'epoch': 0.3}

 10%|█         | 273/2685 [1:31:37<13:19:53, 19.90s/it]


 10%|█         | 275/2685 [1:32:18<13:24:36, 20.03s/it]

 10%|█         | 276/2685 [1:32:36<12:58:44, 19.40s/it]

 10%|█         | 277/2685 [1:32:52<12:18:37, 18.40s/it]
{'loss': 0.4941, 'learning_rate': 1.9721724257579905e-06, 'rewards/chosen': -0.4552134573459625, 'rewards/rejected': -0.9427241086959839, 'rewards/accuracies': 0.625, 'rewards/margins': 0.487510621547699, 'policy_logps/rejected': -328.15093994140625, 'policy_logps/chosen': -298.6256408691406, 'referece_logps/rejected': -318.72369384765625, 'referece_logps/chosen': -294.07354736328125, 'logits/rejected': -0.24087026715278625, 'logits/chosen': -0.2131379097700119, 'epoch': 0.31}


 10%|█         | 279/2685 [1:33:33<13:13:03, 19.78s/it]

 10%|█         | 280/2685 [1:33:54<13:22:18, 20.02s/it]

 10%|█         | 281/2685 [1:34:12<12:59:36, 19.46s/it]
{'loss': 0.4631, 'learning_rate': 1.971030588093226e-06, 'rewards/chosen': -0.06674137711524963, 'rewards/rejected': -1.9537333250045776, 'rewards/accuracies': 1.0, 'rewards/margins': 1.8869919776916504, 'policy_logps/rejected': -386.1280212402344, 'policy_logps/chosen': -359.3085021972656, 'referece_logps/rejected': -366.5906982421875, 'referece_logps/chosen': -358.64111328125, 'logits/rejected': -0.7398005127906799, 'logits/chosen': -0.6834788918495178, 'epoch': 0.31}

 11%|█         | 282/2685 [1:34:29<12:36:09, 18.88s/it]

 11%|█         | 283/2685 [1:34:49<12:48:03, 19.19s/it]

 11%|█         | 284/2685 [1:35:07<12:30:23, 18.75s/it]

 11%|█         | 285/2685 [1:35:29<13:09:47, 19.74s/it]


 11%|█         | 287/2685 [1:36:02<11:56:16, 17.92s/it]
{'loss': 0.443, 'learning_rate': 1.969275439537199e-06, 'rewards/chosen': -0.5182195901870728, 'rewards/rejected': -2.0499610900878906, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5317416191101074, 'policy_logps/rejected': -451.05889892578125, 'policy_logps/chosen': -444.8927001953125, 'referece_logps/rejected': -430.55926513671875, 'referece_logps/chosen': -439.71051025390625, 'logits/rejected': -0.5991714000701904, 'logits/chosen': -0.7885837554931641, 'epoch': 0.32}


 11%|█         | 289/2685 [1:36:44<12:56:44, 19.45s/it]

 11%|█         | 290/2685 [1:37:04<13:06:39, 19.71s/it]
{'loss': 0.4042, 'learning_rate': 1.9683788137104436e-06, 'rewards/chosen': 0.07013798505067825, 'rewards/rejected': -2.1164040565490723, 'rewards/accuracies': 1.0, 'rewards/margins': 2.18654203414917, 'policy_logps/rejected': -370.7364501953125, 'policy_logps/chosen': -363.6905822753906, 'referece_logps/rejected': -349.5723876953125, 'referece_logps/chosen': -364.3919677734375, 'logits/rejected': -0.4405793249607086, 'logits/chosen': -0.48964348435401917, 'epoch': 0.32}

 11%|█         | 291/2685 [1:37:23<12:53:57, 19.40s/it]

 11%|█         | 292/2685 [1:37:45<13:28:43, 20.28s/it]

 11%|█         | 293/2685 [1:38:07<13:48:12, 20.77s/it]

 11%|█         | 294/2685 [1:38:25<13:14:23, 19.93s/it]

 11%|█         | 295/2685 [1:38:44<12:58:34, 19.55s/it]

 11%|█         | 296/2685 [1:39:05<13:17:10, 20.02s/it]

 11%|█         | 297/2685 [1:39:25<13:24:19, 20.21s/it]


 11%|█         | 299/2685 [1:40:06<13:28:59, 20.34s/it]
{'loss': 0.475, 'learning_rate': 1.965612871479123e-06, 'rewards/chosen': -0.3677803874015808, 'rewards/rejected': -1.2014920711517334, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8337116837501526, 'policy_logps/rejected': -349.6304626464844, 'policy_logps/chosen': -402.36187744140625, 'referece_logps/rejected': -337.61553955078125, 'referece_logps/chosen': -398.6840515136719, 'logits/rejected': -0.6940771341323853, 'logits/chosen': -0.7103323936462402, 'epoch': 0.33}

 11%|█         | 300/2685 [1:40:29<13:54:39, 21.00s/it]


 11%|█         | 302/2685 [1:41:04<12:44:57, 19.26s/it]
{'loss': 0.4099, 'learning_rate': 1.9646655760771114e-06, 'rewards/chosen': -0.5849653482437134, 'rewards/rejected': -1.4951218366622925, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9101564288139343, 'policy_logps/rejected': -345.1724853515625, 'policy_logps/chosen': -265.19989013671875, 'referece_logps/rejected': -330.22125244140625, 'referece_logps/chosen': -259.3502502441406, 'logits/rejected': -0.8749682903289795, 'logits/chosen': -0.9538462162017822, 'epoch': 0.34}

 11%|█▏        | 303/2685 [1:41:21<12:18:57, 18.61s/it]

 11%|█▏        | 304/2685 [1:41:44<13:07:42, 19.85s/it]

 11%|█▏        | 305/2685 [1:42:07<13:51:04, 20.95s/it]

 11%|█▏        | 306/2685 [1:42:29<13:54:58, 21.06s/it]

 11%|█▏        | 307/2685 [1:42:45<12:52:36, 19.49s/it]


 12%|█▏        | 309/2685 [1:43:20<12:23:46, 18.78s/it]
{'loss': 0.4028, 'learning_rate': 1.9624060988436964e-06, 'rewards/chosen': -0.6523208618164062, 'rewards/rejected': -1.8318736553192139, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1795529127120972, 'policy_logps/rejected': -349.8271179199219, 'policy_logps/chosen': -286.4473876953125, 'referece_logps/rejected': -331.5084228515625, 'referece_logps/chosen': -279.9241943359375, 'logits/rejected': -0.5237829089164734, 'logits/chosen': -0.39535510540008545, 'epoch': 0.35}

 12%|█▏        | 310/2685 [1:43:41<12:44:59, 19.33s/it]

 12%|█▏        | 311/2685 [1:44:00<12:38:18, 19.17s/it]

 12%|█▏        | 312/2685 [1:44:20<12:45:58, 19.37s/it]

 12%|█▏        | 313/2685 [1:44:41<13:05:48, 19.88s/it]

 12%|█▏        | 314/2685 [1:45:01<13:06:55, 19.91s/it]

 12%|█▏        | 315/2685 [1:45:21<13:16:08, 20.16s/it]

 12%|█▏        | 316/2685 [1:45:40<12:57:46, 19.70s/it]

 12%|█▏        | 317/2685 [1:46:04<13:44:52, 20.90s/it]


 12%|█▏        | 319/2685 [1:46:41<13:10:12, 20.04s/it]
{'loss': 0.3902, 'learning_rate': 1.959059247901202e-06, 'rewards/chosen': -1.1486862897872925, 'rewards/rejected': -2.9317245483398438, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7830383777618408, 'policy_logps/rejected': -490.52294921875, 'policy_logps/chosen': -443.9700622558594, 'referece_logps/rejected': -461.2057189941406, 'referece_logps/chosen': -432.4831848144531, 'logits/rejected': 0.2312455177307129, 'logits/chosen': 0.07056926190853119, 'epoch': 0.36}


 12%|█▏        | 321/2685 [1:47:20<13:09:51, 20.05s/it]

 12%|█▏        | 322/2685 [1:47:36<12:21:54, 18.84s/it]
{'loss': 0.3949, 'learning_rate': 1.9580279496429816e-06, 'rewards/chosen': 0.056617364287376404, 'rewards/rejected': -1.4171825647354126, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4737999439239502, 'policy_logps/rejected': -406.7333068847656, 'policy_logps/chosen': -413.2740478515625, 'referece_logps/rejected': -392.5614929199219, 'referece_logps/chosen': -413.8402099609375, 'logits/rejected': -1.843003511428833, 'logits/chosen': -1.7804418802261353, 'epoch': 0.36}

 12%|█▏        | 323/2685 [1:47:56<12:29:08, 19.03s/it]

 12%|█▏        | 324/2685 [1:48:12<11:49:00, 18.02s/it]

 12%|█▏        | 325/2685 [1:48:31<12:08:07, 18.51s/it]

 12%|█▏        | 326/2685 [1:48:51<12:24:42, 18.94s/it]


 12%|█▏        | 328/2685 [1:49:29<12:27:03, 19.02s/it]
{'loss': 0.3677, 'learning_rate': 1.9559277172969374e-06, 'rewards/chosen': -0.888866126537323, 'rewards/rejected': -2.1232712268829346, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2344050407409668, 'policy_logps/rejected': -355.7398376464844, 'policy_logps/chosen': -303.0685729980469, 'referece_logps/rejected': -334.5071105957031, 'referece_logps/chosen': -294.1799621582031, 'logits/rejected': 0.092164546251297, 'logits/chosen': 0.08032830059528351, 'epoch': 0.37}

 12%|█▏        | 329/2685 [1:49:49<12:43:34, 19.45s/it]

 12%|█▏        | 330/2685 [1:50:08<12:34:35, 19.23s/it]

 12%|█▏        | 331/2685 [1:50:28<12:38:47, 19.34s/it]

 12%|█▏        | 332/2685 [1:50:50<13:13:35, 20.24s/it]

 12%|█▏        | 333/2685 [1:51:10<13:15:48, 20.30s/it]

 12%|█▏        | 334/2685 [1:51:33<13:37:26, 20.86s/it]


 13%|█▎        | 336/2685 [1:52:11<12:55:09, 19.80s/it]

 13%|█▎        | 337/2685 [1:52:31<12:59:56, 19.93s/it]
{'loss': 0.3285, 'learning_rate': 1.9526834867767965e-06, 'rewards/chosen': -0.6602045893669128, 'rewards/rejected': -1.5084279775619507, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8482233881950378, 'policy_logps/rejected': -393.70404052734375, 'policy_logps/chosen': -319.8735046386719, 'referece_logps/rejected': -378.6197509765625, 'referece_logps/chosen': -313.27142333984375, 'logits/rejected': 0.04330502077937126, 'logits/chosen': 0.12953351438045502, 'epoch': 0.38}

 13%|█▎        | 338/2685 [1:52:50<12:46:59, 19.61s/it]

 13%|█▎        | 339/2685 [1:53:12<13:18:26, 20.42s/it]

 13%|█▎        | 340/2685 [1:53:29<12:30:38, 19.21s/it]

 13%|█▎        | 341/2685 [1:53:48<12:37:33, 19.39s/it]


 13%|█▎        | 343/2685 [1:54:27<12:28:55, 19.19s/it]
{'loss': 0.4743, 'learning_rate': 1.9504582436998966e-06, 'rewards/chosen': -0.7997095584869385, 'rewards/rejected': -2.944606304168701, 'rewards/accuracies': 0.875, 'rewards/margins': 2.144896984100342, 'policy_logps/rejected': -520.9564819335938, 'policy_logps/chosen': -330.2545166015625, 'referece_logps/rejected': -491.5104064941406, 'referece_logps/chosen': -322.2574157714844, 'logits/rejected': 0.2710236608982086, 'logits/chosen': 0.24383272230625153, 'epoch': 0.38}

 13%|█▎        | 344/2685 [1:54:45<12:09:42, 18.70s/it]

 13%|█▎        | 345/2685 [1:55:06<12:43:13, 19.57s/it]


 13%|█▎        | 347/2685 [1:55:47<12:57:33, 19.95s/it]

 13%|█▎        | 348/2685 [1:56:09<13:21:48, 20.59s/it]

 13%|█▎        | 349/2685 [1:56:28<12:59:06, 20.01s/it]

 13%|█▎        | 350/2685 [1:56:46<12:40:43, 19.55s/it]

 13%|█▎        | 351/2685 [1:57:05<12:32:01, 19.33s/it]

 13%|█▎        | 352/2685 [1:57:29<13:27:01, 20.76s/it]

 13%|█▎        | 353/2685 [1:57:47<12:49:29, 19.80s/it]

 13%|█▎        | 354/2685 [1:58:09<13:17:33, 20.53s/it]

 13%|█▎        | 355/2685 [1:58:25<12:24:58, 19.18s/it]

 13%|█▎        | 356/2685 [1:58:44<12:23:19, 19.15s/it]

 13%|█▎        | 357/2685 [1:59:02<12:08:58, 18.79s/it]

 13%|█▎        | 358/2685 [1:59:19<11:49:08, 18.28s/it]

 13%|█▎        | 359/2685 [1:59:40<12:14:26, 18.95s/it]

 13%|█▎        | 360/2685 [2:00:00<12:26:19, 19.26s/it]

 13%|█▎        | 361/2685 [2:00:16<11:53:08, 18.41s/it]

 13%|█▎        | 362/2685 [2:00:31<11:10:58, 17.33s/it]

 14%|█▎        | 363/2685 [2:00:49<11:19:36, 17.56s/it]

 14%|█▎        | 364/2685 [2:01:07<11:20:34, 17.59s/it]

 14%|█▎        | 365/2685 [2:01:22<10:59:23, 17.05s/it]

 14%|█▎        | 366/2685 [2:01:43<11:38:12, 18.07s/it]

 14%|█▎        | 367/2685 [2:02:02<11:56:34, 18.55s/it]

 14%|█▎        | 368/2685 [2:02:24<12:28:22, 19.38s/it]

 14%|█▎        | 369/2685 [2:02:46<12:58:51, 20.18s/it]

 14%|█▍        | 370/2685 [2:03:08<13:26:22, 20.90s/it]

 14%|█▍        | 371/2685 [2:03:28<13:09:12, 20.46s/it]

 14%|█▍        | 372/2685 [2:03:48<13:10:07, 20.50s/it]

 14%|█▍        | 373/2685 [2:04:04<12:06:59, 18.87s/it]

 14%|█▍        | 374/2685 [2:04:22<12:01:25, 18.73s/it]

 14%|█▍        | 375/2685 [2:04:38<11:35:02, 18.05s/it]

 14%|█▍        | 376/2685 [2:04:58<11:51:24, 18.49s/it]

 14%|█▍        | 377/2685 [2:05:17<12:03:40, 18.81s/it]

 14%|█▍        | 378/2685 [2:05:39<12:30:24, 19.52s/it]

 14%|█▍        | 379/2685 [2:05:59<12:34:28, 19.63s/it]

 14%|█▍        | 380/2685 [2:06:20<12:52:15, 20.10s/it]

 14%|█▍        | 381/2685 [2:06:42<13:20:52, 20.86s/it]

 14%|█▍        | 382/2685 [2:07:04<13:28:00, 21.05s/it]

 14%|█▍        | 383/2685 [2:07:23<13:08:22, 20.55s/it]

 14%|█▍        | 384/2685 [2:07:45<13:20:32, 20.87s/it]

 14%|█▍        | 385/2685 [2:08:05<13:08:16, 20.56s/it]

 14%|█▍        | 386/2685 [2:08:25<12:59:23, 20.34s/it]

 14%|█▍        | 387/2685 [2:08:48<13:34:38, 21.27s/it]

 14%|█▍        | 388/2685 [2:09:07<13:09:32, 20.62s/it]
{'loss': 0.4603, 'learning_rate': 1.932189944407526e-06, 'rewards/chosen': -0.2578682005405426, 'rewards/rejected': -2.4057040214538574, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1478357315063477, 'policy_logps/rejected': -494.65435791015625, 'policy_logps/chosen': -505.8609619140625, 'referece_logps/rejected': -470.5973815917969, 'referece_logps/chosen': -503.28228759765625, 'logits/rejected': 0.05440029501914978, 'logits/chosen': -0.04353618621826172, 'epoch': 0.43}


 15%|█▍        | 390/2685 [2:09:47<12:56:54, 20.31s/it]

 15%|█▍        | 391/2685 [2:10:09<13:05:11, 20.54s/it]

 15%|█▍        | 392/2685 [2:10:32<13:37:17, 21.39s/it]

 15%|█▍        | 393/2685 [2:10:53<13:32:32, 21.27s/it]

 15%|█▍        | 394/2685 [2:11:14<13:28:50, 21.18s/it]

 15%|█▍        | 395/2685 [2:11:33<13:05:36, 20.58s/it]

 15%|█▍        | 396/2685 [2:11:55<13:21:30, 21.01s/it]

 15%|█▍        | 397/2685 [2:12:14<12:59:49, 20.45s/it]

 15%|█▍        | 398/2685 [2:12:31<12:21:55, 19.46s/it]
{'loss': 0.4114, 'learning_rate': 1.9277552337030023e-06, 'rewards/chosen': -1.0033413171768188, 'rewards/rejected': -2.2543725967407227, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2510313987731934, 'policy_logps/rejected': -436.2131042480469, 'policy_logps/chosen': -434.1876220703125, 'referece_logps/rejected': -413.66943359375, 'referece_logps/chosen': -424.15423583984375, 'logits/rejected': -0.5809581875801086, 'logits/chosen': -0.5120932459831238, 'epoch': 0.44}


 15%|█▍        | 400/2685 [2:13:13<12:44:44, 20.08s/it]

 15%|█▍        | 401/2685 [2:13:35<13:08:29, 20.71s/it]

 15%|█▍        | 402/2685 [2:13:57<13:22:57, 21.10s/it]
{'loss': 0.4037, 'learning_rate': 1.925943502849695e-06, 'rewards/chosen': -0.8735478520393372, 'rewards/rejected': -1.3135762214660645, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4400283694267273, 'policy_logps/rejected': -469.19970703125, 'policy_logps/chosen': -410.9137268066406, 'referece_logps/rejected': -456.0639343261719, 'referece_logps/chosen': -402.17822265625, 'logits/rejected': 0.6494405269622803, 'logits/chosen': 0.6920858025550842, 'epoch': 0.45}


 15%|█▌        | 404/2685 [2:14:32<12:20:15, 19.47s/it]

 15%|█▌        | 405/2685 [2:14:53<12:34:22, 19.85s/it]
{'loss': 0.4076, 'learning_rate': 1.924570551266159e-06, 'rewards/chosen': -0.9798145294189453, 'rewards/rejected': -2.909334421157837, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9295198917388916, 'policy_logps/rejected': -496.267822265625, 'policy_logps/chosen': -455.9674377441406, 'referece_logps/rejected': -467.1745300292969, 'referece_logps/chosen': -446.1692810058594, 'logits/rejected': 0.2812251150608063, 'logits/chosen': 0.2681947350502014, 'epoch': 0.45}

 15%|█▌        | 406/2685 [2:15:15<12:55:03, 20.41s/it]


 15%|█▌        | 408/2685 [2:15:55<12:46:04, 20.19s/it]

 15%|█▌        | 409/2685 [2:16:15<12:41:27, 20.07s/it]

 15%|█▌        | 410/2685 [2:16:36<12:55:27, 20.45s/it]

 15%|█▌        | 411/2685 [2:16:54<12:23:56, 19.63s/it]

 15%|█▌        | 412/2685 [2:17:11<11:48:56, 18.71s/it]
{'loss': 0.4269, 'learning_rate': 1.9213199284651985e-06, 'rewards/chosen': 0.011618811637163162, 'rewards/rejected': -2.100529432296753, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1121480464935303, 'policy_logps/rejected': -482.3673400878906, 'policy_logps/chosen': -370.54248046875, 'referece_logps/rejected': -461.3620300292969, 'referece_logps/chosen': -370.65863037109375, 'logits/rejected': 0.05404524505138397, 'logits/chosen': 0.05575823411345482, 'epoch': 0.46}


 15%|█▌        | 414/2685 [2:17:49<11:59:12, 19.00s/it]

 15%|█▌        | 415/2685 [2:18:08<11:48:43, 18.73s/it]
{'loss': 0.3831, 'learning_rate': 1.9199066758674746e-06, 'rewards/chosen': -1.313092827796936, 'rewards/rejected': -1.8315372467041016, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5184445381164551, 'policy_logps/rejected': -487.865478515625, 'policy_logps/chosen': -478.475830078125, 'referece_logps/rejected': -469.55010986328125, 'referece_logps/chosen': -465.34490966796875, 'logits/rejected': -0.663772463798523, 'logits/chosen': -0.58128821849823, 'epoch': 0.46}


 16%|█▌        | 417/2685 [2:18:45<11:50:00, 18.78s/it]
{'loss': 0.3469, 'learning_rate': 1.9189578116202307e-06, 'rewards/chosen': -0.11105351150035858, 'rewards/rejected': -2.6728122234344482, 'rewards/accuracies': 0.875, 'rewards/margins': 2.561758518218994, 'policy_logps/rejected': -417.93499755859375, 'policy_logps/chosen': -407.452880859375, 'referece_logps/rejected': -391.20684814453125, 'referece_logps/chosen': -406.34234619140625, 'logits/rejected': -0.08857057988643646, 'logits/chosen': -0.19703532755374908, 'epoch': 0.47}


 16%|█▌        | 419/2685 [2:19:18<11:09:01, 17.71s/it]

 16%|█▌        | 420/2685 [2:19:40<11:56:28, 18.98s/it]

 16%|█▌        | 421/2685 [2:20:02<12:36:55, 20.06s/it]

 16%|█▌        | 422/2685 [2:20:25<13:00:05, 20.68s/it]

 16%|█▌        | 423/2685 [2:20:46<13:10:45, 20.98s/it]
{'loss': 0.4694, 'learning_rate': 1.9160791397007957e-06, 'rewards/chosen': -0.4598333239555359, 'rewards/rejected': -2.3745203018188477, 'rewards/accuracies': 1.0, 'rewards/margins': 1.914686918258667, 'policy_logps/rejected': -476.13116455078125, 'policy_logps/chosen': -501.161865234375, 'referece_logps/rejected': -452.3859558105469, 'referece_logps/chosen': -496.56353759765625, 'logits/rejected': -0.4162571430206299, 'logits/chosen': -0.37332847714424133, 'epoch': 0.47}


 16%|█▌        | 425/2685 [2:21:25<12:39:47, 20.17s/it]

 16%|█▌        | 426/2685 [2:21:42<12:10:39, 19.41s/it]

 16%|█▌        | 427/2685 [2:22:02<12:14:03, 19.51s/it]

 16%|█▌        | 428/2685 [2:22:22<12:14:02, 19.51s/it]

 16%|█▌        | 429/2685 [2:22:43<12:37:54, 20.16s/it]

 16%|█▌        | 430/2685 [2:23:03<12:35:22, 20.10s/it]

 16%|█▌        | 431/2685 [2:23:24<12:43:22, 20.32s/it]

 16%|█▌        | 432/2685 [2:23:47<13:09:37, 21.03s/it]

 16%|█▌        | 433/2685 [2:24:10<13:36:06, 21.74s/it]

 16%|█▌        | 434/2685 [2:24:30<13:14:41, 21.18s/it]

 16%|█▌        | 435/2685 [2:24:48<12:40:36, 20.28s/it]

 16%|█▌        | 436/2685 [2:25:10<12:53:44, 20.64s/it]
{'loss': 0.3506, 'learning_rate': 1.9096775514234535e-06, 'rewards/chosen': -0.7023078799247742, 'rewards/rejected': -1.3186825513839722, 'rewards/accuracies': 0.875, 'rewards/margins': 0.6163747310638428, 'policy_logps/rejected': -339.2260437011719, 'policy_logps/chosen': -379.69879150390625, 'referece_logps/rejected': -326.0392150878906, 'referece_logps/chosen': -372.67572021484375, 'logits/rejected': -0.21179775893688202, 'logits/chosen': -0.24645337462425232, 'epoch': 0.49}

 16%|█▋        | 437/2685 [2:25:29<12:40:48, 20.31s/it]


 16%|█▋        | 439/2685 [2:26:09<12:32:26, 20.10s/it]

 16%|█▋        | 440/2685 [2:26:30<12:45:12, 20.45s/it]

 16%|█▋        | 441/2685 [2:26:52<13:00:55, 20.88s/it]

 16%|█▋        | 442/2685 [2:27:13<12:54:46, 20.73s/it]

 16%|█▋        | 443/2685 [2:27:27<11:45:10, 18.87s/it]
{'loss': 0.4602, 'learning_rate': 1.9061377579914937e-06, 'rewards/chosen': -0.8688989877700806, 'rewards/rejected': -1.1486507654190063, 'rewards/accuracies': 0.625, 'rewards/margins': 0.2797517776489258, 'policy_logps/rejected': -364.7396240234375, 'policy_logps/chosen': -353.8455505371094, 'referece_logps/rejected': -353.25311279296875, 'referece_logps/chosen': -345.15655517578125, 'logits/rejected': -0.055027056485414505, 'logits/chosen': 0.05596145614981651, 'epoch': 0.49}


 17%|█▋        | 445/2685 [2:28:03<11:34:04, 18.59s/it]

 17%|█▋        | 446/2685 [2:28:23<11:46:14, 18.93s/it]

 17%|█▋        | 447/2685 [2:28:45<12:17:57, 19.78s/it]

 17%|█▋        | 448/2685 [2:29:01<11:40:56, 18.80s/it]
{'loss': 0.3574, 'learning_rate': 1.9035697524303523e-06, 'rewards/chosen': -1.5743072032928467, 'rewards/rejected': -3.8093204498291016, 'rewards/accuracies': 0.875, 'rewards/margins': 2.235013484954834, 'policy_logps/rejected': -510.8924560546875, 'policy_logps/chosen': -477.03778076171875, 'referece_logps/rejected': -472.79925537109375, 'referece_logps/chosen': -461.29473876953125, 'logits/rejected': 0.38947051763534546, 'logits/chosen': 0.3656337261199951, 'epoch': 0.5}

 17%|█▋        | 449/2685 [2:29:18<11:13:44, 18.08s/it]

 17%|█▋        | 450/2685 [2:29:40<11:56:56, 19.25s/it]

 17%|█▋        | 451/2685 [2:30:00<12:03:52, 19.44s/it]


 17%|█▋        | 453/2685 [2:30:39<12:12:27, 19.69s/it]
{'loss': 0.3639, 'learning_rate': 1.9009688679024189e-06, 'rewards/chosen': -1.8513000011444092, 'rewards/rejected': -2.880448579788208, 'rewards/accuracies': 0.625, 'rewards/margins': 1.0291485786437988, 'policy_logps/rejected': -603.1494750976562, 'policy_logps/chosen': -643.7801513671875, 'referece_logps/rejected': -574.344970703125, 'referece_logps/chosen': -625.2671508789062, 'logits/rejected': 0.47680994868278503, 'logits/chosen': 0.42697829008102417, 'epoch': 0.51}


 17%|█▋        | 455/2685 [2:31:17<11:49:14, 19.08s/it]

 17%|█▋        | 456/2685 [2:31:36<11:56:35, 19.29s/it]

 17%|█▋        | 457/2685 [2:31:57<12:06:59, 19.58s/it]

 17%|█▋        | 458/2685 [2:32:16<12:07:17, 19.59s/it]
{'loss': 0.303, 'learning_rate': 1.8983351990483084e-06, 'rewards/chosen': -0.8931241035461426, 'rewards/rejected': -3.080625295639038, 'rewards/accuracies': 1.0, 'rewards/margins': 2.1875011920928955, 'policy_logps/rejected': -548.202880859375, 'policy_logps/chosen': -520.739990234375, 'referece_logps/rejected': -517.3966674804688, 'referece_logps/chosen': -511.8087463378906, 'logits/rejected': 0.4342440962791443, 'logits/chosen': 0.3786660432815552, 'epoch': 0.51}


 17%|█▋        | 460/2685 [2:32:58<12:31:41, 20.27s/it]

 17%|█▋        | 461/2685 [2:33:18<12:33:57, 20.34s/it]

 17%|█▋        | 462/2685 [2:33:36<12:01:32, 19.47s/it]

 17%|█▋        | 463/2685 [2:33:57<12:19:56, 19.98s/it]

 17%|█▋        | 464/2685 [2:34:17<12:18:14, 19.94s/it]

 17%|█▋        | 465/2685 [2:34:39<12:40:28, 20.55s/it]
{'loss': 0.3744, 'learning_rate': 1.894593167678237e-06, 'rewards/chosen': -0.019946947693824768, 'rewards/rejected': -2.0014758110046387, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9815287590026855, 'policy_logps/rejected': -366.1025390625, 'policy_logps/chosen': -326.9488830566406, 'referece_logps/rejected': -346.0877685546875, 'referece_logps/chosen': -326.74945068359375, 'logits/rejected': -0.7863466143608093, 'logits/chosen': -0.7818036079406738, 'epoch': 0.52}


 17%|█▋        | 467/2685 [2:35:18<12:19:53, 20.02s/it]

 17%|█▋        | 468/2685 [2:35:41<12:54:04, 20.95s/it]

 17%|█▋        | 469/2685 [2:35:57<12:03:02, 19.58s/it]

 18%|█▊        | 470/2685 [2:36:17<12:07:48, 19.71s/it]

 18%|█▊        | 471/2685 [2:36:39<12:27:04, 20.25s/it]

 18%|█▊        | 472/2685 [2:36:58<12:19:32, 20.05s/it]

 18%|█▊        | 473/2685 [2:37:19<12:29:43, 20.34s/it]
{'loss': 0.365, 'learning_rate': 1.8902384508083516e-06, 'rewards/chosen': -0.8726181387901306, 'rewards/rejected': -2.615363836288452, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7427458763122559, 'policy_logps/rejected': -329.27459716796875, 'policy_logps/chosen': -280.7927551269531, 'referece_logps/rejected': -303.1209716796875, 'referece_logps/chosen': -272.0665588378906, 'logits/rejected': -0.6972799301147461, 'logits/chosen': -0.7134343981742859, 'epoch': 0.53}


 18%|█▊        | 475/2685 [2:37:53<11:28:06, 18.68s/it]

 18%|█▊        | 476/2685 [2:38:15<11:56:02, 19.45s/it]

 18%|█▊        | 477/2685 [2:38:34<11:58:49, 19.53s/it]
{'loss': 0.3559, 'learning_rate': 1.888029968750498e-06, 'rewards/chosen': -1.295436143875122, 'rewards/rejected': -3.1440436840057373, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8486075401306152, 'policy_logps/rejected': -366.84405517578125, 'policy_logps/chosen': -396.8782043457031, 'referece_logps/rejected': -335.4035949707031, 'referece_logps/chosen': -383.923828125, 'logits/rejected': -0.2846487760543823, 'logits/chosen': 0.018720686435699463, 'epoch': 0.53}


 18%|█▊        | 479/2685 [2:39:17<12:25:10, 20.27s/it]
{'loss': 0.328, 'learning_rate': 1.886917969238029e-06, 'rewards/chosen': -0.28779199719429016, 'rewards/rejected': -2.5844218730926514, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2966299057006836, 'policy_logps/rejected': -402.75408935546875, 'policy_logps/chosen': -469.6584777832031, 'referece_logps/rejected': -376.9098815917969, 'referece_logps/chosen': -466.780517578125, 'logits/rejected': -0.08319911360740662, 'logits/chosen': -0.060597553849220276, 'epoch': 0.54}


 18%|█▊        | 481/2685 [2:39:53<11:53:18, 19.42s/it]

 18%|█▊        | 482/2685 [2:40:13<11:55:28, 19.49s/it]
{'loss': 0.4436, 'learning_rate': 1.8852402900628278e-06, 'rewards/chosen': -1.0653692483901978, 'rewards/rejected': -2.868227958679199, 'rewards/accuracies': 0.625, 'rewards/margins': 1.802858591079712, 'policy_logps/rejected': -554.4048461914062, 'policy_logps/chosen': -518.6288452148438, 'referece_logps/rejected': -525.7225952148438, 'referece_logps/chosen': -507.97509765625, 'logits/rejected': 0.3255719542503357, 'logits/chosen': 0.34298425912857056, 'epoch': 0.54}


 18%|█▊        | 484/2685 [2:40:47<11:11:44, 18.31s/it]

 18%|█▊        | 485/2685 [2:41:06<11:09:17, 18.25s/it]

 18%|█▊        | 486/2685 [2:41:21<10:39:20, 17.44s/it]
{'loss': 0.3564, 'learning_rate': 1.8829853495629818e-06, 'rewards/chosen': -0.4876244366168976, 'rewards/rejected': -2.8136072158813477, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3259823322296143, 'policy_logps/rejected': -279.9965515136719, 'policy_logps/chosen': -250.9954376220703, 'referece_logps/rejected': -251.8604736328125, 'referece_logps/chosen': -246.1191864013672, 'logits/rejected': -0.52897709608078, 'logits/chosen': -0.4719066917896271, 'epoch': 0.54}


 18%|█▊        | 488/2685 [2:41:54<10:10:58, 16.69s/it]
{'loss': 0.3988, 'learning_rate': 1.8818501648171418e-06, 'rewards/chosen': -0.28335076570510864, 'rewards/rejected': -1.4034888744354248, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1201379299163818, 'policy_logps/rejected': -422.7294921875, 'policy_logps/chosen': -371.3658447265625, 'referece_logps/rejected': -408.69464111328125, 'referece_logps/chosen': -368.5323486328125, 'logits/rejected': -1.3832452297210693, 'logits/chosen': -1.4150326251983643, 'epoch': 0.55}


 18%|█▊        | 490/2685 [2:42:35<11:26:50, 18.77s/it]

 18%|█▊        | 491/2685 [2:42:53<11:18:12, 18.55s/it]
{'loss': 0.4121, 'learning_rate': 1.880137763156124e-06, 'rewards/chosen': -0.522732138633728, 'rewards/rejected': -1.704859972000122, 'rewards/accuracies': 1.0, 'rewards/margins': 1.1821279525756836, 'policy_logps/rejected': -390.66925048828125, 'policy_logps/chosen': -229.95181274414062, 'referece_logps/rejected': -373.6206359863281, 'referece_logps/chosen': -224.7244873046875, 'logits/rejected': -0.8980500102043152, 'logits/chosen': -0.8479844927787781, 'epoch': 0.55}


 18%|█▊        | 493/2685 [2:43:33<11:47:44, 19.37s/it]
{'loss': 0.2856, 'learning_rate': 1.878989755380062e-06, 'rewards/chosen': -0.5544093251228333, 'rewards/rejected': -1.9104366302490234, 'rewards/accuracies': 0.75, 'rewards/margins': 1.356027364730835, 'policy_logps/rejected': -288.6204833984375, 'policy_logps/chosen': -269.3861083984375, 'referece_logps/rejected': -269.51611328125, 'referece_logps/chosen': -263.842041015625, 'logits/rejected': -0.269317626953125, 'logits/chosen': -0.22957240045070648, 'epoch': 0.55}


 18%|█▊        | 495/2685 [2:44:12<11:38:52, 19.15s/it]

 18%|█▊        | 496/2685 [2:44:34<12:10:44, 20.03s/it]

 19%|█▊        | 497/2685 [2:44:56<12:27:03, 20.49s/it]

 19%|█▊        | 498/2685 [2:45:12<11:40:39, 19.22s/it]
{'loss': 0.4354, 'learning_rate': 1.876097361389604e-06, 'rewards/chosen': 0.00778542086482048, 'rewards/rejected': -1.8826555013656616, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8904409408569336, 'policy_logps/rejected': -476.14794921875, 'policy_logps/chosen': -418.304931640625, 'referece_logps/rejected': -457.32135009765625, 'referece_logps/chosen': -418.3828125, 'logits/rejected': -0.058169156312942505, 'logits/chosen': -0.08875399082899094, 'epoch': 0.56}


 19%|█▊        | 500/2685 [2:45:43<10:31:27, 17.34s/it]
 19%|█▊        | 500/2685 [2:45:43<10:31:27, 17.34s/it]/mnt/petrelfs/songmingyang/anaconda3/envs/vcd_origin/lib/python3.9/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.3942, 'learning_rate': 1.874346616144582e-06, 'rewards/chosen': -0.1515708863735199, 'rewards/rejected': -3.0213370323181152, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8697664737701416, 'policy_logps/rejected': -488.96954345703125, 'policy_logps/chosen': -323.7861328125, 'referece_logps/rejected': -458.7561950683594, 'referece_logps/chosen': -322.2703857421875, 'logits/rejected': -0.39948996901512146, 'logits/chosen': -0.5587812066078186, 'epoch': 0.56}

 19%|█▊        | 502/2685 [2:46:43<13:54:51, 22.95s/it]
{'loss': 0.4546, 'learning_rate': 1.8737604880064667e-06, 'rewards/chosen': -0.8528268933296204, 'rewards/rejected': -2.1654224395751953, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3125956058502197, 'policy_logps/rejected': -454.69830322265625, 'policy_logps/chosen': -400.8476867675781, 'referece_logps/rejected': -433.0440673828125, 'referece_logps/chosen': -392.31939697265625, 'logits/rejected': -0.28439319133758545, 'logits/chosen': -0.21833010017871857, 'epoch': 0.56}

 19%|█▊        | 503/2685 [2:47:03<13:19:48, 21.99s/it]

 19%|█▉        | 504/2685 [2:47:21<12:36:59, 20.82s/it]


 19%|█▉        | 506/2685 [2:48:02<12:27:15, 20.58s/it]

 19%|█▉        | 507/2685 [2:48:24<12:35:43, 20.82s/it]
{'loss': 0.4219, 'learning_rate': 1.8708107878245976e-06, 'rewards/chosen': -1.398964524269104, 'rewards/rejected': -1.9674147367477417, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5684503316879272, 'policy_logps/rejected': -312.7021484375, 'policy_logps/chosen': -294.43096923828125, 'referece_logps/rejected': -293.02801513671875, 'referece_logps/chosen': -280.44134521484375, 'logits/rejected': -0.3533993363380432, 'logits/chosen': -0.31537535786628723, 'epoch': 0.57}


 19%|█▉        | 509/2685 [2:49:04<12:19:48, 20.40s/it]

 19%|█▉        | 510/2685 [2:49:24<12:11:03, 20.17s/it]

 19%|█▉        | 511/2685 [2:49:44<12:15:28, 20.30s/it]

 19%|█▉        | 512/2685 [2:50:02<11:45:24, 19.48s/it]
{'loss': 0.3612, 'learning_rate': 1.8678294007044515e-06, 'rewards/chosen': -1.6704784631729126, 'rewards/rejected': -3.772275924682617, 'rewards/accuracies': 0.875, 'rewards/margins': 2.101797342300415, 'policy_logps/rejected': -504.8742980957031, 'policy_logps/chosen': -601.5346069335938, 'referece_logps/rejected': -467.15155029296875, 'referece_logps/chosen': -584.8297729492188, 'logits/rejected': 0.5511137843132019, 'logits/chosen': 0.4510314464569092, 'epoch': 0.57}

 19%|█▉        | 513/2685 [2:50:21<11:45:19, 19.48s/it]


 19%|█▉        | 514/2685 [2:50:41<11:44:38, 19.47s/it]
{'loss': 0.4412, 'learning_rate': 1.8660254037844386e-06, 'rewards/chosen': -1.1637415885925293, 'rewards/rejected': -2.3586924076080322, 'rewards/accuracies': 0.625, 'rewards/margins': 1.194950819015503, 'policy_logps/rejected': -379.1979675292969, 'policy_logps/chosen': -373.405029296875, 'referece_logps/rejected': -355.6109924316406, 'referece_logps/chosen': -361.76763916015625, 'logits/rejected': -0.043991588056087494, 'logits/chosen': -0.12556545436382294, 'epoch': 0.58}

 19%|█▉        | 516/2685 [2:51:18<11:27:33, 19.02s/it]
{'loss': 0.3286, 'learning_rate': 1.865421549276919e-06, 'rewards/chosen': -0.13590528070926666, 'rewards/rejected': -1.9824244976043701, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8465193510055542, 'policy_logps/rejected': -436.112060546875, 'policy_logps/chosen': -452.1310729980469, 'referece_logps/rejected': -416.2878112792969, 'referece_logps/chosen': -450.77203369140625, 'logits/rejected': -0.47385549545288086, 'logits/chosen': -0.596796989440918, 'epoch': 0.58}


 19%|█▉        | 518/2685 [2:51:52<10:45:19, 17.87s/it]

 19%|█▉        | 519/2685 [2:52:12<11:12:01, 18.62s/it]
{'loss': 0.3834, 'learning_rate': 1.8636024314567065e-06, 'rewards/chosen': -0.008704662322998047, 'rewards/rejected': -1.9602266550064087, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9515219926834106, 'policy_logps/rejected': -268.4337158203125, 'policy_logps/chosen': -237.97244262695312, 'referece_logps/rejected': -248.83145141601562, 'referece_logps/chosen': -237.8854217529297, 'logits/rejected': -0.3030053675174713, 'logits/chosen': -0.26209524273872375, 'epoch': 0.58}

 19%|█▉        | 520/2685 [2:52:29<10:52:18, 18.08s/it]


 19%|█▉        | 522/2685 [2:52:58<9:56:16, 16.54s/it]

 19%|█▉        | 523/2685 [2:53:18<10:27:59, 17.43s/it]

 20%|█▉        | 524/2685 [2:53:38<10:58:40, 18.29s/it]

 20%|█▉        | 525/2685 [2:53:58<11:13:49, 18.72s/it]

 20%|█▉        | 526/2685 [2:54:19<11:38:33, 19.41s/it]
{'loss': 0.4094, 'learning_rate': 1.8593138700906487e-06, 'rewards/chosen': -1.1364632844924927, 'rewards/rejected': -2.3346128463745117, 'rewards/accuracies': 0.75, 'rewards/margins': 1.1981496810913086, 'policy_logps/rejected': -384.3025207519531, 'policy_logps/chosen': -395.4952087402344, 'referece_logps/rejected': -360.9563903808594, 'referece_logps/chosen': -384.13055419921875, 'logits/rejected': 0.04266161099076271, 'logits/chosen': 0.11901049315929413, 'epoch': 0.59}


 20%|█▉        | 528/2685 [2:54:58<11:41:56, 19.53s/it]
{'loss': 0.3824, 'learning_rate': 1.858077296698318e-06, 'rewards/chosen': -0.2980717718601227, 'rewards/rejected': -1.8784229755401611, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5803511142730713, 'policy_logps/rejected': -407.5084533691406, 'policy_logps/chosen': -366.3898620605469, 'referece_logps/rejected': -388.7242431640625, 'referece_logps/chosen': -363.4091491699219, 'logits/rejected': -0.1864873766899109, 'logits/chosen': -0.2018209993839264, 'epoch': 0.59}


 20%|█▉        | 530/2685 [2:55:40<12:03:18, 20.14s/it]
{'loss': 0.4196, 'learning_rate': 1.8568357275182502e-06, 'rewards/chosen': -0.42808228731155396, 'rewards/rejected': -2.251635789871216, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8235533237457275, 'policy_logps/rejected': -349.08038330078125, 'policy_logps/chosen': -313.982666015625, 'referece_logps/rejected': -326.5639953613281, 'referece_logps/chosen': -309.7018127441406, 'logits/rejected': -0.048291824758052826, 'logits/chosen': -0.018104683607816696, 'epoch': 0.59}


 20%|█▉        | 532/2685 [2:56:21<11:56:36, 19.97s/it]

 20%|█▉        | 533/2685 [2:56:43<12:21:44, 20.68s/it]

 20%|█▉        | 534/2685 [2:57:02<12:06:49, 20.27s/it]

 20%|█▉        | 535/2685 [2:57:19<11:25:29, 19.13s/it]
{'loss': 0.4733, 'learning_rate': 1.8537099955052599e-06, 'rewards/chosen': -0.36876457929611206, 'rewards/rejected': -2.131528615951538, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7627637386322021, 'policy_logps/rejected': -344.55517578125, 'policy_logps/chosen': -280.6086120605469, 'referece_logps/rejected': -323.2398681640625, 'referece_logps/chosen': -276.92095947265625, 'logits/rejected': -0.1601080447435379, 'logits/chosen': -0.17432980239391327, 'epoch': 0.6}


 20%|██        | 537/2685 [2:57:58<11:33:39, 19.38s/it]

 20%|██        | 538/2685 [2:58:19<11:42:31, 19.63s/it]

 20%|██        | 539/2685 [2:58:40<12:03:35, 20.23s/it]

 20%|██        | 540/2685 [2:58:57<11:25:19, 19.17s/it]

 20%|██        | 541/2685 [2:59:17<11:32:14, 19.37s/it]
{'loss': 0.3832, 'learning_rate': 1.8499181218045365e-06, 'rewards/chosen': -0.22297175228595734, 'rewards/rejected': -3.302269697189331, 'rewards/accuracies': 0.875, 'rewards/margins': 3.079298257827759, 'policy_logps/rejected': -395.060302734375, 'policy_logps/chosen': -374.9530029296875, 'referece_logps/rejected': -362.03759765625, 'referece_logps/chosen': -372.7232666015625, 'logits/rejected': -0.538115382194519, 'logits/chosen': -0.5523343682289124, 'epoch': 0.6}

 20%|██        | 542/2685 [2:59:36<11:28:39, 19.28s/it]


 20%|██        | 544/2685 [3:00:12<11:04:02, 18.61s/it]
{'loss': 0.3604, 'learning_rate': 1.8480054720497366e-06, 'rewards/chosen': -0.6421321630477905, 'rewards/rejected': -1.447364091873169, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8052319884300232, 'policy_logps/rejected': -306.4293518066406, 'policy_logps/chosen': -322.3123779296875, 'referece_logps/rejected': -291.9557189941406, 'referece_logps/chosen': -315.89111328125, 'logits/rejected': -1.1717743873596191, 'logits/chosen': -1.1341334581375122, 'epoch': 0.61}


 20%|██        | 546/2685 [3:00:45<10:15:46, 17.27s/it]

 20%|██        | 547/2685 [3:01:05<10:47:03, 18.16s/it]

 20%|██        | 548/2685 [3:01:23<10:44:52, 18.11s/it]

 20%|██        | 549/2685 [3:01:41<10:45:13, 18.12s/it]

 20%|██        | 550/2685 [3:02:01<10:59:46, 18.54s/it]
{'loss': 0.4101, 'learning_rate': 1.8441468720061813e-06, 'rewards/chosen': -0.5357052683830261, 'rewards/rejected': -2.062624216079712, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5269190073013306, 'policy_logps/rejected': -249.5045166015625, 'policy_logps/chosen': -225.723876953125, 'referece_logps/rejected': -228.87826538085938, 'referece_logps/chosen': -220.3668212890625, 'logits/rejected': -0.4917481243610382, 'logits/chosen': -0.5497235655784607, 'epoch': 0.61}


 21%|██        | 552/2685 [3:02:35<10:24:05, 17.56s/it]
{'loss': 0.5079, 'learning_rate': 1.8428508326109156e-06, 'rewards/chosen': -0.8404287099838257, 'rewards/rejected': -1.813788652420044, 'rewards/accuracies': 0.5, 'rewards/margins': 0.9733600616455078, 'policy_logps/rejected': -358.76702880859375, 'policy_logps/chosen': -439.91412353515625, 'referece_logps/rejected': -340.629150390625, 'referece_logps/chosen': -431.5098876953125, 'logits/rejected': 0.09340746700763702, 'logits/chosen': 0.11486673355102539, 'epoch': 0.62}

 21%|██        | 553/2685 [3:02:56<11:06:20, 18.75s/it]


 21%|██        | 555/2685 [3:03:35<11:14:16, 18.99s/it]

 21%|██        | 556/2685 [3:03:53<11:05:47, 18.76s/it]

 21%|██        | 557/2685 [3:04:10<10:42:02, 18.10s/it]

 21%|██        | 558/2685 [3:04:31<11:14:36, 19.03s/it]
{'loss': 0.4117, 'learning_rate': 1.8389333019214525e-06, 'rewards/chosen': -0.9538972973823547, 'rewards/rejected': -2.223958730697632, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2700616121292114, 'policy_logps/rejected': -389.2958679199219, 'policy_logps/chosen': -414.65765380859375, 'referece_logps/rejected': -367.0562744140625, 'referece_logps/chosen': -405.1186828613281, 'logits/rejected': 0.10695189237594604, 'logits/chosen': 0.11773841828107834, 'epoch': 0.62}


 21%|██        | 560/2685 [3:05:11<11:33:03, 19.57s/it]
{'loss': 0.4401, 'learning_rate': 1.8376176795327982e-06, 'rewards/chosen': -0.9396295547485352, 'rewards/rejected': -3.3845062255859375, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4448764324188232, 'policy_logps/rejected': -346.0208740234375, 'policy_logps/chosen': -253.25291442871094, 'referece_logps/rejected': -312.1758117675781, 'referece_logps/chosen': -243.85662841796875, 'logits/rejected': -0.03262241184711456, 'logits/chosen': 0.05568055808544159, 'epoch': 0.63}


 21%|██        | 562/2685 [3:05:49<11:20:00, 19.22s/it]

 21%|██        | 563/2685 [3:06:09<11:34:59, 19.65s/it]
{'loss': 0.3947, 'learning_rate': 1.83563510459427e-06, 'rewards/chosen': -0.4418577551841736, 'rewards/rejected': -2.2962324619293213, 'rewards/accuracies': 0.875, 'rewards/margins': 1.854374885559082, 'policy_logps/rejected': -485.2332763671875, 'policy_logps/chosen': -375.6017150878906, 'referece_logps/rejected': -462.27093505859375, 'referece_logps/chosen': -371.18310546875, 'logits/rejected': -0.7311810255050659, 'logits/chosen': -0.7394763231277466, 'epoch': 0.63}

 21%|██        | 564/2685 [3:06:30<11:49:35, 20.07s/it]

 21%|██        | 565/2685 [3:06:54<12:27:32, 21.16s/it]


 21%|██        | 567/2685 [3:07:32<11:39:36, 19.82s/it]
{'loss': 0.3279, 'learning_rate': 1.8329746479158263e-06, 'rewards/chosen': -0.6938846111297607, 'rewards/rejected': -1.6251882314682007, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9313035607337952, 'policy_logps/rejected': -436.1850280761719, 'policy_logps/chosen': -441.97705078125, 'referece_logps/rejected': -419.9331359863281, 'referece_logps/chosen': -435.03826904296875, 'logits/rejected': 0.29186493158340454, 'logits/chosen': 0.2886888384819031, 'epoch': 0.63}

 21%|██        | 568/2685 [3:07:54<12:07:10, 20.61s/it]


 21%|██        | 570/2685 [3:08:30<11:26:59, 19.49s/it]

 21%|██▏       | 571/2685 [3:08:53<12:09:05, 20.69s/it]
{'loss': 0.419, 'learning_rate': 1.8302947927123763e-06, 'rewards/chosen': -1.3341504335403442, 'rewards/rejected': -3.0662689208984375, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7321181297302246, 'policy_logps/rejected': -407.7861328125, 'policy_logps/chosen': -320.1822814941406, 'referece_logps/rejected': -377.1234436035156, 'referece_logps/chosen': -306.84075927734375, 'logits/rejected': -0.5603259801864624, 'logits/chosen': -0.567450761795044, 'epoch': 0.64}


 21%|██▏       | 573/2685 [3:09:36<12:18:12, 20.97s/it]

 21%|██▏       | 574/2685 [3:09:53<11:39:05, 19.87s/it]

 21%|██▏       | 575/2685 [3:10:14<11:49:36, 20.18s/it]

 21%|██▏       | 576/2685 [3:10:34<11:48:07, 20.15s/it]
{'loss': 0.3688, 'learning_rate': 1.8269177896516058e-06, 'rewards/chosen': -0.8818637132644653, 'rewards/rejected': -1.8629056215286255, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9810417890548706, 'policy_logps/rejected': -398.6512451171875, 'policy_logps/chosen': -353.8073425292969, 'referece_logps/rejected': -380.022216796875, 'referece_logps/chosen': -344.98870849609375, 'logits/rejected': 0.6689516305923462, 'logits/chosen': 0.6116228103637695, 'epoch': 0.64}

 21%|██▏       | 577/2685 [3:10:50<11:07:01, 18.99s/it]


 22%|██▏       | 579/2685 [3:11:31<11:35:43, 19.82s/it]
{'loss': 0.2793, 'learning_rate': 1.8248771368173522e-06, 'rewards/chosen': -0.7045807242393494, 'rewards/rejected': -3.53776216506958, 'rewards/accuracies': 1.0, 'rewards/margins': 2.833181619644165, 'policy_logps/rejected': -355.97869873046875, 'policy_logps/chosen': -297.96966552734375, 'referece_logps/rejected': -320.60107421875, 'referece_logps/chosen': -290.92388916015625, 'logits/rejected': -0.2775173783302307, 'logits/chosen': -0.35722729563713074, 'epoch': 0.65}

 22%|██▏       | 580/2685 [3:11:55<12:13:12, 20.90s/it]


 22%|██▏       | 582/2685 [3:12:38<12:25:21, 21.27s/it]
{'loss': 0.4323, 'learning_rate': 1.8228256783781105e-06, 'rewards/chosen': -1.7173881530761719, 'rewards/rejected': -2.4723682403564453, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7549799680709839, 'policy_logps/rejected': -476.4300231933594, 'policy_logps/chosen': -406.9521484375, 'referece_logps/rejected': -451.70635986328125, 'referece_logps/chosen': -389.7782287597656, 'logits/rejected': -0.4193277359008789, 'logits/chosen': -0.383002907037735, 'epoch': 0.65}

 22%|██▏       | 583/2685 [3:12:56<11:53:30, 20.37s/it]

 22%|██▏       | 584/2685 [3:13:14<11:28:25, 19.66s/it]

 22%|██▏       | 585/2685 [3:13:32<11:11:30, 19.19s/it]

 22%|██▏       | 586/2685 [3:13:54<11:32:10, 19.79s/it]

 22%|██▏       | 587/2685 [3:14:16<11:54:31, 20.43s/it]

 22%|██▏       | 588/2685 [3:14:35<11:48:54, 20.28s/it]


 22%|██▏       | 590/2685 [3:15:16<11:52:36, 20.41s/it]
{'loss': 0.3573, 'learning_rate': 1.8173024999557492e-06, 'rewards/chosen': -2.2818994522094727, 'rewards/rejected': -3.3258180618286133, 'rewards/accuracies': 0.875, 'rewards/margins': 1.0439186096191406, 'policy_logps/rejected': -617.0853881835938, 'policy_logps/chosen': -578.1588745117188, 'referece_logps/rejected': -583.8272094726562, 'referece_logps/chosen': -555.3399047851562, 'logits/rejected': 0.44189298152923584, 'logits/chosen': 0.38825225830078125, 'epoch': 0.66}

 22%|██▏       | 591/2685 [3:15:34<11:29:00, 19.74s/it]

 22%|██▏       | 592/2685 [3:15:52<11:10:47, 19.23s/it]

 22%|██▏       | 593/2685 [3:16:14<11:39:06, 20.05s/it]

 22%|██▏       | 594/2685 [3:16:35<11:41:01, 20.12s/it]

 22%|██▏       | 595/2685 [3:16:55<11:47:37, 20.31s/it]

 22%|██▏       | 596/2685 [3:17:14<11:25:05, 19.68s/it]

 22%|██▏       | 597/2685 [3:17:33<11:25:20, 19.69s/it]

 22%|██▏       | 598/2685 [3:17:55<11:41:02, 20.15s/it]


 22%|██▏       | 600/2685 [3:18:30<10:52:20, 18.77s/it]

 22%|██▏       | 601/2685 [3:18:50<11:02:42, 19.08s/it]
{'loss': 0.3687, 'learning_rate': 1.8095839237202326e-06, 'rewards/chosen': -0.8404764533042908, 'rewards/rejected': -2.660526752471924, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8200503587722778, 'policy_logps/rejected': -429.76104736328125, 'policy_logps/chosen': -384.6835021972656, 'referece_logps/rejected': -403.1557922363281, 'referece_logps/chosen': -376.27874755859375, 'logits/rejected': -0.4628366529941559, 'logits/chosen': -0.5127438902854919, 'epoch': 0.67}

 22%|██▏       | 602/2685 [3:19:08<10:45:00, 18.58s/it]

 22%|██▏       | 603/2685 [3:19:25<10:36:42, 18.35s/it]


 23%|██▎       | 605/2685 [3:20:11<11:45:15, 20.34s/it]
{'loss': 0.3689, 'learning_rate': 1.8067417460341848e-06, 'rewards/chosen': 0.04783334583044052, 'rewards/rejected': -2.1955726146698, 'rewards/accuracies': 1.0, 'rewards/margins': 2.243406057357788, 'policy_logps/rejected': -385.11895751953125, 'policy_logps/chosen': -352.435546875, 'referece_logps/rejected': -363.1632385253906, 'referece_logps/chosen': -352.91387939453125, 'logits/rejected': 0.29408523440361023, 'logits/chosen': 0.2210671305656433, 'epoch': 0.68}

 23%|██▎       | 606/2685 [3:20:30<11:34:03, 20.03s/it]

 23%|██▎       | 607/2685 [3:20:49<11:27:59, 19.86s/it]

 23%|██▎       | 608/2685 [3:21:09<11:25:07, 19.79s/it]


 23%|██▎       | 610/2685 [3:21:50<11:38:23, 20.19s/it]
{'loss': 0.449, 'learning_rate': 1.8031626116531883e-06, 'rewards/chosen': -0.9649685025215149, 'rewards/rejected': -1.2375221252441406, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2725536823272705, 'policy_logps/rejected': -449.7491149902344, 'policy_logps/chosen': -463.1202392578125, 'referece_logps/rejected': -437.3739318847656, 'referece_logps/chosen': -453.4705505371094, 'logits/rejected': -0.23583108186721802, 'logits/chosen': -0.3819471001625061, 'epoch': 0.68}

 23%|██▎       | 611/2685 [3:22:13<12:10:02, 21.12s/it]

 23%|██▎       | 612/2685 [3:22:32<11:42:40, 20.34s/it]


 23%|██▎       | 614/2685 [3:23:12<11:39:30, 20.27s/it]

 23%|██▎       | 615/2685 [3:23:28<10:53:46, 18.95s/it]
{'loss': 0.3152, 'learning_rate': 1.7995542519060644e-06, 'rewards/chosen': -0.1692432463169098, 'rewards/rejected': -2.889359951019287, 'rewards/accuracies': 1.0, 'rewards/margins': 2.72011661529541, 'policy_logps/rejected': -455.5787658691406, 'policy_logps/chosen': -393.70306396484375, 'referece_logps/rejected': -426.6852111816406, 'referece_logps/chosen': -392.0106506347656, 'logits/rejected': -0.050230205059051514, 'logits/chosen': -0.2201816439628601, 'epoch': 0.69}

 23%|██▎       | 616/2685 [3:23:48<11:03:42, 19.25s/it]

 23%|██▎       | 617/2685 [3:24:08<11:10:38, 19.46s/it]

 23%|██▎       | 618/2685 [3:24:25<10:48:39, 18.83s/it]

 23%|██▎       | 619/2685 [3:24:45<10:56:45, 19.07s/it]

 23%|██▎       | 620/2685 [3:25:04<10:50:10, 18.89s/it]


 23%|██▎       | 622/2685 [3:25:43<11:04:03, 19.31s/it]
{'loss': 0.4267, 'learning_rate': 1.7944536998427866e-06, 'rewards/chosen': -1.0697879791259766, 'rewards/rejected': -2.68859601020813, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6188081502914429, 'policy_logps/rejected': -374.2013244628906, 'policy_logps/chosen': -373.8794860839844, 'referece_logps/rejected': -347.31536865234375, 'referece_logps/chosen': -363.18157958984375, 'logits/rejected': -0.6645984053611755, 'logits/chosen': -0.8029506802558899, 'epoch': 0.69}


 23%|██▎       | 624/2685 [3:26:25<11:32:46, 20.17s/it]

 23%|██▎       | 625/2685 [3:26:41<10:46:52, 18.84s/it]

 23%|██▎       | 626/2685 [3:27:01<10:59:38, 19.22s/it]
{'loss': 0.4736, 'learning_rate': 1.7915136357872309e-06, 'rewards/chosen': -0.5756765604019165, 'rewards/rejected': -2.5198476314544678, 'rewards/accuracies': 1.0, 'rewards/margins': 1.9441709518432617, 'policy_logps/rejected': -263.750732421875, 'policy_logps/chosen': -265.3558349609375, 'referece_logps/rejected': -238.55224609375, 'referece_logps/chosen': -259.5990905761719, 'logits/rejected': -1.300680160522461, 'logits/chosen': -1.1494672298431396, 'epoch': 0.7}

 23%|██▎       | 627/2685 [3:27:20<11:00:50, 19.27s/it]

 23%|██▎       | 628/2685 [3:27:40<11:04:30, 19.38s/it]


 23%|██▎       | 630/2685 [3:28:21<11:17:53, 19.79s/it]

 24%|██▎       | 631/2685 [3:28:41<11:22:39, 19.94s/it]
{'loss': 0.3517, 'learning_rate': 1.7878126424177157e-06, 'rewards/chosen': -1.2152588367462158, 'rewards/rejected': -2.945632219314575, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7303731441497803, 'policy_logps/rejected': -429.4814147949219, 'policy_logps/chosen': -325.7710266113281, 'referece_logps/rejected': -400.0251159667969, 'referece_logps/chosen': -313.61846923828125, 'logits/rejected': -0.2713076174259186, 'logits/chosen': -0.4972424805164337, 'epoch': 0.71}

 24%|██▎       | 632/2685 [3:29:02<11:33:44, 20.28s/it]


 24%|██▎       | 634/2685 [3:29:41<11:20:21, 19.90s/it]

 24%|██▎       | 635/2685 [3:30:03<11:38:46, 20.45s/it]

 24%|██▎       | 636/2685 [3:30:25<11:56:00, 20.97s/it]
{'loss': 0.3376, 'learning_rate': 1.7840829822345558e-06, 'rewards/chosen': -1.0462615489959717, 'rewards/rejected': -2.9989123344421387, 'rewards/accuracies': 0.875, 'rewards/margins': 1.952650785446167, 'policy_logps/rejected': -510.1712341308594, 'policy_logps/chosen': -493.17034912109375, 'referece_logps/rejected': -480.1820983886719, 'referece_logps/chosen': -482.7077331542969, 'logits/rejected': -0.05954744666814804, 'logits/chosen': -0.0006843805313110352, 'epoch': 0.71}


 24%|██▍       | 638/2685 [3:31:01<11:00:32, 19.36s/it]
{'loss': 0.3441, 'learning_rate': 1.7825831218185472e-06, 'rewards/chosen': -0.3708110451698303, 'rewards/rejected': -1.8783823251724243, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5075712203979492, 'policy_logps/rejected': -334.7076110839844, 'policy_logps/chosen': -304.11566162109375, 'referece_logps/rejected': -315.92376708984375, 'referece_logps/chosen': -300.4075622558594, 'logits/rejected': 0.09011851251125336, 'logits/chosen': 0.10415609925985336, 'epoch': 0.71}

 24%|██▍       | 639/2685 [3:31:22<11:18:58, 19.91s/it]

 24%|██▍       | 640/2685 [3:31:42<11:14:20, 19.78s/it]

 24%|██▍       | 641/2685 [3:32:01<11:03:23, 19.47s/it]

 24%|██▍       | 642/2685 [3:32:18<10:42:01, 18.86s/it]

 24%|██▍       | 643/2685 [3:32:38<10:50:08, 19.10s/it]

 24%|██▍       | 644/2685 [3:32:54<10:23:32, 18.33s/it]

 24%|██▍       | 645/2685 [3:33:14<10:42:53, 18.91s/it]


 24%|██▍       | 647/2685 [3:33:57<11:22:59, 20.11s/it]

 24%|██▍       | 648/2685 [3:34:17<11:19:15, 20.01s/it]
{'loss': 0.3731, 'learning_rate': 1.7750156514834587e-06, 'rewards/chosen': -1.5087085962295532, 'rewards/rejected': -3.114055871963501, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6053475141525269, 'policy_logps/rejected': -374.01373291015625, 'policy_logps/chosen': -310.9688720703125, 'referece_logps/rejected': -342.8731994628906, 'referece_logps/chosen': -295.8817443847656, 'logits/rejected': -0.5009039044380188, 'logits/chosen': -0.38323819637298584, 'epoch': 0.72}


 24%|██▍       | 650/2685 [3:34:57<11:19:53, 20.05s/it]
{'loss': 0.346, 'learning_rate': 1.773488585447203e-06, 'rewards/chosen': -1.238321304321289, 'rewards/rejected': -2.8658690452575684, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6275479793548584, 'policy_logps/rejected': -266.1507263183594, 'policy_logps/chosen': -278.945556640625, 'referece_logps/rejected': -237.49203491210938, 'referece_logps/chosen': -266.5623474121094, 'logits/rejected': -0.8827472925186157, 'logits/chosen': -0.8149358034133911, 'epoch': 0.73}


 24%|██▍       | 652/2685 [3:35:37<11:16:33, 19.97s/it]
{'loss': 0.4673, 'learning_rate': 1.7719570161047708e-06, 'rewards/chosen': -1.2241618633270264, 'rewards/rejected': -2.4480466842651367, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2238849401474, 'policy_logps/rejected': -428.8865661621094, 'policy_logps/chosen': -330.1875305175781, 'referece_logps/rejected': -404.4060974121094, 'referece_logps/chosen': -317.9459228515625, 'logits/rejected': -0.046896979212760925, 'logits/chosen': -0.1450205147266388, 'epoch': 0.73}

 24%|██▍       | 653/2685 [3:35:57<11:13:10, 19.88s/it]

 24%|██▍       | 654/2685 [3:36:19<11:34:18, 20.51s/it]


 24%|██▍       | 656/2685 [3:36:55<10:43:36, 19.03s/it]
{'loss': 0.3681, 'learning_rate': 1.768880403195172e-06, 'rewards/chosen': -0.727665364742279, 'rewards/rejected': -2.197458505630493, 'rewards/accuracies': 0.75, 'rewards/margins': 1.4697932004928589, 'policy_logps/rejected': -264.44586181640625, 'policy_logps/chosen': -252.16513061523438, 'referece_logps/rejected': -242.4712677001953, 'referece_logps/chosen': -244.88848876953125, 'logits/rejected': -0.36646461486816406, 'logits/chosen': -0.24938562512397766, 'epoch': 0.73}


 25%|██▍       | 658/2685 [3:37:31<10:17:28, 18.28s/it]
{'loss': 0.3704, 'learning_rate': 1.7673353775402665e-06, 'rewards/chosen': -0.20177555084228516, 'rewards/rejected': -2.5670840740203857, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3653082847595215, 'policy_logps/rejected': -350.4640197753906, 'policy_logps/chosen': -287.0027160644531, 'referece_logps/rejected': -324.793212890625, 'referece_logps/chosen': -284.9849853515625, 'logits/rejected': -0.4156281352043152, 'logits/chosen': -0.2487156242132187, 'epoch': 0.74}

 25%|██▍       | 659/2685 [3:37:49<10:11:35, 18.11s/it]


 25%|██▍       | 661/2685 [3:38:30<10:42:09, 19.04s/it]
{'loss': 0.438, 'learning_rate': 1.765009465347743e-06, 'rewards/chosen': -0.17320767045021057, 'rewards/rejected': -1.6272861957550049, 'rewards/accuracies': 1.0, 'rewards/margins': 1.4540785551071167, 'policy_logps/rejected': -365.58062744140625, 'policy_logps/chosen': -280.5543212890625, 'referece_logps/rejected': -349.30780029296875, 'referece_logps/chosen': -278.8222351074219, 'logits/rejected': -0.7283908128738403, 'logits/chosen': -0.738499641418457, 'epoch': 0.74}

 25%|██▍       | 662/2685 [3:38:43<9:45:23, 17.36s/it]

 25%|██▍       | 663/2685 [3:39:02<10:02:12, 17.87s/it]

 25%|██▍       | 664/2685 [3:39:24<10:47:19, 19.22s/it]

 25%|██▍       | 665/2685 [3:39:44<10:52:59, 19.40s/it]

 25%|██▍       | 666/2685 [3:40:03<10:42:53, 19.11s/it]

 25%|██▍       | 667/2685 [3:40:23<10:52:43, 19.41s/it]

 25%|██▍       | 668/2685 [3:40:37<9:57:00, 17.76s/it]

 25%|██▍       | 669/2685 [3:40:56<10:14:21, 18.28s/it]

 25%|██▍       | 670/2685 [3:41:10<9:24:19, 16.80s/it]

 25%|██▍       | 671/2685 [3:41:25<9:06:08, 16.27s/it]

 25%|██▌       | 672/2685 [3:41:44<9:39:31, 17.27s/it]

 25%|██▌       | 673/2685 [3:42:08<10:40:48, 19.11s/it]

 25%|██▌       | 674/2685 [3:42:25<10:21:09, 18.53s/it]

 25%|██▌       | 675/2685 [3:42:42<10:03:46, 18.02s/it]


 25%|██▌       | 677/2685 [3:43:16<9:46:06, 17.51s/it]
{'loss': 0.4268, 'learning_rate': 1.7524360998424197e-06, 'rewards/chosen': -1.4599287509918213, 'rewards/rejected': -3.5839953422546387, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1240663528442383, 'policy_logps/rejected': -361.37060546875, 'policy_logps/chosen': -384.1143798828125, 'referece_logps/rejected': -325.5306396484375, 'referece_logps/chosen': -369.51513671875, 'logits/rejected': 0.15795935690402985, 'logits/chosen': 0.054602570831775665, 'epoch': 0.76}

 25%|██▌       | 678/2685 [3:43:37<10:19:17, 18.51s/it]


 25%|██▌       | 680/2685 [3:44:18<10:55:10, 19.61s/it]
{'loss': 0.4712, 'learning_rate': 1.7500472390247278e-06, 'rewards/chosen': -0.9069857001304626, 'rewards/rejected': -2.436481237411499, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5294952392578125, 'policy_logps/rejected': -418.8170166015625, 'policy_logps/chosen': -413.51226806640625, 'referece_logps/rejected': -394.45220947265625, 'referece_logps/chosen': -404.44244384765625, 'logits/rejected': -1.0814114809036255, 'logits/chosen': -1.1121876239776611, 'epoch': 0.76}

 25%|██▌       | 681/2685 [3:44:39<11:11:01, 20.09s/it]

 25%|██▌       | 682/2685 [3:44:57<10:46:36, 19.37s/it]

 25%|██▌       | 683/2685 [3:45:18<11:02:29, 19.86s/it]

 25%|██▌       | 684/2685 [3:45:41<11:34:14, 20.82s/it]

 26%|██▌       | 685/2685 [3:46:01<11:28:20, 20.65s/it]

 26%|██▌       | 686/2685 [3:46:18<10:45:36, 19.38s/it]

 26%|██▌       | 687/2685 [3:46:36<10:30:49, 18.94s/it]

 26%|██▌       | 688/2685 [3:46:57<10:50:03, 19.53s/it]

 26%|██▌       | 689/2685 [3:47:19<11:13:53, 20.26s/it]


 26%|██▌       | 691/2685 [3:48:00<11:20:05, 20.46s/it]

 26%|██▌       | 692/2685 [3:48:20<11:10:42, 20.19s/it]

 26%|██▌       | 693/2685 [3:48:37<10:43:04, 19.37s/it]

 26%|██▌       | 694/2685 [3:48:53<10:11:11, 18.42s/it]

 26%|██▌       | 695/2685 [3:49:13<10:21:21, 18.73s/it]

 26%|██▌       | 696/2685 [3:49:33<10:34:02, 19.13s/it]

 26%|██▌       | 697/2685 [3:49:54<10:52:32, 19.69s/it]

 26%|██▌       | 698/2685 [3:50:15<11:05:00, 20.08s/it]

 26%|██▌       | 699/2685 [3:50:34<10:59:46, 19.93s/it]

 26%|██▌       | 700/2685 [3:50:54<10:57:00, 19.86s/it]

 26%|██▌       | 701/2685 [3:51:14<10:54:16, 19.79s/it]

 26%|██▌       | 702/2685 [3:51:32<10:38:45, 19.33s/it]

 26%|██▌       | 703/2685 [3:51:53<10:55:59, 19.86s/it]

 26%|██▌       | 704/2685 [3:52:14<11:09:25, 20.28s/it]

 26%|██▋       | 705/2685 [3:52:33<10:51:17, 19.74s/it]

 26%|██▋       | 706/2685 [3:52:52<10:49:51, 19.70s/it]

 26%|██▋       | 707/2685 [3:53:06<9:48:27, 17.85s/it]

 26%|██▋       | 708/2685 [3:53:26<10:13:13, 18.61s/it]

 26%|██▋       | 709/2685 [3:53:48<10:41:15, 19.47s/it]

 26%|██▋       | 710/2685 [3:54:08<10:46:41, 19.65s/it]

 26%|██▋       | 711/2685 [3:54:28<10:50:01, 19.76s/it]

 27%|██▋       | 712/2685 [3:54:49<11:05:10, 20.23s/it]

 27%|██▋       | 713/2685 [3:55:09<11:04:37, 20.22s/it]

 27%|██▋       | 714/2685 [3:55:29<10:57:54, 20.03s/it]

 27%|██▋       | 715/2685 [3:55:51<11:17:47, 20.64s/it]

 27%|██▋       | 716/2685 [3:56:11<11:15:39, 20.59s/it]

 27%|██▋       | 717/2685 [3:56:31<11:03:13, 20.22s/it]

 27%|██▋       | 718/2685 [3:56:50<10:55:39, 20.00s/it]

 27%|██▋       | 719/2685 [3:57:13<11:27:06, 20.97s/it]

 27%|██▋       | 720/2685 [3:57:32<11:04:24, 20.29s/it]

 27%|██▋       | 721/2685 [3:57:54<11:21:25, 20.82s/it]

 27%|██▋       | 722/2685 [3:58:12<10:47:40, 19.80s/it]

 27%|██▋       | 723/2685 [3:58:32<10:55:52, 20.06s/it]

 27%|██▋       | 724/2685 [3:58:48<10:14:48, 18.81s/it]

 27%|██▋       | 725/2685 [3:59:08<10:21:53, 19.04s/it]

 27%|██▋       | 726/2685 [3:59:30<10:51:32, 19.96s/it]

 27%|██▋       | 727/2685 [3:59:46<10:13:15, 18.79s/it]

 27%|██▋       | 728/2685 [4:00:07<10:36:29, 19.51s/it]

 27%|██▋       | 729/2685 [4:00:26<10:32:29, 19.40s/it]

 27%|██▋       | 730/2685 [4:00:43<10:02:07, 18.48s/it]

 27%|██▋       | 731/2685 [4:00:58<9:32:17, 17.57s/it]

 27%|██▋       | 732/2685 [4:01:20<10:09:37, 18.73s/it]

 27%|██▋       | 733/2685 [4:01:40<10:23:18, 19.16s/it]

 27%|██▋       | 734/2685 [4:01:59<10:23:06, 19.16s/it]

 27%|██▋       | 735/2685 [4:02:19<10:28:14, 19.33s/it]

 27%|██▋       | 736/2685 [4:02:42<11:03:40, 20.43s/it]

 27%|██▋       | 737/2685 [4:03:03<11:13:30, 20.74s/it]

 27%|██▋       | 738/2685 [4:03:23<11:02:06, 20.40s/it]

 28%|██▊       | 739/2685 [4:03:45<11:17:48, 20.90s/it]

 28%|██▊       | 740/2685 [4:04:04<11:05:54, 20.54s/it]

 28%|██▊       | 741/2685 [4:04:24<11:00:51, 20.40s/it]

 28%|██▊       | 742/2685 [4:04:46<11:14:44, 20.84s/it]

 28%|██▊       | 743/2685 [4:05:06<11:06:10, 20.58s/it]

 28%|██▊       | 744/2685 [4:05:28<11:20:25, 21.03s/it]

 28%|██▊       | 745/2685 [4:05:47<10:52:47, 20.19s/it]

 28%|██▊       | 746/2685 [4:06:05<10:34:19, 19.63s/it]

 28%|██▊       | 747/2685 [4:06:24<10:24:30, 19.33s/it]

 28%|██▊       | 748/2685 [4:06:47<11:01:03, 20.48s/it]

 28%|██▊       | 749/2685 [4:07:07<10:55:04, 20.30s/it]

 28%|██▊       | 750/2685 [4:07:26<10:41:22, 19.89s/it]

 28%|██▊       | 751/2685 [4:07:42<10:11:06, 18.96s/it]

 28%|██▊       | 752/2685 [4:08:04<10:40:01, 19.87s/it]

 28%|██▊       | 753/2685 [4:08:24<10:38:58, 19.84s/it]

 28%|██▊       | 754/2685 [4:08:46<11:02:36, 20.59s/it]

 28%|██▊       | 755/2685 [4:09:08<11:12:44, 20.91s/it]

 28%|██▊       | 756/2685 [4:09:28<11:04:38, 20.67s/it]

 28%|██▊       | 757/2685 [4:09:48<10:51:20, 20.27s/it]

 28%|██▊       | 758/2685 [4:10:08<10:48:43, 20.20s/it]

 28%|██▊       | 759/2685 [4:10:27<10:38:51, 19.90s/it]

 28%|██▊       | 760/2685 [4:10:45<10:26:08, 19.52s/it]

 28%|██▊       | 761/2685 [4:11:06<10:33:30, 19.76s/it]

 28%|██▊       | 762/2685 [4:11:22<9:57:14, 18.63s/it]

 28%|██▊       | 763/2685 [4:11:42<10:14:15, 19.18s/it]

 28%|██▊       | 764/2685 [4:12:02<10:20:44, 19.39s/it]

 28%|██▊       | 765/2685 [4:12:20<10:06:46, 18.96s/it]

 29%|██▊       | 766/2685 [4:12:40<10:12:16, 19.14s/it]

 29%|██▊       | 767/2685 [4:12:59<10:15:15, 19.25s/it]

 29%|██▊       | 768/2685 [4:13:19<10:18:50, 19.37s/it]

 29%|██▊       | 769/2685 [4:13:41<10:46:20, 20.24s/it]
{'loss': 0.2432, 'learning_rate': 1.6748486268449647e-06, 'rewards/chosen': -1.2467120885849, 'rewards/rejected': -3.3714358806610107, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1247236728668213, 'policy_logps/rejected': -338.1724548339844, 'policy_logps/chosen': -276.6484069824219, 'referece_logps/rejected': -304.4580993652344, 'referece_logps/chosen': -264.1813049316406, 'logits/rejected': -0.9189825057983398, 'logits/chosen': -0.8908562660217285, 'epoch': 0.86}

 29%|██▊       | 770/2685 [4:14:02<10:56:35, 20.57s/it]

 29%|██▊       | 771/2685 [4:14:20<10:30:53, 19.78s/it]


 29%|██▉       | 773/2685 [4:14:57<10:01:30, 18.88s/it]
{'loss': 0.3551, 'learning_rate': 1.6712795572539331e-06, 'rewards/chosen': -0.3407484292984009, 'rewards/rejected': -2.5797338485717773, 'rewards/accuracies': 0.875, 'rewards/margins': 2.238985776901245, 'policy_logps/rejected': -371.0384521484375, 'policy_logps/chosen': -373.0945739746094, 'referece_logps/rejected': -345.24114990234375, 'referece_logps/chosen': -369.68707275390625, 'logits/rejected': -0.8302087187767029, 'logits/chosen': -0.8934369683265686, 'epoch': 0.86}

 29%|██▉       | 774/2685 [4:15:19<10:27:03, 19.69s/it]

 29%|██▉       | 775/2685 [4:15:34<9:50:31, 18.55s/it]


 29%|██▉       | 777/2685 [4:16:15<10:20:54, 19.53s/it]
{'loss': 0.266, 'learning_rate': 1.6676948547342038e-06, 'rewards/chosen': -0.521851122379303, 'rewards/rejected': -1.9128845930099487, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3910335302352905, 'policy_logps/rejected': -543.7500610351562, 'policy_logps/chosen': -355.9342041015625, 'referece_logps/rejected': -524.6212768554688, 'referece_logps/chosen': -350.7156677246094, 'logits/rejected': -1.046303391456604, 'logits/chosen': -1.1414048671722412, 'epoch': 0.87}


 29%|██▉       | 779/2685 [4:16:53<10:03:43, 19.01s/it]

 29%|██▉       | 780/2685 [4:17:15<10:34:56, 20.00s/it]

 29%|██▉       | 781/2685 [4:17:33<10:15:58, 19.41s/it]

 29%|██▉       | 782/2685 [4:17:55<10:40:29, 20.19s/it]

 29%|██▉       | 783/2685 [4:18:16<10:46:05, 20.38s/it]

 29%|██▉       | 784/2685 [4:18:33<10:15:28, 19.43s/it]

 29%|██▉       | 785/2685 [4:18:52<10:04:22, 19.09s/it]

 29%|██▉       | 786/2685 [4:19:13<10:28:02, 19.84s/it]

 29%|██▉       | 787/2685 [4:19:36<10:51:22, 20.59s/it]

 29%|██▉       | 788/2685 [4:19:54<10:31:59, 19.99s/it]

 29%|██▉       | 789/2685 [4:20:17<10:59:33, 20.87s/it]

 29%|██▉       | 790/2685 [4:20:37<10:50:39, 20.60s/it]

 29%|██▉       | 791/2685 [4:21:00<11:09:23, 21.21s/it]
{'loss': 0.3273, 'learning_rate': 1.6550264951275402e-06, 'rewards/chosen': -0.9214745759963989, 'rewards/rejected': -3.3337743282318115, 'rewards/accuracies': 1.0, 'rewards/margins': 2.412299633026123, 'policy_logps/rejected': -373.1651916503906, 'policy_logps/chosen': -336.3213806152344, 'referece_logps/rejected': -339.82745361328125, 'referece_logps/chosen': -327.1065979003906, 'logits/rejected': 0.2830757200717926, 'logits/chosen': 0.40227028727531433, 'epoch': 0.88}


 30%|██▉       | 793/2685 [4:21:36<10:21:53, 19.72s/it]

 30%|██▉       | 794/2685 [4:21:54<10:01:02, 19.07s/it]
{'loss': 0.4266, 'learning_rate': 1.6522874112781212e-06, 'rewards/chosen': -1.890815019607544, 'rewards/rejected': -2.486877202987671, 'rewards/accuracies': 0.625, 'rewards/margins': 0.5960620641708374, 'policy_logps/rejected': -412.5589904785156, 'policy_logps/chosen': -333.600341796875, 'referece_logps/rejected': -387.6902160644531, 'referece_logps/chosen': -314.69219970703125, 'logits/rejected': -0.19435957074165344, 'logits/chosen': -0.17875386774539948, 'epoch': 0.89}


 30%|██▉       | 796/2685 [4:22:32<9:56:55, 18.96s/it]
{'loss': 0.4126, 'learning_rate': 1.6504566060816174e-06, 'rewards/chosen': -0.9431487917900085, 'rewards/rejected': -2.634086847305298, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6909383535385132, 'policy_logps/rejected': -413.1268310546875, 'policy_logps/chosen': -373.9408874511719, 'referece_logps/rejected': -386.7859802246094, 'referece_logps/chosen': -364.5094299316406, 'logits/rejected': -0.7514331340789795, 'logits/chosen': -0.7367925047874451, 'epoch': 0.89}

 30%|██▉       | 797/2685 [4:22:53<10:10:30, 19.40s/it]


 30%|██▉       | 799/2685 [4:23:32<10:14:56, 19.56s/it]

 30%|██▉       | 800/2685 [4:23:52<10:17:30, 19.66s/it]

 30%|██▉       | 801/2685 [4:24:14<10:40:58, 20.41s/it]

 30%|██▉       | 802/2685 [4:24:32<10:15:38, 19.62s/it]

 30%|██▉       | 803/2685 [4:24:48<9:37:21, 18.41s/it]

 30%|██▉       | 804/2685 [4:25:08<9:59:47, 19.13s/it]

 30%|██▉       | 805/2685 [4:25:24<9:25:46, 18.06s/it]
{'loss': 0.3392, 'learning_rate': 1.6421712729157018e-06, 'rewards/chosen': -0.6397072076797485, 'rewards/rejected': -3.497750997543335, 'rewards/accuracies': 1.0, 'rewards/margins': 2.858043909072876, 'policy_logps/rejected': -307.1034240722656, 'policy_logps/chosen': -308.43829345703125, 'referece_logps/rejected': -272.12591552734375, 'referece_logps/chosen': -302.04119873046875, 'logits/rejected': -0.7182181477546692, 'logits/chosen': -0.750068187713623, 'epoch': 0.9}
[2024-03-29 02:32:40,439] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 807/2685 [4:25:53<8:38:09, 16.55s/it]
{'loss': 0.3135, 'learning_rate': 1.640319771686725e-06, 'rewards/chosen': 0.012132570147514343, 'rewards/rejected': -2.028610944747925, 'rewards/accuracies': 0.875, 'rewards/margins': 2.040743350982666, 'policy_logps/rejected': -300.0686340332031, 'policy_logps/chosen': -317.2423095703125, 'referece_logps/rejected': -279.7825622558594, 'referece_logps/chosen': -317.3636474609375, 'logits/rejected': -0.16765214502811432, 'logits/chosen': -0.10083553940057755, 'epoch': 0.9}

 30%|███       | 808/2685 [4:26:05<7:49:21, 15.00s/it]

 30%|███       | 809/2685 [4:26:25<8:36:08, 16.51s/it]


 30%|███       | 811/2685 [4:26:58<8:50:26, 16.98s/it]
{'loss': 0.2844, 'learning_rate': 1.6366055960667552e-06, 'rewards/chosen': -0.8133401870727539, 'rewards/rejected': -2.7492153644561768, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9358751773834229, 'policy_logps/rejected': -290.96270751953125, 'policy_logps/chosen': -271.2669982910156, 'referece_logps/rejected': -263.4705505371094, 'referece_logps/chosen': -263.13360595703125, 'logits/rejected': -0.24103057384490967, 'logits/chosen': -0.23812095820903778, 'epoch': 0.91}
[2024-03-29 02:34:25,005] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 30%|███       | 813/2685 [4:27:43<10:11:31, 19.60s/it]
{'loss': 0.3827, 'learning_rate': 1.63474294329996e-06, 'rewards/chosen': -0.393763929605484, 'rewards/rejected': -2.7444024085998535, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3506386280059814, 'policy_logps/rejected': -253.9454803466797, 'policy_logps/chosen': -284.67047119140625, 'referece_logps/rejected': -226.50144958496094, 'referece_logps/chosen': -280.7328186035156, 'logits/rejected': -1.1365282535552979, 'logits/chosen': -1.1964179277420044, 'epoch': 0.91}

 30%|███       | 814/2685 [4:28:03<10:22:33, 19.96s/it]

 30%|███       | 815/2685 [4:28:23<10:16:47, 19.79s/it]
[2024-03-29 02:35:26,509] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 30%|███       | 816/2685 [4:28:43<10:16:04, 19.78s/it]
{'loss': 0.3092, 'learning_rate': 1.6319420384463038e-06, 'rewards/chosen': -0.8105757236480713, 'rewards/rejected': -2.1454107761383057, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3348349332809448, 'policy_logps/rejected': -357.36395263671875, 'policy_logps/chosen': -288.354248046875, 'referece_logps/rejected': -335.90985107421875, 'referece_logps/chosen': -280.24847412109375, 'logits/rejected': -0.9350743293762207, 'logits/chosen': -0.9507355093955994, 'epoch': 0.91}


 30%|███       | 818/2685 [4:29:22<10:07:41, 19.53s/it]

 31%|███       | 819/2685 [4:29:41<10:02:01, 19.36s/it]

 31%|███       | 820/2685 [4:30:02<10:23:37, 20.06s/it]
[2024-03-29 02:37:05,930] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 821/2685 [4:30:22<10:24:02, 20.09s/it]

 31%|███       | 822/2685 [4:30:44<10:40:46, 20.64s/it]

 31%|███       | 823/2685 [4:31:02<10:15:55, 19.85s/it]

 31%|███       | 824/2685 [4:31:20<9:52:31, 19.10s/it]

 31%|███       | 825/2685 [4:31:40<10:07:39, 19.60s/it]

 31%|███       | 826/2685 [4:32:00<10:11:04, 19.72s/it]

 31%|███       | 827/2685 [4:32:22<10:26:46, 20.24s/it]
[2024-03-29 02:39:25,583] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███       | 828/2685 [4:32:41<10:15:14, 19.88s/it]

 31%|███       | 829/2685 [4:33:00<10:04:55, 19.56s/it]

 31%|███       | 830/2685 [4:33:20<10:15:06, 19.90s/it]

 31%|███       | 831/2685 [4:33:38<9:57:27, 19.34s/it]
{'loss': 0.3658, 'learning_rate': 1.6178140784513729e-06, 'rewards/chosen': -1.2482761144638062, 'rewards/rejected': -2.782818078994751, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5345418453216553, 'policy_logps/rejected': -399.2640380859375, 'policy_logps/chosen': -290.1294860839844, 'referece_logps/rejected': -371.43585205078125, 'referece_logps/chosen': -277.6467590332031, 'logits/rejected': -0.588445246219635, 'logits/chosen': -0.6555071473121643, 'epoch': 0.93}


 31%|███       | 833/2685 [4:34:18<9:58:47, 19.40s/it]

 31%|███       | 834/2685 [4:34:41<10:32:33, 20.50s/it]

 31%|███       | 835/2685 [4:35:01<10:28:06, 20.37s/it]

 31%|███       | 836/2685 [4:35:25<11:00:45, 21.44s/it]

 31%|███       | 837/2685 [4:35:49<11:22:07, 22.15s/it]

 31%|███       | 838/2685 [4:36:07<10:41:51, 20.85s/it]

 31%|███       | 839/2685 [4:36:27<10:36:26, 20.69s/it]

 31%|███▏      | 840/2685 [4:36:47<10:30:10, 20.49s/it]

 31%|███▏      | 841/2685 [4:37:08<10:36:57, 20.73s/it]
[2024-03-29 02:44:11,850] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 31%|███▏      | 842/2685 [4:37:25<9:56:51, 19.43s/it]
[2024-03-29 02:44:28,261] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2658, 'learning_rate': 1.6073247227028613e-06, 'rewards/chosen': -1.002387523651123, 'rewards/rejected': -3.798487901687622, 'rewards/accuracies': 0.75, 'rewards/margins': 2.796100616455078, 'policy_logps/rejected': -431.1691589355469, 'policy_logps/chosen': -374.84423828125, 'referece_logps/rejected': -393.18426513671875, 'referece_logps/chosen': -364.8204040527344, 'logits/rejected': -0.3792772889137268, 'logits/chosen': -0.303727924823761, 'epoch': 0.94}


 31%|███▏      | 844/2685 [4:37:59<9:13:23, 18.04s/it]
{'loss': 0.4203, 'learning_rate': 1.605406021206367e-06, 'rewards/chosen': -1.2997231483459473, 'rewards/rejected': -2.4761576652526855, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1764345169067383, 'policy_logps/rejected': -412.17901611328125, 'policy_logps/chosen': -414.5621643066406, 'referece_logps/rejected': -387.41741943359375, 'referece_logps/chosen': -401.5649108886719, 'logits/rejected': -1.041502833366394, 'logits/chosen': -0.9680987596511841, 'epoch': 0.94}


 32%|███▏      | 846/2685 [4:38:41<9:59:14, 19.55s/it]

 32%|███▏      | 847/2685 [4:38:59<9:45:10, 19.10s/it]

 32%|███▏      | 848/2685 [4:39:19<9:52:11, 19.34s/it]

 32%|███▏      | 849/2685 [4:39:32<8:58:47, 17.61s/it]

 32%|███▏      | 850/2685 [4:39:52<9:21:28, 18.36s/it]

 32%|███▏      | 851/2685 [4:40:16<10:05:45, 19.82s/it]
[2024-03-29 02:47:19,207] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 852/2685 [4:40:31<9:20:53, 18.36s/it]
[2024-03-29 02:47:34,165] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 853/2685 [4:40:51<9:42:36, 19.08s/it]

 32%|███▏      | 854/2685 [4:41:09<9:30:02, 18.68s/it]

 32%|███▏      | 855/2685 [4:41:31<10:00:15, 19.68s/it]
[2024-03-29 02:48:34,689] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 32%|███▏      | 856/2685 [4:41:50<9:49:20, 19.33s/it]

 32%|███▏      | 857/2685 [4:42:09<9:53:01, 19.46s/it]

 32%|███▏      | 858/2685 [4:42:31<10:16:53, 20.26s/it]

 32%|███▏      | 859/2685 [4:42:51<10:10:48, 20.07s/it]

 32%|███▏      | 860/2685 [4:43:08<9:39:13, 19.04s/it]

 32%|███▏      | 861/2685 [4:43:27<9:42:00, 19.15s/it]
{'loss': 0.2645, 'learning_rate': 1.58895588099049e-06, 'rewards/chosen': -1.353762149810791, 'rewards/rejected': -2.514890670776367, 'rewards/accuracies': 0.875, 'rewards/margins': 1.1611285209655762, 'policy_logps/rejected': -373.959228515625, 'policy_logps/chosen': -401.75213623046875, 'referece_logps/rejected': -348.81036376953125, 'referece_logps/chosen': -388.21453857421875, 'logits/rejected': -0.24952766299247742, 'logits/chosen': -0.29789674282073975, 'epoch': 0.96}


 32%|███▏      | 863/2685 [4:43:59<8:52:52, 17.55s/it]

 32%|███▏      | 864/2685 [4:44:23<9:44:33, 19.26s/it]
{'loss': 0.3782, 'learning_rate': 1.5860270000757695e-06, 'rewards/chosen': -0.7252435684204102, 'rewards/rejected': -3.4049103260040283, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6796669960021973, 'policy_logps/rejected': -427.57794189453125, 'policy_logps/chosen': -474.4163513183594, 'referece_logps/rejected': -393.5288391113281, 'referece_logps/chosen': -467.1639404296875, 'logits/rejected': -0.30048659443855286, 'logits/chosen': -0.2213669717311859, 'epoch': 0.97}

 32%|███▏      | 865/2685 [4:44:42<9:46:16, 19.33s/it]


 32%|███▏      | 867/2685 [4:45:25<10:19:19, 20.44s/it]

 32%|███▏      | 868/2685 [4:45:45<10:16:28, 20.36s/it]

 32%|███▏      | 869/2685 [4:46:04<9:59:53, 19.82s/it]

 32%|███▏      | 870/2685 [4:46:23<9:56:59, 19.74s/it]

 32%|███▏      | 871/2685 [4:46:43<10:00:34, 19.86s/it]

 32%|███▏      | 872/2685 [4:47:06<10:26:37, 20.74s/it]
[2024-03-29 02:54:09,682] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 873/2685 [4:47:28<10:37:49, 21.12s/it]
[2024-03-29 02:54:31,694] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 874/2685 [4:47:51<10:56:07, 21.74s/it]
[2024-03-29 02:54:54,874] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 33%|███▎      | 875/2685 [4:48:12<10:45:32, 21.40s/it]

 33%|███▎      | 876/2685 [4:48:31<10:29:06, 20.87s/it]
{'loss': 0.3237, 'learning_rate': 1.5742350940868964e-06, 'rewards/chosen': -1.3725754022598267, 'rewards/rejected': -3.0294880867004395, 'rewards/accuracies': 0.875, 'rewards/margins': 1.656912922859192, 'policy_logps/rejected': -447.7150573730469, 'policy_logps/chosen': -386.5057373046875, 'referece_logps/rejected': -417.420166015625, 'referece_logps/chosen': -372.7799377441406, 'logits/rejected': -0.31646227836608887, 'logits/chosen': -0.23286853730678558, 'epoch': 0.98}


 33%|███▎      | 878/2685 [4:49:10<9:54:21, 19.74s/it]
{'loss': 0.3907, 'learning_rate': 1.5722580081707672e-06, 'rewards/chosen': -0.4619865417480469, 'rewards/rejected': -2.1894655227661133, 'rewards/accuracies': 0.625, 'rewards/margins': 1.727479338645935, 'policy_logps/rejected': -330.063720703125, 'policy_logps/chosen': -325.84112548828125, 'referece_logps/rejected': -308.1690979003906, 'referece_logps/chosen': -321.22125244140625, 'logits/rejected': -0.38111019134521484, 'logits/chosen': -0.19057826697826385, 'epoch': 0.98}

 33%|███▎      | 879/2685 [4:49:29<9:53:17, 19.71s/it]


 33%|███▎      | 881/2685 [4:50:06<9:40:20, 19.30s/it]
{'loss': 0.3338, 'learning_rate': 1.5692861359103302e-06, 'rewards/chosen': -0.9106993079185486, 'rewards/rejected': -2.3653435707092285, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4546444416046143, 'policy_logps/rejected': -227.16448974609375, 'policy_logps/chosen': -251.88294982910156, 'referece_logps/rejected': -203.5110626220703, 'referece_logps/chosen': -242.77593994140625, 'logits/rejected': -0.579240620136261, 'logits/chosen': -0.6229994893074036, 'epoch': 0.98}


 33%|███▎      | 883/2685 [4:50:45<9:48:27, 19.59s/it]
{'loss': 0.3025, 'learning_rate': 1.5673007423010401e-06, 'rewards/chosen': -0.51276034116745, 'rewards/rejected': -2.4182991981506348, 'rewards/accuracies': 0.75, 'rewards/margins': 1.905538558959961, 'policy_logps/rejected': -260.6033935546875, 'policy_logps/chosen': -311.8486022949219, 'referece_logps/rejected': -236.42041015625, 'referece_logps/chosen': -306.72100830078125, 'logits/rejected': -0.5730962753295898, 'logits/chosen': -0.43055611848831177, 'epoch': 0.99}

 33%|███▎      | 884/2685 [4:51:07<10:07:15, 20.23s/it]

 33%|███▎      | 885/2685 [4:51:25<9:49:20, 19.64s/it]


 33%|███▎      | 887/2685 [4:52:06<9:58:24, 19.97s/it]

 33%|███▎      | 888/2685 [4:52:26<9:58:26, 19.98s/it]

 33%|███▎      | 889/2685 [4:52:46<9:58:13, 19.99s/it]
{'loss': 0.3214, 'learning_rate': 1.5613247906113241e-06, 'rewards/chosen': -1.1778624057769775, 'rewards/rejected': -5.338898181915283, 'rewards/accuracies': 0.875, 'rewards/margins': 4.161036014556885, 'policy_logps/rejected': -575.713623046875, 'policy_logps/chosen': -431.3072204589844, 'referece_logps/rejected': -522.32470703125, 'referece_logps/chosen': -419.5285949707031, 'logits/rejected': 0.2967221736907959, 'logits/chosen': 0.3576669692993164, 'epoch': 0.99}


 33%|███▎      | 891/2685 [4:53:22<9:39:26, 19.38s/it]
{'loss': 0.3452, 'learning_rate': 1.5593262550857232e-06, 'rewards/chosen': -1.5152816772460938, 'rewards/rejected': -3.069098472595215, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5538166761398315, 'policy_logps/rejected': -330.770751953125, 'policy_logps/chosen': -382.40423583984375, 'referece_logps/rejected': -300.07977294921875, 'referece_logps/chosen': -367.2514343261719, 'logits/rejected': -0.1269519329071045, 'logits/chosen': -0.2409607619047165, 'epoch': 1.0}


 33%|███▎      | 893/2685 [4:53:56<9:02:14, 18.16s/it]

 33%|███▎      | 894/2685 [4:54:18<9:40:50, 19.46s/it]
{'loss': 0.362, 'learning_rate': 1.5563223496179773e-06, 'rewards/chosen': -2.0371572971343994, 'rewards/rejected': -4.090502738952637, 'rewards/accuracies': 0.875, 'rewards/margins': 2.0533456802368164, 'policy_logps/rejected': -504.078125, 'policy_logps/chosen': -488.6186218261719, 'referece_logps/rejected': -463.173095703125, 'referece_logps/chosen': -468.24700927734375, 'logits/rejected': 0.1341070681810379, 'logits/chosen': 0.14613917469978333, 'epoch': 1.0}

 33%|███▎      | 895/2685 [4:54:41<10:07:23, 20.36s/it]

 33%|███▎      | 896/2685 [4:55:01<10:05:26, 20.31s/it]

 33%|███▎      | 897/2685 [4:55:21<10:00:31, 20.15s/it]

 33%|███▎      | 898/2685 [4:55:42<10:04:57, 20.31s/it]

 33%|███▎      | 899/2685 [4:56:01<9:59:03, 20.13s/it]

 34%|███▎      | 900/2685 [4:56:22<10:00:05, 20.17s/it]

 34%|███▎      | 901/2685 [4:56:43<10:14:57, 20.68s/it]


 34%|███▎      | 903/2685 [4:57:23<9:57:33, 20.12s/it]
{'loss': 0.3956, 'learning_rate': 1.5472670653106744e-06, 'rewards/chosen': -0.9151597023010254, 'rewards/rejected': -2.431260824203491, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5161011219024658, 'policy_logps/rejected': -391.2201232910156, 'policy_logps/chosen': -320.23974609375, 'referece_logps/rejected': -366.9075012207031, 'referece_logps/chosen': -311.0881042480469, 'logits/rejected': -0.3875459134578705, 'logits/chosen': -0.39852166175842285, 'epoch': 1.01}


 34%|███▎      | 905/2685 [4:58:00<9:44:06, 19.69s/it]

 34%|███▎      | 906/2685 [4:58:23<10:05:12, 20.41s/it]
{'loss': 0.2773, 'learning_rate': 1.5442342463748204e-06, 'rewards/chosen': -1.8991672992706299, 'rewards/rejected': -3.762538194656372, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8633708953857422, 'policy_logps/rejected': -428.1663513183594, 'policy_logps/chosen': -361.91180419921875, 'referece_logps/rejected': -390.54095458984375, 'referece_logps/chosen': -342.92010498046875, 'logits/rejected': 0.11390452086925507, 'logits/chosen': 0.24753518402576447, 'epoch': 1.01}


 34%|███▍      | 908/2685 [4:59:00<9:38:33, 19.54s/it]
{'loss': 0.4394, 'learning_rate': 1.5422084039167475e-06, 'rewards/chosen': -0.27063536643981934, 'rewards/rejected': -2.526597499847412, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2559621334075928, 'policy_logps/rejected': -338.8980407714844, 'policy_logps/chosen': -329.5638122558594, 'referece_logps/rejected': -313.6320495605469, 'referece_logps/chosen': -326.8574523925781, 'logits/rejected': -0.5901615023612976, 'logits/chosen': -0.6311503648757935, 'epoch': 1.01}


 34%|███▍      | 910/2685 [4:59:30<8:35:35, 17.43s/it]
{'loss': 0.2353, 'learning_rate': 1.5401794046823495e-06, 'rewards/chosen': -1.0390734672546387, 'rewards/rejected': -4.427274703979492, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3882012367248535, 'policy_logps/rejected': -500.34722900390625, 'policy_logps/chosen': -335.982666015625, 'referece_logps/rejected': -456.0744934082031, 'referece_logps/chosen': -325.5919494628906, 'logits/rejected': -0.35254043340682983, 'logits/chosen': -0.40385034680366516, 'epoch': 1.02}

 34%|███▍      | 911/2685 [4:59:52<9:14:16, 18.75s/it]

 34%|███▍      | 912/2685 [5:00:14<9:39:41, 19.62s/it]

 34%|███▍      | 913/2685 [5:00:30<9:07:05, 18.52s/it]


 34%|███▍      | 915/2685 [5:01:11<9:39:53, 19.66s/it]
{'loss': 0.3019, 'learning_rate': 1.5350931732676538e-06, 'rewards/chosen': -1.7819691896438599, 'rewards/rejected': -4.748125076293945, 'rewards/accuracies': 0.875, 'rewards/margins': 2.966156244277954, 'policy_logps/rejected': -345.3477783203125, 'policy_logps/chosen': -302.5782165527344, 'referece_logps/rejected': -297.86651611328125, 'referece_logps/chosen': -284.758544921875, 'logits/rejected': -0.6067332625389099, 'logits/chosen': -0.671686053276062, 'epoch': 1.02}

 34%|███▍      | 916/2685 [5:01:29<9:30:05, 19.34s/it]

 34%|███▍      | 917/2685 [5:01:47<9:17:46, 18.93s/it]


 34%|███▍      | 919/2685 [5:02:22<8:51:09, 18.05s/it]

 34%|███▍      | 920/2685 [5:02:36<8:14:34, 16.81s/it]
{'loss': 0.4321, 'learning_rate': 1.5299874709591757e-06, 'rewards/chosen': -0.6847319006919861, 'rewards/rejected': -3.157377004623413, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4726455211639404, 'policy_logps/rejected': -271.6239318847656, 'policy_logps/chosen': -269.07647705078125, 'referece_logps/rejected': -240.05015563964844, 'referece_logps/chosen': -262.2291259765625, 'logits/rejected': -0.9452018737792969, 'logits/chosen': -0.9285306334495544, 'epoch': 1.03}


 34%|███▍      | 922/2685 [5:03:14<8:51:07, 18.08s/it]

 34%|███▍      | 923/2685 [5:03:31<8:37:54, 17.64s/it]
{'loss': 0.3197, 'learning_rate': 1.5269147808025974e-06, 'rewards/chosen': -0.2554091215133667, 'rewards/rejected': -2.1352808475494385, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8798716068267822, 'policy_logps/rejected': -296.0603942871094, 'policy_logps/chosen': -179.05789184570312, 'referece_logps/rejected': -274.7075500488281, 'referece_logps/chosen': -176.50379943847656, 'logits/rejected': -0.7429893612861633, 'logits/chosen': -0.8280834555625916, 'epoch': 1.03}

 34%|███▍      | 924/2685 [5:03:50<8:50:48, 18.09s/it]


 34%|███▍      | 926/2685 [5:04:30<9:22:27, 19.19s/it]
{'loss': 0.3296, 'learning_rate': 1.5238351882447642e-06, 'rewards/chosen': -0.47802412509918213, 'rewards/rejected': -2.1346261501312256, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6566020250320435, 'policy_logps/rejected': -375.1445007324219, 'policy_logps/chosen': -350.4294128417969, 'referece_logps/rejected': -353.7982482910156, 'referece_logps/chosen': -345.649169921875, 'logits/rejected': -0.5098211765289307, 'logits/chosen': -0.46201854944229126, 'epoch': 1.03}

 35%|███▍      | 927/2685 [5:04:48<9:06:38, 18.66s/it]


 35%|███▍      | 929/2685 [5:05:25<9:00:20, 18.46s/it]
{'loss': 0.4245, 'learning_rate': 1.5207487336272734e-06, 'rewards/chosen': -2.1316020488739014, 'rewards/rejected': -4.081580638885498, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9499789476394653, 'policy_logps/rejected': -375.1584167480469, 'policy_logps/chosen': -359.79217529296875, 'referece_logps/rejected': -334.3426513671875, 'referece_logps/chosen': -338.4761657714844, 'logits/rejected': 0.5413187146186829, 'logits/chosen': 0.5224179029464722, 'epoch': 1.04}

 35%|███▍      | 930/2685 [5:05:46<9:22:51, 19.24s/it]

 35%|███▍      | 931/2685 [5:06:03<9:07:20, 18.72s/it]

 35%|███▍      | 932/2685 [5:06:22<9:03:01, 18.59s/it]


 35%|███▍      | 934/2685 [5:07:03<9:33:09, 19.64s/it]
{'loss': 0.4314, 'learning_rate': 1.5155895034310441e-06, 'rewards/chosen': -0.3911053240299225, 'rewards/rejected': -3.4628567695617676, 'rewards/accuracies': 0.875, 'rewards/margins': 3.071751356124878, 'policy_logps/rejected': -476.8840026855469, 'policy_logps/chosen': -507.58892822265625, 'referece_logps/rejected': -442.2554626464844, 'referece_logps/chosen': -503.6778564453125, 'logits/rejected': 0.6002038717269897, 'logits/chosen': 0.6255213618278503, 'epoch': 1.04}


 35%|███▍      | 936/2685 [5:07:47<10:05:37, 20.78s/it]
{'loss': 0.3489, 'learning_rate': 1.513520547681304e-06, 'rewards/chosen': -0.5583078265190125, 'rewards/rejected': -2.137338876724243, 'rewards/accuracies': 1.0, 'rewards/margins': 1.579031229019165, 'policy_logps/rejected': -351.57586669921875, 'policy_logps/chosen': -376.1239013671875, 'referece_logps/rejected': -330.2024841308594, 'referece_logps/chosen': -370.5408020019531, 'logits/rejected': -0.6401388049125671, 'logits/chosen': -0.5494899749755859, 'epoch': 1.05}

 35%|███▍      | 937/2685 [5:08:06<9:51:39, 20.31s/it]


 35%|███▍      | 939/2685 [5:08:47<9:54:43, 20.44s/it]
{'loss': 0.2972, 'learning_rate': 1.5104115120377783e-06, 'rewards/chosen': -0.9215207099914551, 'rewards/rejected': -2.6601388454437256, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7386178970336914, 'policy_logps/rejected': -342.3406677246094, 'policy_logps/chosen': -328.5443115234375, 'referece_logps/rejected': -315.73931884765625, 'referece_logps/chosen': -319.3291320800781, 'logits/rejected': -0.5075262784957886, 'logits/chosen': -0.4742148518562317, 'epoch': 1.05}


 35%|███▌      | 941/2685 [5:09:29<10:10:28, 21.00s/it]
{'loss': 0.3748, 'learning_rate': 1.5083351045277067e-06, 'rewards/chosen': -0.9172247648239136, 'rewards/rejected': -3.075361728668213, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1581368446350098, 'policy_logps/rejected': -325.00799560546875, 'policy_logps/chosen': -253.31033325195312, 'referece_logps/rejected': -294.25439453125, 'referece_logps/chosen': -244.13809204101562, 'logits/rejected': -0.5297059416770935, 'logits/chosen': -0.6364261507987976, 'epoch': 1.05}

 35%|███▌      | 942/2685 [5:09:48<9:54:44, 20.47s/it]

 35%|███▌      | 943/2685 [5:10:10<10:06:13, 20.88s/it]

 35%|███▌      | 944/2685 [5:10:32<10:11:02, 21.06s/it]


 35%|███▌      | 946/2685 [5:11:09<9:33:58, 19.80s/it]

 35%|███▌      | 947/2685 [5:11:29<9:33:07, 19.79s/it]
{'loss': 0.2702, 'learning_rate': 1.5020881730583116e-06, 'rewards/chosen': -0.5319307446479797, 'rewards/rejected': -3.0899531841278076, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5580227375030518, 'policy_logps/rejected': -370.59521484375, 'policy_logps/chosen': -271.3243408203125, 'referece_logps/rejected': -339.6957092285156, 'referece_logps/chosen': -266.0050354003906, 'logits/rejected': -1.3037772178649902, 'logits/chosen': -1.3514275550842285, 'epoch': 1.06}

 35%|███▌      | 948/2685 [5:11:46<9:09:57, 19.00s/it]


 35%|███▌      | 950/2685 [5:12:25<9:21:35, 19.42s/it]

 35%|███▌      | 951/2685 [5:12:45<9:27:27, 19.64s/it]

 35%|███▌      | 952/2685 [5:13:05<9:29:10, 19.71s/it]
{'loss': 0.3604, 'learning_rate': 1.496862286023957e-06, 'rewards/chosen': -0.9652723670005798, 'rewards/rejected': -3.3924171924591064, 'rewards/accuracies': 0.875, 'rewards/margins': 2.427145004272461, 'policy_logps/rejected': -520.9234619140625, 'policy_logps/chosen': -464.0554504394531, 'referece_logps/rejected': -486.9993591308594, 'referece_logps/chosen': -454.4027404785156, 'logits/rejected': 0.8265068531036377, 'logits/chosen': 0.8634516596794128, 'epoch': 1.06}

 35%|███▌      | 953/2685 [5:13:26<9:39:22, 20.07s/it]

 36%|███▌      | 954/2685 [5:13:46<9:40:49, 20.13s/it]


 36%|███▌      | 956/2685 [5:14:30<10:01:02, 20.86s/it]
{'loss': 0.2822, 'learning_rate': 1.4926685498240026e-06, 'rewards/chosen': 0.17147044837474823, 'rewards/rejected': -4.133845806121826, 'rewards/accuracies': 1.0, 'rewards/margins': 4.30531644821167, 'policy_logps/rejected': -463.23919677734375, 'policy_logps/chosen': -436.85430908203125, 'referece_logps/rejected': -421.90069580078125, 'referece_logps/chosen': -438.56903076171875, 'logits/rejected': 0.015584081411361694, 'logits/chosen': -0.13688716292381287, 'epoch': 1.07}

 36%|███▌      | 957/2685 [5:14:53<10:17:07, 21.43s/it]

 36%|███▌      | 958/2685 [5:15:12<10:02:24, 20.93s/it]

 36%|███▌      | 959/2685 [5:15:30<9:33:56, 19.95s/it]

 36%|███▌      | 960/2685 [5:15:50<9:38:15, 20.11s/it]

 36%|███▌      | 961/2685 [5:16:10<9:36:01, 20.05s/it]

 36%|███▌      | 962/2685 [5:16:30<9:36:14, 20.07s/it]

 36%|███▌      | 963/2685 [5:16:48<9:17:51, 19.44s/it]


 36%|███▌      | 965/2685 [5:17:26<9:15:09, 19.37s/it]
{'loss': 0.3889, 'learning_rate': 1.4831908430096113e-06, 'rewards/chosen': -1.3799114227294922, 'rewards/rejected': -2.740257501602173, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3603463172912598, 'policy_logps/rejected': -335.56890869140625, 'policy_logps/chosen': -297.37469482421875, 'referece_logps/rejected': -308.1663513183594, 'referece_logps/chosen': -283.5755920410156, 'logits/rejected': -1.036978840827942, 'logits/chosen': -0.8718481659889221, 'epoch': 1.08}

 36%|███▌      | 966/2685 [5:17:47<9:28:45, 19.85s/it]

 36%|███▌      | 967/2685 [5:18:07<9:31:34, 19.96s/it]

 36%|███▌      | 968/2685 [5:18:26<9:23:35, 19.69s/it]

 36%|███▌      | 969/2685 [5:18:45<9:13:59, 19.37s/it]

 36%|███▌      | 970/2685 [5:19:02<8:57:24, 18.80s/it]


 36%|███▌      | 972/2685 [5:19:44<9:27:13, 19.87s/it]
{'loss': 0.2894, 'learning_rate': 1.4757798517640936e-06, 'rewards/chosen': -0.36332759261131287, 'rewards/rejected': -2.7135448455810547, 'rewards/accuracies': 0.75, 'rewards/margins': 2.35021710395813, 'policy_logps/rejected': -360.94024658203125, 'policy_logps/chosen': -315.1593017578125, 'referece_logps/rejected': -333.8048095703125, 'referece_logps/chosen': -311.5260314941406, 'logits/rejected': -0.3142983615398407, 'logits/chosen': -0.30271902680397034, 'epoch': 1.09}

 36%|███▌      | 973/2685 [5:20:02<9:15:50, 19.48s/it]

 36%|███▋      | 974/2685 [5:20:21<9:06:30, 19.16s/it]

 36%|███▋      | 975/2685 [5:20:41<9:17:40, 19.57s/it]

 36%|███▋      | 976/2685 [5:20:59<9:03:36, 19.09s/it]

 36%|███▋      | 977/2685 [5:21:20<9:13:44, 19.45s/it]

 36%|███▋      | 978/2685 [5:21:38<9:07:49, 19.26s/it]


 36%|███▋      | 980/2685 [5:22:16<9:04:07, 19.15s/it]
{'loss': 0.3091, 'learning_rate': 1.467268628273062e-06, 'rewards/chosen': -1.5904065370559692, 'rewards/rejected': -3.0770065784454346, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4865999221801758, 'policy_logps/rejected': -466.5701599121094, 'policy_logps/chosen': -445.42401123046875, 'referece_logps/rejected': -435.8001403808594, 'referece_logps/chosen': -429.51995849609375, 'logits/rejected': -0.45163851976394653, 'logits/chosen': -0.3325052857398987, 'epoch': 1.09}

 37%|███▋      | 981/2685 [5:22:37<9:15:02, 19.54s/it]

 37%|███▋      | 982/2685 [5:22:57<9:19:32, 19.71s/it]

 37%|███▋      | 983/2685 [5:23:14<8:58:43, 18.99s/it]

 37%|███▋      | 984/2685 [5:23:37<9:34:17, 20.26s/it]

 37%|███▋      | 985/2685 [5:23:57<9:31:02, 20.15s/it]

 37%|███▋      | 986/2685 [5:24:17<9:26:41, 20.01s/it]

 37%|███▋      | 987/2685 [5:24:38<9:34:42, 20.31s/it]

 37%|███▋      | 988/2685 [5:24:51<8:32:14, 18.11s/it]

 37%|███▋      | 989/2685 [5:25:11<8:49:15, 18.72s/it]


 37%|███▋      | 991/2685 [5:25:50<9:03:09, 19.24s/it]
{'loss': 0.3237, 'learning_rate': 1.4554947866906338e-06, 'rewards/chosen': -0.9727178812026978, 'rewards/rejected': -3.5744714736938477, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6017537117004395, 'policy_logps/rejected': -415.7784423828125, 'policy_logps/chosen': -360.960205078125, 'referece_logps/rejected': -380.03369140625, 'referece_logps/chosen': -351.2330322265625, 'logits/rejected': -0.012663780711591244, 'logits/chosen': 0.06306512653827667, 'epoch': 1.11}

 37%|███▋      | 992/2685 [5:26:10<9:06:31, 19.37s/it]

 37%|███▋      | 993/2685 [5:26:30<9:08:25, 19.45s/it]

 37%|███▋      | 994/2685 [5:26:50<9:12:23, 19.60s/it]

 37%|███▋      | 995/2685 [5:27:09<9:11:39, 19.59s/it]


 37%|███▋      | 997/2685 [5:27:46<8:55:17, 19.03s/it]
{'loss': 0.3683, 'learning_rate': 1.4490387467392389e-06, 'rewards/chosen': -1.3693087100982666, 'rewards/rejected': -4.273256301879883, 'rewards/accuracies': 0.875, 'rewards/margins': 2.903947353363037, 'policy_logps/rejected': -581.9183349609375, 'policy_logps/chosen': -453.8919677734375, 'referece_logps/rejected': -539.185791015625, 'referece_logps/chosen': -440.19891357421875, 'logits/rejected': 0.6195407509803772, 'logits/chosen': 0.6196149587631226, 'epoch': 1.11}

 37%|███▋      | 998/2685 [5:28:04<8:41:48, 18.56s/it]

 37%|███▋      | 999/2685 [5:28:20<8:18:40, 17.75s/it]

 37%|███▋      | 1000/2685 [5:28:42<8:54:20, 19.03s/it]

 37%|███▋      | 1001/2685 [5:29:14<10:42:03, 22.88s/it]

 37%|███▋      | 1002/2685 [5:29:33<10:14:46, 21.92s/it]

 37%|███▋      | 1003/2685 [5:29:53<9:55:28, 21.24s/it]


 37%|███▋      | 1005/2685 [5:30:27<8:48:40, 18.88s/it]
{'loss': 0.2801, 'learning_rate': 1.4403941515576343e-06, 'rewards/chosen': -0.5417671799659729, 'rewards/rejected': -3.093989372253418, 'rewards/accuracies': 1.0, 'rewards/margins': 2.55222225189209, 'policy_logps/rejected': -406.61846923828125, 'policy_logps/chosen': -417.6357421875, 'referece_logps/rejected': -375.6785583496094, 'referece_logps/chosen': -412.2180480957031, 'logits/rejected': 0.18933424353599548, 'logits/chosen': 0.2094462513923645, 'epoch': 1.12}

 37%|███▋      | 1006/2685 [5:30:46<8:54:43, 19.11s/it]

 38%|███▊      | 1007/2685 [5:31:06<8:55:58, 19.16s/it]

 38%|███▊      | 1008/2685 [5:31:26<9:08:53, 19.64s/it]

 38%|███▊      | 1009/2685 [5:31:48<9:27:44, 20.33s/it]

 38%|███▊      | 1010/2685 [5:32:08<9:25:01, 20.24s/it]

 38%|███▊      | 1011/2685 [5:32:31<9:41:22, 20.84s/it]

 38%|███▊      | 1012/2685 [5:32:50<9:31:21, 20.49s/it]

 38%|███▊      | 1013/2685 [5:33:10<9:25:41, 20.30s/it]


 38%|███▊      | 1015/2685 [5:33:55<9:56:46, 21.44s/it]
{'loss': 0.2946, 'learning_rate': 1.429530812587888e-06, 'rewards/chosen': -1.6932241916656494, 'rewards/rejected': -3.496617555618286, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8033932447433472, 'policy_logps/rejected': -338.3104553222656, 'policy_logps/chosen': -283.7242736816406, 'referece_logps/rejected': -303.34429931640625, 'referece_logps/chosen': -266.7920227050781, 'logits/rejected': 0.21467477083206177, 'logits/chosen': 0.1968856155872345, 'epoch': 1.13}

 38%|███▊      | 1016/2685 [5:34:16<9:56:22, 21.44s/it]

 38%|███▊      | 1017/2685 [5:34:37<9:53:54, 21.36s/it]

 38%|███▊      | 1018/2685 [5:34:55<9:25:40, 20.36s/it]

 38%|███▊      | 1019/2685 [5:35:14<9:08:28, 19.75s/it]

 38%|███▊      | 1020/2685 [5:35:33<9:02:01, 19.53s/it]


 38%|███▊      | 1022/2685 [5:36:11<8:58:01, 19.41s/it]
{'loss': 0.4075, 'learning_rate': 1.4218891825009618e-06, 'rewards/chosen': -0.9353126883506775, 'rewards/rejected': -1.6868607997894287, 'rewards/accuracies': 0.875, 'rewards/margins': 0.7515482306480408, 'policy_logps/rejected': -449.4095153808594, 'policy_logps/chosen': -415.94140625, 'referece_logps/rejected': -432.5409240722656, 'referece_logps/chosen': -406.5882873535156, 'logits/rejected': -0.20624661445617676, 'logits/chosen': -0.26384836435317993, 'epoch': 1.14}

 38%|███▊      | 1023/2685 [5:36:31<8:59:19, 19.47s/it]

 38%|███▊      | 1024/2685 [5:36:50<8:57:44, 19.42s/it]

 38%|███▊      | 1025/2685 [5:37:10<8:59:49, 19.51s/it]

 38%|███▊      | 1026/2685 [5:37:33<9:27:47, 20.53s/it]

 38%|███▊      | 1027/2685 [5:37:52<9:19:46, 20.26s/it]

 38%|███▊      | 1028/2685 [5:38:10<8:55:04, 19.38s/it]

 38%|███▊      | 1029/2685 [5:38:32<9:16:15, 20.15s/it]

 38%|███▊      | 1030/2685 [5:38:50<9:04:23, 19.74s/it]

 38%|███▊      | 1031/2685 [5:39:10<9:07:46, 19.87s/it]

 38%|███▊      | 1032/2685 [5:39:34<9:36:58, 20.94s/it]

 38%|███▊      | 1033/2685 [5:39:52<9:12:22, 20.06s/it]

 39%|███▊      | 1034/2685 [5:40:14<9:25:05, 20.54s/it]

 39%|███▊      | 1035/2685 [5:40:33<9:18:59, 20.33s/it]

 39%|███▊      | 1036/2685 [5:40:52<9:07:57, 19.94s/it]

 39%|███▊      | 1037/2685 [5:41:12<9:01:19, 19.71s/it]

 39%|███▊      | 1038/2685 [5:41:30<8:52:15, 19.39s/it]

 39%|███▊      | 1039/2685 [5:41:52<9:10:35, 20.07s/it]

 39%|███▊      | 1040/2685 [5:42:12<9:10:09, 20.07s/it]

 39%|███▉      | 1041/2685 [5:42:32<9:08:21, 20.01s/it]

 39%|███▉      | 1042/2685 [5:42:52<9:07:29, 19.99s/it]

 39%|███▉      | 1043/2685 [5:43:08<8:39:45, 18.99s/it]

 39%|███▉      | 1044/2685 [5:43:28<8:46:01, 19.23s/it]

 39%|███▉      | 1045/2685 [5:43:51<9:15:12, 20.31s/it]


 39%|███▉      | 1047/2685 [5:44:29<8:59:41, 19.77s/it]

 39%|███▉      | 1048/2685 [5:44:51<9:17:27, 20.43s/it]

 39%|███▉      | 1049/2685 [5:45:12<9:20:23, 20.55s/it]

 39%|███▉      | 1050/2685 [5:45:30<9:00:12, 19.82s/it]

 39%|███▉      | 1051/2685 [5:45:50<9:00:52, 19.86s/it]

 39%|███▉      | 1052/2685 [5:46:10<9:01:14, 19.89s/it]

 39%|███▉      | 1053/2685 [5:46:29<8:47:50, 19.41s/it]

 39%|███▉      | 1054/2685 [5:46:49<8:52:22, 19.58s/it]

 39%|███▉      | 1055/2685 [5:47:07<8:43:36, 19.27s/it]

 39%|███▉      | 1056/2685 [5:47:25<8:28:54, 18.74s/it]

 39%|███▉      | 1057/2685 [5:47:46<8:52:47, 19.64s/it]

 39%|███▉      | 1058/2685 [5:48:06<8:49:51, 19.54s/it]

 39%|███▉      | 1059/2685 [5:48:27<9:04:27, 20.09s/it]

 39%|███▉      | 1060/2685 [5:48:48<9:10:10, 20.31s/it]

 40%|███▉      | 1061/2685 [5:49:05<8:46:38, 19.46s/it]

 40%|███▉      | 1062/2685 [5:49:27<9:03:47, 20.10s/it]

 40%|███▉      | 1063/2685 [5:49:49<9:21:23, 20.77s/it]

 40%|███▉      | 1064/2685 [5:50:09<9:16:10, 20.59s/it]
{'loss': 0.3219, 'learning_rate': 1.3754267532481605e-06, 'rewards/chosen': -1.0279585123062134, 'rewards/rejected': -3.907273054122925, 'rewards/accuracies': 1.0, 'rewards/margins': 2.879314422607422, 'policy_logps/rejected': -601.629638671875, 'policy_logps/chosen': -542.3130493164062, 'referece_logps/rejected': -562.556884765625, 'referece_logps/chosen': -532.033447265625, 'logits/rejected': -0.24965927004814148, 'logits/chosen': -0.26222437620162964, 'epoch': 1.19}


 40%|███▉      | 1066/2685 [5:50:44<8:22:59, 18.64s/it]

 40%|███▉      | 1067/2685 [5:51:03<8:29:32, 18.89s/it]

 40%|███▉      | 1068/2685 [5:51:22<8:27:20, 18.83s/it]

 40%|███▉      | 1069/2685 [5:51:40<8:26:38, 18.81s/it]

 40%|███▉      | 1070/2685 [5:51:55<7:50:16, 17.47s/it]
{'loss': 0.3391, 'learning_rate': 1.3687077786179488e-06, 'rewards/chosen': -1.2288861274719238, 'rewards/rejected': -2.770163059234619, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5412766933441162, 'policy_logps/rejected': -285.36431884765625, 'policy_logps/chosen': -272.2740478515625, 'referece_logps/rejected': -257.66265869140625, 'referece_logps/chosen': -259.98516845703125, 'logits/rejected': -0.9800929427146912, 'logits/chosen': -1.0542094707489014, 'epoch': 1.2}


 40%|███▉      | 1072/2685 [5:52:31<8:00:26, 17.87s/it]
{'loss': 0.265, 'learning_rate': 1.3664638097229243e-06, 'rewards/chosen': -0.7639061212539673, 'rewards/rejected': -3.77681565284729, 'rewards/accuracies': 0.75, 'rewards/margins': 3.012909173965454, 'policy_logps/rejected': -522.1234741210938, 'policy_logps/chosen': -437.23260498046875, 'referece_logps/rejected': -484.35528564453125, 'referece_logps/chosen': -429.593505859375, 'logits/rejected': -0.2963418662548065, 'logits/chosen': -0.40393686294555664, 'epoch': 1.2}


 40%|████      | 1074/2685 [5:53:12<8:38:46, 19.32s/it]
{'loss': 0.2629, 'learning_rate': 1.3642177072492422e-06, 'rewards/chosen': -1.5395280122756958, 'rewards/rejected': -2.832192897796631, 'rewards/accuracies': 0.75, 'rewards/margins': 1.2926650047302246, 'policy_logps/rejected': -415.2132263183594, 'policy_logps/chosen': -363.7690734863281, 'referece_logps/rejected': -386.8912658691406, 'referece_logps/chosen': -348.373779296875, 'logits/rejected': -1.142282485961914, 'logits/chosen': -1.0140891075134277, 'epoch': 1.2}


 40%|████      | 1076/2685 [5:53:45<8:04:03, 18.05s/it]

 40%|████      | 1077/2685 [5:54:05<8:17:55, 18.58s/it]

 40%|████      | 1078/2685 [5:54:27<8:45:13, 19.61s/it]
{'loss': 0.2467, 'learning_rate': 1.3597191538861318e-06, 'rewards/chosen': -1.1184289455413818, 'rewards/rejected': -3.965714931488037, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8472862243652344, 'policy_logps/rejected': -602.13818359375, 'policy_logps/chosen': -521.3360595703125, 'referece_logps/rejected': -562.4811401367188, 'referece_logps/chosen': -510.1517333984375, 'logits/rejected': 0.5172622799873352, 'logits/chosen': 0.5282124280929565, 'epoch': 1.2}


 40%|████      | 1080/2685 [5:55:02<8:20:37, 18.72s/it]

 40%|████      | 1081/2685 [5:55:21<8:24:34, 18.87s/it]

 40%|████      | 1082/2685 [5:55:41<8:31:48, 19.16s/it]

 40%|████      | 1083/2685 [5:56:01<8:36:43, 19.35s/it]
{'loss': 0.3451, 'learning_rate': 1.3540841939967962e-06, 'rewards/chosen': -1.048213005065918, 'rewards/rejected': -3.199319362640381, 'rewards/accuracies': 0.625, 'rewards/margins': 2.151106595993042, 'policy_logps/rejected': -261.4765930175781, 'policy_logps/chosen': -300.90716552734375, 'referece_logps/rejected': -229.4833984375, 'referece_logps/chosen': -290.4250183105469, 'logits/rejected': -0.4041719436645508, 'logits/chosen': -0.3624912202358246, 'epoch': 1.21}


 40%|████      | 1085/2685 [5:56:33<7:48:38, 17.57s/it]

 40%|████      | 1086/2685 [5:56:53<8:05:08, 18.20s/it]
{'loss': 0.3084, 'learning_rate': 1.350697020425601e-06, 'rewards/chosen': -1.4443180561065674, 'rewards/rejected': -3.27730131149292, 'rewards/accuracies': 0.625, 'rewards/margins': 1.8329832553863525, 'policy_logps/rejected': -571.2332153320312, 'policy_logps/chosen': -472.0442810058594, 'referece_logps/rejected': -538.460205078125, 'referece_logps/chosen': -457.6011047363281, 'logits/rejected': -0.023252127692103386, 'logits/chosen': -0.015039511024951935, 'epoch': 1.21}


 41%|████      | 1088/2685 [5:57:31<8:23:26, 18.91s/it]

 41%|████      | 1089/2685 [5:57:54<8:48:39, 19.87s/it]
{'loss': 0.312, 'learning_rate': 1.34730525284482e-06, 'rewards/chosen': -1.262636661529541, 'rewards/rejected': -3.6985387802124023, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4359018802642822, 'policy_logps/rejected': -374.5810546875, 'policy_logps/chosen': -350.1019592285156, 'referece_logps/rejected': -337.5956726074219, 'referece_logps/chosen': -337.4756164550781, 'logits/rejected': -0.044636793434619904, 'logits/chosen': -0.059727247804403305, 'epoch': 1.22}

 41%|████      | 1090/2685 [5:58:13<8:42:52, 19.67s/it]


 41%|████      | 1092/2685 [5:58:52<8:45:47, 19.80s/it]

 41%|████      | 1093/2685 [5:59:14<9:04:55, 20.54s/it]

 41%|████      | 1094/2685 [5:59:33<8:49:44, 19.98s/it]

 41%|████      | 1095/2685 [5:59:53<8:53:15, 20.12s/it]

 41%|████      | 1096/2685 [6:00:10<8:27:34, 19.17s/it]

 41%|████      | 1097/2685 [6:00:31<8:39:05, 19.61s/it]

 41%|████      | 1098/2685 [6:00:51<8:42:45, 19.76s/it]

 41%|████      | 1099/2685 [6:01:11<8:47:12, 19.94s/it]

 41%|████      | 1100/2685 [6:01:32<8:52:27, 20.16s/it]

 41%|████      | 1101/2685 [6:01:50<8:31:04, 19.36s/it]
{'loss': 0.3337, 'learning_rate': 1.3336931319359426e-06, 'rewards/chosen': -1.5533069372177124, 'rewards/rejected': -3.9233272075653076, 'rewards/accuracies': 0.875, 'rewards/margins': 2.370020627975464, 'policy_logps/rejected': -387.6670227050781, 'policy_logps/chosen': -335.3563232421875, 'referece_logps/rejected': -348.4337463378906, 'referece_logps/chosen': -319.8232421875, 'logits/rejected': -0.6166750192642212, 'logits/chosen': -0.687537670135498, 'epoch': 1.23}

 41%|████      | 1102/2685 [6:02:11<8:46:09, 19.94s/it]


 41%|████      | 1104/2685 [6:02:51<8:50:48, 20.14s/it]

 41%|████      | 1105/2685 [6:03:14<9:06:04, 20.74s/it]

 41%|████      | 1106/2685 [6:03:34<9:04:06, 20.68s/it]

 41%|████      | 1107/2685 [6:03:56<9:11:36, 20.97s/it]

 41%|████▏     | 1108/2685 [6:04:11<8:28:57, 19.36s/it]
{'loss': 0.3053, 'learning_rate': 1.3257202462100598e-06, 'rewards/chosen': -0.6140145063400269, 'rewards/rejected': -3.64908766746521, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0350732803344727, 'policy_logps/rejected': -404.06414794921875, 'policy_logps/chosen': -321.2052917480469, 'referece_logps/rejected': -367.57330322265625, 'referece_logps/chosen': -315.0651550292969, 'logits/rejected': -0.5488153100013733, 'logits/chosen': -0.5823711156845093, 'epoch': 1.24}


 41%|████▏     | 1110/2685 [6:04:51<8:37:35, 19.72s/it]

 41%|████▏     | 1111/2685 [6:05:11<8:39:36, 19.81s/it]
{'loss': 0.3161, 'learning_rate': 1.3222961506332218e-06, 'rewards/chosen': -0.9130340218544006, 'rewards/rejected': -3.5575873851776123, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6445531845092773, 'policy_logps/rejected': -369.020751953125, 'policy_logps/chosen': -290.9808349609375, 'referece_logps/rejected': -333.4449157714844, 'referece_logps/chosen': -281.85052490234375, 'logits/rejected': -0.07004924863576889, 'logits/chosen': -0.09250548481941223, 'epoch': 1.24}

 41%|████▏     | 1112/2685 [6:05:31<8:39:05, 19.80s/it]

 41%|████▏     | 1113/2685 [6:05:51<8:39:21, 19.82s/it]


 42%|████▏     | 1115/2685 [6:06:34<9:01:59, 20.71s/it]

 42%|████▏     | 1116/2685 [6:06:54<8:54:09, 20.43s/it]

 42%|████▏     | 1117/2685 [6:07:15<8:54:52, 20.47s/it]

 42%|████▏     | 1118/2685 [6:07:37<9:08:38, 21.01s/it]

 42%|████▏     | 1119/2685 [6:07:57<8:57:57, 20.61s/it]

 42%|████▏     | 1120/2685 [6:08:16<8:49:49, 20.31s/it]

 42%|████▏     | 1121/2685 [6:08:36<8:45:15, 20.15s/it]

 42%|████▏     | 1122/2685 [6:08:52<8:11:12, 18.86s/it]

 42%|████▏     | 1123/2685 [6:09:14<8:39:48, 19.97s/it]

 42%|████▏     | 1124/2685 [6:09:32<8:19:27, 19.20s/it]

 42%|████▏     | 1125/2685 [6:09:55<8:49:43, 20.37s/it]

 42%|████▏     | 1126/2685 [6:10:15<8:45:02, 20.21s/it]

 42%|████▏     | 1127/2685 [6:10:36<8:53:10, 20.53s/it]

 42%|████▏     | 1128/2685 [6:10:58<9:05:42, 21.03s/it]

 42%|████▏     | 1129/2685 [6:11:20<9:14:28, 21.38s/it]
{'loss': 0.3919, 'learning_rate': 1.3016644895885697e-06, 'rewards/chosen': -0.6773412227630615, 'rewards/rejected': -3.761683702468872, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0843427181243896, 'policy_logps/rejected': -276.0089111328125, 'policy_logps/chosen': -243.5392303466797, 'referece_logps/rejected': -238.39205932617188, 'referece_logps/chosen': -236.7658233642578, 'logits/rejected': -0.28690311312675476, 'logits/chosen': -0.40863311290740967, 'epoch': 1.26}


 42%|████▏     | 1131/2685 [6:12:01<8:59:07, 20.82s/it]
{'loss': 0.3363, 'learning_rate': 1.2993631229733582e-06, 'rewards/chosen': -0.2176734060049057, 'rewards/rejected': -2.4753177165985107, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2576441764831543, 'policy_logps/rejected': -332.6374206542969, 'policy_logps/chosen': -360.87542724609375, 'referece_logps/rejected': -307.8842468261719, 'referece_logps/chosen': -358.6986999511719, 'logits/rejected': -0.7201776504516602, 'logits/chosen': -0.5413432121276855, 'epoch': 1.26}


 42%|████▏     | 1133/2685 [6:12:39<8:32:59, 19.83s/it]

 42%|████▏     | 1134/2685 [6:12:58<8:30:59, 19.77s/it]

 42%|████▏     | 1135/2685 [6:13:18<8:28:25, 19.68s/it]

 42%|████▏     | 1136/2685 [6:13:40<8:46:02, 20.38s/it]

 42%|████▏     | 1137/2685 [6:13:59<8:37:37, 20.06s/it]

 42%|████▏     | 1138/2685 [6:14:21<8:53:50, 20.71s/it]

 42%|████▏     | 1139/2685 [6:14:43<8:59:48, 20.95s/it]

 42%|████▏     | 1140/2685 [6:15:01<8:39:46, 20.19s/it]

 42%|████▏     | 1141/2685 [6:15:21<8:33:10, 19.94s/it]

 43%|████▎     | 1142/2685 [6:15:41<8:38:36, 20.17s/it]
{'loss': 0.1922, 'learning_rate': 1.286674811769128e-06, 'rewards/chosen': 0.04926244169473648, 'rewards/rejected': -1.1009050607681274, 'rewards/accuracies': 0.75, 'rewards/margins': 1.150167465209961, 'policy_logps/rejected': -277.7724304199219, 'policy_logps/chosen': -310.4922180175781, 'referece_logps/rejected': -266.76336669921875, 'referece_logps/chosen': -310.98486328125, 'logits/rejected': -0.2189059853553772, 'logits/chosen': -0.32166630029678345, 'epoch': 1.28}


 43%|████▎     | 1144/2685 [6:16:16<8:03:47, 18.84s/it]

 43%|████▎     | 1145/2685 [6:16:37<8:19:31, 19.46s/it]

 43%|████▎     | 1146/2685 [6:16:50<7:28:53, 17.50s/it]

 43%|████▎     | 1147/2685 [6:17:10<7:48:31, 18.28s/it]
{'loss': 0.326, 'learning_rate': 1.280890572764777e-06, 'rewards/chosen': -1.367791771888733, 'rewards/rejected': -2.9432010650634766, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5754094123840332, 'policy_logps/rejected': -438.5733642578125, 'policy_logps/chosen': -398.4869079589844, 'referece_logps/rejected': -409.141357421875, 'referece_logps/chosen': -384.8089904785156, 'logits/rejected': -0.8541791439056396, 'logits/chosen': -0.9752340316772461, 'epoch': 1.28}


 43%|████▎     | 1149/2685 [6:17:51<8:19:06, 19.50s/it]
{'loss': 0.2889, 'learning_rate': 1.2785740034821328e-06, 'rewards/chosen': -0.7685942053794861, 'rewards/rejected': -4.352146625518799, 'rewards/accuracies': 0.875, 'rewards/margins': 3.583552360534668, 'policy_logps/rejected': -408.56060791015625, 'policy_logps/chosen': -381.7522277832031, 'referece_logps/rejected': -365.0390930175781, 'referece_logps/chosen': -374.0662536621094, 'logits/rejected': -0.36257439851760864, 'logits/chosen': -0.39851418137550354, 'epoch': 1.28}


 43%|████▎     | 1151/2685 [6:18:27<7:52:32, 18.48s/it]

 43%|████▎     | 1152/2685 [6:18:45<7:52:11, 18.48s/it]
{'loss': 0.2823, 'learning_rate': 1.2750961127544778e-06, 'rewards/chosen': -1.0919222831726074, 'rewards/rejected': -3.745480537414551, 'rewards/accuracies': 0.875, 'rewards/margins': 2.653557777404785, 'policy_logps/rejected': -342.9497375488281, 'policy_logps/chosen': -341.763916015625, 'referece_logps/rejected': -305.49493408203125, 'referece_logps/chosen': -330.8446960449219, 'logits/rejected': -0.8644272685050964, 'logits/chosen': -0.8086774349212646, 'epoch': 1.29}


 43%|████▎     | 1154/2685 [6:19:23<8:04:52, 19.00s/it]

 43%|████▎     | 1155/2685 [6:19:45<8:25:35, 19.83s/it]
{'loss': 0.3119, 'learning_rate': 1.2716146183629618e-06, 'rewards/chosen': -1.271532654762268, 'rewards/rejected': -3.577235698699951, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3057031631469727, 'policy_logps/rejected': -353.799072265625, 'policy_logps/chosen': -270.35198974609375, 'referece_logps/rejected': -318.0267028808594, 'referece_logps/chosen': -257.63665771484375, 'logits/rejected': 0.013543237000703812, 'logits/chosen': -0.025907747447490692, 'epoch': 1.29}


 43%|████▎     | 1157/2685 [6:20:26<8:30:31, 20.05s/it]
{'loss': 0.3317, 'learning_rate': 1.2692916425862123e-06, 'rewards/chosen': -1.2028181552886963, 'rewards/rejected': -3.0280380249023438, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8252201080322266, 'policy_logps/rejected': -447.1390380859375, 'policy_logps/chosen': -383.5639953613281, 'referece_logps/rejected': -416.858642578125, 'referece_logps/chosen': -371.53582763671875, 'logits/rejected': 0.2797589898109436, 'logits/chosen': 0.2112141251564026, 'epoch': 1.29}


 43%|████▎     | 1159/2685 [6:21:06<8:28:35, 20.00s/it]

 43%|████▎     | 1160/2685 [6:21:24<8:17:42, 19.58s/it]

 43%|████▎     | 1161/2685 [6:21:39<7:42:14, 18.20s/it]

 43%|████▎     | 1162/2685 [6:21:55<7:19:38, 17.32s/it]
{'loss': 0.3158, 'learning_rate': 1.2634773734722095e-06, 'rewards/chosen': 1.0153242349624634, 'rewards/rejected': -1.7249157428741455, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7402396202087402, 'policy_logps/rejected': -335.1513671875, 'policy_logps/chosen': -285.6839904785156, 'referece_logps/rejected': -317.9022216796875, 'referece_logps/chosen': -295.83721923828125, 'logits/rejected': -0.501617968082428, 'logits/chosen': -0.395805299282074, 'epoch': 1.3}

 43%|████▎     | 1163/2685 [6:22:14<7:38:28, 18.07s/it]

 43%|████▎     | 1164/2685 [6:22:37<8:10:23, 19.34s/it]

 43%|████▎     | 1165/2685 [6:22:59<8:29:58, 20.13s/it]


 43%|████▎     | 1167/2685 [6:23:39<8:28:02, 20.08s/it]

 44%|████▎     | 1168/2685 [6:23:58<8:14:48, 19.57s/it]

 44%|████▎     | 1169/2685 [6:24:20<8:32:37, 20.29s/it]
{'loss': 0.3108, 'learning_rate': 1.2553213373773471e-06, 'rewards/chosen': -0.1089944839477539, 'rewards/rejected': -2.3610498905181885, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2520556449890137, 'policy_logps/rejected': -411.9284973144531, 'policy_logps/chosen': -346.9314270019531, 'referece_logps/rejected': -388.3179931640625, 'referece_logps/chosen': -345.84149169921875, 'logits/rejected': 0.59398353099823, 'logits/chosen': 0.5873957872390747, 'epoch': 1.31}


 44%|████▎     | 1171/2685 [6:24:58<8:14:01, 19.58s/it]

 44%|████▎     | 1172/2685 [6:25:18<8:13:21, 19.56s/it]
{'loss': 0.3334, 'learning_rate': 1.2518202850310247e-06, 'rewards/chosen': -0.9456658959388733, 'rewards/rejected': -4.863353729248047, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9176878929138184, 'policy_logps/rejected': -423.8534851074219, 'policy_logps/chosen': -428.11212158203125, 'referece_logps/rejected': -375.2199401855469, 'referece_logps/chosen': -418.6554870605469, 'logits/rejected': -0.2295936495065689, 'logits/chosen': -0.39878636598587036, 'epoch': 1.31}

 44%|████▎     | 1173/2685 [6:25:37<8:09:56, 19.44s/it]

 44%|████▎     | 1174/2685 [6:26:00<8:39:43, 20.64s/it]

 44%|████▍     | 1175/2685 [6:26:19<8:21:34, 19.93s/it]

 44%|████▍     | 1176/2685 [6:26:39<8:24:19, 20.05s/it]

 44%|████▍     | 1177/2685 [6:27:03<8:55:55, 21.32s/it]


 44%|████▍     | 1179/2685 [6:27:43<8:35:55, 20.56s/it]

 44%|████▍     | 1180/2685 [6:28:02<8:17:51, 19.85s/it]

 44%|████▍     | 1181/2685 [6:28:22<8:21:10, 19.99s/it]
{'loss': 0.2889, 'learning_rate': 1.2412975191063677e-06, 'rewards/chosen': -1.6982933282852173, 'rewards/rejected': -4.2299299240112305, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5316362380981445, 'policy_logps/rejected': -485.6117858886719, 'policy_logps/chosen': -442.29827880859375, 'referece_logps/rejected': -443.3125, 'referece_logps/chosen': -425.31536865234375, 'logits/rejected': 0.30989301204681396, 'logits/chosen': 0.34520840644836426, 'epoch': 1.32}


 44%|████▍     | 1183/2685 [6:29:00<8:07:35, 19.48s/it]
{'loss': 0.2834, 'learning_rate': 1.2389552196277024e-06, 'rewards/chosen': -1.2071176767349243, 'rewards/rejected': -3.2632744312286377, 'rewards/accuracies': 1.0, 'rewards/margins': 2.056156873703003, 'policy_logps/rejected': -350.0530090332031, 'policy_logps/chosen': -372.65740966796875, 'referece_logps/rejected': -317.4202880859375, 'referece_logps/chosen': -360.58624267578125, 'logits/rejected': 0.08652505278587341, 'logits/chosen': 0.04604780673980713, 'epoch': 1.32}


 44%|████▍     | 1185/2685 [6:29:36<7:51:20, 18.85s/it]
{'loss': 0.3933, 'learning_rate': 1.236611528934562e-06, 'rewards/chosen': -1.771013617515564, 'rewards/rejected': -4.323304653167725, 'rewards/accuracies': 0.75, 'rewards/margins': 2.55229115486145, 'policy_logps/rejected': -513.8665161132812, 'policy_logps/chosen': -475.2182312011719, 'referece_logps/rejected': -470.63348388671875, 'referece_logps/chosen': -457.50811767578125, 'logits/rejected': 0.29974666237831116, 'logits/chosen': 0.3302692472934723, 'epoch': 1.32}


 44%|████▍     | 1187/2685 [6:30:18<8:16:23, 19.88s/it]
{'loss': 0.296, 'learning_rate': 1.234266460672082e-06, 'rewards/chosen': -1.8612945079803467, 'rewards/rejected': -5.512618064880371, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6513235569000244, 'policy_logps/rejected': -487.3687744140625, 'policy_logps/chosen': -398.37548828125, 'referece_logps/rejected': -432.24261474609375, 'referece_logps/chosen': -379.76251220703125, 'logits/rejected': 0.010425969958305359, 'logits/chosen': -0.1456339806318283, 'epoch': 1.33}


 44%|████▍     | 1189/2685 [6:30:58<8:14:44, 19.84s/it]

 44%|████▍     | 1190/2685 [6:31:18<8:17:50, 19.98s/it]
{'loss': 0.3203, 'learning_rate': 1.2307463052043036e-06, 'rewards/chosen': -1.4883071184158325, 'rewards/rejected': -5.153033256530762, 'rewards/accuracies': 1.0, 'rewards/margins': 3.6647260189056396, 'policy_logps/rejected': -628.3222045898438, 'policy_logps/chosen': -561.9620971679688, 'referece_logps/rejected': -576.7918701171875, 'referece_logps/chosen': -547.0790405273438, 'logits/rejected': -0.08787503838539124, 'logits/chosen': -0.09021159261465073, 'epoch': 1.33}


 44%|████▍     | 1192/2685 [6:32:00<8:25:45, 20.33s/it]
{'loss': 0.3695, 'learning_rate': 1.2283978527683821e-06, 'rewards/chosen': -1.9585846662521362, 'rewards/rejected': -4.982072830200195, 'rewards/accuracies': 0.875, 'rewards/margins': 3.0234882831573486, 'policy_logps/rejected': -455.79229736328125, 'policy_logps/chosen': -416.2132568359375, 'referece_logps/rejected': -405.97149658203125, 'referece_logps/chosen': -396.62744140625, 'logits/rejected': 0.719200611114502, 'logits/chosen': 0.665429413318634, 'epoch': 1.33}


 44%|████▍     | 1194/2685 [6:32:38<8:17:55, 20.04s/it]

 45%|████▍     | 1195/2685 [6:32:58<8:15:52, 19.97s/it]

 45%|████▍     | 1196/2685 [6:33:14<7:44:43, 18.73s/it]

 45%|████▍     | 1197/2685 [6:33:30<7:23:11, 17.87s/it]
{'loss': 0.2828, 'learning_rate': 1.2225209339563143e-06, 'rewards/chosen': -0.7896104454994202, 'rewards/rejected': -3.3304121494293213, 'rewards/accuracies': 0.75, 'rewards/margins': 2.540801525115967, 'policy_logps/rejected': -426.6656799316406, 'policy_logps/chosen': -336.79766845703125, 'referece_logps/rejected': -393.361572265625, 'referece_logps/chosen': -328.9015808105469, 'logits/rejected': -0.04560965299606323, 'logits/chosen': -0.11210614442825317, 'epoch': 1.34}

 45%|████▍     | 1198/2685 [6:33:51<7:47:11, 18.85s/it]


 45%|████▍     | 1200/2685 [6:34:26<7:33:38, 18.33s/it]
{'loss': 0.3512, 'learning_rate': 1.2189908823816773e-06, 'rewards/chosen': -0.3020744323730469, 'rewards/rejected': -3.0572121143341064, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7551379203796387, 'policy_logps/rejected': -370.2904052734375, 'policy_logps/chosen': -342.19793701171875, 'referece_logps/rejected': -339.7182922363281, 'referece_logps/chosen': -339.17718505859375, 'logits/rejected': -0.42505359649658203, 'logits/chosen': -0.4776361882686615, 'epoch': 1.34}

 45%|████▍     | 1201/2685 [6:34:48<7:55:41, 19.23s/it]

 45%|████▍     | 1202/2685 [6:35:06<7:46:08, 18.86s/it]


 45%|████▍     | 1204/2685 [6:35:45<7:54:06, 19.21s/it]
{'loss': 0.3138, 'learning_rate': 1.2142796925171443e-06, 'rewards/chosen': -1.0575650930404663, 'rewards/rejected': -4.487183570861816, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4296183586120605, 'policy_logps/rejected': -521.94677734375, 'policy_logps/chosen': -471.1203918457031, 'referece_logps/rejected': -477.07489013671875, 'referece_logps/chosen': -460.544677734375, 'logits/rejected': 0.47226324677467346, 'logits/chosen': 0.31698495149612427, 'epoch': 1.35}

 45%|████▍     | 1205/2685 [6:36:04<7:51:07, 19.10s/it]

 45%|████▍     | 1206/2685 [6:36:25<8:06:57, 19.75s/it]

 45%|████▍     | 1207/2685 [6:36:47<8:24:49, 20.49s/it]

 45%|████▍     | 1208/2685 [6:37:03<7:51:47, 19.17s/it]


 45%|████▌     | 1210/2685 [6:37:49<8:32:09, 20.83s/it]

 45%|████▌     | 1211/2685 [6:38:08<8:23:39, 20.50s/it]

 45%|████▌     | 1212/2685 [6:38:28<8:19:41, 20.35s/it]

 45%|████▌     | 1213/2685 [6:38:50<8:29:18, 20.76s/it]

 45%|████▌     | 1214/2685 [6:39:13<8:45:17, 21.43s/it]

 45%|████▌     | 1215/2685 [6:39:34<8:44:13, 21.40s/it]

 45%|████▌     | 1216/2685 [6:39:56<8:46:59, 21.52s/it]
{'loss': 0.3142, 'learning_rate': 1.200116621164417e-06, 'rewards/chosen': -0.3562829792499542, 'rewards/rejected': -3.436661720275879, 'rewards/accuracies': 0.875, 'rewards/margins': 3.080378532409668, 'policy_logps/rejected': -462.4058837890625, 'policy_logps/chosen': -488.4089050292969, 'referece_logps/rejected': -428.03924560546875, 'referece_logps/chosen': -484.8460693359375, 'logits/rejected': 0.372123658657074, 'logits/chosen': 0.33196502923965454, 'epoch': 1.36}

 45%|████▌     | 1217/2685 [6:40:18<8:47:23, 21.56s/it]

 45%|████▌     | 1218/2685 [6:40:36<8:21:43, 20.52s/it]


 45%|████▌     | 1220/2685 [6:41:21<8:45:38, 21.53s/it]
{'loss': 0.2791, 'learning_rate': 1.1953861299420834e-06, 'rewards/chosen': -1.1007970571517944, 'rewards/rejected': -2.781482696533203, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6806856393814087, 'policy_logps/rejected': -318.5765686035156, 'policy_logps/chosen': -324.3466796875, 'referece_logps/rejected': -290.7617492675781, 'referece_logps/chosen': -313.3387756347656, 'logits/rejected': -0.4388878047466278, 'logits/chosen': -0.5038618445396423, 'epoch': 1.36}


 46%|████▌     | 1222/2685 [6:42:02<8:33:23, 21.06s/it]

 46%|████▌     | 1223/2685 [6:42:18<7:55:29, 19.51s/it]

 46%|████▌     | 1224/2685 [6:42:40<8:13:24, 20.26s/it]
{'loss': 0.3422, 'learning_rate': 1.190651088517967e-06, 'rewards/chosen': -1.067418098449707, 'rewards/rejected': -2.426055669784546, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3586375713348389, 'policy_logps/rejected': -319.6138916015625, 'policy_logps/chosen': -264.0831604003906, 'referece_logps/rejected': -295.3533935546875, 'referece_logps/chosen': -253.40896606445312, 'logits/rejected': -0.7003564834594727, 'logits/chosen': -0.8411539196968079, 'epoch': 1.37}


 46%|████▌     | 1226/2685 [6:43:25<8:40:34, 21.41s/it]
{'loss': 0.2349, 'learning_rate': 1.1882818959358107e-06, 'rewards/chosen': -0.5803796648979187, 'rewards/rejected': -2.552436113357544, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9720563888549805, 'policy_logps/rejected': -641.2780151367188, 'policy_logps/chosen': -506.1751708984375, 'referece_logps/rejected': -615.753662109375, 'referece_logps/chosen': -500.371337890625, 'logits/rejected': -0.22123371064662933, 'logits/chosen': -0.27937793731689453, 'epoch': 1.37}


 46%|████▌     | 1228/2685 [6:44:04<8:16:07, 20.43s/it]

 46%|████▌     | 1229/2685 [6:44:24<8:12:28, 20.29s/it]
{'loss': 0.2839, 'learning_rate': 1.1847260560171894e-06, 'rewards/chosen': -0.8001191020011902, 'rewards/rejected': -4.383524417877197, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5834057331085205, 'policy_logps/rejected': -440.372314453125, 'policy_logps/chosen': -362.05267333984375, 'referece_logps/rejected': -396.53704833984375, 'referece_logps/chosen': -354.05145263671875, 'logits/rejected': -0.06515731662511826, 'logits/chosen': 0.0036322418600320816, 'epoch': 1.37}


 46%|████▌     | 1231/2685 [6:44:59<7:23:45, 18.31s/it]
{'loss': 0.358, 'learning_rate': 1.18235414883514e-06, 'rewards/chosen': -1.387049674987793, 'rewards/rejected': -2.9539406299591064, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5668909549713135, 'policy_logps/rejected': -443.35699462890625, 'policy_logps/chosen': -412.8738098144531, 'referece_logps/rejected': -413.8175354003906, 'referece_logps/chosen': -399.0033264160156, 'logits/rejected': -0.9108096957206726, 'logits/chosen': -0.7912710309028625, 'epoch': 1.38}


 46%|████▌     | 1233/2685 [6:45:41<7:50:50, 19.46s/it]

 46%|████▌     | 1234/2685 [6:45:59<7:37:30, 18.92s/it]
{'loss': 0.3542, 'learning_rate': 1.178794301731132e-06, 'rewards/chosen': -0.6779539585113525, 'rewards/rejected': -3.100503444671631, 'rewards/accuracies': 0.75, 'rewards/margins': 2.422549247741699, 'policy_logps/rejected': -399.7385559082031, 'policy_logps/chosen': -340.43341064453125, 'referece_logps/rejected': -368.7334899902344, 'referece_logps/chosen': -333.65386962890625, 'logits/rejected': 0.423799991607666, 'logits/chosen': 0.4930295944213867, 'epoch': 1.38}

 46%|████▌     | 1235/2685 [6:46:18<7:42:23, 19.13s/it]

 46%|████▌     | 1236/2685 [6:46:40<8:00:59, 19.92s/it]


 46%|████▌     | 1238/2685 [6:47:19<7:46:56, 19.36s/it]

 46%|████▌     | 1239/2685 [6:47:39<7:52:53, 19.62s/it]
{'loss': 0.3155, 'learning_rate': 1.1728560415036199e-06, 'rewards/chosen': -3.0106492042541504, 'rewards/rejected': -5.6045308113098145, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5938823223114014, 'policy_logps/rejected': -569.7592163085938, 'policy_logps/chosen': -501.3031311035156, 'referece_logps/rejected': -513.7138671875, 'referece_logps/chosen': -471.19659423828125, 'logits/rejected': 0.04650191217660904, 'logits/chosen': 0.05021291971206665, 'epoch': 1.38}

 46%|████▌     | 1240/2685 [6:48:00<8:02:52, 20.05s/it]


 46%|████▋     | 1242/2685 [6:48:37<7:49:39, 19.53s/it]
{'loss': 0.3909, 'learning_rate': 1.1692900523951164e-06, 'rewards/chosen': -0.5306842923164368, 'rewards/rejected': -2.358243942260742, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8275595903396606, 'policy_logps/rejected': -372.08099365234375, 'policy_logps/chosen': -343.49395751953125, 'referece_logps/rejected': -348.49853515625, 'referece_logps/chosen': -338.1871337890625, 'logits/rejected': -0.8546295166015625, 'logits/chosen': -0.909208357334137, 'epoch': 1.39}

 46%|████▋     | 1243/2685 [6:48:58<8:01:23, 20.03s/it]

 46%|████▋     | 1244/2685 [6:49:19<8:03:12, 20.12s/it]

 46%|████▋     | 1245/2685 [6:49:38<8:00:43, 20.03s/it]

 46%|████▋     | 1246/2685 [6:49:58<7:59:23, 19.99s/it]

 46%|████▋     | 1247/2685 [6:50:18<7:55:19, 19.83s/it]


 47%|████▋     | 1249/2685 [6:51:02<8:18:50, 20.84s/it]

 47%|████▋     | 1250/2685 [6:51:15<7:28:03, 18.73s/it]

 47%|████▋     | 1251/2685 [6:51:35<7:34:48, 19.03s/it]
{'loss': 0.3797, 'learning_rate': 1.1585789662205834e-06, 'rewards/chosen': -1.1133064031600952, 'rewards/rejected': -2.665008306503296, 'rewards/accuracies': 0.625, 'rewards/margins': 1.5517019033432007, 'policy_logps/rejected': -357.4283142089844, 'policy_logps/chosen': -329.4320068359375, 'referece_logps/rejected': -330.77825927734375, 'referece_logps/chosen': -318.2989196777344, 'logits/rejected': 0.09095114469528198, 'logits/chosen': 0.04088878259062767, 'epoch': 1.4}

 47%|████▋     | 1252/2685 [6:51:55<7:39:16, 19.23s/it]

 47%|████▋     | 1253/2685 [6:52:14<7:38:50, 19.23s/it]

 47%|████▋     | 1254/2685 [6:52:36<8:01:08, 20.17s/it]

 47%|████▋     | 1255/2685 [6:52:57<8:02:49, 20.26s/it]


 47%|████▋     | 1257/2685 [6:53:37<8:06:20, 20.43s/it]

 47%|████▋     | 1258/2685 [6:53:58<8:06:20, 20.45s/it]
{'loss': 0.2709, 'learning_rate': 1.1502351311753827e-06, 'rewards/chosen': -1.3595620393753052, 'rewards/rejected': -4.213291645050049, 'rewards/accuracies': 0.875, 'rewards/margins': 2.853729724884033, 'policy_logps/rejected': -351.03277587890625, 'policy_logps/chosen': -393.775634765625, 'referece_logps/rejected': -308.8998718261719, 'referece_logps/chosen': -380.17999267578125, 'logits/rejected': 0.14750084280967712, 'logits/chosen': 0.045075684785842896, 'epoch': 1.41}

 47%|████▋     | 1259/2685 [6:54:19<8:11:46, 20.69s/it]


 47%|████▋     | 1261/2685 [6:54:58<7:56:59, 20.10s/it]
{'loss': 0.4188, 'learning_rate': 1.1466558871129789e-06, 'rewards/chosen': -0.7158649563789368, 'rewards/rejected': -2.2454957962036133, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5296308994293213, 'policy_logps/rejected': -380.8260498046875, 'policy_logps/chosen': -419.0198974609375, 'referece_logps/rejected': -358.37109375, 'referece_logps/chosen': -411.86126708984375, 'logits/rejected': -0.0034940987825393677, 'logits/chosen': -0.10426554083824158, 'epoch': 1.41}


 47%|████▋     | 1263/2685 [6:55:38<7:57:00, 20.13s/it]

 47%|████▋     | 1264/2685 [6:55:58<7:54:47, 20.05s/it]
{'loss': 0.3303, 'learning_rate': 1.143074721909214e-06, 'rewards/chosen': -1.1733863353729248, 'rewards/rejected': -2.880308151245117, 'rewards/accuracies': 0.875, 'rewards/margins': 1.706921935081482, 'policy_logps/rejected': -311.1597900390625, 'policy_logps/chosen': -274.1600036621094, 'referece_logps/rejected': -282.356689453125, 'referece_logps/chosen': -262.4261474609375, 'logits/rejected': -1.1147336959838867, 'logits/chosen': -1.0825026035308838, 'epoch': 1.41}

 47%|████▋     | 1265/2685 [6:56:16<7:45:26, 19.67s/it]


 47%|████▋     | 1267/2685 [6:56:58<7:54:28, 20.08s/it]
{'loss': 0.269, 'learning_rate': 1.1394916824761127e-06, 'rewards/chosen': -1.312798261642456, 'rewards/rejected': -2.572908639907837, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2601102590560913, 'policy_logps/rejected': -367.1080322265625, 'policy_logps/chosen': -341.6356201171875, 'referece_logps/rejected': -341.3789367675781, 'referece_logps/chosen': -328.50762939453125, 'logits/rejected': -0.8592005372047424, 'logits/chosen': -0.7567476630210876, 'epoch': 1.42}

 47%|████▋     | 1268/2685 [6:57:13<7:19:31, 18.61s/it]

 47%|████▋     | 1269/2685 [6:57:25<6:30:22, 16.54s/it]

 47%|████▋     | 1270/2685 [6:57:42<6:37:38, 16.86s/it]


 47%|████▋     | 1272/2685 [6:58:26<7:36:01, 19.36s/it]

 47%|████▋     | 1273/2685 [6:58:46<7:40:26, 19.57s/it]
{'loss': 0.3558, 'learning_rate': 1.1323201686921437e-06, 'rewards/chosen': -2.1715474128723145, 'rewards/rejected': -4.577601909637451, 'rewards/accuracies': 0.75, 'rewards/margins': 2.406054973602295, 'policy_logps/rejected': -477.70050048828125, 'policy_logps/chosen': -411.2311096191406, 'referece_logps/rejected': -431.9244384765625, 'referece_logps/chosen': -389.515625, 'logits/rejected': -0.46233266592025757, 'logits/chosen': -0.3275449872016907, 'epoch': 1.42}


 47%|████▋     | 1275/2685 [6:59:26<7:46:19, 19.84s/it]
{'loss': 0.317, 'learning_rate': 1.1299281047808876e-06, 'rewards/chosen': -0.7189115881919861, 'rewards/rejected': -2.605896234512329, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8869848251342773, 'policy_logps/rejected': -540.430419921875, 'policy_logps/chosen': -509.9092102050781, 'referece_logps/rejected': -514.3715209960938, 'referece_logps/chosen': -502.7200927734375, 'logits/rejected': -0.2339431345462799, 'logits/chosen': -0.20630131661891937, 'epoch': 1.42}

 48%|████▊     | 1276/2685 [6:59:45<7:41:31, 19.65s/it]

 48%|████▊     | 1277/2685 [7:00:01<7:14:56, 18.53s/it]

 48%|████▊     | 1278/2685 [7:00:21<7:20:12, 18.77s/it]


 48%|████▊     | 1280/2685 [7:01:00<7:34:26, 19.41s/it]

 48%|████▊     | 1281/2685 [7:01:22<7:50:55, 20.12s/it]
{'loss': 0.3473, 'learning_rate': 1.1227474300715054e-06, 'rewards/chosen': -0.728079080581665, 'rewards/rejected': -2.9069724082946777, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1788930892944336, 'policy_logps/rejected': -418.71881103515625, 'policy_logps/chosen': -295.2185363769531, 'referece_logps/rejected': -389.6490478515625, 'referece_logps/chosen': -287.937744140625, 'logits/rejected': -0.18226024508476257, 'logits/chosen': -0.1813402771949768, 'epoch': 1.43}


 48%|████▊     | 1283/2685 [7:02:02<7:45:46, 19.93s/it]
{'loss': 0.3411, 'learning_rate': 1.12035242396141e-06, 'rewards/chosen': -0.3414854109287262, 'rewards/rejected': -2.4288089275360107, 'rewards/accuracies': 1.0, 'rewards/margins': 2.0873231887817383, 'policy_logps/rejected': -313.7946472167969, 'policy_logps/chosen': -453.1690673828125, 'referece_logps/rejected': -289.5065612792969, 'referece_logps/chosen': -449.75421142578125, 'logits/rejected': -0.03588039427995682, 'logits/chosen': -0.014503933489322662, 'epoch': 1.43}

 48%|████▊     | 1284/2685 [7:02:23<7:53:53, 20.29s/it]

 48%|████▊     | 1285/2685 [7:02:43<7:49:06, 20.10s/it]

 48%|████▊     | 1286/2685 [7:03:01<7:37:49, 19.64s/it]

 48%|████▊     | 1287/2685 [7:03:22<7:43:40, 19.90s/it]

 48%|████▊     | 1288/2685 [7:03:43<7:50:23, 20.20s/it]

 48%|████▊     | 1289/2685 [7:04:03<7:47:40, 20.10s/it]

 48%|████▊     | 1290/2685 [7:04:23<7:51:32, 20.28s/it]

 48%|████▊     | 1291/2685 [7:04:39<7:21:37, 19.01s/it]

 48%|████▊     | 1292/2685 [7:05:00<7:28:25, 19.32s/it]

 48%|████▊     | 1293/2685 [7:05:21<7:41:16, 19.88s/it]


 48%|████▊     | 1295/2685 [7:06:04<8:01:58, 20.80s/it]

 48%|████▊     | 1296/2685 [7:06:26<8:10:59, 21.21s/it]
{'loss': 0.3148, 'learning_rate': 1.1047684281702598e-06, 'rewards/chosen': -1.0488828420639038, 'rewards/rejected': -1.9465489387512207, 'rewards/accuracies': 0.875, 'rewards/margins': 0.8976662158966064, 'policy_logps/rejected': -257.042236328125, 'policy_logps/chosen': -260.1808166503906, 'referece_logps/rejected': -237.5767364501953, 'referece_logps/chosen': -249.6919708251953, 'logits/rejected': -0.8918840289115906, 'logits/chosen': -0.9251528978347778, 'epoch': 1.45}

 48%|████▊     | 1297/2685 [7:06:48<8:11:18, 21.24s/it]

 48%|████▊     | 1298/2685 [7:07:08<8:01:52, 20.85s/it]

 48%|████▊     | 1299/2685 [7:07:26<7:45:14, 20.14s/it]

 48%|████▊     | 1300/2685 [7:07:46<7:41:26, 19.99s/it]

 48%|████▊     | 1301/2685 [7:08:07<7:52:07, 20.47s/it]

 48%|████▊     | 1302/2685 [7:08:19<6:51:54, 17.87s/it]

 49%|████▊     | 1303/2685 [7:08:37<6:53:56, 17.97s/it]

 49%|████▊     | 1304/2685 [7:08:57<7:06:55, 18.55s/it]

 49%|████▊     | 1305/2685 [7:09:16<7:05:27, 18.50s/it]


 49%|████▊     | 1307/2685 [7:09:57<7:29:27, 19.57s/it]
{'loss': 0.3141, 'learning_rate': 1.0915616878794702e-06, 'rewards/chosen': -2.154869794845581, 'rewards/rejected': -4.789066791534424, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6341965198516846, 'policy_logps/rejected': -523.7123413085938, 'policy_logps/chosen': -495.5040283203125, 'referece_logps/rejected': -475.8216857910156, 'referece_logps/chosen': -473.9553527832031, 'logits/rejected': 0.278153657913208, 'logits/chosen': 0.3005828261375427, 'epoch': 1.46}

 49%|████▊     | 1308/2685 [7:10:12<6:58:47, 18.25s/it]


 49%|████▉     | 1310/2685 [7:10:49<7:02:10, 18.42s/it]
{'loss': 0.3343, 'learning_rate': 1.0879569530640785e-06, 'rewards/chosen': -2.7062606811523438, 'rewards/rejected': -5.47229528427124, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7660348415374756, 'policy_logps/rejected': -459.8420104980469, 'policy_logps/chosen': -399.8648376464844, 'referece_logps/rejected': -405.1190490722656, 'referece_logps/chosen': -372.80224609375, 'logits/rejected': 0.4946211278438568, 'logits/chosen': 0.45919734239578247, 'epoch': 1.46}

 49%|████▉     | 1311/2685 [7:11:08<7:09:29, 18.75s/it]

 49%|████▉     | 1312/2685 [7:11:28<7:17:16, 19.11s/it]

 49%|████▉     | 1313/2685 [7:11:48<7:21:04, 19.29s/it]


 49%|████▉     | 1315/2685 [7:12:29<7:33:54, 19.88s/it]
{'loss': 0.2265, 'learning_rate': 1.0819465245738646e-06, 'rewards/chosen': -1.4720807075500488, 'rewards/rejected': -4.031132221221924, 'rewards/accuracies': 0.875, 'rewards/margins': 2.559051513671875, 'policy_logps/rejected': -403.39251708984375, 'policy_logps/chosen': -314.2505187988281, 'referece_logps/rejected': -363.0811767578125, 'referece_logps/chosen': -299.52972412109375, 'logits/rejected': -0.3618493378162384, 'logits/chosen': -0.32579562067985535, 'epoch': 1.47}


 49%|████▉     | 1317/2685 [7:13:07<7:18:48, 19.25s/it]
{'loss': 0.5881, 'learning_rate': 1.079541506006124e-06, 'rewards/chosen': -1.4967443943023682, 'rewards/rejected': -3.4192216396331787, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9224774837493896, 'policy_logps/rejected': -490.7256164550781, 'policy_logps/chosen': -389.8174133300781, 'referece_logps/rejected': -456.53338623046875, 'referece_logps/chosen': -374.8499755859375, 'logits/rejected': -0.5923337936401367, 'logits/chosen': -0.5804126262664795, 'epoch': 1.47}

 49%|████▉     | 1318/2685 [7:13:26<7:20:20, 19.33s/it]

 49%|████▉     | 1319/2685 [7:13:46<7:23:02, 19.46s/it]

 49%|████▉     | 1320/2685 [7:14:05<7:22:25, 19.45s/it]

 49%|████▉     | 1321/2685 [7:14:24<7:18:56, 19.31s/it]

 49%|████▉     | 1322/2685 [7:14:44<7:17:58, 19.28s/it]

 49%|████▉     | 1323/2685 [7:15:03<7:21:10, 19.43s/it]


 49%|████▉     | 1325/2685 [7:15:43<7:25:43, 19.66s/it]

 49%|████▉     | 1326/2685 [7:16:03<7:25:44, 19.68s/it]
{'loss': 0.3975, 'learning_rate': 1.0687133938526602e-06, 'rewards/chosen': -1.6824390888214111, 'rewards/rejected': -2.848127603530884, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1656885147094727, 'policy_logps/rejected': -460.6318359375, 'policy_logps/chosen': -442.6722106933594, 'referece_logps/rejected': -432.1505126953125, 'referece_logps/chosen': -425.8478088378906, 'logits/rejected': 0.3175565004348755, 'logits/chosen': 0.3502189517021179, 'epoch': 1.48}

 49%|████▉     | 1327/2685 [7:16:25<7:40:47, 20.36s/it]

 49%|████▉     | 1328/2685 [7:16:44<7:30:09, 19.90s/it]


 50%|████▉     | 1330/2685 [7:17:21<7:19:28, 19.46s/it]
{'loss': 0.2809, 'learning_rate': 1.0638982233137977e-06, 'rewards/chosen': -1.3286999464035034, 'rewards/rejected': -4.56169319152832, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2329931259155273, 'policy_logps/rejected': -540.3956909179688, 'policy_logps/chosen': -423.1127624511719, 'referece_logps/rejected': -494.77874755859375, 'referece_logps/chosen': -409.82574462890625, 'logits/rejected': -0.18373191356658936, 'logits/chosen': -0.28293541073799133, 'epoch': 1.49}

 50%|████▉     | 1331/2685 [7:17:43<7:32:48, 20.07s/it]

 50%|████▉     | 1332/2685 [7:18:05<7:45:00, 20.62s/it]

 50%|████▉     | 1333/2685 [7:18:24<7:37:48, 20.32s/it]

 50%|████▉     | 1334/2685 [7:18:46<7:45:28, 20.67s/it]

 50%|████▉     | 1335/2685 [7:19:09<7:58:04, 21.25s/it]


 50%|████▉     | 1337/2685 [7:19:49<7:49:06, 20.88s/it]
{'loss': 0.25, 'learning_rate': 1.05546816165998e-06, 'rewards/chosen': -2.099846839904785, 'rewards/rejected': -4.607643127441406, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5077965259552, 'policy_logps/rejected': -499.02215576171875, 'policy_logps/chosen': -438.443115234375, 'referece_logps/rejected': -452.9456787109375, 'referece_logps/chosen': -417.4447326660156, 'logits/rejected': -0.1285955309867859, 'logits/chosen': -0.21089811623096466, 'epoch': 1.49}


 50%|████▉     | 1339/2685 [7:20:27<7:24:30, 19.81s/it]
{'loss': 0.2592, 'learning_rate': 1.0530588197054786e-06, 'rewards/chosen': -2.4734532833099365, 'rewards/rejected': -3.9321000576019287, 'rewards/accuracies': 0.75, 'rewards/margins': 1.458646535873413, 'policy_logps/rejected': -381.78369140625, 'policy_logps/chosen': -289.84033203125, 'referece_logps/rejected': -342.4627380371094, 'referece_logps/chosen': -265.10577392578125, 'logits/rejected': -0.9935288429260254, 'logits/chosen': -0.9270381927490234, 'epoch': 1.5}

 50%|████▉     | 1340/2685 [7:20:46<7:19:10, 19.59s/it]

 50%|████▉     | 1341/2685 [7:21:09<7:37:25, 20.42s/it]

 50%|████▉     | 1342/2685 [7:21:29<7:34:18, 20.30s/it]


 50%|█████     | 1344/2685 [7:22:08<7:21:34, 19.76s/it]

 50%|█████     | 1345/2685 [7:22:27<7:21:17, 19.76s/it]

 50%|█████     | 1346/2685 [7:22:46<7:09:37, 19.25s/it]

 50%|█████     | 1347/2685 [7:23:08<7:28:33, 20.11s/it]
{'loss': 0.1922, 'learning_rate': 1.043418503064937e-06, 'rewards/chosen': -2.790566921234131, 'rewards/rejected': -6.372829437255859, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5822622776031494, 'policy_logps/rejected': -519.9890747070312, 'policy_logps/chosen': -564.4785766601562, 'referece_logps/rejected': -456.2607116699219, 'referece_logps/chosen': -536.5729370117188, 'logits/rejected': 0.8994372487068176, 'logits/chosen': 0.8800416588783264, 'epoch': 1.51}


 50%|█████     | 1349/2685 [7:23:47<7:26:03, 20.03s/it]
{'loss': 0.2612, 'learning_rate': 1.0410077568574799e-06, 'rewards/chosen': -0.6616789698600769, 'rewards/rejected': -2.528589963912964, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8669109344482422, 'policy_logps/rejected': -369.3266296386719, 'policy_logps/chosen': -360.520263671875, 'referece_logps/rejected': -344.0407409667969, 'referece_logps/chosen': -353.9034423828125, 'logits/rejected': -0.09845972061157227, 'logits/chosen': 0.06014850735664368, 'epoch': 1.51}

 50%|█████     | 1350/2685 [7:24:05<7:08:50, 19.27s/it]

 50%|█████     | 1351/2685 [7:24:24<7:09:48, 19.33s/it]

 50%|█████     | 1352/2685 [7:24:43<7:07:11, 19.23s/it]

 50%|█████     | 1353/2685 [7:25:02<7:02:11, 19.02s/it]

 50%|█████     | 1354/2685 [7:25:19<6:48:53, 18.43s/it]

 50%|█████     | 1355/2685 [7:25:41<7:13:29, 19.56s/it]

 51%|█████     | 1356/2685 [7:26:01<7:14:39, 19.62s/it]

 51%|█████     | 1357/2685 [7:26:22<7:24:48, 20.10s/it]

 51%|█████     | 1358/2685 [7:26:42<7:25:10, 20.13s/it]

 51%|█████     | 1359/2685 [7:27:01<7:16:30, 19.75s/it]

 51%|█████     | 1360/2685 [7:27:21<7:18:53, 19.87s/it]

 51%|█████     | 1361/2685 [7:27:40<7:11:14, 19.54s/it]

 51%|█████     | 1362/2685 [7:28:00<7:13:07, 19.64s/it]

 51%|█████     | 1363/2685 [7:28:16<6:49:17, 18.58s/it]

 51%|█████     | 1364/2685 [7:28:38<7:08:16, 19.45s/it]

 51%|█████     | 1365/2685 [7:28:59<7:21:40, 20.08s/it]

 51%|█████     | 1366/2685 [7:29:16<7:03:33, 19.27s/it]

 51%|█████     | 1367/2685 [7:29:35<7:00:36, 19.15s/it]

 51%|█████     | 1368/2685 [7:29:55<7:00:47, 19.17s/it]

 51%|█████     | 1369/2685 [7:30:16<7:14:41, 19.82s/it]

 51%|█████     | 1370/2685 [7:30:36<7:13:30, 19.78s/it]

 51%|█████     | 1371/2685 [7:30:51<6:44:33, 18.47s/it]

 51%|█████     | 1372/2685 [7:31:12<7:00:40, 19.22s/it]

 51%|█████     | 1373/2685 [7:31:30<6:52:56, 18.88s/it]

 51%|█████     | 1374/2685 [7:31:47<6:40:13, 18.32s/it]

 51%|█████     | 1375/2685 [7:32:06<6:46:14, 18.61s/it]

 51%|█████     | 1376/2685 [7:32:23<6:30:50, 17.91s/it]

 51%|█████▏    | 1377/2685 [7:32:42<6:37:56, 18.25s/it]

 51%|█████▏    | 1378/2685 [7:33:01<6:47:27, 18.71s/it]

 51%|█████▏    | 1379/2685 [7:33:17<6:24:57, 17.69s/it]

 51%|█████▏    | 1380/2685 [7:33:34<6:19:02, 17.43s/it]

 51%|█████▏    | 1381/2685 [7:33:55<6:43:42, 18.58s/it]

 51%|█████▏    | 1382/2685 [7:34:12<6:36:36, 18.26s/it]

 52%|█████▏    | 1383/2685 [7:34:31<6:36:29, 18.27s/it]

 52%|█████▏    | 1384/2685 [7:34:50<6:45:55, 18.72s/it]

 52%|█████▏    | 1385/2685 [7:35:08<6:34:44, 18.22s/it]

 52%|█████▏    | 1386/2685 [7:35:27<6:44:26, 18.68s/it]

 52%|█████▏    | 1387/2685 [7:35:47<6:53:26, 19.11s/it]

 52%|█████▏    | 1388/2685 [7:36:00<6:14:04, 17.30s/it]

 52%|█████▏    | 1389/2685 [7:36:23<6:47:03, 18.85s/it]

 52%|█████▏    | 1390/2685 [7:36:42<6:51:18, 19.06s/it]

 52%|█████▏    | 1391/2685 [7:37:00<6:40:59, 18.59s/it]

 52%|█████▏    | 1392/2685 [7:37:18<6:38:34, 18.50s/it]


 52%|█████▏    | 1394/2685 [7:38:00<7:03:12, 19.67s/it]

 52%|█████▏    | 1395/2685 [7:38:17<6:45:58, 18.88s/it]
{'loss': 0.3846, 'learning_rate': 9.855231202248097e-07, 'rewards/chosen': -0.7248772978782654, 'rewards/rejected': -3.427180528640747, 'rewards/accuracies': 1.0, 'rewards/margins': 2.702303171157837, 'policy_logps/rejected': -351.1138916015625, 'policy_logps/chosen': -220.07644653320312, 'referece_logps/rejected': -316.842041015625, 'referece_logps/chosen': -212.82766723632812, 'logits/rejected': -0.2266848385334015, 'logits/chosen': -0.4019037187099457, 'epoch': 1.56}


 52%|█████▏    | 1397/2685 [7:38:56<6:43:27, 18.79s/it]

 52%|█████▏    | 1398/2685 [7:39:15<6:48:08, 19.03s/it]

 52%|█████▏    | 1399/2685 [7:39:34<6:46:11, 18.95s/it]

 52%|█████▏    | 1400/2685 [7:39:54<6:51:53, 19.23s/it]

 52%|█████▏    | 1401/2685 [7:40:12<6:46:10, 18.98s/it]

 52%|█████▏    | 1402/2685 [7:40:32<6:49:19, 19.14s/it]

 52%|█████▏    | 1403/2685 [7:40:52<6:58:05, 19.57s/it]

 52%|█████▏    | 1404/2685 [7:41:07<6:25:22, 18.05s/it]

 52%|█████▏    | 1405/2685 [7:41:26<6:29:59, 18.28s/it]

 52%|█████▏    | 1406/2685 [7:41:46<6:41:16, 18.82s/it]

 52%|█████▏    | 1407/2685 [7:42:03<6:28:47, 18.25s/it]

 52%|█████▏    | 1408/2685 [7:42:22<6:33:44, 18.50s/it]

 52%|█████▏    | 1409/2685 [7:42:40<6:34:24, 18.55s/it]

 53%|█████▎    | 1410/2685 [7:43:03<6:57:12, 19.63s/it]

 53%|█████▎    | 1411/2685 [7:43:24<7:05:28, 20.04s/it]

 53%|█████▎    | 1412/2685 [7:43:38<6:31:12, 18.44s/it]

 53%|█████▎    | 1413/2685 [7:44:00<6:54:24, 19.55s/it]

 53%|█████▎    | 1414/2685 [7:44:24<7:17:16, 20.64s/it]

 53%|█████▎    | 1415/2685 [7:44:42<6:59:57, 19.84s/it]

 53%|█████▎    | 1416/2685 [7:45:00<6:51:09, 19.44s/it]

 53%|█████▎    | 1417/2685 [7:45:19<6:44:53, 19.16s/it]

 53%|█████▎    | 1418/2685 [7:45:40<6:56:54, 19.74s/it]

 53%|█████▎    | 1419/2685 [7:46:01<7:05:08, 20.15s/it]

 53%|█████▎    | 1420/2685 [7:46:22<7:12:17, 20.50s/it]

 53%|█████▎    | 1421/2685 [7:46:42<7:07:18, 20.28s/it]

 53%|█████▎    | 1422/2685 [7:47:03<7:12:36, 20.55s/it]

 53%|█████▎    | 1423/2685 [7:47:22<7:04:21, 20.18s/it]

 53%|█████▎    | 1424/2685 [7:47:41<6:53:38, 19.68s/it]

 53%|█████▎    | 1425/2685 [7:48:01<6:54:00, 19.71s/it]

 53%|█████▎    | 1426/2685 [7:48:20<6:53:13, 19.69s/it]

 53%|█████▎    | 1427/2685 [7:48:36<6:26:48, 18.45s/it]

 53%|█████▎    | 1428/2685 [7:48:56<6:35:11, 18.86s/it]

 53%|█████▎    | 1429/2685 [7:49:14<6:29:04, 18.59s/it]

 53%|█████▎    | 1430/2685 [7:49:33<6:35:54, 18.93s/it]

 53%|█████▎    | 1431/2685 [7:49:52<6:34:29, 18.87s/it]

 53%|█████▎    | 1432/2685 [7:50:11<6:33:02, 18.82s/it]

 53%|█████▎    | 1433/2685 [7:50:29<6:30:44, 18.73s/it]

 53%|█████▎    | 1434/2685 [7:50:49<6:38:55, 19.13s/it]

 53%|█████▎    | 1435/2685 [7:51:08<6:37:53, 19.10s/it]

 53%|█████▎    | 1436/2685 [7:51:26<6:30:03, 18.74s/it]

 54%|█████▎    | 1437/2685 [7:51:44<6:24:55, 18.51s/it]

 54%|█████▎    | 1438/2685 [7:52:04<6:33:56, 18.95s/it]

 54%|█████▎    | 1439/2685 [7:52:26<6:52:00, 19.84s/it]

 54%|█████▎    | 1440/2685 [7:52:47<7:00:27, 20.26s/it]

 54%|█████▎    | 1441/2685 [7:53:09<7:06:42, 20.58s/it]

 54%|█████▎    | 1442/2685 [7:53:30<7:13:12, 20.91s/it]
{'loss': 0.3274, 'learning_rate': 9.288796139513848e-07, 'rewards/chosen': -1.2131222486495972, 'rewards/rejected': -2.831775665283203, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6186537742614746, 'policy_logps/rejected': -382.73284912109375, 'policy_logps/chosen': -306.93878173828125, 'referece_logps/rejected': -354.41510009765625, 'referece_logps/chosen': -294.8075866699219, 'logits/rejected': -1.0848958492279053, 'logits/chosen': -1.1493405103683472, 'epoch': 1.61}


 54%|█████▍    | 1444/2685 [7:54:10<7:01:46, 20.39s/it]
{'loss': 0.2893, 'learning_rate': 9.264730358234326e-07, 'rewards/chosen': -1.1375560760498047, 'rewards/rejected': -3.218082904815674, 'rewards/accuracies': 0.75, 'rewards/margins': 2.08052659034729, 'policy_logps/rejected': -542.2166137695312, 'policy_logps/chosen': -460.3546142578125, 'referece_logps/rejected': -510.0357360839844, 'referece_logps/chosen': -448.9790954589844, 'logits/rejected': -0.2091488242149353, 'logits/chosen': -0.19195586442947388, 'epoch': 1.61}

 54%|█████▍    | 1445/2685 [7:54:30<6:57:29, 20.20s/it]


 54%|█████▍    | 1447/2685 [7:55:09<6:47:03, 19.73s/it]

 54%|█████▍    | 1448/2685 [7:55:27<6:36:54, 19.25s/it]

 54%|█████▍    | 1449/2685 [7:55:44<6:22:40, 18.58s/it]

 54%|█████▍    | 1450/2685 [7:56:03<6:27:51, 18.84s/it]

 54%|█████▍    | 1451/2685 [7:56:25<6:43:03, 19.60s/it]

 54%|█████▍    | 1452/2685 [7:56:43<6:34:34, 19.20s/it]

 54%|█████▍    | 1453/2685 [7:57:01<6:29:35, 18.97s/it]

 54%|█████▍    | 1454/2685 [7:57:23<6:45:42, 19.77s/it]
{'loss': 0.3664, 'learning_rate': 9.144468465092523e-07, 'rewards/chosen': -0.7734991312026978, 'rewards/rejected': -2.9241034984588623, 'rewards/accuracies': 0.875, 'rewards/margins': 2.150604486465454, 'policy_logps/rejected': -428.7945556640625, 'policy_logps/chosen': -421.32110595703125, 'referece_logps/rejected': -399.55352783203125, 'referece_logps/chosen': -413.5860900878906, 'logits/rejected': 0.553890585899353, 'logits/chosen': 0.48389869928359985, 'epoch': 1.62}

 54%|█████▍    | 1455/2685 [7:57:42<6:43:53, 19.70s/it]

 54%|█████▍    | 1456/2685 [7:58:02<6:44:54, 19.77s/it]


 54%|█████▍    | 1458/2685 [7:58:41<6:38:11, 19.47s/it]
{'loss': 0.256, 'learning_rate': 9.096397594542978e-07, 'rewards/chosen': -0.9584425687789917, 'rewards/rejected': -3.1065568923950195, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1481142044067383, 'policy_logps/rejected': -447.21746826171875, 'policy_logps/chosen': -395.34423828125, 'referece_logps/rejected': -416.15191650390625, 'referece_logps/chosen': -385.7597961425781, 'logits/rejected': -0.284970760345459, 'logits/chosen': -0.31740498542785645, 'epoch': 1.63}


 54%|█████▍    | 1460/2685 [7:59:21<6:45:48, 19.88s/it]

 54%|█████▍    | 1461/2685 [7:59:42<6:48:37, 20.03s/it]

 54%|█████▍    | 1462/2685 [8:00:01<6:46:42, 19.95s/it]

 54%|█████▍    | 1463/2685 [8:00:21<6:46:24, 19.95s/it]

 55%|█████▍    | 1464/2685 [8:00:40<6:37:08, 19.52s/it]

 55%|█████▍    | 1465/2685 [8:00:58<6:28:42, 19.12s/it]
{'loss': 0.3135, 'learning_rate': 9.012324879740731e-07, 'rewards/chosen': -1.0119494199752808, 'rewards/rejected': -3.4368464946746826, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4248971939086914, 'policy_logps/rejected': -488.967529296875, 'policy_logps/chosen': -490.5950927734375, 'referece_logps/rejected': -454.59906005859375, 'referece_logps/chosen': -480.4756164550781, 'logits/rejected': -0.5319476127624512, 'logits/chosen': -0.6069285273551941, 'epoch': 1.64}


 55%|█████▍    | 1467/2685 [8:01:40<6:47:07, 20.06s/it]
{'loss': 0.3229, 'learning_rate': 8.988316780125679e-07, 'rewards/chosen': -0.15929469466209412, 'rewards/rejected': -2.911515235900879, 'rewards/accuracies': 0.875, 'rewards/margins': 2.752220869064331, 'policy_logps/rejected': -378.53765869140625, 'policy_logps/chosen': -320.9044494628906, 'referece_logps/rejected': -349.4225158691406, 'referece_logps/chosen': -319.3114929199219, 'logits/rejected': -0.0696234256029129, 'logits/chosen': -0.06056591868400574, 'epoch': 1.64}

 55%|█████▍    | 1468/2685 [8:02:02<7:01:15, 20.77s/it]


 55%|█████▍    | 1470/2685 [8:02:41<6:50:28, 20.27s/it]

 55%|█████▍    | 1471/2685 [8:02:58<6:29:51, 19.27s/it]
{'loss': 0.3123, 'learning_rate': 8.940318390915572e-07, 'rewards/chosen': -0.055583205074071884, 'rewards/rejected': -2.9571330547332764, 'rewards/accuracies': 0.875, 'rewards/margins': 2.901549816131592, 'policy_logps/rejected': -247.54335021972656, 'policy_logps/chosen': -251.06707763671875, 'referece_logps/rejected': -217.9720001220703, 'referece_logps/chosen': -250.51124572753906, 'logits/rejected': -0.6902516484260559, 'logits/chosen': -0.6601901650428772, 'epoch': 1.64}


 55%|█████▍    | 1473/2685 [8:03:37<6:37:00, 19.65s/it]

 55%|█████▍    | 1474/2685 [8:03:55<6:27:28, 19.20s/it]
{'loss': 0.2735, 'learning_rate': 8.904335732925349e-07, 'rewards/chosen': -0.3622220456600189, 'rewards/rejected': -1.815537452697754, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4533154964447021, 'policy_logps/rejected': -224.40719604492188, 'policy_logps/chosen': -210.45851135253906, 'referece_logps/rejected': -206.2518310546875, 'referece_logps/chosen': -206.83627319335938, 'logits/rejected': -0.4918416738510132, 'logits/chosen': -0.39503082633018494, 'epoch': 1.65}


 55%|█████▍    | 1476/2685 [8:04:36<6:37:38, 19.73s/it]

 55%|█████▌    | 1477/2685 [8:04:51<6:09:37, 18.36s/it]
{'loss': 0.3839, 'learning_rate': 8.868367427757792e-07, 'rewards/chosen': -1.7745227813720703, 'rewards/rejected': -3.862733840942383, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0882110595703125, 'policy_logps/rejected': -373.37603759765625, 'policy_logps/chosen': -355.2520446777344, 'referece_logps/rejected': -334.74871826171875, 'referece_logps/chosen': -337.5068054199219, 'logits/rejected': -0.3021388649940491, 'logits/chosen': -0.26758238673210144, 'epoch': 1.65}

 55%|█████▌    | 1478/2685 [8:05:08<6:01:52, 17.99s/it]

 55%|█████▌    | 1479/2685 [8:05:29<6:14:42, 18.64s/it]


 55%|█████▌    | 1481/2685 [8:06:06<6:15:13, 18.70s/it]

 55%|█████▌    | 1482/2685 [8:06:24<6:07:47, 18.34s/it]
{'loss': 0.2092, 'learning_rate': 8.808453427279673e-07, 'rewards/chosen': -0.5200155377388, 'rewards/rejected': -5.213350772857666, 'rewards/accuracies': 1.0, 'rewards/margins': 4.69333553314209, 'policy_logps/rejected': -634.0284423828125, 'policy_logps/chosen': -471.3072509765625, 'referece_logps/rejected': -581.8948974609375, 'referece_logps/chosen': -466.107177734375, 'logits/rejected': 0.1121739074587822, 'logits/chosen': -0.07112468779087067, 'epoch': 1.66}


 55%|█████▌    | 1484/2685 [8:07:04<6:26:44, 19.32s/it]

 55%|█████▌    | 1485/2685 [8:07:24<6:31:35, 19.58s/it]

 55%|█████▌    | 1486/2685 [8:07:46<6:45:49, 20.31s/it]

 55%|█████▌    | 1487/2685 [8:08:04<6:31:53, 19.63s/it]
{'loss': 0.3144, 'learning_rate': 8.748582784627654e-07, 'rewards/chosen': -1.1623311042785645, 'rewards/rejected': -2.1155762672424316, 'rewards/accuracies': 0.875, 'rewards/margins': 0.9532450437545776, 'policy_logps/rejected': -286.1491394042969, 'policy_logps/chosen': -262.53521728515625, 'referece_logps/rejected': -264.993408203125, 'referece_logps/chosen': -250.91188049316406, 'logits/rejected': -0.1313873678445816, 'logits/chosen': -0.1885114461183548, 'epoch': 1.66}

 55%|█████▌    | 1488/2685 [8:08:25<6:38:01, 19.95s/it]

 55%|█████▌    | 1489/2685 [8:08:37<5:53:20, 17.73s/it]


 56%|█████▌    | 1491/2685 [8:09:10<5:40:00, 17.09s/it]
{'loss': 0.2345, 'learning_rate': 8.700718952191124e-07, 'rewards/chosen': -0.4801050126552582, 'rewards/rejected': -2.348451614379883, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8683466911315918, 'policy_logps/rejected': -341.3028564453125, 'policy_logps/chosen': -321.2558898925781, 'referece_logps/rejected': -317.8183898925781, 'referece_logps/chosen': -316.45489501953125, 'logits/rejected': 0.32883715629577637, 'logits/chosen': 0.3302960693836212, 'epoch': 1.67}


 56%|█████▌    | 1493/2685 [8:09:48<6:02:56, 18.27s/it]

 56%|█████▌    | 1494/2685 [8:10:08<6:10:17, 18.65s/it]
{'loss': 0.2538, 'learning_rate': 8.664840873735565e-07, 'rewards/chosen': -1.8188817501068115, 'rewards/rejected': -5.336126327514648, 'rewards/accuracies': 1.0, 'rewards/margins': 3.517244815826416, 'policy_logps/rejected': -498.5018005371094, 'policy_logps/chosen': -539.4573364257812, 'referece_logps/rejected': -445.1405334472656, 'referece_logps/chosen': -521.2684936523438, 'logits/rejected': 0.5196594595909119, 'logits/chosen': 0.4772007465362549, 'epoch': 1.67}
[2024-03-29 06:17:26,692] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 56%|█████▌    | 1496/2685 [8:10:44<6:12:17, 18.79s/it]

 56%|█████▌    | 1497/2685 [8:11:02<6:08:04, 18.59s/it]

 56%|█████▌    | 1498/2685 [8:11:22<6:13:09, 18.86s/it]

 56%|█████▌    | 1499/2685 [8:11:42<6:22:17, 19.34s/it]
{'loss': 0.3741, 'learning_rate': 8.605083175238872e-07, 'rewards/chosen': -0.9671584963798523, 'rewards/rejected': -3.5262489318847656, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5590906143188477, 'policy_logps/rejected': -515.0097045898438, 'policy_logps/chosen': -458.7073974609375, 'referece_logps/rejected': -479.7471923828125, 'referece_logps/chosen': -449.03582763671875, 'logits/rejected': -0.9256526827812195, 'logits/chosen': -0.8768168091773987, 'epoch': 1.67}

 56%|█████▌    | 1500/2685 [8:12:05<6:41:14, 20.32s/it]


 56%|█████▌    | 1502/2685 [8:13:05<8:00:31, 24.37s/it]
{'loss': 0.3949, 'learning_rate': 8.569252780907861e-07, 'rewards/chosen': -1.4945523738861084, 'rewards/rejected': -3.7741732597351074, 'rewards/accuracies': 1.0, 'rewards/margins': 2.279621124267578, 'policy_logps/rejected': -292.27996826171875, 'policy_logps/chosen': -225.18807983398438, 'referece_logps/rejected': -254.5382537841797, 'referece_logps/chosen': -210.24256896972656, 'logits/rejected': -0.15135055780410767, 'logits/chosen': 0.027711857110261917, 'epoch': 1.68}
[2024-03-29 06:20:28,609] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 56%|█████▌    | 1504/2685 [8:13:46<7:22:09, 22.46s/it]

 56%|█████▌    | 1505/2685 [8:14:03<6:49:12, 20.81s/it]
{'loss': 0.3027, 'learning_rate': 8.53344112887021e-07, 'rewards/chosen': -0.46759387850761414, 'rewards/rejected': -3.778022527694702, 'rewards/accuracies': 0.875, 'rewards/margins': 3.310429096221924, 'policy_logps/rejected': -427.65826416015625, 'policy_logps/chosen': -458.7162170410156, 'referece_logps/rejected': -389.8780517578125, 'referece_logps/chosen': -454.040283203125, 'logits/rejected': -0.4377681612968445, 'logits/chosen': -0.5120240449905396, 'epoch': 1.68}

 56%|█████▌    | 1506/2685 [8:14:17<6:12:21, 18.95s/it]


 56%|█████▌    | 1508/2685 [8:14:59<6:31:12, 19.94s/it]
{'loss': 0.3615, 'learning_rate': 8.497648688246173e-07, 'rewards/chosen': -1.3393213748931885, 'rewards/rejected': -3.72060489654541, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3812835216522217, 'policy_logps/rejected': -331.4779052734375, 'policy_logps/chosen': -306.5456848144531, 'referece_logps/rejected': -294.2718505859375, 'referece_logps/chosen': -293.1524658203125, 'logits/rejected': -0.21380159258842468, 'logits/chosen': -0.1531035304069519, 'epoch': 1.68}


 56%|█████▌    | 1510/2685 [8:15:42<6:46:17, 20.75s/it]

 56%|█████▋    | 1511/2685 [8:16:02<6:40:36, 20.47s/it]

 56%|█████▋    | 1512/2685 [8:16:22<6:38:09, 20.37s/it]
{'loss': 0.2449, 'learning_rate': 8.449956128855091e-07, 'rewards/chosen': -1.3842836618423462, 'rewards/rejected': -5.374330520629883, 'rewards/accuracies': 1.0, 'rewards/margins': 3.990046739578247, 'policy_logps/rejected': -515.903076171875, 'policy_logps/chosen': -508.3302001953125, 'referece_logps/rejected': -462.15972900390625, 'referece_logps/chosen': -494.48736572265625, 'logits/rejected': 0.1808682531118393, 'logits/chosen': 0.3148031234741211, 'epoch': 1.69}

 56%|█████▋    | 1513/2685 [8:16:42<6:33:22, 20.14s/it]


 56%|█████▋    | 1515/2685 [8:17:21<6:28:00, 19.90s/it]

 56%|█████▋    | 1516/2685 [8:17:38<6:12:41, 19.13s/it]

 56%|█████▋    | 1517/2685 [8:17:59<6:21:36, 19.60s/it]
{'loss': 0.4253, 'learning_rate': 8.390391322246398e-07, 'rewards/chosen': -2.521632432937622, 'rewards/rejected': -2.5777993202209473, 'rewards/accuracies': 0.625, 'rewards/margins': 0.05616702139377594, 'policy_logps/rejected': -552.79296875, 'policy_logps/chosen': -515.7310791015625, 'referece_logps/rejected': -527.0150146484375, 'referece_logps/chosen': -490.5147399902344, 'logits/rejected': 0.7680305242538452, 'logits/chosen': 0.7218372225761414, 'epoch': 1.69}

 57%|█████▋    | 1518/2685 [8:18:19<6:24:18, 19.76s/it]

 57%|█████▋    | 1519/2685 [8:18:38<6:14:48, 19.29s/it]


 57%|█████▋    | 1521/2685 [8:19:15<6:06:12, 18.88s/it]

 57%|█████▋    | 1522/2685 [8:19:37<6:21:46, 19.70s/it]
[2024-03-29 06:26:40,307] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 1523/2685 [8:19:54<6:10:01, 19.11s/it]
[2024-03-29 06:26:58,036] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2475, 'learning_rate': 8.31899105757852e-07, 'rewards/chosen': -1.699237585067749, 'rewards/rejected': -3.7463624477386475, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0471251010894775, 'policy_logps/rejected': -409.2696533203125, 'policy_logps/chosen': -386.43682861328125, 'referece_logps/rejected': -371.8059997558594, 'referece_logps/chosen': -369.4444580078125, 'logits/rejected': -0.01588013768196106, 'logits/chosen': -0.015668585896492004, 'epoch': 1.7}


 57%|█████▋    | 1525/2685 [8:20:36<6:27:42, 20.05s/it]

 57%|█████▋    | 1526/2685 [8:20:53<6:07:36, 19.03s/it]

 57%|█████▋    | 1527/2685 [8:21:14<6:20:18, 19.70s/it]
[2024-03-29 06:28:17,958] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 1528/2685 [8:21:37<6:39:14, 20.70s/it]
[2024-03-29 06:28:40,994] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 57%|█████▋    | 1529/2685 [8:21:58<6:40:10, 20.77s/it]

 57%|█████▋    | 1530/2685 [8:22:13<6:04:31, 18.94s/it]

 57%|█████▋    | 1531/2685 [8:22:32<6:07:12, 19.09s/it]
{'loss': 0.289, 'learning_rate': 8.223928367502729e-07, 'rewards/chosen': -2.193685293197632, 'rewards/rejected': -3.9714596271514893, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7777743339538574, 'policy_logps/rejected': -548.8614501953125, 'policy_logps/chosen': -453.03497314453125, 'referece_logps/rejected': -509.1468505859375, 'referece_logps/chosen': -431.09808349609375, 'logits/rejected': 0.014714867807924747, 'logits/chosen': -0.06807208806276321, 'epoch': 1.71}


 57%|█████▋    | 1533/2685 [8:23:09<5:58:11, 18.66s/it]

 57%|█████▋    | 1534/2685 [8:23:28<6:04:31, 19.00s/it]

 57%|█████▋    | 1535/2685 [8:23:49<6:10:58, 19.36s/it]

 57%|█████▋    | 1536/2685 [8:24:09<6:14:38, 19.56s/it]
{'loss': 0.3725, 'learning_rate': 8.164597640007298e-07, 'rewards/chosen': -1.915899395942688, 'rewards/rejected': -4.6955389976501465, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7796390056610107, 'policy_logps/rejected': -548.7091064453125, 'policy_logps/chosen': -497.02886962890625, 'referece_logps/rejected': -501.75372314453125, 'referece_logps/chosen': -477.8699035644531, 'logits/rejected': 0.275937020778656, 'logits/chosen': 0.3980266749858856, 'epoch': 1.72}


 57%|█████▋    | 1538/2685 [8:24:46<6:10:22, 19.37s/it]

 57%|█████▋    | 1539/2685 [8:25:07<6:13:54, 19.58s/it]

 57%|█████▋    | 1540/2685 [8:25:22<5:47:13, 18.20s/it]
{'loss': 0.4136, 'learning_rate': 8.117181040641891e-07, 'rewards/chosen': -0.9326463937759399, 'rewards/rejected': -3.4536356925964355, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5209896564483643, 'policy_logps/rejected': -324.0191650390625, 'policy_logps/chosen': -285.5628356933594, 'referece_logps/rejected': -289.4827880859375, 'referece_logps/chosen': -276.2364196777344, 'logits/rejected': -0.552635669708252, 'logits/chosen': -0.5401865243911743, 'epoch': 1.72}


 57%|█████▋    | 1542/2685 [8:26:01<6:01:25, 18.97s/it]

 57%|█████▋    | 1543/2685 [8:26:23<6:19:08, 19.92s/it]
[2024-03-29 06:33:27,057] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 58%|█████▊    | 1544/2685 [8:26:43<6:15:52, 19.77s/it]
[2024-03-29 06:33:46,464] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2479, 'learning_rate': 8.069808288842268e-07, 'rewards/chosen': -1.2741241455078125, 'rewards/rejected': -4.372623443603516, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0984995365142822, 'policy_logps/rejected': -454.2084045410156, 'policy_logps/chosen': -371.5181884765625, 'referece_logps/rejected': -410.482177734375, 'referece_logps/chosen': -358.7769775390625, 'logits/rejected': -0.6590307354927063, 'logits/chosen': -0.5587309002876282, 'epoch': 1.73}


 58%|█████▊    | 1546/2685 [8:27:27<6:38:37, 21.00s/it]
{'loss': 0.3866, 'learning_rate': 8.046138700579171e-07, 'rewards/chosen': -1.6113673448562622, 'rewards/rejected': -2.060778856277466, 'rewards/accuracies': 0.5, 'rewards/margins': 0.44941166043281555, 'policy_logps/rejected': -337.21832275390625, 'policy_logps/chosen': -417.362548828125, 'referece_logps/rejected': -316.6105041503906, 'referece_logps/chosen': -401.2488708496094, 'logits/rejected': 0.30096203088760376, 'logits/chosen': 0.43627727031707764, 'epoch': 1.73}
[2024-03-29 06:34:51,592] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 58%|█████▊    | 1548/2685 [8:28:08<6:30:07, 20.59s/it]
{'loss': 0.3307, 'learning_rate': 8.022480487837086e-07, 'rewards/chosen': -0.5254507064819336, 'rewards/rejected': -1.9610689878463745, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4356180429458618, 'policy_logps/rejected': -194.77435302734375, 'policy_logps/chosen': -221.80545043945312, 'referece_logps/rejected': -175.16366577148438, 'referece_logps/chosen': -216.55093383789062, 'logits/rejected': -0.5840389728546143, 'logits/chosen': -0.6278589963912964, 'epoch': 1.73}


 58%|█████▊    | 1550/2685 [8:28:45<6:09:58, 19.56s/it]
{'loss': 0.2906, 'learning_rate': 7.998833788355831e-07, 'rewards/chosen': -0.6641489267349243, 'rewards/rejected': -4.670536041259766, 'rewards/accuracies': 0.875, 'rewards/margins': 4.006386756896973, 'policy_logps/rejected': -623.538330078125, 'policy_logps/chosen': -422.4476623535156, 'referece_logps/rejected': -576.8330078125, 'referece_logps/chosen': -415.80615234375, 'logits/rejected': -0.03350937366485596, 'logits/chosen': 0.019850850105285645, 'epoch': 1.73}

 58%|█████▊    | 1551/2685 [8:29:00<5:43:07, 18.15s/it]

 58%|█████▊    | 1552/2685 [8:29:16<5:30:33, 17.51s/it]


 58%|█████▊    | 1554/2685 [8:30:00<6:09:50, 19.62s/it]
{'loss': 0.3508, 'learning_rate': 7.951575479799117e-07, 'rewards/chosen': -1.1244304180145264, 'rewards/rejected': -3.126108169555664, 'rewards/accuracies': 0.625, 'rewards/margins': 2.0016777515411377, 'policy_logps/rejected': -269.11907958984375, 'policy_logps/chosen': -248.8949737548828, 'referece_logps/rejected': -237.8579559326172, 'referece_logps/chosen': -237.6507110595703, 'logits/rejected': -0.3988966941833496, 'logits/chosen': -0.5453038811683655, 'epoch': 1.74}

 58%|█████▊    | 1555/2685 [8:30:13<5:30:07, 17.53s/it]

 58%|█████▊    | 1556/2685 [8:30:29<5:21:35, 17.09s/it]


 58%|█████▊    | 1558/2685 [8:31:02<5:20:36, 17.07s/it]
{'loss': 0.3611, 'learning_rate': 7.904364875472512e-07, 'rewards/chosen': -1.780375599861145, 'rewards/rejected': -2.8422935009002686, 'rewards/accuracies': 0.75, 'rewards/margins': 1.061918020248413, 'policy_logps/rejected': -239.67697143554688, 'policy_logps/chosen': -229.8933563232422, 'referece_logps/rejected': -211.25404357910156, 'referece_logps/chosen': -212.089599609375, 'logits/rejected': -0.5858848094940186, 'logits/chosen': -0.681858241558075, 'epoch': 1.74}

 58%|█████▊    | 1559/2685 [8:31:24<5:48:21, 18.56s/it]


 58%|█████▊    | 1561/2685 [8:32:02<5:48:57, 18.63s/it]

 58%|█████▊    | 1562/2685 [8:32:22<5:58:43, 19.17s/it]

 58%|█████▊    | 1563/2685 [8:32:42<6:03:23, 19.43s/it]

 58%|█████▊    | 1564/2685 [8:33:02<6:06:02, 19.59s/it]
{'loss': 0.2935, 'learning_rate': 7.833640819156411e-07, 'rewards/chosen': -2.2075231075286865, 'rewards/rejected': -3.1636691093444824, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9561460614204407, 'policy_logps/rejected': -426.0467834472656, 'policy_logps/chosen': -405.5044860839844, 'referece_logps/rejected': -394.41015625, 'referece_logps/chosen': -383.42926025390625, 'logits/rejected': -0.8328483700752258, 'logits/chosen': -0.8733268976211548, 'epoch': 1.75}

 58%|█████▊    | 1565/2685 [8:33:21<6:01:08, 19.35s/it]

 58%|█████▊    | 1566/2685 [8:33:41<6:04:38, 19.55s/it]

 58%|█████▊    | 1567/2685 [8:33:57<5:46:28, 18.59s/it]

 58%|█████▊    | 1568/2685 [8:34:17<5:51:47, 18.90s/it]


 58%|█████▊    | 1570/2685 [8:34:56<6:00:23, 19.39s/it]
{'loss': 0.3595, 'learning_rate': 7.763030276690376e-07, 'rewards/chosen': -0.9131954908370972, 'rewards/rejected': -3.143695831298828, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2305002212524414, 'policy_logps/rejected': -380.2023620605469, 'policy_logps/chosen': -369.8990478515625, 'referece_logps/rejected': -348.765380859375, 'referece_logps/chosen': -360.7671203613281, 'logits/rejected': 0.2109881341457367, 'logits/chosen': 0.1651187241077423, 'epoch': 1.75}

 59%|█████▊    | 1571/2685 [8:35:15<6:02:52, 19.54s/it]


 59%|█████▊    | 1573/2685 [8:35:50<5:40:31, 18.37s/it]

 59%|█████▊    | 1574/2685 [8:36:10<5:49:11, 18.86s/it]

 59%|█████▊    | 1575/2685 [8:36:30<5:53:12, 19.09s/it]
{'loss': 0.297, 'learning_rate': 7.704277539403303e-07, 'rewards/chosen': -0.7187977433204651, 'rewards/rejected': -2.816598892211914, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0978009700775146, 'policy_logps/rejected': -226.37059020996094, 'policy_logps/chosen': -288.0827941894531, 'referece_logps/rejected': -198.20458984375, 'referece_logps/chosen': -280.894775390625, 'logits/rejected': -0.34066036343574524, 'logits/chosen': -0.26405930519104004, 'epoch': 1.76}

 59%|█████▊    | 1576/2685 [8:36:53<6:12:45, 20.17s/it]

 59%|█████▊    | 1577/2685 [8:37:11<6:01:07, 19.56s/it]

 59%|█████▉    | 1578/2685 [8:37:32<6:08:53, 19.99s/it]

 59%|█████▉    | 1579/2685 [8:37:49<5:54:57, 19.26s/it]

 59%|█████▉    | 1580/2685 [8:38:05<5:35:55, 18.24s/it]


 59%|█████▉    | 1582/2685 [8:38:40<5:25:43, 17.72s/it]
{'loss': 0.3587, 'learning_rate': 7.622164526696905e-07, 'rewards/chosen': -1.3793625831604004, 'rewards/rejected': -3.293424367904663, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9140619039535522, 'policy_logps/rejected': -420.4178466796875, 'policy_logps/chosen': -366.5220947265625, 'referece_logps/rejected': -387.4835510253906, 'referece_logps/chosen': -352.7284851074219, 'logits/rejected': -0.35906782746315, 'logits/chosen': -0.47614219784736633, 'epoch': 1.77}

 59%|█████▉    | 1583/2685 [8:38:58<5:26:23, 17.77s/it]

 59%|█████▉    | 1584/2685 [8:39:17<5:36:24, 18.33s/it]
[2024-03-29 06:46:44,356] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 59%|█████▉    | 1586/2685 [8:40:00<6:00:41, 19.69s/it]

 59%|█████▉    | 1587/2685 [8:40:22<6:12:31, 20.36s/it]
[2024-03-29 06:47:25,627] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3039, 'learning_rate': 7.56361586266478e-07, 'rewards/chosen': -1.1304851770401, 'rewards/rejected': -3.980534076690674, 'rewards/accuracies': 0.75, 'rewards/margins': 2.850048780441284, 'policy_logps/rejected': -507.69281005859375, 'policy_logps/chosen': -448.62176513671875, 'referece_logps/rejected': -467.8874816894531, 'referece_logps/chosen': -437.3169250488281, 'logits/rejected': 0.46064433455467224, 'logits/chosen': 0.4830702543258667, 'epoch': 1.77}
[2024-03-29 06:47:42,283] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 59%|█████▉    | 1589/2685 [8:40:58<5:54:13, 19.39s/it]
{'loss': 0.3455, 'learning_rate': 7.540221101196918e-07, 'rewards/chosen': -1.0939725637435913, 'rewards/rejected': -2.8598074913024902, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7658350467681885, 'policy_logps/rejected': -251.28042602539062, 'policy_logps/chosen': -180.62213134765625, 'referece_logps/rejected': -222.68234252929688, 'referece_logps/chosen': -169.68240356445312, 'logits/rejected': -0.8476791381835938, 'logits/chosen': -0.8950104117393494, 'epoch': 1.78}

 59%|█████▉    | 1590/2685 [8:41:15<5:40:18, 18.65s/it]

 59%|█████▉    | 1591/2685 [8:41:35<5:48:14, 19.10s/it]


 59%|█████▉    | 1593/2685 [8:42:18<6:12:08, 20.45s/it]
[2024-03-29 06:49:22,035] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2896, 'learning_rate': 7.493474677412793e-07, 'rewards/chosen': -1.4512004852294922, 'rewards/rejected': -3.101435899734497, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6502354145050049, 'policy_logps/rejected': -377.79718017578125, 'policy_logps/chosen': -284.60772705078125, 'referece_logps/rejected': -346.7828063964844, 'referece_logps/chosen': -270.095703125, 'logits/rejected': -0.15661156177520752, 'logits/chosen': 0.010374154895544052, 'epoch': 1.78}
[2024-03-29 06:49:45,120] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 59%|█████▉    | 1595/2685 [8:43:01<6:13:54, 20.58s/it]
[2024-03-29 06:50:04,170] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 59%|█████▉    | 1596/2685 [8:43:20<6:08:47, 20.32s/it]

 59%|█████▉    | 1597/2685 [8:43:42<6:18:32, 20.88s/it]

 60%|█████▉    | 1598/2685 [8:43:59<5:52:33, 19.46s/it]

 60%|█████▉    | 1599/2685 [8:44:17<5:45:01, 19.06s/it]

 60%|█████▉    | 1600/2685 [8:44:36<5:47:57, 19.24s/it]
{'loss': 0.3215, 'learning_rate': 7.411809548974791e-07, 'rewards/chosen': -0.4088134765625, 'rewards/rejected': -2.3057711124420166, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8969576358795166, 'policy_logps/rejected': -345.2327575683594, 'policy_logps/chosen': -350.1040344238281, 'referece_logps/rejected': -322.1750183105469, 'referece_logps/chosen': -346.0158996582031, 'logits/rejected': 0.024730101227760315, 'logits/chosen': 0.01960752159357071, 'epoch': 1.79}

 60%|█████▉    | 1601/2685 [8:44:55<5:45:16, 19.11s/it]

 60%|█████▉    | 1602/2685 [8:45:16<5:51:42, 19.49s/it]


 60%|█████▉    | 1604/2685 [8:45:56<6:01:06, 20.04s/it]
{'loss': 0.2166, 'learning_rate': 7.365226265277907e-07, 'rewards/chosen': -0.5482261180877686, 'rewards/rejected': -3.3729043006896973, 'rewards/accuracies': 1.0, 'rewards/margins': 2.824678421020508, 'policy_logps/rejected': -348.09100341796875, 'policy_logps/chosen': -305.86419677734375, 'referece_logps/rejected': -314.36199951171875, 'referece_logps/chosen': -300.3819580078125, 'logits/rejected': -0.6667846441268921, 'logits/chosen': -0.7252794504165649, 'epoch': 1.79}

 60%|█████▉    | 1605/2685 [8:46:16<5:58:18, 19.91s/it]
[2024-03-29 06:53:41,626] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 60%|█████▉    | 1607/2685 [8:46:53<5:37:05, 18.76s/it]
[2024-03-29 06:53:56,229] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3373, 'learning_rate': 7.330329010258482e-07, 'rewards/chosen': -0.6018100380897522, 'rewards/rejected': -2.8617947101593018, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2599847316741943, 'policy_logps/rejected': -460.368408203125, 'policy_logps/chosen': -353.3091125488281, 'referece_logps/rejected': -431.7503967285156, 'referece_logps/chosen': -347.291015625, 'logits/rejected': -0.7840668559074402, 'logits/chosen': -0.8267945051193237, 'epoch': 1.8}

 60%|█████▉    | 1608/2685 [8:47:13<5:45:56, 19.27s/it]
[2024-03-29 06:54:37,854] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 60%|█████▉    | 1610/2685 [8:47:53<5:49:04, 19.48s/it]
[2024-03-29 06:54:56,506] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.3343, 'learning_rate': 7.29546672700501e-07, 'rewards/chosen': -1.5667428970336914, 'rewards/rejected': -4.662142753601074, 'rewards/accuracies': 1.0, 'rewards/margins': 3.095400094985962, 'policy_logps/rejected': -508.36358642578125, 'policy_logps/chosen': -382.58233642578125, 'referece_logps/rejected': -461.7421569824219, 'referece_logps/chosen': -366.9148864746094, 'logits/rejected': -0.0677567720413208, 'logits/chosen': -0.17519882321357727, 'epoch': 1.8}

 60%|██████    | 1611/2685 [8:48:14<5:55:47, 19.88s/it]


 60%|██████    | 1613/2685 [8:48:49<5:31:54, 18.58s/it]
{'loss': 0.3777, 'learning_rate': 7.260639872201328e-07, 'rewards/chosen': -1.1806674003601074, 'rewards/rejected': -3.481548547744751, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3008811473846436, 'policy_logps/rejected': -286.12896728515625, 'policy_logps/chosen': -284.3802795410156, 'referece_logps/rejected': -251.3134765625, 'referece_logps/chosen': -272.5736389160156, 'logits/rejected': -0.5561354756355286, 'logits/chosen': -0.42845892906188965, 'epoch': 1.8}
[2024-03-29 06:56:13,774] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 60%|██████    | 1614/2685 [8:49:10<5:45:12, 19.34s/it]


 60%|██████    | 1616/2685 [8:49:49<5:45:06, 19.37s/it]
{'loss': 0.3329, 'learning_rate': 7.225848902067175e-07, 'rewards/chosen': -0.9770218133926392, 'rewards/rejected': -2.3001043796539307, 'rewards/accuracies': 0.75, 'rewards/margins': 1.323082447052002, 'policy_logps/rejected': -440.21734619140625, 'policy_logps/chosen': -280.7250671386719, 'referece_logps/rejected': -417.2162780761719, 'referece_logps/chosen': -270.9548645019531, 'logits/rejected': -0.2879906892776489, 'logits/chosen': -0.36970946192741394, 'epoch': 1.81}

 60%|██████    | 1617/2685 [8:50:08<5:45:16, 19.40s/it]

 60%|██████    | 1618/2685 [8:50:28<5:44:01, 19.35s/it]
[2024-03-29 06:57:54,161] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 60%|██████    | 1619/2685 [8:50:51<6:02:34, 20.41s/it]

 60%|██████    | 1620/2685 [8:51:10<5:59:35, 20.26s/it]

 60%|██████    | 1621/2685 [8:51:30<5:57:29, 20.16s/it]

 60%|██████    | 1622/2685 [8:51:52<6:03:16, 20.51s/it]
[2024-03-29 06:59:17,787] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 60%|██████    | 1623/2685 [8:52:14<6:13:24, 21.10s/it]

 60%|██████    | 1624/2685 [8:52:34<6:05:05, 20.65s/it]

 61%|██████    | 1625/2685 [8:52:55<6:06:15, 20.73s/it]
[2024-03-29 07:00:17,359] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 61%|██████    | 1626/2685 [8:53:14<5:56:59, 20.23s/it]


 61%|██████    | 1628/2685 [8:53:53<5:49:30, 19.84s/it]

 61%|██████    | 1629/2685 [8:54:13<5:49:03, 19.83s/it]
{'loss': 0.222, 'learning_rate': 7.075513807085299e-07, 'rewards/chosen': -1.6122702360153198, 'rewards/rejected': -4.582346439361572, 'rewards/accuracies': 0.875, 'rewards/margins': 2.970076084136963, 'policy_logps/rejected': -453.01806640625, 'policy_logps/chosen': -397.2070617675781, 'referece_logps/rejected': -407.1945495605469, 'referece_logps/chosen': -381.08441162109375, 'logits/rejected': -0.4394129514694214, 'logits/chosen': -0.4067993760108948, 'epoch': 1.82}


 61%|██████    | 1631/2685 [8:54:53<5:49:27, 19.89s/it]

 61%|██████    | 1632/2685 [8:55:13<5:48:42, 19.87s/it]
{'loss': 0.3426, 'learning_rate': 7.040921907226447e-07, 'rewards/chosen': -1.5715893507003784, 'rewards/rejected': -5.30488395690918, 'rewards/accuracies': 0.875, 'rewards/margins': 3.7332942485809326, 'policy_logps/rejected': -630.44970703125, 'policy_logps/chosen': -515.2938232421875, 'referece_logps/rejected': -577.40087890625, 'referece_logps/chosen': -499.577880859375, 'logits/rejected': 0.02677537500858307, 'logits/chosen': 0.02606293559074402, 'epoch': 1.82}

 61%|██████    | 1633/2685 [8:55:35<5:57:21, 20.38s/it]

 61%|██████    | 1634/2685 [8:55:56<5:59:50, 20.54s/it]

 61%|██████    | 1635/2685 [8:56:14<5:47:09, 19.84s/it]

 61%|██████    | 1636/2685 [8:56:34<5:48:19, 19.92s/it]
[2024-03-29 07:03:58,128] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 61%|██████    | 1638/2685 [8:57:17<6:03:30, 20.83s/it]
[2024-03-29 07:04:20,739] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 61%|██████    | 1639/2685 [8:57:39<6:11:08, 21.29s/it]
{'loss': 0.3296, 'learning_rate': 6.960359001085732e-07, 'rewards/chosen': -2.113579273223877, 'rewards/rejected': -4.281088829040527, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1675095558166504, 'policy_logps/rejected': -414.5561828613281, 'policy_logps/chosen': -388.2640075683594, 'referece_logps/rejected': -371.74530029296875, 'referece_logps/chosen': -367.1282653808594, 'logits/rejected': -0.16361558437347412, 'logits/chosen': -0.17389953136444092, 'epoch': 1.83}


 61%|██████    | 1641/2685 [8:58:13<5:28:30, 18.88s/it]

 61%|██████    | 1642/2685 [8:58:36<5:45:40, 19.89s/it]
{'loss': 0.3019, 'learning_rate': 6.925898070244751e-07, 'rewards/chosen': -1.5924599170684814, 'rewards/rejected': -3.6100802421569824, 'rewards/accuracies': 1.0, 'rewards/margins': 2.017620325088501, 'policy_logps/rejected': -386.4917297363281, 'policy_logps/chosen': -392.5989685058594, 'referece_logps/rejected': -350.39093017578125, 'referece_logps/chosen': -376.67437744140625, 'logits/rejected': 0.09428587555885315, 'logits/chosen': -0.07537594437599182, 'epoch': 1.83}
[2024-03-29 07:06:00,097] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 61%|██████    | 1643/2685 [8:58:56<5:50:01, 20.16s/it]

 61%|██████    | 1644/2685 [8:59:19<6:00:27, 20.78s/it]

 61%|██████▏   | 1645/2685 [8:59:36<5:44:20, 19.87s/it]


 61%|██████▏   | 1647/2685 [9:00:12<5:18:55, 18.44s/it]
{'loss': 0.2994, 'learning_rate': 6.868552896356117e-07, 'rewards/chosen': -0.9356502294540405, 'rewards/rejected': -4.12898588180542, 'rewards/accuracies': 0.875, 'rewards/margins': 3.19333553314209, 'policy_logps/rejected': -382.92138671875, 'policy_logps/chosen': -382.5374755859375, 'referece_logps/rejected': -341.63153076171875, 'referece_logps/chosen': -373.18096923828125, 'logits/rejected': 0.05681559443473816, 'logits/chosen': 0.029451943933963776, 'epoch': 1.84}

 61%|██████▏   | 1648/2685 [9:00:34<5:40:03, 19.68s/it]
[2024-03-29 07:07:58,850] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 61%|██████▏   | 1650/2685 [9:01:17<5:57:15, 20.71s/it]
[2024-03-29 07:08:21,063] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 61%|██████▏   | 1651/2685 [9:01:40<6:04:49, 21.17s/it]
{'loss': 0.3841, 'learning_rate': 6.822758698811051e-07, 'rewards/chosen': -0.6850969195365906, 'rewards/rejected': -3.1343133449554443, 'rewards/accuracies': 1.0, 'rewards/margins': 2.449216365814209, 'policy_logps/rejected': -248.36428833007812, 'policy_logps/chosen': -215.9598388671875, 'referece_logps/rejected': -217.02114868164062, 'referece_logps/chosen': -209.10887145996094, 'logits/rejected': -1.0507323741912842, 'logits/chosen': -1.0433098077774048, 'epoch': 1.84}

 62%|██████▏   | 1652/2685 [9:01:55<5:35:17, 19.48s/it]


 62%|██████▏   | 1654/2685 [9:02:34<5:33:43, 19.42s/it]
{'loss': 0.2808, 'learning_rate': 6.788461549908077e-07, 'rewards/chosen': -0.31079238653182983, 'rewards/rejected': -2.5211689472198486, 'rewards/accuracies': 1.0, 'rewards/margins': 2.210376739501953, 'policy_logps/rejected': -317.4454345703125, 'policy_logps/chosen': -280.0561218261719, 'referece_logps/rejected': -292.2337341308594, 'referece_logps/chosen': -276.9482116699219, 'logits/rejected': -0.7775946855545044, 'logits/chosen': -0.7782353758811951, 'epoch': 1.85}

 62%|██████▏   | 1655/2685 [9:02:54<5:39:43, 19.79s/it]


 62%|██████▏   | 1657/2685 [9:03:30<5:19:20, 18.64s/it]
{'loss': 0.3177, 'learning_rate': 6.75420647104723e-07, 'rewards/chosen': -1.5778672695159912, 'rewards/rejected': -2.794694423675537, 'rewards/accuracies': 0.5, 'rewards/margins': 1.2168272733688354, 'policy_logps/rejected': -439.53204345703125, 'policy_logps/chosen': -350.76239013671875, 'referece_logps/rejected': -411.5850830078125, 'referece_logps/chosen': -334.9837341308594, 'logits/rejected': -0.40898799896240234, 'logits/chosen': -0.34879958629608154, 'epoch': 1.85}

 62%|██████▏   | 1658/2685 [9:03:44<4:59:04, 17.47s/it]

 62%|██████▏   | 1659/2685 [9:04:05<5:17:41, 18.58s/it]

 62%|██████▏   | 1660/2685 [9:04:26<5:25:23, 19.05s/it]

 62%|██████▏   | 1661/2685 [9:04:47<5:37:50, 19.80s/it]

 62%|██████▏   | 1662/2685 [9:05:08<5:40:23, 19.96s/it]

 62%|██████▏   | 1663/2685 [9:05:30<5:52:45, 20.71s/it]

 62%|██████▏   | 1664/2685 [9:05:50<5:47:31, 20.42s/it]

 62%|██████▏   | 1665/2685 [9:06:07<5:32:17, 19.55s/it]

 62%|██████▏   | 1666/2685 [9:06:27<5:33:55, 19.66s/it]

 62%|██████▏   | 1667/2685 [9:06:47<5:33:00, 19.63s/it]

 62%|██████▏   | 1668/2685 [9:07:07<5:37:19, 19.90s/it]

 62%|██████▏   | 1669/2685 [9:07:25<5:25:56, 19.25s/it]

 62%|██████▏   | 1670/2685 [9:07:41<5:07:23, 18.17s/it]

 62%|██████▏   | 1671/2685 [9:08:01<5:20:19, 18.95s/it]

 62%|██████▏   | 1672/2685 [9:08:21<5:21:22, 19.04s/it]

 62%|██████▏   | 1673/2685 [9:08:41<5:28:59, 19.51s/it]

 62%|██████▏   | 1674/2685 [9:09:04<5:45:46, 20.52s/it]

 62%|██████▏   | 1675/2685 [9:09:24<5:42:33, 20.35s/it]

 62%|██████▏   | 1676/2685 [9:09:46<5:50:45, 20.86s/it]
[2024-03-29 07:17:10,019] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 62%|██████▏   | 1677/2685 [9:10:06<5:47:21, 20.68s/it]

 62%|██████▏   | 1678/2685 [9:10:26<5:42:47, 20.42s/it]

 63%|██████▎   | 1679/2685 [9:10:45<5:35:13, 19.99s/it]

 63%|██████▎   | 1680/2685 [9:11:04<5:27:07, 19.53s/it]

 63%|██████▎   | 1681/2685 [9:11:26<5:40:17, 20.34s/it]

 63%|██████▎   | 1682/2685 [9:11:46<5:37:44, 20.20s/it]
[2024-03-29 07:19:11,089] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1683/2685 [9:12:07<5:44:48, 20.65s/it]
[2024-03-29 07:19:31,225] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1684/2685 [9:12:28<5:41:54, 20.49s/it]

 63%|██████▎   | 1685/2685 [9:12:48<5:43:30, 20.61s/it]

 63%|██████▎   | 1686/2685 [9:13:03<5:14:16, 18.88s/it]

 63%|██████▎   | 1687/2685 [9:13:18<4:55:11, 17.75s/it]

 63%|██████▎   | 1688/2685 [9:13:37<5:00:03, 18.06s/it]
[2024-03-29 07:21:03,257] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1689/2685 [9:14:00<5:21:30, 19.37s/it]
[2024-03-29 07:21:25,817] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1690/2685 [9:14:22<5:37:04, 20.33s/it]

 63%|██████▎   | 1691/2685 [9:14:42<5:33:17, 20.12s/it]
[2024-03-29 07:22:02,517] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1692/2685 [9:14:59<5:17:48, 19.20s/it]
[2024-03-29 07:22:24,839] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1693/2685 [9:15:21<5:32:57, 20.14s/it]

 63%|██████▎   | 1694/2685 [9:15:35<4:58:59, 18.10s/it]
[2024-03-29 07:22:58,482] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1695/2685 [9:15:55<5:09:31, 18.76s/it]

 63%|██████▎   | 1696/2685 [9:16:10<4:50:05, 17.60s/it]
[2024-03-29 07:23:33,658] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1697/2685 [9:16:30<5:03:03, 18.40s/it]

 63%|██████▎   | 1698/2685 [9:16:47<4:53:49, 17.86s/it]

 63%|██████▎   | 1699/2685 [9:17:06<5:02:15, 18.39s/it]

 63%|██████▎   | 1700/2685 [9:17:26<5:06:29, 18.67s/it]

 63%|██████▎   | 1701/2685 [9:17:42<4:54:31, 17.96s/it]

 63%|██████▎   | 1702/2685 [9:18:04<5:15:03, 19.23s/it]

 63%|██████▎   | 1703/2685 [9:18:26<5:29:36, 20.14s/it]
[2024-03-29 07:25:51,049] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 63%|██████▎   | 1704/2685 [9:18:47<5:33:56, 20.42s/it]

 64%|██████▎   | 1705/2685 [9:19:08<5:32:20, 20.35s/it]

 64%|██████▎   | 1706/2685 [9:19:28<5:33:15, 20.42s/it]
[2024-03-29 07:26:53,802] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▎   | 1707/2685 [9:19:50<5:40:32, 20.89s/it]
[2024-03-29 07:27:16,014] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▎   | 1708/2685 [9:20:12<5:46:38, 21.29s/it]

 64%|██████▎   | 1709/2685 [9:20:32<5:37:38, 20.76s/it]

 64%|██████▎   | 1710/2685 [9:20:53<5:38:17, 20.82s/it]

 64%|██████▎   | 1711/2685 [9:21:12<5:31:07, 20.40s/it]


 64%|██████▍   | 1713/2685 [9:21:51<5:24:01, 20.00s/it]
{'loss': 0.3288, 'learning_rate': 6.123064718590099e-07, 'rewards/chosen': -0.9381989240646362, 'rewards/rejected': -1.9230505228042603, 'rewards/accuracies': 0.75, 'rewards/margins': 0.9848514795303345, 'policy_logps/rejected': -246.2930908203125, 'policy_logps/chosen': -253.92532348632812, 'referece_logps/rejected': -227.06259155273438, 'referece_logps/chosen': -244.5433349609375, 'logits/rejected': -0.4348338842391968, 'logits/chosen': -0.4381924271583557, 'epoch': 1.91}

 64%|██████▍   | 1714/2685 [9:22:11<5:22:14, 19.91s/it]

 64%|██████▍   | 1715/2685 [9:22:32<5:27:07, 20.23s/it]

 64%|██████▍   | 1716/2685 [9:22:52<5:24:40, 20.10s/it]

 64%|██████▍   | 1717/2685 [9:23:13<5:28:50, 20.38s/it]
[2024-03-29 07:30:37,645] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▍   | 1718/2685 [9:23:34<5:32:43, 20.64s/it]
[2024-03-29 07:31:00,420] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▍   | 1719/2685 [9:23:57<5:42:40, 21.28s/it]

 64%|██████▍   | 1720/2685 [9:24:13<5:17:30, 19.74s/it]

 64%|██████▍   | 1721/2685 [9:24:35<5:28:30, 20.45s/it]
[2024-03-29 07:32:00,331] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 64%|██████▍   | 1722/2685 [9:24:57<5:34:05, 20.82s/it]

 64%|██████▍   | 1723/2685 [9:25:16<5:28:40, 20.50s/it]

 64%|██████▍   | 1724/2685 [9:25:35<5:20:24, 20.00s/it]

 64%|██████▍   | 1725/2685 [9:25:55<5:18:20, 19.90s/it]

 64%|██████▍   | 1726/2685 [9:26:15<5:16:27, 19.80s/it]

 64%|██████▍   | 1727/2685 [9:26:34<5:13:16, 19.62s/it]

 64%|██████▍   | 1728/2685 [9:26:54<5:14:10, 19.70s/it]

 64%|██████▍   | 1729/2685 [9:27:15<5:21:12, 20.16s/it]

 64%|██████▍   | 1730/2685 [9:27:34<5:16:39, 19.89s/it]

 64%|██████▍   | 1731/2685 [9:27:56<5:24:05, 20.38s/it]

 65%|██████▍   | 1732/2685 [9:28:10<4:55:11, 18.59s/it]
[2024-03-29 07:35:37,485] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 65%|██████▍   | 1733/2685 [9:28:34<5:19:47, 20.16s/it]


 65%|██████▍   | 1735/2685 [9:29:10<5:01:39, 19.05s/it]

 65%|██████▍   | 1736/2685 [9:29:29<5:00:45, 19.01s/it]

 65%|██████▍   | 1737/2685 [9:29:51<5:11:59, 19.75s/it]

 65%|██████▍   | 1738/2685 [9:30:11<5:15:17, 19.98s/it]

 65%|██████▍   | 1739/2685 [9:30:31<5:12:37, 19.83s/it]

 65%|██████▍   | 1740/2685 [9:30:47<4:57:35, 18.89s/it]

 65%|██████▍   | 1741/2685 [9:31:07<5:01:21, 19.15s/it]

 65%|██████▍   | 1742/2685 [9:31:27<5:05:00, 19.41s/it]

 65%|██████▍   | 1743/2685 [9:31:48<5:11:25, 19.84s/it]

 65%|██████▍   | 1744/2685 [9:32:09<5:18:29, 20.31s/it]

 65%|██████▍   | 1745/2685 [9:32:29<5:16:01, 20.17s/it]

 65%|██████▌   | 1746/2685 [9:32:47<5:04:13, 19.44s/it]

 65%|██████▌   | 1747/2685 [9:33:07<5:05:34, 19.55s/it]

 65%|██████▌   | 1748/2685 [9:33:26<5:03:37, 19.44s/it]

 65%|██████▌   | 1749/2685 [9:33:39<4:34:20, 17.59s/it]

 65%|██████▌   | 1750/2685 [9:33:59<4:42:56, 18.16s/it]

 65%|██████▌   | 1751/2685 [9:34:19<4:50:24, 18.66s/it]

 65%|██████▌   | 1752/2685 [9:34:37<4:50:32, 18.68s/it]

 65%|██████▌   | 1753/2685 [9:35:00<5:07:38, 19.81s/it]

 65%|██████▌   | 1754/2685 [9:35:20<5:11:14, 20.06s/it]

 65%|██████▌   | 1755/2685 [9:35:39<5:05:15, 19.69s/it]

 65%|██████▌   | 1756/2685 [9:36:00<5:10:25, 20.05s/it]

 65%|██████▌   | 1757/2685 [9:36:20<5:07:03, 19.85s/it]

 65%|██████▌   | 1758/2685 [9:36:40<5:08:17, 19.95s/it]

 66%|██████▌   | 1759/2685 [9:37:00<5:09:15, 20.04s/it]

 66%|██████▌   | 1760/2685 [9:37:18<4:58:13, 19.34s/it]

 66%|██████▌   | 1761/2685 [9:37:37<4:59:52, 19.47s/it]

 66%|██████▌   | 1762/2685 [9:38:01<5:17:45, 20.66s/it]
[2024-03-29 07:45:04,540] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 66%|██████▌   | 1763/2685 [9:38:19<5:05:58, 19.91s/it]

 66%|██████▌   | 1764/2685 [9:38:36<4:52:45, 19.07s/it]

 66%|██████▌   | 1765/2685 [9:38:56<4:56:18, 19.32s/it]
{'loss': 0.2211, 'learning_rate': 5.55278372456498e-07, 'rewards/chosen': -1.6250264644622803, 'rewards/rejected': -3.9060609340667725, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2810349464416504, 'policy_logps/rejected': -267.77667236328125, 'policy_logps/chosen': -257.0247497558594, 'referece_logps/rejected': -228.716064453125, 'referece_logps/chosen': -240.7744903564453, 'logits/rejected': -0.4780152440071106, 'logits/chosen': -0.590074360370636, 'epoch': 1.97}


 66%|██████▌   | 1767/2685 [9:39:31<4:43:03, 18.50s/it]

 66%|██████▌   | 1768/2685 [9:39:54<5:02:21, 19.78s/it]
[2024-03-29 07:46:57,523] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2713, 'learning_rate': 5.520395566082953e-07, 'rewards/chosen': -0.8556486964225769, 'rewards/rejected': -2.746988296508789, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8913395404815674, 'policy_logps/rejected': -468.7878723144531, 'policy_logps/chosen': -327.843017578125, 'referece_logps/rejected': -441.3179931640625, 'referece_logps/chosen': -319.28656005859375, 'logits/rejected': 0.9722787141799927, 'logits/chosen': 0.9473133087158203, 'epoch': 1.98}


 66%|██████▌   | 1770/2685 [9:40:31<4:49:48, 19.00s/it]

 66%|██████▌   | 1771/2685 [9:40:53<5:02:52, 19.88s/it]

 66%|██████▌   | 1772/2685 [9:41:11<4:55:30, 19.42s/it]

 66%|██████▌   | 1773/2685 [9:41:31<4:56:46, 19.52s/it]

 66%|██████▌   | 1774/2685 [9:41:52<5:00:57, 19.82s/it]

 66%|██████▌   | 1775/2685 [9:42:11<4:59:28, 19.75s/it]

 66%|██████▌   | 1776/2685 [9:42:30<4:53:27, 19.37s/it]
{'loss': 0.3047, 'learning_rate': 5.434315179571531e-07, 'rewards/chosen': -0.5211885571479797, 'rewards/rejected': -2.6554958820343018, 'rewards/accuracies': 1.0, 'rewards/margins': 2.134307384490967, 'policy_logps/rejected': -295.72607421875, 'policy_logps/chosen': -196.48643493652344, 'referece_logps/rejected': -269.17108154296875, 'referece_logps/chosen': -191.27455139160156, 'logits/rejected': -0.10617172718048096, 'logits/chosen': -0.05722306668758392, 'epoch': 1.98}


 66%|██████▌   | 1778/2685 [9:43:08<4:53:48, 19.44s/it]

 66%|██████▋   | 1779/2685 [9:43:28<4:54:48, 19.52s/it]

 66%|██████▋   | 1780/2685 [9:43:50<5:06:26, 20.32s/it]
{'loss': 0.3345, 'learning_rate': 5.391433976003845e-07, 'rewards/chosen': -0.7702471613883972, 'rewards/rejected': -2.6174490451812744, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8472018241882324, 'policy_logps/rejected': -447.7601013183594, 'policy_logps/chosen': -285.8548583984375, 'referece_logps/rejected': -421.5856018066406, 'referece_logps/chosen': -278.1523742675781, 'logits/rejected': 0.6729132533073425, 'logits/chosen': 0.7833176851272583, 'epoch': 1.99}


 66%|██████▋   | 1782/2685 [9:44:25<4:43:31, 18.84s/it]
{'loss': 0.285, 'learning_rate': 5.370033558948792e-07, 'rewards/chosen': -1.6102322340011597, 'rewards/rejected': -3.8648533821105957, 'rewards/accuracies': 1.0, 'rewards/margins': 2.2546210289001465, 'policy_logps/rejected': -555.0660400390625, 'policy_logps/chosen': -462.47613525390625, 'referece_logps/rejected': -516.4175415039062, 'referece_logps/chosen': -446.373779296875, 'logits/rejected': 0.27651435136795044, 'logits/chosen': 0.11048514395952225, 'epoch': 1.99}

 66%|██████▋   | 1783/2685 [9:44:41<4:32:15, 18.11s/it]


 66%|██████▋   | 1785/2685 [9:45:19<4:35:29, 18.37s/it]
{'loss': 0.2278, 'learning_rate': 5.337983514754722e-07, 'rewards/chosen': -1.1535375118255615, 'rewards/rejected': -3.3624470233917236, 'rewards/accuracies': 0.875, 'rewards/margins': 2.208909511566162, 'policy_logps/rejected': -294.1235046386719, 'policy_logps/chosen': -247.47268676757812, 'referece_logps/rejected': -260.4990234375, 'referece_logps/chosen': -235.93731689453125, 'logits/rejected': -0.5100346207618713, 'logits/chosen': -0.43287691473960876, 'epoch': 1.99}


 67%|██████▋   | 1787/2685 [9:46:02<4:57:27, 19.88s/it]

 67%|██████▋   | 1788/2685 [9:46:16<4:30:25, 18.09s/it]
{'loss': 0.2812, 'learning_rate': 5.30599454136301e-07, 'rewards/chosen': -0.8301825523376465, 'rewards/rejected': -3.973297357559204, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1431148052215576, 'policy_logps/rejected': -455.1075744628906, 'policy_logps/chosen': -325.61529541015625, 'referece_logps/rejected': -415.3746337890625, 'referece_logps/chosen': -317.3135070800781, 'logits/rejected': -0.8491864204406738, 'logits/chosen': -0.8765332102775574, 'epoch': 2.0}


 67%|██████▋   | 1790/2685 [9:46:54<4:37:32, 18.61s/it]

 67%|██████▋   | 1791/2685 [9:47:10<4:27:17, 17.94s/it]
{'loss': 0.3107, 'learning_rate': 5.27406705781813e-07, 'rewards/chosen': -1.158706784248352, 'rewards/rejected': -2.756065845489502, 'rewards/accuracies': 0.625, 'rewards/margins': 1.59735906124115, 'policy_logps/rejected': -303.1575927734375, 'policy_logps/chosen': -354.9586181640625, 'referece_logps/rejected': -275.596923828125, 'referece_logps/chosen': -343.37152099609375, 'logits/rejected': 0.06469254195690155, 'logits/chosen': -0.07665455341339111, 'epoch': 2.0}
[2024-03-29 07:54:31,002] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 1793/2685 [9:47:49<4:39:57, 18.83s/it]

 67%|██████▋   | 1794/2685 [9:48:07<4:34:27, 18.48s/it]

 67%|██████▋   | 1795/2685 [9:48:28<4:48:52, 19.47s/it]
[2024-03-29 07:55:31,955] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.2808, 'learning_rate': 5.23159345339663e-07, 'rewards/chosen': -2.7064828872680664, 'rewards/rejected': -5.583225727081299, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8767430782318115, 'policy_logps/rejected': -580.3629760742188, 'policy_logps/chosen': -545.5986328125, 'referece_logps/rejected': -524.53076171875, 'referece_logps/chosen': -518.5338745117188, 'logits/rejected': 0.2052306830883026, 'logits/chosen': 0.1820477396249771, 'epoch': 2.01}


 67%|██████▋   | 1797/2685 [9:49:03<4:31:34, 18.35s/it]

 67%|██████▋   | 1798/2685 [9:49:23<4:38:40, 18.85s/it]

 67%|██████▋   | 1799/2685 [9:49:40<4:30:16, 18.30s/it]
{'loss': 0.334, 'learning_rate': 5.189230896836994e-07, 'rewards/chosen': -1.1669032573699951, 'rewards/rejected': -3.312934398651123, 'rewards/accuracies': 0.75, 'rewards/margins': 2.146030902862549, 'policy_logps/rejected': -332.699951171875, 'policy_logps/chosen': -372.1444396972656, 'referece_logps/rejected': -299.57061767578125, 'referece_logps/chosen': -360.4753723144531, 'logits/rejected': -0.49968940019607544, 'logits/chosen': -0.5678237080574036, 'epoch': 2.01}


 67%|██████▋   | 1801/2685 [9:50:20<4:42:43, 19.19s/it]

 67%|██████▋   | 1802/2685 [9:50:40<4:45:31, 19.40s/it]

 67%|██████▋   | 1803/2685 [9:50:59<4:43:24, 19.28s/it]

 67%|██████▋   | 1804/2685 [9:51:18<4:45:31, 19.45s/it]

 67%|██████▋   | 1805/2685 [9:51:41<4:57:44, 20.30s/it]
[2024-03-29 07:58:44,377] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 1806/2685 [9:52:01<4:56:21, 20.23s/it]

 67%|██████▋   | 1807/2685 [9:52:21<4:55:12, 20.17s/it]
{'loss': 0.3056, 'learning_rate': 5.104842870894099e-07, 'rewards/chosen': -0.016559511423110962, 'rewards/rejected': -2.9325554370880127, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9159958362579346, 'policy_logps/rejected': -337.1749267578125, 'policy_logps/chosen': -338.5361328125, 'referece_logps/rejected': -307.849365234375, 'referece_logps/chosen': -338.37054443359375, 'logits/rejected': -0.8957905173301697, 'logits/chosen': -0.9380279779434204, 'epoch': 2.02}
[2024-03-29 07:59:47,309] [WARNING] [stage3.py:2069:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time


 67%|██████▋   | 1809/2685 [9:53:04<5:03:56, 20.82s/it]

 67%|██████▋   | 1810/2685 [9:53:24<4:59:43, 20.55s/it]
[2024-03-29 08:00:27,709] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 67%|██████▋   | 1811/2685 [9:53:45<5:01:14, 20.68s/it]

 67%|██████▋   | 1812/2685 [9:54:06<5:02:20, 20.78s/it]

 68%|██████▊   | 1813/2685 [9:54:27<5:03:44, 20.90s/it]

 68%|██████▊   | 1814/2685 [9:54:44<4:46:15, 19.72s/it]

 68%|██████▊   | 1815/2685 [9:55:04<4:47:47, 19.85s/it]

 68%|██████▊   | 1816/2685 [9:55:24<4:46:35, 19.79s/it]

 68%|██████▊   | 1817/2685 [9:55:45<4:52:25, 20.21s/it]
[2024-03-29 08:02:48,846] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 1818/2685 [9:56:03<4:39:37, 19.35s/it]
[2024-03-29 08:03:06,186] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 1819/2685 [9:56:19<4:26:42, 18.48s/it]

 68%|██████▊   | 1820/2685 [9:56:40<4:38:02, 19.29s/it]

 68%|██████▊   | 1821/2685 [9:57:00<4:40:51, 19.50s/it]

 68%|██████▊   | 1822/2685 [9:57:20<4:42:28, 19.64s/it]
[2024-03-29 08:04:23,766] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 1823/2685 [9:57:41<4:49:18, 20.14s/it]
{'loss': 0.2425, 'learning_rate': 4.93744262545905e-07, 'rewards/chosen': -1.4486770629882812, 'rewards/rejected': -3.114093065261841, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6654160022735596, 'policy_logps/rejected': -418.77178955078125, 'policy_logps/chosen': -367.19073486328125, 'referece_logps/rejected': -387.630859375, 'referece_logps/chosen': -352.7039794921875, 'logits/rejected': -1.1929956674575806, 'logits/chosen': -1.2841298580169678, 'epoch': 2.04}


 68%|██████▊   | 1825/2685 [9:58:21<4:47:02, 20.03s/it]

 68%|██████▊   | 1826/2685 [9:58:39<4:38:47, 19.47s/it]
{'loss': 0.3954, 'learning_rate': 4.906263210158483e-07, 'rewards/chosen': -0.503500759601593, 'rewards/rejected': -3.61757230758667, 'rewards/accuracies': 0.625, 'rewards/margins': 3.1140713691711426, 'policy_logps/rejected': -374.8045349121094, 'policy_logps/chosen': -342.8417053222656, 'referece_logps/rejected': -338.6288146972656, 'referece_logps/chosen': -337.8066711425781, 'logits/rejected': -1.2620347738265991, 'logits/chosen': -1.3215919733047485, 'epoch': 2.04}

 68%|██████▊   | 1827/2685 [9:58:54<4:19:26, 18.14s/it]


 68%|██████▊   | 1829/2685 [9:59:29<4:11:29, 17.63s/it]
{'loss': 0.2874, 'learning_rate': 4.875150521046832e-07, 'rewards/chosen': -1.1920270919799805, 'rewards/rejected': -2.868802070617676, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6767748594284058, 'policy_logps/rejected': -431.5848388671875, 'policy_logps/chosen': -257.34259033203125, 'referece_logps/rejected': -402.8968200683594, 'referece_logps/chosen': -245.4223175048828, 'logits/rejected': 0.03178834915161133, 'logits/chosen': 0.023305818438529968, 'epoch': 2.04}
[2024-03-29 08:06:53,280] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

 68%|██████▊   | 1830/2685 [9:59:50<4:23:39, 18.50s/it]

 68%|██████▊   | 1831/2685 [10:00:10<4:30:08, 18.98s/it]


 68%|██████▊   | 1833/2685 [10:00:50<4:39:42, 19.70s/it]
{'loss': 0.3175, 'learning_rate': 4.833771436166068e-07, 'rewards/chosen': -1.6253273487091064, 'rewards/rejected': -5.985250949859619, 'rewards/accuracies': 1.0, 'rewards/margins': 4.359923362731934, 'policy_logps/rejected': -533.7584838867188, 'policy_logps/chosen': -539.1906127929688, 'referece_logps/rejected': -473.9059753417969, 'referece_logps/chosen': -522.9373168945312, 'logits/rejected': -0.5516880750656128, 'logits/chosen': -0.43755459785461426, 'epoch': 2.05}


 68%|██████▊   | 1835/2685 [10:01:31<4:45:51, 20.18s/it]

 68%|██████▊   | 1836/2685 [10:01:54<4:54:47, 20.83s/it]

 68%|██████▊   | 1837/2685 [10:02:15<4:56:18, 20.97s/it]

 68%|██████▊   | 1838/2685 [10:02:35<4:52:44, 20.74s/it]

 68%|██████▊   | 1839/2685 [10:02:54<4:42:32, 20.04s/it]

 69%|██████▊   | 1840/2685 [10:03:13<4:40:01, 19.88s/it]

 69%|██████▊   | 1841/2685 [10:03:33<4:38:00, 19.76s/it]
{'loss': 0.2822, 'learning_rate': 4.7513751645751955e-07, 'rewards/chosen': -1.4972445964813232, 'rewards/rejected': -3.0918540954589844, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5946094989776611, 'policy_logps/rejected': -473.77593994140625, 'policy_logps/chosen': -408.0281066894531, 'referece_logps/rejected': -442.857421875, 'referece_logps/chosen': -393.0556640625, 'logits/rejected': 0.3246224820613861, 'logits/chosen': 0.5077036619186401, 'epoch': 2.06}


 69%|██████▊   | 1843/2685 [10:04:15<4:44:51, 20.30s/it]
{'loss': 0.3362, 'learning_rate': 4.7308521919740286e-07, 'rewards/chosen': -0.7371420860290527, 'rewards/rejected': -2.172733783721924, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4355915784835815, 'policy_logps/rejected': -334.8349609375, 'policy_logps/chosen': -316.447509765625, 'referece_logps/rejected': -313.1076354980469, 'referece_logps/chosen': -309.0760803222656, 'logits/rejected': 0.16669611632823944, 'logits/chosen': 0.11589150875806808, 'epoch': 2.06}

 69%|██████▊   | 1844/2685 [10:04:34<4:41:56, 20.11s/it]


 69%|██████▉   | 1846/2685 [10:05:15<4:39:22, 19.98s/it]

 69%|██████▉   | 1847/2685 [10:05:36<4:44:25, 20.36s/it]

 69%|██████▉   | 1848/2685 [10:05:54<4:34:20, 19.67s/it]

 69%|██████▉   | 1849/2685 [10:06:12<4:26:13, 19.11s/it]

 69%|██████▉   | 1850/2685 [10:06:32<4:29:19, 19.35s/it]

 69%|██████▉   | 1851/2685 [10:06:50<4:23:44, 18.97s/it]

 69%|██████▉   | 1852/2685 [10:07:10<4:27:26, 19.26s/it]

 69%|██████▉   | 1853/2685 [10:07:30<4:29:44, 19.45s/it]

 69%|██████▉   | 1854/2685 [10:07:49<4:30:44, 19.55s/it]

 69%|██████▉   | 1855/2685 [10:08:09<4:31:16, 19.61s/it]
{'loss': 0.3114, 'learning_rate': 4.6083627503513136e-07, 'rewards/chosen': -1.2551261186599731, 'rewards/rejected': -3.5393714904785156, 'rewards/accuracies': 0.875, 'rewards/margins': 2.284245491027832, 'policy_logps/rejected': -329.5657653808594, 'policy_logps/chosen': -340.63043212890625, 'referece_logps/rejected': -294.17205810546875, 'referece_logps/chosen': -328.07916259765625, 'logits/rejected': -0.34213751554489136, 'logits/chosen': -0.3044273853302002, 'epoch': 2.07}


 69%|██████▉   | 1857/2685 [10:08:49<4:33:11, 19.80s/it]

 69%|██████▉   | 1858/2685 [10:09:05<4:19:27, 18.82s/it]

 69%|██████▉   | 1859/2685 [10:09:27<4:31:48, 19.74s/it]
{'loss': 0.2535, 'learning_rate': 4.567782795195816e-07, 'rewards/chosen': -0.4176887571811676, 'rewards/rejected': -3.2857532501220703, 'rewards/accuracies': 1.0, 'rewards/margins': 2.8680644035339355, 'policy_logps/rejected': -334.1167297363281, 'policy_logps/chosen': -310.85546875, 'referece_logps/rejected': -301.25921630859375, 'referece_logps/chosen': -306.6785583496094, 'logits/rejected': -0.6508119106292725, 'logits/chosen': -0.7378963828086853, 'epoch': 2.08}

 69%|██████▉   | 1860/2685 [10:09:45<4:22:01, 19.06s/it]

 69%|██████▉   | 1861/2685 [10:10:02<4:15:31, 18.61s/it]

 69%|██████▉   | 1862/2685 [10:10:21<4:14:45, 18.57s/it]

 69%|██████▉   | 1863/2685 [10:10:41<4:21:00, 19.05s/it]


 69%|██████▉   | 1865/2685 [10:11:16<4:06:04, 18.00s/it]
{'loss': 0.2104, 'learning_rate': 4.5071503573306947e-07, 'rewards/chosen': -0.8373666405677795, 'rewards/rejected': -3.798759937286377, 'rewards/accuracies': 1.0, 'rewards/margins': 2.961393117904663, 'policy_logps/rejected': -349.344482421875, 'policy_logps/chosen': -273.2488098144531, 'referece_logps/rejected': -311.3569030761719, 'referece_logps/chosen': -264.8751220703125, 'logits/rejected': 0.03726402670145035, 'logits/chosen': 0.05609217658638954, 'epoch': 2.08}


 70%|██████▉   | 1867/2685 [10:11:52<4:01:54, 17.74s/it]

 70%|██████▉   | 1868/2685 [10:12:12<4:09:49, 18.35s/it]

 70%|██████▉   | 1869/2685 [10:12:30<4:09:25, 18.34s/it]

 70%|██████▉   | 1870/2685 [10:12:50<4:15:01, 18.77s/it]

 70%|██████▉   | 1871/2685 [10:13:06<4:02:40, 17.89s/it]
{'loss': 0.2886, 'learning_rate': 4.446805736239594e-07, 'rewards/chosen': -0.427773654460907, 'rewards/rejected': -4.653265476226807, 'rewards/accuracies': 1.0, 'rewards/margins': 4.225491523742676, 'policy_logps/rejected': -396.4051818847656, 'policy_logps/chosen': -458.6271057128906, 'referece_logps/rejected': -349.87255859375, 'referece_logps/chosen': -454.349365234375, 'logits/rejected': -0.281618595123291, 'logits/chosen': -0.24787431955337524, 'epoch': 2.09}

 70%|██████▉   | 1872/2685 [10:13:29<4:25:18, 19.58s/it]

 70%|██████▉   | 1873/2685 [10:13:45<4:10:26, 18.51s/it]


 70%|██████▉   | 1875/2685 [10:14:24<4:15:32, 18.93s/it]
{'loss': 0.2756, 'learning_rate': 4.406737449142769e-07, 'rewards/chosen': -2.97021222114563, 'rewards/rejected': -4.7032904624938965, 'rewards/accuracies': 0.75, 'rewards/margins': 1.733078122138977, 'policy_logps/rejected': -535.4969482421875, 'policy_logps/chosen': -493.25439453125, 'referece_logps/rejected': -488.4640808105469, 'referece_logps/chosen': -463.5522766113281, 'logits/rejected': 0.6442936658859253, 'logits/chosen': 0.6010979413986206, 'epoch': 2.09}


 70%|██████▉   | 1877/2685 [10:15:04<4:21:00, 19.38s/it]

 70%|██████▉   | 1878/2685 [10:15:24<4:22:46, 19.54s/it]

 70%|██████▉   | 1879/2685 [10:15:42<4:17:44, 19.19s/it]

 70%|███████   | 1880/2685 [10:16:02<4:19:57, 19.38s/it]

 70%|███████   | 1881/2685 [10:16:22<4:20:43, 19.46s/it]
{'loss': 0.2185, 'learning_rate': 4.34687954173974e-07, 'rewards/chosen': -1.3606235980987549, 'rewards/rejected': -3.808375120162964, 'rewards/accuracies': 1.0, 'rewards/margins': 2.447751760482788, 'policy_logps/rejected': -370.43536376953125, 'policy_logps/chosen': -404.5901184082031, 'referece_logps/rejected': -332.35162353515625, 'referece_logps/chosen': -390.98382568359375, 'logits/rejected': -0.7440621852874756, 'logits/chosen': -0.7790055274963379, 'epoch': 2.1}

 70%|███████   | 1882/2685 [10:16:44<4:30:15, 20.19s/it]

 70%|███████   | 1883/2685 [10:17:05<4:32:55, 20.42s/it]


 70%|███████   | 1885/2685 [10:17:46<4:32:30, 20.44s/it]

 70%|███████   | 1886/2685 [10:18:06<4:29:41, 20.25s/it]

 70%|███████   | 1887/2685 [10:18:24<4:22:04, 19.70s/it]

 70%|███████   | 1888/2685 [10:18:40<4:07:01, 18.60s/it]

 70%|███████   | 1889/2685 [10:19:00<4:12:39, 19.04s/it]
{'loss': 0.385, 'learning_rate': 4.267530316853816e-07, 'rewards/chosen': -2.113558053970337, 'rewards/rejected': -5.276177883148193, 'rewards/accuracies': 0.75, 'rewards/margins': 3.1626200675964355, 'policy_logps/rejected': -527.4573974609375, 'policy_logps/chosen': -548.5560913085938, 'referece_logps/rejected': -474.69561767578125, 'referece_logps/chosen': -527.4205322265625, 'logits/rejected': 0.1672600507736206, 'logits/chosen': 0.2351539582014084, 'epoch': 2.11}


 70%|███████   | 1891/2685 [10:19:42<4:24:40, 20.00s/it]
{'loss': 0.2632, 'learning_rate': 4.2477761595063455e-07, 'rewards/chosen': -1.0608861446380615, 'rewards/rejected': -5.224851608276367, 'rewards/accuracies': 1.0, 'rewards/margins': 4.163965225219727, 'policy_logps/rejected': -624.1337890625, 'policy_logps/chosen': -343.8526611328125, 'referece_logps/rejected': -571.8853149414062, 'referece_logps/chosen': -333.24383544921875, 'logits/rejected': -0.8377972841262817, 'logits/chosen': -0.9483211636543274, 'epoch': 2.11}


 71%|███████   | 1893/2685 [10:20:20<4:16:39, 19.44s/it]
{'loss': 0.251, 'learning_rate': 4.228055492019793e-07, 'rewards/chosen': -1.7890205383300781, 'rewards/rejected': -3.8436903953552246, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0546703338623047, 'policy_logps/rejected': -440.88037109375, 'policy_logps/chosen': -470.75372314453125, 'referece_logps/rejected': -402.4434509277344, 'referece_logps/chosen': -452.8634338378906, 'logits/rejected': -0.22217929363250732, 'logits/chosen': -0.18195687234401703, 'epoch': 2.12}


 71%|███████   | 1895/2685 [10:21:02<4:26:48, 20.26s/it]
{'loss': 0.2725, 'learning_rate': 4.2083684292093046e-07, 'rewards/chosen': -2.2233943939208984, 'rewards/rejected': -3.9873526096343994, 'rewards/accuracies': 0.875, 'rewards/margins': 1.763958215713501, 'policy_logps/rejected': -457.90130615234375, 'policy_logps/chosen': -435.4574890136719, 'referece_logps/rejected': -418.0277404785156, 'referece_logps/chosen': -413.2235107421875, 'logits/rejected': -0.3051430583000183, 'logits/chosen': -0.26568251848220825, 'epoch': 2.12}


 71%|███████   | 1897/2685 [10:21:44<4:30:14, 20.58s/it]

 71%|███████   | 1898/2685 [10:22:04<4:27:29, 20.39s/it]
{'loss': 0.2875, 'learning_rate': 4.1789010944376746e-07, 'rewards/chosen': -1.1364421844482422, 'rewards/rejected': -3.537003993988037, 'rewards/accuracies': 0.875, 'rewards/margins': 2.400561809539795, 'policy_logps/rejected': -392.5630187988281, 'policy_logps/chosen': -300.50909423828125, 'referece_logps/rejected': -357.19293212890625, 'referece_logps/chosen': -289.1446533203125, 'logits/rejected': -0.8753302693367004, 'logits/chosen': -0.8122798204421997, 'epoch': 2.12}

 71%|███████   | 1899/2685 [10:22:26<4:31:16, 20.71s/it]


 71%|███████   | 1901/2685 [10:23:01<4:10:04, 19.14s/it]
{'loss': 0.3314, 'learning_rate': 4.149510014046922e-07, 'rewards/chosen': -1.5107817649841309, 'rewards/rejected': -4.697085380554199, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1863036155700684, 'policy_logps/rejected': -424.9421691894531, 'policy_logps/chosen': -391.9822998046875, 'referece_logps/rejected': -377.97125244140625, 'referece_logps/chosen': -376.8745422363281, 'logits/rejected': -0.1949821561574936, 'logits/chosen': -0.16790857911109924, 'epoch': 2.12}


 71%|███████   | 1903/2685 [10:23:41<4:14:15, 19.51s/it]

 71%|███████   | 1904/2685 [10:23:59<4:07:05, 18.98s/it]
{'loss': 0.1897, 'learning_rate': 4.1201955730500337e-07, 'rewards/chosen': -0.581895649433136, 'rewards/rejected': -3.861963987350464, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2800683975219727, 'policy_logps/rejected': -442.2271728515625, 'policy_logps/chosen': -355.44500732421875, 'referece_logps/rejected': -403.6075134277344, 'referece_logps/chosen': -349.6260986328125, 'logits/rejected': -0.8061209917068481, 'logits/chosen': -0.8849795460700989, 'epoch': 2.13}


 71%|███████   | 1906/2685 [10:24:41<4:19:46, 20.01s/it]

 71%|███████   | 1907/2685 [10:25:03<4:25:00, 20.44s/it]

 71%|███████   | 1908/2685 [10:25:22<4:22:44, 20.29s/it]

 71%|███████   | 1909/2685 [10:25:41<4:14:08, 19.65s/it]
{'loss': 0.2943, 'learning_rate': 4.0715095236835917e-07, 'rewards/chosen': -1.9262888431549072, 'rewards/rejected': -4.534754276275635, 'rewards/accuracies': 0.75, 'rewards/margins': 2.6084649562835693, 'policy_logps/rejected': -379.0939636230469, 'policy_logps/chosen': -339.2686767578125, 'referece_logps/rejected': -333.7464294433594, 'referece_logps/chosen': -320.0057678222656, 'logits/rejected': -0.21983447670936584, 'logits/chosen': -0.22913284599781036, 'epoch': 2.13}

 71%|███████   | 1910/2685 [10:26:01<4:17:38, 19.95s/it]

 71%|███████   | 1911/2685 [10:26:22<4:20:15, 20.17s/it]


 71%|███████   | 1913/2685 [10:27:01<4:15:52, 19.89s/it]

 71%|███████▏  | 1914/2685 [10:27:21<4:15:49, 19.91s/it]

 71%|███████▏  | 1915/2685 [10:27:43<4:21:23, 20.37s/it]

 71%|███████▏  | 1916/2685 [10:28:03<4:21:22, 20.39s/it]

 71%|███████▏  | 1917/2685 [10:28:23<4:18:33, 20.20s/it]
{'loss': 0.2417, 'learning_rate': 3.994061286868361e-07, 'rewards/chosen': -1.5768545866012573, 'rewards/rejected': -5.017163276672363, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4403085708618164, 'policy_logps/rejected': -467.35968017578125, 'policy_logps/chosen': -445.1182861328125, 'referece_logps/rejected': -417.18804931640625, 'referece_logps/chosen': -429.3497314453125, 'logits/rejected': 0.30675584077835083, 'logits/chosen': 0.2941151559352875, 'epoch': 2.14}

 71%|███████▏  | 1918/2685 [10:28:42<4:14:41, 19.92s/it]

 71%|███████▏  | 1919/2685 [10:29:03<4:19:56, 20.36s/it]


 72%|███████▏  | 1921/2685 [10:29:35<3:49:42, 18.04s/it]

 72%|███████▏  | 1922/2685 [10:29:57<4:02:54, 19.10s/it]

 72%|███████▏  | 1923/2685 [10:30:17<4:05:59, 19.37s/it]
{'loss': 0.3601, 'learning_rate': 3.936341867570533e-07, 'rewards/chosen': -1.1701370477676392, 'rewards/rejected': -3.0853371620178223, 'rewards/accuracies': 0.75, 'rewards/margins': 1.9151999950408936, 'policy_logps/rejected': -293.9723815917969, 'policy_logps/chosen': -271.9112548828125, 'referece_logps/rejected': -263.1189880371094, 'referece_logps/chosen': -260.2098693847656, 'logits/rejected': -0.554379403591156, 'logits/chosen': -0.6278995275497437, 'epoch': 2.15}


 72%|███████▏  | 1925/2685 [10:30:59<4:15:02, 20.13s/it]

 72%|███████▏  | 1926/2685 [10:31:15<3:58:59, 18.89s/it]
{'loss': 0.3052, 'learning_rate': 3.9076011168886116e-07, 'rewards/chosen': -1.456246256828308, 'rewards/rejected': -2.9992446899414062, 'rewards/accuracies': 0.75, 'rewards/margins': 1.5429980754852295, 'policy_logps/rejected': -276.419677734375, 'policy_logps/chosen': -287.1636962890625, 'referece_logps/rejected': -246.42724609375, 'referece_logps/chosen': -272.60125732421875, 'logits/rejected': -1.20845365524292, 'logits/chosen': -1.3439931869506836, 'epoch': 2.15}


 72%|███████▏  | 1928/2685 [10:31:51<3:47:19, 18.02s/it]
{'loss': 0.2442, 'learning_rate': 3.8884849311903156e-07, 'rewards/chosen': 0.028377927839756012, 'rewards/rejected': -2.8268308639526367, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8552086353302, 'policy_logps/rejected': -338.1448974609375, 'policy_logps/chosen': -367.40789794921875, 'referece_logps/rejected': -309.8765869140625, 'referece_logps/chosen': -367.69171142578125, 'logits/rejected': -0.8718708753585815, 'logits/chosen': -0.8795467019081116, 'epoch': 2.15}

 72%|███████▏  | 1929/2685 [10:32:11<3:53:21, 18.52s/it]


 72%|███████▏  | 1931/2685 [10:32:45<3:46:37, 18.03s/it]
{'loss': 0.2906, 'learning_rate': 3.859877403016727e-07, 'rewards/chosen': -1.0004945993423462, 'rewards/rejected': -2.6217517852783203, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6212570667266846, 'policy_logps/rejected': -271.92718505859375, 'policy_logps/chosen': -233.28248596191406, 'referece_logps/rejected': -245.70965576171875, 'referece_logps/chosen': -223.27752685546875, 'logits/rejected': -0.37089359760284424, 'logits/chosen': -0.4063076674938202, 'epoch': 2.16}

 72%|███████▏  | 1932/2685 [10:33:02<3:42:08, 17.70s/it]

 72%|███████▏  | 1933/2685 [10:33:22<3:49:06, 18.28s/it]

 72%|███████▏  | 1934/2685 [10:33:42<3:57:33, 18.98s/it]

 72%|███████▏  | 1935/2685 [10:34:03<4:01:36, 19.33s/it]

 72%|███████▏  | 1936/2685 [10:34:24<4:10:22, 20.06s/it]

 72%|███████▏  | 1937/2685 [10:34:42<4:02:25, 19.45s/it]


 72%|███████▏  | 1939/2685 [10:35:19<3:56:08, 18.99s/it]
{'loss': 0.2699, 'learning_rate': 3.7839849060663973e-07, 'rewards/chosen': -1.5385034084320068, 'rewards/rejected': -4.193487167358398, 'rewards/accuracies': 0.875, 'rewards/margins': 2.6549835205078125, 'policy_logps/rejected': -394.00897216796875, 'policy_logps/chosen': -398.70709228515625, 'referece_logps/rejected': -352.0741271972656, 'referece_logps/chosen': -383.322021484375, 'logits/rejected': -0.015056893229484558, 'logits/chosen': 0.0020861178636550903, 'epoch': 2.17}

 72%|███████▏  | 1940/2685 [10:35:36<3:48:05, 18.37s/it]

 72%|███████▏  | 1941/2685 [10:35:58<4:01:38, 19.49s/it]


 72%|███████▏  | 1943/2685 [10:36:35<3:55:54, 19.08s/it]

 72%|███████▏  | 1944/2685 [10:36:58<4:06:38, 19.97s/it]
{'loss': 0.2703, 'learning_rate': 3.7368456912761525e-07, 'rewards/chosen': -0.8254013657569885, 'rewards/rejected': -3.18135404586792, 'rewards/accuracies': 0.75, 'rewards/margins': 2.355952739715576, 'policy_logps/rejected': -421.8899841308594, 'policy_logps/chosen': -391.3735046386719, 'referece_logps/rejected': -390.076416015625, 'referece_logps/chosen': -383.1195068359375, 'logits/rejected': 0.1470862627029419, 'logits/chosen': 0.1226465106010437, 'epoch': 2.17}

 72%|███████▏  | 1945/2685 [10:37:19<4:11:03, 20.36s/it]

 72%|███████▏  | 1946/2685 [10:37:40<4:14:21, 20.65s/it]


 73%|███████▎  | 1948/2685 [10:38:20<4:09:43, 20.33s/it]

 73%|███████▎  | 1949/2685 [10:38:40<4:08:01, 20.22s/it]
{'loss': 0.365, 'learning_rate': 3.6899343792482286e-07, 'rewards/chosen': -1.7900264263153076, 'rewards/rejected': -5.138359069824219, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3483328819274902, 'policy_logps/rejected': -403.1695556640625, 'policy_logps/chosen': -358.275634765625, 'referece_logps/rejected': -351.78594970703125, 'referece_logps/chosen': -340.3753356933594, 'logits/rejected': -0.3202062249183655, 'logits/chosen': -0.2270982563495636, 'epoch': 2.18}


 73%|███████▎  | 1951/2685 [10:39:18<4:02:44, 19.84s/it]
{'loss': 0.3317, 'learning_rate': 3.671234049859041e-07, 'rewards/chosen': -1.5147831439971924, 'rewards/rejected': -4.038148403167725, 'rewards/accuracies': 0.75, 'rewards/margins': 2.5233654975891113, 'policy_logps/rejected': -490.0325622558594, 'policy_logps/chosen': -478.5112609863281, 'referece_logps/rejected': -449.65106201171875, 'referece_logps/chosen': -463.3634033203125, 'logits/rejected': 0.22825519740581512, 'logits/chosen': 0.17557039856910706, 'epoch': 2.18}


 73%|███████▎  | 1953/2685 [10:39:58<4:02:51, 19.91s/it]

 73%|███████▎  | 1954/2685 [10:40:20<4:10:01, 20.52s/it]
{'loss': 0.2978, 'learning_rate': 3.643252676984729e-07, 'rewards/chosen': -1.1300780773162842, 'rewards/rejected': -3.421322822570801, 'rewards/accuracies': 0.75, 'rewards/margins': 2.2912449836730957, 'policy_logps/rejected': -320.47064208984375, 'policy_logps/chosen': -260.3881530761719, 'referece_logps/rejected': -286.2574157714844, 'referece_logps/chosen': -249.08737182617188, 'logits/rejected': 0.038967713713645935, 'logits/chosen': 0.03885984420776367, 'epoch': 2.18}

 73%|███████▎  | 1955/2685 [10:40:40<4:08:48, 20.45s/it]

 73%|███████▎  | 1956/2685 [10:41:02<4:14:19, 20.93s/it]

 73%|███████▎  | 1957/2685 [10:41:24<4:15:58, 21.10s/it]

 73%|███████▎  | 1958/2685 [10:41:44<4:12:54, 20.87s/it]

 73%|███████▎  | 1959/2685 [10:42:06<4:14:29, 21.03s/it]


 73%|███████▎  | 1961/2685 [10:42:42<3:58:05, 19.73s/it]
{'loss': 0.1969, 'learning_rate': 3.578287270842986e-07, 'rewards/chosen': -0.6596137285232544, 'rewards/rejected': -3.6083123683929443, 'rewards/accuracies': 1.0, 'rewards/margins': 2.9486992359161377, 'policy_logps/rejected': -326.75250244140625, 'policy_logps/chosen': -307.66729736328125, 'referece_logps/rejected': -290.6693420410156, 'referece_logps/chosen': -301.0711364746094, 'logits/rejected': -0.02278028428554535, 'logits/chosen': -0.015839993953704834, 'epoch': 2.19}

 73%|███████▎  | 1962/2685 [10:42:59<3:48:09, 18.93s/it]

 73%|███████▎  | 1963/2685 [10:43:19<3:51:39, 19.25s/it]

 73%|███████▎  | 1964/2685 [10:43:40<3:54:43, 19.53s/it]

 73%|███████▎  | 1965/2685 [10:43:56<3:43:37, 18.64s/it]

 73%|███████▎  | 1966/2685 [10:44:17<3:52:38, 19.41s/it]

 73%|███████▎  | 1967/2685 [10:44:37<3:53:02, 19.47s/it]

 73%|███████▎  | 1968/2685 [10:44:55<3:49:17, 19.19s/it]

 73%|███████▎  | 1969/2685 [10:45:12<3:38:01, 18.27s/it]

 73%|███████▎  | 1970/2685 [10:45:32<3:44:02, 18.80s/it]

 73%|███████▎  | 1971/2685 [10:45:48<3:35:29, 18.11s/it]

 73%|███████▎  | 1972/2685 [10:46:09<3:45:10, 18.95s/it]

 73%|███████▎  | 1973/2685 [10:46:26<3:38:12, 18.39s/it]

 74%|███████▎  | 1974/2685 [10:46:47<3:48:31, 19.28s/it]

 74%|███████▎  | 1975/2685 [10:47:09<3:57:32, 20.07s/it]

 74%|███████▎  | 1976/2685 [10:47:30<3:58:38, 20.20s/it]

 74%|███████▎  | 1977/2685 [10:47:49<3:53:46, 19.81s/it]

 74%|███████▎  | 1978/2685 [10:48:11<4:03:46, 20.69s/it]

 74%|███████▎  | 1979/2685 [10:48:31<4:00:43, 20.46s/it]

 74%|███████▎  | 1980/2685 [10:48:51<3:57:42, 20.23s/it]

 74%|███████▍  | 1981/2685 [10:49:10<3:53:55, 19.94s/it]

 74%|███████▍  | 1982/2685 [10:49:32<4:01:06, 20.58s/it]

 74%|███████▍  | 1983/2685 [10:49:50<3:50:03, 19.66s/it]

 74%|███████▍  | 1984/2685 [10:50:08<3:45:08, 19.27s/it]


 74%|███████▍  | 1986/2685 [10:50:49<3:50:58, 19.83s/it]
{'loss': 0.1707, 'learning_rate': 3.350038810593671e-07, 'rewards/chosen': -1.3003443479537964, 'rewards/rejected': -4.224093437194824, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9237492084503174, 'policy_logps/rejected': -489.0210266113281, 'policy_logps/chosen': -464.69873046875, 'referece_logps/rejected': -446.78009033203125, 'referece_logps/chosen': -451.6952819824219, 'logits/rejected': 0.15326358377933502, 'logits/chosen': 0.24140653014183044, 'epoch': 2.22}

 74%|███████▍  | 1987/2685 [10:51:10<3:54:40, 20.17s/it]

 74%|███████▍  | 1988/2685 [10:51:31<3:58:55, 20.57s/it]

 74%|███████▍  | 1989/2685 [10:51:52<4:00:52, 20.77s/it]

 74%|███████▍  | 1990/2685 [10:52:12<3:55:52, 20.36s/it]

 74%|███████▍  | 1991/2685 [10:52:32<3:53:25, 20.18s/it]

 74%|███████▍  | 1992/2685 [10:52:54<3:59:13, 20.71s/it]

 74%|███████▍  | 1993/2685 [10:53:14<3:56:41, 20.52s/it]

 74%|███████▍  | 1994/2685 [10:53:30<3:41:08, 19.20s/it]

 74%|███████▍  | 1995/2685 [10:53:52<3:50:54, 20.08s/it]

 74%|███████▍  | 1996/2685 [10:54:10<3:44:56, 19.59s/it]

 74%|███████▍  | 1997/2685 [10:54:34<3:58:08, 20.77s/it]


 74%|███████▍  | 1999/2685 [10:55:13<3:52:12, 20.31s/it]
{'loss': 0.2357, 'learning_rate': 3.233727266962425e-07, 'rewards/chosen': -1.376030445098877, 'rewards/rejected': -3.594449520111084, 'rewards/accuracies': 0.875, 'rewards/margins': 2.218419075012207, 'policy_logps/rejected': -318.43585205078125, 'policy_logps/chosen': -371.825439453125, 'referece_logps/rejected': -282.49139404296875, 'referece_logps/chosen': -358.0651550292969, 'logits/rejected': 0.4165184497833252, 'logits/chosen': 0.21368160843849182, 'epoch': 2.23}

 74%|███████▍  | 2000/2685 [10:55:35<3:59:15, 20.96s/it]

 75%|███████▍  | 2001/2685 [10:56:12<4:50:43, 25.50s/it]

 75%|███████▍  | 2002/2685 [10:56:34<4:40:06, 24.61s/it]

 75%|███████▍  | 2003/2685 [10:56:52<4:15:23, 22.47s/it]

 75%|███████▍  | 2004/2685 [10:57:16<4:20:55, 22.99s/it]

 75%|███████▍  | 2005/2685 [10:57:34<4:05:44, 21.68s/it]

 75%|███████▍  | 2006/2685 [10:57:53<3:53:46, 20.66s/it]

 75%|███████▍  | 2007/2685 [10:58:12<3:48:07, 20.19s/it]

 75%|███████▍  | 2008/2685 [10:58:33<3:51:33, 20.52s/it]

 75%|███████▍  | 2009/2685 [10:58:53<3:48:37, 20.29s/it]


 75%|███████▍  | 2011/2685 [10:59:29<3:35:30, 19.18s/it]
{'loss': 0.3043, 'learning_rate': 3.127839355995743e-07, 'rewards/chosen': -1.3054214715957642, 'rewards/rejected': -3.4920501708984375, 'rewards/accuracies': 1.0, 'rewards/margins': 2.186628580093384, 'policy_logps/rejected': -334.3087158203125, 'policy_logps/chosen': -283.35064697265625, 'referece_logps/rejected': -299.3882141113281, 'referece_logps/chosen': -270.2964172363281, 'logits/rejected': -0.655890703201294, 'logits/chosen': -0.70097815990448, 'epoch': 2.25}

 75%|███████▍  | 2012/2685 [10:59:49<3:35:58, 19.25s/it]

 75%|███████▍  | 2013/2685 [11:00:10<3:41:01, 19.73s/it]

 75%|███████▌  | 2014/2685 [11:00:27<3:33:13, 19.07s/it]

 75%|███████▌  | 2015/2685 [11:00:42<3:18:06, 17.74s/it]

 75%|███████▌  | 2016/2685 [11:01:00<3:19:51, 17.92s/it]


 75%|███████▌  | 2018/2685 [11:01:41<3:32:38, 19.13s/it]

 75%|███████▌  | 2019/2685 [11:01:57<3:21:51, 18.19s/it]
{'loss': 0.2843, 'learning_rate': 3.0580462484584455e-07, 'rewards/chosen': -0.73781818151474, 'rewards/rejected': -3.299525022506714, 'rewards/accuracies': 0.875, 'rewards/margins': 2.561707019805908, 'policy_logps/rejected': -381.51568603515625, 'policy_logps/chosen': -347.3276672363281, 'referece_logps/rejected': -348.5203857421875, 'referece_logps/chosen': -339.949462890625, 'logits/rejected': -0.9235701560974121, 'logits/chosen': -0.9652887582778931, 'epoch': 2.26}


 75%|███████▌  | 2021/2685 [11:02:29<3:08:01, 16.99s/it]
{'loss': 0.2823, 'learning_rate': 3.0406987595818576e-07, 'rewards/chosen': -1.3938697576522827, 'rewards/rejected': -4.14529275894165, 'rewards/accuracies': 1.0, 'rewards/margins': 2.751422643661499, 'policy_logps/rejected': -467.70855712890625, 'policy_logps/chosen': -414.5457763671875, 'referece_logps/rejected': -426.255615234375, 'referece_logps/chosen': -400.60711669921875, 'logits/rejected': -0.29719752073287964, 'logits/chosen': -0.4097963571548462, 'epoch': 2.26}

 75%|███████▌  | 2022/2685 [11:02:49<3:16:20, 17.77s/it]

 75%|███████▌  | 2023/2685 [11:03:08<3:19:21, 18.07s/it]

 75%|███████▌  | 2024/2685 [11:03:27<3:22:52, 18.42s/it]

 75%|███████▌  | 2025/2685 [11:03:49<3:33:49, 19.44s/it]

 75%|███████▌  | 2026/2685 [11:04:11<3:42:55, 20.30s/it]


 76%|███████▌  | 2028/2685 [11:04:48<3:32:12, 19.38s/it]
{'loss': 0.2827, 'learning_rate': 2.9803022849099515e-07, 'rewards/chosen': -1.435826301574707, 'rewards/rejected': -3.528261184692383, 'rewards/accuracies': 0.625, 'rewards/margins': 2.092435121536255, 'policy_logps/rejected': -535.1747436523438, 'policy_logps/chosen': -467.636474609375, 'referece_logps/rejected': -499.8921203613281, 'referece_logps/chosen': -453.27825927734375, 'logits/rejected': 0.7722791433334351, 'logits/chosen': 0.7094476222991943, 'epoch': 2.27}

 76%|███████▌  | 2029/2685 [11:05:09<3:38:24, 19.98s/it]

 76%|███████▌  | 2030/2685 [11:05:29<3:37:25, 19.92s/it]

 76%|███████▌  | 2031/2685 [11:05:51<3:45:27, 20.68s/it]

 76%|███████▌  | 2032/2685 [11:06:12<3:45:06, 20.68s/it]

 76%|███████▌  | 2033/2685 [11:06:29<3:34:41, 19.76s/it]

 76%|███████▌  | 2034/2685 [11:06:49<3:32:19, 19.57s/it]

 76%|███████▌  | 2035/2685 [11:07:08<3:32:25, 19.61s/it]

 76%|███████▌  | 2036/2685 [11:07:27<3:30:02, 19.42s/it]

 76%|███████▌  | 2037/2685 [11:07:48<3:34:53, 19.90s/it]

 76%|███████▌  | 2038/2685 [11:08:08<3:35:23, 19.98s/it]


 76%|███████▌  | 2040/2685 [11:08:44<3:24:08, 18.99s/it]
{'loss': 0.2214, 'learning_rate': 2.877932604309369e-07, 'rewards/chosen': -1.0895613431930542, 'rewards/rejected': -3.7022485733032227, 'rewards/accuracies': 1.0, 'rewards/margins': 2.612687349319458, 'policy_logps/rejected': -403.61041259765625, 'policy_logps/chosen': -320.60345458984375, 'referece_logps/rejected': -366.5879211425781, 'referece_logps/chosen': -309.7078552246094, 'logits/rejected': -0.3897332549095154, 'logits/chosen': -0.5167759656906128, 'epoch': 2.28}

 76%|███████▌  | 2041/2685 [11:09:03<3:25:16, 19.13s/it]


 76%|███████▌  | 2043/2685 [11:09:42<3:25:58, 19.25s/it]
{'loss': 0.3364, 'learning_rate': 2.852572590608735e-07, 'rewards/chosen': -2.1343841552734375, 'rewards/rejected': -5.161440372467041, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0270559787750244, 'policy_logps/rejected': -613.8837890625, 'policy_logps/chosen': -460.46636962890625, 'referece_logps/rejected': -562.2693481445312, 'referece_logps/chosen': -439.12249755859375, 'logits/rejected': 0.48483794927597046, 'logits/chosen': 0.5422065258026123, 'epoch': 2.28}


 76%|███████▌  | 2045/2685 [11:10:20<3:23:43, 19.10s/it]
{'loss': 0.3076, 'learning_rate': 2.8357179103661165e-07, 'rewards/chosen': -0.7653275728225708, 'rewards/rejected': -3.7737390995025635, 'rewards/accuracies': 0.875, 'rewards/margins': 3.008411169052124, 'policy_logps/rejected': -394.18463134765625, 'policy_logps/chosen': -330.7049560546875, 'referece_logps/rejected': -356.447265625, 'referece_logps/chosen': -323.05169677734375, 'logits/rejected': 0.4497637152671814, 'logits/chosen': 0.3922058939933777, 'epoch': 2.28}

 76%|███████▌  | 2046/2685 [11:10:40<3:25:56, 19.34s/it]

 76%|███████▌  | 2047/2685 [11:11:01<3:30:47, 19.82s/it]

 76%|███████▋  | 2048/2685 [11:11:21<3:32:43, 20.04s/it]

 76%|███████▋  | 2049/2685 [11:11:41<3:31:56, 19.99s/it]

 76%|███████▋  | 2050/2685 [11:11:57<3:18:59, 18.80s/it]

 76%|███████▋  | 2051/2685 [11:12:15<3:15:00, 18.45s/it]

 76%|███████▋  | 2052/2685 [11:12:37<3:24:55, 19.42s/it]

 76%|███████▋  | 2053/2685 [11:12:56<3:24:30, 19.41s/it]

 76%|███████▋  | 2054/2685 [11:13:14<3:21:13, 19.13s/it]

 77%|███████▋  | 2055/2685 [11:13:32<3:15:35, 18.63s/it]

 77%|███████▋  | 2056/2685 [11:13:50<3:13:11, 18.43s/it]

 77%|███████▋  | 2057/2685 [11:14:11<3:20:38, 19.17s/it]

 77%|███████▋  | 2058/2685 [11:14:27<3:10:36, 18.24s/it]

 77%|███████▋  | 2059/2685 [11:14:48<3:20:35, 19.23s/it]

 77%|███████▋  | 2060/2685 [11:15:07<3:18:39, 19.07s/it]

 77%|███████▋  | 2061/2685 [11:15:28<3:25:08, 19.73s/it]

 77%|███████▋  | 2062/2685 [11:15:49<3:27:58, 20.03s/it]

 77%|███████▋  | 2063/2685 [11:16:07<3:22:42, 19.55s/it]

 77%|███████▋  | 2064/2685 [11:16:29<3:27:11, 20.02s/it]

 77%|███████▋  | 2065/2685 [11:16:48<3:25:37, 19.90s/it]

 77%|███████▋  | 2066/2685 [11:17:07<3:22:58, 19.67s/it]

 77%|███████▋  | 2067/2685 [11:17:29<3:29:50, 20.37s/it]

 77%|███████▋  | 2068/2685 [11:17:51<3:34:21, 20.85s/it]

 77%|███████▋  | 2069/2685 [11:18:12<3:33:28, 20.79s/it]

 77%|███████▋  | 2070/2685 [11:18:32<3:30:05, 20.50s/it]

 77%|███████▋  | 2071/2685 [11:18:52<3:27:31, 20.28s/it]


 77%|███████▋  | 2073/2685 [11:19:26<3:07:52, 18.42s/it]
{'loss': 0.289, 'learning_rate': 2.6041762426715563e-07, 'rewards/chosen': -1.666507363319397, 'rewards/rejected': -2.9981532096862793, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3316457271575928, 'policy_logps/rejected': -295.54351806640625, 'policy_logps/chosen': -350.52069091796875, 'referece_logps/rejected': -265.56201171875, 'referece_logps/chosen': -333.8556213378906, 'logits/rejected': -0.407190203666687, 'logits/chosen': -0.20215457677841187, 'epoch': 2.32}


 77%|███████▋  | 2075/2685 [11:20:02<3:08:15, 18.52s/it]

 77%|███████▋  | 2076/2685 [11:20:23<3:13:20, 19.05s/it]

 77%|███████▋  | 2077/2685 [11:20:42<3:14:51, 19.23s/it]

 77%|███████▋  | 2078/2685 [11:21:02<3:16:55, 19.47s/it]

 77%|███████▋  | 2079/2685 [11:21:22<3:17:02, 19.51s/it]

 77%|███████▋  | 2080/2685 [11:21:41<3:15:40, 19.41s/it]

 78%|███████▊  | 2081/2685 [11:22:02<3:18:32, 19.72s/it]
{'loss': 0.2171, 'learning_rate': 2.5395601429130874e-07, 'rewards/chosen': -0.3329961895942688, 'rewards/rejected': -1.5447335243225098, 'rewards/accuracies': 1.0, 'rewards/margins': 1.2117373943328857, 'policy_logps/rejected': -208.791015625, 'policy_logps/chosen': -202.24948120117188, 'referece_logps/rejected': -193.34368896484375, 'referece_logps/chosen': -198.9195098876953, 'logits/rejected': -0.7627617120742798, 'logits/chosen': -0.9453102350234985, 'epoch': 2.33}

 78%|███████▊  | 2082/2685 [11:22:21<3:17:32, 19.66s/it]


 78%|███████▊  | 2084/2685 [11:23:01<3:17:51, 19.75s/it]

 78%|███████▊  | 2085/2685 [11:23:20<3:15:46, 19.58s/it]

 78%|███████▊  | 2086/2685 [11:23:40<3:15:32, 19.59s/it]

 78%|███████▊  | 2087/2685 [11:23:59<3:15:43, 19.64s/it]

 78%|███████▊  | 2088/2685 [11:24:23<3:26:07, 20.72s/it]

 78%|███████▊  | 2089/2685 [11:24:43<3:24:28, 20.59s/it]

 78%|███████▊  | 2090/2685 [11:25:05<3:28:29, 21.02s/it]

 78%|███████▊  | 2091/2685 [11:25:25<3:25:23, 20.75s/it]

 78%|███████▊  | 2092/2685 [11:25:38<3:02:42, 18.49s/it]

 78%|███████▊  | 2093/2685 [11:25:57<3:02:50, 18.53s/it]

 78%|███████▊  | 2094/2685 [11:26:14<2:59:56, 18.27s/it]

 78%|███████▊  | 2095/2685 [11:26:36<3:10:15, 19.35s/it]

 78%|███████▊  | 2096/2685 [11:26:55<3:07:02, 19.05s/it]

 78%|███████▊  | 2097/2685 [11:27:14<3:07:02, 19.09s/it]

 78%|███████▊  | 2098/2685 [11:27:34<3:11:04, 19.53s/it]

 78%|███████▊  | 2099/2685 [11:27:51<3:01:17, 18.56s/it]

 78%|███████▊  | 2100/2685 [11:28:10<3:03:39, 18.84s/it]

 78%|███████▊  | 2101/2685 [11:28:30<3:07:07, 19.22s/it]

 78%|███████▊  | 2102/2685 [11:28:51<3:10:35, 19.62s/it]

 78%|███████▊  | 2103/2685 [11:29:11<3:10:46, 19.67s/it]

 78%|███████▊  | 2104/2685 [11:29:30<3:09:59, 19.62s/it]

 78%|███████▊  | 2105/2685 [11:29:48<3:04:10, 19.05s/it]

 78%|███████▊  | 2106/2685 [11:30:09<3:10:06, 19.70s/it]

 78%|███████▊  | 2107/2685 [11:30:26<3:02:50, 18.98s/it]

 79%|███████▊  | 2108/2685 [11:30:46<3:04:33, 19.19s/it]

 79%|███████▊  | 2109/2685 [11:31:04<2:59:33, 18.70s/it]

 79%|███████▊  | 2110/2685 [11:31:25<3:07:43, 19.59s/it]

 79%|███████▊  | 2111/2685 [11:31:47<3:12:51, 20.16s/it]
{'loss': 0.2373, 'learning_rate': 2.303487620950677e-07, 'rewards/chosen': -2.697794198989868, 'rewards/rejected': -5.543042182922363, 'rewards/accuracies': 1.0, 'rewards/margins': 2.845247745513916, 'policy_logps/rejected': -573.5361328125, 'policy_logps/chosen': -558.5379638671875, 'referece_logps/rejected': -518.105712890625, 'referece_logps/chosen': -531.5599975585938, 'logits/rejected': 0.19295743107795715, 'logits/chosen': 0.11354535818099976, 'epoch': 2.36}


 79%|███████▊  | 2113/2685 [11:32:27<3:12:16, 20.17s/it]

 79%|███████▊  | 2114/2685 [11:32:47<3:10:36, 20.03s/it]

 79%|███████▉  | 2115/2685 [11:33:05<3:05:30, 19.53s/it]
{'loss': 0.3047, 'learning_rate': 2.2727663686743382e-07, 'rewards/chosen': -1.2615598440170288, 'rewards/rejected': -2.9052586555480957, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6436989307403564, 'policy_logps/rejected': -352.1564025878906, 'policy_logps/chosen': -338.2049560546875, 'referece_logps/rejected': -323.1037902832031, 'referece_logps/chosen': -325.5893859863281, 'logits/rejected': -0.6781252026557922, 'logits/chosen': -0.6698572039604187, 'epoch': 2.36}


 79%|███████▉  | 2117/2685 [11:33:45<3:06:51, 19.74s/it]

 79%|███████▉  | 2118/2685 [11:34:02<2:58:53, 18.93s/it]
{'loss': 0.2469, 'learning_rate': 2.2498434851654125e-07, 'rewards/chosen': -1.2944543361663818, 'rewards/rejected': -4.710529327392578, 'rewards/accuracies': 0.875, 'rewards/margins': 3.4160752296447754, 'policy_logps/rejected': -601.1226806640625, 'policy_logps/chosen': -396.3905029296875, 'referece_logps/rejected': -554.017333984375, 'referece_logps/chosen': -383.44598388671875, 'logits/rejected': 0.0008761081844568253, 'logits/chosen': -0.1748325228691101, 'epoch': 2.37}


 79%|███████▉  | 2120/2685 [11:34:41<2:57:15, 18.82s/it]

 79%|███████▉  | 2121/2685 [11:35:01<3:00:44, 19.23s/it]

 79%|███████▉  | 2122/2685 [11:35:21<3:02:30, 19.45s/it]

 79%|███████▉  | 2123/2685 [11:35:39<2:57:50, 18.99s/it]

 79%|███████▉  | 2124/2685 [11:35:57<2:56:35, 18.89s/it]

 79%|███████▉  | 2125/2685 [11:36:17<2:58:53, 19.17s/it]

 79%|███████▉  | 2126/2685 [11:36:37<3:00:01, 19.32s/it]

 79%|███████▉  | 2127/2685 [11:36:53<2:50:34, 18.34s/it]

 79%|███████▉  | 2128/2685 [11:37:10<2:48:00, 18.10s/it]

 79%|███████▉  | 2129/2685 [11:37:29<2:48:41, 18.20s/it]

 79%|███████▉  | 2130/2685 [11:37:49<2:54:57, 18.91s/it]

 79%|███████▉  | 2131/2685 [11:38:09<2:56:33, 19.12s/it]

 79%|███████▉  | 2132/2685 [11:38:29<2:57:40, 19.28s/it]

 79%|███████▉  | 2133/2685 [11:38:49<2:59:38, 19.53s/it]

 79%|███████▉  | 2134/2685 [11:39:10<3:04:45, 20.12s/it]
{'loss': 0.2167, 'learning_rate': 2.1293100060169366e-07, 'rewards/chosen': -1.6442592144012451, 'rewards/rejected': -5.386992454528809, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7427330017089844, 'policy_logps/rejected': -461.41424560546875, 'policy_logps/chosen': -484.22906494140625, 'referece_logps/rejected': -407.5443115234375, 'referece_logps/chosen': -467.7864685058594, 'logits/rejected': -0.06020721048116684, 'logits/chosen': -0.10418182611465454, 'epoch': 2.38}

 80%|███████▉  | 2135/2685 [11:39:30<3:03:34, 20.03s/it]

 80%|███████▉  | 2136/2685 [11:39:50<3:02:46, 19.97s/it]


 80%|███████▉  | 2138/2685 [11:40:28<2:57:01, 19.42s/it]

 80%|███████▉  | 2139/2685 [11:40:47<2:55:44, 19.31s/it]
{'loss': 0.2643, 'learning_rate': 2.0922426306230157e-07, 'rewards/chosen': -1.3198986053466797, 'rewards/rejected': -4.8820600509643555, 'rewards/accuracies': 1.0, 'rewards/margins': 3.562161922454834, 'policy_logps/rejected': -461.3486328125, 'policy_logps/chosen': -400.5655212402344, 'referece_logps/rejected': -412.5280456542969, 'referece_logps/chosen': -387.3665771484375, 'logits/rejected': -0.15227589011192322, 'logits/chosen': -0.25083670020103455, 'epoch': 2.39}


 80%|███████▉  | 2141/2685 [11:41:25<2:55:26, 19.35s/it]

 80%|███████▉  | 2142/2685 [11:41:45<2:55:37, 19.41s/it]

 80%|███████▉  | 2143/2685 [11:42:00<2:43:10, 18.06s/it]

 80%|███████▉  | 2144/2685 [11:42:22<2:53:12, 19.21s/it]
{'loss': 0.2922, 'learning_rate': 2.0554630015721343e-07, 'rewards/chosen': -1.401208519935608, 'rewards/rejected': -3.2177772521972656, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8165688514709473, 'policy_logps/rejected': -383.874267578125, 'policy_logps/chosen': -255.6432342529297, 'referece_logps/rejected': -351.69647216796875, 'referece_logps/chosen': -241.6311492919922, 'logits/rejected': -0.7216507792472839, 'logits/chosen': -0.7482591867446899, 'epoch': 2.4}


 80%|███████▉  | 2146/2685 [11:43:05<3:02:30, 20.32s/it]

 80%|███████▉  | 2147/2685 [11:43:28<3:08:20, 21.00s/it]

 80%|████████  | 2148/2685 [11:43:51<3:15:41, 21.86s/it]
{'loss': 0.2493, 'learning_rate': 2.0262473754104926e-07, 'rewards/chosen': -0.9163302779197693, 'rewards/rejected': -3.941143274307251, 'rewards/accuracies': 0.875, 'rewards/margins': 3.024813175201416, 'policy_logps/rejected': -366.95361328125, 'policy_logps/chosen': -318.28631591796875, 'referece_logps/rejected': -327.5422058105469, 'referece_logps/chosen': -309.1230163574219, 'logits/rejected': -0.25350770354270935, 'logits/chosen': -0.2678017318248749, 'epoch': 2.4}


 80%|████████  | 2150/2685 [11:44:31<3:05:44, 20.83s/it]

 80%|████████  | 2151/2685 [11:44:53<3:07:50, 21.11s/it]
{'loss': 0.3143, 'learning_rate': 2.0044574809393543e-07, 'rewards/chosen': -1.6466469764709473, 'rewards/rejected': -3.445307970046997, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7986609935760498, 'policy_logps/rejected': -296.73968505859375, 'policy_logps/chosen': -311.748046875, 'referece_logps/rejected': -262.2865905761719, 'referece_logps/chosen': -295.2815856933594, 'logits/rejected': -0.9965652227401733, 'logits/chosen': -0.9057313203811646, 'epoch': 2.4}

 80%|████████  | 2152/2685 [11:45:10<2:56:36, 19.88s/it]

 80%|████████  | 2153/2685 [11:45:29<2:52:26, 19.45s/it]

 80%|████████  | 2154/2685 [11:45:49<2:53:15, 19.58s/it]


 80%|████████  | 2156/2685 [11:46:30<2:57:00, 20.08s/it]
{'loss': 0.2928, 'learning_rate': 1.968373883468115e-07, 'rewards/chosen': -1.2524223327636719, 'rewards/rejected': -3.85105037689209, 'rewards/accuracies': 0.875, 'rewards/margins': 2.598628044128418, 'policy_logps/rejected': -431.5671691894531, 'policy_logps/chosen': -432.983642578125, 'referece_logps/rejected': -393.0567321777344, 'referece_logps/chosen': -420.4593811035156, 'logits/rejected': -0.2939394414424896, 'logits/chosen': -0.2735188901424408, 'epoch': 2.41}


 80%|████████  | 2158/2685 [11:47:13<3:02:39, 20.80s/it]

 80%|████████  | 2159/2685 [11:47:34<3:01:41, 20.73s/it]
{'loss': 0.3025, 'learning_rate': 1.9468639231523199e-07, 'rewards/chosen': -0.3654358685016632, 'rewards/rejected': -2.706125259399414, 'rewards/accuracies': 0.75, 'rewards/margins': 2.3406894207000732, 'policy_logps/rejected': -256.34710693359375, 'policy_logps/chosen': -220.9057159423828, 'referece_logps/rejected': -229.28585815429688, 'referece_logps/chosen': -217.25137329101562, 'logits/rejected': -0.021976595744490623, 'logits/chosen': -0.17303256690502167, 'epoch': 2.41}

 80%|████████  | 2160/2685 [11:47:53<2:55:37, 20.07s/it]

 80%|████████  | 2161/2685 [11:48:09<2:45:51, 18.99s/it]

 81%|████████  | 2162/2685 [11:48:29<2:47:41, 19.24s/it]


 81%|████████  | 2164/2685 [11:49:04<2:41:31, 18.60s/it]

 81%|████████  | 2165/2685 [11:49:26<2:49:43, 19.58s/it]
{'loss': 0.3308, 'learning_rate': 1.904160762797673e-07, 'rewards/chosen': -1.3592772483825684, 'rewards/rejected': -4.360820770263672, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0015430450439453, 'policy_logps/rejected': -561.9600219726562, 'policy_logps/chosen': -421.08282470703125, 'referece_logps/rejected': -518.3518676757812, 'referece_logps/chosen': -407.49005126953125, 'logits/rejected': 0.059078115969896317, 'logits/chosen': 0.22385503351688385, 'epoch': 2.42}

 81%|████████  | 2166/2685 [11:49:43<2:41:59, 18.73s/it]


 81%|████████  | 2168/2685 [11:50:16<2:30:26, 17.46s/it]

 81%|████████  | 2169/2685 [11:50:38<2:42:39, 18.91s/it]
{'loss': 0.2057, 'learning_rate': 1.875927523896047e-07, 'rewards/chosen': -1.0421944856643677, 'rewards/rejected': -3.9940967559814453, 'rewards/accuracies': 0.875, 'rewards/margins': 2.9519026279449463, 'policy_logps/rejected': -454.0301513671875, 'policy_logps/chosen': -415.41351318359375, 'referece_logps/rejected': -414.08917236328125, 'referece_logps/chosen': -404.9915466308594, 'logits/rejected': 0.11284617334604263, 'logits/chosen': 0.052310436964035034, 'epoch': 2.42}


 81%|████████  | 2171/2685 [11:51:18<2:46:33, 19.44s/it]

 81%|████████  | 2172/2685 [11:51:38<2:46:06, 19.43s/it]
{'loss': 0.2643, 'learning_rate': 1.8548767184498826e-07, 'rewards/chosen': -0.33749836683273315, 'rewards/rejected': -2.67689847946167, 'rewards/accuracies': 0.875, 'rewards/margins': 2.339400053024292, 'policy_logps/rejected': -228.38243103027344, 'policy_logps/chosen': -186.203369140625, 'referece_logps/rejected': -201.61343383789062, 'referece_logps/chosen': -182.8284149169922, 'logits/rejected': -0.4782290458679199, 'logits/chosen': -0.40804964303970337, 'epoch': 2.43}


 81%|████████  | 2174/2685 [11:52:16<2:42:58, 19.14s/it]

 81%|████████  | 2175/2685 [11:52:36<2:45:22, 19.46s/it]
{'loss': 0.2996, 'learning_rate': 1.8339326112995423e-07, 'rewards/chosen': -0.6214507818222046, 'rewards/rejected': -3.237361192703247, 'rewards/accuracies': 1.0, 'rewards/margins': 2.615910530090332, 'policy_logps/rejected': -417.33660888671875, 'policy_logps/chosen': -432.4667053222656, 'referece_logps/rejected': -384.96295166015625, 'referece_logps/chosen': -426.2522277832031, 'logits/rejected': -0.5224202275276184, 'logits/chosen': -0.7437946796417236, 'epoch': 2.43}

 81%|████████  | 2176/2685 [11:52:57<2:50:36, 20.11s/it]

 81%|████████  | 2177/2685 [11:53:15<2:44:31, 19.43s/it]

 81%|████████  | 2178/2685 [11:53:35<2:45:11, 19.55s/it]


 81%|████████  | 2180/2685 [11:54:16<2:47:52, 19.95s/it]

 81%|████████  | 2181/2685 [11:54:37<2:49:41, 20.20s/it]
{'loss': 0.2354, 'learning_rate': 1.792365587927239e-07, 'rewards/chosen': -0.36443430185317993, 'rewards/rejected': -1.8284974098205566, 'rewards/accuracies': 0.875, 'rewards/margins': 1.4640631675720215, 'policy_logps/rejected': -268.79736328125, 'policy_logps/chosen': -240.136474609375, 'referece_logps/rejected': -250.51239013671875, 'referece_logps/chosen': -236.49212646484375, 'logits/rejected': -0.5354194641113281, 'logits/chosen': -0.6544538736343384, 'epoch': 2.44}

 81%|████████▏ | 2182/2685 [11:54:57<2:49:55, 20.27s/it]


 81%|████████▏ | 2184/2685 [11:55:33<2:40:35, 19.23s/it]

 81%|████████▏ | 2185/2685 [11:55:52<2:41:05, 19.33s/it]
{'loss': 0.2388, 'learning_rate': 1.7648930317530864e-07, 'rewards/chosen': -1.3751589059829712, 'rewards/rejected': -3.598515033721924, 'rewards/accuracies': 0.875, 'rewards/margins': 2.223356008529663, 'policy_logps/rejected': -400.1943359375, 'policy_logps/chosen': -335.680908203125, 'referece_logps/rejected': -364.2091979980469, 'referece_logps/chosen': -321.9293212890625, 'logits/rejected': 0.027908077463507652, 'logits/chosen': 0.07949468493461609, 'epoch': 2.44}

 81%|████████▏ | 2186/2685 [11:56:12<2:40:52, 19.34s/it]


 81%|████████▏ | 2188/2685 [11:56:53<2:45:27, 19.98s/it]

 82%|████████▏ | 2189/2685 [11:57:12<2:44:08, 19.86s/it]

 82%|████████▏ | 2190/2685 [11:57:25<2:25:50, 17.68s/it]

 82%|████████▏ | 2191/2685 [11:57:45<2:30:45, 18.31s/it]

 82%|████████▏ | 2192/2685 [11:58:02<2:28:15, 18.04s/it]

 82%|████████▏ | 2193/2685 [11:58:20<2:28:26, 18.10s/it]

 82%|████████▏ | 2194/2685 [11:58:41<2:33:33, 18.76s/it]

 82%|████████▏ | 2195/2685 [11:58:54<2:21:13, 17.29s/it]

 82%|████████▏ | 2196/2685 [11:59:14<2:27:21, 18.08s/it]

 82%|████████▏ | 2197/2685 [11:59:32<2:26:37, 18.03s/it]

 82%|████████▏ | 2198/2685 [11:59:49<2:23:09, 17.64s/it]

 82%|████████▏ | 2199/2685 [12:00:07<2:22:32, 17.60s/it]

 82%|████████▏ | 2200/2685 [12:00:29<2:33:34, 19.00s/it]

 82%|████████▏ | 2201/2685 [12:00:48<2:34:54, 19.20s/it]

 82%|████████▏ | 2202/2685 [12:01:08<2:36:05, 19.39s/it]
{'loss': 0.3848, 'learning_rate': 1.6502818756759273e-07, 'rewards/chosen': -1.0146219730377197, 'rewards/rejected': -2.914095163345337, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8994730710983276, 'policy_logps/rejected': -275.52105712890625, 'policy_logps/chosen': -309.1111755371094, 'referece_logps/rejected': -246.38011169433594, 'referece_logps/chosen': -298.9649658203125, 'logits/rejected': -0.07503393292427063, 'logits/chosen': -0.07493830472230911, 'epoch': 2.46}

 82%|████████▏ | 2203/2685 [12:01:30<2:40:54, 20.03s/it]

 82%|████████▏ | 2204/2685 [12:01:50<2:41:20, 20.12s/it]


 82%|████████▏ | 2206/2685 [12:02:27<2:33:03, 19.17s/it]
{'loss': 0.3333, 'learning_rate': 1.6238232046720156e-07, 'rewards/chosen': -1.1539030075073242, 'rewards/rejected': -5.7613983154296875, 'rewards/accuracies': 0.75, 'rewards/margins': 4.607495307922363, 'policy_logps/rejected': -475.8579406738281, 'policy_logps/chosen': -477.21014404296875, 'referece_logps/rejected': -418.24395751953125, 'referece_logps/chosen': -465.6711120605469, 'logits/rejected': -0.049288295209407806, 'logits/chosen': -0.12664856016635895, 'epoch': 2.46}

 82%|████████▏ | 2207/2685 [12:02:50<2:40:25, 20.14s/it]

 82%|████████▏ | 2208/2685 [12:03:08<2:35:35, 19.57s/it]

 82%|████████▏ | 2209/2685 [12:03:28<2:35:34, 19.61s/it]


 82%|████████▏ | 2211/2685 [12:04:09<2:40:23, 20.30s/it]

 82%|████████▏ | 2212/2685 [12:04:29<2:39:53, 20.28s/it]

 82%|████████▏ | 2213/2685 [12:04:49<2:37:54, 20.07s/it]
{'loss': 0.2549, 'learning_rate': 1.577990277362491e-07, 'rewards/chosen': -0.011921234428882599, 'rewards/rejected': -3.186267375946045, 'rewards/accuracies': 1.0, 'rewards/margins': 3.1743459701538086, 'policy_logps/rejected': -332.9220275878906, 'policy_logps/chosen': -252.68310546875, 'referece_logps/rejected': -301.0593566894531, 'referece_logps/chosen': -252.56390380859375, 'logits/rejected': -0.4644603133201599, 'logits/chosen': -0.6875530481338501, 'epoch': 2.47}

 82%|████████▏ | 2214/2685 [12:05:04<2:24:53, 18.46s/it]


 83%|████████▎ | 2216/2685 [12:05:43<2:29:11, 19.09s/it]

 83%|████████▎ | 2217/2685 [12:06:05<2:35:36, 19.95s/it]

 83%|████████▎ | 2218/2685 [12:06:25<2:35:20, 19.96s/it]

 83%|████████▎ | 2219/2685 [12:06:47<2:39:36, 20.55s/it]

 83%|████████▎ | 2220/2685 [12:07:07<2:37:23, 20.31s/it]

 83%|████████▎ | 2221/2685 [12:07:25<2:32:40, 19.74s/it]
{'loss': 0.287, 'learning_rate': 1.5263454768293904e-07, 'rewards/chosen': -0.6997268199920654, 'rewards/rejected': -4.172068119049072, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4723410606384277, 'policy_logps/rejected': -462.13287353515625, 'policy_logps/chosen': -390.8410339355469, 'referece_logps/rejected': -420.41217041015625, 'referece_logps/chosen': -383.84381103515625, 'logits/rejected': -0.3874369263648987, 'logits/chosen': -0.46292752027511597, 'epoch': 2.48}


 83%|████████▎ | 2223/2685 [12:08:07<2:35:41, 20.22s/it]

 83%|████████▎ | 2224/2685 [12:08:27<2:34:06, 20.06s/it]
{'loss': 0.2656, 'learning_rate': 1.5071819227766036e-07, 'rewards/chosen': -1.9620534181594849, 'rewards/rejected': -4.210571765899658, 'rewards/accuracies': 0.75, 'rewards/margins': 2.248518228530884, 'policy_logps/rejected': -325.8124084472656, 'policy_logps/chosen': -219.5831756591797, 'referece_logps/rejected': -283.7066955566406, 'referece_logps/chosen': -199.96266174316406, 'logits/rejected': 0.04451628401875496, 'logits/chosen': -0.12534081935882568, 'epoch': 2.48}

 83%|████████▎ | 2225/2685 [12:08:48<2:36:57, 20.47s/it]

 83%|████████▎ | 2226/2685 [12:09:09<2:36:28, 20.46s/it]

 83%|████████▎ | 2227/2685 [12:09:28<2:33:29, 20.11s/it]


 83%|████████▎ | 2229/2685 [12:10:07<2:30:57, 19.86s/it]

 83%|████████▎ | 2230/2685 [12:10:22<2:18:22, 18.25s/it]

 83%|████████▎ | 2231/2685 [12:10:41<2:21:12, 18.66s/it]
{'loss': 0.2995, 'learning_rate': 1.4629000449474004e-07, 'rewards/chosen': -1.1108129024505615, 'rewards/rejected': -5.056693077087402, 'rewards/accuracies': 1.0, 'rewards/margins': 3.945880889892578, 'policy_logps/rejected': -458.5404052734375, 'policy_logps/chosen': -403.4008483886719, 'referece_logps/rejected': -407.97344970703125, 'referece_logps/chosen': -392.2927551269531, 'logits/rejected': 0.196879580616951, 'logits/chosen': 0.2050498127937317, 'epoch': 2.49}

 83%|████████▎ | 2232/2685 [12:10:58<2:16:20, 18.06s/it]


 83%|████████▎ | 2234/2685 [12:11:39<2:23:37, 19.11s/it]

 83%|████████▎ | 2235/2685 [12:11:59<2:25:14, 19.37s/it]
{'loss': 0.2511, 'learning_rate': 1.4378692823441207e-07, 'rewards/chosen': -0.4922402501106262, 'rewards/rejected': -3.3401103019714355, 'rewards/accuracies': 1.0, 'rewards/margins': 2.847869873046875, 'policy_logps/rejected': -305.8939208984375, 'policy_logps/chosen': -249.56619262695312, 'referece_logps/rejected': -272.4927978515625, 'referece_logps/chosen': -244.64378356933594, 'logits/rejected': -0.5006818175315857, 'logits/chosen': -0.5822566747665405, 'epoch': 2.5}

 83%|████████▎ | 2236/2685 [12:12:16<2:20:34, 18.79s/it]

 83%|████████▎ | 2237/2685 [12:12:32<2:13:16, 17.85s/it]


 83%|████████▎ | 2239/2685 [12:13:08<2:12:04, 17.77s/it]
{'loss': 0.3814, 'learning_rate': 1.413037916814036e-07, 'rewards/chosen': -2.3938403129577637, 'rewards/rejected': -4.49337100982666, 'rewards/accuracies': 0.75, 'rewards/margins': 2.0995306968688965, 'policy_logps/rejected': -429.4118957519531, 'policy_logps/chosen': -393.3268737792969, 'referece_logps/rejected': -384.47821044921875, 'referece_logps/chosen': -369.38848876953125, 'logits/rejected': -0.2239552140235901, 'logits/chosen': -0.14570780098438263, 'epoch': 2.5}


 83%|████████▎ | 2241/2685 [12:13:45<2:13:56, 18.10s/it]

 84%|████████▎ | 2242/2685 [12:14:02<2:10:35, 17.69s/it]
{'loss': 0.2529, 'learning_rate': 1.3945455950417684e-07, 'rewards/chosen': -1.0629785060882568, 'rewards/rejected': -2.7655301094055176, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7025513648986816, 'policy_logps/rejected': -268.8946533203125, 'policy_logps/chosen': -196.9861297607422, 'referece_logps/rejected': -241.23934936523438, 'referece_logps/chosen': -186.35635375976562, 'logits/rejected': -0.3645125925540924, 'logits/chosen': -0.288595974445343, 'epoch': 2.51}


 84%|████████▎ | 2244/2685 [12:14:40<2:15:03, 18.38s/it]
{'loss': 0.2807, 'learning_rate': 1.3822799925645035e-07, 'rewards/chosen': -1.411198616027832, 'rewards/rejected': -4.003544330596924, 'rewards/accuracies': 0.875, 'rewards/margins': 2.592345714569092, 'policy_logps/rejected': -370.0270080566406, 'policy_logps/chosen': -357.7271423339844, 'referece_logps/rejected': -329.9916076660156, 'referece_logps/chosen': -343.6151428222656, 'logits/rejected': -0.8558059334754944, 'logits/chosen': -0.8301658630371094, 'epoch': 2.51}


 84%|████████▎ | 2246/2685 [12:15:14<2:07:53, 17.48s/it]
{'loss': 0.3417, 'learning_rate': 1.3700645630729356e-07, 'rewards/chosen': -0.11415807902812958, 'rewards/rejected': -1.6865849494934082, 'rewards/accuracies': 0.875, 'rewards/margins': 1.5724267959594727, 'policy_logps/rejected': -266.67156982421875, 'policy_logps/chosen': -368.8944396972656, 'referece_logps/rejected': -249.80572509765625, 'referece_logps/chosen': -367.75286865234375, 'logits/rejected': 0.125131756067276, 'logits/chosen': 0.13762876391410828, 'epoch': 2.51}


 84%|████████▎ | 2248/2685 [12:15:49<2:07:34, 17.52s/it]

 84%|████████▍ | 2249/2685 [12:16:08<2:09:22, 17.80s/it]
{'loss': 0.3484, 'learning_rate': 1.3518356486768444e-07, 'rewards/chosen': -0.7999905943870544, 'rewards/rejected': -3.0258326530456543, 'rewards/accuracies': 0.875, 'rewards/margins': 2.225841999053955, 'policy_logps/rejected': -231.8765411376953, 'policy_logps/chosen': -208.0802001953125, 'referece_logps/rejected': -201.61822509765625, 'referece_logps/chosen': -200.08029174804688, 'logits/rejected': -0.31213855743408203, 'logits/chosen': -0.34246179461479187, 'epoch': 2.51}

 84%|████████▍ | 2250/2685 [12:16:25<2:08:16, 17.69s/it]

 84%|████████▍ | 2251/2685 [12:16:43<2:08:01, 17.70s/it]

 84%|████████▍ | 2252/2685 [12:17:04<2:14:30, 18.64s/it]

 84%|████████▍ | 2253/2685 [12:17:18<2:05:55, 17.49s/it]

 84%|████████▍ | 2254/2685 [12:17:33<1:59:44, 16.67s/it]

 84%|████████▍ | 2255/2685 [12:17:53<2:06:43, 17.68s/it]


 84%|████████▍ | 2257/2685 [12:18:36<2:19:06, 19.50s/it]
{'loss': 0.2547, 'learning_rate': 1.303779706403142e-07, 'rewards/chosen': -0.9996681213378906, 'rewards/rejected': -2.6971840858459473, 'rewards/accuracies': 1.0, 'rewards/margins': 1.6975162029266357, 'policy_logps/rejected': -282.76361083984375, 'policy_logps/chosen': -302.35845947265625, 'referece_logps/rejected': -255.791748046875, 'referece_logps/chosen': -292.3617858886719, 'logits/rejected': -0.11877778172492981, 'logits/chosen': -0.2136320173740387, 'epoch': 2.52}

 84%|████████▍ | 2258/2685 [12:18:57<2:22:11, 19.98s/it]

 84%|████████▍ | 2259/2685 [12:19:17<2:21:50, 19.98s/it]

 84%|████████▍ | 2260/2685 [12:19:37<2:22:50, 20.17s/it]


 84%|████████▍ | 2262/2685 [12:20:14<2:15:09, 19.17s/it]

 84%|████████▍ | 2263/2685 [12:20:34<2:16:18, 19.38s/it]
{'loss': 0.2984, 'learning_rate': 1.268269119062131e-07, 'rewards/chosen': -2.0737111568450928, 'rewards/rejected': -5.070659160614014, 'rewards/accuracies': 0.75, 'rewards/margins': 2.996948480606079, 'policy_logps/rejected': -404.6495666503906, 'policy_logps/chosen': -368.07916259765625, 'referece_logps/rejected': -353.9429931640625, 'referece_logps/chosen': -347.342041015625, 'logits/rejected': -0.014812782406806946, 'logits/chosen': -0.0743364542722702, 'epoch': 2.53}

 84%|████████▍ | 2264/2685 [12:20:55<2:18:44, 19.77s/it]

 84%|████████▍ | 2265/2685 [12:21:15<2:18:24, 19.77s/it]

 84%|████████▍ | 2266/2685 [12:21:35<2:19:27, 19.97s/it]

 84%|████████▍ | 2267/2685 [12:21:57<2:24:30, 20.74s/it]


 85%|████████▍ | 2269/2685 [12:22:38<2:23:15, 20.66s/it]

 85%|████████▍ | 2270/2685 [12:22:54<2:12:48, 19.20s/it]
{'loss': 0.298, 'learning_rate': 1.2274184957765464e-07, 'rewards/chosen': -0.3999985456466675, 'rewards/rejected': -2.0147461891174316, 'rewards/accuracies': 0.875, 'rewards/margins': 1.6147476434707642, 'policy_logps/rejected': -337.93524169921875, 'policy_logps/chosen': -374.59832763671875, 'referece_logps/rejected': -317.78778076171875, 'referece_logps/chosen': -370.59832763671875, 'logits/rejected': 0.09385349601507187, 'logits/chosen': 0.11041489243507385, 'epoch': 2.54}


 85%|████████▍ | 2272/2685 [12:23:37<2:19:17, 20.24s/it]
{'loss': 0.2627, 'learning_rate': 1.2158616800496058e-07, 'rewards/chosen': -1.148528814315796, 'rewards/rejected': -2.789038896560669, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6405099630355835, 'policy_logps/rejected': -393.37896728515625, 'policy_logps/chosen': -292.6991882324219, 'referece_logps/rejected': -365.48858642578125, 'referece_logps/chosen': -281.2138977050781, 'logits/rejected': -0.12107683718204498, 'logits/chosen': -0.07979260385036469, 'epoch': 2.54}


 85%|████████▍ | 2274/2685 [12:24:20<2:24:49, 21.14s/it]
{'loss': 0.226, 'learning_rate': 1.2043560062077474e-07, 'rewards/chosen': -0.6839520335197449, 'rewards/rejected': -2.8392672538757324, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1553151607513428, 'policy_logps/rejected': -336.5538330078125, 'policy_logps/chosen': -307.4949035644531, 'referece_logps/rejected': -308.16119384765625, 'referece_logps/chosen': -300.6553649902344, 'logits/rejected': -0.6411375999450684, 'logits/chosen': -0.8098869323730469, 'epoch': 2.54}

 85%|████████▍ | 2275/2685 [12:24:39<2:19:25, 20.40s/it]


 85%|████████▍ | 2277/2685 [12:25:18<2:15:29, 19.93s/it]

 85%|████████▍ | 2278/2685 [12:25:38<2:15:31, 19.98s/it]
{'loss': 0.2544, 'learning_rate': 1.1814983518285826e-07, 'rewards/chosen': -1.4173715114593506, 'rewards/rejected': -3.1772654056549072, 'rewards/accuracies': 0.875, 'rewards/margins': 1.7598938941955566, 'policy_logps/rejected': -421.7131652832031, 'policy_logps/chosen': -465.67138671875, 'referece_logps/rejected': -389.94049072265625, 'referece_logps/chosen': -451.4976806640625, 'logits/rejected': 0.6237789988517761, 'logits/chosen': 0.554787278175354, 'epoch': 2.55}

 85%|████████▍ | 2279/2685 [12:25:58<2:14:48, 19.92s/it]

 85%|████████▍ | 2280/2685 [12:26:18<2:14:39, 19.95s/it]

 85%|████████▍ | 2281/2685 [12:26:40<2:18:10, 20.52s/it]

 85%|████████▍ | 2282/2685 [12:27:02<2:20:49, 20.97s/it]

 85%|████████▌ | 2283/2685 [12:27:24<2:23:10, 21.37s/it]

 85%|████████▌ | 2284/2685 [12:27:46<2:22:39, 21.35s/it]


 85%|████████▌ | 2286/2685 [12:28:24<2:13:48, 20.12s/it]
{'loss': 0.3082, 'learning_rate': 1.1363996731159187e-07, 'rewards/chosen': -1.06406569480896, 'rewards/rejected': -3.6702256202697754, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6061601638793945, 'policy_logps/rejected': -403.05010986328125, 'policy_logps/chosen': -305.7070007324219, 'referece_logps/rejected': -366.3478698730469, 'referece_logps/chosen': -295.0663146972656, 'logits/rejected': -0.12351176142692566, 'logits/chosen': -0.1431930512189865, 'epoch': 2.55}

 85%|████████▌ | 2287/2685 [12:28:44<2:12:59, 20.05s/it]


 85%|████████▌ | 2289/2685 [12:29:29<2:19:32, 21.14s/it]
{'loss': 0.2173, 'learning_rate': 1.1197003124950222e-07, 'rewards/chosen': -0.8162553310394287, 'rewards/rejected': -4.0929274559021, 'rewards/accuracies': 1.0, 'rewards/margins': 3.276672124862671, 'policy_logps/rejected': -595.5433349609375, 'policy_logps/chosen': -390.26739501953125, 'referece_logps/rejected': -554.6140747070312, 'referece_logps/chosen': -382.104736328125, 'logits/rejected': 0.5212866067886353, 'logits/chosen': 0.4905880391597748, 'epoch': 2.56}

 85%|████████▌ | 2290/2685 [12:29:50<2:19:05, 21.13s/it]

 85%|████████▌ | 2291/2685 [12:30:10<2:16:48, 20.83s/it]

 85%|████████▌ | 2292/2685 [12:30:27<2:10:02, 19.85s/it]


 85%|████████▌ | 2294/2685 [12:31:09<2:13:00, 20.41s/it]

 85%|████████▌ | 2295/2685 [12:31:29<2:11:31, 20.23s/it]
{'loss': 0.276, 'learning_rate': 1.0866507950420523e-07, 'rewards/chosen': -0.5306761264801025, 'rewards/rejected': -3.083258628845215, 'rewards/accuracies': 1.0, 'rewards/margins': 2.552582263946533, 'policy_logps/rejected': -279.5828857421875, 'policy_logps/chosen': -257.8016662597656, 'referece_logps/rejected': -248.75027465820312, 'referece_logps/chosen': -252.49490356445312, 'logits/rejected': -0.24068382382392883, 'logits/chosen': -0.3732396364212036, 'epoch': 2.56}


 86%|████████▌ | 2297/2685 [12:32:09<2:11:12, 20.29s/it]
{'loss': 0.3454, 'learning_rate': 1.0757379923277665e-07, 'rewards/chosen': -0.3393498361110687, 'rewards/rejected': -2.715122699737549, 'rewards/accuracies': 0.75, 'rewards/margins': 2.375772714614868, 'policy_logps/rejected': -501.84002685546875, 'policy_logps/chosen': -406.7156066894531, 'referece_logps/rejected': -474.68878173828125, 'referece_logps/chosen': -403.32208251953125, 'logits/rejected': 0.11657333374023438, 'logits/chosen': 0.13797569274902344, 'epoch': 2.57}

 86%|████████▌ | 2298/2685 [12:32:30<2:10:56, 20.30s/it]

 86%|████████▌ | 2299/2685 [12:32:50<2:10:01, 20.21s/it]


 86%|████████▌ | 2301/2685 [12:33:33<2:14:09, 20.96s/it]
{'loss': 0.2525, 'learning_rate': 1.0540683232176307e-07, 'rewards/chosen': -1.1960363388061523, 'rewards/rejected': -4.0499701499938965, 'rewards/accuracies': 0.75, 'rewards/margins': 2.8539340496063232, 'policy_logps/rejected': -417.6254577636719, 'policy_logps/chosen': -432.2329406738281, 'referece_logps/rejected': -377.125732421875, 'referece_logps/chosen': -420.27252197265625, 'logits/rejected': 0.4367339015007019, 'logits/chosen': 0.3833102583885193, 'epoch': 2.57}

 86%|████████▌ | 2302/2685 [12:33:54<2:14:45, 21.11s/it]


 86%|████████▌ | 2304/2685 [12:34:32<2:04:23, 19.59s/it]

 86%|████████▌ | 2305/2685 [12:34:53<2:08:31, 20.29s/it]

 86%|████████▌ | 2306/2685 [12:35:13<2:07:15, 20.15s/it]
{'loss': 0.2217, 'learning_rate': 1.0272742667795109e-07, 'rewards/chosen': -1.9893285036087036, 'rewards/rejected': -5.5089616775512695, 'rewards/accuracies': 1.0, 'rewards/margins': 3.5196332931518555, 'policy_logps/rejected': -436.6609191894531, 'policy_logps/chosen': -395.5191650390625, 'referece_logps/rejected': -381.5712890625, 'referece_logps/chosen': -375.6258850097656, 'logits/rejected': -0.2091289758682251, 'logits/chosen': -0.09080188721418381, 'epoch': 2.58}


 86%|████████▌ | 2308/2685 [12:35:46<1:55:52, 18.44s/it]

 86%|████████▌ | 2309/2685 [12:36:05<1:57:45, 18.79s/it]

 86%|████████▌ | 2310/2685 [12:36:25<1:59:06, 19.06s/it]

 86%|████████▌ | 2311/2685 [12:36:46<2:01:38, 19.51s/it]
{'loss': 0.3087, 'learning_rate': 1.0008067086007877e-07, 'rewards/chosen': -1.3253501653671265, 'rewards/rejected': -3.7678675651550293, 'rewards/accuracies': 0.875, 'rewards/margins': 2.4425177574157715, 'policy_logps/rejected': -436.3365173339844, 'policy_logps/chosen': -392.222412109375, 'referece_logps/rejected': -398.6578369140625, 'referece_logps/chosen': -378.96893310546875, 'logits/rejected': -0.5227060914039612, 'logits/chosen': -0.6122308969497681, 'epoch': 2.58}


 86%|████████▌ | 2313/2685 [12:37:28<2:04:42, 20.11s/it]
{'loss': 0.3044, 'learning_rate': 9.903113209758096e-08, 'rewards/chosen': -1.7630900144577026, 'rewards/rejected': -3.0349717140197754, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2718818187713623, 'policy_logps/rejected': -436.0299072265625, 'policy_logps/chosen': -356.4783630371094, 'referece_logps/rejected': -405.68017578125, 'referece_logps/chosen': -338.8474426269531, 'logits/rejected': 0.33764758706092834, 'logits/chosen': 0.33413538336753845, 'epoch': 2.58}

 86%|████████▌ | 2314/2685 [12:37:48<2:05:06, 20.23s/it]


 86%|████████▋ | 2316/2685 [12:38:33<2:11:39, 21.41s/it]
{'loss': 0.2755, 'learning_rate': 9.74666611779179e-08, 'rewards/chosen': -0.7375612258911133, 'rewards/rejected': -3.081042766571045, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3434817790985107, 'policy_logps/rejected': -338.4905700683594, 'policy_logps/chosen': -268.3737487792969, 'referece_logps/rejected': -307.68017578125, 'referece_logps/chosen': -260.9981689453125, 'logits/rejected': -0.9576247334480286, 'logits/chosen': -0.8968667387962341, 'epoch': 2.59}

 86%|████████▋ | 2317/2685 [12:38:53<2:08:04, 20.88s/it]


 86%|████████▋ | 2319/2685 [12:39:27<1:55:49, 18.99s/it]
{'loss': 0.3217, 'learning_rate': 9.591401313261139e-08, 'rewards/chosen': -1.5940141677856445, 'rewards/rejected': -3.391655683517456, 'rewards/accuracies': 0.875, 'rewards/margins': 1.797641396522522, 'policy_logps/rejected': -359.80841064453125, 'policy_logps/chosen': -338.3816833496094, 'referece_logps/rejected': -325.891845703125, 'referece_logps/chosen': -322.4415588378906, 'logits/rejected': -0.5334872007369995, 'logits/chosen': -0.3827708959579468, 'epoch': 2.59}

 86%|████████▋ | 2320/2685 [12:39:47<1:56:26, 19.14s/it]

 86%|████████▋ | 2321/2685 [12:40:07<1:57:55, 19.44s/it]


 87%|████████▋ | 2323/2685 [12:40:48<2:00:28, 19.97s/it]

 87%|████████▋ | 2324/2685 [12:41:04<1:52:25, 18.69s/it]

 87%|████████▋ | 2325/2685 [12:41:22<1:50:37, 18.44s/it]
{'loss': 0.2769, 'learning_rate': 9.284426686653302e-08, 'rewards/chosen': -1.1326574087142944, 'rewards/rejected': -4.864108562469482, 'rewards/accuracies': 1.0, 'rewards/margins': 3.7314515113830566, 'policy_logps/rejected': -454.5721130371094, 'policy_logps/chosen': -512.5066528320312, 'referece_logps/rejected': -405.9309997558594, 'referece_logps/chosen': -501.1800537109375, 'logits/rejected': -0.5184974074363708, 'logits/chosen': -0.5116243362426758, 'epoch': 2.6}

 87%|████████▋ | 2326/2685 [12:41:41<1:51:44, 18.68s/it]


 87%|████████▋ | 2328/2685 [12:42:20<1:52:40, 18.94s/it]
{'loss': 0.327, 'learning_rate': 9.132720885837508e-08, 'rewards/chosen': -2.307966947555542, 'rewards/rejected': -4.158649444580078, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8506821393966675, 'policy_logps/rejected': -485.9139099121094, 'policy_logps/chosen': -527.711181640625, 'referece_logps/rejected': -444.327392578125, 'referece_logps/chosen': -504.6314392089844, 'logits/rejected': -0.13898345828056335, 'logits/chosen': -0.06320823729038239, 'epoch': 2.6}

 87%|████████▋ | 2329/2685 [12:42:39<1:53:36, 19.15s/it]

 87%|████████▋ | 2330/2685 [12:42:59<1:54:13, 19.30s/it]

 87%|████████▋ | 2331/2685 [12:43:19<1:54:33, 19.42s/it]


 87%|████████▋ | 2333/2685 [12:43:56<1:50:22, 18.81s/it]

 87%|████████▋ | 2334/2685 [12:44:16<1:52:57, 19.31s/it]

 87%|████████▋ | 2335/2685 [12:44:36<1:53:09, 19.40s/it]

 87%|████████▋ | 2336/2685 [12:44:54<1:50:50, 19.05s/it]
{'loss': 0.2766, 'learning_rate': 8.733996822478684e-08, 'rewards/chosen': -2.460777521133423, 'rewards/rejected': -3.6109020709991455, 'rewards/accuracies': 0.75, 'rewards/margins': 1.150124192237854, 'policy_logps/rejected': -345.67840576171875, 'policy_logps/chosen': -377.370361328125, 'referece_logps/rejected': -309.5694274902344, 'referece_logps/chosen': -352.7625732421875, 'logits/rejected': -0.1590147316455841, 'logits/chosen': -0.156926691532135, 'epoch': 2.61}


 87%|████████▋ | 2338/2685 [12:45:34<1:53:22, 19.60s/it]
{'loss': 0.2487, 'learning_rate': 8.635642756568518e-08, 'rewards/chosen': -0.9198921918869019, 'rewards/rejected': -2.7354772090911865, 'rewards/accuracies': 0.875, 'rewards/margins': 1.8155851364135742, 'policy_logps/rejected': -332.1623229980469, 'policy_logps/chosen': -362.7726135253906, 'referece_logps/rejected': -304.80755615234375, 'referece_logps/chosen': -353.57366943359375, 'logits/rejected': -0.9013928174972534, 'logits/chosen': -0.919992208480835, 'epoch': 2.61}


 87%|████████▋ | 2340/2685 [12:46:14<1:53:38, 19.76s/it]

 87%|████████▋ | 2341/2685 [12:46:32<1:50:02, 19.19s/it]
{'loss': 0.3681, 'learning_rate': 8.48910920428747e-08, 'rewards/chosen': -2.03157377243042, 'rewards/rejected': -5.6643524169921875, 'rewards/accuracies': 0.75, 'rewards/margins': 3.6327784061431885, 'policy_logps/rejected': -527.47119140625, 'policy_logps/chosen': -496.39013671875, 'referece_logps/rejected': -470.8276672363281, 'referece_logps/chosen': -476.07440185546875, 'logits/rejected': -0.07655729353427887, 'logits/chosen': 0.007973273284733295, 'epoch': 2.62}


 87%|████████▋ | 2343/2685 [12:47:14<1:54:51, 20.15s/it]

 87%|████████▋ | 2344/2685 [12:47:36<1:57:16, 20.64s/it]

 87%|████████▋ | 2345/2685 [12:47:58<1:59:48, 21.14s/it]

 87%|████████▋ | 2346/2685 [12:48:18<1:57:44, 20.84s/it]
{'loss': 0.2847, 'learning_rate': 8.247551471289371e-08, 'rewards/chosen': -1.9877574443817139, 'rewards/rejected': -4.670018196105957, 'rewards/accuracies': 0.875, 'rewards/margins': 2.682260274887085, 'policy_logps/rejected': -471.00518798828125, 'policy_logps/chosen': -451.277587890625, 'referece_logps/rejected': -424.3050231933594, 'referece_logps/chosen': -431.3999938964844, 'logits/rejected': -0.2961830198764801, 'logits/chosen': -0.4436357617378235, 'epoch': 2.62}

 87%|████████▋ | 2347/2685 [12:48:41<2:00:32, 21.40s/it]

 87%|████████▋ | 2348/2685 [12:49:02<1:59:40, 21.31s/it]


 88%|████████▊ | 2350/2685 [12:49:38<1:49:04, 19.54s/it]
{'loss': 0.4032, 'learning_rate': 8.056708713033567e-08, 'rewards/chosen': -1.003735899925232, 'rewards/rejected': -2.951558828353882, 'rewards/accuracies': 0.625, 'rewards/margins': 1.9478228092193604, 'policy_logps/rejected': -316.72039794921875, 'policy_logps/chosen': -267.22857666015625, 'referece_logps/rejected': -287.2048034667969, 'referece_logps/chosen': -257.191162109375, 'logits/rejected': -0.5055878758430481, 'logits/chosen': -0.49421998858451843, 'epoch': 2.63}

 88%|████████▊ | 2351/2685 [12:49:56<1:45:34, 18.97s/it]

 88%|████████▊ | 2352/2685 [12:50:17<1:49:08, 19.66s/it]


 88%|████████▊ | 2354/2685 [12:50:57<1:48:44, 19.71s/it]
{'loss': 0.2226, 'learning_rate': 7.868007153480139e-08, 'rewards/chosen': -1.7055609226226807, 'rewards/rejected': -5.511757850646973, 'rewards/accuracies': 1.0, 'rewards/margins': 3.806196928024292, 'policy_logps/rejected': -535.6675415039062, 'policy_logps/chosen': -537.70654296875, 'referece_logps/rejected': -480.5499267578125, 'referece_logps/chosen': -520.6510009765625, 'logits/rejected': 0.11627775430679321, 'logits/chosen': 0.054423216730356216, 'epoch': 2.63}

 88%|████████▊ | 2355/2685 [12:51:17<1:49:17, 19.87s/it]

 88%|████████▊ | 2356/2685 [12:51:38<1:51:09, 20.27s/it]

 88%|████████▊ | 2357/2685 [12:52:00<1:53:51, 20.83s/it]

 88%|████████▊ | 2358/2685 [12:52:17<1:47:11, 19.67s/it]


 88%|████████▊ | 2360/2685 [12:52:55<1:44:02, 19.21s/it]
{'loss': 0.3583, 'learning_rate': 7.588979161090814e-08, 'rewards/chosen': -2.3484487533569336, 'rewards/rejected': -3.950883626937866, 'rewards/accuracies': 0.75, 'rewards/margins': 1.6024348735809326, 'policy_logps/rejected': -427.0390625, 'policy_logps/chosen': -407.8149108886719, 'referece_logps/rejected': -387.5301818847656, 'referece_logps/chosen': -384.3304443359375, 'logits/rejected': -0.6152656078338623, 'logits/chosen': -0.6554927825927734, 'epoch': 2.64}


 88%|████████▊ | 2362/2685 [12:53:37<1:49:19, 20.31s/it]
{'loss': 0.2536, 'learning_rate': 7.497045158632553e-08, 'rewards/chosen': -1.0744110345840454, 'rewards/rejected': -5.0677714347839355, 'rewards/accuracies': 1.0, 'rewards/margins': 3.9933602809906006, 'policy_logps/rejected': -611.7542114257812, 'policy_logps/chosen': -453.494873046875, 'referece_logps/rejected': -561.0764770507812, 'referece_logps/chosen': -442.7507629394531, 'logits/rejected': 0.2962122857570648, 'logits/chosen': 0.1769208163022995, 'epoch': 2.64}

 88%|████████▊ | 2363/2685 [12:53:56<1:46:03, 19.76s/it]

 88%|████████▊ | 2364/2685 [12:54:16<1:47:28, 20.09s/it]

 88%|████████▊ | 2365/2685 [12:54:36<1:47:06, 20.08s/it]

 88%|████████▊ | 2366/2685 [12:54:56<1:46:08, 19.96s/it]

 88%|████████▊ | 2367/2685 [12:55:15<1:43:28, 19.52s/it]

 88%|████████▊ | 2368/2685 [12:55:30<1:36:54, 18.34s/it]


 88%|████████▊ | 2370/2685 [12:56:11<1:40:42, 19.18s/it]
{'loss': 0.1785, 'learning_rate': 7.13470004277379e-08, 'rewards/chosen': -1.0695582628250122, 'rewards/rejected': -3.010040283203125, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9404819011688232, 'policy_logps/rejected': -374.6934814453125, 'policy_logps/chosen': -403.8581237792969, 'referece_logps/rejected': -344.5930480957031, 'referece_logps/chosen': -393.1625671386719, 'logits/rejected': -0.5113422870635986, 'logits/chosen': -0.5215662121772766, 'epoch': 2.65}

 88%|████████▊ | 2371/2685 [12:56:29<1:38:10, 18.76s/it]


 88%|████████▊ | 2373/2685 [12:57:09<1:41:47, 19.58s/it]
{'loss': 0.2872, 'learning_rate': 7.001049078217613e-08, 'rewards/chosen': -1.4683594703674316, 'rewards/rejected': -4.541783809661865, 'rewards/accuracies': 1.0, 'rewards/margins': 3.0734238624572754, 'policy_logps/rejected': -538.5572509765625, 'policy_logps/chosen': -455.5514831542969, 'referece_logps/rejected': -493.139404296875, 'referece_logps/chosen': -440.8678894042969, 'logits/rejected': 0.553936243057251, 'logits/chosen': 0.5212967395782471, 'epoch': 2.65}

 88%|████████▊ | 2374/2685 [12:57:29<1:42:47, 19.83s/it]

 88%|████████▊ | 2375/2685 [12:57:50<1:43:34, 20.05s/it]

 88%|████████▊ | 2376/2685 [12:58:10<1:42:54, 19.98s/it]

 89%|████████▊ | 2377/2685 [12:58:23<1:31:20, 17.79s/it]

 89%|████████▊ | 2378/2685 [12:58:41<1:31:36, 17.90s/it]

 89%|████████▊ | 2379/2685 [12:59:02<1:36:37, 18.95s/it]


 89%|████████▊ | 2381/2685 [12:59:43<1:40:12, 19.78s/it]

 89%|████████▊ | 2382/2685 [13:00:03<1:40:28, 19.90s/it]
{'loss': 0.2331, 'learning_rate': 6.607412632269604e-08, 'rewards/chosen': -1.4043617248535156, 'rewards/rejected': -4.251909255981445, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8475475311279297, 'policy_logps/rejected': -372.293212890625, 'policy_logps/chosen': -368.2837829589844, 'referece_logps/rejected': -329.7741394042969, 'referece_logps/chosen': -354.2401428222656, 'logits/rejected': 0.27952516078948975, 'logits/chosen': 0.317202091217041, 'epoch': 2.66}

 89%|████████▉ | 2383/2685 [13:00:24<1:42:05, 20.28s/it]

 89%|████████▉ | 2384/2685 [13:00:45<1:41:31, 20.24s/it]

 89%|████████▉ | 2385/2685 [13:01:06<1:42:39, 20.53s/it]

 89%|████████▉ | 2386/2685 [13:01:23<1:37:09, 19.50s/it]

 89%|████████▉ | 2387/2685 [13:01:43<1:37:13, 19.58s/it]


 89%|████████▉ | 2389/2685 [13:02:21<1:35:53, 19.44s/it]
{'loss': 0.3277, 'learning_rate': 6.308860887523049e-08, 'rewards/chosen': -2.2308170795440674, 'rewards/rejected': -3.648376941680908, 'rewards/accuracies': 0.625, 'rewards/margins': 1.4175598621368408, 'policy_logps/rejected': -508.6585998535156, 'policy_logps/chosen': -517.9068603515625, 'referece_logps/rejected': -472.17486572265625, 'referece_logps/chosen': -495.5986328125, 'logits/rejected': -0.3775864839553833, 'logits/chosen': -0.44046908617019653, 'epoch': 2.67}

 89%|████████▉ | 2390/2685 [13:02:41<1:35:50, 19.49s/it]

 89%|████████▉ | 2391/2685 [13:02:56<1:29:03, 18.18s/it]

 89%|████████▉ | 2392/2685 [13:03:16<1:31:03, 18.65s/it]

 89%|████████▉ | 2393/2685 [13:03:32<1:27:35, 18.00s/it]

 89%|████████▉ | 2394/2685 [13:03:53<1:31:07, 18.79s/it]

 89%|████████▉ | 2395/2685 [13:04:14<1:34:22, 19.52s/it]

 89%|████████▉ | 2396/2685 [13:04:34<1:34:44, 19.67s/it]

 89%|████████▉ | 2397/2685 [13:04:50<1:28:36, 18.46s/it]


 89%|████████▉ | 2399/2685 [13:05:26<1:25:36, 17.96s/it]

 89%|████████▉ | 2400/2685 [13:05:46<1:28:08, 18.56s/it]

 89%|████████▉ | 2401/2685 [13:06:08<1:32:53, 19.63s/it]

 89%|████████▉ | 2402/2685 [13:06:22<1:24:34, 17.93s/it]
{'loss': 0.2672, 'learning_rate': 5.772151330341713e-08, 'rewards/chosen': -1.3957247734069824, 'rewards/rejected': -3.511448860168457, 'rewards/accuracies': 0.875, 'rewards/margins': 2.1157238483428955, 'policy_logps/rejected': -412.0744323730469, 'policy_logps/chosen': -399.2043762207031, 'referece_logps/rejected': -376.9599609375, 'referece_logps/chosen': -385.24713134765625, 'logits/rejected': 0.09411101043224335, 'logits/chosen': 0.16245293617248535, 'epoch': 2.68}

 89%|████████▉ | 2403/2685 [13:06:42<1:27:59, 18.72s/it]

 90%|████████▉ | 2404/2685 [13:07:02<1:29:19, 19.07s/it]

 90%|████████▉ | 2405/2685 [13:07:23<1:31:27, 19.60s/it]

 90%|████████▉ | 2406/2685 [13:07:41<1:28:12, 18.97s/it]

 90%|████████▉ | 2407/2685 [13:08:00<1:28:59, 19.21s/it]

 90%|████████▉ | 2408/2685 [13:08:21<1:30:22, 19.58s/it]

 90%|████████▉ | 2409/2685 [13:08:39<1:28:29, 19.24s/it]

 90%|████████▉ | 2410/2685 [13:08:59<1:28:57, 19.41s/it]

 90%|████████▉ | 2411/2685 [13:09:20<1:31:24, 20.02s/it]

 90%|████████▉ | 2412/2685 [13:09:42<1:33:31, 20.56s/it]

 90%|████████▉ | 2413/2685 [13:10:01<1:31:10, 20.11s/it]

 90%|████████▉ | 2414/2685 [13:10:21<1:30:07, 19.95s/it]


 90%|████████▉ | 2416/2685 [13:11:00<1:28:32, 19.75s/it]
{'loss': 0.3146, 'learning_rate': 5.220080975898067e-08, 'rewards/chosen': -1.4213775396347046, 'rewards/rejected': -4.19488000869751, 'rewards/accuracies': 0.75, 'rewards/margins': 2.7735025882720947, 'policy_logps/rejected': -543.6815185546875, 'policy_logps/chosen': -438.0189208984375, 'referece_logps/rejected': -501.73272705078125, 'referece_logps/chosen': -423.80511474609375, 'logits/rejected': 0.35860148072242737, 'logits/chosen': 0.4170057773590088, 'epoch': 2.7}

 90%|█████████ | 2417/2685 [13:11:17<1:24:03, 18.82s/it]


 90%|█████████ | 2419/2685 [13:11:56<1:25:44, 19.34s/it]

 90%|█████████ | 2420/2685 [13:12:16<1:26:06, 19.49s/it]
{'loss': 0.3056, 'learning_rate': 5.0673061177888496e-08, 'rewards/chosen': -1.4866364002227783, 'rewards/rejected': -3.59310245513916, 'rewards/accuracies': 0.875, 'rewards/margins': 2.106466054916382, 'policy_logps/rejected': -337.1203918457031, 'policy_logps/chosen': -341.0591125488281, 'referece_logps/rejected': -301.18939208984375, 'referece_logps/chosen': -326.1927185058594, 'logits/rejected': -0.15391585230827332, 'logits/chosen': -0.24307231605052948, 'epoch': 2.7}

 90%|█████████ | 2421/2685 [13:12:35<1:24:27, 19.19s/it]

 90%|█████████ | 2422/2685 [13:12:54<1:25:02, 19.40s/it]


 90%|█████████ | 2424/2685 [13:13:32<1:24:01, 19.32s/it]

 90%|█████████ | 2425/2685 [13:13:53<1:26:09, 19.88s/it]

 90%|█████████ | 2426/2685 [13:14:09<1:20:31, 18.65s/it]

 90%|█████████ | 2427/2685 [13:14:29<1:21:27, 18.95s/it]

 90%|█████████ | 2428/2685 [13:14:49<1:22:32, 19.27s/it]

 90%|█████████ | 2429/2685 [13:15:05<1:17:46, 18.23s/it]

 91%|█████████ | 2430/2685 [13:15:27<1:22:41, 19.46s/it]

 91%|█████████ | 2431/2685 [13:15:45<1:20:05, 18.92s/it]

 91%|█████████ | 2432/2685 [13:16:05<1:21:20, 19.29s/it]

 91%|█████████ | 2433/2685 [13:16:25<1:21:49, 19.48s/it]

 91%|█████████ | 2434/2685 [13:16:44<1:21:48, 19.55s/it]

 91%|█████████ | 2435/2685 [13:17:06<1:23:53, 20.13s/it]

 91%|█████████ | 2436/2685 [13:17:28<1:25:59, 20.72s/it]

 91%|█████████ | 2437/2685 [13:17:49<1:25:30, 20.69s/it]

 91%|█████████ | 2438/2685 [13:18:11<1:26:44, 21.07s/it]

 91%|█████████ | 2439/2685 [13:18:28<1:21:44, 19.94s/it]

 91%|█████████ | 2440/2685 [13:18:50<1:23:40, 20.49s/it]

 91%|█████████ | 2441/2685 [13:19:10<1:22:46, 20.35s/it]

 91%|█████████ | 2442/2685 [13:19:28<1:19:27, 19.62s/it]

 91%|█████████ | 2443/2685 [13:19:48<1:19:50, 19.80s/it]

 91%|█████████ | 2444/2685 [13:20:06<1:18:13, 19.47s/it]

 91%|█████████ | 2445/2685 [13:20:27<1:18:57, 19.74s/it]

 91%|█████████ | 2446/2685 [13:20:48<1:20:30, 20.21s/it]
{'loss': 0.2989, 'learning_rate': 4.1283122921496605e-08, 'rewards/chosen': -0.7098274230957031, 'rewards/rejected': -3.233858346939087, 'rewards/accuracies': 0.875, 'rewards/margins': 2.5240306854248047, 'policy_logps/rejected': -487.5534362792969, 'policy_logps/chosen': -389.7119140625, 'referece_logps/rejected': -455.21484375, 'referece_logps/chosen': -382.6136474609375, 'logits/rejected': 0.012240001931786537, 'logits/chosen': 0.03284712880849838, 'epoch': 2.73}


 91%|█████████ | 2448/2685 [13:21:26<1:17:59, 19.74s/it]

 91%|█████████ | 2449/2685 [13:21:45<1:17:01, 19.58s/it]

 91%|█████████ | 2450/2685 [13:22:07<1:19:39, 20.34s/it]

 91%|█████████▏| 2451/2685 [13:22:29<1:21:11, 20.82s/it]

 91%|█████████▏| 2452/2685 [13:22:46<1:16:04, 19.59s/it]

 91%|█████████▏| 2453/2685 [13:23:07<1:17:48, 20.12s/it]

 91%|█████████▏| 2454/2685 [13:23:27<1:16:52, 19.97s/it]

 91%|█████████▏| 2455/2685 [13:23:48<1:18:24, 20.45s/it]

 91%|█████████▏| 2456/2685 [13:24:08<1:17:27, 20.30s/it]

 92%|█████████▏| 2457/2685 [13:24:28<1:16:17, 20.08s/it]

 92%|█████████▏| 2458/2685 [13:24:48<1:15:26, 19.94s/it]

 92%|█████████▏| 2459/2685 [13:25:06<1:13:31, 19.52s/it]

 92%|█████████▏| 2460/2685 [13:25:26<1:13:36, 19.63s/it]
{'loss': 0.3016, 'learning_rate': 3.6617139511609387e-08, 'rewards/chosen': -1.8712117671966553, 'rewards/rejected': -4.679858684539795, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8086466789245605, 'policy_logps/rejected': -379.0157775878906, 'policy_logps/chosen': -308.3592224121094, 'referece_logps/rejected': -332.2171630859375, 'referece_logps/chosen': -289.6471252441406, 'logits/rejected': 0.13171951472759247, 'logits/chosen': -0.013877497054636478, 'epoch': 2.75}

 92%|█████████▏| 2461/2685 [13:25:43<1:10:29, 18.88s/it]


 92%|█████████▏| 2463/2685 [13:26:25<1:13:01, 19.73s/it]

 92%|█████████▏| 2464/2685 [13:26:47<1:15:20, 20.45s/it]

 92%|█████████▏| 2465/2685 [13:27:07<1:14:32, 20.33s/it]

 92%|█████████▏| 2466/2685 [13:27:29<1:16:06, 20.85s/it]

 92%|█████████▏| 2467/2685 [13:27:49<1:14:25, 20.48s/it]

 92%|█████████▏| 2468/2685 [13:28:10<1:14:59, 20.74s/it]

 92%|█████████▏| 2469/2685 [13:28:32<1:16:05, 21.14s/it]

 92%|█████████▏| 2470/2685 [13:28:54<1:16:42, 21.41s/it]

 92%|█████████▏| 2471/2685 [13:29:14<1:14:43, 20.95s/it]

 92%|█████████▏| 2472/2685 [13:29:31<1:10:45, 19.93s/it]

 92%|█████████▏| 2473/2685 [13:29:54<1:12:49, 20.61s/it]

 92%|█████████▏| 2474/2685 [13:30:15<1:12:50, 20.71s/it]

 92%|█████████▏| 2475/2685 [13:30:34<1:11:25, 20.41s/it]

 92%|█████████▏| 2476/2685 [13:30:54<1:10:17, 20.18s/it]

 92%|█████████▏| 2477/2685 [13:31:13<1:08:54, 19.88s/it]

 92%|█████████▏| 2478/2685 [13:31:34<1:10:07, 20.32s/it]

 92%|█████████▏| 2479/2685 [13:31:52<1:06:42, 19.43s/it]

 92%|█████████▏| 2480/2685 [13:32:11<1:06:17, 19.40s/it]

 92%|█████████▏| 2481/2685 [13:32:31<1:06:24, 19.53s/it]

 92%|█████████▏| 2482/2685 [13:32:47<1:02:14, 18.39s/it]

 92%|█████████▏| 2483/2685 [13:33:07<1:03:31, 18.87s/it]

 93%|█████████▎| 2484/2685 [13:33:28<1:05:39, 19.60s/it]

 93%|█████████▎| 2485/2685 [13:33:44<1:01:40, 18.50s/it]

 93%|█████████▎| 2486/2685 [13:34:03<1:02:22, 18.81s/it]

 93%|█████████▎| 2487/2685 [13:34:20<59:46, 18.11s/it]
{'loss': 0.3284, 'learning_rate': 2.839566469923105e-08, 'rewards/chosen': -1.6884980201721191, 'rewards/rejected': -3.4721169471740723, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7836185693740845, 'policy_logps/rejected': -416.84844970703125, 'policy_logps/chosen': -456.1448669433594, 'referece_logps/rejected': -382.12725830078125, 'referece_logps/chosen': -439.2598876953125, 'logits/rejected': -0.7936034202575684, 'logits/chosen': -0.8225855827331543, 'epoch': 2.78}


 93%|█████████▎| 2489/2685 [13:35:02<1:04:13, 19.66s/it]

 93%|█████████▎| 2490/2685 [13:35:22<1:04:42, 19.91s/it]

 93%|█████████▎| 2491/2685 [13:35:42<1:04:07, 19.83s/it]

 93%|█████████▎| 2492/2685 [13:36:03<1:05:03, 20.23s/it]

 93%|█████████▎| 2493/2685 [13:36:19<1:00:45, 18.99s/it]

 93%|█████████▎| 2494/2685 [13:36:38<1:00:43, 19.08s/it]

 93%|█████████▎| 2495/2685 [13:36:58<1:00:55, 19.24s/it]

 93%|█████████▎| 2496/2685 [13:37:13<56:06, 17.81s/it]

 93%|█████████▎| 2497/2685 [13:37:29<54:14, 17.31s/it]

 93%|█████████▎| 2498/2685 [13:37:51<58:35, 18.80s/it]

 93%|█████████▎| 2499/2685 [13:38:11<59:27, 19.18s/it]

 93%|█████████▎| 2500/2685 [13:38:30<59:17, 19.23s/it]

 93%|█████████▎| 2501/2685 [13:39:06<1:13:29, 23.96s/it]

 93%|█████████▎| 2502/2685 [13:39:24<1:08:29, 22.46s/it]

 93%|█████████▎| 2503/2685 [13:39:44<1:05:40, 21.65s/it]
{'loss': 0.2395, 'learning_rate': 2.400960372288474e-08, 'rewards/chosen': -1.830157995223999, 'rewards/rejected': -2.999403953552246, 'rewards/accuracies': 0.625, 'rewards/margins': 1.169245958328247, 'policy_logps/rejected': -435.0681457519531, 'policy_logps/chosen': -413.22430419921875, 'referece_logps/rejected': -405.0740966796875, 'referece_logps/chosen': -394.9227294921875, 'logits/rejected': -0.4186639189720154, 'logits/chosen': -0.3273470401763916, 'epoch': 2.8}


 93%|█████████▎| 2505/2685 [13:40:23<1:01:21, 20.45s/it]

 93%|█████████▎| 2506/2685 [13:40:45<1:02:03, 20.80s/it]

 93%|█████████▎| 2507/2685 [13:41:00<56:32, 19.06s/it]

 93%|█████████▎| 2508/2685 [13:41:22<58:53, 19.96s/it]

 93%|█████████▎| 2509/2685 [13:41:42<58:32, 19.96s/it]

 93%|█████████▎| 2510/2685 [13:41:59<55:42, 19.10s/it]

 94%|█████████▎| 2511/2685 [13:42:14<52:31, 18.11s/it]

 94%|█████████▎| 2512/2685 [13:42:35<54:06, 18.77s/it]

 94%|█████████▎| 2513/2685 [13:42:53<52:58, 18.48s/it]

 94%|█████████▎| 2514/2685 [13:43:07<49:01, 17.20s/it]

 94%|█████████▎| 2515/2685 [13:43:21<46:35, 16.44s/it]

 94%|█████████▎| 2516/2685 [13:43:41<48:56, 17.38s/it]

 94%|█████████▎| 2517/2685 [13:43:57<47:53, 17.10s/it]

 94%|█████████▍| 2518/2685 [13:44:15<47:39, 17.12s/it]

 94%|█████████▍| 2519/2685 [13:44:37<51:28, 18.60s/it]

 94%|█████████▍| 2520/2685 [13:44:56<51:54, 18.88s/it]

 94%|█████████▍| 2521/2685 [13:45:17<53:33, 19.59s/it]

 94%|█████████▍| 2522/2685 [13:45:38<53:50, 19.82s/it]

 94%|█████████▍| 2523/2685 [13:45:59<54:39, 20.24s/it]

 94%|█████████▍| 2524/2685 [13:46:20<54:56, 20.47s/it]

 94%|█████████▍| 2525/2685 [13:46:42<55:40, 20.88s/it]

 94%|█████████▍| 2526/2685 [13:47:02<54:19, 20.50s/it]

 94%|█████████▍| 2527/2685 [13:47:19<51:28, 19.55s/it]

 94%|█████████▍| 2528/2685 [13:47:37<50:23, 19.26s/it]

 94%|█████████▍| 2529/2685 [13:47:58<51:21, 19.76s/it]

 94%|█████████▍| 2530/2685 [13:48:15<48:57, 18.95s/it]

 94%|█████████▍| 2531/2685 [13:48:38<51:06, 19.91s/it]
{'loss': 0.273, 'learning_rate': 1.720994905914208e-08, 'rewards/chosen': -1.3085501194000244, 'rewards/rejected': -3.0225701332092285, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7140203714370728, 'policy_logps/rejected': -378.5125732421875, 'policy_logps/chosen': -363.2772216796875, 'referece_logps/rejected': -348.2868957519531, 'referece_logps/chosen': -350.1917419433594, 'logits/rejected': -0.6048567891120911, 'logits/chosen': -0.6149488687515259, 'epoch': 2.83}


 94%|█████████▍| 2533/2685 [13:49:15<48:15, 19.05s/it]

 94%|█████████▍| 2534/2685 [13:49:31<45:50, 18.21s/it]

 94%|█████████▍| 2535/2685 [13:49:48<44:19, 17.73s/it]

 94%|█████████▍| 2536/2685 [13:50:06<44:29, 17.91s/it]

 94%|█████████▍| 2537/2685 [13:50:28<47:11, 19.13s/it]

 95%|█████████▍| 2538/2685 [13:50:49<48:31, 19.81s/it]

 95%|█████████▍| 2539/2685 [13:51:10<48:44, 20.03s/it]

 95%|█████████▍| 2540/2685 [13:51:30<48:08, 19.92s/it]
{'loss': 0.3037, 'learning_rate': 1.5262159164289302e-08, 'rewards/chosen': -1.6327857971191406, 'rewards/rejected': -3.9635465145111084, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3307607173919678, 'policy_logps/rejected': -465.69842529296875, 'policy_logps/chosen': -457.5127868652344, 'referece_logps/rejected': -426.0629577636719, 'referece_logps/chosen': -441.1849365234375, 'logits/rejected': -0.07947403192520142, 'logits/chosen': -0.13475464284420013, 'epoch': 2.84}


 95%|█████████▍| 2542/2685 [13:52:08<46:11, 19.38s/it]

 95%|█████████▍| 2543/2685 [13:52:28<46:33, 19.67s/it]

 95%|█████████▍| 2544/2685 [13:52:48<46:30, 19.79s/it]

 95%|█████████▍| 2545/2685 [13:53:08<46:03, 19.74s/it]

 95%|█████████▍| 2546/2685 [13:53:30<47:12, 20.38s/it]

 95%|█████████▍| 2547/2685 [13:53:50<46:27, 20.20s/it]

 95%|█████████▍| 2548/2685 [13:54:10<46:03, 20.17s/it]
{'loss': 0.3467, 'learning_rate': 1.3628247272871708e-08, 'rewards/chosen': -0.30195340514183044, 'rewards/rejected': -2.203763484954834, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9018101692199707, 'policy_logps/rejected': -296.4609680175781, 'policy_logps/chosen': -403.1309814453125, 'referece_logps/rejected': -274.42333984375, 'referece_logps/chosen': -400.1114196777344, 'logits/rejected': -1.05690336227417, 'logits/chosen': -1.1681935787200928, 'epoch': 2.85}


 95%|█████████▍| 2550/2685 [13:54:48<43:54, 19.51s/it]

 95%|█████████▌| 2551/2685 [13:55:06<42:41, 19.11s/it]

 95%|█████████▌| 2552/2685 [13:55:25<42:07, 19.00s/it]

 95%|█████████▌| 2553/2685 [13:55:45<42:24, 19.28s/it]

 95%|█████████▌| 2554/2685 [13:56:04<42:09, 19.31s/it]
{'loss': 0.2896, 'learning_rate': 1.2463103550075849e-08, 'rewards/chosen': -1.5994682312011719, 'rewards/rejected': -3.425560712814331, 'rewards/accuracies': 0.75, 'rewards/margins': 1.8260926008224487, 'policy_logps/rejected': -387.56573486328125, 'policy_logps/chosen': -365.62640380859375, 'referece_logps/rejected': -353.31011962890625, 'referece_logps/chosen': -349.6316833496094, 'logits/rejected': -0.5462892055511475, 'logits/chosen': -0.5872106552124023, 'epoch': 2.85}


 95%|█████████▌| 2556/2685 [13:56:42<41:43, 19.40s/it]

 95%|█████████▌| 2557/2685 [13:57:04<43:00, 20.16s/it]

 95%|█████████▌| 2558/2685 [13:57:23<41:31, 19.61s/it]

 95%|█████████▌| 2559/2685 [13:57:42<41:13, 19.63s/it]
{'loss': 0.337, 'learning_rate': 1.1531675671888619e-08, 'rewards/chosen': -1.9036163091659546, 'rewards/rejected': -4.144651412963867, 'rewards/accuracies': 0.75, 'rewards/margins': 2.241034746170044, 'policy_logps/rejected': -371.96502685546875, 'policy_logps/chosen': -375.78314208984375, 'referece_logps/rejected': -330.5185241699219, 'referece_logps/chosen': -356.7469787597656, 'logits/rejected': 0.09663748741149902, 'logits/chosen': 0.05213804543018341, 'epoch': 2.86}


 95%|█████████▌| 2561/2685 [13:58:16<37:07, 17.97s/it]

 95%|█████████▌| 2562/2685 [13:58:37<38:33, 18.81s/it]

 95%|█████████▌| 2563/2685 [13:58:55<37:46, 18.58s/it]
{'loss': 0.2632, 'learning_rate': 1.0812428922044592e-08, 'rewards/chosen': -1.7191190719604492, 'rewards/rejected': -3.056321382522583, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3372019529342651, 'policy_logps/rejected': -270.3800964355469, 'policy_logps/chosen': -286.2071533203125, 'referece_logps/rejected': -239.81686401367188, 'referece_logps/chosen': -269.0159912109375, 'logits/rejected': -0.19062761962413788, 'logits/chosen': -0.20284685492515564, 'epoch': 2.86}


 96%|█████████▌| 2565/2685 [13:59:33<37:26, 18.72s/it]

 96%|█████████▌| 2566/2685 [13:59:52<37:26, 18.88s/it]
{'loss': 0.2804, 'learning_rate': 1.0288110634917634e-08, 'rewards/chosen': -0.6098520159721375, 'rewards/rejected': -3.0650861263275146, 'rewards/accuracies': 0.75, 'rewards/margins': 2.4552340507507324, 'policy_logps/rejected': -441.5295104980469, 'policy_logps/chosen': -395.0897521972656, 'referece_logps/rejected': -410.878662109375, 'referece_logps/chosen': -388.9912109375, 'logits/rejected': -0.47978028655052185, 'logits/chosen': -0.5055023431777954, 'epoch': 2.87}


 96%|█████████▌| 2568/2685 [14:00:28<35:37, 18.27s/it]

 96%|█████████▌| 2569/2685 [14:00:48<36:17, 18.77s/it]

 96%|█████████▌| 2570/2685 [14:01:01<32:41, 17.06s/it]
{'loss': 0.3194, 'learning_rate': 9.60918834416391e-09, 'rewards/chosen': -1.15900456905365, 'rewards/rejected': -3.314333438873291, 'rewards/accuracies': 0.75, 'rewards/margins': 2.1553287506103516, 'policy_logps/rejected': -497.42584228515625, 'policy_logps/chosen': -451.49078369140625, 'referece_logps/rejected': -464.28253173828125, 'referece_logps/chosen': -439.9007263183594, 'logits/rejected': 0.207041397690773, 'logits/chosen': 0.2868746221065521, 'epoch': 2.87}


 96%|█████████▌| 2572/2685 [14:01:34<32:28, 17.24s/it]

 96%|█████████▌| 2573/2685 [14:01:50<31:12, 16.72s/it]

 96%|█████████▌| 2574/2685 [14:02:08<31:46, 17.17s/it]

 96%|█████████▌| 2575/2685 [14:02:28<32:59, 18.00s/it]

 96%|█████████▌| 2576/2685 [14:02:48<33:56, 18.69s/it]

 96%|█████████▌| 2577/2685 [14:03:06<33:14, 18.46s/it]

 96%|█████████▌| 2578/2685 [14:03:22<31:34, 17.71s/it]
{'loss': 0.2659, 'learning_rate': 8.32055245417873e-09, 'rewards/chosen': -1.2388191223144531, 'rewards/rejected': -3.9440181255340576, 'rewards/accuracies': 0.875, 'rewards/margins': 2.7051990032196045, 'policy_logps/rejected': -351.5547180175781, 'policy_logps/chosen': -292.0586242675781, 'referece_logps/rejected': -312.11456298828125, 'referece_logps/chosen': -279.67047119140625, 'logits/rejected': -0.3836092948913574, 'logits/chosen': -0.2933637201786041, 'epoch': 2.88}


 96%|█████████▌| 2580/2685 [14:04:01<32:36, 18.64s/it]

 96%|█████████▌| 2581/2685 [14:04:17<30:54, 17.83s/it]
{'loss': 0.3514, 'learning_rate': 7.861123868892483e-09, 'rewards/chosen': -0.6711516976356506, 'rewards/rejected': -2.9085869789123535, 'rewards/accuracies': 0.875, 'rewards/margins': 2.2374351024627686, 'policy_logps/rejected': -292.0068054199219, 'policy_logps/chosen': -270.99273681640625, 'referece_logps/rejected': -262.9208984375, 'referece_logps/chosen': -264.28118896484375, 'logits/rejected': -0.7284754514694214, 'logits/chosen': -0.7641335725784302, 'epoch': 2.88}


 96%|█████████▌| 2583/2685 [14:04:57<32:10, 18.92s/it]

 96%|█████████▌| 2584/2685 [14:05:19<33:21, 19.82s/it]

 96%|█████████▋| 2585/2685 [14:05:41<33:53, 20.33s/it]

 96%|█████████▋| 2586/2685 [14:06:01<33:16, 20.17s/it]
{'loss': 0.2231, 'learning_rate': 7.124293957000649e-09, 'rewards/chosen': -1.3080743551254272, 'rewards/rejected': -3.5034193992614746, 'rewards/accuracies': 1.0, 'rewards/margins': 2.195344924926758, 'policy_logps/rejected': -367.39178466796875, 'policy_logps/chosen': -300.01153564453125, 'referece_logps/rejected': -332.3575744628906, 'referece_logps/chosen': -286.9307556152344, 'logits/rejected': 0.07516022026538849, 'logits/chosen': 0.0477338470518589, 'epoch': 2.89}

 96%|█████████▋| 2587/2685 [14:06:20<32:32, 19.92s/it]


 96%|█████████▋| 2589/2685 [14:06:57<30:52, 19.30s/it]

 96%|█████████▋| 2590/2685 [14:07:19<31:48, 20.09s/it]

 96%|█████████▋| 2591/2685 [14:07:39<31:26, 20.07s/it]

 97%|█████████▋| 2592/2685 [14:07:57<30:17, 19.54s/it]
{'loss': 0.2981, 'learning_rate': 6.2877901067573955e-09, 'rewards/chosen': -1.0434749126434326, 'rewards/rejected': -4.859014511108398, 'rewards/accuracies': 1.0, 'rewards/margins': 3.8155391216278076, 'policy_logps/rejected': -313.7394714355469, 'policy_logps/chosen': -272.01171875, 'referece_logps/rejected': -265.14935302734375, 'referece_logps/chosen': -261.57696533203125, 'logits/rejected': -0.8138824701309204, 'logits/chosen': -0.7513506412506104, 'epoch': 2.9}


 97%|█████████▋| 2594/2685 [14:08:33<28:28, 18.78s/it]

 97%|█████████▋| 2595/2685 [14:08:50<27:12, 18.14s/it]

 97%|█████████▋| 2596/2685 [14:09:11<28:25, 19.16s/it]
{'loss': 0.3155, 'learning_rate': 5.759045487373937e-09, 'rewards/chosen': -0.609220027923584, 'rewards/rejected': -1.4389336109161377, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8297136425971985, 'policy_logps/rejected': -397.3267517089844, 'policy_logps/chosen': -360.0293884277344, 'referece_logps/rejected': -382.9373779296875, 'referece_logps/chosen': -353.93719482421875, 'logits/rejected': -1.143061876296997, 'logits/chosen': -1.2107771635055542, 'epoch': 2.9}


 97%|█████████▋| 2598/2685 [14:09:49<27:42, 19.11s/it]
{'loss': 0.3212, 'learning_rate': 5.503355230027429e-09, 'rewards/chosen': -1.2637251615524292, 'rewards/rejected': -3.620250701904297, 'rewards/accuracies': 0.875, 'rewards/margins': 2.3565256595611572, 'policy_logps/rejected': -408.18536376953125, 'policy_logps/chosen': -365.3844909667969, 'referece_logps/rejected': -371.9828796386719, 'referece_logps/chosen': -352.74725341796875, 'logits/rejected': -0.28564712405204773, 'logits/chosen': -0.27425915002822876, 'epoch': 2.9}


 97%|█████████▋| 2600/2685 [14:10:25<25:47, 18.21s/it]

 97%|█████████▋| 2601/2685 [14:10:46<26:31, 18.94s/it]

 97%|█████████▋| 2602/2685 [14:11:06<26:35, 19.22s/it]

 97%|█████████▋| 2603/2685 [14:11:28<27:25, 20.07s/it]
{'loss': 0.2935, 'learning_rate': 4.889464145711896e-09, 'rewards/chosen': -0.43788954615592957, 'rewards/rejected': -4.422422885894775, 'rewards/accuracies': 0.875, 'rewards/margins': 3.9845330715179443, 'policy_logps/rejected': -379.9424133300781, 'policy_logps/chosen': -367.848388671875, 'referece_logps/rejected': -335.7181701660156, 'referece_logps/chosen': -363.4695129394531, 'logits/rejected': 0.14054186642169952, 'logits/chosen': 0.09827645868062973, 'epoch': 2.91}

 97%|█████████▋| 2604/2685 [14:11:48<27:19, 20.24s/it]

 97%|█████████▋| 2605/2685 [14:12:08<26:44, 20.06s/it]

 97%|█████████▋| 2606/2685 [14:12:28<26:29, 20.12s/it]


 97%|█████████▋| 2608/2685 [14:13:08<25:32, 19.91s/it]

 97%|█████████▋| 2609/2685 [14:13:28<25:14, 19.93s/it]
{'loss': 0.3472, 'learning_rate': 4.200593837964561e-09, 'rewards/chosen': -1.0106250047683716, 'rewards/rejected': -3.8342270851135254, 'rewards/accuracies': 0.875, 'rewards/margins': 2.8236021995544434, 'policy_logps/rejected': -274.4735412597656, 'policy_logps/chosen': -229.03640747070312, 'referece_logps/rejected': -236.13125610351562, 'referece_logps/chosen': -218.93017578125, 'logits/rejected': -0.6431203484535217, 'logits/chosen': -0.7258856296539307, 'epoch': 2.92}


 97%|█████████▋| 2611/2685 [14:14:07<24:20, 19.73s/it]

 97%|█████████▋| 2612/2685 [14:14:27<24:04, 19.78s/it]
{'loss': 0.2824, 'learning_rate': 3.87572341214204e-09, 'rewards/chosen': -3.0867676734924316, 'rewards/rejected': -4.8023905754089355, 'rewards/accuracies': 0.75, 'rewards/margins': 1.7156226634979248, 'policy_logps/rejected': -412.2688903808594, 'policy_logps/chosen': -372.646240234375, 'referece_logps/rejected': -364.2449951171875, 'referece_logps/chosen': -341.778564453125, 'logits/rejected': -0.2695356011390686, 'logits/chosen': -0.4539545178413391, 'epoch': 2.92}


 97%|█████████▋| 2614/2685 [14:15:04<22:30, 19.02s/it]

 97%|█████████▋| 2615/2685 [14:15:23<22:17, 19.10s/it]
{'loss': 0.2531, 'learning_rate': 3.5639018695670097e-09, 'rewards/chosen': -0.8678195476531982, 'rewards/rejected': -4.7481818199157715, 'rewards/accuracies': 0.875, 'rewards/margins': 3.880362033843994, 'policy_logps/rejected': -473.1738586425781, 'policy_logps/chosen': -434.8891296386719, 'referece_logps/rejected': -425.6920166015625, 'referece_logps/chosen': -426.2109069824219, 'logits/rejected': 0.26519709825515747, 'logits/chosen': 0.2611599266529083, 'epoch': 2.92}

 97%|█████████▋| 2616/2685 [14:15:43<22:09, 19.27s/it]

 97%|█████████▋| 2617/2685 [14:16:03<22:00, 19.42s/it]

 98%|█████████▊| 2618/2685 [14:16:22<21:51, 19.58s/it]

 98%|█████████▊| 2619/2685 [14:16:43<21:45, 19.78s/it]

 98%|█████████▊| 2620/2685 [14:17:03<21:32, 19.88s/it]


 98%|█████████▊| 2622/2685 [14:17:43<21:06, 20.10s/it]

 98%|█████████▊| 2623/2685 [14:18:05<21:22, 20.68s/it]

 98%|█████████▊| 2624/2685 [14:18:26<20:56, 20.59s/it]

 98%|█████████▊| 2625/2685 [14:18:39<18:29, 18.49s/it]

 98%|█████████▊| 2626/2685 [14:18:59<18:33, 18.87s/it]

 98%|█████████▊| 2627/2685 [14:19:20<18:44, 19.40s/it]
{'loss': 0.3113, 'learning_rate': 2.447183661683283e-09, 'rewards/chosen': -0.5596626400947571, 'rewards/rejected': -5.2250237464904785, 'rewards/accuracies': 1.0, 'rewards/margins': 4.665360927581787, 'policy_logps/rejected': -419.459716796875, 'policy_logps/chosen': -432.73919677734375, 'referece_logps/rejected': -367.2094421386719, 'referece_logps/chosen': -427.142578125, 'logits/rejected': 0.07430174946784973, 'logits/chosen': 0.1185588389635086, 'epoch': 2.94}


 98%|█████████▊| 2629/2685 [14:19:56<17:21, 18.60s/it]
{'loss': 0.3626, 'learning_rate': 2.2813853199292744e-09, 'rewards/chosen': -0.894631028175354, 'rewards/rejected': -4.270982265472412, 'rewards/accuracies': 1.0, 'rewards/margins': 3.3763515949249268, 'policy_logps/rejected': -494.2139587402344, 'policy_logps/chosen': -366.7794189453125, 'referece_logps/rejected': -451.504150390625, 'referece_logps/chosen': -357.8331298828125, 'logits/rejected': -1.0618692636489868, 'logits/chosen': -1.0480352640151978, 'epoch': 2.94}


 98%|█████████▊| 2631/2685 [14:20:36<17:12, 19.12s/it]

 98%|█████████▊| 2632/2685 [14:20:55<17:03, 19.31s/it]

 98%|█████████▊| 2633/2685 [14:21:15<16:56, 19.54s/it]
{'loss': 0.296, 'learning_rate': 1.967215936493516e-09, 'rewards/chosen': -2.21773099899292, 'rewards/rejected': -4.13859224319458, 'rewards/accuracies': 0.875, 'rewards/margins': 1.920861005783081, 'policy_logps/rejected': -544.61376953125, 'policy_logps/chosen': -393.4010314941406, 'referece_logps/rejected': -503.2278747558594, 'referece_logps/chosen': -371.2237243652344, 'logits/rejected': -0.5207059979438782, 'logits/chosen': -0.5782403349876404, 'epoch': 2.94}

 98%|█████████▊| 2634/2685 [14:21:35<16:40, 19.62s/it]


 98%|█████████▊| 2636/2685 [14:22:14<16:02, 19.64s/it]

 98%|█████████▊| 2637/2685 [14:22:36<16:13, 20.28s/it]
{'loss': 0.2474, 'learning_rate': 1.6762889938303215e-09, 'rewards/chosen': -1.0053330659866333, 'rewards/rejected': -3.746542453765869, 'rewards/accuracies': 1.0, 'rewards/margins': 2.7412092685699463, 'policy_logps/rejected': -568.2086791992188, 'policy_logps/chosen': -438.9433898925781, 'referece_logps/rejected': -530.7432250976562, 'referece_logps/chosen': -428.8900146484375, 'logits/rejected': -0.12551456689834595, 'logits/chosen': -0.11712156236171722, 'epoch': 2.95}

 98%|█████████▊| 2638/2685 [14:22:57<16:06, 20.57s/it]

 98%|█████████▊| 2639/2685 [14:23:17<15:33, 20.30s/it]

 98%|█████████▊| 2640/2685 [14:23:39<15:37, 20.83s/it]

 98%|█████████▊| 2641/2685 [14:23:59<15:01, 20.48s/it]

 98%|█████████▊| 2642/2685 [14:24:18<14:21, 20.04s/it]

 98%|█████████▊| 2643/2685 [14:24:38<13:57, 19.94s/it]


 99%|█████████▊| 2645/2685 [14:25:12<12:23, 18.59s/it]

 99%|█████████▊| 2646/2685 [14:25:35<12:45, 19.64s/it]

 99%|█████████▊| 2647/2685 [14:25:57<12:54, 20.39s/it]

 99%|█████████▊| 2648/2685 [14:26:16<12:26, 20.18s/it]

 99%|█████████▊| 2649/2685 [14:26:36<12:02, 20.08s/it]

 99%|█████████▊| 2650/2685 [14:26:56<11:43, 20.11s/it]

 99%|█████████▊| 2651/2685 [14:27:18<11:40, 20.60s/it]
{'loss': 0.2734, 'learning_rate': 8.411718462786322e-10, 'rewards/chosen': -0.07025796175003052, 'rewards/rejected': -2.710043430328369, 'rewards/accuracies': 1.0, 'rewards/margins': 2.6397852897644043, 'policy_logps/rejected': -252.24159240722656, 'policy_logps/chosen': -334.73687744140625, 'referece_logps/rejected': -225.14114379882812, 'referece_logps/chosen': -334.0343017578125, 'logits/rejected': -0.39909735321998596, 'logits/chosen': -0.4330894947052002, 'epoch': 2.96}


 99%|█████████▉| 2653/2685 [14:27:57<10:30, 19.71s/it]
{'loss': 0.3145, 'learning_rate': 7.451330121498456e-10, 'rewards/chosen': -2.582024574279785, 'rewards/rejected': -4.777949333190918, 'rewards/accuracies': 0.875, 'rewards/margins': 2.195924758911133, 'policy_logps/rejected': -299.56689453125, 'policy_logps/chosen': -309.52349853515625, 'referece_logps/rejected': -251.78738403320312, 'referece_logps/chosen': -283.7032775878906, 'logits/rejected': -0.9204429984092712, 'logits/chosen': -0.90389484167099, 'epoch': 2.96}

 99%|█████████▉| 2654/2685 [14:28:18<10:25, 20.17s/it]

 99%|█████████▉| 2655/2685 [14:28:38<10:05, 20.20s/it]


 99%|█████████▉| 2657/2685 [14:29:17<09:11, 19.70s/it]
{'loss': 0.2628, 'learning_rate': 5.705090702819992e-10, 'rewards/chosen': -1.0838675498962402, 'rewards/rejected': -2.252960205078125, 'rewards/accuracies': 0.625, 'rewards/margins': 1.1690927743911743, 'policy_logps/rejected': -362.76519775390625, 'policy_logps/chosen': -328.9722900390625, 'referece_logps/rejected': -340.235595703125, 'referece_logps/chosen': -318.133544921875, 'logits/rejected': 0.08979807794094086, 'logits/chosen': 0.10052628815174103, 'epoch': 2.97}

 99%|█████████▉| 2658/2685 [14:29:36<08:50, 19.64s/it]

 99%|█████████▉| 2659/2685 [14:29:56<08:30, 19.64s/it]


 99%|█████████▉| 2661/2685 [14:30:34<07:45, 19.39s/it]

 99%|█████████▉| 2662/2685 [14:30:55<07:34, 19.75s/it]
{'loss': 0.3096, 'learning_rate': 3.8495999114251943e-10, 'rewards/chosen': -1.0907505750656128, 'rewards/rejected': -3.7085955142974854, 'rewards/accuracies': 1.0, 'rewards/margins': 2.617845296859741, 'policy_logps/rejected': -477.4243469238281, 'policy_logps/chosen': -419.5450439453125, 'referece_logps/rejected': -440.33837890625, 'referece_logps/chosen': -408.6375732421875, 'logits/rejected': 0.4513428807258606, 'logits/chosen': 0.4392223060131073, 'epoch': 2.97}


 99%|█████████▉| 2664/2685 [14:31:37<07:05, 20.26s/it]
{'loss': 0.2836, 'learning_rate': 3.2092470356948066e-10, 'rewards/chosen': -1.5297611951828003, 'rewards/rejected': -4.085028648376465, 'rewards/accuracies': 0.75, 'rewards/margins': 2.555267333984375, 'policy_logps/rejected': -400.3095703125, 'policy_logps/chosen': -448.3639831542969, 'referece_logps/rejected': -359.4592590332031, 'referece_logps/chosen': -433.0663757324219, 'logits/rejected': 0.13681727647781372, 'logits/chosen': 0.0863649994134903, 'epoch': 2.98}

 99%|█████████▉| 2665/2685 [14:31:58<06:50, 20.54s/it]

 99%|█████████▉| 2666/2685 [14:32:18<06:24, 20.23s/it]

 99%|█████████▉| 2667/2685 [14:32:34<05:41, 19.00s/it]

 99%|█████████▉| 2668/2685 [14:32:56<05:40, 20.03s/it]

 99%|█████████▉| 2669/2685 [14:33:11<04:56, 18.56s/it]

 99%|█████████▉| 2670/2685 [14:33:24<04:12, 16.81s/it]

 99%|█████████▉| 2671/2685 [14:33:44<04:07, 17.71s/it]

100%|█████████▉| 2672/2685 [14:34:03<03:57, 18.26s/it]

100%|█████████▉| 2673/2685 [14:34:23<03:45, 18.77s/it]


100%|█████████▉| 2675/2685 [14:34:55<02:52, 17.29s/it]
{'loss': 0.235, 'learning_rate': 7.277505163139075e-11, 'rewards/chosen': -0.8220298886299133, 'rewards/rejected': -4.9578022956848145, 'rewards/accuracies': 1.0, 'rewards/margins': 4.135772705078125, 'policy_logps/rejected': -356.5700988769531, 'policy_logps/chosen': -325.1077880859375, 'referece_logps/rejected': -306.9920654296875, 'referece_logps/chosen': -316.8874816894531, 'logits/rejected': 0.27571287751197815, 'logits/chosen': 0.2780546545982361, 'epoch': 2.99}

100%|█████████▉| 2676/2685 [14:35:14<02:41, 17.99s/it]

100%|█████████▉| 2677/2685 [14:35:34<02:27, 18.49s/it]


100%|█████████▉| 2679/2685 [14:36:11<01:51, 18.64s/it]
{'loss': 0.2657, 'learning_rate': 2.61992219652285e-11, 'rewards/chosen': -1.099614143371582, 'rewards/rejected': -3.5360257625579834, 'rewards/accuracies': 1.0, 'rewards/margins': 2.4364116191864014, 'policy_logps/rejected': -391.48992919921875, 'policy_logps/chosen': -384.34600830078125, 'referece_logps/rejected': -356.1296691894531, 'referece_logps/chosen': -373.3498840332031, 'logits/rejected': -0.8715531229972839, 'logits/chosen': -0.7475206851959229, 'epoch': 2.99}

100%|█████████▉| 2680/2685 [14:36:30<01:33, 18.65s/it]


100%|█████████▉| 2682/2685 [14:37:05<00:53, 17.86s/it]
{'loss': 0.1653, 'learning_rate': 6.549826941482095e-12, 'rewards/chosen': -1.1359598636627197, 'rewards/rejected': -3.073725700378418, 'rewards/accuracies': 0.875, 'rewards/margins': 1.9377655982971191, 'policy_logps/rejected': -342.30474853515625, 'policy_logps/chosen': -280.7843017578125, 'referece_logps/rejected': -311.5675048828125, 'referece_logps/chosen': -269.4246826171875, 'logits/rejected': -0.4272828996181488, 'logits/chosen': -0.4075705111026764, 'epoch': 3.0}

100%|█████████▉| 2683/2685 [14:37:25<00:36, 18.35s/it]

100%|█████████▉| 2684/2685 [14:37:45<00:18, 18.86s/it]
{'loss': 0.2316, 'learning_rate': 0.0, 'rewards/chosen': -1.2857897281646729, 'rewards/rejected': -4.752517223358154, 'rewards/accuracies': 0.75, 'rewards/margins': 3.4667272567749023, 'policy_logps/rejected': -417.3390197753906, 'policy_logps/chosen': -337.60955810546875, 'referece_logps/rejected': -369.8138427734375, 'referece_logps/chosen': -324.7516784667969, 'logits/rejected': -0.7652381062507629, 'logits/chosen': -0.8733162879943848, 'epoch': 3.0}

100%|██████████| 2685/2685 [14:38:04<00:00, 19.62s/it]